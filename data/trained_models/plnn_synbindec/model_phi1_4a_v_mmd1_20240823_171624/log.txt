Args:
Namespace(name='model_phi1_4a_v_mmd1', outdir='out/model_training/model_phi1_4a_v_mmd1', training_data='data/training_data/data_phi1_4a/training', validation_data='data/training_data/data_phi1_4a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2218502886

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.755043425514145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.755043425514145 | validation: 3.624600160715233]
	TIME [epoch: 43.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.216649813445154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.216649813445154 | validation: 3.876391457923545]
	TIME [epoch: 1.01 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.242831197872504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.242831197872504 | validation: 5.505364723516964]
	TIME [epoch: 0.913 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.048906930664071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.048906930664071 | validation: 5.311870865274529]
	TIME [epoch: 0.91 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.858895260025016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.858895260025016 | validation: 4.37612931189566]
	TIME [epoch: 0.918 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6211638226377225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6211638226377225 | validation: 3.3838583973022063]
	TIME [epoch: 0.913 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.953068589698091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.953068589698091 | validation: 2.589109430363401]
	TIME [epoch: 0.914 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5082060493467253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5082060493467253 | validation: 2.4159611218866868]
	TIME [epoch: 0.909 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.273808978339176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.273808978339176 | validation: 2.518459706275968]
	TIME [epoch: 0.919 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0367155376269888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0367155376269888 | validation: 1.6572819403795833]
	TIME [epoch: 0.916 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.097148794424918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.097148794424918 | validation: 2.124809307066561]
	TIME [epoch: 0.912 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.43432400003305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.43432400003305 | validation: 2.011078984417924]
	TIME [epoch: 0.913 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9429144744082356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9429144744082356 | validation: 1.9256831743736478]
	TIME [epoch: 0.913 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.085962771249231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.085962771249231 | validation: 1.5116216565468903]
	TIME [epoch: 0.913 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.623078391810735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.623078391810735 | validation: 1.3394813333221158]
	TIME [epoch: 0.916 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5359877932643662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5359877932643662 | validation: 1.2776906349249724]
	TIME [epoch: 0.924 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5758637291967623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5758637291967623 | validation: 1.2885158889109576]
	TIME [epoch: 0.914 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5146546149821045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5146546149821045 | validation: 1.0985312981738626]
	TIME [epoch: 0.913 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4056445925395784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4056445925395784 | validation: 1.0163331685196426]
	TIME [epoch: 0.913 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3555523256801925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3555523256801925 | validation: 1.0767528137443068]
	TIME [epoch: 0.92 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3431601904117236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3431601904117236 | validation: 1.1979865011944024]
	TIME [epoch: 0.914 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3726027572329107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3726027572329107 | validation: 1.0219591513095363]
	TIME [epoch: 0.912 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3852003165831213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3852003165831213 | validation: 1.1612253831709645]
	TIME [epoch: 0.914 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3647631848361288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3647631848361288 | validation: 1.0436681252514823]
	TIME [epoch: 0.916 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2329938952045933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2329938952045933 | validation: 0.9033094218996699]
	TIME [epoch: 0.917 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2041465274750267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2041465274750267 | validation: 1.027305444639724]
	TIME [epoch: 0.914 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1849195462933526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1849195462933526 | validation: 0.8957195621719198]
	TIME [epoch: 0.919 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2154399618219065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2154399618219065 | validation: 0.9757645184685751]
	TIME [epoch: 0.92 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1928312624054689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1928312624054689 | validation: 1.1483148997671806]
	TIME [epoch: 0.921 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2055406667844624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2055406667844624 | validation: 0.7964344398084328]
	TIME [epoch: 0.917 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1729966068367095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1729966068367095 | validation: 1.1186679548762324]
	TIME [epoch: 0.912 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.150567359142817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.150567359142817 | validation: 0.761649225311079]
	TIME [epoch: 0.913 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0746221620140115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0746221620140115 | validation: 0.8727547091655588]
	TIME [epoch: 0.914 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0481949559728279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0481949559728279 | validation: 0.921779829069998]
	TIME [epoch: 0.914 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0396782616730706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0396782616730706 | validation: 0.6991342499413287]
	TIME [epoch: 0.914 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0794005458346336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0794005458346336 | validation: 1.0129747554745812]
	TIME [epoch: 0.913 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0910942064305513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0910942064305513 | validation: 0.6762561587605369]
	TIME [epoch: 0.908 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1598483947920857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1598483947920857 | validation: 0.7868494595419887]
	TIME [epoch: 0.909 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.008403999566125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.008403999566125 | validation: 1.152132072632862]
	TIME [epoch: 0.908 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0585977671946025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0585977671946025 | validation: 0.6556966340522887]
	TIME [epoch: 0.907 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1160656288325717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1160656288325717 | validation: 0.8142761331932381]
	TIME [epoch: 0.912 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9796526621044663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9796526621044663 | validation: 1.0408882644264688]
	TIME [epoch: 0.917 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0475085580130326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0475085580130326 | validation: 0.5640064036717288]
	TIME [epoch: 0.917 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0738253743309245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0738253743309245 | validation: 0.6316873972048371]
	TIME [epoch: 0.909 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9702592829138419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9702592829138419 | validation: 1.0618559834870167]
	TIME [epoch: 0.916 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9987207848558519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9987207848558519 | validation: 0.6512705229162514]
	TIME [epoch: 0.911 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9485721009494094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9485721009494094 | validation: 0.6694913369794627]
	TIME [epoch: 0.913 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.96378075089026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.96378075089026 | validation: 0.8158754995774578]
	TIME [epoch: 0.914 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9706393903405334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9706393903405334 | validation: 0.9042747546577705]
	TIME [epoch: 0.913 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0413889345118301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0413889345118301 | validation: 0.7543990257788178]
	TIME [epoch: 0.916 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9922843597029517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9922843597029517 | validation: 0.8755317321894753]
	TIME [epoch: 0.916 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9748563412744625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9748563412744625 | validation: 0.6936521607310997]
	TIME [epoch: 0.913 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9134147368849503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9134147368849503 | validation: 0.7323992049978053]
	TIME [epoch: 0.914 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8805565271729225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8805565271729225 | validation: 0.7055933250124671]
	TIME [epoch: 0.916 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8803286602245529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8803286602245529 | validation: 0.6846938636646026]
	TIME [epoch: 0.916 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8856413930612446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8856413930612446 | validation: 0.7416347162694609]
	TIME [epoch: 0.916 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8975360453577461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8975360453577461 | validation: 0.7665136651224476]
	TIME [epoch: 0.928 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9812018602038041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9812018602038041 | validation: 0.812196210045544]
	TIME [epoch: 0.915 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.985465828042178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.985465828042178 | validation: 1.0806310254230334]
	TIME [epoch: 0.926 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0486568270328034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0486568270328034 | validation: 0.5958683669780351]
	TIME [epoch: 0.919 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9033340592459908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9033340592459908 | validation: 0.7050073295206591]
	TIME [epoch: 0.915 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8531104209719597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8531104209719597 | validation: 0.7149663732826478]
	TIME [epoch: 0.91 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8584284076922453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8584284076922453 | validation: 0.6890290167883525]
	TIME [epoch: 0.909 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8612860136886417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8612860136886417 | validation: 0.783102357858679]
	TIME [epoch: 0.913 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8934746333453366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8934746333453366 | validation: 0.687250010204881]
	TIME [epoch: 0.914 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8991816201661695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8991816201661695 | validation: 0.9564358596990055]
	TIME [epoch: 0.912 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9814698619295604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9814698619295604 | validation: 0.6534219717366851]
	TIME [epoch: 0.915 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8892179579075691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8892179579075691 | validation: 0.6631471516683279]
	TIME [epoch: 0.916 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8592572185729435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8592572185729435 | validation: 0.7729027713504801]
	TIME [epoch: 0.912 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.863173145462548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.863173145462548 | validation: 0.5506086801673395]
	TIME [epoch: 0.912 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9242483411958833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9242483411958833 | validation: 0.8634298364824938]
	TIME [epoch: 0.911 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8858261609693467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8858261609693467 | validation: 0.5511390439511835]
	TIME [epoch: 0.917 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8645896498381538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8645896498381538 | validation: 0.7874894260829306]
	TIME [epoch: 0.912 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8304593724564377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8304593724564377 | validation: 0.5788024493894071]
	TIME [epoch: 0.913 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.825272149332615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.825272149332615 | validation: 0.8502273472363016]
	TIME [epoch: 0.912 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8486085209184907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8486085209184907 | validation: 0.7575118285415887]
	TIME [epoch: 0.912 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.000236603598773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.000236603598773 | validation: 1.313231203725815]
	TIME [epoch: 0.912 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1811520318988926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1811520318988926 | validation: 0.7824577479966781]
	TIME [epoch: 0.911 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8636783640244398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8636783640244398 | validation: 0.5530376381225898]
	TIME [epoch: 0.911 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9133022507413562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9133022507413562 | validation: 0.7412977870359556]
	TIME [epoch: 0.911 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8411729673184841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8411729673184841 | validation: 0.7258735852020245]
	TIME [epoch: 0.913 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8387983786299498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8387983786299498 | validation: 0.5655095454268132]
	TIME [epoch: 0.911 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8475671713858337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8475671713858337 | validation: 0.7386300327052239]
	TIME [epoch: 0.913 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.831712832629321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.831712832629321 | validation: 0.6410644774537119]
	TIME [epoch: 0.913 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8389402609632742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8389402609632742 | validation: 0.7159236033469965]
	TIME [epoch: 0.913 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8707291690401172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8707291690401172 | validation: 0.8850309993261259]
	TIME [epoch: 0.912 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0000348459899737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0000348459899737 | validation: 0.7171541422777298]
	TIME [epoch: 0.913 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8841610727082727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8841610727082727 | validation: 0.7677747942925847]
	TIME [epoch: 0.913 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8572352148566769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8572352148566769 | validation: 0.6368346722881302]
	TIME [epoch: 0.914 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8167367800037405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8167367800037405 | validation: 0.6504417428124563]
	TIME [epoch: 0.911 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8076477526453469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8076477526453469 | validation: 0.6341045106290774]
	TIME [epoch: 0.913 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8067776694286782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8067776694286782 | validation: 0.6399565664265658]
	TIME [epoch: 0.913 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7960946698780623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7960946698780623 | validation: 0.6769930620058553]
	TIME [epoch: 0.91 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8303158553411928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8303158553411928 | validation: 1.0124877767965448]
	TIME [epoch: 0.912 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0884051491356161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0884051491356161 | validation: 0.9470329969416975]
	TIME [epoch: 0.913 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1227185458509417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1227185458509417 | validation: 0.8466618775522934]
	TIME [epoch: 0.913 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9226255915965084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9226255915965084 | validation: 0.646651257063946]
	TIME [epoch: 0.914 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8210010687347747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8210010687347747 | validation: 0.6765353033259894]
	TIME [epoch: 0.91 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8873940786279804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8873940786279804 | validation: 0.7155276349999656]
	TIME [epoch: 0.911 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8443068884820599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8443068884820599 | validation: 0.6781125985492388]
	TIME [epoch: 0.907 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8051784826497271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8051784826497271 | validation: 0.644412452112762]
	TIME [epoch: 0.911 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8043657561258521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8043657561258521 | validation: 0.7120483985019089]
	TIME [epoch: 0.913 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.806880396163142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.806880396163142 | validation: 0.5915001068636373]
	TIME [epoch: 0.912 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8043194247416793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8043194247416793 | validation: 0.8224685011627708]
	TIME [epoch: 0.916 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8326766175796771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8326766175796771 | validation: 0.6264062749131739]
	TIME [epoch: 0.924 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8591772113513167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8591772113513167 | validation: 0.9918354588293842]
	TIME [epoch: 0.911 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9313109198408287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9313109198408287 | validation: 0.6261819754115537]
	TIME [epoch: 0.914 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8522807865650428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8522807865650428 | validation: 0.6610948494604459]
	TIME [epoch: 0.912 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9438328407269353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9438328407269353 | validation: 0.7788270848129092]
	TIME [epoch: 0.918 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.881644458834488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.881644458834488 | validation: 0.5666020437903717]
	TIME [epoch: 0.912 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8292215424511575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8292215424511575 | validation: 0.6600398194744594]
	TIME [epoch: 0.911 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.776556678931299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.776556678931299 | validation: 0.6819557211364448]
	TIME [epoch: 0.913 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7779059752785402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7779059752785402 | validation: 0.5647758045047526]
	TIME [epoch: 0.917 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7787483360968283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7787483360968283 | validation: 0.8565722392149528]
	TIME [epoch: 0.914 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8349961460554925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8349961460554925 | validation: 0.696536264601504]
	TIME [epoch: 0.913 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9404330291468277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9404330291468277 | validation: 1.1435323758532057]
	TIME [epoch: 0.914 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0141743773109102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0141743773109102 | validation: 0.7422603373368954]
	TIME [epoch: 0.917 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8848276303047247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8848276303047247 | validation: 0.4933077106865727]
	TIME [epoch: 0.914 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9109371018373797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9109371018373797 | validation: 0.6477642495653484]
	TIME [epoch: 0.912 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7764349291849443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7764349291849443 | validation: 0.7221487519192474]
	TIME [epoch: 0.917 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7978669920379122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7978669920379122 | validation: 0.5788100617723693]
	TIME [epoch: 0.912 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8042953676345621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8042953676345621 | validation: 0.6392164111249419]
	TIME [epoch: 0.912 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.791825589665697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.791825589665697 | validation: 0.8359544312530036]
	TIME [epoch: 0.914 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8554680775705349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8554680775705349 | validation: 0.7118711591091902]
	TIME [epoch: 0.914 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9297548790101814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9297548790101814 | validation: 0.9515694431967987]
	TIME [epoch: 0.916 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9766654252778775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9766654252778775 | validation: 0.6438528171852833]
	TIME [epoch: 0.913 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7900451186300632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7900451186300632 | validation: 0.5708060899605594]
	TIME [epoch: 0.915 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7717490206649806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7717490206649806 | validation: 0.6426182504403761]
	TIME [epoch: 0.913 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7631827897342135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7631827897342135 | validation: 0.6384183044162047]
	TIME [epoch: 0.914 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7576258426955812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7576258426955812 | validation: 0.6194472556279144]
	TIME [epoch: 0.912 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7668042828523969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7668042828523969 | validation: 0.6465868986911407]
	TIME [epoch: 0.914 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7932027400306814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7932027400306814 | validation: 0.8958079607126447]
	TIME [epoch: 0.911 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9869823990237592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9869823990237592 | validation: 0.8748420832949269]
	TIME [epoch: 0.916 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0485684158100304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0485684158100304 | validation: 0.8725404510242166]
	TIME [epoch: 0.915 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9064455922700814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9064455922700814 | validation: 0.5962859205097238]
	TIME [epoch: 0.914 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7752059040914813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7752059040914813 | validation: 0.6528822915257697]
	TIME [epoch: 0.913 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8480698338938625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8480698338938625 | validation: 0.7515899259721328]
	TIME [epoch: 0.916 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8360245285433038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8360245285433038 | validation: 0.6107662662776749]
	TIME [epoch: 0.914 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7600198329520569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7600198329520569 | validation: 0.6127476806843046]
	TIME [epoch: 0.912 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7534322079906511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7534322079906511 | validation: 0.6505761080705259]
	TIME [epoch: 0.914 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7609742962506837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7609742962506837 | validation: 0.5924452785741597]
	TIME [epoch: 0.914 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7643265317751865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7643265317751865 | validation: 0.7445499109625754]
	TIME [epoch: 0.914 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8115490071879714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8115490071879714 | validation: 0.768312908219485]
	TIME [epoch: 0.914 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9435819208616502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9435819208616502 | validation: 1.0313394661488873]
	TIME [epoch: 0.914 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0610671724760246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0610671724760246 | validation: 0.670001876178249]
	TIME [epoch: 0.915 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7806220580052795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7806220580052795 | validation: 0.5891459350931941]
	TIME [epoch: 0.913 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8037815643077693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8037815643077693 | validation: 0.7927209449458619]
	TIME [epoch: 0.914 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8211494506647727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8211494506647727 | validation: 0.565005936363255]
	TIME [epoch: 0.914 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7630559108040307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7630559108040307 | validation: 0.5963347092984367]
	TIME [epoch: 0.914 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7503757162407763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7503757162407763 | validation: 0.635830004101011]
	TIME [epoch: 0.913 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7519792957323028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7519792957323028 | validation: 0.5902140503530341]
	TIME [epoch: 0.912 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7483232745872949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7483232745872949 | validation: 0.7055512021859873]
	TIME [epoch: 0.91 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7753390286822555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7753390286822555 | validation: 0.7126985685392967]
	TIME [epoch: 0.912 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9017605698503832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9017605698503832 | validation: 1.0857507877710726]
	TIME [epoch: 0.918 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1638782121693505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1638782121693505 | validation: 0.6104206020760495]
	TIME [epoch: 0.923 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7605900123700449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7605900123700449 | validation: 0.5724466548250299]
	TIME [epoch: 0.915 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7979472554695821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7979472554695821 | validation: 0.8314218537333915]
	TIME [epoch: 0.911 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8540358331255442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8540358331255442 | validation: 0.5960784481317821]
	TIME [epoch: 0.919 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.759455708353273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.759455708353273 | validation: 0.6071314189246257]
	TIME [epoch: 0.918 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7438549072853681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7438549072853681 | validation: 0.6164135672654432]
	TIME [epoch: 0.913 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7335821813611968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7335821813611968 | validation: 0.5770250221458845]
	TIME [epoch: 0.915 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7361969867791467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7361969867791467 | validation: 0.7067763457049394]
	TIME [epoch: 0.916 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7572980985473479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7572980985473479 | validation: 0.6794486736060565]
	TIME [epoch: 0.915 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8536871075406728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8536871075406728 | validation: 1.0641980710733556]
	TIME [epoch: 0.924 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0902467514867482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0902467514867482 | validation: 0.6624893686862292]
	TIME [epoch: 0.916 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7743883121662444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7743883121662444 | validation: 0.5003701354760973]
	TIME [epoch: 0.914 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7803176544164123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7803176544164123 | validation: 0.797536276893729]
	TIME [epoch: 0.915 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8069592228409661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8069592228409661 | validation: 0.5365093843745953]
	TIME [epoch: 0.913 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7580792770222456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7580792770222456 | validation: 0.6389842448176168]
	TIME [epoch: 0.912 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7428861851264652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7428861851264652 | validation: 0.6044208376350341]
	TIME [epoch: 0.917 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7290285799294824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7290285799294824 | validation: 0.5546079774161768]
	TIME [epoch: 0.912 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7294534091694361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7294534091694361 | validation: 0.7255540627369688]
	TIME [epoch: 0.914 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7793573319585386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7793573319585386 | validation: 0.773687495441247]
	TIME [epoch: 0.915 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9242928068631668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9242928068631668 | validation: 1.0571042533467023]
	TIME [epoch: 0.915 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.102289015802227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.102289015802227 | validation: 0.5832649530543477]
	TIME [epoch: 0.915 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7242163941865423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7242163941865423 | validation: 0.6489477539820937]
	TIME [epoch: 0.913 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8115585446311289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8115585446311289 | validation: 0.7106627314402101]
	TIME [epoch: 0.911 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8600319520647355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8600319520647355 | validation: 0.6273172101025272]
	TIME [epoch: 0.912 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.741958510455527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.741958510455527 | validation: 0.5577878374958009]
	TIME [epoch: 0.912 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7333149569609535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7333149569609535 | validation: 0.6987977104771906]
	TIME [epoch: 0.913 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7476953284248012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7476953284248012 | validation: 0.5663508524297208]
	TIME [epoch: 0.912 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.755434746892609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.755434746892609 | validation: 0.8054504569851825]
	TIME [epoch: 0.913 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7975786981344326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7975786981344326 | validation: 0.6220947548655922]
	TIME [epoch: 0.916 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7895235224416206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7895235224416206 | validation: 0.757846176876857]
	TIME [epoch: 0.915 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8121322026432255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8121322026432255 | validation: 0.6435545229990126]
	TIME [epoch: 0.914 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7864945473171981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7864945473171981 | validation: 0.5965848517859339]
	TIME [epoch: 0.912 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7922495750921786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7922495750921786 | validation: 0.6731133835678721]
	TIME [epoch: 0.914 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7497749174739269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7497749174739269 | validation: 0.5513577692545043]
	TIME [epoch: 0.912 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7300915254903154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7300915254903154 | validation: 0.5937468885469416]
	TIME [epoch: 0.913 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.709112001587757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.709112001587757 | validation: 0.5979099480576008]
	TIME [epoch: 0.913 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7007864990956398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7007864990956398 | validation: 0.5943056322404857]
	TIME [epoch: 0.913 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7013707071104409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7013707071104409 | validation: 0.6307856783913363]
	TIME [epoch: 0.911 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7181176948367479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7181176948367479 | validation: 0.7245453723318349]
	TIME [epoch: 0.911 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8868144473286736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8868144473286736 | validation: 1.372173086378389]
	TIME [epoch: 0.913 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1853270468266337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1853270468266337 | validation: 0.6398567723599508]
	TIME [epoch: 0.913 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.728107184870702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.728107184870702 | validation: 0.5324392846760341]
	TIME [epoch: 0.914 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8389487888698429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8389487888698429 | validation: 0.7301040589784109]
	TIME [epoch: 0.912 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7795380191677276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7795380191677276 | validation: 0.6368154857197417]
	TIME [epoch: 0.914 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7373209325694056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7373209325694056 | validation: 0.5143794691733943]
	TIME [epoch: 0.912 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7302899313780662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7302899313780662 | validation: 0.6397565944184205]
	TIME [epoch: 0.915 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7003338952010478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7003338952010478 | validation: 0.5979624108275002]
	TIME [epoch: 42.3 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6863615265220863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6863615265220863 | validation: 0.5471801190132871]
	TIME [epoch: 1.79 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6909952813341679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6909952813341679 | validation: 0.60540077911496]
	TIME [epoch: 1.78 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7101996249667261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7101996249667261 | validation: 0.5725877401913324]
	TIME [epoch: 1.78 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7623991906788828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7623991906788828 | validation: 0.7166018013124084]
	TIME [epoch: 1.78 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8509831928234901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8509831928234901 | validation: 0.9983810201355907]
	TIME [epoch: 1.78 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9412858618743405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9412858618743405 | validation: 0.551347746250805]
	TIME [epoch: 1.78 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7372303911656787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7372303911656787 | validation: 0.5916577799178908]
	TIME [epoch: 1.77 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6935465088788368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6935465088788368 | validation: 0.6041565752350531]
	TIME [epoch: 1.79 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6956625538108459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6956625538108459 | validation: 0.5670974586946239]
	TIME [epoch: 1.78 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.706249090513553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.706249090513553 | validation: 0.7297038239352577]
	TIME [epoch: 1.79 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7377093951852194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7377093951852194 | validation: 0.5853505164289643]
	TIME [epoch: 1.79 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7237080777347207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7237080777347207 | validation: 0.6871682530420371]
	TIME [epoch: 1.79 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7310391704502132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7310391704502132 | validation: 0.6053959308422482]
	TIME [epoch: 1.79 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7261610595474564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7261610595474564 | validation: 0.6362949178717351]
	TIME [epoch: 1.78 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7244316338188008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7244316338188008 | validation: 0.5944820142501828]
	TIME [epoch: 1.77 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.708159035722434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.708159035722434 | validation: 0.6365094214907959]
	TIME [epoch: 1.78 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7166816799861888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7166816799861888 | validation: 0.5413486201003642]
	TIME [epoch: 1.79 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7149606031512006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7149606031512006 | validation: 0.7203340001775855]
	TIME [epoch: 1.77 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7214943058167848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7214943058167848 | validation: 0.5647820866338757]
	TIME [epoch: 1.78 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7193132632962675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7193132632962675 | validation: 0.6739272525891643]
	TIME [epoch: 1.78 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7083677829951773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7083677829951773 | validation: 0.5517281399905664]
	TIME [epoch: 1.78 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7053646656856188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7053646656856188 | validation: 0.5850279258147173]
	TIME [epoch: 1.77 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6967309407588045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6967309407588045 | validation: 0.599555583814552]
	TIME [epoch: 1.78 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6802147645303336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6802147645303336 | validation: 0.5570027505790883]
	TIME [epoch: 1.77 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6722446987909407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6722446987909407 | validation: 0.5504870418761832]
	TIME [epoch: 1.77 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.664470654241731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.664470654241731 | validation: 0.6365494836133005]
	TIME [epoch: 1.79 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6853486086714818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6853486086714818 | validation: 0.5491655965564552]
	TIME [epoch: 1.79 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6853101176620546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6853101176620546 | validation: 0.6865818930427912]
	TIME [epoch: 1.78 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7004974753371712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7004974753371712 | validation: 0.5970986964388151]
	TIME [epoch: 1.77 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7819085161433552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7819085161433552 | validation: 0.6929734204265922]
	TIME [epoch: 1.77 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7246938503476177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7246938503476177 | validation: 0.5551716772187224]
	TIME [epoch: 1.78 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6724007374217524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6724007374217524 | validation: 0.4997523950544267]
	TIME [epoch: 1.78 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6522208151500044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6522208151500044 | validation: 0.5750642608029332]
	TIME [epoch: 1.77 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6407443812027449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6407443812027449 | validation: 0.5140693132654003]
	TIME [epoch: 1.77 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.655093406039293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.655093406039293 | validation: 0.6017340303495943]
	TIME [epoch: 1.77 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6547403394406317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6547403394406317 | validation: 0.4762841046788044]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_237.pth
	Model improved!!!
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6714115112983619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6714115112983619 | validation: 0.5680396728744669]
	TIME [epoch: 1.78 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6532323150655174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6532323150655174 | validation: 0.6126543153453453]
	TIME [epoch: 1.78 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6699075371737588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6699075371737588 | validation: 0.5419206767238748]
	TIME [epoch: 1.78 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6721537730913391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6721537730913391 | validation: 0.5104551259598166]
	TIME [epoch: 1.78 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6836649495061532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6836649495061532 | validation: 0.7940334671533673]
	TIME [epoch: 1.79 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.782891474879698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.782891474879698 | validation: 0.6924951267061865]
	TIME [epoch: 1.78 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8304684329382441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8304684329382441 | validation: 0.5156586421994972]
	TIME [epoch: 1.78 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6375809995597439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6375809995597439 | validation: 0.6154456662185107]
	TIME [epoch: 1.78 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6587136004174843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6587136004174843 | validation: 0.4965411290829609]
	TIME [epoch: 1.78 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6383165399322307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6383165399322307 | validation: 0.5328787522059016]
	TIME [epoch: 1.78 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6316496099434302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6316496099434302 | validation: 0.5212831197757575]
	TIME [epoch: 1.78 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6273301237539596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6273301237539596 | validation: 0.5024294428852009]
	TIME [epoch: 1.78 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6217343022023663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6217343022023663 | validation: 0.5149957117977186]
	TIME [epoch: 1.77 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6083604794426526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6083604794426526 | validation: 0.4728431576897738]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_251.pth
	Model improved!!!
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6079059883577073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6079059883577073 | validation: 0.5718656496529374]
	TIME [epoch: 1.77 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6145696823926836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6145696823926836 | validation: 0.5060990057812059]
	TIME [epoch: 1.77 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.642878975381361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.642878975381361 | validation: 0.5578336878620289]
	TIME [epoch: 1.78 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6290088904807608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6290088904807608 | validation: 0.6017143782134804]
	TIME [epoch: 1.78 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.657190975155452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.657190975155452 | validation: 0.6037452199291728]
	TIME [epoch: 1.77 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.734878597059347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.734878597059347 | validation: 0.7255697614790372]
	TIME [epoch: 1.78 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7050765999774137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7050765999774137 | validation: 0.482151497435258]
	TIME [epoch: 1.78 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.657545616227373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.657545616227373 | validation: 0.4985448923070454]
	TIME [epoch: 1.78 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.598913712605125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.598913712605125 | validation: 0.5472853201466309]
	TIME [epoch: 1.78 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6122958579978756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6122958579978756 | validation: 0.5008012413161936]
	TIME [epoch: 1.77 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5941341333800091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5941341333800091 | validation: 0.536370598568285]
	TIME [epoch: 1.77 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5877331756503926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5877331756503926 | validation: 0.4790415265337929]
	TIME [epoch: 1.77 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5847281806086996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5847281806086996 | validation: 0.6951378224550946]
	TIME [epoch: 1.78 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6461291313805774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6461291313805774 | validation: 0.61317566781171]
	TIME [epoch: 1.78 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7577043220827955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7577043220827955 | validation: 0.5154943722425964]
	TIME [epoch: 1.79 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6886418347104933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6886418347104933 | validation: 0.5089082139379325]
	TIME [epoch: 1.77 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6046900349424035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6046900349424035 | validation: 0.618870128520292]
	TIME [epoch: 1.77 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6159494318178367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6159494318178367 | validation: 0.489569260009447]
	TIME [epoch: 1.77 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6098445940622275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6098445940622275 | validation: 0.5864630462966967]
	TIME [epoch: 1.78 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.604080265041224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.604080265041224 | validation: 0.5672357919029262]
	TIME [epoch: 1.77 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6703334907684244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6703334907684244 | validation: 0.7002911657160887]
	TIME [epoch: 1.77 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6429989343926595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6429989343926595 | validation: 0.45946569818378774]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_273.pth
	Model improved!!!
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6076783616734133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6076783616734133 | validation: 0.45017791758883985]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_274.pth
	Model improved!!!
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5562438914723786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5562438914723786 | validation: 0.6303401911944679]
	TIME [epoch: 1.78 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5883003435290939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5883003435290939 | validation: 0.4704471327628803]
	TIME [epoch: 1.78 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6335460775884156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6335460775884156 | validation: 0.47996164951310344]
	TIME [epoch: 1.79 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5432244895231237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5432244895231237 | validation: 0.7014384619389226]
	TIME [epoch: 1.77 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6311708229010775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6311708229010775 | validation: 0.4498824497293048]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_279.pth
	Model improved!!!
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.658140385751346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.658140385751346 | validation: 0.4647485297555868]
	TIME [epoch: 1.78 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5980058965082805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5980058965082805 | validation: 0.5310185942912536]
	TIME [epoch: 1.78 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5562830581076961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5562830581076961 | validation: 0.5396559693622115]
	TIME [epoch: 1.77 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5488029417905246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5488029417905246 | validation: 0.4365277786842767]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_283.pth
	Model improved!!!
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5300414812649108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5300414812649108 | validation: 0.518379926684042]
	TIME [epoch: 1.78 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5261720921055507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5261720921055507 | validation: 0.48619536838564276]
	TIME [epoch: 1.78 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5120179907570519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5120179907570519 | validation: 0.5062238381553599]
	TIME [epoch: 1.78 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4994915369264611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4994915369264611 | validation: 0.4192931000935305]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_287.pth
	Model improved!!!
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5350825316389017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5350825316389017 | validation: 1.0232833690925676]
	TIME [epoch: 1.78 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9555738410322578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9555738410322578 | validation: 0.5760016889530433]
	TIME [epoch: 1.78 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6954789645241148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6954789645241148 | validation: 0.4755121066630416]
	TIME [epoch: 1.78 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6132470682425558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6132470682425558 | validation: 0.595632828316598]
	TIME [epoch: 1.79 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5474772892524097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5474772892524097 | validation: 0.439862896058905]
	TIME [epoch: 1.78 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5031943456583181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5031943456583181 | validation: 0.47015389394152296]
	TIME [epoch: 1.78 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49334550296357316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49334550296357316 | validation: 0.4957124317969]
	TIME [epoch: 1.78 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48467110935601104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48467110935601104 | validation: 0.41318752838074746]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_295.pth
	Model improved!!!
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4735217248766961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4735217248766961 | validation: 0.5680239367582606]
	TIME [epoch: 1.79 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.498918693356136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.498918693356136 | validation: 0.4215104588493448]
	TIME [epoch: 1.78 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6394636371648718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6394636371648718 | validation: 0.4432338581851005]
	TIME [epoch: 1.79 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46445382920844147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46445382920844147 | validation: 0.733833956465277]
	TIME [epoch: 1.78 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6526756277277468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6526756277277468 | validation: 0.4577239852782434]
	TIME [epoch: 1.78 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6396576069678905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6396576069678905 | validation: 0.429327297197385]
	TIME [epoch: 1.78 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5749936829262454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5749936829262454 | validation: 0.5074445279789722]
	TIME [epoch: 1.77 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4854124118498354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4854124118498354 | validation: 0.5294685922192491]
	TIME [epoch: 1.77 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4906424911272511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4906424911272511 | validation: 0.40175382250328395]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_304.pth
	Model improved!!!
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4678368911146881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4678368911146881 | validation: 0.4473707588068492]
	TIME [epoch: 1.77 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44696841167833895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44696841167833895 | validation: 0.41457817880267234]
	TIME [epoch: 1.77 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4268513733320207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4268513733320207 | validation: 0.4304187474382971]
	TIME [epoch: 1.77 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4118421132988198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4118421132988198 | validation: 0.37351079999263215]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_308.pth
	Model improved!!!
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4172735983227268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4172735983227268 | validation: 0.8305045139307481]
	TIME [epoch: 1.77 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8030741795457544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8030741795457544 | validation: 0.5588357865348774]
	TIME [epoch: 1.78 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7526957178298065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7526957178298065 | validation: 0.4827273237518628]
	TIME [epoch: 1.77 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6932300444244478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6932300444244478 | validation: 0.41366271342490424]
	TIME [epoch: 1.79 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4522367866750628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4522367866750628 | validation: 0.6439453957518806]
	TIME [epoch: 1.78 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5441623948871063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5441623948871063 | validation: 0.389987274057609]
	TIME [epoch: 1.78 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4560819746817248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4560819746817248 | validation: 0.43059916194311865]
	TIME [epoch: 1.8 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42658921533522065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42658921533522065 | validation: 0.48132143523447796]
	TIME [epoch: 1.78 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41208060320586326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41208060320586326 | validation: 0.33720920820140377]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_317.pth
	Model improved!!!
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4114210283896429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4114210283896429 | validation: 0.4988452258053895]
	TIME [epoch: 1.78 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4095897471839304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4095897471839304 | validation: 0.36345419736692364]
	TIME [epoch: 1.77 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45285483854467345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45285483854467345 | validation: 0.5264282428775493]
	TIME [epoch: 1.77 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4442838936112233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4442838936112233 | validation: 0.41902639337539455]
	TIME [epoch: 1.78 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5276256477809111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5276256477809111 | validation: 0.36589919949210503]
	TIME [epoch: 1.78 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37252421407042485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37252421407042485 | validation: 0.5353255657155213]
	TIME [epoch: 1.78 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46383773108820875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46383773108820875 | validation: 0.38319503685754785]
	TIME [epoch: 1.78 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.501105065364032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.501105065364032 | validation: 0.37138454495321427]
	TIME [epoch: 1.78 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37154120720037775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37154120720037775 | validation: 0.5203267161949438]
	TIME [epoch: 1.78 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4268525788579409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4268525788579409 | validation: 0.358131888570337]
	TIME [epoch: 1.78 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4337020436742612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4337020436742612 | validation: 0.36482548682000493]
	TIME [epoch: 1.78 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3266807802611421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3266807802611421 | validation: 0.4494666341618979]
	TIME [epoch: 1.78 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35870779054170226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35870779054170226 | validation: 0.33705628874336657]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_330.pth
	Model improved!!!
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44742576325802264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44742576325802264 | validation: 0.430322563273948]
	TIME [epoch: 1.79 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3183041445383293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3183041445383293 | validation: 0.3077317032054914]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_332.pth
	Model improved!!!
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2820945803667227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2820945803667227 | validation: 0.35196381058081017]
	TIME [epoch: 1.78 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27167318697750176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27167318697750176 | validation: 0.28875732586736197]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_334.pth
	Model improved!!!
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32984409478962406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32984409478962406 | validation: 0.7484177519541477]
	TIME [epoch: 1.77 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7379304673956153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7379304673956153 | validation: 0.4403748941764173]
	TIME [epoch: 1.77 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4868991829013972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4868991829013972 | validation: 0.3644536794833543]
	TIME [epoch: 1.78 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31519261294561324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31519261294561324 | validation: 0.4550251421948652]
	TIME [epoch: 1.78 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39540244106320216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39540244106320216 | validation: 0.3282138149155743]
	TIME [epoch: 1.78 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4016610416388018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4016610416388018 | validation: 0.42872298337202913]
	TIME [epoch: 1.79 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30391593275842715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30391593275842715 | validation: 0.3859824561949748]
	TIME [epoch: 1.78 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2913198057060469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2913198057060469 | validation: 0.2741488435443151]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_342.pth
	Model improved!!!
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3449643719862908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3449643719862908 | validation: 0.4812259266376748]
	TIME [epoch: 1.79 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3383436120672516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3383436120672516 | validation: 0.346396208680962]
	TIME [epoch: 1.78 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37474502276337057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37474502276337057 | validation: 0.3478141446547529]
	TIME [epoch: 1.77 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24315430326022663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24315430326022663 | validation: 0.2842576774488233]
	TIME [epoch: 1.79 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22956360904584963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22956360904584963 | validation: 0.31417859184252084]
	TIME [epoch: 1.79 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22276000331493895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22276000331493895 | validation: 0.25175206629185315]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_348.pth
	Model improved!!!
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21852126659950416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21852126659950416 | validation: 0.527810100075571]
	TIME [epoch: 1.77 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41720076544368523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41720076544368523 | validation: 0.48106708498413736]
	TIME [epoch: 1.78 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7480034941607002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7480034941607002 | validation: 0.48055744957444646]
	TIME [epoch: 1.78 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4232315428630193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4232315428630193 | validation: 0.6074272435258735]
	TIME [epoch: 1.79 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47137572438442377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47137572438442377 | validation: 0.36225779825430715]
	TIME [epoch: 1.79 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28453939371125336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28453939371125336 | validation: 0.29516620302178104]
	TIME [epoch: 1.79 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2625945087596171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2625945087596171 | validation: 0.39056068460549026]
	TIME [epoch: 1.79 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2596753850174022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2596753850174022 | validation: 0.2782071614144149]
	TIME [epoch: 1.79 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2544114408364193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2544114408364193 | validation: 0.3602748043245022]
	TIME [epoch: 1.79 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27842113857160694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27842113857160694 | validation: 0.2870935801087807]
	TIME [epoch: 1.78 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38475594890088266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38475594890088266 | validation: 0.42125002556974894]
	TIME [epoch: 1.79 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28071503289902744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28071503289902744 | validation: 0.26309025442536277]
	TIME [epoch: 1.79 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23457487287338916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23457487287338916 | validation: 0.3104866111695308]
	TIME [epoch: 1.79 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21249730771914288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21249730771914288 | validation: 0.25811224566997515]
	TIME [epoch: 1.79 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2209329948465026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2209329948465026 | validation: 0.3682714821324984]
	TIME [epoch: 1.79 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25007633111245436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25007633111245436 | validation: 0.3014391200155783]
	TIME [epoch: 1.79 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4352350562254714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4352350562254714 | validation: 0.39972196253921427]
	TIME [epoch: 1.79 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23112847054082886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23112847054082886 | validation: 0.3023792654521145]
	TIME [epoch: 1.79 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.219091607619809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.219091607619809 | validation: 0.2459207737938025]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_367.pth
	Model improved!!!
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29968329287306855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29968329287306855 | validation: 0.3683030514853724]
	TIME [epoch: 1.78 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24422700984615822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24422700984615822 | validation: 0.2947438205912189]
	TIME [epoch: 1.78 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2859922502330124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2859922502330124 | validation: 0.38233093350719494]
	TIME [epoch: 1.78 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2686940774681793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2686940774681793 | validation: 0.2629580623408146]
	TIME [epoch: 1.79 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25148193803768154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25148193803768154 | validation: 0.3444202943749359]
	TIME [epoch: 1.79 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21204071920213377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21204071920213377 | validation: 0.25939672804086683]
	TIME [epoch: 1.78 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2748934998599345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2748934998599345 | validation: 0.40709988732716645]
	TIME [epoch: 1.79 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30243849956368446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30243849956368446 | validation: 0.26618185728474003]
	TIME [epoch: 1.78 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31713264771305044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31713264771305044 | validation: 0.28746548256728116]
	TIME [epoch: 1.77 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17583321007520156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17583321007520156 | validation: 0.3138190368509527]
	TIME [epoch: 1.77 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20099049419285295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20099049419285295 | validation: 0.24407173111947444]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_378.pth
	Model improved!!!
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2579232800460426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2579232800460426 | validation: 0.3861618760740102]
	TIME [epoch: 1.78 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23278478531891889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23278478531891889 | validation: 0.2595334685182664]
	TIME [epoch: 1.78 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31523231663954554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31523231663954554 | validation: 0.3923688613981375]
	TIME [epoch: 1.77 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2558475421717481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2558475421717481 | validation: 0.2502727976746714]
	TIME [epoch: 1.78 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20685204176686978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20685204176686978 | validation: 0.2753144279699539]
	TIME [epoch: 1.78 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17769172108378506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17769172108378506 | validation: 0.23290471207191046]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_384.pth
	Model improved!!!
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18436003848612814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18436003848612814 | validation: 0.33673066956354536]
	TIME [epoch: 1.78 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22452286151728656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22452286151728656 | validation: 0.23320893532597867]
	TIME [epoch: 1.78 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3155450233574261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3155450233574261 | validation: 0.38912579654420787]
	TIME [epoch: 1.79 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21303743017265242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21303743017265242 | validation: 0.206785551104448]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_388.pth
	Model improved!!!
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16622482654988402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16622482654988402 | validation: 0.2862310866177377]
	TIME [epoch: 1.79 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17075617591653092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17075617591653092 | validation: 0.22184869502113527]
	TIME [epoch: 1.79 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21150982739449176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21150982739449176 | validation: 0.4436219767421197]
	TIME [epoch: 1.79 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3129149415400033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3129149415400033 | validation: 0.25167333672823805]
	TIME [epoch: 1.79 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34496772285551663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34496772285551663 | validation: 0.25413512629505425]
	TIME [epoch: 1.79 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14200689091365634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14200689091365634 | validation: 0.2527826203307865]
	TIME [epoch: 1.78 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14015603538822974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14015603538822974 | validation: 0.19095389377843067]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_395.pth
	Model improved!!!
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18220366998682605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18220366998682605 | validation: 0.41912867357128736]
	TIME [epoch: 1.78 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27854061976054917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27854061976054917 | validation: 0.2288086304103307]
	TIME [epoch: 1.79 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3385931981769787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3385931981769787 | validation: 0.2499872512628341]
	TIME [epoch: 1.78 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13270997951242192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13270997951242192 | validation: 0.34443253538233853]
	TIME [epoch: 1.78 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24244277859692445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24244277859692445 | validation: 0.23801365448814857]
	TIME [epoch: 1.78 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26044520700231105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26044520700231105 | validation: 0.3133673903614744]
	TIME [epoch: 1.78 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23105932744042673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23105932744042673 | validation: 0.2615675644939716]
	TIME [epoch: 1.79 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15752094851363357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15752094851363357 | validation: 0.18331238207309852]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_403.pth
	Model improved!!!
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12829453088309847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12829453088309847 | validation: 0.25282271851834454]
	TIME [epoch: 1.78 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15417616207833337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15417616207833337 | validation: 0.2121595434496474]
	TIME [epoch: 1.78 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.201800367496018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.201800367496018 | validation: 0.32732912442993634]
	TIME [epoch: 1.77 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2111070046722225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2111070046722225 | validation: 0.17317664832234847]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_407.pth
	Model improved!!!
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20014351782343145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20014351782343145 | validation: 0.30080044386908666]
	TIME [epoch: 1.77 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13844246709904096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13844246709904096 | validation: 0.15733314101817766]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_409.pth
	Model improved!!!
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1293806958306898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1293806958306898 | validation: 0.3045562445077073]
	TIME [epoch: 1.78 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18366068572195304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18366068572195304 | validation: 0.2345370506306387]
	TIME [epoch: 1.79 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26939893623213823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26939893623213823 | validation: 0.4086220165470737]
	TIME [epoch: 1.79 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29359999438338014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29359999438338014 | validation: 0.16892871646951077]
	TIME [epoch: 1.79 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15913720967987913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15913720967987913 | validation: 0.2649797139794371]
	TIME [epoch: 1.8 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13708872968319616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13708872968319616 | validation: 0.187233865570052]
	TIME [epoch: 1.78 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12328598361989514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12328598361989514 | validation: 0.19945052459897408]
	TIME [epoch: 1.78 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1264714806909359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1264714806909359 | validation: 0.1941635143418186]
	TIME [epoch: 1.77 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11593654677702352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11593654677702352 | validation: 0.18167876553057255]
	TIME [epoch: 1.79 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1386697386275505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1386697386275505 | validation: 0.18964672060691967]
	TIME [epoch: 1.79 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2520737552496169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2520737552496169 | validation: 0.4575746469541508]
	TIME [epoch: 1.78 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2845590439152267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2845590439152267 | validation: 0.1567912099483856]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_421.pth
	Model improved!!!
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22886185462473624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22886185462473624 | validation: 0.2601757146403442]
	TIME [epoch: 1.78 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16100192039681258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16100192039681258 | validation: 0.20842842489602242]
	TIME [epoch: 1.78 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.115798412554429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.115798412554429 | validation: 0.1226823494225708]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_424.pth
	Model improved!!!
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10580925886411806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10580925886411806 | validation: 0.18128320166237413]
	TIME [epoch: 1.78 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09152356640699885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09152356640699885 | validation: 0.13446784658969402]
	TIME [epoch: 1.78 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09732321680985105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09732321680985105 | validation: 0.16009678177003367]
	TIME [epoch: 1.79 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13270737261227095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13270737261227095 | validation: 0.40179292866436267]
	TIME [epoch: 1.78 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2816698820323444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2816698820323444 | validation: 0.18712816000548338]
	TIME [epoch: 1.78 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2789621102929039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2789621102929039 | validation: 0.24254982129036953]
	TIME [epoch: 1.77 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10644804506219131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10644804506219131 | validation: 0.18745736935904322]
	TIME [epoch: 1.78 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10564455334246281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10564455334246281 | validation: 0.25515592709881857]
	TIME [epoch: 1.78 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13159622605129073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13159622605129073 | validation: 0.14722893537711723]
	TIME [epoch: 1.78 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1577880526066283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1577880526066283 | validation: 0.3757067097475529]
	TIME [epoch: 1.78 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25453978567021174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25453978567021174 | validation: 0.20492111343617064]
	TIME [epoch: 1.78 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.299252963201166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.299252963201166 | validation: 0.20735884164844795]
	TIME [epoch: 1.78 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10498325436879835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10498325436879835 | validation: 0.1523146968789787]
	TIME [epoch: 1.78 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09123456444081096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09123456444081096 | validation: 0.1655627135521421]
	TIME [epoch: 1.78 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.119466049778731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.119466049778731 | validation: 0.18550779514271248]
	TIME [epoch: 1.79 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1614456038503468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1614456038503468 | validation: 0.14632006594520278]
	TIME [epoch: 1.78 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14449141487160888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14449141487160888 | validation: 0.24426464106880222]
	TIME [epoch: 1.77 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10633606262247224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10633606262247224 | validation: 0.08209951167622825]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_442.pth
	Model improved!!!
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13383317444999135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13383317444999135 | validation: 0.3090538587180359]
	TIME [epoch: 1.78 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1707254189912116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1707254189912116 | validation: 0.14664279337468572]
	TIME [epoch: 1.78 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1558001613485745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1558001613485745 | validation: 0.18221718755344193]
	TIME [epoch: 1.78 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1331809090728887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1331809090728887 | validation: 0.17902170240793258]
	TIME [epoch: 1.79 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1136862589515777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1136862589515777 | validation: 0.18776655517372953]
	TIME [epoch: 1.77 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13445464367559265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13445464367559265 | validation: 0.1717577914165924]
	TIME [epoch: 1.78 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15330714409378915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15330714409378915 | validation: 0.31627784576066]
	TIME [epoch: 1.77 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16545900887339635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16545900887339635 | validation: 0.11018004737980672]
	TIME [epoch: 1.79 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13485106207673891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13485106207673891 | validation: 0.23826847932419085]
	TIME [epoch: 1.78 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1572089150819783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1572089150819783 | validation: 0.1898166257050046]
	TIME [epoch: 1.78 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17610992194959552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17610992194959552 | validation: 0.1890308184784837]
	TIME [epoch: 1.78 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15060398164931055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15060398164931055 | validation: 0.11455507125131809]
	TIME [epoch: 1.78 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08490968681611195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08490968681611195 | validation: 0.1757983654454267]
	TIME [epoch: 1.78 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07471127963847327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07471127963847327 | validation: 0.09965468336110861]
	TIME [epoch: 1.79 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06487498335814026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06487498335814026 | validation: 0.16530378977432633]
	TIME [epoch: 1.78 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07575490929525457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07575490929525457 | validation: 0.10849220446959379]
	TIME [epoch: 1.77 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0931707255667791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0931707255667791 | validation: 0.30690091050466045]
	TIME [epoch: 1.78 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.190287044330555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.190287044330555 | validation: 0.20029036577948395]
	TIME [epoch: 1.78 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2890507110193717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2890507110193717 | validation: 0.3047427533921983]
	TIME [epoch: 1.77 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15178885284183377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15178885284183377 | validation: 0.07788862906755828]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_462.pth
	Model improved!!!
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07430155591268284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07430155591268284 | validation: 0.12810191259208836]
	TIME [epoch: 1.8 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07835998928729856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07835998928729856 | validation: 0.1235801473175729]
	TIME [epoch: 1.79 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10250062992746557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10250062992746557 | validation: 0.14072109643353997]
	TIME [epoch: 1.79 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12262970340844279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12262970340844279 | validation: 0.19028035817965613]
	TIME [epoch: 1.78 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12024952751393464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12024952751393464 | validation: 0.11907681208231265]
	TIME [epoch: 1.78 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15390374313276797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15390374313276797 | validation: 0.328993349115984]
	TIME [epoch: 1.77 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19934912329372964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19934912329372964 | validation: 0.145147316145501]
	TIME [epoch: 1.78 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11414958896223584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11414958896223584 | validation: 0.1057901922224584]
	TIME [epoch: 1.77 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15816432813712944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15816432813712944 | validation: 0.26835107649638273]
	TIME [epoch: 1.78 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11687116069417909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11687116069417909 | validation: 0.12764812482723378]
	TIME [epoch: 1.79 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10007815799938367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10007815799938367 | validation: 0.15576345213308007]
	TIME [epoch: 1.78 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13833964678566632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13833964678566632 | validation: 0.10937929359876658]
	TIME [epoch: 1.78 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11657140937735286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11657140937735286 | validation: 0.2534554931396489]
	TIME [epoch: 1.78 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12005846559053439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12005846559053439 | validation: 0.11287693271949989]
	TIME [epoch: 1.79 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09908499135240241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09908499135240241 | validation: 0.11416047980408589]
	TIME [epoch: 1.78 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07174182186453437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07174182186453437 | validation: 0.10223011630054778]
	TIME [epoch: 1.77 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06446076176353574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06446076176353574 | validation: 0.1364955443525997]
	TIME [epoch: 1.78 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07600630551459077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07600630551459077 | validation: 0.11110533965963985]
	TIME [epoch: 1.78 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10756813480902506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10756813480902506 | validation: 0.30060888792302964]
	TIME [epoch: 1.78 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1936538242487424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1936538242487424 | validation: 0.1568599798446303]
	TIME [epoch: 1.77 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15615138511170967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15615138511170967 | validation: 0.15323447714226446]
	TIME [epoch: 1.78 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12438079750993902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12438079750993902 | validation: 0.14160308750112344]
	TIME [epoch: 1.78 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10346226316298332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10346226316298332 | validation: 0.16872757773127095]
	TIME [epoch: 1.78 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1207385265058056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1207385265058056 | validation: 0.1424552037074053]
	TIME [epoch: 1.77 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10227412525128882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10227412525128882 | validation: 0.13015729811785937]
	TIME [epoch: 1.78 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07169650581617687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07169650581617687 | validation: 0.06569660319159527]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_488.pth
	Model improved!!!
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04702833276257168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04702833276257168 | validation: 0.09818248992310367]
	TIME [epoch: 1.8 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04298648079162785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04298648079162785 | validation: 0.05551700617563485]
	TIME [epoch: 1.8 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_490.pth
	Model improved!!!
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0526675528641714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0526675528641714 | validation: 0.2697365889075836]
	TIME [epoch: 1.79 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1562839983608066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1562839983608066 | validation: 0.21159440147730293]
	TIME [epoch: 1.78 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29031764261981996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29031764261981996 | validation: 0.2265384606053739]
	TIME [epoch: 1.78 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16051662943669762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16051662943669762 | validation: 0.13600823949013033]
	TIME [epoch: 1.78 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06812415836002698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06812415836002698 | validation: 0.0684818285241897]
	TIME [epoch: 1.78 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06607877507942747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06607877507942747 | validation: 0.17598833937037509]
	TIME [epoch: 1.78 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08169932125824722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08169932125824722 | validation: 0.07245202479760826]
	TIME [epoch: 1.78 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051415880029484454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051415880029484454 | validation: 0.05314914430854877]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_498.pth
	Model improved!!!
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04413171290181921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04413171290181921 | validation: 0.11142078507780157]
	TIME [epoch: 1.79 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06299231316894705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06299231316894705 | validation: 0.11608876123373957]
	TIME [epoch: 1.79 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12133256991530544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12133256991530544 | validation: 0.21517695848381757]
	TIME [epoch: 43.9 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15031244564552682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15031244564552682 | validation: 0.16172179047012736]
	TIME [epoch: 3.53 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12239885127522578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12239885127522578 | validation: 0.08524563742924947]
	TIME [epoch: 3.51 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09115409085619668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09115409085619668 | validation: 0.2684726549099276]
	TIME [epoch: 3.51 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13710778796825474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13710778796825474 | validation: 0.17983543280630912]
	TIME [epoch: 3.52 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16611121498961193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16611121498961193 | validation: 0.2083979902823817]
	TIME [epoch: 3.52 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21322034150752164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21322034150752164 | validation: 0.09966104212893254]
	TIME [epoch: 3.5 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07247485708675074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07247485708675074 | validation: 0.10789491620151825]
	TIME [epoch: 3.53 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04758406066397023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04758406066397023 | validation: 0.07292450585579782]
	TIME [epoch: 3.53 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04354256104399495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04354256104399495 | validation: 0.0749366011390361]
	TIME [epoch: 3.51 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04437408248244481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04437408248244481 | validation: 0.07155214390729674]
	TIME [epoch: 3.5 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046394507453242265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046394507453242265 | validation: 0.10360991470877295]
	TIME [epoch: 3.51 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058945605668685806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058945605668685806 | validation: 0.11790005574842]
	TIME [epoch: 3.51 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11012029793786468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11012029793786468 | validation: 0.2700239480650582]
	TIME [epoch: 3.54 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20333719550545495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20333719550545495 | validation: 0.1298442909374765]
	TIME [epoch: 3.51 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11904156763411804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11904156763411804 | validation: 0.18227200803716165]
	TIME [epoch: 3.51 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12232971576713547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12232971576713547 | validation: 0.15839737379882196]
	TIME [epoch: 3.52 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10582579186500204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10582579186500204 | validation: 0.1332481353090773]
	TIME [epoch: 3.52 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08869383172266805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08869383172266805 | validation: 0.08330773904001308]
	TIME [epoch: 3.51 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04976936528818879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04976936528818879 | validation: 0.05891239176971208]
	TIME [epoch: 3.51 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03862867330686922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03862867330686922 | validation: 0.06232726630688719]
	TIME [epoch: 3.51 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034879968094818345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034879968094818345 | validation: 0.11359925107650182]
	TIME [epoch: 3.51 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05219450809940735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05219450809940735 | validation: 0.10342831362920385]
	TIME [epoch: 3.52 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09696685601468985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09696685601468985 | validation: 0.25729324246685065]
	TIME [epoch: 3.52 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19781357354665907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19781357354665907 | validation: 0.14525232936021915]
	TIME [epoch: 3.51 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1239102296008198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1239102296008198 | validation: 0.10592649998198185]
	TIME [epoch: 3.52 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10122502438303023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10122502438303023 | validation: 0.1838752439132354]
	TIME [epoch: 3.51 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09857898238673667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09857898238673667 | validation: 0.07096123382751401]
	TIME [epoch: 3.53 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05267871338445468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05267871338445468 | validation: 0.08257178696550586]
	TIME [epoch: 3.51 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.078534023589235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.078534023589235 | validation: 0.2402369302759381]
	TIME [epoch: 3.51 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12087961965896972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12087961965896972 | validation: 0.13261603976458375]
	TIME [epoch: 3.53 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10442614151523405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10442614151523405 | validation: 0.12643364676988433]
	TIME [epoch: 3.51 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09484952467200192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09484952467200192 | validation: 0.11211320840611755]
	TIME [epoch: 3.51 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10256184164014837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10256184164014837 | validation: 0.17557888002749056]
	TIME [epoch: 3.52 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13541502170144196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13541502170144196 | validation: 0.11032897291560442]
	TIME [epoch: 3.52 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07745738020620285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07745738020620285 | validation: 0.10493307580532574]
	TIME [epoch: 3.51 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07086871541396021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07086871541396021 | validation: 0.0784749266765784]
	TIME [epoch: 3.51 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04292404228775796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04292404228775796 | validation: 0.06525200822028905]
	TIME [epoch: 3.52 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036422490973087335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036422490973087335 | validation: 0.07094088635873631]
	TIME [epoch: 3.52 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03732895789286794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03732895789286794 | validation: 0.060858971257554274]
	TIME [epoch: 3.51 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044478747994679796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044478747994679796 | validation: 0.10432261570776935]
	TIME [epoch: 3.54 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06262430790151467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06262430790151467 | validation: 0.11173463984796267]
	TIME [epoch: 3.52 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09446025856933223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09446025856933223 | validation: 0.15433873953085547]
	TIME [epoch: 3.53 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10191524842237609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10191524842237609 | validation: 0.09278497280744635]
	TIME [epoch: 3.53 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07099918982854078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07099918982854078 | validation: 0.07020101699340202]
	TIME [epoch: 3.51 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07497025731041158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07497025731041158 | validation: 0.33660046436866736]
	TIME [epoch: 3.51 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21724415637063943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21724415637063943 | validation: 0.22340928383030173]
	TIME [epoch: 3.51 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23614755620322053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23614755620322053 | validation: 0.14445261977124121]
	TIME [epoch: 3.52 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14246008141685734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14246008141685734 | validation: 0.12756276541056627]
	TIME [epoch: 3.52 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053905547630754766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053905547630754766 | validation: 0.06893136013170732]
	TIME [epoch: 3.51 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05753716436198297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05753716436198297 | validation: 0.10394174380323902]
	TIME [epoch: 3.51 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08132956471067104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08132956471067104 | validation: 0.09096991807287476]
	TIME [epoch: 3.52 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06396741587348574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06396741587348574 | validation: 0.07462375587783536]
	TIME [epoch: 3.51 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06087706525300703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06087706525300703 | validation: 0.06672730140097767]
	TIME [epoch: 3.53 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04340121129497815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04340121129497815 | validation: 0.07880687202089706]
	TIME [epoch: 3.51 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047415549856135206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047415549856135206 | validation: 0.07191002204494268]
	TIME [epoch: 3.53 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05687238842190295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05687238842190295 | validation: 0.09786492321063238]
	TIME [epoch: 3.51 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07821720290241077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07821720290241077 | validation: 0.09770992643339661]
	TIME [epoch: 3.51 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08083074673014419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08083074673014419 | validation: 0.09513579062794994]
	TIME [epoch: 3.53 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08023624252006353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08023624252006353 | validation: 0.0843185571449218]
	TIME [epoch: 3.53 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05950468137502764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05950468137502764 | validation: 0.12688524616148558]
	TIME [epoch: 3.52 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07103056641220742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07103056641220742 | validation: 0.11904387416545958]
	TIME [epoch: 3.51 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09585863882058476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09585863882058476 | validation: 0.33004922479093424]
	TIME [epoch: 3.52 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19468790692409366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19468790692409366 | validation: 0.08960989339610914]
	TIME [epoch: 3.51 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08054902730954344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08054902730954344 | validation: 0.052487920808919064]
	TIME [epoch: 3.51 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_565.pth
	Model improved!!!
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03929282788171367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03929282788171367 | validation: 0.06624447522039235]
	TIME [epoch: 3.53 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03217063784204514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03217063784204514 | validation: 0.040367425124189074]
	TIME [epoch: 3.55 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_567.pth
	Model improved!!!
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03434486651144909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03434486651144909 | validation: 0.10034328614384701]
	TIME [epoch: 3.54 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0446980600939251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0446980600939251 | validation: 0.05350918084887568]
	TIME [epoch: 3.51 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04864201240654186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04864201240654186 | validation: 0.16756404352877308]
	TIME [epoch: 3.51 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08735790754489464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08735790754489464 | validation: 0.11752673789074138]
	TIME [epoch: 3.52 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11579557497961103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11579557497961103 | validation: 0.12339498853639048]
	TIME [epoch: 3.53 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1827115875459706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1827115875459706 | validation: 0.11296544869397518]
	TIME [epoch: 3.54 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07056343361591305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07056343361591305 | validation: 0.06959982741887967]
	TIME [epoch: 3.51 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04335244579257255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04335244579257255 | validation: 0.11039214239443612]
	TIME [epoch: 3.51 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0815521763197726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0815521763197726 | validation: 0.21156229970175935]
	TIME [epoch: 3.51 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10916218885478332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10916218885478332 | validation: 0.0896989791096354]
	TIME [epoch: 3.51 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06419745703575899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06419745703575899 | validation: 0.09703090564239769]
	TIME [epoch: 3.52 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08592561361448635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08592561361448635 | validation: 0.12592277257720938]
	TIME [epoch: 3.51 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1847774888491616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1847774888491616 | validation: 0.23735067384025885]
	TIME [epoch: 3.51 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14184346282574134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14184346282574134 | validation: 0.08927258588910489]
	TIME [epoch: 3.54 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05395628986930065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05395628986930065 | validation: 0.05480892679338143]
	TIME [epoch: 3.53 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0505221444640366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0505221444640366 | validation: 0.11691267967624973]
	TIME [epoch: 3.53 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05187790700569653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05187790700569653 | validation: 0.056387766648182215]
	TIME [epoch: 3.53 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03540881311419896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03540881311419896 | validation: 0.04111628865264036]
	TIME [epoch: 3.51 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036932487701765035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036932487701765035 | validation: 0.06575420426836884]
	TIME [epoch: 3.51 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035305071524102276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035305071524102276 | validation: 0.04345247349528906]
	TIME [epoch: 3.51 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036800060836681796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036800060836681796 | validation: 0.07787168392482915]
	TIME [epoch: 3.52 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06623180875396706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06623180875396706 | validation: 0.23240159237278024]
	TIME [epoch: 3.51 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2058229167436641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2058229167436641 | validation: 0.1999983714910159]
	TIME [epoch: 3.53 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1892851546045371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1892851546045371 | validation: 0.20457908625389354]
	TIME [epoch: 3.51 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1389746228793479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1389746228793479 | validation: 0.07854936665235389]
	TIME [epoch: 3.52 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03932803579887904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03932803579887904 | validation: 0.056388966810015886]
	TIME [epoch: 3.52 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052221310427972546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052221310427972546 | validation: 0.1216907817915149]
	TIME [epoch: 3.53 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055570842231940316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055570842231940316 | validation: 0.052395196181214146]
	TIME [epoch: 3.54 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045739952161853906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045739952161853906 | validation: 0.09967959898522248]
	TIME [epoch: 3.51 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06642875143934912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06642875143934912 | validation: 0.14371438367946263]
	TIME [epoch: 3.52 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10972088086165309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10972088086165309 | validation: 0.1102456762829771]
	TIME [epoch: 3.53 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07886890652410332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07886890652410332 | validation: 0.07549912814030985]
	TIME [epoch: 3.53 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045144397348136175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045144397348136175 | validation: 0.05390081175335992]
	TIME [epoch: 3.52 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0312354434824538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0312354434824538 | validation: 0.036675978464036973]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_601.pth
	Model improved!!!
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027045911692415603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027045911692415603 | validation: 0.05404076491322649]
	TIME [epoch: 3.52 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039110163652489933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039110163652489933 | validation: 0.08415216282668621]
	TIME [epoch: 3.53 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09794659715312999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09794659715312999 | validation: 0.18061668754611673]
	TIME [epoch: 3.51 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17572880771952876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17572880771952876 | validation: 0.2667322884376246]
	TIME [epoch: 3.53 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20979726053042483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20979726053042483 | validation: 0.051412949190254645]
	TIME [epoch: 3.53 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06784332156261051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06784332156261051 | validation: 0.14065361143117938]
	TIME [epoch: 3.54 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.076064365328773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.076064365328773 | validation: 0.0800724893786208]
	TIME [epoch: 3.53 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055137765476686446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055137765476686446 | validation: 0.08557009516492445]
	TIME [epoch: 3.53 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07524670096057891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07524670096057891 | validation: 0.11464299679542395]
	TIME [epoch: 3.53 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06875245470245403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06875245470245403 | validation: 0.05860617687368123]
	TIME [epoch: 3.54 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041143861113261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041143861113261 | validation: 0.048714150964093074]
	TIME [epoch: 3.54 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03676361028960155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03676361028960155 | validation: 0.0688666802732614]
	TIME [epoch: 3.52 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0347865120954734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0347865120954734 | validation: 0.0658281925930433]
	TIME [epoch: 3.53 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04905864312872016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04905864312872016 | validation: 0.13748844113860026]
	TIME [epoch: 3.52 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0925912642802166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0925912642802166 | validation: 0.1174128270704241]
	TIME [epoch: 3.53 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10727097514145736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10727097514145736 | validation: 0.08351332731877863]
	TIME [epoch: 3.53 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05794062002252522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05794062002252522 | validation: 0.046380093296438496]
	TIME [epoch: 3.54 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02702002971824856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02702002971824856 | validation: 0.06376339607381047]
	TIME [epoch: 3.53 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03302220447784292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03302220447784292 | validation: 0.09026864105276383]
	TIME [epoch: 3.53 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09547899330170567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09547899330170567 | validation: 0.3105257763753848]
	TIME [epoch: 3.54 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.248404962928874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.248404962928874 | validation: 0.11736162595331101]
	TIME [epoch: 3.54 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.085998002692743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.085998002692743 | validation: 0.07645117903330746]
	TIME [epoch: 3.54 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054770709512003954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054770709512003954 | validation: 0.09209794274794564]
	TIME [epoch: 3.54 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05196108534997111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05196108534997111 | validation: 0.0439196712135852]
	TIME [epoch: 3.54 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03362888451435476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03362888451435476 | validation: 0.04297722883855071]
	TIME [epoch: 3.53 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03060653655327645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03060653655327645 | validation: 0.061995524340233414]
	TIME [epoch: 3.53 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035856126743892276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035856126743892276 | validation: 0.053304253186464506]
	TIME [epoch: 3.53 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05555128211263603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05555128211263603 | validation: 0.11077561718053153]
	TIME [epoch: 3.53 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06537416187785813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06537416187785813 | validation: 0.08311350324546253]
	TIME [epoch: 3.53 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06940381295829637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06940381295829637 | validation: 0.09388703598618797]
	TIME [epoch: 3.53 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05998415591716315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05998415591716315 | validation: 0.08057292730977576]
	TIME [epoch: 3.53 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057409877772818786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057409877772818786 | validation: 0.05732165670792532]
	TIME [epoch: 3.54 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045393581457734526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045393581457734526 | validation: 0.12600813586444468]
	TIME [epoch: 3.54 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07296610938036918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07296610938036918 | validation: 0.15234404414206595]
	TIME [epoch: 3.54 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13427832183180957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13427832183180957 | validation: 0.2744558654332396]
	TIME [epoch: 3.52 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21263346250826526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21263346250826526 | validation: 0.07466529837746844]
	TIME [epoch: 3.52 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053794584117341594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053794584117341594 | validation: 0.03634514342534661]
	TIME [epoch: 3.53 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_638.pth
	Model improved!!!
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05169647872140047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05169647872140047 | validation: 0.16881742267578886]
	TIME [epoch: 3.54 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07894368331889308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07894368331889308 | validation: 0.04367993864271994]
	TIME [epoch: 3.53 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02899685405374113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02899685405374113 | validation: 0.05059652443427583]
	TIME [epoch: 3.54 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04179437175076271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04179437175076271 | validation: 0.0919657319584699]
	TIME [epoch: 3.53 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050627745853787494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050627745853787494 | validation: 0.05855744049132869]
	TIME [epoch: 3.53 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04069844262945572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04069844262945572 | validation: 0.06133359740541701]
	TIME [epoch: 3.54 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0768699428352414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0768699428352414 | validation: 0.12337858177211816]
	TIME [epoch: 3.53 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09679926771550303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09679926771550303 | validation: 0.1157313518785545]
	TIME [epoch: 3.55 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12085033078647647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12085033078647647 | validation: 0.08501943126865266]
	TIME [epoch: 3.54 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08057071392295867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08057071392295867 | validation: 0.15240658040053642]
	TIME [epoch: 3.55 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10645294190213128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10645294190213128 | validation: 0.16268204253477347]
	TIME [epoch: 3.53 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10348878089445572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10348878089445572 | validation: 0.11286313842898324]
	TIME [epoch: 3.53 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06420482054656114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06420482054656114 | validation: 0.05103248291633973]
	TIME [epoch: 3.54 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027208582886674066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027208582886674066 | validation: 0.046477449051240904]
	TIME [epoch: 3.54 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029401692576226587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029401692576226587 | validation: 0.06463147394015449]
	TIME [epoch: 3.53 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04090999668030062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04090999668030062 | validation: 0.060945495478237965]
	TIME [epoch: 3.53 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05914718679575273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05914718679575273 | validation: 0.09280202412364014]
	TIME [epoch: 3.53 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07001749190207654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07001749190207654 | validation: 0.09314179934767619]
	TIME [epoch: 3.54 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06835647504528551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06835647504528551 | validation: 0.058091581185890244]
	TIME [epoch: 3.53 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04716400285909723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04716400285909723 | validation: 0.13949039365649252]
	TIME [epoch: 3.54 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07336679751270732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07336679751270732 | validation: 0.13899669174068566]
	TIME [epoch: 3.53 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12362921679360823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12362921679360823 | validation: 0.2276694514389751]
	TIME [epoch: 3.54 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14478832376798814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14478832376798814 | validation: 0.08579849370945968]
	TIME [epoch: 3.54 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048873411644655265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048873411644655265 | validation: 0.04620733604846847]
	TIME [epoch: 3.54 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05177468691414078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05177468691414078 | validation: 0.14632142159437916]
	TIME [epoch: 3.53 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06256152859547288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06256152859547288 | validation: 0.04648070834765944]
	TIME [epoch: 3.54 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0327530861186545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0327530861186545 | validation: 0.04292559253546188]
	TIME [epoch: 3.53 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030269874908328217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030269874908328217 | validation: 0.07299085390467874]
	TIME [epoch: 3.53 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03217004388465715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03217004388465715 | validation: 0.05528291738921831]
	TIME [epoch: 3.53 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03945403991195267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03945403991195267 | validation: 0.08529521152369056]
	TIME [epoch: 3.53 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07816626309836608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07816626309836608 | validation: 0.15715758069806893]
	TIME [epoch: 3.52 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11439038440124435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11439038440124435 | validation: 0.07011599123948871]
	TIME [epoch: 3.51 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07564728332264647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07564728332264647 | validation: 0.07768670727812489]
	TIME [epoch: 3.52 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05202218894407299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05202218894407299 | validation: 0.09149199947689976]
	TIME [epoch: 3.51 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0570590558014734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0570590558014734 | validation: 0.09251123750948348]
	TIME [epoch: 3.51 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06491092194207174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06491092194207174 | validation: 0.30691353200407556]
	TIME [epoch: 3.53 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16102469648468018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16102469648468018 | validation: 0.10594919649009876]
	TIME [epoch: 3.53 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0932316449164426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0932316449164426 | validation: 0.063052376345269]
	TIME [epoch: 3.53 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054485285055424486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054485285055424486 | validation: 0.053771707954261595]
	TIME [epoch: 3.52 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031975884491049766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031975884491049766 | validation: 0.06915915660430498]
	TIME [epoch: 3.53 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03372478020736888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03372478020736888 | validation: 0.03490856993317664]
	TIME [epoch: 3.51 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_679.pth
	Model improved!!!
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028623228641509506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028623228641509506 | validation: 0.05010255835432197]
	TIME [epoch: 3.54 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029297822935742936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029297822935742936 | validation: 0.04343493206449416]
	TIME [epoch: 3.53 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03428221391464144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03428221391464144 | validation: 0.11394158684537006]
	TIME [epoch: 3.53 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07371966313385933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07371966313385933 | validation: 0.12303389085129841]
	TIME [epoch: 3.52 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10475449074996057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10475449074996057 | validation: 0.09243552261674653]
	TIME [epoch: 3.54 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09673156991925791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09673156991925791 | validation: 0.09680477938171643]
	TIME [epoch: 3.54 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06187867168429421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06187867168429421 | validation: 0.1908142010133574]
	TIME [epoch: 3.55 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10427931869224384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10427931869224384 | validation: 0.14815300336161483]
	TIME [epoch: 3.53 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11648934524917856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11648934524917856 | validation: 0.15079371507136308]
	TIME [epoch: 3.54 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08812003537116082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08812003537116082 | validation: 0.03979716074703571]
	TIME [epoch: 3.52 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027861098678245856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027861098678245856 | validation: 0.04187078634726828]
	TIME [epoch: 3.52 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031055605092599345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031055605092599345 | validation: 0.08880463005646766]
	TIME [epoch: 3.51 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051646481701405504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051646481701405504 | validation: 0.040098239347102604]
	TIME [epoch: 3.55 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057178959378494466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057178959378494466 | validation: 0.07756875381203704]
	TIME [epoch: 3.53 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03492650804512598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03492650804512598 | validation: 0.03631801478713386]
	TIME [epoch: 3.53 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02309643564938896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02309643564938896 | validation: 0.04891118494723269]
	TIME [epoch: 3.54 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02439598049387058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02439598049387058 | validation: 0.056743070194581774]
	TIME [epoch: 3.52 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04714746204530933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04714746204530933 | validation: 0.13926769855832694]
	TIME [epoch: 3.52 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12079778532374306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12079778532374306 | validation: 0.2665794638542432]
	TIME [epoch: 3.51 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20760408627168936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20760408627168936 | validation: 0.05735615378815136]
	TIME [epoch: 3.52 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04552967059088446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04552967059088446 | validation: 0.0681926534561598]
	TIME [epoch: 3.53 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031225331486617058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031225331486617058 | validation: 0.04761848308425784]
	TIME [epoch: 3.55 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046339652396429884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046339652396429884 | validation: 0.10190251132591387]
	TIME [epoch: 3.53 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05716368702415114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05716368702415114 | validation: 0.04232703203328817]
	TIME [epoch: 3.52 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03507268239116968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03507268239116968 | validation: 0.05500454714622262]
	TIME [epoch: 3.51 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025683830854955687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025683830854955687 | validation: 0.0635821443256233]
	TIME [epoch: 3.52 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03749701367903991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03749701367903991 | validation: 0.09034693572943231]
	TIME [epoch: 3.53 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08714444848510024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08714444848510024 | validation: 0.12985930472949647]
	TIME [epoch: 3.52 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09530643250029174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09530643250029174 | validation: 0.13563538584551216]
	TIME [epoch: 3.52 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08290307070204246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08290307070204246 | validation: 0.06442446750132791]
	TIME [epoch: 3.54 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04675797356717019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04675797356717019 | validation: 0.17251282375452007]
	TIME [epoch: 3.52 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08774520371394666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08774520371394666 | validation: 0.17566383258509674]
	TIME [epoch: 3.52 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13179058777848895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13179058777848895 | validation: 0.18386670719945825]
	TIME [epoch: 3.52 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10724307423072132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10724307423072132 | validation: 0.046571960567638286]
	TIME [epoch: 3.52 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03834308846303517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03834308846303517 | validation: 0.049539871383560564]
	TIME [epoch: 3.53 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04723707144235566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04723707144235566 | validation: 0.08886672889816818]
	TIME [epoch: 3.54 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052158302771689265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052158302771689265 | validation: 0.044563997039585336]
	TIME [epoch: 3.54 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03381122825874404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03381122825874404 | validation: 0.034933001053689454]
	TIME [epoch: 3.54 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01955481757594983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01955481757594983 | validation: 0.03200817205569208]
	TIME [epoch: 3.53 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_718.pth
	Model improved!!!
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019089665977067113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019089665977067113 | validation: 0.03129492349544286]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_719.pth
	Model improved!!!
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023957714467034617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023957714467034617 | validation: 0.1024441370019042]
	TIME [epoch: 3.54 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058109763938385635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058109763938385635 | validation: 0.1367347036186912]
	TIME [epoch: 3.52 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12274832202560151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12274832202560151 | validation: 0.2495968804078991]
	TIME [epoch: 3.52 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1725187173435568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1725187173435568 | validation: 0.051173819077321214]
	TIME [epoch: 3.52 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03579621602345591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03579621602345591 | validation: 0.05805937168034658]
	TIME [epoch: 3.54 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05522565733803716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05522565733803716 | validation: 0.12861176021947104]
	TIME [epoch: 3.52 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06482062407625218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06482062407625218 | validation: 0.04013624207918792]
	TIME [epoch: 3.51 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03520061479374974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03520061479374974 | validation: 0.04238780118129934]
	TIME [epoch: 3.53 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03780486624317391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03780486624317391 | validation: 0.06876027372750396]
	TIME [epoch: 3.53 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03911690710303359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03911690710303359 | validation: 0.05713552025011394]
	TIME [epoch: 3.54 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044011138526706815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044011138526706815 | validation: 0.1623141988385668]
	TIME [epoch: 3.51 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0981499974417072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0981499974417072 | validation: 0.12744193262884956]
	TIME [epoch: 3.5 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10447245138655038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10447245138655038 | validation: 0.12194431742339443]
	TIME [epoch: 3.51 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044662028894133794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044662028894133794 | validation: 0.050820986191986744]
	TIME [epoch: 3.49 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016151587886206815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016151587886206815 | validation: 0.05210083204861435]
	TIME [epoch: 3.49 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0217099108339801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0217099108339801 | validation: 0.07519210804161756]
	TIME [epoch: 3.51 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03045787893233002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03045787893233002 | validation: 0.051236094293851175]
	TIME [epoch: 3.52 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04197787835673023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04197787835673023 | validation: 0.10696897645276798]
	TIME [epoch: 3.51 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10066252819499673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10066252819499673 | validation: 0.278979246182438]
	TIME [epoch: 3.52 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24488885800815183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24488885800815183 | validation: 0.13329209154933186]
	TIME [epoch: 3.51 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0795038089961743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0795038089961743 | validation: 0.09558486840760823]
	TIME [epoch: 3.51 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0446248513564039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0446248513564039 | validation: 0.04097318135693833]
	TIME [epoch: 3.51 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030554462716528112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030554462716528112 | validation: 0.05342137142709578]
	TIME [epoch: 3.52 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03125822374638725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03125822374638725 | validation: 0.03896409172869456]
	TIME [epoch: 3.5 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030509211798220477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030509211798220477 | validation: 0.05935908533542488]
	TIME [epoch: 3.52 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03191877258604353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03191877258604353 | validation: 0.033981923248188495]
	TIME [epoch: 3.51 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027884574450269435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027884574450269435 | validation: 0.04828951259864607]
	TIME [epoch: 3.51 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027863579133807805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027863579133807805 | validation: 0.03116594442525801]
	TIME [epoch: 3.51 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_747.pth
	Model improved!!!
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035029879393983415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035029879393983415 | validation: 0.08964871473294243]
	TIME [epoch: 3.54 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05250262016660967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05250262016660967 | validation: 0.03351837230157506]
	TIME [epoch: 3.56 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04978371271377076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04978371271377076 | validation: 0.04443481423823508]
	TIME [epoch: 3.54 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03534158460166821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03534158460166821 | validation: 0.1178013096827667]
	TIME [epoch: 3.53 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07020509199924548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07020509199924548 | validation: 0.22114818329514563]
	TIME [epoch: 3.53 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19828610205640435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19828610205640435 | validation: 0.28212367969490354]
	TIME [epoch: 3.54 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18231000639616535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18231000639616535 | validation: 0.09867947080350253]
	TIME [epoch: 3.54 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047362417228367926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047362417228367926 | validation: 0.09316916162201766]
	TIME [epoch: 3.55 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07991692088725899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07991692088725899 | validation: 0.10179431767570812]
	TIME [epoch: 3.56 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04763790811207045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04763790811207045 | validation: 0.05593840052665097]
	TIME [epoch: 3.56 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03302998309612878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03302998309612878 | validation: 0.05776669448984524]
	TIME [epoch: 3.55 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03293075586649612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03293075586649612 | validation: 0.04947929729904147]
	TIME [epoch: 3.55 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03748975975643012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03748975975643012 | validation: 0.08509102461494056]
	TIME [epoch: 3.55 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0476165732923981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0476165732923981 | validation: 0.07001765632355694]
	TIME [epoch: 3.55 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04972385580019438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04972385580019438 | validation: 0.07838370154984334]
	TIME [epoch: 3.55 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04314927273150333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04314927273150333 | validation: 0.04038688950676871]
	TIME [epoch: 3.55 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030235250711623172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030235250711623172 | validation: 0.03990866909297]
	TIME [epoch: 3.55 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025529681890966032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025529681890966032 | validation: 0.05987965093537076]
	TIME [epoch: 3.53 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03353106769077253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03353106769077253 | validation: 0.08506925528865078]
	TIME [epoch: 3.55 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0735757899058784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0735757899058784 | validation: 0.18661354188877674]
	TIME [epoch: 3.53 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13634653730990315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13634653730990315 | validation: 0.09659084381680456]
	TIME [epoch: 3.55 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07678726681706066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07678726681706066 | validation: 0.06284241089500825]
	TIME [epoch: 3.55 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03773936654818897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03773936654818897 | validation: 0.0780947450078331]
	TIME [epoch: 3.55 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04613256699058863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04613256699058863 | validation: 0.05579991922668216]
	TIME [epoch: 3.56 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05020381164570376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05020381164570376 | validation: 0.09629995815914918]
	TIME [epoch: 3.55 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04181601847703011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04181601847703011 | validation: 0.04175499493509455]
	TIME [epoch: 3.54 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03461957713384933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03461957713384933 | validation: 0.064005486015746]
	TIME [epoch: 3.55 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03465427938987411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03465427938987411 | validation: 0.042681416366615516]
	TIME [epoch: 3.53 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04872404173262654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04872404173262654 | validation: 0.08431814115698373]
	TIME [epoch: 3.55 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05077186511086462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05077186511086462 | validation: 0.15498974605765656]
	TIME [epoch: 3.54 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09338696711357447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09338696711357447 | validation: 0.10059811762470983]
	TIME [epoch: 3.54 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12301455298643306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12301455298643306 | validation: 0.23561504003654843]
	TIME [epoch: 3.54 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12707533830593915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12707533830593915 | validation: 0.11638130870841286]
	TIME [epoch: 3.54 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05793676746851929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05793676746851929 | validation: 0.07418154094687648]
	TIME [epoch: 3.54 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0436405872537542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0436405872537542 | validation: 0.06055512297763976]
	TIME [epoch: 3.53 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03560181262934583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03560181262934583 | validation: 0.035187151013380914]
	TIME [epoch: 3.55 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03253345975392773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03253345975392773 | validation: 0.03115474221993101]
	TIME [epoch: 3.55 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_784.pth
	Model improved!!!
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017287172494390685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017287172494390685 | validation: 0.029685171037540693]
	TIME [epoch: 3.53 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_785.pth
	Model improved!!!
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01401843171761667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01401843171761667 | validation: 0.022643005679769082]
	TIME [epoch: 3.53 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_786.pth
	Model improved!!!
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01661302613749944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01661302613749944 | validation: 0.04985149314304918]
	TIME [epoch: 3.54 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026163776791394068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026163776791394068 | validation: 0.050883057865502505]
	TIME [epoch: 3.57 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06401349771367344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06401349771367344 | validation: 0.17645023424731401]
	TIME [epoch: 3.56 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12164215394922094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12164215394922094 | validation: 0.11017799792131511]
	TIME [epoch: 3.54 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10590010744080992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10590010744080992 | validation: 0.07989201933161512]
	TIME [epoch: 3.56 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05251167590637294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05251167590637294 | validation: 0.16753529428277159]
	TIME [epoch: 3.54 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07044027187115091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07044027187115091 | validation: 0.17649656640143538]
	TIME [epoch: 3.57 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12745525454625617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12745525454625617 | validation: 0.2321442091029466]
	TIME [epoch: 3.54 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15054238379659826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15054238379659826 | validation: 0.04218933239764316]
	TIME [epoch: 3.57 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02675902178267435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02675902178267435 | validation: 0.04716370583855603]
	TIME [epoch: 3.55 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027667256254035537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027667256254035537 | validation: 0.04538815629889517]
	TIME [epoch: 3.57 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03334210316949665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03334210316949665 | validation: 0.03698490667003914]
	TIME [epoch: 3.55 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02391922073437529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02391922073437529 | validation: 0.026925387158158922]
	TIME [epoch: 3.56 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019769539707547145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019769539707547145 | validation: 0.05037928922659651]
	TIME [epoch: 3.56 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028909358071053787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028909358071053787 | validation: 0.051332392545053875]
	TIME [epoch: 3.53 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04276540137270034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04276540137270034 | validation: 0.09335386895051724]
	TIME [epoch: 3.52 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06466465683769206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06466465683769206 | validation: 0.08196823170926357]
	TIME [epoch: 3.52 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07112303831147758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07112303831147758 | validation: 0.06645963461833618]
	TIME [epoch: 3.51 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041860448676089505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041860448676089505 | validation: 0.05022752505478021]
	TIME [epoch: 3.52 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03585330390495046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03585330390495046 | validation: 0.07830125346647548]
	TIME [epoch: 3.51 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05924587944376809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05924587944376809 | validation: 0.17594354544885735]
	TIME [epoch: 3.52 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12088106917748397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12088106917748397 | validation: 0.09649039888169128]
	TIME [epoch: 3.51 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07265434330067722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07265434330067722 | validation: 0.035933233102618235]
	TIME [epoch: 3.52 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022363810575862036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022363810575862036 | validation: 0.0366330374792745]
	TIME [epoch: 3.54 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01695763078176933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01695763078176933 | validation: 0.030911780861039396]
	TIME [epoch: 3.53 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025303764294503713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025303764294503713 | validation: 0.07714543507556935]
	TIME [epoch: 3.52 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043795343646226954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043795343646226954 | validation: 0.056201905252686195]
	TIME [epoch: 3.51 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07048625408975241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07048625408975241 | validation: 0.13808692110149431]
	TIME [epoch: 3.53 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06837558002032543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06837558002032543 | validation: 0.027922653606408034]
	TIME [epoch: 3.52 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0230852104022813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0230852104022813 | validation: 0.03573921881607169]
	TIME [epoch: 3.51 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03691308004299939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03691308004299939 | validation: 0.16688449521077545]
	TIME [epoch: 3.51 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1039326320789026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1039326320789026 | validation: 0.15848257525533016]
	TIME [epoch: 3.52 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15062361689042666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15062361689042666 | validation: 0.16940498928005499]
	TIME [epoch: 3.52 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13671787785759557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13671787785759557 | validation: 0.04306805772378966]
	TIME [epoch: 3.51 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03354368196908012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03354368196908012 | validation: 0.06964756082073523]
	TIME [epoch: 3.51 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03721642636721967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03721642636721967 | validation: 0.06032358231523365]
	TIME [epoch: 3.51 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033922893392842504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033922893392842504 | validation: 0.032987818085517165]
	TIME [epoch: 3.51 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028147821226416098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028147821226416098 | validation: 0.045951971101759484]
	TIME [epoch: 3.53 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030932095405981765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030932095405981765 | validation: 0.05124767601329629]
	TIME [epoch: 3.52 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038892966703150664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038892966703150664 | validation: 0.06390239198939432]
	TIME [epoch: 3.51 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044815884134510674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044815884134510674 | validation: 0.07312533041530798]
	TIME [epoch: 3.52 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04686092851285066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04686092851285066 | validation: 0.0682602520501342]
	TIME [epoch: 3.52 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044685990474880484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044685990474880484 | validation: 0.03809172683866453]
	TIME [epoch: 3.53 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024426891249754376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024426891249754376 | validation: 0.04416564403512685]
	TIME [epoch: 3.52 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02670464065217402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02670464065217402 | validation: 0.08950235480931698]
	TIME [epoch: 3.51 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0732534694535688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0732534694535688 | validation: 0.22061730498674775]
	TIME [epoch: 3.54 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16455864671960121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16455864671960121 | validation: 0.09816689905141479]
	TIME [epoch: 3.5 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08227586335889576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08227586335889576 | validation: 0.09241993278687936]
	TIME [epoch: 3.51 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0421661575603066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0421661575603066 | validation: 0.028375314474177217]
	TIME [epoch: 3.53 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022607908377516334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022607908377516334 | validation: 0.032954403248259545]
	TIME [epoch: 3.53 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025269315343318263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025269315343318263 | validation: 0.049808357282306426]
	TIME [epoch: 3.52 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025298224616125575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025298224616125575 | validation: 0.03738571472292547]
	TIME [epoch: 3.53 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04236336363700415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04236336363700415 | validation: 0.12046216725382544]
	TIME [epoch: 3.51 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08477342171075762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08477342171075762 | validation: 0.07396398296298691]
	TIME [epoch: 3.52 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054598230627772665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054598230627772665 | validation: 0.060657238121894136]
	TIME [epoch: 3.51 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034298431306096194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034298431306096194 | validation: 0.07565902010568319]
	TIME [epoch: 3.51 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03866793342531241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03866793342531241 | validation: 0.07651988695617673]
	TIME [epoch: 3.51 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05127210492187878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05127210492187878 | validation: 0.17591073032157262]
	TIME [epoch: 3.52 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08092058067054086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08092058067054086 | validation: 0.1012804996726433]
	TIME [epoch: 3.52 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1029347093758715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1029347093758715 | validation: 0.23212636040770054]
	TIME [epoch: 3.51 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16847911123008585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16847911123008585 | validation: 0.09031456700099323]
	TIME [epoch: 3.51 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0547878269304109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0547878269304109 | validation: 0.07893015028638878]
	TIME [epoch: 3.51 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0599405714539154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0599405714539154 | validation: 0.0628886741410186]
	TIME [epoch: 3.52 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03220986640455388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03220986640455388 | validation: 0.041319376323292446]
	TIME [epoch: 3.54 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02138117327428381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02138117327428381 | validation: 0.031802450591082276]
	TIME [epoch: 3.53 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018998069143632532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018998069143632532 | validation: 0.023283113743059736]
	TIME [epoch: 3.52 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019000281088514127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019000281088514127 | validation: 0.03832540360564976]
	TIME [epoch: 3.52 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01703813410238715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01703813410238715 | validation: 0.027103292775660495]
	TIME [epoch: 3.52 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019605587684766987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019605587684766987 | validation: 0.059068977321285944]
	TIME [epoch: 3.51 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03406638189609028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03406638189609028 | validation: 0.08350960073844663]
	TIME [epoch: 3.51 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07913787117257785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07913787117257785 | validation: 0.12778218594428084]
	TIME [epoch: 3.52 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09035495746811234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09035495746811234 | validation: 0.05340379046488152]
	TIME [epoch: 3.52 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04635402918096917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04635402918096917 | validation: 0.03908410442498459]
	TIME [epoch: 3.5 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02604227444472528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02604227444472528 | validation: 0.06102083924767798]
	TIME [epoch: 3.51 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06103768383698758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06103768383698758 | validation: 0.12147155473143789]
	TIME [epoch: 3.52 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1259840236372325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1259840236372325 | validation: 0.34740942351904736]
	TIME [epoch: 3.54 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19229275867495602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19229275867495602 | validation: 0.05896702607162638]
	TIME [epoch: 3.52 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032754341846018685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032754341846018685 | validation: 0.08111415716955524]
	TIME [epoch: 3.52 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07126535243644319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07126535243644319 | validation: 0.11812367943729281]
	TIME [epoch: 3.51 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057915713943785065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057915713943785065 | validation: 0.052779218319019355]
	TIME [epoch: 3.51 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03028507136163001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03028507136163001 | validation: 0.03751969576910125]
	TIME [epoch: 3.51 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03299358790987504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03299358790987504 | validation: 0.051632222829449764]
	TIME [epoch: 3.5 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027874658713001176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027874658713001176 | validation: 0.026717434412647113]
	TIME [epoch: 3.51 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022654883354199608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022654883354199608 | validation: 0.048332513333284005]
	TIME [epoch: 3.51 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03351789424846822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03351789424846822 | validation: 0.046427285591888016]
	TIME [epoch: 3.52 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042902148837644496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042902148837644496 | validation: 0.07761438118471992]
	TIME [epoch: 3.52 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05635229304141427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05635229304141427 | validation: 0.10840273746208284]
	TIME [epoch: 3.53 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09323062241699998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09323062241699998 | validation: 0.12127822713934425]
	TIME [epoch: 3.53 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08498099206890962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08498099206890962 | validation: 0.06810540130024292]
	TIME [epoch: 3.52 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04769648042359803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04769648042359803 | validation: 0.03339838124829162]
	TIME [epoch: 3.52 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025429042750365307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025429042750365307 | validation: 0.03840590410682959]
	TIME [epoch: 3.53 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027657284409187213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027657284409187213 | validation: 0.04487334682137544]
	TIME [epoch: 3.52 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034493926790798365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034493926790798365 | validation: 0.07361367994022065]
	TIME [epoch: 3.51 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05352853007022544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05352853007022544 | validation: 0.07759570660893247]
	TIME [epoch: 3.53 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05168988890985173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05168988890985173 | validation: 0.049282358024688756]
	TIME [epoch: 3.52 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026378098819199187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026378098819199187 | validation: 0.08241230266422876]
	TIME [epoch: 3.52 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025464253181148394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025464253181148394 | validation: 0.07090839219484027]
	TIME [epoch: 3.52 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031731368193991985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031731368193991985 | validation: 0.11221271299320745]
	TIME [epoch: 3.51 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05856240115661119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05856240115661119 | validation: 0.11879503736084135]
	TIME [epoch: 3.51 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10016297089628937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10016297089628937 | validation: 0.13122658717509092]
	TIME [epoch: 3.54 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08223840147602506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08223840147602506 | validation: 0.030290351335872136]
	TIME [epoch: 3.53 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171624/states/model_phi1_4a_v_mmd1_887.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 2269.935 seconds.
