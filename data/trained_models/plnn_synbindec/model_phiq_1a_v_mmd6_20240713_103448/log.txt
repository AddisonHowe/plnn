Args:
Namespace(name='model_phiq_1a_v_mmd6', outdir='out/model_training/model_phiq_1a_v_mmd6', training_data='data/training_data/data_phiq_1a/training', validation_data='data/training_data/data_phiq_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=250, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='warmup_cosine_decay', learning_rate=0.001, nepochs_warmup=20, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.01, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 4005622857

Training model...

Saving initial model state to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_0.pth
EPOCH 1/2000:
	Training over batches...
	Encountered nan in loss. Reverting update and performing model surgery (1/4).
		New model confinement_factor: 0.010000000000000002
		[batch 4/4] avg loss: 5.0027776018324		[learning rate: 0.0013375]
	Learning Rate: 0.0013375
	LOSS [training: 5.0027776018324 | validation: 5.078095982903632]
	TIME [epoch: 157 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.907699489077359		[learning rate: 0.0017875]
	Learning Rate: 0.0017875
	LOSS [training: 4.907699489077359 | validation: 5.071738097919808]
	TIME [epoch: 9.89 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.767549072001131		[learning rate: 0.0022375]
	Learning Rate: 0.0022375
	LOSS [training: 4.767549072001131 | validation: 4.88781489102726]
	TIME [epoch: 9.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.775051464558765		[learning rate: 0.0026875]
	Learning Rate: 0.0026875
	LOSS [training: 4.775051464558765 | validation: 5.106226006822467]
	TIME [epoch: 9.73 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.922908696822025		[learning rate: 0.0031375]
	Learning Rate: 0.0031375
	LOSS [training: 4.922908696822025 | validation: 4.838678079827645]
	TIME [epoch: 9.73 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.639003878058039		[learning rate: 0.0035875]
	Learning Rate: 0.0035875
	LOSS [training: 4.639003878058039 | validation: 4.880416907317345]
	TIME [epoch: 9.74 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.532319490041144		[learning rate: 0.0040375]
	Learning Rate: 0.0040375
	LOSS [training: 4.532319490041144 | validation: 4.508647017987369]
	TIME [epoch: 9.79 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.671248627292295		[learning rate: 0.0044875]
	Learning Rate: 0.0044875
	LOSS [training: 4.671248627292295 | validation: 4.715692225305374]
	TIME [epoch: 9.75 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.943379121167222		[learning rate: 0.0049375]
	Learning Rate: 0.0049375
	LOSS [training: 4.943379121167222 | validation: 4.949609456738042]
	TIME [epoch: 9.74 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.0074180391992895		[learning rate: 0.0053875]
	Learning Rate: 0.0053875
	LOSS [training: 5.0074180391992895 | validation: 4.939612512168296]
	TIME [epoch: 9.74 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.951211619444033		[learning rate: 0.0058375]
	Learning Rate: 0.0058375
	LOSS [training: 4.951211619444033 | validation: 5.270262366865714]
	TIME [epoch: 9.76 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.063305943760645		[learning rate: 0.0062875]
	Learning Rate: 0.0062875
	LOSS [training: 5.063305943760645 | validation: 4.796846660612817]
	TIME [epoch: 9.78 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.7965114794233035		[learning rate: 0.0067375]
	Learning Rate: 0.0067375
	LOSS [training: 4.7965114794233035 | validation: 4.6219649676221914]
	TIME [epoch: 9.76 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.590386972101581		[learning rate: 0.0071875]
	Learning Rate: 0.0071875
	LOSS [training: 4.590386972101581 | validation: 4.567335166939852]
	TIME [epoch: 9.75 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.662825722569544		[learning rate: 0.0076375]
	Learning Rate: 0.0076375
	LOSS [training: 4.662825722569544 | validation: 4.51595270025442]
	TIME [epoch: 9.75 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.4507454790277885		[learning rate: 0.0080875]
	Learning Rate: 0.0080875
	LOSS [training: 4.4507454790277885 | validation: 4.599806148958071]
	TIME [epoch: 9.79 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.418194437698755		[learning rate: 0.0085375]
	Learning Rate: 0.0085375
	LOSS [training: 4.418194437698755 | validation: 4.442734694885296]
	TIME [epoch: 9.78 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.327390574521621		[learning rate: 0.0089875]
	Learning Rate: 0.0089875
	LOSS [training: 4.327390574521621 | validation: 4.775619035446775]
	TIME [epoch: 9.73 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.4683688129966255		[learning rate: 0.0094375]
	Learning Rate: 0.0094375
	LOSS [training: 4.4683688129966255 | validation: 4.407577997801279]
	TIME [epoch: 9.73 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.107348021165909		[learning rate: 0.0098875]
	Learning Rate: 0.0098875
	LOSS [training: 4.107348021165909 | validation: 4.040627258980744]
	TIME [epoch: 9.72 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9861022189256703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9861022189256703 | validation: 4.14835392781486]
	TIME [epoch: 9.8 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0082221933171		[learning rate: 0.01]
	Learning Rate: 0.00999998
	LOSS [training: 4.0082221933171 | validation: 3.6294456256388097]
	TIME [epoch: 9.73 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.838164590120796		[learning rate: 0.01]
	Learning Rate: 0.00999995
	LOSS [training: 3.838164590120796 | validation: 3.5772247296735173]
	TIME [epoch: 9.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.710417220440332		[learning rate: 0.0099999]
	Learning Rate: 0.00999991
	LOSS [training: 3.710417220440332 | validation: 3.7949327705616254]
	TIME [epoch: 9.75 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5821390510769944		[learning rate: 0.0099999]
	Learning Rate: 0.00999986
	LOSS [training: 3.5821390510769944 | validation: 3.607642739836284]
	TIME [epoch: 9.74 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4539572307644746		[learning rate: 0.0099998]
	Learning Rate: 0.00999979
	LOSS [training: 3.4539572307644746 | validation: 3.5493663020497026]
	TIME [epoch: 9.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4292085932476404		[learning rate: 0.0099997]
	Learning Rate: 0.00999971
	LOSS [training: 3.4292085932476404 | validation: 3.41203768489287]
	TIME [epoch: 9.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.375129519409167		[learning rate: 0.0099996]
	Learning Rate: 0.00999961
	LOSS [training: 3.375129519409167 | validation: 3.213026235304618]
	TIME [epoch: 9.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1804244405547877		[learning rate: 0.0099995]
	Learning Rate: 0.00999951
	LOSS [training: 3.1804244405547877 | validation: 3.232717905711122]
	TIME [epoch: 9.74 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0847687987078567		[learning rate: 0.0099994]
	Learning Rate: 0.00999939
	LOSS [training: 3.0847687987078567 | validation: 3.1650121893743006]
	TIME [epoch: 9.78 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.846610351982853		[learning rate: 0.0099993]
	Learning Rate: 0.00999926
	LOSS [training: 2.846610351982853 | validation: 2.762711720839528]
	TIME [epoch: 9.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7075557581963525		[learning rate: 0.0099991]
	Learning Rate: 0.00999911
	LOSS [training: 2.7075557581963525 | validation: 2.6356426485748496]
	TIME [epoch: 9.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.591186535046267		[learning rate: 0.009999]
	Learning Rate: 0.00999896
	LOSS [training: 2.591186535046267 | validation: 2.564073501240573]
	TIME [epoch: 9.73 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5167728512520133		[learning rate: 0.0099988]
	Learning Rate: 0.00999879
	LOSS [training: 2.5167728512520133 | validation: 2.5166836253333145]
	TIME [epoch: 9.73 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4097612703570155		[learning rate: 0.0099986]
	Learning Rate: 0.0099986
	LOSS [training: 2.4097612703570155 | validation: 2.2947865594039216]
	TIME [epoch: 9.79 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.30283874796559		[learning rate: 0.0099984]
	Learning Rate: 0.00999841
	LOSS [training: 2.30283874796559 | validation: 2.221360030842116]
	TIME [epoch: 9.73 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1711658002138097		[learning rate: 0.0099982]
	Learning Rate: 0.0099982
	LOSS [training: 2.1711658002138097 | validation: 2.118236288163449]
	TIME [epoch: 9.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.086421052719032		[learning rate: 0.009998]
	Learning Rate: 0.00999798
	LOSS [training: 2.086421052719032 | validation: 2.0372644951442815]
	TIME [epoch: 9.72 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0011168351727173		[learning rate: 0.0099977]
	Learning Rate: 0.00999774
	LOSS [training: 2.0011168351727173 | validation: 1.9631756229498305]
	TIME [epoch: 9.78 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9181684945680615		[learning rate: 0.0099975]
	Learning Rate: 0.0099975
	LOSS [training: 1.9181684945680615 | validation: 1.8964651097404663]
	TIME [epoch: 9.78 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.838673404823991		[learning rate: 0.0099972]
	Learning Rate: 0.00999724
	LOSS [training: 1.838673404823991 | validation: 1.8319228616473295]
	TIME [epoch: 9.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7885293513982288		[learning rate: 0.009997]
	Learning Rate: 0.00999696
	LOSS [training: 1.7885293513982288 | validation: 1.7944420614568677]
	TIME [epoch: 9.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.746475704297765		[learning rate: 0.0099967]
	Learning Rate: 0.00999668
	LOSS [training: 1.746475704297765 | validation: 1.748276815147198]
	TIME [epoch: 9.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6963332386217598		[learning rate: 0.0099964]
	Learning Rate: 0.00999638
	LOSS [training: 1.6963332386217598 | validation: 1.7100470233822347]
	TIME [epoch: 9.79 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6402225570002267		[learning rate: 0.0099961]
	Learning Rate: 0.00999607
	LOSS [training: 1.6402225570002267 | validation: 1.6114608064688907]
	TIME [epoch: 9.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5718700783502535		[learning rate: 0.0099957]
	Learning Rate: 0.00999575
	LOSS [training: 1.5718700783502535 | validation: 1.5945231939945252]
	TIME [epoch: 9.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6001979259570718		[learning rate: 0.0099954]
	Learning Rate: 0.00999541
	LOSS [training: 1.6001979259570718 | validation: 1.57982508585911]
	TIME [epoch: 9.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.525448029208473		[learning rate: 0.0099951]
	Learning Rate: 0.00999506
	LOSS [training: 1.525448029208473 | validation: 1.4795125273155194]
	TIME [epoch: 9.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4772383362326527		[learning rate: 0.0099947]
	Learning Rate: 0.0099947
	LOSS [training: 1.4772383362326527 | validation: 1.4506525729861282]
	TIME [epoch: 9.78 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4966803846170942		[learning rate: 0.0099943]
	Learning Rate: 0.00999432
	LOSS [training: 1.4966803846170942 | validation: 1.5074228116641435]
	TIME [epoch: 9.76 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4807814122296192		[learning rate: 0.0099939]
	Learning Rate: 0.00999393
	LOSS [training: 1.4807814122296192 | validation: 1.4522089880526758]
	TIME [epoch: 9.76 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4267107000049393		[learning rate: 0.0099935]
	Learning Rate: 0.00999353
	LOSS [training: 1.4267107000049393 | validation: 1.4052646720292996]
	TIME [epoch: 9.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.408111398274311		[learning rate: 0.0099931]
	Learning Rate: 0.00999312
	LOSS [training: 1.408111398274311 | validation: 1.4056048719128504]
	TIME [epoch: 9.8 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3945034823284848		[learning rate: 0.0099927]
	Learning Rate: 0.00999269
	LOSS [training: 1.3945034823284848 | validation: 1.381565859103877]
	TIME [epoch: 9.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3878130710888261		[learning rate: 0.0099923]
	Learning Rate: 0.00999225
	LOSS [training: 1.3878130710888261 | validation: 1.4171710190648046]
	TIME [epoch: 9.75 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.387916379530235		[learning rate: 0.0099918]
	Learning Rate: 0.0099918
	LOSS [training: 1.387916379530235 | validation: 1.3615390432783046]
	TIME [epoch: 9.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3523315363993902		[learning rate: 0.0099913]
	Learning Rate: 0.00999134
	LOSS [training: 1.3523315363993902 | validation: 1.3643056772630713]
	TIME [epoch: 9.75 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3559917766023426		[learning rate: 0.0099909]
	Learning Rate: 0.00999086
	LOSS [training: 1.3559917766023426 | validation: 1.3407493555602845]
	TIME [epoch: 9.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.335235697276889		[learning rate: 0.0099904]
	Learning Rate: 0.00999037
	LOSS [training: 1.335235697276889 | validation: 1.3282161479828216]
	TIME [epoch: 9.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3177424540772882		[learning rate: 0.0099899]
	Learning Rate: 0.00998987
	LOSS [training: 1.3177424540772882 | validation: 1.3081630433520228]
	TIME [epoch: 9.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3211253909982048		[learning rate: 0.0099893]
	Learning Rate: 0.00998935
	LOSS [training: 1.3211253909982048 | validation: 1.31311446222167]
	TIME [epoch: 9.74 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3118366617232329		[learning rate: 0.0099888]
	Learning Rate: 0.00998882
	LOSS [training: 1.3118366617232329 | validation: 1.302476861986966]
	TIME [epoch: 9.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3000192333553833		[learning rate: 0.0099883]
	Learning Rate: 0.00998828
	LOSS [training: 1.3000192333553833 | validation: 1.2923891469973234]
	TIME [epoch: 9.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.288562497998993		[learning rate: 0.0099877]
	Learning Rate: 0.00998772
	LOSS [training: 1.288562497998993 | validation: 1.291035084307866]
	TIME [epoch: 9.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.295250693159815		[learning rate: 0.0099872]
	Learning Rate: 0.00998716
	LOSS [training: 1.295250693159815 | validation: 1.296368450349397]
	TIME [epoch: 9.74 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.284700579098863		[learning rate: 0.0099866]
	Learning Rate: 0.00998658
	LOSS [training: 1.284700579098863 | validation: 1.2878291347480848]
	TIME [epoch: 9.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2877237512298576		[learning rate: 0.009986]
	Learning Rate: 0.00998598
	LOSS [training: 1.2877237512298576 | validation: 1.307412994058115]
	TIME [epoch: 9.8 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2881195207995537		[learning rate: 0.0099854]
	Learning Rate: 0.00998538
	LOSS [training: 1.2881195207995537 | validation: 1.2659610985175043]
	TIME [epoch: 9.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.275138900168973		[learning rate: 0.0099848]
	Learning Rate: 0.00998476
	LOSS [training: 1.275138900168973 | validation: 1.2777421304249819]
	TIME [epoch: 9.75 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2619408331801085		[learning rate: 0.0099841]
	Learning Rate: 0.00998413
	LOSS [training: 1.2619408331801085 | validation: 1.2666081467415127]
	TIME [epoch: 9.75 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2539433493751986		[learning rate: 0.0099835]
	Learning Rate: 0.00998348
	LOSS [training: 1.2539433493751986 | validation: 1.2611269584156455]
	TIME [epoch: 9.76 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2670399771816954		[learning rate: 0.0099828]
	Learning Rate: 0.00998283
	LOSS [training: 1.2670399771816954 | validation: 1.2549282008366305]
	TIME [epoch: 9.79 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.254520795905529		[learning rate: 0.0099822]
	Learning Rate: 0.00998216
	LOSS [training: 1.254520795905529 | validation: 1.2486132157894674]
	TIME [epoch: 9.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2383365066097287		[learning rate: 0.0099815]
	Learning Rate: 0.00998147
	LOSS [training: 1.2383365066097287 | validation: 1.2331822967893882]
	TIME [epoch: 9.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2597882471566368		[learning rate: 0.0099808]
	Learning Rate: 0.00998078
	LOSS [training: 1.2597882471566368 | validation: 1.2248906588050477]
	TIME [epoch: 9.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2235675960646335		[learning rate: 0.0099801]
	Learning Rate: 0.00998007
	LOSS [training: 1.2235675960646335 | validation: 1.20170308974689]
	TIME [epoch: 9.78 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.294060769742079		[learning rate: 0.0099793]
	Learning Rate: 0.00997935
	LOSS [training: 1.294060769742079 | validation: 1.233984776805789]
	TIME [epoch: 9.75 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2497213969719823		[learning rate: 0.0099786]
	Learning Rate: 0.00997862
	LOSS [training: 1.2497213969719823 | validation: 1.2364069134515454]
	TIME [epoch: 9.74 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2300698813730366		[learning rate: 0.0099779]
	Learning Rate: 0.00997787
	LOSS [training: 1.2300698813730366 | validation: 1.2133641633651941]
	TIME [epoch: 9.74 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2133627348019635		[learning rate: 0.0099771]
	Learning Rate: 0.00997711
	LOSS [training: 1.2133627348019635 | validation: 1.183148705452167]
	TIME [epoch: 9.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1955602329896138		[learning rate: 0.0099763]
	Learning Rate: 0.00997634
	LOSS [training: 1.1955602329896138 | validation: 1.2343401512963528]
	TIME [epoch: 9.81 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2146599703011982		[learning rate: 0.0099756]
	Learning Rate: 0.00997555
	LOSS [training: 1.2146599703011982 | validation: 1.178220108950102]
	TIME [epoch: 9.73 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1657309409866214		[learning rate: 0.0099748]
	Learning Rate: 0.00997476
	LOSS [training: 1.1657309409866214 | validation: 1.1621860360189995]
	TIME [epoch: 9.73 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2989693287971624		[learning rate: 0.0099739]
	Learning Rate: 0.00997395
	LOSS [training: 1.2989693287971624 | validation: 1.2787061011240557]
	TIME [epoch: 9.75 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2060356015032807		[learning rate: 0.0099731]
	Learning Rate: 0.00997312
	LOSS [training: 1.2060356015032807 | validation: 1.1668538402191744]
	TIME [epoch: 9.76 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1636988430438162		[learning rate: 0.0099723]
	Learning Rate: 0.00997229
	LOSS [training: 1.1636988430438162 | validation: 1.1426233143026714]
	TIME [epoch: 9.78 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1722716084343956		[learning rate: 0.0099714]
	Learning Rate: 0.00997144
	LOSS [training: 1.1722716084343956 | validation: 1.142191047051329]
	TIME [epoch: 9.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1352037176761232		[learning rate: 0.0099706]
	Learning Rate: 0.00997058
	LOSS [training: 1.1352037176761232 | validation: 1.1078413985653408]
	TIME [epoch: 9.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1499711307441154		[learning rate: 0.0099697]
	Learning Rate: 0.0099697
	LOSS [training: 1.1499711307441154 | validation: 1.2970136057806463]
	TIME [epoch: 9.74 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2186646735754119		[learning rate: 0.0099688]
	Learning Rate: 0.00996882
	LOSS [training: 1.2186646735754119 | validation: 1.144511911180036]
	TIME [epoch: 9.78 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1541370730066378		[learning rate: 0.0099679]
	Learning Rate: 0.00996792
	LOSS [training: 1.1541370730066378 | validation: 1.1041580431117692]
	TIME [epoch: 9.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1510946113986427		[learning rate: 0.009967]
	Learning Rate: 0.009967
	LOSS [training: 1.1510946113986427 | validation: 1.0831532028567394]
	TIME [epoch: 9.73 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1578680486466761		[learning rate: 0.0099661]
	Learning Rate: 0.00996608
	LOSS [training: 1.1578680486466761 | validation: 1.1512338951652745]
	TIME [epoch: 9.73 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1171281210649517		[learning rate: 0.0099651]
	Learning Rate: 0.00996514
	LOSS [training: 1.1171281210649517 | validation: 1.1109878461195795]
	TIME [epoch: 9.73 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1950323180351965		[learning rate: 0.0099642]
	Learning Rate: 0.00996419
	LOSS [training: 1.1950323180351965 | validation: 1.2129979030078166]
	TIME [epoch: 9.78 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1424148195796002		[learning rate: 0.0099632]
	Learning Rate: 0.00996323
	LOSS [training: 1.1424148195796002 | validation: 1.1078377379730138]
	TIME [epoch: 9.73 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.109464207627596		[learning rate: 0.0099623]
	Learning Rate: 0.00996225
	LOSS [training: 1.109464207627596 | validation: 1.0501003644553026]
	TIME [epoch: 9.73 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1818316460509877		[learning rate: 0.0099613]
	Learning Rate: 0.00996126
	LOSS [training: 1.1818316460509877 | validation: 1.1797254522212932]
	TIME [epoch: 9.73 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1503988210484426		[learning rate: 0.0099603]
	Learning Rate: 0.00996026
	LOSS [training: 1.1503988210484426 | validation: 1.1166619575955612]
	TIME [epoch: 9.75 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0819975207372745		[learning rate: 0.0099592]
	Learning Rate: 0.00995925
	LOSS [training: 1.0819975207372745 | validation: 1.0143159630452674]
	TIME [epoch: 9.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.148578719037078		[learning rate: 0.0099582]
	Learning Rate: 0.00995822
	LOSS [training: 1.148578719037078 | validation: 1.1595014265833052]
	TIME [epoch: 9.73 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1082833493032647		[learning rate: 0.0099572]
	Learning Rate: 0.00995718
	LOSS [training: 1.1082833493032647 | validation: 1.028631826978513]
	TIME [epoch: 9.73 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0303849361457529		[learning rate: 0.0099561]
	Learning Rate: 0.00995613
	LOSS [training: 1.0303849361457529 | validation: 1.0697597713088496]
	TIME [epoch: 9.73 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2249337600894155		[learning rate: 0.0099551]
	Learning Rate: 0.00995506
	LOSS [training: 1.2249337600894155 | validation: 1.1875418925765076]
	TIME [epoch: 9.77 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1540345284126274		[learning rate: 0.009954]
	Learning Rate: 0.00995398
	LOSS [training: 1.1540345284126274 | validation: 1.137782265171539]
	TIME [epoch: 9.75 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1138289893721889		[learning rate: 0.0099529]
	Learning Rate: 0.00995289
	LOSS [training: 1.1138289893721889 | validation: 1.090256120361802]
	TIME [epoch: 9.74 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0726875976704238		[learning rate: 0.0099518]
	Learning Rate: 0.00995179
	LOSS [training: 1.0726875976704238 | validation: 1.0076913326599177]
	TIME [epoch: 9.73 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0738755042965387		[learning rate: 0.0099507]
	Learning Rate: 0.00995067
	LOSS [training: 1.0738755042965387 | validation: 1.1285301771551015]
	TIME [epoch: 9.74 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0721999480841482		[learning rate: 0.0099495]
	Learning Rate: 0.00994955
	LOSS [training: 1.0721999480841482 | validation: 0.9524999008227808]
	TIME [epoch: 9.78 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.047641471125211		[learning rate: 0.0099484]
	Learning Rate: 0.0099484
	LOSS [training: 1.047641471125211 | validation: 1.080725784484279]
	TIME [epoch: 9.76 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.183984402069361		[learning rate: 0.0099473]
	Learning Rate: 0.00994725
	LOSS [training: 1.183984402069361 | validation: 1.1042222919494291]
	TIME [epoch: 9.74 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0648489131118852		[learning rate: 0.0099461]
	Learning Rate: 0.00994608
	LOSS [training: 1.0648489131118852 | validation: 0.9986258392540426]
	TIME [epoch: 9.73 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0502125618226237		[learning rate: 0.0099449]
	Learning Rate: 0.0099449
	LOSS [training: 1.0502125618226237 | validation: 0.9886526066871566]
	TIME [epoch: 9.75 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0322671413791205		[learning rate: 0.0099437]
	Learning Rate: 0.00994371
	LOSS [training: 1.0322671413791205 | validation: 0.9911116296349944]
	TIME [epoch: 9.79 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9999905333043526		[learning rate: 0.0099425]
	Learning Rate: 0.00994251
	LOSS [training: 0.9999905333043526 | validation: 0.9638061861518632]
	TIME [epoch: 9.73 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1212166958976133		[learning rate: 0.0099413]
	Learning Rate: 0.00994129
	LOSS [training: 1.1212166958976133 | validation: 1.1462763051931386]
	TIME [epoch: 9.73 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0744371251692646		[learning rate: 0.0099401]
	Learning Rate: 0.00994006
	LOSS [training: 1.0744371251692646 | validation: 1.0746777964496603]
	TIME [epoch: 9.74 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0357998203181127		[learning rate: 0.0099388]
	Learning Rate: 0.00993882
	LOSS [training: 1.0357998203181127 | validation: 1.02109265742913]
	TIME [epoch: 9.75 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.045559774508241		[learning rate: 0.0099376]
	Learning Rate: 0.00993756
	LOSS [training: 1.045559774508241 | validation: 0.9394577638429968]
	TIME [epoch: 9.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1122506285931066		[learning rate: 0.0099363]
	Learning Rate: 0.00993629
	LOSS [training: 1.1122506285931066 | validation: 1.092180150466385]
	TIME [epoch: 9.75 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0168131927655164		[learning rate: 0.009935]
	Learning Rate: 0.00993501
	LOSS [training: 1.0168131927655164 | validation: 0.9222424531946071]
	TIME [epoch: 9.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9802586009104741		[learning rate: 0.0099337]
	Learning Rate: 0.00993372
	LOSS [training: 0.9802586009104741 | validation: 1.0242641581924072]
	TIME [epoch: 9.76 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0267153932888364		[learning rate: 0.0099324]
	Learning Rate: 0.00993241
	LOSS [training: 1.0267153932888364 | validation: 0.9382039730727554]
	TIME [epoch: 9.78 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9917793053313837		[learning rate: 0.0099311]
	Learning Rate: 0.00993109
	LOSS [training: 0.9917793053313837 | validation: 1.1219114594025856]
	TIME [epoch: 9.74 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0538381781933992		[learning rate: 0.0099298]
	Learning Rate: 0.00992976
	LOSS [training: 1.0538381781933992 | validation: 0.9468047797553343]
	TIME [epoch: 9.74 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9423119503769424		[learning rate: 0.0099284]
	Learning Rate: 0.00992842
	LOSS [training: 0.9423119503769424 | validation: 0.974825723483123]
	TIME [epoch: 9.74 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9698156623335924		[learning rate: 0.0099271]
	Learning Rate: 0.00992706
	LOSS [training: 0.9698156623335924 | validation: 0.9815685364630518]
	TIME [epoch: 9.73 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9487957630815874		[learning rate: 0.0099257]
	Learning Rate: 0.00992569
	LOSS [training: 0.9487957630815874 | validation: 0.944482491451214]
	TIME [epoch: 9.78 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9680067337474518		[learning rate: 0.0099243]
	Learning Rate: 0.00992431
	LOSS [training: 0.9680067337474518 | validation: 0.8875438079479145]
	TIME [epoch: 9.75 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_129.pth
	Model improved!!!
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0199399251985397		[learning rate: 0.0099229]
	Learning Rate: 0.00992291
	LOSS [training: 1.0199399251985397 | validation: 1.0954962835649804]
	TIME [epoch: 9.73 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0501748337613188		[learning rate: 0.0099215]
	Learning Rate: 0.00992151
	LOSS [training: 1.0501748337613188 | validation: 1.0152491496266862]
	TIME [epoch: 9.73 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9592369169455449		[learning rate: 0.0099201]
	Learning Rate: 0.00992008
	LOSS [training: 0.9592369169455449 | validation: 0.9880869519987082]
	TIME [epoch: 9.73 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9930141264110451		[learning rate: 0.0099187]
	Learning Rate: 0.00991865
	LOSS [training: 0.9930141264110451 | validation: 0.896494867148418]
	TIME [epoch: 9.78 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.907436120074116		[learning rate: 0.0099172]
	Learning Rate: 0.00991721
	LOSS [training: 0.907436120074116 | validation: 0.8946056614499234]
	TIME [epoch: 9.73 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.025136286316998		[learning rate: 0.0099157]
	Learning Rate: 0.00991575
	LOSS [training: 1.025136286316998 | validation: 0.9470772830091532]
	TIME [epoch: 9.73 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9135133272979132		[learning rate: 0.0099143]
	Learning Rate: 0.00991428
	LOSS [training: 0.9135133272979132 | validation: 1.185297831319045]
	TIME [epoch: 9.73 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0619558671812352		[learning rate: 0.0099128]
	Learning Rate: 0.0099128
	LOSS [training: 1.0619558671812352 | validation: 0.9672164859700181]
	TIME [epoch: 9.74 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9185292196446879		[learning rate: 0.0099113]
	Learning Rate: 0.0099113
	LOSS [training: 0.9185292196446879 | validation: 0.9065675453481893]
	TIME [epoch: 9.77 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9541941385339012		[learning rate: 0.0099098]
	Learning Rate: 0.00990979
	LOSS [training: 0.9541941385339012 | validation: 0.9015556843609355]
	TIME [epoch: 9.73 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8606974159035867		[learning rate: 0.0099083]
	Learning Rate: 0.00990827
	LOSS [training: 0.8606974159035867 | validation: 0.9856492962826413]
	TIME [epoch: 9.73 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9348114590570188		[learning rate: 0.0099067]
	Learning Rate: 0.00990674
	LOSS [training: 0.9348114590570188 | validation: 0.8384358881474523]
	TIME [epoch: 9.73 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_141.pth
	Model improved!!!
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8790878016916259		[learning rate: 0.0099052]
	Learning Rate: 0.00990519
	LOSS [training: 0.8790878016916259 | validation: 0.7932780488213955]
	TIME [epoch: 9.79 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8996575829032056		[learning rate: 0.0099036]
	Learning Rate: 0.00990363
	LOSS [training: 0.8996575829032056 | validation: 0.999063539818462]
	TIME [epoch: 9.77 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9652462113380385		[learning rate: 0.0099021]
	Learning Rate: 0.00990206
	LOSS [training: 0.9652462113380385 | validation: 0.8491017440106636]
	TIME [epoch: 9.75 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9521617079638642		[learning rate: 0.0099005]
	Learning Rate: 0.00990048
	LOSS [training: 0.9521617079638642 | validation: 1.007594224062686]
	TIME [epoch: 9.74 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9285166309459972		[learning rate: 0.0098989]
	Learning Rate: 0.00989888
	LOSS [training: 0.9285166309459972 | validation: 0.8031218126392983]
	TIME [epoch: 9.74 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9873063993067264		[learning rate: 0.0098973]
	Learning Rate: 0.00989727
	LOSS [training: 0.9873063993067264 | validation: 1.0013087281302508]
	TIME [epoch: 9.78 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9391159783451888		[learning rate: 0.0098956]
	Learning Rate: 0.00989565
	LOSS [training: 0.9391159783451888 | validation: 0.8293456724380688]
	TIME [epoch: 9.75 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.858673634095665		[learning rate: 0.009894]
	Learning Rate: 0.00989401
	LOSS [training: 0.858673634095665 | validation: 0.948365421844722]
	TIME [epoch: 9.74 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9216362462162659		[learning rate: 0.0098924]
	Learning Rate: 0.00989237
	LOSS [training: 0.9216362462162659 | validation: 0.8146597682267938]
	TIME [epoch: 9.74 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8741044208058316		[learning rate: 0.0098907]
	Learning Rate: 0.00989071
	LOSS [training: 0.8741044208058316 | validation: 0.8651895473199395]
	TIME [epoch: 9.74 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9139225954762474		[learning rate: 0.009889]
	Learning Rate: 0.00988904
	LOSS [training: 0.9139225954762474 | validation: 0.8823998939614157]
	TIME [epoch: 9.79 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8649877644195416		[learning rate: 0.0098874]
	Learning Rate: 0.00988735
	LOSS [training: 0.8649877644195416 | validation: 0.8903976615381618]
	TIME [epoch: 9.74 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9072003909315631		[learning rate: 0.0098857]
	Learning Rate: 0.00988565
	LOSS [training: 0.9072003909315631 | validation: 0.8854315919126801]
	TIME [epoch: 9.74 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8697091235940135		[learning rate: 0.0098839]
	Learning Rate: 0.00988395
	LOSS [training: 0.8697091235940135 | validation: 0.8052925595523351]
	TIME [epoch: 9.74 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9263232213251414		[learning rate: 0.0098822]
	Learning Rate: 0.00988222
	LOSS [training: 0.9263232213251414 | validation: 0.9611639455602458]
	TIME [epoch: 9.74 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8854320520323448		[learning rate: 0.0098805]
	Learning Rate: 0.00988049
	LOSS [training: 0.8854320520323448 | validation: 0.7771376678305337]
	TIME [epoch: 9.77 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.783262787171191		[learning rate: 0.0098787]
	Learning Rate: 0.00987874
	LOSS [training: 0.783262787171191 | validation: 0.7796675081909756]
	TIME [epoch: 9.76 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9172561449034377		[learning rate: 0.009877]
	Learning Rate: 0.00987698
	LOSS [training: 0.9172561449034377 | validation: 0.787184862808394]
	TIME [epoch: 9.75 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8175901567816306		[learning rate: 0.0098752]
	Learning Rate: 0.00987521
	LOSS [training: 0.8175901567816306 | validation: 0.8038658280152453]
	TIME [epoch: 9.75 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8912743023929035		[learning rate: 0.0098734]
	Learning Rate: 0.00987343
	LOSS [training: 0.8912743023929035 | validation: 0.9368057560589003]
	TIME [epoch: 9.77 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8110484867770174		[learning rate: 0.0098716]
	Learning Rate: 0.00987163
	LOSS [training: 0.8110484867770174 | validation: 0.8086021369145229]
	TIME [epoch: 9.79 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8729746610080513		[learning rate: 0.0098698]
	Learning Rate: 0.00986982
	LOSS [training: 0.8729746610080513 | validation: 0.826315575043895]
	TIME [epoch: 9.74 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7710097500063983		[learning rate: 0.009868]
	Learning Rate: 0.009868
	LOSS [training: 0.7710097500063983 | validation: 0.9246125663408828]
	TIME [epoch: 9.74 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8368067379384827		[learning rate: 0.0098662]
	Learning Rate: 0.00986616
	LOSS [training: 0.8368067379384827 | validation: 0.8419778972715927]
	TIME [epoch: 9.74 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9139910808448276		[learning rate: 0.0098643]
	Learning Rate: 0.00986431
	LOSS [training: 0.9139910808448276 | validation: 0.812924495905634]
	TIME [epoch: 9.79 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8289170712631948		[learning rate: 0.0098625]
	Learning Rate: 0.00986245
	LOSS [training: 0.8289170712631948 | validation: 0.9603500201199096]
	TIME [epoch: 9.75 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9001227868834408		[learning rate: 0.0098606]
	Learning Rate: 0.00986058
	LOSS [training: 0.9001227868834408 | validation: 0.8008314282355402]
	TIME [epoch: 9.74 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7799413225333465		[learning rate: 0.0098587]
	Learning Rate: 0.0098587
	LOSS [training: 0.7799413225333465 | validation: 0.8730238210424968]
	TIME [epoch: 9.73 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8648628590720697		[learning rate: 0.0098568]
	Learning Rate: 0.0098568
	LOSS [training: 0.8648628590720697 | validation: 0.9082547385396533]
	TIME [epoch: 9.74 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8560596194620163		[learning rate: 0.0098549]
	Learning Rate: 0.00985489
	LOSS [training: 0.8560596194620163 | validation: 0.7867822439199]
	TIME [epoch: 9.8 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7626545660360305		[learning rate: 0.009853]
	Learning Rate: 0.00985297
	LOSS [training: 0.7626545660360305 | validation: 1.736764802234832]
	TIME [epoch: 9.75 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0723811944303416		[learning rate: 0.009851]
	Learning Rate: 0.00985103
	LOSS [training: 1.0723811944303416 | validation: 0.9609570318909899]
	TIME [epoch: 9.73 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8856648785638493		[learning rate: 0.0098491]
	Learning Rate: 0.00984909
	LOSS [training: 0.8856648785638493 | validation: 0.8668317962592096]
	TIME [epoch: 9.74 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8032035848329593		[learning rate: 0.0098471]
	Learning Rate: 0.00984713
	LOSS [training: 0.8032035848329593 | validation: 0.7890280627763517]
	TIME [epoch: 9.73 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7946663840535703		[learning rate: 0.0098452]
	Learning Rate: 0.00984516
	LOSS [training: 0.7946663840535703 | validation: 0.8222749687238315]
	TIME [epoch: 9.79 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8133343616260269		[learning rate: 0.0098432]
	Learning Rate: 0.00984317
	LOSS [training: 0.8133343616260269 | validation: 0.9271196952612601]
	TIME [epoch: 9.72 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7994203560441675		[learning rate: 0.0098412]
	Learning Rate: 0.00984118
	LOSS [training: 0.7994203560441675 | validation: 0.9181993405736797]
	TIME [epoch: 9.72 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7838359135001755		[learning rate: 0.0098392]
	Learning Rate: 0.00983917
	LOSS [training: 0.7838359135001755 | validation: 0.8904337249773131]
	TIME [epoch: 9.72 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8726894553451552		[learning rate: 0.0098371]
	Learning Rate: 0.00983714
	LOSS [training: 0.8726894553451552 | validation: 0.7439367990052594]
	TIME [epoch: 9.74 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_180.pth
	Model improved!!!
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7317574000630778		[learning rate: 0.0098351]
	Learning Rate: 0.00983511
	LOSS [training: 0.7317574000630778 | validation: 0.7352814303891608]
	TIME [epoch: 9.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8014272586170776		[learning rate: 0.0098331]
	Learning Rate: 0.00983306
	LOSS [training: 0.8014272586170776 | validation: 0.8265917626759738]
	TIME [epoch: 9.73 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8019832587489569		[learning rate: 0.009831]
	Learning Rate: 0.00983101
	LOSS [training: 0.8019832587489569 | validation: 0.8057209418738509]
	TIME [epoch: 9.73 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9628505009239026		[learning rate: 0.0098289]
	Learning Rate: 0.00982894
	LOSS [training: 0.9628505009239026 | validation: 0.7508096997588523]
	TIME [epoch: 9.72 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8197756137636212		[learning rate: 0.0098269]
	Learning Rate: 0.00982685
	LOSS [training: 0.8197756137636212 | validation: 0.8219343936167935]
	TIME [epoch: 9.76 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.725042836066221		[learning rate: 0.0098248]
	Learning Rate: 0.00982476
	LOSS [training: 0.725042836066221 | validation: 0.8564366986865868]
	TIME [epoch: 9.78 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8158335598550719		[learning rate: 0.0098226]
	Learning Rate: 0.00982265
	LOSS [training: 0.8158335598550719 | validation: 0.6679131775768867]
	TIME [epoch: 9.72 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_187.pth
	Model improved!!!
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7673724414798182		[learning rate: 0.0098205]
	Learning Rate: 0.00982053
	LOSS [training: 0.7673724414798182 | validation: 0.7599608599847492]
	TIME [epoch: 9.74 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7138981362847007		[learning rate: 0.0098184]
	Learning Rate: 0.00981839
	LOSS [training: 0.7138981362847007 | validation: 0.9576611293896689]
	TIME [epoch: 9.75 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8585820627371605		[learning rate: 0.0098162]
	Learning Rate: 0.00981625
	LOSS [training: 0.8585820627371605 | validation: 0.7250778796846329]
	TIME [epoch: 9.79 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8193167342483739		[learning rate: 0.0098141]
	Learning Rate: 0.00981409
	LOSS [training: 0.8193167342483739 | validation: 0.8722103949174833]
	TIME [epoch: 9.76 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.801859422954652		[learning rate: 0.0098119]
	Learning Rate: 0.00981192
	LOSS [training: 0.801859422954652 | validation: 0.7403412621018537]
	TIME [epoch: 9.75 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7953529562510584		[learning rate: 0.0098097]
	Learning Rate: 0.00980974
	LOSS [training: 0.7953529562510584 | validation: 0.7729010264972982]
	TIME [epoch: 9.74 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7352785931628723		[learning rate: 0.0098075]
	Learning Rate: 0.00980754
	LOSS [training: 0.7352785931628723 | validation: 0.7995406916367801]
	TIME [epoch: 9.75 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7729596780592152		[learning rate: 0.0098053]
	Learning Rate: 0.00980534
	LOSS [training: 0.7729596780592152 | validation: 0.7040770762947928]
	TIME [epoch: 9.79 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7060661040187909		[learning rate: 0.0098031]
	Learning Rate: 0.00980312
	LOSS [training: 0.7060661040187909 | validation: 0.784589884064669]
	TIME [epoch: 9.74 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7679285495125392		[learning rate: 0.0098009]
	Learning Rate: 0.00980088
	LOSS [training: 0.7679285495125392 | validation: 0.6712589965551454]
	TIME [epoch: 9.75 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7882003078036114		[learning rate: 0.0097986]
	Learning Rate: 0.00979864
	LOSS [training: 0.7882003078036114 | validation: 0.8472680418373892]
	TIME [epoch: 9.74 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8416248621102171		[learning rate: 0.0097964]
	Learning Rate: 0.00979638
	LOSS [training: 0.8416248621102171 | validation: 0.7802362860161043]
	TIME [epoch: 9.73 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7407918890376217		[learning rate: 0.0097941]
	Learning Rate: 0.00979412
	LOSS [training: 0.7407918890376217 | validation: 0.7433679847836092]
	TIME [epoch: 9.81 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7654914550269116		[learning rate: 0.0097918]
	Learning Rate: 0.00979183
	LOSS [training: 0.7654914550269116 | validation: 0.8560787294039705]
	TIME [epoch: 114 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8083438584194684		[learning rate: 0.0097895]
	Learning Rate: 0.00978954
	LOSS [training: 0.8083438584194684 | validation: 0.7626516140706575]
	TIME [epoch: 19.4 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7060045040848797		[learning rate: 0.0097872]
	Learning Rate: 0.00978723
	LOSS [training: 0.7060045040848797 | validation: 1.0223890843734165]
	TIME [epoch: 19.3 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7941459672937221		[learning rate: 0.0097849]
	Learning Rate: 0.00978492
	LOSS [training: 0.7941459672937221 | validation: 1.032125078621199]
	TIME [epoch: 19.3 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7118691581514449		[learning rate: 0.0097826]
	Learning Rate: 0.00978259
	LOSS [training: 0.7118691581514449 | validation: 0.824721688137092]
	TIME [epoch: 19.3 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.804498399105731		[learning rate: 0.0097802]
	Learning Rate: 0.00978024
	LOSS [training: 0.804498399105731 | validation: 0.7731594106644244]
	TIME [epoch: 19.3 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.681673861201056		[learning rate: 0.0097779]
	Learning Rate: 0.00977789
	LOSS [training: 0.681673861201056 | validation: 0.6214631391915537]
	TIME [epoch: 19.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_207.pth
	Model improved!!!
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6866650770384495		[learning rate: 0.0097755]
	Learning Rate: 0.00977552
	LOSS [training: 0.6866650770384495 | validation: 0.6414256415640834]
	TIME [epoch: 19.3 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6426358570396614		[learning rate: 0.0097731]
	Learning Rate: 0.00977314
	LOSS [training: 0.6426358570396614 | validation: 0.6586591461680147]
	TIME [epoch: 19.3 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6718455256459925		[learning rate: 0.0097708]
	Learning Rate: 0.00977075
	LOSS [training: 0.6718455256459925 | validation: 1.0666509230712666]
	TIME [epoch: 19.3 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8659260033837202		[learning rate: 0.0097683]
	Learning Rate: 0.00976835
	LOSS [training: 0.8659260033837202 | validation: 0.8220788777149587]
	TIME [epoch: 19.3 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7055241381498272		[learning rate: 0.0097659]
	Learning Rate: 0.00976593
	LOSS [training: 0.7055241381498272 | validation: 0.7464479682028076]
	TIME [epoch: 19.3 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6689237262449416		[learning rate: 0.0097635]
	Learning Rate: 0.0097635
	LOSS [training: 0.6689237262449416 | validation: 0.6696615777573771]
	TIME [epoch: 19.3 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6700413473115445		[learning rate: 0.0097611]
	Learning Rate: 0.00976106
	LOSS [training: 0.6700413473115445 | validation: 0.815841965728518]
	TIME [epoch: 19.3 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6429207223732273		[learning rate: 0.0097586]
	Learning Rate: 0.00975861
	LOSS [training: 0.6429207223732273 | validation: 0.6444517322552463]
	TIME [epoch: 19.3 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6155988001103787		[learning rate: 0.0097561]
	Learning Rate: 0.00975615
	LOSS [training: 0.6155988001103787 | validation: 0.7145378518039105]
	TIME [epoch: 19.3 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.690714633693523		[learning rate: 0.0097537]
	Learning Rate: 0.00975367
	LOSS [training: 0.690714633693523 | validation: 0.7662227490067217]
	TIME [epoch: 19.3 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7128927738509124		[learning rate: 0.0097512]
	Learning Rate: 0.00975118
	LOSS [training: 0.7128927738509124 | validation: 0.7107366900299337]
	TIME [epoch: 19.3 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6359336597779816		[learning rate: 0.0097487]
	Learning Rate: 0.00974868
	LOSS [training: 0.6359336597779816 | validation: 0.8655395041847292]
	TIME [epoch: 19.3 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7249119535550836		[learning rate: 0.0097462]
	Learning Rate: 0.00974616
	LOSS [training: 0.7249119535550836 | validation: 0.7239243054528164]
	TIME [epoch: 19.3 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6157519811132645		[learning rate: 0.0097436]
	Learning Rate: 0.00974364
	LOSS [training: 0.6157519811132645 | validation: 0.5535220879926375]
	TIME [epoch: 19.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7171751200826104		[learning rate: 0.0097411]
	Learning Rate: 0.0097411
	LOSS [training: 0.7171751200826104 | validation: 0.8218811542358448]
	TIME [epoch: 19.3 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7591318223215892		[learning rate: 0.0097385]
	Learning Rate: 0.00973855
	LOSS [training: 0.7591318223215892 | validation: 0.915331152954229]
	TIME [epoch: 19.3 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.669597109211712		[learning rate: 0.009736]
	Learning Rate: 0.00973599
	LOSS [training: 0.669597109211712 | validation: 0.6116663392450947]
	TIME [epoch: 19.3 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.59859334365055		[learning rate: 0.0097334]
	Learning Rate: 0.00973341
	LOSS [training: 0.59859334365055 | validation: 0.6857798442141989]
	TIME [epoch: 19.3 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.663146659547192		[learning rate: 0.0097308]
	Learning Rate: 0.00973083
	LOSS [training: 0.663146659547192 | validation: 0.6229088693921623]
	TIME [epoch: 19.3 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.622737998303388		[learning rate: 0.0097282]
	Learning Rate: 0.00972823
	LOSS [training: 0.622737998303388 | validation: 0.8243328555900084]
	TIME [epoch: 19.3 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6475085396596101		[learning rate: 0.0097256]
	Learning Rate: 0.00972562
	LOSS [training: 0.6475085396596101 | validation: 0.6086588478926782]
	TIME [epoch: 19.3 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6106566725190383		[learning rate: 0.009723]
	Learning Rate: 0.00972299
	LOSS [training: 0.6106566725190383 | validation: 0.6907808963207468]
	TIME [epoch: 19.3 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.656456408567732		[learning rate: 0.0097204]
	Learning Rate: 0.00972036
	LOSS [training: 0.656456408567732 | validation: 0.7773236875030227]
	TIME [epoch: 19.3 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6326967624675987		[learning rate: 0.0097177]
	Learning Rate: 0.00971771
	LOSS [training: 0.6326967624675987 | validation: 0.5568629782257155]
	TIME [epoch: 19.3 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5185315071309907		[learning rate: 0.0097151]
	Learning Rate: 0.00971505
	LOSS [training: 0.5185315071309907 | validation: 0.5952645991158012]
	TIME [epoch: 19.3 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9639892842154998		[learning rate: 0.0097124]
	Learning Rate: 0.00971238
	LOSS [training: 0.9639892842154998 | validation: 1.1426174048271793]
	TIME [epoch: 19.3 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8476000338093184		[learning rate: 0.0097097]
	Learning Rate: 0.0097097
	LOSS [training: 0.8476000338093184 | validation: 0.7408328767252734]
	TIME [epoch: 19.3 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6410750212823167		[learning rate: 0.009707]
	Learning Rate: 0.009707
	LOSS [training: 0.6410750212823167 | validation: 0.570051400517622]
	TIME [epoch: 19.3 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6408615594867327		[learning rate: 0.0097043]
	Learning Rate: 0.00970429
	LOSS [training: 0.6408615594867327 | validation: 0.8495711973944392]
	TIME [epoch: 19.3 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6430294781825945		[learning rate: 0.0097016]
	Learning Rate: 0.00970157
	LOSS [training: 0.6430294781825945 | validation: 0.7299282258301814]
	TIME [epoch: 19.3 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6858869633155514		[learning rate: 0.0096988]
	Learning Rate: 0.00969884
	LOSS [training: 0.6858869633155514 | validation: 0.635233581210572]
	TIME [epoch: 19.3 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6550639192771777		[learning rate: 0.0096961]
	Learning Rate: 0.0096961
	LOSS [training: 0.6550639192771777 | validation: 0.699184851268033]
	TIME [epoch: 19.3 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5805564218786591		[learning rate: 0.0096933]
	Learning Rate: 0.00969334
	LOSS [training: 0.5805564218786591 | validation: 0.6325195784768799]
	TIME [epoch: 19.3 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5892163922826379		[learning rate: 0.0096906]
	Learning Rate: 0.00969057
	LOSS [training: 0.5892163922826379 | validation: 0.6436415478938333]
	TIME [epoch: 19.3 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6038020683755242		[learning rate: 0.0096878]
	Learning Rate: 0.00968779
	LOSS [training: 0.6038020683755242 | validation: 0.8584465108969863]
	TIME [epoch: 19.3 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5889906010479553		[learning rate: 0.009685]
	Learning Rate: 0.009685
	LOSS [training: 0.5889906010479553 | validation: 0.6382636752316873]
	TIME [epoch: 19.3 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6448435546065879		[learning rate: 0.0096822]
	Learning Rate: 0.0096822
	LOSS [training: 0.6448435546065879 | validation: 0.7918937557479052]
	TIME [epoch: 19.3 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7285996773421692		[learning rate: 0.0096794]
	Learning Rate: 0.00967938
	LOSS [training: 0.7285996773421692 | validation: 0.7064752935475425]
	TIME [epoch: 19.3 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6236049900060476		[learning rate: 0.0096766]
	Learning Rate: 0.00967655
	LOSS [training: 0.6236049900060476 | validation: 0.5505832383172546]
	TIME [epoch: 19.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6111588217013431		[learning rate: 0.0096737]
	Learning Rate: 0.00967371
	LOSS [training: 0.6111588217013431 | validation: 0.7820115143482178]
	TIME [epoch: 19.3 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6428752760896667		[learning rate: 0.0096709]
	Learning Rate: 0.00967086
	LOSS [training: 0.6428752760896667 | validation: 0.5549703436367219]
	TIME [epoch: 19.3 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5480425302026052		[learning rate: 0.009668]
	Learning Rate: 0.009668
	LOSS [training: 0.5480425302026052 | validation: 0.6151252824593554]
	TIME [epoch: 19.3 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6237848845796323		[learning rate: 0.0096651]
	Learning Rate: 0.00966512
	LOSS [training: 0.6237848845796323 | validation: 0.7622744833676206]
	TIME [epoch: 19.3 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6295680998433735		[learning rate: 0.0096622]
	Learning Rate: 0.00966223
	LOSS [training: 0.6295680998433735 | validation: 0.5635533086356218]
	TIME [epoch: 19.3 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5567844633514702		[learning rate: 0.0096593]
	Learning Rate: 0.00965933
	LOSS [training: 0.5567844633514702 | validation: 0.537165129658119]
	TIME [epoch: 19.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_252.pth
	Model improved!!!
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5397478078233969		[learning rate: 0.0096564]
	Learning Rate: 0.00965642
	LOSS [training: 0.5397478078233969 | validation: 0.7216878035964397]
	TIME [epoch: 19.3 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7112418714173022		[learning rate: 0.0096535]
	Learning Rate: 0.00965349
	LOSS [training: 0.7112418714173022 | validation: 0.6755731409738004]
	TIME [epoch: 19.3 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6359532330411197		[learning rate: 0.0096506]
	Learning Rate: 0.00965056
	LOSS [training: 0.6359532330411197 | validation: 0.6569214706432362]
	TIME [epoch: 19.3 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5539057405202304		[learning rate: 0.0096476]
	Learning Rate: 0.00964761
	LOSS [training: 0.5539057405202304 | validation: 0.5116508349103901]
	TIME [epoch: 19.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_256.pth
	Model improved!!!
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5739318272823801		[learning rate: 0.0096447]
	Learning Rate: 0.00964465
	LOSS [training: 0.5739318272823801 | validation: 0.8293453408450836]
	TIME [epoch: 19.3 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6053645162373723		[learning rate: 0.0096417]
	Learning Rate: 0.00964168
	LOSS [training: 0.6053645162373723 | validation: 0.6871372820803208]
	TIME [epoch: 19.3 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6091633385500381		[learning rate: 0.0096387]
	Learning Rate: 0.0096387
	LOSS [training: 0.6091633385500381 | validation: 0.6948180605602736]
	TIME [epoch: 19.3 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5744989617455903		[learning rate: 0.0096357]
	Learning Rate: 0.0096357
	LOSS [training: 0.5744989617455903 | validation: 0.7637134085684141]
	TIME [epoch: 19.3 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6456751628456029		[learning rate: 0.0096327]
	Learning Rate: 0.00963269
	LOSS [training: 0.6456751628456029 | validation: 0.6088936660870459]
	TIME [epoch: 19.3 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.633434383325528		[learning rate: 0.0096297]
	Learning Rate: 0.00962967
	LOSS [training: 0.633434383325528 | validation: 0.9743086110076409]
	TIME [epoch: 19.3 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6794105264035297		[learning rate: 0.0096266]
	Learning Rate: 0.00962664
	LOSS [training: 0.6794105264035297 | validation: 0.6993606010660078]
	TIME [epoch: 19.3 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5524329330380873		[learning rate: 0.0096236]
	Learning Rate: 0.0096236
	LOSS [training: 0.5524329330380873 | validation: 0.5117470045715258]
	TIME [epoch: 19.3 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5439558007309783		[learning rate: 0.0096205]
	Learning Rate: 0.00962054
	LOSS [training: 0.5439558007309783 | validation: 0.7021848733061109]
	TIME [epoch: 19.3 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5971591975194319		[learning rate: 0.0096175]
	Learning Rate: 0.00961748
	LOSS [training: 0.5971591975194319 | validation: 0.5773154318241388]
	TIME [epoch: 19.3 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.556898474489932		[learning rate: 0.0096144]
	Learning Rate: 0.0096144
	LOSS [training: 0.556898474489932 | validation: 0.5539165343047382]
	TIME [epoch: 19.3 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5342794661235323		[learning rate: 0.0096113]
	Learning Rate: 0.00961131
	LOSS [training: 0.5342794661235323 | validation: 0.5374537342413298]
	TIME [epoch: 19.3 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5440609494042867		[learning rate: 0.0096082]
	Learning Rate: 0.00960821
	LOSS [training: 0.5440609494042867 | validation: 0.5728617675107263]
	TIME [epoch: 19.3 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6058336327042972		[learning rate: 0.0096051]
	Learning Rate: 0.00960509
	LOSS [training: 0.6058336327042972 | validation: 0.5993070967676275]
	TIME [epoch: 19.3 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5676406500511901		[learning rate: 0.009602]
	Learning Rate: 0.00960197
	LOSS [training: 0.5676406500511901 | validation: 0.5324072462293403]
	TIME [epoch: 19.3 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6302600617472407		[learning rate: 0.0095988]
	Learning Rate: 0.00959883
	LOSS [training: 0.6302600617472407 | validation: 0.5868141160721982]
	TIME [epoch: 19.3 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5388479441554738		[learning rate: 0.0095957]
	Learning Rate: 0.00959568
	LOSS [training: 0.5388479441554738 | validation: 0.8066380531866548]
	TIME [epoch: 19.3 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5435487276791251		[learning rate: 0.0095925]
	Learning Rate: 0.00959252
	LOSS [training: 0.5435487276791251 | validation: 0.6451188384279558]
	TIME [epoch: 19.3 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6207787701428397		[learning rate: 0.0095893]
	Learning Rate: 0.00958934
	LOSS [training: 0.6207787701428397 | validation: 0.6596035114554486]
	TIME [epoch: 19.3 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5990470313418204		[learning rate: 0.0095862]
	Learning Rate: 0.00958616
	LOSS [training: 0.5990470313418204 | validation: 0.5409804607995012]
	TIME [epoch: 19.3 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5761996710979261		[learning rate: 0.009583]
	Learning Rate: 0.00958296
	LOSS [training: 0.5761996710979261 | validation: 0.5955167812943509]
	TIME [epoch: 19.3 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5489744050822516		[learning rate: 0.0095798]
	Learning Rate: 0.00957975
	LOSS [training: 0.5489744050822516 | validation: 0.5172741661440791]
	TIME [epoch: 19.3 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5054905333863803		[learning rate: 0.0095765]
	Learning Rate: 0.00957653
	LOSS [training: 0.5054905333863803 | validation: 0.9355636221339185]
	TIME [epoch: 19.3 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5581116061475468		[learning rate: 0.0095733]
	Learning Rate: 0.0095733
	LOSS [training: 0.5581116061475468 | validation: 0.5385105794502782]
	TIME [epoch: 19.3 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.528999759143111		[learning rate: 0.0095701]
	Learning Rate: 0.00957006
	LOSS [training: 0.528999759143111 | validation: 0.6128610864492596]
	TIME [epoch: 19.3 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.539653150258286		[learning rate: 0.0095668]
	Learning Rate: 0.0095668
	LOSS [training: 0.539653150258286 | validation: 0.671317070755221]
	TIME [epoch: 19.3 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5762067813691437		[learning rate: 0.0095635]
	Learning Rate: 0.00956353
	LOSS [training: 0.5762067813691437 | validation: 0.544479011546269]
	TIME [epoch: 19.3 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5777060931013441		[learning rate: 0.0095603]
	Learning Rate: 0.00956026
	LOSS [training: 0.5777060931013441 | validation: 0.5567912307457341]
	TIME [epoch: 19.3 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4946671750948981		[learning rate: 0.009557]
	Learning Rate: 0.00955696
	LOSS [training: 0.4946671750948981 | validation: 0.5219895573149211]
	TIME [epoch: 19.3 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6228351074710284		[learning rate: 0.0095537]
	Learning Rate: 0.00955366
	LOSS [training: 0.6228351074710284 | validation: 0.5572959476158589]
	TIME [epoch: 19.3 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5905936964387658		[learning rate: 0.0095503]
	Learning Rate: 0.00955035
	LOSS [training: 0.5905936964387658 | validation: 0.5788114446047833]
	TIME [epoch: 19.3 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5361577137839597		[learning rate: 0.009547]
	Learning Rate: 0.00954702
	LOSS [training: 0.5361577137839597 | validation: 0.6374830747608404]
	TIME [epoch: 19.3 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.573426381826984		[learning rate: 0.0095437]
	Learning Rate: 0.00954369
	LOSS [training: 0.573426381826984 | validation: 0.5985480136577798]
	TIME [epoch: 19.3 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5190585148562223		[learning rate: 0.0095403]
	Learning Rate: 0.00954034
	LOSS [training: 0.5190585148562223 | validation: 0.5975591306174111]
	TIME [epoch: 19.3 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5873802505867959		[learning rate: 0.009537]
	Learning Rate: 0.00953698
	LOSS [training: 0.5873802505867959 | validation: 0.8740169309419421]
	TIME [epoch: 19.3 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6027104229252586		[learning rate: 0.0095336]
	Learning Rate: 0.0095336
	LOSS [training: 0.6027104229252586 | validation: 0.6041478353598944]
	TIME [epoch: 19.3 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5317256065221019		[learning rate: 0.0095302]
	Learning Rate: 0.00953022
	LOSS [training: 0.5317256065221019 | validation: 0.5808995413484631]
	TIME [epoch: 19.3 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5629758950846585		[learning rate: 0.0095268]
	Learning Rate: 0.00952682
	LOSS [training: 0.5629758950846585 | validation: 0.5273734775088308]
	TIME [epoch: 19.3 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4858295335955254		[learning rate: 0.0095234]
	Learning Rate: 0.00952342
	LOSS [training: 0.4858295335955254 | validation: 0.7378147004238642]
	TIME [epoch: 19.3 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.619842133203297		[learning rate: 0.00952]
	Learning Rate: 0.00952
	LOSS [training: 0.619842133203297 | validation: 0.555721056840923]
	TIME [epoch: 19.3 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5377482199096959		[learning rate: 0.0095166]
	Learning Rate: 0.00951657
	LOSS [training: 0.5377482199096959 | validation: 0.5176842911282862]
	TIME [epoch: 19.3 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5809160613086359		[learning rate: 0.0095131]
	Learning Rate: 0.00951313
	LOSS [training: 0.5809160613086359 | validation: 0.638226535584782]
	TIME [epoch: 19.3 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.57924477979655		[learning rate: 0.0095097]
	Learning Rate: 0.00950967
	LOSS [training: 0.57924477979655 | validation: 0.5317630055003877]
	TIME [epoch: 19.3 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5642603384019795		[learning rate: 0.0095062]
	Learning Rate: 0.00950621
	LOSS [training: 0.5642603384019795 | validation: 0.5801316697498398]
	TIME [epoch: 19.3 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.542724990848782		[learning rate: 0.0095027]
	Learning Rate: 0.00950273
	LOSS [training: 0.542724990848782 | validation: 0.6230052708927254]
	TIME [epoch: 19.3 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5765918386138142		[learning rate: 0.0094992]
	Learning Rate: 0.00949924
	LOSS [training: 0.5765918386138142 | validation: 0.5854254195653714]
	TIME [epoch: 19.3 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5750274212792269		[learning rate: 0.0094957]
	Learning Rate: 0.00949574
	LOSS [training: 0.5750274212792269 | validation: 0.5448057854257441]
	TIME [epoch: 19.3 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5434662438225304		[learning rate: 0.0094922]
	Learning Rate: 0.00949223
	LOSS [training: 0.5434662438225304 | validation: 0.578520780126847]
	TIME [epoch: 19.3 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5615602209197906		[learning rate: 0.0094887]
	Learning Rate: 0.00948871
	LOSS [training: 0.5615602209197906 | validation: 0.5188173184350671]
	TIME [epoch: 19.3 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5625256880673821		[learning rate: 0.0094852]
	Learning Rate: 0.00948517
	LOSS [training: 0.5625256880673821 | validation: 0.6619762477770699]
	TIME [epoch: 19.3 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4985315309340702		[learning rate: 0.0094816]
	Learning Rate: 0.00948163
	LOSS [training: 0.4985315309340702 | validation: 0.5184808945665639]
	TIME [epoch: 19.3 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.537173231258685		[learning rate: 0.0094781]
	Learning Rate: 0.00947807
	LOSS [training: 0.537173231258685 | validation: 0.9191351733202133]
	TIME [epoch: 19.3 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6919725007722071		[learning rate: 0.0094745]
	Learning Rate: 0.0094745
	LOSS [training: 0.6919725007722071 | validation: 0.592609783390236]
	TIME [epoch: 19.3 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5046588083589818		[learning rate: 0.0094709]
	Learning Rate: 0.00947092
	LOSS [training: 0.5046588083589818 | validation: 0.5298783000027585]
	TIME [epoch: 19.3 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5063887593018085		[learning rate: 0.0094673]
	Learning Rate: 0.00946733
	LOSS [training: 0.5063887593018085 | validation: 0.5297177211951325]
	TIME [epoch: 19.3 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5267021538894696		[learning rate: 0.0094637]
	Learning Rate: 0.00946373
	LOSS [training: 0.5267021538894696 | validation: 0.5184778684932315]
	TIME [epoch: 19.3 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5228409535475739		[learning rate: 0.0094601]
	Learning Rate: 0.00946011
	LOSS [training: 0.5228409535475739 | validation: 0.519619557216746]
	TIME [epoch: 19.3 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5264459191802755		[learning rate: 0.0094565]
	Learning Rate: 0.00945649
	LOSS [training: 0.5264459191802755 | validation: 0.5666241286583826]
	TIME [epoch: 19.3 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.523498916358052		[learning rate: 0.0094528]
	Learning Rate: 0.00945285
	LOSS [training: 0.523498916358052 | validation: 0.5711718734390704]
	TIME [epoch: 19.3 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49943076658664454		[learning rate: 0.0094492]
	Learning Rate: 0.0094492
	LOSS [training: 0.49943076658664454 | validation: 0.49148945565853097]
	TIME [epoch: 19.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_316.pth
	Model improved!!!
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5025954456264704		[learning rate: 0.0094455]
	Learning Rate: 0.00944554
	LOSS [training: 0.5025954456264704 | validation: 0.4975902812236682]
	TIME [epoch: 19.3 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45873504952246713		[learning rate: 0.0094419]
	Learning Rate: 0.00944187
	LOSS [training: 0.45873504952246713 | validation: 0.5840666736707552]
	TIME [epoch: 19.3 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5880329828423454		[learning rate: 0.0094382]
	Learning Rate: 0.00943818
	LOSS [training: 0.5880329828423454 | validation: 0.5068705504025799]
	TIME [epoch: 19.3 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46149022645711246		[learning rate: 0.0094345]
	Learning Rate: 0.00943449
	LOSS [training: 0.46149022645711246 | validation: 0.5949097227289424]
	TIME [epoch: 19.3 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.570943980776751		[learning rate: 0.0094308]
	Learning Rate: 0.00943078
	LOSS [training: 0.570943980776751 | validation: 0.604268953072211]
	TIME [epoch: 19.3 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5802559307813793		[learning rate: 0.0094271]
	Learning Rate: 0.00942707
	LOSS [training: 0.5802559307813793 | validation: 0.6164463470164712]
	TIME [epoch: 19.3 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6341426858347632		[learning rate: 0.0094233]
	Learning Rate: 0.00942334
	LOSS [training: 0.6341426858347632 | validation: 0.631595455052562]
	TIME [epoch: 19.3 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5545547026711175		[learning rate: 0.0094196]
	Learning Rate: 0.0094196
	LOSS [training: 0.5545547026711175 | validation: 0.5117323844737719]
	TIME [epoch: 19.3 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48724226541234084		[learning rate: 0.0094158]
	Learning Rate: 0.00941585
	LOSS [training: 0.48724226541234084 | validation: 1.0204927854696688]
	TIME [epoch: 19.3 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5588727491270874		[learning rate: 0.0094121]
	Learning Rate: 0.00941208
	LOSS [training: 0.5588727491270874 | validation: 0.5832900268830026]
	TIME [epoch: 19.3 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5019618129405077		[learning rate: 0.0094083]
	Learning Rate: 0.00940831
	LOSS [training: 0.5019618129405077 | validation: 0.49149816608301744]
	TIME [epoch: 19.3 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5661394082165476		[learning rate: 0.0094045]
	Learning Rate: 0.00940452
	LOSS [training: 0.5661394082165476 | validation: 0.5969247194535182]
	TIME [epoch: 19.3 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5574799965956416		[learning rate: 0.0094007]
	Learning Rate: 0.00940073
	LOSS [training: 0.5574799965956416 | validation: 0.5472718800772783]
	TIME [epoch: 19.3 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5233217419309845		[learning rate: 0.0093969]
	Learning Rate: 0.00939692
	LOSS [training: 0.5233217419309845 | validation: 0.5492910678406291]
	TIME [epoch: 19.3 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5198047245997802		[learning rate: 0.0093931]
	Learning Rate: 0.0093931
	LOSS [training: 0.5198047245997802 | validation: 0.5787855791575472]
	TIME [epoch: 19.3 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5018108715523631		[learning rate: 0.0093893]
	Learning Rate: 0.00938927
	LOSS [training: 0.5018108715523631 | validation: 0.5292973671601015]
	TIME [epoch: 19.3 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4757655418018792		[learning rate: 0.0093854]
	Learning Rate: 0.00938543
	LOSS [training: 0.4757655418018792 | validation: 0.5723661833246707]
	TIME [epoch: 19.3 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5486874657711355		[learning rate: 0.0093816]
	Learning Rate: 0.00938157
	LOSS [training: 0.5486874657711355 | validation: 0.5992984479222838]
	TIME [epoch: 19.3 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4737112503615434		[learning rate: 0.0093777]
	Learning Rate: 0.00937771
	LOSS [training: 0.4737112503615434 | validation: 0.6256377660715287]
	TIME [epoch: 19.3 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4948969204601392		[learning rate: 0.0093738]
	Learning Rate: 0.00937383
	LOSS [training: 0.4948969204601392 | validation: 0.8211999967974218]
	TIME [epoch: 19.3 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4914349132436765		[learning rate: 0.0093699]
	Learning Rate: 0.00936995
	LOSS [training: 0.4914349132436765 | validation: 0.5250252261407031]
	TIME [epoch: 19.3 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5801108376043915		[learning rate: 0.009366]
	Learning Rate: 0.00936605
	LOSS [training: 0.5801108376043915 | validation: 0.5597030300694541]
	TIME [epoch: 19.3 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4991872442802658		[learning rate: 0.0093621]
	Learning Rate: 0.00936214
	LOSS [training: 0.4991872442802658 | validation: 0.6168748896384735]
	TIME [epoch: 19.3 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.582502130528111		[learning rate: 0.0093582]
	Learning Rate: 0.00935822
	LOSS [training: 0.582502130528111 | validation: 0.5910699219112023]
	TIME [epoch: 19.3 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4917419191177439		[learning rate: 0.0093543]
	Learning Rate: 0.00935429
	LOSS [training: 0.4917419191177439 | validation: 0.5624412663427059]
	TIME [epoch: 19.3 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5225946486824836		[learning rate: 0.0093503]
	Learning Rate: 0.00935034
	LOSS [training: 0.5225946486824836 | validation: 0.5374933527052801]
	TIME [epoch: 19.3 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5136080292157429		[learning rate: 0.0093464]
	Learning Rate: 0.00934639
	LOSS [training: 0.5136080292157429 | validation: 0.48111507553588373]
	TIME [epoch: 19.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_343.pth
	Model improved!!!
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45546911992980194		[learning rate: 0.0093424]
	Learning Rate: 0.00934243
	LOSS [training: 0.45546911992980194 | validation: 0.5085853783265134]
	TIME [epoch: 19.3 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5579948487509432		[learning rate: 0.0093384]
	Learning Rate: 0.00933845
	LOSS [training: 0.5579948487509432 | validation: 0.5982131897099541]
	TIME [epoch: 19.3 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5167206060932232		[learning rate: 0.0093345]
	Learning Rate: 0.00933446
	LOSS [training: 0.5167206060932232 | validation: 0.5098237496971691]
	TIME [epoch: 19.3 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5861086428040028		[learning rate: 0.0093305]
	Learning Rate: 0.00933046
	LOSS [training: 0.5861086428040028 | validation: 0.5309513921881663]
	TIME [epoch: 19.3 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5108225598736503		[learning rate: 0.0093265]
	Learning Rate: 0.00932645
	LOSS [training: 0.5108225598736503 | validation: 0.5451130507324468]
	TIME [epoch: 19.3 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5148976712748341		[learning rate: 0.0093224]
	Learning Rate: 0.00932243
	LOSS [training: 0.5148976712748341 | validation: 0.5142414286832042]
	TIME [epoch: 19.3 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5061454273894797		[learning rate: 0.0093184]
	Learning Rate: 0.0093184
	LOSS [training: 0.5061454273894797 | validation: 0.4460950647806048]
	TIME [epoch: 19.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_350.pth
	Model improved!!!
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4377450869610748		[learning rate: 0.0093144]
	Learning Rate: 0.00931436
	LOSS [training: 0.4377450869610748 | validation: 0.7761584222040109]
	TIME [epoch: 19.3 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6446241516811625		[learning rate: 0.0093103]
	Learning Rate: 0.0093103
	LOSS [training: 0.6446241516811625 | validation: 0.6172819980250076]
	TIME [epoch: 19.3 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5185327471032054		[learning rate: 0.0093062]
	Learning Rate: 0.00930624
	LOSS [training: 0.5185327471032054 | validation: 0.4814833537647158]
	TIME [epoch: 19.3 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.476255212150983		[learning rate: 0.0093022]
	Learning Rate: 0.00930216
	LOSS [training: 0.476255212150983 | validation: 0.7611186679683871]
	TIME [epoch: 19.3 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5561581915360668		[learning rate: 0.0092981]
	Learning Rate: 0.00929808
	LOSS [training: 0.5561581915360668 | validation: 0.5056122773131018]
	TIME [epoch: 19.3 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44971550069822214		[learning rate: 0.009294]
	Learning Rate: 0.00929398
	LOSS [training: 0.44971550069822214 | validation: 0.47080758074700363]
	TIME [epoch: 19.3 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5253088546668486		[learning rate: 0.0092899]
	Learning Rate: 0.00928987
	LOSS [training: 0.5253088546668486 | validation: 0.7209762384661641]
	TIME [epoch: 19.3 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5014913976300119		[learning rate: 0.0092857]
	Learning Rate: 0.00928575
	LOSS [training: 0.5014913976300119 | validation: 0.49455843437482855]
	TIME [epoch: 19.3 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5230782184713034		[learning rate: 0.0092816]
	Learning Rate: 0.00928162
	LOSS [training: 0.5230782184713034 | validation: 0.4955116017458294]
	TIME [epoch: 19.3 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5220521748155849		[learning rate: 0.0092775]
	Learning Rate: 0.00927748
	LOSS [training: 0.5220521748155849 | validation: 0.5379530730484565]
	TIME [epoch: 19.3 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4522643470600205		[learning rate: 0.0092733]
	Learning Rate: 0.00927332
	LOSS [training: 0.4522643470600205 | validation: 0.5649082597747391]
	TIME [epoch: 19.3 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4465280464988204		[learning rate: 0.0092692]
	Learning Rate: 0.00926916
	LOSS [training: 0.4465280464988204 | validation: 0.4732169811266829]
	TIME [epoch: 19.3 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5208781960523696		[learning rate: 0.009265]
	Learning Rate: 0.00926498
	LOSS [training: 0.5208781960523696 | validation: 0.48556593884846816]
	TIME [epoch: 19.3 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45812632237135265		[learning rate: 0.0092608]
	Learning Rate: 0.0092608
	LOSS [training: 0.45812632237135265 | validation: 0.4829228455982286]
	TIME [epoch: 19.3 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4940016703384191		[learning rate: 0.0092566]
	Learning Rate: 0.0092566
	LOSS [training: 0.4940016703384191 | validation: 0.4999733301505262]
	TIME [epoch: 19.3 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47910347217541194		[learning rate: 0.0092524]
	Learning Rate: 0.00925239
	LOSS [training: 0.47910347217541194 | validation: 0.44341390009345216]
	TIME [epoch: 19.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_366.pth
	Model improved!!!
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43652321107588454		[learning rate: 0.0092482]
	Learning Rate: 0.00924817
	LOSS [training: 0.43652321107588454 | validation: 0.7262454204352962]
	TIME [epoch: 19.3 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6039776688823454		[learning rate: 0.0092439]
	Learning Rate: 0.00924394
	LOSS [training: 0.6039776688823454 | validation: 0.508116935958797]
	TIME [epoch: 19.3 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45880156749000844		[learning rate: 0.0092397]
	Learning Rate: 0.0092397
	LOSS [training: 0.45880156749000844 | validation: 0.5340556826576803]
	TIME [epoch: 19.3 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46370588316518174		[learning rate: 0.0092355]
	Learning Rate: 0.00923545
	LOSS [training: 0.46370588316518174 | validation: 0.5124656647593484]
	TIME [epoch: 19.3 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4754227810004919		[learning rate: 0.0092312]
	Learning Rate: 0.00923119
	LOSS [training: 0.4754227810004919 | validation: 0.4981887976790764]
	TIME [epoch: 19.3 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4239080467576875		[learning rate: 0.0092269]
	Learning Rate: 0.00922692
	LOSS [training: 0.4239080467576875 | validation: 0.42462744933356716]
	TIME [epoch: 19.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_372.pth
	Model improved!!!
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5083227476120762		[learning rate: 0.0092226]
	Learning Rate: 0.00922263
	LOSS [training: 0.5083227476120762 | validation: 0.4578397263185763]
	TIME [epoch: 19.3 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46717960590526875		[learning rate: 0.0092183]
	Learning Rate: 0.00921834
	LOSS [training: 0.46717960590526875 | validation: 0.4643909482964259]
	TIME [epoch: 19.3 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.454064245591531		[learning rate: 0.009214]
	Learning Rate: 0.00921403
	LOSS [training: 0.454064245591531 | validation: 0.5975654408232349]
	TIME [epoch: 19.3 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5667774985954803		[learning rate: 0.0092097]
	Learning Rate: 0.00920972
	LOSS [training: 0.5667774985954803 | validation: 0.5194534266613045]
	TIME [epoch: 19.3 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4677735137517209		[learning rate: 0.0092054]
	Learning Rate: 0.00920539
	LOSS [training: 0.4677735137517209 | validation: 0.5287523861673368]
	TIME [epoch: 19.3 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4960025408804356		[learning rate: 0.009201]
	Learning Rate: 0.00920105
	LOSS [training: 0.4960025408804356 | validation: 0.46440003827234744]
	TIME [epoch: 19.3 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49966989456900535		[learning rate: 0.0091967]
	Learning Rate: 0.0091967
	LOSS [training: 0.49966989456900535 | validation: 0.7020855305696252]
	TIME [epoch: 19.3 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5325947269424441		[learning rate: 0.0091923]
	Learning Rate: 0.00919234
	LOSS [training: 0.5325947269424441 | validation: 0.49591654766571336]
	TIME [epoch: 19.3 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4527467189432589		[learning rate: 0.009188]
	Learning Rate: 0.00918797
	LOSS [training: 0.4527467189432589 | validation: 0.42857212255564536]
	TIME [epoch: 19.3 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4616003177560088		[learning rate: 0.0091836]
	Learning Rate: 0.00918359
	LOSS [training: 0.4616003177560088 | validation: 0.5124225880502352]
	TIME [epoch: 19.3 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43914159877132697		[learning rate: 0.0091792]
	Learning Rate: 0.0091792
	LOSS [training: 0.43914159877132697 | validation: 0.5543118036991466]
	TIME [epoch: 19.3 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4801935953387237		[learning rate: 0.0091748]
	Learning Rate: 0.0091748
	LOSS [training: 0.4801935953387237 | validation: 0.4354220876583429]
	TIME [epoch: 19.3 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44801759571648153		[learning rate: 0.0091704]
	Learning Rate: 0.00917038
	LOSS [training: 0.44801759571648153 | validation: 0.5480861281924347]
	TIME [epoch: 19.3 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.474288351978122		[learning rate: 0.009166]
	Learning Rate: 0.00916596
	LOSS [training: 0.474288351978122 | validation: 0.5461057553074989]
	TIME [epoch: 19.3 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44295050522221885		[learning rate: 0.0091615]
	Learning Rate: 0.00916152
	LOSS [training: 0.44295050522221885 | validation: 0.5389569356754862]
	TIME [epoch: 19.3 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5940436591096342		[learning rate: 0.0091571]
	Learning Rate: 0.00915708
	LOSS [training: 0.5940436591096342 | validation: 0.5207894010388789]
	TIME [epoch: 19.3 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46108293220768826		[learning rate: 0.0091526]
	Learning Rate: 0.00915262
	LOSS [training: 0.46108293220768826 | validation: 0.47320879215846523]
	TIME [epoch: 19.3 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5895502432202514		[learning rate: 0.0091482]
	Learning Rate: 0.00914816
	LOSS [training: 0.5895502432202514 | validation: 0.51315286704456]
	TIME [epoch: 19.3 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.455749825839008		[learning rate: 0.0091437]
	Learning Rate: 0.00914368
	LOSS [training: 0.455749825839008 | validation: 0.47816277592803463]
	TIME [epoch: 19.3 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4874904201668806		[learning rate: 0.0091392]
	Learning Rate: 0.00913919
	LOSS [training: 0.4874904201668806 | validation: 0.6793597605038515]
	TIME [epoch: 19.3 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47498275575083587		[learning rate: 0.0091347]
	Learning Rate: 0.00913469
	LOSS [training: 0.47498275575083587 | validation: 0.5282679094750304]
	TIME [epoch: 19.3 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4419766190381001		[learning rate: 0.0091302]
	Learning Rate: 0.00913018
	LOSS [training: 0.4419766190381001 | validation: 0.5642121541766727]
	TIME [epoch: 19.3 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4693075710946645		[learning rate: 0.0091257]
	Learning Rate: 0.00912566
	LOSS [training: 0.4693075710946645 | validation: 0.5161163937442875]
	TIME [epoch: 19.3 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4895269934122942		[learning rate: 0.0091211]
	Learning Rate: 0.00912113
	LOSS [training: 0.4895269934122942 | validation: 0.5363336378305381]
	TIME [epoch: 19.3 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4882679896776684		[learning rate: 0.0091166]
	Learning Rate: 0.00911659
	LOSS [training: 0.4882679896776684 | validation: 0.48086482341504533]
	TIME [epoch: 19.3 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4376468879069054		[learning rate: 0.009112]
	Learning Rate: 0.00911204
	LOSS [training: 0.4376468879069054 | validation: 0.56004316984681]
	TIME [epoch: 19.3 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5268198008424818		[learning rate: 0.0091075]
	Learning Rate: 0.00910748
	LOSS [training: 0.5268198008424818 | validation: 0.5699045150290228]
	TIME [epoch: 19.3 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4866844239848811		[learning rate: 0.0091029]
	Learning Rate: 0.00910291
	LOSS [training: 0.4866844239848811 | validation: 0.49319968580435325]
	TIME [epoch: 19.3 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.471128106612473		[learning rate: 0.0090983]
	Learning Rate: 0.00909832
	LOSS [training: 0.471128106612473 | validation: 0.4440540125540963]
	TIME [epoch: 19.3 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4853732657060408		[learning rate: 0.0090937]
	Learning Rate: 0.00909373
	LOSS [training: 0.4853732657060408 | validation: 0.48856402480951755]
	TIME [epoch: 19.3 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46282933972566176		[learning rate: 0.0090891]
	Learning Rate: 0.00908912
	LOSS [training: 0.46282933972566176 | validation: 0.4755557284681855]
	TIME [epoch: 19.3 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43720880601660483		[learning rate: 0.0090845]
	Learning Rate: 0.00908451
	LOSS [training: 0.43720880601660483 | validation: 0.4982137760494736]
	TIME [epoch: 19.3 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42407165631465676		[learning rate: 0.0090799]
	Learning Rate: 0.00907988
	LOSS [training: 0.42407165631465676 | validation: 0.4425358818198447]
	TIME [epoch: 19.3 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45250001899875736		[learning rate: 0.0090752]
	Learning Rate: 0.00907525
	LOSS [training: 0.45250001899875736 | validation: 0.6672806927167687]
	TIME [epoch: 19.3 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5707870225311508		[learning rate: 0.0090706]
	Learning Rate: 0.0090706
	LOSS [training: 0.5707870225311508 | validation: 0.5204898666777579]
	TIME [epoch: 19.3 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41636869638989726		[learning rate: 0.0090659]
	Learning Rate: 0.00906595
	LOSS [training: 0.41636869638989726 | validation: 0.4100243977278382]
	TIME [epoch: 19.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_408.pth
	Model improved!!!
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4575212524185311		[learning rate: 0.0090613]
	Learning Rate: 0.00906128
	LOSS [training: 0.4575212524185311 | validation: 0.6186925245286958]
	TIME [epoch: 19.3 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5322567613035074		[learning rate: 0.0090566]
	Learning Rate: 0.0090566
	LOSS [training: 0.5322567613035074 | validation: 0.4913926177558655]
	TIME [epoch: 19.3 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4493161137005235		[learning rate: 0.0090519]
	Learning Rate: 0.00905191
	LOSS [training: 0.4493161137005235 | validation: 0.5187826194416786]
	TIME [epoch: 19.3 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5093341295039564		[learning rate: 0.0090472]
	Learning Rate: 0.00904722
	LOSS [training: 0.5093341295039564 | validation: 0.47207757541345324]
	TIME [epoch: 19.3 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41245973090865706		[learning rate: 0.0090425]
	Learning Rate: 0.00904251
	LOSS [training: 0.41245973090865706 | validation: 0.4212952237497106]
	TIME [epoch: 19.3 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4646808406820252		[learning rate: 0.0090378]
	Learning Rate: 0.00903779
	LOSS [training: 0.4646808406820252 | validation: 0.52298564124186]
	TIME [epoch: 19.3 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47326867602314837		[learning rate: 0.0090331]
	Learning Rate: 0.00903306
	LOSS [training: 0.47326867602314837 | validation: 0.42240152200914827]
	TIME [epoch: 19.3 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4228687880832808		[learning rate: 0.0090283]
	Learning Rate: 0.00902832
	LOSS [training: 0.4228687880832808 | validation: 0.4592483513083926]
	TIME [epoch: 19.3 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46098755094116095		[learning rate: 0.0090236]
	Learning Rate: 0.00902357
	LOSS [training: 0.46098755094116095 | validation: 0.6079074759543692]
	TIME [epoch: 19.3 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4687456099157422		[learning rate: 0.0090188]
	Learning Rate: 0.00901881
	LOSS [training: 0.4687456099157422 | validation: 0.4438412434241633]
	TIME [epoch: 19.3 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3948963907257068		[learning rate: 0.009014]
	Learning Rate: 0.00901404
	LOSS [training: 0.3948963907257068 | validation: 0.4463615312386473]
	TIME [epoch: 19.3 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5192757837645543		[learning rate: 0.0090093]
	Learning Rate: 0.00900926
	LOSS [training: 0.5192757837645543 | validation: 0.42722750494854506]
	TIME [epoch: 19.3 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3919597376818748		[learning rate: 0.0090045]
	Learning Rate: 0.00900447
	LOSS [training: 0.3919597376818748 | validation: 0.4773842313790524]
	TIME [epoch: 19.3 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46299793723784877		[learning rate: 0.0089997]
	Learning Rate: 0.00899967
	LOSS [training: 0.46299793723784877 | validation: 0.4811373186679039]
	TIME [epoch: 19.3 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43858534932887594		[learning rate: 0.0089949]
	Learning Rate: 0.00899485
	LOSS [training: 0.43858534932887594 | validation: 0.5967240187835222]
	TIME [epoch: 19.3 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45296313397892407		[learning rate: 0.00899]
	Learning Rate: 0.00899003
	LOSS [training: 0.45296313397892407 | validation: 0.45708834593113673]
	TIME [epoch: 19.3 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43188620138744427		[learning rate: 0.0089852]
	Learning Rate: 0.0089852
	LOSS [training: 0.43188620138744427 | validation: 0.5304696176439174]
	TIME [epoch: 19.3 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4381240390378722		[learning rate: 0.0089804]
	Learning Rate: 0.00898036
	LOSS [training: 0.4381240390378722 | validation: 0.6358033156550607]
	TIME [epoch: 19.3 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4513145750328146		[learning rate: 0.0089755]
	Learning Rate: 0.0089755
	LOSS [training: 0.4513145750328146 | validation: 0.5754137378862618]
	TIME [epoch: 19.3 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5159270379083052		[learning rate: 0.0089706]
	Learning Rate: 0.00897064
	LOSS [training: 0.5159270379083052 | validation: 0.5174518072314344]
	TIME [epoch: 19.3 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45427892624249316		[learning rate: 0.0089658]
	Learning Rate: 0.00896577
	LOSS [training: 0.45427892624249316 | validation: 0.5604978784747903]
	TIME [epoch: 19.3 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42805522885049896		[learning rate: 0.0089609]
	Learning Rate: 0.00896088
	LOSS [training: 0.42805522885049896 | validation: 0.4580841123556495]
	TIME [epoch: 19.3 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48529831032594195		[learning rate: 0.008956]
	Learning Rate: 0.00895599
	LOSS [training: 0.48529831032594195 | validation: 0.41937203578989396]
	TIME [epoch: 19.3 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44998609216752605		[learning rate: 0.0089511]
	Learning Rate: 0.00895109
	LOSS [training: 0.44998609216752605 | validation: 0.5103304945496075]
	TIME [epoch: 19.3 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4002173903530319		[learning rate: 0.0089462]
	Learning Rate: 0.00894617
	LOSS [training: 0.4002173903530319 | validation: 0.5752057150558503]
	TIME [epoch: 19.3 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43478358184105814		[learning rate: 0.0089413]
	Learning Rate: 0.00894125
	LOSS [training: 0.43478358184105814 | validation: 0.5251050352249205]
	TIME [epoch: 19.3 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43848824266135694		[learning rate: 0.0089363]
	Learning Rate: 0.00893632
	LOSS [training: 0.43848824266135694 | validation: 0.4871747123874993]
	TIME [epoch: 19.3 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4321475512146646		[learning rate: 0.0089314]
	Learning Rate: 0.00893137
	LOSS [training: 0.4321475512146646 | validation: 0.47742987648426205]
	TIME [epoch: 19.3 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4471963488971453		[learning rate: 0.0089264]
	Learning Rate: 0.00892642
	LOSS [training: 0.4471963488971453 | validation: 0.5443082086722373]
	TIME [epoch: 19.3 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4555970973502522		[learning rate: 0.0089215]
	Learning Rate: 0.00892145
	LOSS [training: 0.4555970973502522 | validation: 0.4145147003769081]
	TIME [epoch: 19.3 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41575571633428865		[learning rate: 0.0089165]
	Learning Rate: 0.00891648
	LOSS [training: 0.41575571633428865 | validation: 0.41619874710705385]
	TIME [epoch: 19.3 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40224215868996294		[learning rate: 0.0089115]
	Learning Rate: 0.0089115
	LOSS [training: 0.40224215868996294 | validation: 0.4550210526984289]
	TIME [epoch: 19.3 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46556661491363904		[learning rate: 0.0089065]
	Learning Rate: 0.0089065
	LOSS [training: 0.46556661491363904 | validation: 0.46185437988279465]
	TIME [epoch: 19.3 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.400349062199881		[learning rate: 0.0089015]
	Learning Rate: 0.0089015
	LOSS [training: 0.400349062199881 | validation: 0.4040533867874525]
	TIME [epoch: 19.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_442.pth
	Model improved!!!
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4568813598642217		[learning rate: 0.0088965]
	Learning Rate: 0.00889648
	LOSS [training: 0.4568813598642217 | validation: 0.6350102965259705]
	TIME [epoch: 19.3 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4650500385244133		[learning rate: 0.0088915]
	Learning Rate: 0.00889146
	LOSS [training: 0.4650500385244133 | validation: 0.4418117506319442]
	TIME [epoch: 19.4 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41870950442813604		[learning rate: 0.0088864]
	Learning Rate: 0.00888642
	LOSS [training: 0.41870950442813604 | validation: 0.41761051605419264]
	TIME [epoch: 19.3 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47571155771457246		[learning rate: 0.0088814]
	Learning Rate: 0.00888138
	LOSS [training: 0.47571155771457246 | validation: 0.4622096285584093]
	TIME [epoch: 19.3 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3782728710217478		[learning rate: 0.0088763]
	Learning Rate: 0.00887633
	LOSS [training: 0.3782728710217478 | validation: 0.3742018631862964]
	TIME [epoch: 19.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_447.pth
	Model improved!!!
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4705015746266169		[learning rate: 0.0088713]
	Learning Rate: 0.00887126
	LOSS [training: 0.4705015746266169 | validation: 0.6831618433137527]
	TIME [epoch: 19.3 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5143885666051067		[learning rate: 0.0088662]
	Learning Rate: 0.00886619
	LOSS [training: 0.5143885666051067 | validation: 0.5002966458630698]
	TIME [epoch: 19.3 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4447772078569885		[learning rate: 0.0088611]
	Learning Rate: 0.0088611
	LOSS [training: 0.4447772078569885 | validation: 0.5774894889349209]
	TIME [epoch: 19.3 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4274464279438056		[learning rate: 0.008856]
	Learning Rate: 0.00885601
	LOSS [training: 0.4274464279438056 | validation: 0.46592938827187874]
	TIME [epoch: 19.3 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42794644151286537		[learning rate: 0.0088509]
	Learning Rate: 0.00885091
	LOSS [training: 0.42794644151286537 | validation: 0.4432072340367197]
	TIME [epoch: 19.3 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44730600449786106		[learning rate: 0.0088458]
	Learning Rate: 0.00884579
	LOSS [training: 0.44730600449786106 | validation: 0.5076766915744322]
	TIME [epoch: 19.3 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4758437976396999		[learning rate: 0.0088407]
	Learning Rate: 0.00884067
	LOSS [training: 0.4758437976396999 | validation: 0.49592470243781606]
	TIME [epoch: 19.3 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41131149393307703		[learning rate: 0.0088355]
	Learning Rate: 0.00883553
	LOSS [training: 0.41131149393307703 | validation: 0.4059900638348352]
	TIME [epoch: 19.3 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45421869082456473		[learning rate: 0.0088304]
	Learning Rate: 0.00883039
	LOSS [training: 0.45421869082456473 | validation: 0.4419814426170999]
	TIME [epoch: 19.3 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4348667435871636		[learning rate: 0.0088252]
	Learning Rate: 0.00882524
	LOSS [training: 0.4348667435871636 | validation: 0.47152635366279594]
	TIME [epoch: 19.3 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48150204170954175		[learning rate: 0.0088201]
	Learning Rate: 0.00882007
	LOSS [training: 0.48150204170954175 | validation: 0.5247257543192496]
	TIME [epoch: 19.3 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41560760054839574		[learning rate: 0.0088149]
	Learning Rate: 0.0088149
	LOSS [training: 0.41560760054839574 | validation: 0.44364769208189464]
	TIME [epoch: 19.3 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4669631407057152		[learning rate: 0.0088097]
	Learning Rate: 0.00880972
	LOSS [training: 0.4669631407057152 | validation: 0.45490947278587157]
	TIME [epoch: 19.3 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4408050894237544		[learning rate: 0.0088045]
	Learning Rate: 0.00880453
	LOSS [training: 0.4408050894237544 | validation: 0.4370909601854813]
	TIME [epoch: 19.3 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4496816604310773		[learning rate: 0.0087993]
	Learning Rate: 0.00879933
	LOSS [training: 0.4496816604310773 | validation: 0.44288332692646404]
	TIME [epoch: 19.3 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3826000501249128		[learning rate: 0.0087941]
	Learning Rate: 0.00879411
	LOSS [training: 0.3826000501249128 | validation: 0.6460504861771275]
	TIME [epoch: 19.3 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.518599841732082		[learning rate: 0.0087889]
	Learning Rate: 0.00878889
	LOSS [training: 0.518599841732082 | validation: 0.4674313157673492]
	TIME [epoch: 19.4 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4369559089032361		[learning rate: 0.0087837]
	Learning Rate: 0.00878366
	LOSS [training: 0.4369559089032361 | validation: 0.42649979818433104]
	TIME [epoch: 19.3 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41010980105094685		[learning rate: 0.0087784]
	Learning Rate: 0.00877842
	LOSS [training: 0.41010980105094685 | validation: 0.6423388290449115]
	TIME [epoch: 19.3 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4663929440206502		[learning rate: 0.0087732]
	Learning Rate: 0.00877317
	LOSS [training: 0.4663929440206502 | validation: 0.4504105624703945]
	TIME [epoch: 19.3 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43383863352962077		[learning rate: 0.0087679]
	Learning Rate: 0.00876791
	LOSS [training: 0.43383863352962077 | validation: 0.4173342153763021]
	TIME [epoch: 19.3 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4433638798495962		[learning rate: 0.0087626]
	Learning Rate: 0.00876264
	LOSS [training: 0.4433638798495962 | validation: 0.4476482753767569]
	TIME [epoch: 19.4 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42826540259268975		[learning rate: 0.0087574]
	Learning Rate: 0.00875736
	LOSS [training: 0.42826540259268975 | validation: 0.46177593924335436]
	TIME [epoch: 19.3 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40234561819103154		[learning rate: 0.0087521]
	Learning Rate: 0.00875207
	LOSS [training: 0.40234561819103154 | validation: 0.5644337423067256]
	TIME [epoch: 19.3 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4879132954028178		[learning rate: 0.0087468]
	Learning Rate: 0.00874677
	LOSS [training: 0.4879132954028178 | validation: 0.5877504879183029]
	TIME [epoch: 19.3 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43397577241562096		[learning rate: 0.0087415]
	Learning Rate: 0.00874146
	LOSS [training: 0.43397577241562096 | validation: 0.42334316350330714]
	TIME [epoch: 19.3 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4107130780149296		[learning rate: 0.0087361]
	Learning Rate: 0.00873614
	LOSS [training: 0.4107130780149296 | validation: 0.5667080285423156]
	TIME [epoch: 19.4 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3981461724451095		[learning rate: 0.0087308]
	Learning Rate: 0.00873082
	LOSS [training: 0.3981461724451095 | validation: 0.5943705375393269]
	TIME [epoch: 19.3 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47607521411883075		[learning rate: 0.0087255]
	Learning Rate: 0.00872548
	LOSS [training: 0.47607521411883075 | validation: 0.4467962160892758]
	TIME [epoch: 19.3 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4027278106392812		[learning rate: 0.0087201]
	Learning Rate: 0.00872013
	LOSS [training: 0.4027278106392812 | validation: 0.4413159180228139]
	TIME [epoch: 19.3 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37845598620851356		[learning rate: 0.0087148]
	Learning Rate: 0.00871478
	LOSS [training: 0.37845598620851356 | validation: 0.43695971609547224]
	TIME [epoch: 19.3 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4111558059305361		[learning rate: 0.0087094]
	Learning Rate: 0.00870941
	LOSS [training: 0.4111558059305361 | validation: 0.43548390215914834]
	TIME [epoch: 19.3 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.442684100999017		[learning rate: 0.008704]
	Learning Rate: 0.00870403
	LOSS [training: 0.442684100999017 | validation: 0.48494525387412096]
	TIME [epoch: 19.3 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43557435126910965		[learning rate: 0.0086986]
	Learning Rate: 0.00869865
	LOSS [training: 0.43557435126910965 | validation: 0.4355292823983278]
	TIME [epoch: 19.3 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4197275724037584		[learning rate: 0.0086933]
	Learning Rate: 0.00869325
	LOSS [training: 0.4197275724037584 | validation: 0.4187807032472928]
	TIME [epoch: 19.3 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4070478725628395		[learning rate: 0.0086879]
	Learning Rate: 0.00868785
	LOSS [training: 0.4070478725628395 | validation: 0.4540610751130857]
	TIME [epoch: 19.3 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4241785869986807		[learning rate: 0.0086824]
	Learning Rate: 0.00868244
	LOSS [training: 0.4241785869986807 | validation: 0.4695944425804067]
	TIME [epoch: 19.3 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4235174300011285		[learning rate: 0.008677]
	Learning Rate: 0.00867701
	LOSS [training: 0.4235174300011285 | validation: 0.6660731607783794]
	TIME [epoch: 19.3 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4956507247605862		[learning rate: 0.0086716]
	Learning Rate: 0.00867158
	LOSS [training: 0.4956507247605862 | validation: 0.5934608909627278]
	TIME [epoch: 19.3 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4428395846816398		[learning rate: 0.0086661]
	Learning Rate: 0.00866614
	LOSS [training: 0.4428395846816398 | validation: 0.40554270883645915]
	TIME [epoch: 19.3 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40522549789164464		[learning rate: 0.0086607]
	Learning Rate: 0.00866069
	LOSS [training: 0.40522549789164464 | validation: 0.4114740719395328]
	TIME [epoch: 19.3 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41668788560960174		[learning rate: 0.0086552]
	Learning Rate: 0.00865523
	LOSS [training: 0.41668788560960174 | validation: 0.4626644996442758]
	TIME [epoch: 19.4 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4238989158441106		[learning rate: 0.0086498]
	Learning Rate: 0.00864976
	LOSS [training: 0.4238989158441106 | validation: 0.4050694877305066]
	TIME [epoch: 19.3 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37747436281414787		[learning rate: 0.0086443]
	Learning Rate: 0.00864428
	LOSS [training: 0.37747436281414787 | validation: 0.3786117397223695]
	TIME [epoch: 19.3 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4089160270943294		[learning rate: 0.0086388]
	Learning Rate: 0.00863879
	LOSS [training: 0.4089160270943294 | validation: 0.447840775878111]
	TIME [epoch: 19.3 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46308659903999017		[learning rate: 0.0086333]
	Learning Rate: 0.00863329
	LOSS [training: 0.46308659903999017 | validation: 0.48788851606628625]
	TIME [epoch: 19.3 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40911967347560807		[learning rate: 0.0086278]
	Learning Rate: 0.00862779
	LOSS [training: 0.40911967347560807 | validation: 0.38636512900640524]
	TIME [epoch: 19.4 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4590187286393238		[learning rate: 0.0086223]
	Learning Rate: 0.00862227
	LOSS [training: 0.4590187286393238 | validation: 0.5172170604192599]
	TIME [epoch: 19.3 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3814778085174897		[learning rate: 0.0086167]
	Learning Rate: 0.00861674
	LOSS [training: 0.3814778085174897 | validation: 0.4997977362946908]
	TIME [epoch: 19.3 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4011277737172589		[learning rate: 0.0086112]
	Learning Rate: 0.00861121
	LOSS [training: 0.4011277737172589 | validation: 0.48603063756905984]
	TIME [epoch: 19.3 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42974592719228		[learning rate: 0.0086057]
	Learning Rate: 0.00860566
	LOSS [training: 0.42974592719228 | validation: 0.5017378360706105]
	TIME [epoch: 19.3 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4223144270688699		[learning rate: 0.0086001]
	Learning Rate: 0.00860011
	LOSS [training: 0.4223144270688699 | validation: 0.4283993177347876]
	TIME [epoch: 19.4 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41231862254558005		[learning rate: 0.0085945]
	Learning Rate: 0.00859455
	LOSS [training: 0.41231862254558005 | validation: 0.4187724799738908]
	TIME [epoch: 19.3 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4321709130172874		[learning rate: 0.008589]
	Learning Rate: 0.00858898
	LOSS [training: 0.4321709130172874 | validation: 0.45099846122744014]
	TIME [epoch: 138 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4242943029757451		[learning rate: 0.0085834]
	Learning Rate: 0.00858339
	LOSS [training: 0.4242943029757451 | validation: 0.5058595487301859]
	TIME [epoch: 41.6 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4512897466225233		[learning rate: 0.0085778]
	Learning Rate: 0.0085778
	LOSS [training: 0.4512897466225233 | validation: 0.4027874675207831]
	TIME [epoch: 41.5 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3934408021684771		[learning rate: 0.0085722]
	Learning Rate: 0.0085722
	LOSS [training: 0.3934408021684771 | validation: 0.549681832935454]
	TIME [epoch: 41.5 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46406530113114663		[learning rate: 0.0085666]
	Learning Rate: 0.00856659
	LOSS [training: 0.46406530113114663 | validation: 0.462819195740681]
	TIME [epoch: 41.5 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40656772777397676		[learning rate: 0.008561]
	Learning Rate: 0.00856098
	LOSS [training: 0.40656772777397676 | validation: 0.463393571844966]
	TIME [epoch: 41.5 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4359176637586138		[learning rate: 0.0085553]
	Learning Rate: 0.00855535
	LOSS [training: 0.4359176637586138 | validation: 0.47652314769812465]
	TIME [epoch: 41.5 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42374066655752607		[learning rate: 0.0085497]
	Learning Rate: 0.00854971
	LOSS [training: 0.42374066655752607 | validation: 0.4252818164772219]
	TIME [epoch: 41.5 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40677333724912207		[learning rate: 0.0085441]
	Learning Rate: 0.00854407
	LOSS [training: 0.40677333724912207 | validation: 0.4309739281555737]
	TIME [epoch: 41.5 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4299326111287143		[learning rate: 0.0085384]
	Learning Rate: 0.00853841
	LOSS [training: 0.4299326111287143 | validation: 0.45048503057123535]
	TIME [epoch: 41.5 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38672446945347927		[learning rate: 0.0085327]
	Learning Rate: 0.00853275
	LOSS [training: 0.38672446945347927 | validation: 0.46211520653071436]
	TIME [epoch: 41.5 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38559353922566		[learning rate: 0.0085271]
	Learning Rate: 0.00852708
	LOSS [training: 0.38559353922566 | validation: 0.5414357128656815]
	TIME [epoch: 41.5 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4254362895367151		[learning rate: 0.0085214]
	Learning Rate: 0.0085214
	LOSS [training: 0.4254362895367151 | validation: 0.5137488724183232]
	TIME [epoch: 41.5 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47804130648398424		[learning rate: 0.0085157]
	Learning Rate: 0.00851571
	LOSS [training: 0.47804130648398424 | validation: 0.4603948663252725]
	TIME [epoch: 41.5 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39857209615204814		[learning rate: 0.00851]
	Learning Rate: 0.00851001
	LOSS [training: 0.39857209615204814 | validation: 0.3993553372142255]
	TIME [epoch: 41.5 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3819867583091088		[learning rate: 0.0085043]
	Learning Rate: 0.0085043
	LOSS [training: 0.3819867583091088 | validation: 0.4400123646179024]
	TIME [epoch: 41.5 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3924096140395936		[learning rate: 0.0084986]
	Learning Rate: 0.00849858
	LOSS [training: 0.3924096140395936 | validation: 0.42288432953259003]
	TIME [epoch: 41.5 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42725189508034694		[learning rate: 0.0084929]
	Learning Rate: 0.00849285
	LOSS [training: 0.42725189508034694 | validation: 0.45276089229608657]
	TIME [epoch: 41.5 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41441849461892327		[learning rate: 0.0084871]
	Learning Rate: 0.00848712
	LOSS [training: 0.41441849461892327 | validation: 0.44210344533044393]
	TIME [epoch: 41.5 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40783875075658593		[learning rate: 0.0084814]
	Learning Rate: 0.00848137
	LOSS [training: 0.40783875075658593 | validation: 0.43720039923879295]
	TIME [epoch: 41.5 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3628857390571059		[learning rate: 0.0084756]
	Learning Rate: 0.00847562
	LOSS [training: 0.3628857390571059 | validation: 0.5370474794557498]
	TIME [epoch: 41.5 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5239010133343265		[learning rate: 0.0084699]
	Learning Rate: 0.00846986
	LOSS [training: 0.5239010133343265 | validation: 0.4377218649388944]
	TIME [epoch: 41.5 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39495191815941766		[learning rate: 0.0084641]
	Learning Rate: 0.00846408
	LOSS [training: 0.39495191815941766 | validation: 0.4040140495201958]
	TIME [epoch: 41.5 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4048099066313275		[learning rate: 0.0084583]
	Learning Rate: 0.0084583
	LOSS [training: 0.4048099066313275 | validation: 0.45493906271361234]
	TIME [epoch: 41.5 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37876919082914023		[learning rate: 0.0084525]
	Learning Rate: 0.00845252
	LOSS [training: 0.37876919082914023 | validation: 0.4433018971752255]
	TIME [epoch: 41.5 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3802818941865529		[learning rate: 0.0084467]
	Learning Rate: 0.00844672
	LOSS [training: 0.3802818941865529 | validation: 0.4591004968791457]
	TIME [epoch: 41.5 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40765340775006786		[learning rate: 0.0084409]
	Learning Rate: 0.00844091
	LOSS [training: 0.40765340775006786 | validation: 0.528756104806818]
	TIME [epoch: 41.5 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4024683968259223		[learning rate: 0.0084351]
	Learning Rate: 0.00843509
	LOSS [training: 0.4024683968259223 | validation: 0.5072559788944622]
	TIME [epoch: 41.5 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4101072495234278		[learning rate: 0.0084293]
	Learning Rate: 0.00842927
	LOSS [training: 0.4101072495234278 | validation: 0.4351912643167595]
	TIME [epoch: 41.5 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41973825693000255		[learning rate: 0.0084234]
	Learning Rate: 0.00842344
	LOSS [training: 0.41973825693000255 | validation: 0.3900595722555936]
	TIME [epoch: 41.5 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38787268251783946		[learning rate: 0.0084176]
	Learning Rate: 0.0084176
	LOSS [training: 0.38787268251783946 | validation: 0.4701146574785293]
	TIME [epoch: 41.5 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41338214594528516		[learning rate: 0.0084117]
	Learning Rate: 0.00841174
	LOSS [training: 0.41338214594528516 | validation: 0.39361818353538935]
	TIME [epoch: 41.5 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3534687615369232		[learning rate: 0.0084059]
	Learning Rate: 0.00840588
	LOSS [training: 0.3534687615369232 | validation: 0.5748910785562883]
	TIME [epoch: 41.5 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45991705756281204		[learning rate: 0.0084]
	Learning Rate: 0.00840002
	LOSS [training: 0.45991705756281204 | validation: 0.425951433172664]
	TIME [epoch: 41.5 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37133499154670174		[learning rate: 0.0083941]
	Learning Rate: 0.00839414
	LOSS [training: 0.37133499154670174 | validation: 0.35895181370278545]
	TIME [epoch: 41.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_535.pth
	Model improved!!!
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4910239436001974		[learning rate: 0.0083883]
	Learning Rate: 0.00838825
	LOSS [training: 0.4910239436001974 | validation: 0.4991508013231618]
	TIME [epoch: 41.5 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4499910992298561		[learning rate: 0.0083824]
	Learning Rate: 0.00838236
	LOSS [training: 0.4499910992298561 | validation: 0.424865211724543]
	TIME [epoch: 41.5 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38966968387045525		[learning rate: 0.0083765]
	Learning Rate: 0.00837646
	LOSS [training: 0.38966968387045525 | validation: 0.46452338569686646]
	TIME [epoch: 41.5 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4072412023425159		[learning rate: 0.0083705]
	Learning Rate: 0.00837054
	LOSS [training: 0.4072412023425159 | validation: 0.49394931065153935]
	TIME [epoch: 41.5 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4072218818865243		[learning rate: 0.0083646]
	Learning Rate: 0.00836462
	LOSS [training: 0.4072218818865243 | validation: 0.4798823934780797]
	TIME [epoch: 41.5 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.412531433760005		[learning rate: 0.0083587]
	Learning Rate: 0.0083587
	LOSS [training: 0.412531433760005 | validation: 0.5438770651755673]
	TIME [epoch: 41.5 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43691080400283844		[learning rate: 0.0083528]
	Learning Rate: 0.00835276
	LOSS [training: 0.43691080400283844 | validation: 0.48120912375415553]
	TIME [epoch: 41.5 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.425624195394563		[learning rate: 0.0083468]
	Learning Rate: 0.00834681
	LOSS [training: 0.425624195394563 | validation: 0.4308324588075682]
	TIME [epoch: 41.5 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40513930617812177		[learning rate: 0.0083409]
	Learning Rate: 0.00834086
	LOSS [training: 0.40513930617812177 | validation: 0.4699233906382764]
	TIME [epoch: 41.5 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39531719050421216		[learning rate: 0.0083349]
	Learning Rate: 0.00833489
	LOSS [training: 0.39531719050421216 | validation: 0.391521467560899]
	TIME [epoch: 41.5 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.397155645247575		[learning rate: 0.0083289]
	Learning Rate: 0.00832892
	LOSS [training: 0.397155645247575 | validation: 0.44297564225913055]
	TIME [epoch: 41.5 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4170103224870842		[learning rate: 0.0083229]
	Learning Rate: 0.00832294
	LOSS [training: 0.4170103224870842 | validation: 0.44779535315944863]
	TIME [epoch: 41.5 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39308994314607143		[learning rate: 0.008317]
	Learning Rate: 0.00831695
	LOSS [training: 0.39308994314607143 | validation: 0.4520309327165535]
	TIME [epoch: 41.5 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45435767138741995		[learning rate: 0.008311]
	Learning Rate: 0.00831095
	LOSS [training: 0.45435767138741995 | validation: 0.423546738736998]
	TIME [epoch: 41.5 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3952430468188053		[learning rate: 0.0083049]
	Learning Rate: 0.00830495
	LOSS [training: 0.3952430468188053 | validation: 0.39850596813382866]
	TIME [epoch: 41.5 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3914374081074666		[learning rate: 0.0082989]
	Learning Rate: 0.00829893
	LOSS [training: 0.3914374081074666 | validation: 0.40593559272811375]
	TIME [epoch: 41.5 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3946750029536682		[learning rate: 0.0082929]
	Learning Rate: 0.00829291
	LOSS [training: 0.3946750029536682 | validation: 0.3849704633459585]
	TIME [epoch: 41.5 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38919524167154723		[learning rate: 0.0082869]
	Learning Rate: 0.00828688
	LOSS [training: 0.38919524167154723 | validation: 0.4305615482078021]
	TIME [epoch: 41.5 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.405141059314906		[learning rate: 0.0082808]
	Learning Rate: 0.00828084
	LOSS [training: 0.405141059314906 | validation: 0.46697099688710764]
	TIME [epoch: 41.5 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.419749584744313		[learning rate: 0.0082748]
	Learning Rate: 0.00827479
	LOSS [training: 0.419749584744313 | validation: 0.446182250794232]
	TIME [epoch: 41.5 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3843087075195435		[learning rate: 0.0082687]
	Learning Rate: 0.00826873
	LOSS [training: 0.3843087075195435 | validation: 0.3895413489867636]
	TIME [epoch: 41.6 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38132247053347923		[learning rate: 0.0082627]
	Learning Rate: 0.00826267
	LOSS [training: 0.38132247053347923 | validation: 0.485242697690497]
	TIME [epoch: 41.5 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41155470046083203		[learning rate: 0.0082566]
	Learning Rate: 0.0082566
	LOSS [training: 0.41155470046083203 | validation: 0.40493740632666875]
	TIME [epoch: 41.5 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35288978085886924		[learning rate: 0.0082505]
	Learning Rate: 0.00825051
	LOSS [training: 0.35288978085886924 | validation: 0.4425103746106288]
	TIME [epoch: 41.6 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4383112507095087		[learning rate: 0.0082444]
	Learning Rate: 0.00824443
	LOSS [training: 0.4383112507095087 | validation: 0.39707184681877017]
	TIME [epoch: 41.5 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3782153264378121		[learning rate: 0.0082383]
	Learning Rate: 0.00823833
	LOSS [training: 0.3782153264378121 | validation: 0.3986090558417164]
	TIME [epoch: 41.5 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3876770868875496		[learning rate: 0.0082322]
	Learning Rate: 0.00823222
	LOSS [training: 0.3876770868875496 | validation: 0.5045205541301208]
	TIME [epoch: 41.5 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3836246401820539		[learning rate: 0.0082261]
	Learning Rate: 0.0082261
	LOSS [training: 0.3836246401820539 | validation: 0.3773543833325928]
	TIME [epoch: 41.6 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3860250475219926		[learning rate: 0.00822]
	Learning Rate: 0.00821998
	LOSS [training: 0.3860250475219926 | validation: 0.36548135493791906]
	TIME [epoch: 41.6 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37354340418735615		[learning rate: 0.0082138]
	Learning Rate: 0.00821385
	LOSS [training: 0.37354340418735615 | validation: 0.35744120739036644]
	TIME [epoch: 41.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_565.pth
	Model improved!!!
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3820464145033744		[learning rate: 0.0082077]
	Learning Rate: 0.00820771
	LOSS [training: 0.3820464145033744 | validation: 0.3921698638236569]
	TIME [epoch: 41.6 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3893776719030944		[learning rate: 0.0082016]
	Learning Rate: 0.00820156
	LOSS [training: 0.3893776719030944 | validation: 0.36616562499547667]
	TIME [epoch: 41.5 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36604535863599463		[learning rate: 0.0081954]
	Learning Rate: 0.00819541
	LOSS [training: 0.36604535863599463 | validation: 0.37698883906075387]
	TIME [epoch: 41.5 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35917810713788184		[learning rate: 0.0081892]
	Learning Rate: 0.00818924
	LOSS [training: 0.35917810713788184 | validation: 0.3815901633977704]
	TIME [epoch: 41.6 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4165735397965117		[learning rate: 0.0081831]
	Learning Rate: 0.00818307
	LOSS [training: 0.4165735397965117 | validation: 0.3855973215511379]
	TIME [epoch: 41.6 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3658304980612744		[learning rate: 0.0081769]
	Learning Rate: 0.00817689
	LOSS [training: 0.3658304980612744 | validation: 0.47760886560656746]
	TIME [epoch: 41.6 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3762550094632418		[learning rate: 0.0081707]
	Learning Rate: 0.0081707
	LOSS [training: 0.3762550094632418 | validation: 0.4006794867626804]
	TIME [epoch: 41.6 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3752737691199181		[learning rate: 0.0081645]
	Learning Rate: 0.0081645
	LOSS [training: 0.3752737691199181 | validation: 0.37346375887263666]
	TIME [epoch: 41.6 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3707928106878649		[learning rate: 0.0081583]
	Learning Rate: 0.0081583
	LOSS [training: 0.3707928106878649 | validation: 0.40164405462379266]
	TIME [epoch: 41.5 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.365936414174032		[learning rate: 0.0081521]
	Learning Rate: 0.00815208
	LOSS [training: 0.365936414174032 | validation: 0.43621986132987367]
	TIME [epoch: 41.5 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40371070014757626		[learning rate: 0.0081459]
	Learning Rate: 0.00814586
	LOSS [training: 0.40371070014757626 | validation: 0.3940204789637069]
	TIME [epoch: 41.6 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3617442968143594		[learning rate: 0.0081396]
	Learning Rate: 0.00813963
	LOSS [training: 0.3617442968143594 | validation: 0.3407013246278773]
	TIME [epoch: 41.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_577.pth
	Model improved!!!
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3960869920448853		[learning rate: 0.0081334]
	Learning Rate: 0.0081334
	LOSS [training: 0.3960869920448853 | validation: 0.5041927894138295]
	TIME [epoch: 41.6 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37849008719557986		[learning rate: 0.0081272]
	Learning Rate: 0.00812715
	LOSS [training: 0.37849008719557986 | validation: 0.47295376990631083]
	TIME [epoch: 41.6 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3600603580105236		[learning rate: 0.0081209]
	Learning Rate: 0.0081209
	LOSS [training: 0.3600603580105236 | validation: 0.31532758846793485]
	TIME [epoch: 41.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_580.pth
	Model improved!!!
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41750358235455715		[learning rate: 0.0081146]
	Learning Rate: 0.00811464
	LOSS [training: 0.41750358235455715 | validation: 0.5563245931372824]
	TIME [epoch: 41.5 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4089503851596762		[learning rate: 0.0081084]
	Learning Rate: 0.00810837
	LOSS [training: 0.4089503851596762 | validation: 0.35830694304540767]
	TIME [epoch: 41.5 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3574193727807715		[learning rate: 0.0081021]
	Learning Rate: 0.00810209
	LOSS [training: 0.3574193727807715 | validation: 0.413556821922732]
	TIME [epoch: 41.5 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.355565328574793		[learning rate: 0.0080958]
	Learning Rate: 0.0080958
	LOSS [training: 0.355565328574793 | validation: 0.38603723664721323]
	TIME [epoch: 41.5 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4044892659180972		[learning rate: 0.0080895]
	Learning Rate: 0.00808951
	LOSS [training: 0.4044892659180972 | validation: 0.4004964406208967]
	TIME [epoch: 41.5 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3593399080381871		[learning rate: 0.0080832]
	Learning Rate: 0.00808321
	LOSS [training: 0.3593399080381871 | validation: 0.35956911364419986]
	TIME [epoch: 41.5 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38506802409839647		[learning rate: 0.0080769]
	Learning Rate: 0.0080769
	LOSS [training: 0.38506802409839647 | validation: 0.36968289047212266]
	TIME [epoch: 41.5 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42236553567509055		[learning rate: 0.0080706]
	Learning Rate: 0.00807058
	LOSS [training: 0.42236553567509055 | validation: 0.43750264199770195]
	TIME [epoch: 41.5 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37667255818227985		[learning rate: 0.0080643]
	Learning Rate: 0.00806426
	LOSS [training: 0.37667255818227985 | validation: 0.388780692399622]
	TIME [epoch: 41.5 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3494693192273782		[learning rate: 0.0080579]
	Learning Rate: 0.00805792
	LOSS [training: 0.3494693192273782 | validation: 0.4785284757171864]
	TIME [epoch: 41.5 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40275925283110453		[learning rate: 0.0080516]
	Learning Rate: 0.00805158
	LOSS [training: 0.40275925283110453 | validation: 0.4069463362846542]
	TIME [epoch: 41.5 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3251999451060675		[learning rate: 0.0080452]
	Learning Rate: 0.00804523
	LOSS [training: 0.3251999451060675 | validation: 0.3901751570770805]
	TIME [epoch: 41.5 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4697537018191946		[learning rate: 0.0080389]
	Learning Rate: 0.00803888
	LOSS [training: 0.4697537018191946 | validation: 0.4250617538366245]
	TIME [epoch: 41.5 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3819355099471686		[learning rate: 0.0080325]
	Learning Rate: 0.00803251
	LOSS [training: 0.3819355099471686 | validation: 0.3858833042068053]
	TIME [epoch: 41.5 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35871539017059545		[learning rate: 0.0080261]
	Learning Rate: 0.00802614
	LOSS [training: 0.35871539017059545 | validation: 0.3510191491057546]
	TIME [epoch: 41.5 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3664423558003035		[learning rate: 0.0080198]
	Learning Rate: 0.00801976
	LOSS [training: 0.3664423558003035 | validation: 0.3408149255745998]
	TIME [epoch: 41.5 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35844083733703036		[learning rate: 0.0080134]
	Learning Rate: 0.00801337
	LOSS [training: 0.35844083733703036 | validation: 0.43305961096819556]
	TIME [epoch: 41.5 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39710131475096144		[learning rate: 0.008007]
	Learning Rate: 0.00800698
	LOSS [training: 0.39710131475096144 | validation: 0.5092778866826281]
	TIME [epoch: 41.5 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3939979924945123		[learning rate: 0.0080006]
	Learning Rate: 0.00800058
	LOSS [training: 0.3939979924945123 | validation: 0.44562225768519736]
	TIME [epoch: 41.5 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3707116781744154		[learning rate: 0.0079942]
	Learning Rate: 0.00799417
	LOSS [training: 0.3707116781744154 | validation: 0.3817607899497009]
	TIME [epoch: 41.5 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3721677761426039		[learning rate: 0.0079877]
	Learning Rate: 0.00798775
	LOSS [training: 0.3721677761426039 | validation: 0.36224856714784825]
	TIME [epoch: 41.5 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34661645543857966		[learning rate: 0.0079813]
	Learning Rate: 0.00798132
	LOSS [training: 0.34661645543857966 | validation: 0.36728478147533905]
	TIME [epoch: 41.5 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36194965665843104		[learning rate: 0.0079749]
	Learning Rate: 0.00797489
	LOSS [training: 0.36194965665843104 | validation: 0.3442027476158237]
	TIME [epoch: 41.5 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35811025178875194		[learning rate: 0.0079684]
	Learning Rate: 0.00796845
	LOSS [training: 0.35811025178875194 | validation: 0.3674789131573821]
	TIME [epoch: 41.5 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.391189760121437		[learning rate: 0.007962]
	Learning Rate: 0.007962
	LOSS [training: 0.391189760121437 | validation: 0.3597012761698616]
	TIME [epoch: 41.5 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3755066562732634		[learning rate: 0.0079555]
	Learning Rate: 0.00795554
	LOSS [training: 0.3755066562732634 | validation: 0.421166521354883]
	TIME [epoch: 41.5 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37187872570147645		[learning rate: 0.0079491]
	Learning Rate: 0.00794908
	LOSS [training: 0.37187872570147645 | validation: 0.34282091448323965]
	TIME [epoch: 41.5 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34746437431250143		[learning rate: 0.0079426]
	Learning Rate: 0.00794261
	LOSS [training: 0.34746437431250143 | validation: 0.3563692456581943]
	TIME [epoch: 41.5 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4012201015863912		[learning rate: 0.0079361]
	Learning Rate: 0.00793613
	LOSS [training: 0.4012201015863912 | validation: 0.3999989065390046]
	TIME [epoch: 41.5 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3976025703316751		[learning rate: 0.0079296]
	Learning Rate: 0.00792964
	LOSS [training: 0.3976025703316751 | validation: 0.3716126964059086]
	TIME [epoch: 41.5 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39481247860929813		[learning rate: 0.0079231]
	Learning Rate: 0.00792315
	LOSS [training: 0.39481247860929813 | validation: 0.3992496901604272]
	TIME [epoch: 41.5 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3705887728112241		[learning rate: 0.0079166]
	Learning Rate: 0.00791665
	LOSS [training: 0.3705887728112241 | validation: 0.3492685262939172]
	TIME [epoch: 41.5 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36203032861118245		[learning rate: 0.0079101]
	Learning Rate: 0.00791014
	LOSS [training: 0.36203032861118245 | validation: 0.38775237678939434]
	TIME [epoch: 41.5 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3766656483853346		[learning rate: 0.0079036]
	Learning Rate: 0.00790362
	LOSS [training: 0.3766656483853346 | validation: 0.4232615339605551]
	TIME [epoch: 41.5 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3339488626362085		[learning rate: 0.0078971]
	Learning Rate: 0.00789709
	LOSS [training: 0.3339488626362085 | validation: 0.45413386116033616]
	TIME [epoch: 41.5 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3970058833711291		[learning rate: 0.0078906]
	Learning Rate: 0.00789056
	LOSS [training: 0.3970058833711291 | validation: 0.408914482052181]
	TIME [epoch: 41.5 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3766575720872798		[learning rate: 0.007884]
	Learning Rate: 0.00788402
	LOSS [training: 0.3766575720872798 | validation: 0.3912489485061327]
	TIME [epoch: 41.5 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36298177433474205		[learning rate: 0.0078775]
	Learning Rate: 0.00787748
	LOSS [training: 0.36298177433474205 | validation: 0.37332316668457244]
	TIME [epoch: 41.5 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36364073175021316		[learning rate: 0.0078709]
	Learning Rate: 0.00787092
	LOSS [training: 0.36364073175021316 | validation: 0.351937798318762]
	TIME [epoch: 41.5 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38750279569655977		[learning rate: 0.0078644]
	Learning Rate: 0.00786436
	LOSS [training: 0.38750279569655977 | validation: 0.3944495212016066]
	TIME [epoch: 41.5 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34594765259714494		[learning rate: 0.0078578]
	Learning Rate: 0.0078578
	LOSS [training: 0.34594765259714494 | validation: 0.33002966107433196]
	TIME [epoch: 41.5 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39348704780995974		[learning rate: 0.0078512]
	Learning Rate: 0.00785122
	LOSS [training: 0.39348704780995974 | validation: 0.4078016891341808]
	TIME [epoch: 41.5 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36331954125685106		[learning rate: 0.0078446]
	Learning Rate: 0.00784464
	LOSS [training: 0.36331954125685106 | validation: 0.3707750706117524]
	TIME [epoch: 41.5 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.358183951007105		[learning rate: 0.007838]
	Learning Rate: 0.00783805
	LOSS [training: 0.358183951007105 | validation: 0.3831007172806237]
	TIME [epoch: 41.5 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36915773775670657		[learning rate: 0.0078314]
	Learning Rate: 0.00783145
	LOSS [training: 0.36915773775670657 | validation: 0.38927092390360135]
	TIME [epoch: 41.5 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36843213393280316		[learning rate: 0.0078248]
	Learning Rate: 0.00782484
	LOSS [training: 0.36843213393280316 | validation: 0.3838599074392889]
	TIME [epoch: 41.5 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35412616935082275		[learning rate: 0.0078182]
	Learning Rate: 0.00781823
	LOSS [training: 0.35412616935082275 | validation: 0.45136106590180525]
	TIME [epoch: 41.5 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36601427168418244		[learning rate: 0.0078116]
	Learning Rate: 0.00781161
	LOSS [training: 0.36601427168418244 | validation: 0.3175123335388503]
	TIME [epoch: 41.5 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4393100464658588		[learning rate: 0.007805]
	Learning Rate: 0.00780499
	LOSS [training: 0.4393100464658588 | validation: 0.4038200241692316]
	TIME [epoch: 41.5 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3629521909351294		[learning rate: 0.0077984]
	Learning Rate: 0.00779835
	LOSS [training: 0.3629521909351294 | validation: 0.40699150056722605]
	TIME [epoch: 41.5 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3463215081079634		[learning rate: 0.0077917]
	Learning Rate: 0.00779171
	LOSS [training: 0.3463215081079634 | validation: 0.3806658070746666]
	TIME [epoch: 41.5 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35064849503469925		[learning rate: 0.0077851]
	Learning Rate: 0.00778506
	LOSS [training: 0.35064849503469925 | validation: 0.4168584799222006]
	TIME [epoch: 41.5 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3864553864377763		[learning rate: 0.0077784]
	Learning Rate: 0.00777841
	LOSS [training: 0.3864553864377763 | validation: 0.3589707900521048]
	TIME [epoch: 41.5 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34968703378962274		[learning rate: 0.0077717]
	Learning Rate: 0.00777175
	LOSS [training: 0.34968703378962274 | validation: 0.3310125261676185]
	TIME [epoch: 41.5 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34925736444252325		[learning rate: 0.0077651]
	Learning Rate: 0.00776508
	LOSS [training: 0.34925736444252325 | validation: 0.3444262633994256]
	TIME [epoch: 41.5 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3552918209810684		[learning rate: 0.0077584]
	Learning Rate: 0.0077584
	LOSS [training: 0.3552918209810684 | validation: 0.5200257668431594]
	TIME [epoch: 41.5 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37456425384073694		[learning rate: 0.0077517]
	Learning Rate: 0.00775172
	LOSS [training: 0.37456425384073694 | validation: 0.3563037075207934]
	TIME [epoch: 41.5 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36295350239353824		[learning rate: 0.007745]
	Learning Rate: 0.00774503
	LOSS [training: 0.36295350239353824 | validation: 0.35978695734017857]
	TIME [epoch: 41.5 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32427120417320754		[learning rate: 0.0077383]
	Learning Rate: 0.00773833
	LOSS [training: 0.32427120417320754 | validation: 0.33284563105892284]
	TIME [epoch: 41.5 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37920101511417237		[learning rate: 0.0077316]
	Learning Rate: 0.00773162
	LOSS [training: 0.37920101511417237 | validation: 0.46005903855031266]
	TIME [epoch: 41.5 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.353036682990775		[learning rate: 0.0077249]
	Learning Rate: 0.00772491
	LOSS [training: 0.353036682990775 | validation: 0.37428091679468967]
	TIME [epoch: 41.5 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3635452029129749		[learning rate: 0.0077182]
	Learning Rate: 0.00771819
	LOSS [training: 0.3635452029129749 | validation: 0.46317812615661524]
	TIME [epoch: 41.5 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36095705923086363		[learning rate: 0.0077115]
	Learning Rate: 0.00771147
	LOSS [training: 0.36095705923086363 | validation: 0.3634914810617617]
	TIME [epoch: 41.5 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3336411261425948		[learning rate: 0.0077047]
	Learning Rate: 0.00770474
	LOSS [training: 0.3336411261425948 | validation: 0.37460215854781065]
	TIME [epoch: 41.5 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34370898889721385		[learning rate: 0.007698]
	Learning Rate: 0.007698
	LOSS [training: 0.34370898889721385 | validation: 0.39289667822008206]
	TIME [epoch: 41.5 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3648982649599305		[learning rate: 0.0076912]
	Learning Rate: 0.00769125
	LOSS [training: 0.3648982649599305 | validation: 0.3280827249557583]
	TIME [epoch: 41.5 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33449202016455415		[learning rate: 0.0076845]
	Learning Rate: 0.0076845
	LOSS [training: 0.33449202016455415 | validation: 0.4260805321012763]
	TIME [epoch: 41.5 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38202612259224655		[learning rate: 0.0076777]
	Learning Rate: 0.00767774
	LOSS [training: 0.38202612259224655 | validation: 0.35966761388398044]
	TIME [epoch: 41.5 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35615917643146916		[learning rate: 0.007671]
	Learning Rate: 0.00767097
	LOSS [training: 0.35615917643146916 | validation: 0.38091366746915245]
	TIME [epoch: 41.5 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3359412424749096		[learning rate: 0.0076642]
	Learning Rate: 0.00766419
	LOSS [training: 0.3359412424749096 | validation: 0.40143150283205664]
	TIME [epoch: 41.5 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35729593043279284		[learning rate: 0.0076574]
	Learning Rate: 0.00765741
	LOSS [training: 0.35729593043279284 | validation: 0.44672479765378725]
	TIME [epoch: 41.5 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3801158614639467		[learning rate: 0.0076506]
	Learning Rate: 0.00765063
	LOSS [training: 0.3801158614639467 | validation: 0.40106935257399723]
	TIME [epoch: 41.5 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35963171738024013		[learning rate: 0.0076438]
	Learning Rate: 0.00764383
	LOSS [training: 0.35963171738024013 | validation: 0.371304204664691]
	TIME [epoch: 41.5 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33589616792740995		[learning rate: 0.007637]
	Learning Rate: 0.00763703
	LOSS [training: 0.33589616792740995 | validation: 0.4094509462131147]
	TIME [epoch: 41.5 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3518440820478622		[learning rate: 0.0076302]
	Learning Rate: 0.00763022
	LOSS [training: 0.3518440820478622 | validation: 0.4049328243965288]
	TIME [epoch: 41.5 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.377353425762452		[learning rate: 0.0076234]
	Learning Rate: 0.00762341
	LOSS [training: 0.377353425762452 | validation: 0.40763544640930466]
	TIME [epoch: 41.5 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3798788717158267		[learning rate: 0.0076166]
	Learning Rate: 0.00761659
	LOSS [training: 0.3798788717158267 | validation: 0.3529571774270183]
	TIME [epoch: 41.5 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3472684236342365		[learning rate: 0.0076098]
	Learning Rate: 0.00760976
	LOSS [training: 0.3472684236342365 | validation: 0.33475665940387966]
	TIME [epoch: 41.5 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37932471511493693		[learning rate: 0.0076029]
	Learning Rate: 0.00760292
	LOSS [training: 0.37932471511493693 | validation: 0.38282816615992665]
	TIME [epoch: 41.5 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3434116926362208		[learning rate: 0.0075961]
	Learning Rate: 0.00759608
	LOSS [training: 0.3434116926362208 | validation: 0.3701877513763607]
	TIME [epoch: 41.5 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3464144412277953		[learning rate: 0.0075892]
	Learning Rate: 0.00758923
	LOSS [training: 0.3464144412277953 | validation: 0.33734654246602513]
	TIME [epoch: 41.5 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3515290759063382		[learning rate: 0.0075824]
	Learning Rate: 0.00758238
	LOSS [training: 0.3515290759063382 | validation: 0.41692945508724594]
	TIME [epoch: 41.5 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3710318421382197		[learning rate: 0.0075755]
	Learning Rate: 0.00757552
	LOSS [training: 0.3710318421382197 | validation: 0.31797202747641173]
	TIME [epoch: 41.5 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35573120493358623		[learning rate: 0.0075686]
	Learning Rate: 0.00756865
	LOSS [training: 0.35573120493358623 | validation: 0.46726608806688164]
	TIME [epoch: 41.5 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3694551424942365		[learning rate: 0.0075618]
	Learning Rate: 0.00756178
	LOSS [training: 0.3694551424942365 | validation: 0.4241687581358316]
	TIME [epoch: 41.5 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3565521350099167		[learning rate: 0.0075549]
	Learning Rate: 0.00755489
	LOSS [training: 0.3565521350099167 | validation: 0.31807278991782106]
	TIME [epoch: 41.5 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3575740242211598		[learning rate: 0.007548]
	Learning Rate: 0.00754801
	LOSS [training: 0.3575740242211598 | validation: 0.3712385923852779]
	TIME [epoch: 41.5 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32027962939026783		[learning rate: 0.0075411]
	Learning Rate: 0.00754111
	LOSS [training: 0.32027962939026783 | validation: 0.3541927814715559]
	TIME [epoch: 41.5 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3636656739446185		[learning rate: 0.0075342]
	Learning Rate: 0.00753421
	LOSS [training: 0.3636656739446185 | validation: 0.3327617755816191]
	TIME [epoch: 41.5 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3570016550861574		[learning rate: 0.0075273]
	Learning Rate: 0.0075273
	LOSS [training: 0.3570016550861574 | validation: 0.34254562269065125]
	TIME [epoch: 41.5 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33929021023183126		[learning rate: 0.0075204]
	Learning Rate: 0.00752039
	LOSS [training: 0.33929021023183126 | validation: 0.33920362475825383]
	TIME [epoch: 41.5 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39015193644030477		[learning rate: 0.0075135]
	Learning Rate: 0.00751347
	LOSS [training: 0.39015193644030477 | validation: 0.3509106756401873]
	TIME [epoch: 41.5 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33657534208391754		[learning rate: 0.0075065]
	Learning Rate: 0.00750654
	LOSS [training: 0.33657534208391754 | validation: 0.3441722969409843]
	TIME [epoch: 41.5 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34686003457441933		[learning rate: 0.0074996]
	Learning Rate: 0.00749961
	LOSS [training: 0.34686003457441933 | validation: 0.4220715595338269]
	TIME [epoch: 41.5 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.350012705510389		[learning rate: 0.0074927]
	Learning Rate: 0.00749267
	LOSS [training: 0.350012705510389 | validation: 0.3827332886508188]
	TIME [epoch: 41.5 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3436398781289548		[learning rate: 0.0074857]
	Learning Rate: 0.00748572
	LOSS [training: 0.3436398781289548 | validation: 0.35549981280542353]
	TIME [epoch: 41.5 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32212205836989743		[learning rate: 0.0074788]
	Learning Rate: 0.00747877
	LOSS [training: 0.32212205836989743 | validation: 0.4076326880909934]
	TIME [epoch: 41.5 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.352430767329865		[learning rate: 0.0074718]
	Learning Rate: 0.00747181
	LOSS [training: 0.352430767329865 | validation: 0.3773652923884434]
	TIME [epoch: 41.5 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3111764324272516		[learning rate: 0.0074648]
	Learning Rate: 0.00746485
	LOSS [training: 0.3111764324272516 | validation: 0.3135124473130263]
	TIME [epoch: 41.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_679.pth
	Model improved!!!
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3787667322590631		[learning rate: 0.0074579]
	Learning Rate: 0.00745788
	LOSS [training: 0.3787667322590631 | validation: 0.39981695948206]
	TIME [epoch: 41.5 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35998844222268184		[learning rate: 0.0074509]
	Learning Rate: 0.0074509
	LOSS [training: 0.35998844222268184 | validation: 0.36941062187833956]
	TIME [epoch: 41.5 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32549726176145466		[learning rate: 0.0074439]
	Learning Rate: 0.00744392
	LOSS [training: 0.32549726176145466 | validation: 0.40101946530479404]
	TIME [epoch: 41.5 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34435398274760626		[learning rate: 0.0074369]
	Learning Rate: 0.00743693
	LOSS [training: 0.34435398274760626 | validation: 0.3360096157038437]
	TIME [epoch: 41.5 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3495417207480477		[learning rate: 0.0074299]
	Learning Rate: 0.00742993
	LOSS [training: 0.3495417207480477 | validation: 0.3517690472123593]
	TIME [epoch: 41.5 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4005845251960387		[learning rate: 0.0074229]
	Learning Rate: 0.00742293
	LOSS [training: 0.4005845251960387 | validation: 0.362312423985909]
	TIME [epoch: 41.5 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3183841703873007		[learning rate: 0.0074159]
	Learning Rate: 0.00741592
	LOSS [training: 0.3183841703873007 | validation: 0.3314033697962993]
	TIME [epoch: 41.5 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37694490003866066		[learning rate: 0.0074089]
	Learning Rate: 0.0074089
	LOSS [training: 0.37694490003866066 | validation: 0.370820042054571]
	TIME [epoch: 41.5 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38962254566349774		[learning rate: 0.0074019]
	Learning Rate: 0.00740188
	LOSS [training: 0.38962254566349774 | validation: 0.3459240504960899]
	TIME [epoch: 41.5 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35449838673323225		[learning rate: 0.0073949]
	Learning Rate: 0.00739485
	LOSS [training: 0.35449838673323225 | validation: 0.3844082133130352]
	TIME [epoch: 41.5 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35111882110363746		[learning rate: 0.0073878]
	Learning Rate: 0.00738782
	LOSS [training: 0.35111882110363746 | validation: 0.337347791836645]
	TIME [epoch: 41.5 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31348086343068005		[learning rate: 0.0073808]
	Learning Rate: 0.00738078
	LOSS [training: 0.31348086343068005 | validation: 0.3328152759390131]
	TIME [epoch: 41.5 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36998610194139747		[learning rate: 0.0073737]
	Learning Rate: 0.00737374
	LOSS [training: 0.36998610194139747 | validation: 0.3234286092065425]
	TIME [epoch: 41.5 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33898076825679063		[learning rate: 0.0073667]
	Learning Rate: 0.00736668
	LOSS [training: 0.33898076825679063 | validation: 0.31237949708531565]
	TIME [epoch: 41.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_693.pth
	Model improved!!!
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.345746809936047		[learning rate: 0.0073596]
	Learning Rate: 0.00735963
	LOSS [training: 0.345746809936047 | validation: 0.37362087434896557]
	TIME [epoch: 41.5 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36330239723187313		[learning rate: 0.0073526]
	Learning Rate: 0.00735256
	LOSS [training: 0.36330239723187313 | validation: 0.351997466804838]
	TIME [epoch: 41.4 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3180731251515865		[learning rate: 0.0073455]
	Learning Rate: 0.00734549
	LOSS [training: 0.3180731251515865 | validation: 0.4534663399797174]
	TIME [epoch: 41.5 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3696822139202242		[learning rate: 0.0073384]
	Learning Rate: 0.00733842
	LOSS [training: 0.3696822139202242 | validation: 0.3510458913655756]
	TIME [epoch: 41.5 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36075832878184105		[learning rate: 0.0073313]
	Learning Rate: 0.00733133
	LOSS [training: 0.36075832878184105 | validation: 0.34619380162396696]
	TIME [epoch: 41.5 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3387052638314313		[learning rate: 0.0073242]
	Learning Rate: 0.00732425
	LOSS [training: 0.3387052638314313 | validation: 0.3512632144324509]
	TIME [epoch: 41.5 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.322186770923813		[learning rate: 0.0073172]
	Learning Rate: 0.00731715
	LOSS [training: 0.322186770923813 | validation: 0.350390413203622]
	TIME [epoch: 41.5 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.328732287800183		[learning rate: 0.0073101]
	Learning Rate: 0.00731005
	LOSS [training: 0.328732287800183 | validation: 0.3257445543769483]
	TIME [epoch: 41.5 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3246879050203117		[learning rate: 0.0073029]
	Learning Rate: 0.00730295
	LOSS [training: 0.3246879050203117 | validation: 0.3549982771758978]
	TIME [epoch: 41.5 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37288793748653265		[learning rate: 0.0072958]
	Learning Rate: 0.00729584
	LOSS [training: 0.37288793748653265 | validation: 0.45957483069153604]
	TIME [epoch: 41.5 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36232101662163174		[learning rate: 0.0072887]
	Learning Rate: 0.00728872
	LOSS [training: 0.36232101662163174 | validation: 0.3169863033764252]
	TIME [epoch: 41.5 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3489923081346158		[learning rate: 0.0072816]
	Learning Rate: 0.00728159
	LOSS [training: 0.3489923081346158 | validation: 0.32117052343360486]
	TIME [epoch: 41.5 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.305116425061037		[learning rate: 0.0072745]
	Learning Rate: 0.00727447
	LOSS [training: 0.305116425061037 | validation: 0.45203626250416806]
	TIME [epoch: 41.5 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43850427174007756		[learning rate: 0.0072673]
	Learning Rate: 0.00726733
	LOSS [training: 0.43850427174007756 | validation: 0.3514771127856702]
	TIME [epoch: 41.5 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35056258527357215		[learning rate: 0.0072602]
	Learning Rate: 0.00726019
	LOSS [training: 0.35056258527357215 | validation: 0.40041419723252414]
	TIME [epoch: 41.5 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3482752345051819		[learning rate: 0.007253]
	Learning Rate: 0.00725304
	LOSS [training: 0.3482752345051819 | validation: 0.3441182854141165]
	TIME [epoch: 41.5 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34269299324208236		[learning rate: 0.0072459]
	Learning Rate: 0.00724589
	LOSS [training: 0.34269299324208236 | validation: 0.35754552902816267]
	TIME [epoch: 41.4 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32157058024212015		[learning rate: 0.0072387]
	Learning Rate: 0.00723873
	LOSS [training: 0.32157058024212015 | validation: 0.368750768315794]
	TIME [epoch: 41.5 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34008082622246416		[learning rate: 0.0072316]
	Learning Rate: 0.00723157
	LOSS [training: 0.34008082622246416 | validation: 0.32070294189020054]
	TIME [epoch: 41.5 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35165728967198767		[learning rate: 0.0072244]
	Learning Rate: 0.0072244
	LOSS [training: 0.35165728967198767 | validation: 0.3327837064422674]
	TIME [epoch: 41.5 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3405984514179129		[learning rate: 0.0072172]
	Learning Rate: 0.00721722
	LOSS [training: 0.3405984514179129 | validation: 0.32305155426862087]
	TIME [epoch: 41.5 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3324020041738126		[learning rate: 0.00721]
	Learning Rate: 0.00721004
	LOSS [training: 0.3324020041738126 | validation: 0.35970521699790026]
	TIME [epoch: 41.4 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34475381435437896		[learning rate: 0.0072029]
	Learning Rate: 0.00720286
	LOSS [training: 0.34475381435437896 | validation: 0.333605699312719]
	TIME [epoch: 41.5 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3234171747291177		[learning rate: 0.0071957]
	Learning Rate: 0.00719566
	LOSS [training: 0.3234171747291177 | validation: 0.32736917335423366]
	TIME [epoch: 41.5 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33758834350917577		[learning rate: 0.0071885]
	Learning Rate: 0.00718847
	LOSS [training: 0.33758834350917577 | validation: 0.4745319199814909]
	TIME [epoch: 41.5 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36931241444107865		[learning rate: 0.0071813]
	Learning Rate: 0.00718126
	LOSS [training: 0.36931241444107865 | validation: 0.357653147528585]
	TIME [epoch: 41.5 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3256933577662064		[learning rate: 0.0071741]
	Learning Rate: 0.00717405
	LOSS [training: 0.3256933577662064 | validation: 0.3825864879845729]
	TIME [epoch: 41.5 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32307656783110184		[learning rate: 0.0071668]
	Learning Rate: 0.00716684
	LOSS [training: 0.32307656783110184 | validation: 0.38615920145000493]
	TIME [epoch: 41.5 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3452673976326963		[learning rate: 0.0071596]
	Learning Rate: 0.00715962
	LOSS [training: 0.3452673976326963 | validation: 0.3088125644824713]
	TIME [epoch: 41.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_722.pth
	Model improved!!!
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3509873511385506		[learning rate: 0.0071524]
	Learning Rate: 0.00715239
	LOSS [training: 0.3509873511385506 | validation: 0.3174893462047731]
	TIME [epoch: 41.5 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3334695752038912		[learning rate: 0.0071452]
	Learning Rate: 0.00714516
	LOSS [training: 0.3334695752038912 | validation: 0.3456614485386107]
	TIME [epoch: 41.5 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3129824711346201		[learning rate: 0.0071379]
	Learning Rate: 0.00713792
	LOSS [training: 0.3129824711346201 | validation: 0.3103598850723366]
	TIME [epoch: 41.4 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3438072092227775		[learning rate: 0.0071307]
	Learning Rate: 0.00713068
	LOSS [training: 0.3438072092227775 | validation: 0.33649882811073506]
	TIME [epoch: 41.5 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3470525210711918		[learning rate: 0.0071234]
	Learning Rate: 0.00712343
	LOSS [training: 0.3470525210711918 | validation: 0.38104935535182394]
	TIME [epoch: 41.5 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33632101678206633		[learning rate: 0.0071162]
	Learning Rate: 0.00711618
	LOSS [training: 0.33632101678206633 | validation: 0.3109839211280889]
	TIME [epoch: 41.5 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34951478660208923		[learning rate: 0.0071089]
	Learning Rate: 0.00710892
	LOSS [training: 0.34951478660208923 | validation: 0.3782835703489425]
	TIME [epoch: 41.5 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34170995836304696		[learning rate: 0.0071017]
	Learning Rate: 0.00710166
	LOSS [training: 0.34170995836304696 | validation: 0.386087554808202]
	TIME [epoch: 41.4 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3463846518121141		[learning rate: 0.0070944]
	Learning Rate: 0.00709439
	LOSS [training: 0.3463846518121141 | validation: 0.3838022925249954]
	TIME [epoch: 41.5 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33924612299937906		[learning rate: 0.0070871]
	Learning Rate: 0.00708711
	LOSS [training: 0.33924612299937906 | validation: 0.39617199054475544]
	TIME [epoch: 41.5 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3301895635304971		[learning rate: 0.0070798]
	Learning Rate: 0.00707983
	LOSS [training: 0.3301895635304971 | validation: 0.38395308106380693]
	TIME [epoch: 41.5 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3354238057974757		[learning rate: 0.0070725]
	Learning Rate: 0.00707255
	LOSS [training: 0.3354238057974757 | validation: 0.40147196022242615]
	TIME [epoch: 41.5 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3341772380867297		[learning rate: 0.0070653]
	Learning Rate: 0.00706526
	LOSS [training: 0.3341772380867297 | validation: 0.35508148198199796]
	TIME [epoch: 41.5 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3313925917175303		[learning rate: 0.007058]
	Learning Rate: 0.00705796
	LOSS [training: 0.3313925917175303 | validation: 0.33583292284161015]
	TIME [epoch: 41.5 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34158566526097034		[learning rate: 0.0070507]
	Learning Rate: 0.00705066
	LOSS [training: 0.34158566526097034 | validation: 0.3603772748225388]
	TIME [epoch: 41.5 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33920314348682834		[learning rate: 0.0070434]
	Learning Rate: 0.00704335
	LOSS [training: 0.33920314348682834 | validation: 0.3216371014767567]
	TIME [epoch: 41.5 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3503130278913795		[learning rate: 0.007036]
	Learning Rate: 0.00703604
	LOSS [training: 0.3503130278913795 | validation: 0.3589436120706213]
	TIME [epoch: 41.5 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3312076255091447		[learning rate: 0.0070287]
	Learning Rate: 0.00702872
	LOSS [training: 0.3312076255091447 | validation: 0.3517438952039863]
	TIME [epoch: 41.5 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30916839084505376		[learning rate: 0.0070214]
	Learning Rate: 0.0070214
	LOSS [training: 0.30916839084505376 | validation: 0.4156644305909738]
	TIME [epoch: 41.5 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33120338962306406		[learning rate: 0.0070141]
	Learning Rate: 0.00701407
	LOSS [training: 0.33120338962306406 | validation: 0.32067132591799563]
	TIME [epoch: 41.5 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3595757148683793		[learning rate: 0.0070067]
	Learning Rate: 0.00700674
	LOSS [training: 0.3595757148683793 | validation: 0.34021093157267535]
	TIME [epoch: 41.5 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3314671979764291		[learning rate: 0.0069994]
	Learning Rate: 0.0069994
	LOSS [training: 0.3314671979764291 | validation: 0.41334252849642134]
	TIME [epoch: 41.5 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3239187177888197		[learning rate: 0.0069921]
	Learning Rate: 0.00699206
	LOSS [training: 0.3239187177888197 | validation: 0.35749608085542717]
	TIME [epoch: 41.5 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3177235482652101		[learning rate: 0.0069847]
	Learning Rate: 0.00698471
	LOSS [training: 0.3177235482652101 | validation: 0.425644184062411]
	TIME [epoch: 41.5 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3682872454285308		[learning rate: 0.0069774]
	Learning Rate: 0.00697736
	LOSS [training: 0.3682872454285308 | validation: 0.34125051622363023]
	TIME [epoch: 41.5 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3260385883662906		[learning rate: 0.00697]
	Learning Rate: 0.00697
	LOSS [training: 0.3260385883662906 | validation: 0.3228292119337597]
	TIME [epoch: 41.5 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3152340243731683		[learning rate: 0.0069626]
	Learning Rate: 0.00696264
	LOSS [training: 0.3152340243731683 | validation: 0.3239071119363889]
	TIME [epoch: 41.5 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31602707484547393		[learning rate: 0.0069553]
	Learning Rate: 0.00695527
	LOSS [training: 0.31602707484547393 | validation: 0.4137437079141914]
	TIME [epoch: 41.4 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3894912125200131		[learning rate: 0.0069479]
	Learning Rate: 0.0069479
	LOSS [training: 0.3894912125200131 | validation: 0.3646045472487893]
	TIME [epoch: 41.5 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3567232594876276		[learning rate: 0.0069405]
	Learning Rate: 0.00694052
	LOSS [training: 0.3567232594876276 | validation: 0.3165257276550846]
	TIME [epoch: 41.5 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3200650892610147		[learning rate: 0.0069331]
	Learning Rate: 0.00693313
	LOSS [training: 0.3200650892610147 | validation: 0.3470196418759733]
	TIME [epoch: 41.5 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3493799889511782		[learning rate: 0.0069257]
	Learning Rate: 0.00692575
	LOSS [training: 0.3493799889511782 | validation: 0.3167175240717103]
	TIME [epoch: 41.5 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3106741488474335		[learning rate: 0.0069184]
	Learning Rate: 0.00691835
	LOSS [training: 0.3106741488474335 | validation: 0.34935326302990954]
	TIME [epoch: 41.5 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.319334515922397		[learning rate: 0.006911]
	Learning Rate: 0.00691095
	LOSS [training: 0.319334515922397 | validation: 0.3072846798243617]
	TIME [epoch: 41.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_756.pth
	Model improved!!!
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3321517977269909		[learning rate: 0.0069036]
	Learning Rate: 0.00690355
	LOSS [training: 0.3321517977269909 | validation: 0.3330942846264875]
	TIME [epoch: 41.5 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3164442698725405		[learning rate: 0.0068961]
	Learning Rate: 0.00689614
	LOSS [training: 0.3164442698725405 | validation: 0.2941953537863742]
	TIME [epoch: 41.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_758.pth
	Model improved!!!
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3113878110128898		[learning rate: 0.0068887]
	Learning Rate: 0.00688873
	LOSS [training: 0.3113878110128898 | validation: 0.3492616195619658]
	TIME [epoch: 41.5 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.342682539278352		[learning rate: 0.0068813]
	Learning Rate: 0.00688131
	LOSS [training: 0.342682539278352 | validation: 0.3935507462762272]
	TIME [epoch: 41.5 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3399946362809449		[learning rate: 0.0068739]
	Learning Rate: 0.00687389
	LOSS [training: 0.3399946362809449 | validation: 0.33409103137078355]
	TIME [epoch: 41.5 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3299937062319923		[learning rate: 0.0068665]
	Learning Rate: 0.00686646
	LOSS [training: 0.3299937062319923 | validation: 0.3203794937798038]
	TIME [epoch: 41.5 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3112686006116021		[learning rate: 0.006859]
	Learning Rate: 0.00685903
	LOSS [training: 0.3112686006116021 | validation: 0.31061361458440406]
	TIME [epoch: 41.5 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3282605796560354		[learning rate: 0.0068516]
	Learning Rate: 0.0068516
	LOSS [training: 0.3282605796560354 | validation: 0.32222282944128633]
	TIME [epoch: 41.5 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3234400880892423		[learning rate: 0.0068442]
	Learning Rate: 0.00684415
	LOSS [training: 0.3234400880892423 | validation: 0.32577395380023294]
	TIME [epoch: 41.5 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3104532555570057		[learning rate: 0.0068367]
	Learning Rate: 0.00683671
	LOSS [training: 0.3104532555570057 | validation: 0.4319117982060617]
	TIME [epoch: 41.5 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3421303217633175		[learning rate: 0.0068293]
	Learning Rate: 0.00682926
	LOSS [training: 0.3421303217633175 | validation: 0.3073334134395536]
	TIME [epoch: 41.5 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33407134400005734		[learning rate: 0.0068218]
	Learning Rate: 0.0068218
	LOSS [training: 0.33407134400005734 | validation: 0.2938003978475085]
	TIME [epoch: 41.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_768.pth
	Model improved!!!
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3352337014891261		[learning rate: 0.0068143]
	Learning Rate: 0.00681434
	LOSS [training: 0.3352337014891261 | validation: 0.3810567667966607]
	TIME [epoch: 41.5 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3430074494768765		[learning rate: 0.0068069]
	Learning Rate: 0.00680688
	LOSS [training: 0.3430074494768765 | validation: 0.32261944979525503]
	TIME [epoch: 41.5 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2912487279102257		[learning rate: 0.0067994]
	Learning Rate: 0.00679941
	LOSS [training: 0.2912487279102257 | validation: 0.4781544786014431]
	TIME [epoch: 41.4 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3930995463898366		[learning rate: 0.0067919]
	Learning Rate: 0.00679193
	LOSS [training: 0.3930995463898366 | validation: 0.32834477310538956]
	TIME [epoch: 41.5 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35163862006080004		[learning rate: 0.0067845]
	Learning Rate: 0.00678445
	LOSS [training: 0.35163862006080004 | validation: 0.3340206324562329]
	TIME [epoch: 41.5 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.307355159234862		[learning rate: 0.006777]
	Learning Rate: 0.00677697
	LOSS [training: 0.307355159234862 | validation: 0.3732211735131557]
	TIME [epoch: 41.5 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32085557395058334		[learning rate: 0.0067695]
	Learning Rate: 0.00676948
	LOSS [training: 0.32085557395058334 | validation: 0.3419080119080992]
	TIME [epoch: 41.5 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3410109239226391		[learning rate: 0.006762]
	Learning Rate: 0.00676199
	LOSS [training: 0.3410109239226391 | validation: 0.3503246105956702]
	TIME [epoch: 41.4 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32079708451360656		[learning rate: 0.0067545]
	Learning Rate: 0.00675449
	LOSS [training: 0.32079708451360656 | validation: 0.3704800036426006]
	TIME [epoch: 41.4 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.344458736446648		[learning rate: 0.006747]
	Learning Rate: 0.00674699
	LOSS [training: 0.344458736446648 | validation: 0.31275072246068625]
	TIME [epoch: 41.4 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3217500271319343		[learning rate: 0.0067395]
	Learning Rate: 0.00673949
	LOSS [training: 0.3217500271319343 | validation: 0.3849524618246015]
	TIME [epoch: 41.5 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3349195986536027		[learning rate: 0.006732]
	Learning Rate: 0.00673198
	LOSS [training: 0.3349195986536027 | validation: 0.3560793908313138]
	TIME [epoch: 41.5 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31627940140520316		[learning rate: 0.0067245]
	Learning Rate: 0.00672446
	LOSS [training: 0.31627940140520316 | validation: 0.3293583938142192]
	TIME [epoch: 41.5 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30443207736624667		[learning rate: 0.0067169]
	Learning Rate: 0.00671694
	LOSS [training: 0.30443207736624667 | validation: 0.32731995834217187]
	TIME [epoch: 41.5 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3091266486913692		[learning rate: 0.0067094]
	Learning Rate: 0.00670942
	LOSS [training: 0.3091266486913692 | validation: 0.44624233548450787]
	TIME [epoch: 41.5 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3261554593941335		[learning rate: 0.0067019]
	Learning Rate: 0.00670189
	LOSS [training: 0.3261554593941335 | validation: 0.29437046762668395]
	TIME [epoch: 41.4 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3126952554893474		[learning rate: 0.0066944]
	Learning Rate: 0.00669436
	LOSS [training: 0.3126952554893474 | validation: 0.29250567304120445]
	TIME [epoch: 41.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_785.pth
	Model improved!!!
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33895571843392563		[learning rate: 0.0066868]
	Learning Rate: 0.00668682
	LOSS [training: 0.33895571843392563 | validation: 0.3304076847792022]
	TIME [epoch: 41.5 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32623410038370626		[learning rate: 0.0066793]
	Learning Rate: 0.00667928
	LOSS [training: 0.32623410038370626 | validation: 0.34358504820851044]
	TIME [epoch: 41.5 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3342081915843143		[learning rate: 0.0066717]
	Learning Rate: 0.00667174
	LOSS [training: 0.3342081915843143 | validation: 0.36103073002381514]
	TIME [epoch: 41.5 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34046383639889827		[learning rate: 0.0066642]
	Learning Rate: 0.00666419
	LOSS [training: 0.34046383639889827 | validation: 0.39733380569703913]
	TIME [epoch: 41.5 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32950288065061895		[learning rate: 0.0066566]
	Learning Rate: 0.00665663
	LOSS [training: 0.32950288065061895 | validation: 0.2970534481936713]
	TIME [epoch: 41.5 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31614458066615453		[learning rate: 0.0066491]
	Learning Rate: 0.00664908
	LOSS [training: 0.31614458066615453 | validation: 0.3011981026719304]
	TIME [epoch: 41.5 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3090863241120681		[learning rate: 0.0066415]
	Learning Rate: 0.00664151
	LOSS [training: 0.3090863241120681 | validation: 0.3402524305689161]
	TIME [epoch: 41.5 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33399697160044245		[learning rate: 0.0066339]
	Learning Rate: 0.00663395
	LOSS [training: 0.33399697160044245 | validation: 0.35601395223584165]
	TIME [epoch: 41.5 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30994225840399203		[learning rate: 0.0066264]
	Learning Rate: 0.00662638
	LOSS [training: 0.30994225840399203 | validation: 0.3245142234482067]
	TIME [epoch: 41.5 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32004625548923143		[learning rate: 0.0066188]
	Learning Rate: 0.0066188
	LOSS [training: 0.32004625548923143 | validation: 0.3257934723795274]
	TIME [epoch: 41.5 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29925021258356965		[learning rate: 0.0066112]
	Learning Rate: 0.00661122
	LOSS [training: 0.29925021258356965 | validation: 0.3324935608406878]
	TIME [epoch: 41.5 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35340558442032366		[learning rate: 0.0066036]
	Learning Rate: 0.00660364
	LOSS [training: 0.35340558442032366 | validation: 0.33887117399838695]
	TIME [epoch: 41.5 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3216334779339714		[learning rate: 0.0065961]
	Learning Rate: 0.00659605
	LOSS [training: 0.3216334779339714 | validation: 0.30451340595183435]
	TIME [epoch: 41.5 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37042731545681085		[learning rate: 0.0065885]
	Learning Rate: 0.00658846
	LOSS [training: 0.37042731545681085 | validation: 0.3689094579456781]
	TIME [epoch: 41.5 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32482551568274154		[learning rate: 0.0065809]
	Learning Rate: 0.00658087
	LOSS [training: 0.32482551568274154 | validation: 0.3127946141311006]
	TIME [epoch: 41.5 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3107300572239265		[learning rate: 0.0065733]
	Learning Rate: 0.00657327
	LOSS [training: 0.3107300572239265 | validation: 0.30169063287863096]
	TIME [epoch: 41.5 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31014969362586364		[learning rate: 0.0065657]
	Learning Rate: 0.00656566
	LOSS [training: 0.31014969362586364 | validation: 0.30913390655630546]
	TIME [epoch: 41.5 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3124038838908581		[learning rate: 0.0065581]
	Learning Rate: 0.00655805
	LOSS [training: 0.3124038838908581 | validation: 0.301035906258467]
	TIME [epoch: 41.5 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32635199600764786		[learning rate: 0.0065504]
	Learning Rate: 0.00655044
	LOSS [training: 0.32635199600764786 | validation: 0.3190816567362534]
	TIME [epoch: 41.5 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3157246846292297		[learning rate: 0.0065428]
	Learning Rate: 0.00654283
	LOSS [training: 0.3157246846292297 | validation: 0.3439989448017491]
	TIME [epoch: 41.5 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3258218587116056		[learning rate: 0.0065352]
	Learning Rate: 0.00653521
	LOSS [training: 0.3258218587116056 | validation: 0.37414845639354555]
	TIME [epoch: 41.5 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32979558884031923		[learning rate: 0.0065276]
	Learning Rate: 0.00652759
	LOSS [training: 0.32979558884031923 | validation: 0.32044606120461294]
	TIME [epoch: 41.5 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31134932019883		[learning rate: 0.00652]
	Learning Rate: 0.00651996
	LOSS [training: 0.31134932019883 | validation: 0.32785197312864]
	TIME [epoch: 41.5 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32078281309629625		[learning rate: 0.0065123]
	Learning Rate: 0.00651233
	LOSS [training: 0.32078281309629625 | validation: 0.3337255878909402]
	TIME [epoch: 41.4 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2965721063273503		[learning rate: 0.0065047]
	Learning Rate: 0.00650469
	LOSS [training: 0.2965721063273503 | validation: 0.32598682341761526]
	TIME [epoch: 41.5 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31855831199142187		[learning rate: 0.0064971]
	Learning Rate: 0.00649705
	LOSS [training: 0.31855831199142187 | validation: 0.30887549878026943]
	TIME [epoch: 41.5 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3221353096256843		[learning rate: 0.0064894]
	Learning Rate: 0.00648941
	LOSS [training: 0.3221353096256843 | validation: 0.35898914490237527]
	TIME [epoch: 41.5 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31761467274376165		[learning rate: 0.0064818]
	Learning Rate: 0.00648176
	LOSS [training: 0.31761467274376165 | validation: 0.2976244843697518]
	TIME [epoch: 41.5 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31952190154128907		[learning rate: 0.0064741]
	Learning Rate: 0.00647411
	LOSS [training: 0.31952190154128907 | validation: 0.33296336448118724]
	TIME [epoch: 41.5 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2921061960867052		[learning rate: 0.0064665]
	Learning Rate: 0.00646646
	LOSS [training: 0.2921061960867052 | validation: 0.3485018757687711]
	TIME [epoch: 41.4 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3086190579160207		[learning rate: 0.0064588]
	Learning Rate: 0.0064588
	LOSS [training: 0.3086190579160207 | validation: 0.29027687323242557]
	TIME [epoch: 41.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_816.pth
	Model improved!!!
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34553160402659927		[learning rate: 0.0064511]
	Learning Rate: 0.00645114
	LOSS [training: 0.34553160402659927 | validation: 0.4982648439554238]
	TIME [epoch: 41.5 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3537548994890241		[learning rate: 0.0064435]
	Learning Rate: 0.00644348
	LOSS [training: 0.3537548994890241 | validation: 0.3593546824925007]
	TIME [epoch: 41.5 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3191638463662737		[learning rate: 0.0064358]
	Learning Rate: 0.00643581
	LOSS [training: 0.3191638463662737 | validation: 0.3234869753041626]
	TIME [epoch: 41.5 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3091439107481004		[learning rate: 0.0064281]
	Learning Rate: 0.00642813
	LOSS [training: 0.3091439107481004 | validation: 0.2985521175982636]
	TIME [epoch: 41.5 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31530613902951155		[learning rate: 0.0064205]
	Learning Rate: 0.00642046
	LOSS [training: 0.31530613902951155 | validation: 0.31644657035373025]
	TIME [epoch: 41.5 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3434460772078264		[learning rate: 0.0064128]
	Learning Rate: 0.00641278
	LOSS [training: 0.3434460772078264 | validation: 0.3408341465038317]
	TIME [epoch: 41.5 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31052937198171454		[learning rate: 0.0064051]
	Learning Rate: 0.00640509
	LOSS [training: 0.31052937198171454 | validation: 0.33085633646930496]
	TIME [epoch: 41.5 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30912164535880016		[learning rate: 0.0063974]
	Learning Rate: 0.00639741
	LOSS [training: 0.30912164535880016 | validation: 0.29338064512338724]
	TIME [epoch: 41.5 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3195514007275059		[learning rate: 0.0063897]
	Learning Rate: 0.00638972
	LOSS [training: 0.3195514007275059 | validation: 0.30767382096295737]
	TIME [epoch: 41.5 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3314868549897657		[learning rate: 0.006382]
	Learning Rate: 0.00638202
	LOSS [training: 0.3314868549897657 | validation: 0.3169402596785318]
	TIME [epoch: 41.5 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33021579299856896		[learning rate: 0.0063743]
	Learning Rate: 0.00637432
	LOSS [training: 0.33021579299856896 | validation: 0.3520466472653835]
	TIME [epoch: 41.5 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3324694955201569		[learning rate: 0.0063666]
	Learning Rate: 0.00636662
	LOSS [training: 0.3324694955201569 | validation: 0.3015974741306125]
	TIME [epoch: 41.5 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32166998882104303		[learning rate: 0.0063589]
	Learning Rate: 0.00635892
	LOSS [training: 0.32166998882104303 | validation: 0.3118477006906658]
	TIME [epoch: 41.5 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30757932593014403		[learning rate: 0.0063512]
	Learning Rate: 0.00635121
	LOSS [training: 0.30757932593014403 | validation: 0.288338457284801]
	TIME [epoch: 41.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_830.pth
	Model improved!!!
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29033703347617695		[learning rate: 0.0063435]
	Learning Rate: 0.0063435
	LOSS [training: 0.29033703347617695 | validation: 0.47527463084743016]
	TIME [epoch: 41.5 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38208299961758513		[learning rate: 0.0063358]
	Learning Rate: 0.00633578
	LOSS [training: 0.38208299961758513 | validation: 0.3221641397766345]
	TIME [epoch: 41.5 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.328196196620151		[learning rate: 0.0063281]
	Learning Rate: 0.00632806
	LOSS [training: 0.328196196620151 | validation: 0.37025890232242264]
	TIME [epoch: 41.5 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31851548744429004		[learning rate: 0.0063203]
	Learning Rate: 0.00632034
	LOSS [training: 0.31851548744429004 | validation: 0.3383967115463143]
	TIME [epoch: 41.4 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3127040622869724		[learning rate: 0.0063126]
	Learning Rate: 0.00631262
	LOSS [training: 0.3127040622869724 | validation: 0.3431841992117615]
	TIME [epoch: 41.5 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3077471674692815		[learning rate: 0.0063049]
	Learning Rate: 0.00630489
	LOSS [training: 0.3077471674692815 | validation: 0.3103438778020151]
	TIME [epoch: 41.5 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3071906359915718		[learning rate: 0.0062972]
	Learning Rate: 0.00629716
	LOSS [training: 0.3071906359915718 | validation: 0.35030819707757993]
	TIME [epoch: 41.5 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32236400870254345		[learning rate: 0.0062894]
	Learning Rate: 0.00628942
	LOSS [training: 0.32236400870254345 | validation: 0.31819949592782293]
	TIME [epoch: 41.5 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.295459778610248		[learning rate: 0.0062817]
	Learning Rate: 0.00628168
	LOSS [training: 0.295459778610248 | validation: 0.30437137392101016]
	TIME [epoch: 41.5 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30235990806443386		[learning rate: 0.0062739]
	Learning Rate: 0.00627394
	LOSS [training: 0.30235990806443386 | validation: 0.4288926137052637]
	TIME [epoch: 41.5 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32501085514243216		[learning rate: 0.0062662]
	Learning Rate: 0.0062662
	LOSS [training: 0.32501085514243216 | validation: 0.3761242758759237]
	TIME [epoch: 41.5 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3212676530394913		[learning rate: 0.0062584]
	Learning Rate: 0.00625845
	LOSS [training: 0.3212676530394913 | validation: 0.32535250339323774]
	TIME [epoch: 41.5 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31095832049357086		[learning rate: 0.0062507]
	Learning Rate: 0.0062507
	LOSS [training: 0.31095832049357086 | validation: 0.2946781282382494]
	TIME [epoch: 41.5 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3215336248137729		[learning rate: 0.0062429]
	Learning Rate: 0.00624294
	LOSS [training: 0.3215336248137729 | validation: 0.32405997319771895]
	TIME [epoch: 41.5 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3049732861886066		[learning rate: 0.0062352]
	Learning Rate: 0.00623518
	LOSS [training: 0.3049732861886066 | validation: 0.3122517588018462]
	TIME [epoch: 41.5 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.292683105944306		[learning rate: 0.0062274]
	Learning Rate: 0.00622742
	LOSS [training: 0.292683105944306 | validation: 0.3051027026319721]
	TIME [epoch: 41.5 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.321820929653224		[learning rate: 0.0062197]
	Learning Rate: 0.00621966
	LOSS [training: 0.321820929653224 | validation: 0.31553760501899486]
	TIME [epoch: 41.5 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3027027056694602		[learning rate: 0.0062119]
	Learning Rate: 0.00621189
	LOSS [training: 0.3027027056694602 | validation: 0.33536359209558086]
	TIME [epoch: 41.5 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29611942242634803		[learning rate: 0.0062041]
	Learning Rate: 0.00620412
	LOSS [training: 0.29611942242634803 | validation: 0.3411107680259842]
	TIME [epoch: 41.5 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3202911713979977		[learning rate: 0.0061963]
	Learning Rate: 0.00619635
	LOSS [training: 0.3202911713979977 | validation: 0.33700691398798965]
	TIME [epoch: 41.5 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32698940645449365		[learning rate: 0.0061886]
	Learning Rate: 0.00618857
	LOSS [training: 0.32698940645449365 | validation: 0.3559853480251559]
	TIME [epoch: 41.5 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30403623526860146		[learning rate: 0.0061808]
	Learning Rate: 0.00618079
	LOSS [training: 0.30403623526860146 | validation: 0.34727761446787486]
	TIME [epoch: 41.5 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33624664114510294		[learning rate: 0.006173]
	Learning Rate: 0.00617301
	LOSS [training: 0.33624664114510294 | validation: 0.30200240869650485]
	TIME [epoch: 41.5 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30683198813608886		[learning rate: 0.0061652]
	Learning Rate: 0.00616522
	LOSS [training: 0.30683198813608886 | validation: 0.3144263728435934]
	TIME [epoch: 41.5 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3142800400832868		[learning rate: 0.0061574]
	Learning Rate: 0.00615743
	LOSS [training: 0.3142800400832868 | validation: 0.28863997139724173]
	TIME [epoch: 41.5 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29234517111384706		[learning rate: 0.0061496]
	Learning Rate: 0.00614964
	LOSS [training: 0.29234517111384706 | validation: 0.33503082216076385]
	TIME [epoch: 41.5 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32206049170434903		[learning rate: 0.0061418]
	Learning Rate: 0.00614184
	LOSS [training: 0.32206049170434903 | validation: 0.3102173186411804]
	TIME [epoch: 41.5 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3005043416504859		[learning rate: 0.006134]
	Learning Rate: 0.00613405
	LOSS [training: 0.3005043416504859 | validation: 0.31429367376339]
	TIME [epoch: 41.5 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33624837825222165		[learning rate: 0.0061262]
	Learning Rate: 0.00612625
	LOSS [training: 0.33624837825222165 | validation: 0.33751535657452875]
	TIME [epoch: 41.5 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30637595350811353		[learning rate: 0.0061184]
	Learning Rate: 0.00611844
	LOSS [training: 0.30637595350811353 | validation: 0.3235182950038207]
	TIME [epoch: 41.5 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30717594741225535		[learning rate: 0.0061106]
	Learning Rate: 0.00611064
	LOSS [training: 0.30717594741225535 | validation: 0.31259210627467127]
	TIME [epoch: 41.5 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30753975130921285		[learning rate: 0.0061028]
	Learning Rate: 0.00610283
	LOSS [training: 0.30753975130921285 | validation: 0.3256352024893504]
	TIME [epoch: 41.5 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.305669234593002		[learning rate: 0.006095]
	Learning Rate: 0.00609502
	LOSS [training: 0.305669234593002 | validation: 0.30817379844831494]
	TIME [epoch: 41.5 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30571997165676557		[learning rate: 0.0060872]
	Learning Rate: 0.0060872
	LOSS [training: 0.30571997165676557 | validation: 0.3127300094442112]
	TIME [epoch: 41.5 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2929300367431157		[learning rate: 0.0060794]
	Learning Rate: 0.00607938
	LOSS [training: 0.2929300367431157 | validation: 0.31196109570462616]
	TIME [epoch: 41.4 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30987853208976257		[learning rate: 0.0060716]
	Learning Rate: 0.00607156
	LOSS [training: 0.30987853208976257 | validation: 0.3102828283416682]
	TIME [epoch: 41.5 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3136312962553356		[learning rate: 0.0060637]
	Learning Rate: 0.00606374
	LOSS [training: 0.3136312962553356 | validation: 0.34599754519048775]
	TIME [epoch: 41.5 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30048131515934706		[learning rate: 0.0060559]
	Learning Rate: 0.00605592
	LOSS [training: 0.30048131515934706 | validation: 0.3212010195695987]
	TIME [epoch: 41.5 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.300151560958436		[learning rate: 0.0060481]
	Learning Rate: 0.00604809
	LOSS [training: 0.300151560958436 | validation: 0.28701622172548646]
	TIME [epoch: 41.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_869.pth
	Model improved!!!
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2997449134527063		[learning rate: 0.0060403]
	Learning Rate: 0.00604026
	LOSS [training: 0.2997449134527063 | validation: 0.3076932448494486]
	TIME [epoch: 41.5 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2965602087563699		[learning rate: 0.0060324]
	Learning Rate: 0.00603242
	LOSS [training: 0.2965602087563699 | validation: 0.3229225275351555]
	TIME [epoch: 41.5 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3108272423785845		[learning rate: 0.0060246]
	Learning Rate: 0.00602459
	LOSS [training: 0.3108272423785845 | validation: 0.3277068552523119]
	TIME [epoch: 41.5 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3399779007358255		[learning rate: 0.0060167]
	Learning Rate: 0.00601675
	LOSS [training: 0.3399779007358255 | validation: 0.3096027127595548]
	TIME [epoch: 41.5 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3041849210737994		[learning rate: 0.0060089]
	Learning Rate: 0.0060089
	LOSS [training: 0.3041849210737994 | validation: 0.3198214003362322]
	TIME [epoch: 41.5 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30769042053728185		[learning rate: 0.0060011]
	Learning Rate: 0.00600106
	LOSS [training: 0.30769042053728185 | validation: 0.30195710236432294]
	TIME [epoch: 41.5 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31590384238680436		[learning rate: 0.0059932]
	Learning Rate: 0.00599321
	LOSS [training: 0.31590384238680436 | validation: 0.37790219661713065]
	TIME [epoch: 41.5 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2994971579516019		[learning rate: 0.0059854]
	Learning Rate: 0.00598536
	LOSS [training: 0.2994971579516019 | validation: 0.289227752188467]
	TIME [epoch: 41.5 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36225397102860046		[learning rate: 0.0059775]
	Learning Rate: 0.00597751
	LOSS [training: 0.36225397102860046 | validation: 0.38665723523925766]
	TIME [epoch: 41.6 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40643559978293103		[learning rate: 0.0059697]
	Learning Rate: 0.00596966
	LOSS [training: 0.40643559978293103 | validation: 0.7962558686685428]
	TIME [epoch: 41.5 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1298535785919068		[learning rate: 0.0059618]
	Learning Rate: 0.0059618
	LOSS [training: 1.1298535785919068 | validation: 1.698725686778634]
	TIME [epoch: 41.5 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5008157402762752		[learning rate: 0.0059539]
	Learning Rate: 0.00595394
	LOSS [training: 1.5008157402762752 | validation: 1.9215347180044575]
	TIME [epoch: 41.6 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.50733305287091		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 1.50733305287091 | validation: 1.6680446969331446]
	TIME [epoch: 41.5 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3223547060976668		[learning rate: 0.0059382]
	Learning Rate: 0.00593822
	LOSS [training: 1.3223547060976668 | validation: 1.4949519830854845]
	TIME [epoch: 41.5 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.57141930303132		[learning rate: 0.0059304]
	Learning Rate: 0.00593035
	LOSS [training: 1.57141930303132 | validation: 1.5571915527877986]
	TIME [epoch: 41.5 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3750162610608925		[learning rate: 0.0059225]
	Learning Rate: 0.00592248
	LOSS [training: 1.3750162610608925 | validation: 1.2178754751945537]
	TIME [epoch: 41.5 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0202567446807054		[learning rate: 0.0059146]
	Learning Rate: 0.00591461
	LOSS [training: 1.0202567446807054 | validation: 0.9265317433382565]
	TIME [epoch: 41.5 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8765268557202852		[learning rate: 0.0059067]
	Learning Rate: 0.00590674
	LOSS [training: 0.8765268557202852 | validation: 0.803580430277743]
	TIME [epoch: 41.5 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.77938622845589		[learning rate: 0.0058989]
	Learning Rate: 0.00589886
	LOSS [training: 0.77938622845589 | validation: 0.6941892508819927]
	TIME [epoch: 41.5 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7177963772415038		[learning rate: 0.005891]
	Learning Rate: 0.00589098
	LOSS [training: 0.7177963772415038 | validation: 1.345360932986033]
	TIME [epoch: 41.5 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2340079387234848		[learning rate: 0.0058831]
	Learning Rate: 0.0058831
	LOSS [training: 1.2340079387234848 | validation: 1.3859161814047833]
	TIME [epoch: 41.5 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2336311867119627		[learning rate: 0.0058752]
	Learning Rate: 0.00587522
	LOSS [training: 1.2336311867119627 | validation: 1.2844601033757883]
	TIME [epoch: 41.5 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2381300315274637		[learning rate: 0.0058673]
	Learning Rate: 0.00586734
	LOSS [training: 1.2381300315274637 | validation: 1.440853663869655]
	TIME [epoch: 41.5 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2380133521078274		[learning rate: 0.0058594]
	Learning Rate: 0.00585945
	LOSS [training: 1.2380133521078274 | validation: 1.1689859650508612]
	TIME [epoch: 41.5 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0715857464829912		[learning rate: 0.0058516]
	Learning Rate: 0.00585156
	LOSS [training: 1.0715857464829912 | validation: 1.3480500456758644]
	TIME [epoch: 41.5 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.293228759781577		[learning rate: 0.0058437]
	Learning Rate: 0.00584367
	LOSS [training: 1.293228759781577 | validation: 1.436551429191098]
	TIME [epoch: 41.5 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1867810594200605		[learning rate: 0.0058358]
	Learning Rate: 0.00583577
	LOSS [training: 1.1867810594200605 | validation: 1.079482211944013]
	TIME [epoch: 41.5 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0543843510041153		[learning rate: 0.0058279]
	Learning Rate: 0.00582788
	LOSS [training: 1.0543843510041153 | validation: 1.034063281439685]
	TIME [epoch: 41.5 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9928182011209125		[learning rate: 0.00582]
	Learning Rate: 0.00581998
	LOSS [training: 0.9928182011209125 | validation: 0.9602640188036471]
	TIME [epoch: 41.5 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8482985253513302		[learning rate: 0.0058121]
	Learning Rate: 0.00581208
	LOSS [training: 0.8482985253513302 | validation: 0.950647730822636]
	TIME [epoch: 41.5 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7662424067523973		[learning rate: 0.0058042]
	Learning Rate: 0.00580418
	LOSS [training: 0.7662424067523973 | validation: 0.9146262163165368]
	TIME [epoch: 41.5 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7192163566465664		[learning rate: 0.0057963]
	Learning Rate: 0.00579627
	LOSS [training: 0.7192163566465664 | validation: 0.8916921751582148]
	TIME [epoch: 41.6 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6577161758806489		[learning rate: 0.0057884]
	Learning Rate: 0.00578837
	LOSS [training: 0.6577161758806489 | validation: 0.7599541624691741]
	TIME [epoch: 41.6 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.586855748346767		[learning rate: 0.0057805]
	Learning Rate: 0.00578046
	LOSS [training: 0.586855748346767 | validation: 0.6078902667766168]
	TIME [epoch: 41.5 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5666897788896841		[learning rate: 0.0057725]
	Learning Rate: 0.00577255
	LOSS [training: 0.5666897788896841 | validation: 0.5992491837855186]
	TIME [epoch: 41.5 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.544302209521839		[learning rate: 0.0057646]
	Learning Rate: 0.00576464
	LOSS [training: 0.544302209521839 | validation: 0.5621277329987967]
	TIME [epoch: 41.6 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4909806563220803		[learning rate: 0.0057567]
	Learning Rate: 0.00575672
	LOSS [training: 0.4909806563220803 | validation: 0.5639355642230739]
	TIME [epoch: 41.6 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4801890850033521		[learning rate: 0.0057488]
	Learning Rate: 0.00574881
	LOSS [training: 0.4801890850033521 | validation: 0.52499398595662]
	TIME [epoch: 41.6 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47735907556465484		[learning rate: 0.0057409]
	Learning Rate: 0.00574089
	LOSS [training: 0.47735907556465484 | validation: 0.6804044943696913]
	TIME [epoch: 41.6 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6480564393997543		[learning rate: 0.005733]
	Learning Rate: 0.00573297
	LOSS [training: 0.6480564393997543 | validation: 0.8113971811156467]
	TIME [epoch: 41.5 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6611893732307318		[learning rate: 0.005725]
	Learning Rate: 0.00572505
	LOSS [training: 0.6611893732307318 | validation: 0.670100092674804]
	TIME [epoch: 41.5 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5770204180167746		[learning rate: 0.0057171]
	Learning Rate: 0.00571712
	LOSS [training: 0.5770204180167746 | validation: 0.7098699119523986]
	TIME [epoch: 41.6 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5352227664158017		[learning rate: 0.0057092]
	Learning Rate: 0.0057092
	LOSS [training: 0.5352227664158017 | validation: 0.5406436598749509]
	TIME [epoch: 41.6 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4753263071176396		[learning rate: 0.0057013]
	Learning Rate: 0.00570127
	LOSS [training: 0.4753263071176396 | validation: 0.4955826724529515]
	TIME [epoch: 41.6 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.473846152497437		[learning rate: 0.0056933]
	Learning Rate: 0.00569334
	LOSS [training: 0.473846152497437 | validation: 0.48767973219127475]
	TIME [epoch: 41.6 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4309171970833097		[learning rate: 0.0056854]
	Learning Rate: 0.00568541
	LOSS [training: 0.4309171970833097 | validation: 0.4824614214117797]
	TIME [epoch: 41.6 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4161176812288794		[learning rate: 0.0056775]
	Learning Rate: 0.00567748
	LOSS [training: 0.4161176812288794 | validation: 0.44893575035382405]
	TIME [epoch: 41.5 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41237425354263074		[learning rate: 0.0056695]
	Learning Rate: 0.00566954
	LOSS [training: 0.41237425354263074 | validation: 0.4382174816655444]
	TIME [epoch: 41.6 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4195163201293931		[learning rate: 0.0056616]
	Learning Rate: 0.00566161
	LOSS [training: 0.4195163201293931 | validation: 0.5887477014306458]
	TIME [epoch: 41.6 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5450957031311194		[learning rate: 0.0056537]
	Learning Rate: 0.00565367
	LOSS [training: 0.5450957031311194 | validation: 0.4317927610095923]
	TIME [epoch: 41.6 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4965206846826671		[learning rate: 0.0056457]
	Learning Rate: 0.00564573
	LOSS [training: 0.4965206846826671 | validation: 0.47713876937785604]
	TIME [epoch: 41.6 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4392353742776917		[learning rate: 0.0056378]
	Learning Rate: 0.00563779
	LOSS [training: 0.4392353742776917 | validation: 0.4100203481513832]
	TIME [epoch: 41.5 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37912906508768435		[learning rate: 0.0056298]
	Learning Rate: 0.00562985
	LOSS [training: 0.37912906508768435 | validation: 0.39496292763684304]
	TIME [epoch: 41.5 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42781989665057973		[learning rate: 0.0056219]
	Learning Rate: 0.0056219
	LOSS [training: 0.42781989665057973 | validation: 0.37318918614887064]
	TIME [epoch: 41.5 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.364914397971534		[learning rate: 0.005614]
	Learning Rate: 0.00561396
	LOSS [training: 0.364914397971534 | validation: 0.396688056826183]
	TIME [epoch: 41.6 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3600152882673304		[learning rate: 0.005606]
	Learning Rate: 0.00560601
	LOSS [training: 0.3600152882673304 | validation: 0.36728247209415915]
	TIME [epoch: 41.6 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35996232569923803		[learning rate: 0.0055981]
	Learning Rate: 0.00559806
	LOSS [training: 0.35996232569923803 | validation: 0.3435785220837357]
	TIME [epoch: 41.6 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3626702231463867		[learning rate: 0.0055901]
	Learning Rate: 0.00559011
	LOSS [training: 0.3626702231463867 | validation: 0.33268666270518854]
	TIME [epoch: 41.6 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44724872671587956		[learning rate: 0.0055822]
	Learning Rate: 0.00558216
	LOSS [training: 0.44724872671587956 | validation: 0.3983784737083439]
	TIME [epoch: 41.6 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41264941154454304		[learning rate: 0.0055742]
	Learning Rate: 0.00557421
	LOSS [training: 0.41264941154454304 | validation: 0.35480569846667476]
	TIME [epoch: 41.5 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3740985935390556		[learning rate: 0.0055663]
	Learning Rate: 0.00556625
	LOSS [training: 0.3740985935390556 | validation: 0.34300666919368333]
	TIME [epoch: 41.5 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3556835185743342		[learning rate: 0.0055583]
	Learning Rate: 0.0055583
	LOSS [training: 0.3556835185743342 | validation: 0.3404135624002317]
	TIME [epoch: 41.5 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3358394365309093		[learning rate: 0.0055503]
	Learning Rate: 0.00555034
	LOSS [training: 0.3358394365309093 | validation: 0.3379526791981016]
	TIME [epoch: 41.5 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3321735878161273		[learning rate: 0.0055424]
	Learning Rate: 0.00554238
	LOSS [training: 0.3321735878161273 | validation: 0.3133769829297468]
	TIME [epoch: 41.5 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31443916186820386		[learning rate: 0.0055344]
	Learning Rate: 0.00553442
	LOSS [training: 0.31443916186820386 | validation: 0.344705514362]
	TIME [epoch: 41.6 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3210149470463296		[learning rate: 0.0055265]
	Learning Rate: 0.00552646
	LOSS [training: 0.3210149470463296 | validation: 0.31350681621665333]
	TIME [epoch: 41.5 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3119376762563174		[learning rate: 0.0055185]
	Learning Rate: 0.00551849
	LOSS [training: 0.3119376762563174 | validation: 0.3240325853018071]
	TIME [epoch: 41.5 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30562326690503044		[learning rate: 0.0055105]
	Learning Rate: 0.00551053
	LOSS [training: 0.30562326690503044 | validation: 0.3751519292978981]
	TIME [epoch: 41.5 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32872176938141473		[learning rate: 0.0055026]
	Learning Rate: 0.00550256
	LOSS [training: 0.32872176938141473 | validation: 0.30380288847772396]
	TIME [epoch: 41.5 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3025277313874685		[learning rate: 0.0054946]
	Learning Rate: 0.0054946
	LOSS [training: 0.3025277313874685 | validation: 0.33348456377193647]
	TIME [epoch: 41.5 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30662802524948296		[learning rate: 0.0054866]
	Learning Rate: 0.00548663
	LOSS [training: 0.30662802524948296 | validation: 0.36631910229634834]
	TIME [epoch: 41.5 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30514864422812954		[learning rate: 0.0054787]
	Learning Rate: 0.00547866
	LOSS [training: 0.30514864422812954 | validation: 0.31943690304986755]
	TIME [epoch: 41.5 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3056906023304485		[learning rate: 0.0054707]
	Learning Rate: 0.00547069
	LOSS [training: 0.3056906023304485 | validation: 0.2869873810306923]
	TIME [epoch: 41.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_942.pth
	Model improved!!!
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3063924197482337		[learning rate: 0.0054627]
	Learning Rate: 0.00546271
	LOSS [training: 0.3063924197482337 | validation: 0.3305753250171327]
	TIME [epoch: 41.5 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3029128194026968		[learning rate: 0.0054547]
	Learning Rate: 0.00545474
	LOSS [training: 0.3029128194026968 | validation: 0.2749391981992875]
	TIME [epoch: 41.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_944.pth
	Model improved!!!
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31028113223518156		[learning rate: 0.0054468]
	Learning Rate: 0.00544677
	LOSS [training: 0.31028113223518156 | validation: 0.3423443908773049]
	TIME [epoch: 41.5 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3196308140195296		[learning rate: 0.0054388]
	Learning Rate: 0.00543879
	LOSS [training: 0.3196308140195296 | validation: 0.37857117367900517]
	TIME [epoch: 41.5 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3138096081736403		[learning rate: 0.0054308]
	Learning Rate: 0.00543082
	LOSS [training: 0.3138096081736403 | validation: 0.3313590022035648]
	TIME [epoch: 41.5 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29401176464013906		[learning rate: 0.0054228]
	Learning Rate: 0.00542284
	LOSS [training: 0.29401176464013906 | validation: 0.32051327567553434]
	TIME [epoch: 41.4 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3124141474361406		[learning rate: 0.0054149]
	Learning Rate: 0.00541486
	LOSS [training: 0.3124141474361406 | validation: 0.32536637751484077]
	TIME [epoch: 41.4 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30135967758677706		[learning rate: 0.0054069]
	Learning Rate: 0.00540688
	LOSS [training: 0.30135967758677706 | validation: 0.3230514850741031]
	TIME [epoch: 41.5 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30738647923267315		[learning rate: 0.0053989]
	Learning Rate: 0.0053989
	LOSS [training: 0.30738647923267315 | validation: 0.3196194221255415]
	TIME [epoch: 41.5 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29251701232582383		[learning rate: 0.0053909]
	Learning Rate: 0.00539092
	LOSS [training: 0.29251701232582383 | validation: 0.3049958713511882]
	TIME [epoch: 41.5 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2920203927409304		[learning rate: 0.0053829]
	Learning Rate: 0.00538293
	LOSS [training: 0.2920203927409304 | validation: 0.32685666231529614]
	TIME [epoch: 41.5 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3320246263907542		[learning rate: 0.0053749]
	Learning Rate: 0.00537495
	LOSS [training: 0.3320246263907542 | validation: 0.3159449652232138]
	TIME [epoch: 41.5 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.298060231480698		[learning rate: 0.005367]
	Learning Rate: 0.00536697
	LOSS [training: 0.298060231480698 | validation: 0.2892463425814522]
	TIME [epoch: 41.5 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31102626235999714		[learning rate: 0.005359]
	Learning Rate: 0.00535898
	LOSS [training: 0.31102626235999714 | validation: 0.29691880081713334]
	TIME [epoch: 41.5 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.292502622871765		[learning rate: 0.005351]
	Learning Rate: 0.00535099
	LOSS [training: 0.292502622871765 | validation: 0.33931960084986734]
	TIME [epoch: 41.5 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2958833018790063		[learning rate: 0.005343]
	Learning Rate: 0.00534301
	LOSS [training: 0.2958833018790063 | validation: 0.35768113737190893]
	TIME [epoch: 41.5 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.317401706656637		[learning rate: 0.005335]
	Learning Rate: 0.00533502
	LOSS [training: 0.317401706656637 | validation: 0.32860511533227477]
	TIME [epoch: 41.5 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31969374822500063		[learning rate: 0.005327]
	Learning Rate: 0.00532703
	LOSS [training: 0.31969374822500063 | validation: 0.3087963281815381]
	TIME [epoch: 41.5 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3008854646817984		[learning rate: 0.005319]
	Learning Rate: 0.00531904
	LOSS [training: 0.3008854646817984 | validation: 0.3086956530062833]
	TIME [epoch: 41.5 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2948710498949228		[learning rate: 0.005311]
	Learning Rate: 0.00531105
	LOSS [training: 0.2948710498949228 | validation: 0.3202397480160715]
	TIME [epoch: 41.5 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2968002718219638		[learning rate: 0.0053031]
	Learning Rate: 0.00530306
	LOSS [training: 0.2968002718219638 | validation: 0.29297796219792205]
	TIME [epoch: 41.5 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2844751375570851		[learning rate: 0.0052951]
	Learning Rate: 0.00529506
	LOSS [training: 0.2844751375570851 | validation: 0.3322711172066626]
	TIME [epoch: 41.5 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3176339238194439		[learning rate: 0.0052871]
	Learning Rate: 0.00528707
	LOSS [training: 0.3176339238194439 | validation: 0.2876606358799334]
	TIME [epoch: 41.5 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2623566071743064		[learning rate: 0.0052791]
	Learning Rate: 0.00527908
	LOSS [training: 0.2623566071743064 | validation: 0.3049488813723388]
	TIME [epoch: 41.5 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.322185328137486		[learning rate: 0.0052711]
	Learning Rate: 0.00527108
	LOSS [training: 0.322185328137486 | validation: 0.3236869956038544]
	TIME [epoch: 41.5 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31510399939401956		[learning rate: 0.0052631]
	Learning Rate: 0.00526309
	LOSS [training: 0.31510399939401956 | validation: 0.3705327494752588]
	TIME [epoch: 41.5 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3140635285321485		[learning rate: 0.0052551]
	Learning Rate: 0.00525509
	LOSS [training: 0.3140635285321485 | validation: 0.32720796812300973]
	TIME [epoch: 41.5 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29690779823900615		[learning rate: 0.0052471]
	Learning Rate: 0.00524709
	LOSS [training: 0.29690779823900615 | validation: 0.3149496269075378]
	TIME [epoch: 41.5 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29952998949780313		[learning rate: 0.0052391]
	Learning Rate: 0.0052391
	LOSS [training: 0.29952998949780313 | validation: 0.27867525863542947]
	TIME [epoch: 41.5 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29406628903047893		[learning rate: 0.0052311]
	Learning Rate: 0.0052311
	LOSS [training: 0.29406628903047893 | validation: 0.32219264532310904]
	TIME [epoch: 41.5 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32762286463580026		[learning rate: 0.0052231]
	Learning Rate: 0.0052231
	LOSS [training: 0.32762286463580026 | validation: 0.27307045753333054]
	TIME [epoch: 41.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_973.pth
	Model improved!!!
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2919559043993538		[learning rate: 0.0052151]
	Learning Rate: 0.0052151
	LOSS [training: 0.2919559043993538 | validation: 0.29109971592437767]
	TIME [epoch: 41.4 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2935199116315072		[learning rate: 0.0052071]
	Learning Rate: 0.0052071
	LOSS [training: 0.2935199116315072 | validation: 0.28594268270907763]
	TIME [epoch: 41.5 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2921315566776259		[learning rate: 0.0051991]
	Learning Rate: 0.0051991
	LOSS [training: 0.2921315566776259 | validation: 0.30462216147100196]
	TIME [epoch: 41.5 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2902671864366648		[learning rate: 0.0051911]
	Learning Rate: 0.0051911
	LOSS [training: 0.2902671864366648 | validation: 0.2992418068385323]
	TIME [epoch: 41.5 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30713990415063697		[learning rate: 0.0051831]
	Learning Rate: 0.0051831
	LOSS [training: 0.30713990415063697 | validation: 0.29681110274364625]
	TIME [epoch: 41.5 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2989433722766676		[learning rate: 0.0051751]
	Learning Rate: 0.0051751
	LOSS [training: 0.2989433722766676 | validation: 0.2981357465958592]
	TIME [epoch: 41.5 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30775741085993996		[learning rate: 0.0051671]
	Learning Rate: 0.0051671
	LOSS [training: 0.30775741085993996 | validation: 0.301574734597543]
	TIME [epoch: 41.4 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29312720075351945		[learning rate: 0.0051591]
	Learning Rate: 0.0051591
	LOSS [training: 0.29312720075351945 | validation: 0.3053591645337458]
	TIME [epoch: 41.4 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2947253811847799		[learning rate: 0.0051511]
	Learning Rate: 0.00515109
	LOSS [training: 0.2947253811847799 | validation: 0.310170808592104]
	TIME [epoch: 41.5 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27880219008098384		[learning rate: 0.0051431]
	Learning Rate: 0.00514309
	LOSS [training: 0.27880219008098384 | validation: 0.33111450297650025]
	TIME [epoch: 41.5 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29737260490548273		[learning rate: 0.0051351]
	Learning Rate: 0.00513509
	LOSS [training: 0.29737260490548273 | validation: 0.32516112345226367]
	TIME [epoch: 41.5 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2669262224317968		[learning rate: 0.0051271]
	Learning Rate: 0.00512708
	LOSS [training: 0.2669262224317968 | validation: 0.2643228465333077]
	TIME [epoch: 41.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_985.pth
	Model improved!!!
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3062538451789878		[learning rate: 0.0051191]
	Learning Rate: 0.00511908
	LOSS [training: 0.3062538451789878 | validation: 0.32468168776741574]
	TIME [epoch: 41.5 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3023529919469825		[learning rate: 0.0051111]
	Learning Rate: 0.00511108
	LOSS [training: 0.3023529919469825 | validation: 0.28038313004061666]
	TIME [epoch: 41.5 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29810987202306505		[learning rate: 0.0051031]
	Learning Rate: 0.00510307
	LOSS [training: 0.29810987202306505 | validation: 0.30607456595441374]
	TIME [epoch: 41.5 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28985298353560873		[learning rate: 0.0050951]
	Learning Rate: 0.00509507
	LOSS [training: 0.28985298353560873 | validation: 0.29137941418751173]
	TIME [epoch: 41.5 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29147398216141496		[learning rate: 0.0050871]
	Learning Rate: 0.00508706
	LOSS [training: 0.29147398216141496 | validation: 0.28725286439482106]
	TIME [epoch: 41.5 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29549494918749114		[learning rate: 0.0050791]
	Learning Rate: 0.00507906
	LOSS [training: 0.29549494918749114 | validation: 0.30755198346991774]
	TIME [epoch: 41.5 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3113264246059097		[learning rate: 0.005071]
	Learning Rate: 0.00507105
	LOSS [training: 0.3113264246059097 | validation: 0.2909123024605691]
	TIME [epoch: 41.5 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2881675081342901		[learning rate: 0.005063]
	Learning Rate: 0.00506304
	LOSS [training: 0.2881675081342901 | validation: 0.2954322757120962]
	TIME [epoch: 41.5 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2890046339423948		[learning rate: 0.005055]
	Learning Rate: 0.00505504
	LOSS [training: 0.2890046339423948 | validation: 0.29367228882972896]
	TIME [epoch: 41.5 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30240818947677217		[learning rate: 0.005047]
	Learning Rate: 0.00504703
	LOSS [training: 0.30240818947677217 | validation: 0.2923670084457719]
	TIME [epoch: 41.5 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31822743384469326		[learning rate: 0.005039]
	Learning Rate: 0.00503903
	LOSS [training: 0.31822743384469326 | validation: 0.381336311833903]
	TIME [epoch: 41.5 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3135266548897867		[learning rate: 0.005031]
	Learning Rate: 0.00503102
	LOSS [training: 0.3135266548897867 | validation: 0.31570285465880726]
	TIME [epoch: 41.5 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30714719901023974		[learning rate: 0.005023]
	Learning Rate: 0.00502301
	LOSS [training: 0.30714719901023974 | validation: 0.2722388803408382]
	TIME [epoch: 41.5 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2770731511827498		[learning rate: 0.005015]
	Learning Rate: 0.00501501
	LOSS [training: 0.2770731511827498 | validation: 0.2858793120426026]
	TIME [epoch: 41.5 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2922714769142615		[learning rate: 0.005007]
	Learning Rate: 0.005007
	LOSS [training: 0.2922714769142615 | validation: 0.2912186192508765]
	TIME [epoch: 41.5 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29117743555014036		[learning rate: 0.004999]
	Learning Rate: 0.004999
	LOSS [training: 0.29117743555014036 | validation: 0.2904509601973714]
	TIME [epoch: 182 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2698506510969483		[learning rate: 0.004991]
	Learning Rate: 0.00499099
	LOSS [training: 0.2698506510969483 | validation: 0.26493292606204344]
	TIME [epoch: 88.2 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28775900229983353		[learning rate: 0.004983]
	Learning Rate: 0.00498298
	LOSS [training: 0.28775900229983353 | validation: 0.30345252479530493]
	TIME [epoch: 88.1 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30079677501556507		[learning rate: 0.004975]
	Learning Rate: 0.00497498
	LOSS [training: 0.30079677501556507 | validation: 0.28051753929108103]
	TIME [epoch: 88.1 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31890677390014016		[learning rate: 0.004967]
	Learning Rate: 0.00496697
	LOSS [training: 0.31890677390014016 | validation: 0.3254626714956431]
	TIME [epoch: 88.1 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2945059460785847		[learning rate: 0.004959]
	Learning Rate: 0.00495896
	LOSS [training: 0.2945059460785847 | validation: 0.29298566333623066]
	TIME [epoch: 88.1 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29626162030039455		[learning rate: 0.004951]
	Learning Rate: 0.00495096
	LOSS [training: 0.29626162030039455 | validation: 0.3171404545985166]
	TIME [epoch: 88.1 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2885445284776338		[learning rate: 0.004943]
	Learning Rate: 0.00494295
	LOSS [training: 0.2885445284776338 | validation: 0.30157700623564476]
	TIME [epoch: 88.1 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3014174356841187		[learning rate: 0.0049349]
	Learning Rate: 0.00493495
	LOSS [training: 0.3014174356841187 | validation: 0.27516149667284706]
	TIME [epoch: 88.1 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28195910002017166		[learning rate: 0.0049269]
	Learning Rate: 0.00492694
	LOSS [training: 0.28195910002017166 | validation: 0.2935654729720364]
	TIME [epoch: 88.1 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2804887520049156		[learning rate: 0.0049189]
	Learning Rate: 0.00491894
	LOSS [training: 0.2804887520049156 | validation: 0.2983047169691902]
	TIME [epoch: 88.1 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29324823356515894		[learning rate: 0.0049109]
	Learning Rate: 0.00491093
	LOSS [training: 0.29324823356515894 | validation: 0.29638990049832487]
	TIME [epoch: 88.1 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27895761882254794		[learning rate: 0.0049029]
	Learning Rate: 0.00490293
	LOSS [training: 0.27895761882254794 | validation: 0.3197621207481375]
	TIME [epoch: 88.1 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2918126103478847		[learning rate: 0.0048949]
	Learning Rate: 0.00489492
	LOSS [training: 0.2918126103478847 | validation: 0.31875904823165324]
	TIME [epoch: 88.1 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29995940549609984		[learning rate: 0.0048869]
	Learning Rate: 0.00488692
	LOSS [training: 0.29995940549609984 | validation: 0.27716822535055546]
	TIME [epoch: 88.1 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27841749970182517		[learning rate: 0.0048789]
	Learning Rate: 0.00487891
	LOSS [training: 0.27841749970182517 | validation: 0.27509572611484445]
	TIME [epoch: 88.1 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28778791642784207		[learning rate: 0.0048709]
	Learning Rate: 0.00487091
	LOSS [training: 0.28778791642784207 | validation: 0.29365299882741264]
	TIME [epoch: 88.1 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2787585063076743		[learning rate: 0.0048629]
	Learning Rate: 0.00486291
	LOSS [training: 0.2787585063076743 | validation: 0.30823969537282037]
	TIME [epoch: 88.1 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3003409314892407		[learning rate: 0.0048549]
	Learning Rate: 0.00485491
	LOSS [training: 0.3003409314892407 | validation: 0.28754506861345513]
	TIME [epoch: 88.1 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3028968818933628		[learning rate: 0.0048469]
	Learning Rate: 0.0048469
	LOSS [training: 0.3028968818933628 | validation: 0.2864195960556176]
	TIME [epoch: 88.1 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27559765002081665		[learning rate: 0.0048389]
	Learning Rate: 0.0048389
	LOSS [training: 0.27559765002081665 | validation: 0.3001196757817442]
	TIME [epoch: 88.1 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2956905463497959		[learning rate: 0.0048309]
	Learning Rate: 0.0048309
	LOSS [training: 0.2956905463497959 | validation: 0.2703354295043472]
	TIME [epoch: 88.1 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28842525470660396		[learning rate: 0.0048229]
	Learning Rate: 0.0048229
	LOSS [training: 0.28842525470660396 | validation: 0.27281967165906773]
	TIME [epoch: 88.1 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2766141292034199		[learning rate: 0.0048149]
	Learning Rate: 0.0048149
	LOSS [training: 0.2766141292034199 | validation: 0.27904785239841146]
	TIME [epoch: 88.2 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27647079388846374		[learning rate: 0.0048069]
	Learning Rate: 0.0048069
	LOSS [training: 0.27647079388846374 | validation: 0.2845050562091315]
	TIME [epoch: 88.3 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2735364234064618		[learning rate: 0.0047989]
	Learning Rate: 0.0047989
	LOSS [training: 0.2735364234064618 | validation: 0.32161176505895916]
	TIME [epoch: 88.4 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.279159407654295		[learning rate: 0.0047909]
	Learning Rate: 0.0047909
	LOSS [training: 0.279159407654295 | validation: 0.3110295479386914]
	TIME [epoch: 88.4 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2834230297462518		[learning rate: 0.0047829]
	Learning Rate: 0.0047829
	LOSS [training: 0.2834230297462518 | validation: 0.29679969348656277]
	TIME [epoch: 88.3 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2897791443486478		[learning rate: 0.0047749]
	Learning Rate: 0.0047749
	LOSS [training: 0.2897791443486478 | validation: 0.3066061883656013]
	TIME [epoch: 88.4 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2726742509984574		[learning rate: 0.0047669]
	Learning Rate: 0.0047669
	LOSS [training: 0.2726742509984574 | validation: 0.3370152606069631]
	TIME [epoch: 88.3 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2969565355728263		[learning rate: 0.0047589]
	Learning Rate: 0.00475891
	LOSS [training: 0.2969565355728263 | validation: 0.2879065037964693]
	TIME [epoch: 88.3 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28509284561046155		[learning rate: 0.0047509]
	Learning Rate: 0.00475091
	LOSS [training: 0.28509284561046155 | validation: 0.28505384121737404]
	TIME [epoch: 88.1 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2851619375979845		[learning rate: 0.0047429]
	Learning Rate: 0.00474292
	LOSS [training: 0.2851619375979845 | validation: 0.283840038514612]
	TIME [epoch: 88.2 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28646593488151717		[learning rate: 0.0047349]
	Learning Rate: 0.00473492
	LOSS [training: 0.28646593488151717 | validation: 0.2821139632455332]
	TIME [epoch: 88.1 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2847367512449531		[learning rate: 0.0047269]
	Learning Rate: 0.00472693
	LOSS [training: 0.2847367512449531 | validation: 0.28571139010681523]
	TIME [epoch: 88.2 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2793407946109044		[learning rate: 0.0047189]
	Learning Rate: 0.00471893
	LOSS [training: 0.2793407946109044 | validation: 0.2773818358157768]
	TIME [epoch: 88.4 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2832451692676383		[learning rate: 0.0047109]
	Learning Rate: 0.00471094
	LOSS [training: 0.2832451692676383 | validation: 0.28909003029386315]
	TIME [epoch: 88.3 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29149792077189823		[learning rate: 0.0047029]
	Learning Rate: 0.00470295
	LOSS [training: 0.29149792077189823 | validation: 0.2767010013222673]
	TIME [epoch: 88.4 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2608392948094763		[learning rate: 0.004695]
	Learning Rate: 0.00469496
	LOSS [training: 0.2608392948094763 | validation: 0.3043313152674625]
	TIME [epoch: 88.3 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3182275553541424		[learning rate: 0.004687]
	Learning Rate: 0.00468697
	LOSS [training: 0.3182275553541424 | validation: 0.2859915372106032]
	TIME [epoch: 88.3 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2819006717298761		[learning rate: 0.004679]
	Learning Rate: 0.00467898
	LOSS [training: 0.2819006717298761 | validation: 0.2761003089404508]
	TIME [epoch: 88.3 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2805733896131779		[learning rate: 0.004671]
	Learning Rate: 0.00467099
	LOSS [training: 0.2805733896131779 | validation: 0.27133674276420633]
	TIME [epoch: 88.3 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26395269405282995		[learning rate: 0.004663]
	Learning Rate: 0.004663
	LOSS [training: 0.26395269405282995 | validation: 0.3404482193998909]
	TIME [epoch: 88.3 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30759714051328		[learning rate: 0.004655]
	Learning Rate: 0.00465501
	LOSS [training: 0.30759714051328 | validation: 0.29071338160772786]
	TIME [epoch: 88.3 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2946256384846302		[learning rate: 0.004647]
	Learning Rate: 0.00464703
	LOSS [training: 0.2946256384846302 | validation: 0.2773200089258117]
	TIME [epoch: 88.3 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27623600041718716		[learning rate: 0.004639]
	Learning Rate: 0.00463904
	LOSS [training: 0.27623600041718716 | validation: 0.27472811166792693]
	TIME [epoch: 88.2 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2885029556598726		[learning rate: 0.0046311]
	Learning Rate: 0.00463106
	LOSS [training: 0.2885029556598726 | validation: 0.2950365346836757]
	TIME [epoch: 88.3 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27614667781850655		[learning rate: 0.0046231]
	Learning Rate: 0.00462307
	LOSS [training: 0.27614667781850655 | validation: 0.32749572546272177]
	TIME [epoch: 88.2 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30826307626440724		[learning rate: 0.0046151]
	Learning Rate: 0.00461509
	LOSS [training: 0.30826307626440724 | validation: 0.29482028440578834]
	TIME [epoch: 88.3 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27821852391155943		[learning rate: 0.0046071]
	Learning Rate: 0.00460711
	LOSS [training: 0.27821852391155943 | validation: 0.28329426642373456]
	TIME [epoch: 88.2 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2801336696327915		[learning rate: 0.0045991]
	Learning Rate: 0.00459913
	LOSS [training: 0.2801336696327915 | validation: 0.2885335304512833]
	TIME [epoch: 88.2 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28773792829429345		[learning rate: 0.0045912]
	Learning Rate: 0.00459115
	LOSS [training: 0.28773792829429345 | validation: 0.2813451862133486]
	TIME [epoch: 88.2 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2763591716942619		[learning rate: 0.0045832]
	Learning Rate: 0.00458317
	LOSS [training: 0.2763591716942619 | validation: 0.28342691281421495]
	TIME [epoch: 88.2 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.291454636337384		[learning rate: 0.0045752]
	Learning Rate: 0.0045752
	LOSS [training: 0.291454636337384 | validation: 0.28478627544756296]
	TIME [epoch: 88.3 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28434976402006373		[learning rate: 0.0045672]
	Learning Rate: 0.00456722
	LOSS [training: 0.28434976402006373 | validation: 0.2898332479129853]
	TIME [epoch: 88.3 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27815718627329167		[learning rate: 0.0045592]
	Learning Rate: 0.00455925
	LOSS [training: 0.27815718627329167 | validation: 0.29715477846831595]
	TIME [epoch: 88.2 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2868065707610759		[learning rate: 0.0045513]
	Learning Rate: 0.00455127
	LOSS [training: 0.2868065707610759 | validation: 0.3129730879865871]
	TIME [epoch: 88.3 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28615711327884863		[learning rate: 0.0045433]
	Learning Rate: 0.0045433
	LOSS [training: 0.28615711327884863 | validation: 0.29838090464244893]
	TIME [epoch: 88.3 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2687564303613855		[learning rate: 0.0045353]
	Learning Rate: 0.00453533
	LOSS [training: 0.2687564303613855 | validation: 0.3238159539641132]
	TIME [epoch: 88.3 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27587739758970187		[learning rate: 0.0045274]
	Learning Rate: 0.00452736
	LOSS [training: 0.27587739758970187 | validation: 0.2975699587072027]
	TIME [epoch: 88.3 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26520531942822306		[learning rate: 0.0045194]
	Learning Rate: 0.00451939
	LOSS [training: 0.26520531942822306 | validation: 0.26080706716874746]
	TIME [epoch: 88.3 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_1061.pth
	Model improved!!!
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2835490892193966		[learning rate: 0.0045114]
	Learning Rate: 0.00451142
	LOSS [training: 0.2835490892193966 | validation: 0.2979100137184151]
	TIME [epoch: 88.3 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2852755034752171		[learning rate: 0.0045035]
	Learning Rate: 0.00450345
	LOSS [training: 0.2852755034752171 | validation: 0.28040333578929405]
	TIME [epoch: 88.3 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3052785314391342		[learning rate: 0.0044955]
	Learning Rate: 0.00449549
	LOSS [training: 0.3052785314391342 | validation: 0.30368592153183643]
	TIME [epoch: 88.3 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28200985897849606		[learning rate: 0.0044875]
	Learning Rate: 0.00448753
	LOSS [training: 0.28200985897849606 | validation: 0.26421389370420034]
	TIME [epoch: 88.3 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2592748893544537		[learning rate: 0.0044796]
	Learning Rate: 0.00447956
	LOSS [training: 0.2592748893544537 | validation: 0.27712843195726666]
	TIME [epoch: 88.3 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.287516005720115		[learning rate: 0.0044716]
	Learning Rate: 0.0044716
	LOSS [training: 0.287516005720115 | validation: 0.2841889078352573]
	TIME [epoch: 88.2 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2815612638222949		[learning rate: 0.0044636]
	Learning Rate: 0.00446364
	LOSS [training: 0.2815612638222949 | validation: 0.2743597543305787]
	TIME [epoch: 88.2 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2786300262693725		[learning rate: 0.0044557]
	Learning Rate: 0.00445568
	LOSS [training: 0.2786300262693725 | validation: 0.2617782046187326]
	TIME [epoch: 88.2 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2683913642103909		[learning rate: 0.0044477]
	Learning Rate: 0.00444773
	LOSS [training: 0.2683913642103909 | validation: 0.2796098638703366]
	TIME [epoch: 88.2 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2893233384436959		[learning rate: 0.0044398]
	Learning Rate: 0.00443977
	LOSS [training: 0.2893233384436959 | validation: 0.3279131445827864]
	TIME [epoch: 88.2 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27375058447345674		[learning rate: 0.0044318]
	Learning Rate: 0.00443182
	LOSS [training: 0.27375058447345674 | validation: 0.2587153382117449]
	TIME [epoch: 88.2 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_1072.pth
	Model improved!!!
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2623299912422975		[learning rate: 0.0044239]
	Learning Rate: 0.00442386
	LOSS [training: 0.2623299912422975 | validation: 0.3316229928126002]
	TIME [epoch: 88.2 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29895744721422574		[learning rate: 0.0044159]
	Learning Rate: 0.00441591
	LOSS [training: 0.29895744721422574 | validation: 0.29331103663178576]
	TIME [epoch: 88.1 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27978213342915326		[learning rate: 0.004408]
	Learning Rate: 0.00440796
	LOSS [training: 0.27978213342915326 | validation: 0.2711084187282105]
	TIME [epoch: 88.1 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.280399734599751		[learning rate: 0.0044]
	Learning Rate: 0.00440002
	LOSS [training: 0.280399734599751 | validation: 0.2970205281609758]
	TIME [epoch: 88.1 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2781544640458984		[learning rate: 0.0043921]
	Learning Rate: 0.00439207
	LOSS [training: 0.2781544640458984 | validation: 0.28089218176415176]
	TIME [epoch: 88.1 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26703030344389267		[learning rate: 0.0043841]
	Learning Rate: 0.00438412
	LOSS [training: 0.26703030344389267 | validation: 0.280274627068802]
	TIME [epoch: 88.1 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2823005768131366		[learning rate: 0.0043762]
	Learning Rate: 0.00437618
	LOSS [training: 0.2823005768131366 | validation: 0.32371404188221287]
	TIME [epoch: 88.2 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3275747145580914		[learning rate: 0.0043682]
	Learning Rate: 0.00436824
	LOSS [training: 0.3275747145580914 | validation: 0.369976710551272]
	TIME [epoch: 88.1 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30478098613178745		[learning rate: 0.0043603]
	Learning Rate: 0.0043603
	LOSS [training: 0.30478098613178745 | validation: 0.28741061421874714]
	TIME [epoch: 88.1 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26933997420747335		[learning rate: 0.0043524]
	Learning Rate: 0.00435236
	LOSS [training: 0.26933997420747335 | validation: 0.28305431396394065]
	TIME [epoch: 88.1 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2613520612380073		[learning rate: 0.0043444]
	Learning Rate: 0.00434442
	LOSS [training: 0.2613520612380073 | validation: 0.29371794483122504]
	TIME [epoch: 88.1 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30979502587811314		[learning rate: 0.0043365]
	Learning Rate: 0.00433649
	LOSS [training: 0.30979502587811314 | validation: 0.31504887273049265]
	TIME [epoch: 88.1 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2870809433414266		[learning rate: 0.0043286]
	Learning Rate: 0.00432855
	LOSS [training: 0.2870809433414266 | validation: 0.27204237192666064]
	TIME [epoch: 88.1 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28680830111005984		[learning rate: 0.0043206]
	Learning Rate: 0.00432062
	LOSS [training: 0.28680830111005984 | validation: 0.28360717762593013]
	TIME [epoch: 88.1 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2785980239661072		[learning rate: 0.0043127]
	Learning Rate: 0.00431269
	LOSS [training: 0.2785980239661072 | validation: 0.2848192345110073]
	TIME [epoch: 88.1 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26899112285049165		[learning rate: 0.0043048]
	Learning Rate: 0.00430477
	LOSS [training: 0.26899112285049165 | validation: 0.26604829656257495]
	TIME [epoch: 88.1 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27046122593368693		[learning rate: 0.0042968]
	Learning Rate: 0.00429684
	LOSS [training: 0.27046122593368693 | validation: 0.2657219137775211]
	TIME [epoch: 88.1 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28449276782208927		[learning rate: 0.0042889]
	Learning Rate: 0.00428891
	LOSS [training: 0.28449276782208927 | validation: 0.2752571676362333]
	TIME [epoch: 88.1 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26976188955184		[learning rate: 0.004281]
	Learning Rate: 0.00428099
	LOSS [training: 0.26976188955184 | validation: 0.26043623091537516]
	TIME [epoch: 88.1 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27953250804679236		[learning rate: 0.0042731]
	Learning Rate: 0.00427307
	LOSS [training: 0.27953250804679236 | validation: 0.2765877413504594]
	TIME [epoch: 88.1 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27066667559800883		[learning rate: 0.0042652]
	Learning Rate: 0.00426515
	LOSS [training: 0.27066667559800883 | validation: 0.2611231270379453]
	TIME [epoch: 88.1 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2750842023857178		[learning rate: 0.0042572]
	Learning Rate: 0.00425724
	LOSS [training: 0.2750842023857178 | validation: 0.268896374876686]
	TIME [epoch: 88.1 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28434871314526655		[learning rate: 0.0042493]
	Learning Rate: 0.00424932
	LOSS [training: 0.28434871314526655 | validation: 0.2688709796563059]
	TIME [epoch: 88.1 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2750846168006666		[learning rate: 0.0042414]
	Learning Rate: 0.00424141
	LOSS [training: 0.2750846168006666 | validation: 0.2792158043386258]
	TIME [epoch: 88.1 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2833043602320535		[learning rate: 0.0042335]
	Learning Rate: 0.0042335
	LOSS [training: 0.2833043602320535 | validation: 0.2861564561246889]
	TIME [epoch: 88.1 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2793045691029052		[learning rate: 0.0042256]
	Learning Rate: 0.00422559
	LOSS [training: 0.2793045691029052 | validation: 0.2695019466084545]
	TIME [epoch: 88.1 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2710756665564356		[learning rate: 0.0042177]
	Learning Rate: 0.00421768
	LOSS [training: 0.2710756665564356 | validation: 0.287659807562771]
	TIME [epoch: 88.1 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27068408096889474		[learning rate: 0.0042098]
	Learning Rate: 0.00420977
	LOSS [training: 0.27068408096889474 | validation: 0.28808567025881116]
	TIME [epoch: 88.1 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2759368459700494		[learning rate: 0.0042019]
	Learning Rate: 0.00420187
	LOSS [training: 0.2759368459700494 | validation: 0.25770630955911245]
	TIME [epoch: 88.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_1101.pth
	Model improved!!!
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2667985883903248		[learning rate: 0.004194]
	Learning Rate: 0.00419397
	LOSS [training: 0.2667985883903248 | validation: 0.269794293422443]
	TIME [epoch: 88.1 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27106764523470017		[learning rate: 0.0041861]
	Learning Rate: 0.00418607
	LOSS [training: 0.27106764523470017 | validation: 0.2878270162527956]
	TIME [epoch: 88.1 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26512531237380077		[learning rate: 0.0041782]
	Learning Rate: 0.00417817
	LOSS [training: 0.26512531237380077 | validation: 0.2533914162016339]
	TIME [epoch: 88.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_1104.pth
	Model improved!!!
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2877297612781723		[learning rate: 0.0041703]
	Learning Rate: 0.00417028
	LOSS [training: 0.2877297612781723 | validation: 0.2681932967430975]
	TIME [epoch: 88.1 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27109396680051895		[learning rate: 0.0041624]
	Learning Rate: 0.00416239
	LOSS [training: 0.27109396680051895 | validation: 0.27979866531946185]
	TIME [epoch: 88.1 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27267227674667305		[learning rate: 0.0041545]
	Learning Rate: 0.0041545
	LOSS [training: 0.27267227674667305 | validation: 0.2892429775309771]
	TIME [epoch: 88.1 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2726626798345825		[learning rate: 0.0041466]
	Learning Rate: 0.00414661
	LOSS [training: 0.2726626798345825 | validation: 0.27697929749314837]
	TIME [epoch: 88.1 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2665013072627719		[learning rate: 0.0041387]
	Learning Rate: 0.00413872
	LOSS [training: 0.2665013072627719 | validation: 0.26948706940579076]
	TIME [epoch: 88.1 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2641851194687157		[learning rate: 0.0041308]
	Learning Rate: 0.00413084
	LOSS [training: 0.2641851194687157 | validation: 0.28819380068976636]
	TIME [epoch: 88.1 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27522695095536864		[learning rate: 0.004123]
	Learning Rate: 0.00412296
	LOSS [training: 0.27522695095536864 | validation: 0.27615656793364607]
	TIME [epoch: 88.1 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2705262394183622		[learning rate: 0.0041151]
	Learning Rate: 0.00411508
	LOSS [training: 0.2705262394183622 | validation: 0.29336170339159007]
	TIME [epoch: 88.1 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26749290851460134		[learning rate: 0.0041072]
	Learning Rate: 0.0041072
	LOSS [training: 0.26749290851460134 | validation: 0.30875815455765887]
	TIME [epoch: 88.1 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27849124066455855		[learning rate: 0.0040993]
	Learning Rate: 0.00409933
	LOSS [training: 0.27849124066455855 | validation: 0.2886229723237731]
	TIME [epoch: 88.1 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2772151449531405		[learning rate: 0.0040915]
	Learning Rate: 0.00409145
	LOSS [training: 0.2772151449531405 | validation: 0.27903777322422174]
	TIME [epoch: 88.1 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2761366679940279		[learning rate: 0.0040836]
	Learning Rate: 0.00408358
	LOSS [training: 0.2761366679940279 | validation: 0.3024610841860852]
	TIME [epoch: 88.1 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26224503867272353		[learning rate: 0.0040757]
	Learning Rate: 0.00407572
	LOSS [training: 0.26224503867272353 | validation: 0.26602580497455786]
	TIME [epoch: 88 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28274073369140446		[learning rate: 0.0040679]
	Learning Rate: 0.00406785
	LOSS [training: 0.28274073369140446 | validation: 0.25942416607475155]
	TIME [epoch: 88.1 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26566578041382205		[learning rate: 0.00406]
	Learning Rate: 0.00405999
	LOSS [training: 0.26566578041382205 | validation: 0.2871948912946083]
	TIME [epoch: 88.1 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2773633742825195		[learning rate: 0.0040521]
	Learning Rate: 0.00405213
	LOSS [training: 0.2773633742825195 | validation: 0.2678052127799196]
	TIME [epoch: 88.1 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2700292685742201		[learning rate: 0.0040443]
	Learning Rate: 0.00404427
	LOSS [training: 0.2700292685742201 | validation: 0.2653332842698989]
	TIME [epoch: 88.1 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2692594374242917		[learning rate: 0.0040364]
	Learning Rate: 0.00403641
	LOSS [training: 0.2692594374242917 | validation: 0.2888186310307608]
	TIME [epoch: 88.1 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2582778194295659		[learning rate: 0.0040286]
	Learning Rate: 0.00402856
	LOSS [training: 0.2582778194295659 | validation: 0.30942618309960923]
	TIME [epoch: 88.1 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28584257270841196		[learning rate: 0.0040207]
	Learning Rate: 0.00402071
	LOSS [training: 0.28584257270841196 | validation: 0.26774816636534166]
	TIME [epoch: 88.1 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27185599885930434		[learning rate: 0.0040129]
	Learning Rate: 0.00401286
	LOSS [training: 0.27185599885930434 | validation: 0.2803432117849063]
	TIME [epoch: 88.1 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2537808255492827		[learning rate: 0.004005]
	Learning Rate: 0.00400502
	LOSS [training: 0.2537808255492827 | validation: 0.32564857370004996]
	TIME [epoch: 88.1 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29439366621339974		[learning rate: 0.0039972]
	Learning Rate: 0.00399717
	LOSS [training: 0.29439366621339974 | validation: 0.27868953251077333]
	TIME [epoch: 88.1 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26746777868301036		[learning rate: 0.0039893]
	Learning Rate: 0.00398933
	LOSS [training: 0.26746777868301036 | validation: 0.2531451555740889]
	TIME [epoch: 88.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_1128.pth
	Model improved!!!
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2734596602540681		[learning rate: 0.0039815]
	Learning Rate: 0.0039815
	LOSS [training: 0.2734596602540681 | validation: 0.26685234023127574]
	TIME [epoch: 88.1 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2676467218760445		[learning rate: 0.0039737]
	Learning Rate: 0.00397366
	LOSS [training: 0.2676467218760445 | validation: 0.2622638440452266]
	TIME [epoch: 88.1 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26359818277873		[learning rate: 0.0039658]
	Learning Rate: 0.00396583
	LOSS [training: 0.26359818277873 | validation: 0.2705929938083118]
	TIME [epoch: 88.1 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26195843750978015		[learning rate: 0.003958]
	Learning Rate: 0.003958
	LOSS [training: 0.26195843750978015 | validation: 0.278909784885057]
	TIME [epoch: 88.1 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2736531372669038		[learning rate: 0.0039502]
	Learning Rate: 0.00395017
	LOSS [training: 0.2736531372669038 | validation: 0.2555470113437812]
	TIME [epoch: 88.1 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27555812533840573		[learning rate: 0.0039423]
	Learning Rate: 0.00394235
	LOSS [training: 0.27555812533840573 | validation: 0.28767922782302074]
	TIME [epoch: 88.1 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27153596506084654		[learning rate: 0.0039345]
	Learning Rate: 0.00393453
	LOSS [training: 0.27153596506084654 | validation: 0.2711574485027526]
	TIME [epoch: 88.1 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26034518310406884		[learning rate: 0.0039267]
	Learning Rate: 0.00392671
	LOSS [training: 0.26034518310406884 | validation: 0.27832016363081]
	TIME [epoch: 88.1 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.272871188676149		[learning rate: 0.0039189]
	Learning Rate: 0.00391889
	LOSS [training: 0.272871188676149 | validation: 0.29010867323067646]
	TIME [epoch: 88.1 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2673162843397099		[learning rate: 0.0039111]
	Learning Rate: 0.00391108
	LOSS [training: 0.2673162843397099 | validation: 0.2719112679600932]
	TIME [epoch: 88.1 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.266537016221307		[learning rate: 0.0039033]
	Learning Rate: 0.00390327
	LOSS [training: 0.266537016221307 | validation: 0.2710784092345146]
	TIME [epoch: 88 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2639495385362977		[learning rate: 0.0038955]
	Learning Rate: 0.00389546
	LOSS [training: 0.2639495385362977 | validation: 0.281189541611014]
	TIME [epoch: 88 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2829625537466183		[learning rate: 0.0038877]
	Learning Rate: 0.00388765
	LOSS [training: 0.2829625537466183 | validation: 0.2705965197375054]
	TIME [epoch: 88 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.272937570587656		[learning rate: 0.0038799]
	Learning Rate: 0.00387985
	LOSS [training: 0.272937570587656 | validation: 0.2608604533484126]
	TIME [epoch: 88 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2730956103656029		[learning rate: 0.0038721]
	Learning Rate: 0.00387205
	LOSS [training: 0.2730956103656029 | validation: 0.2764600722182571]
	TIME [epoch: 88 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2667085118098019		[learning rate: 0.0038643]
	Learning Rate: 0.00386426
	LOSS [training: 0.2667085118098019 | validation: 0.25619592127773155]
	TIME [epoch: 88 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25796635240167815		[learning rate: 0.0038565]
	Learning Rate: 0.00385646
	LOSS [training: 0.25796635240167815 | validation: 0.2543064538993983]
	TIME [epoch: 88.1 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2710430454784058		[learning rate: 0.0038487]
	Learning Rate: 0.00384867
	LOSS [training: 0.2710430454784058 | validation: 0.2723500052287075]
	TIME [epoch: 88.1 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2681827295981303		[learning rate: 0.0038409]
	Learning Rate: 0.00384089
	LOSS [training: 0.2681827295981303 | validation: 0.2711347368737416]
	TIME [epoch: 88 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2556757853021811		[learning rate: 0.0038331]
	Learning Rate: 0.0038331
	LOSS [training: 0.2556757853021811 | validation: 0.290161111769397]
	TIME [epoch: 88 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2645125927693936		[learning rate: 0.0038253]
	Learning Rate: 0.00382532
	LOSS [training: 0.2645125927693936 | validation: 0.28068217313206345]
	TIME [epoch: 88.1 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27375976599927426		[learning rate: 0.0038175]
	Learning Rate: 0.00381754
	LOSS [training: 0.27375976599927426 | validation: 0.25998639985856475]
	TIME [epoch: 88.1 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2726635344656999		[learning rate: 0.0038098]
	Learning Rate: 0.00380977
	LOSS [training: 0.2726635344656999 | validation: 0.27994682848561736]
	TIME [epoch: 88.1 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.267898528647319		[learning rate: 0.003802]
	Learning Rate: 0.003802
	LOSS [training: 0.267898528647319 | validation: 0.2688442092192501]
	TIME [epoch: 88.1 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27017123545494404		[learning rate: 0.0037942]
	Learning Rate: 0.00379423
	LOSS [training: 0.27017123545494404 | validation: 0.28146514761923813]
	TIME [epoch: 88.1 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2671238120792145		[learning rate: 0.0037865]
	Learning Rate: 0.00378646
	LOSS [training: 0.2671238120792145 | validation: 0.2623189155892466]
	TIME [epoch: 88.1 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26131116293748197		[learning rate: 0.0037787]
	Learning Rate: 0.0037787
	LOSS [training: 0.26131116293748197 | validation: 0.25724116106856676]
	TIME [epoch: 88.1 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25888517939525996		[learning rate: 0.0037709]
	Learning Rate: 0.00377094
	LOSS [training: 0.25888517939525996 | validation: 0.2645541352775638]
	TIME [epoch: 88.1 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.268233795556188		[learning rate: 0.0037632]
	Learning Rate: 0.00376318
	LOSS [training: 0.268233795556188 | validation: 0.24070765027634078]
	TIME [epoch: 88.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd6_20240713_103448/states/model_phiq_1a_v_mmd6_1157.pth
	Model improved!!!
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2734327209202201		[learning rate: 0.0037554]
	Learning Rate: 0.00375543
	LOSS [training: 0.2734327209202201 | validation: 0.26946416785812655]
	TIME [epoch: 88.1 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25566779453080496		[learning rate: 0.0037477]
	Learning Rate: 0.00374768
	LOSS [training: 0.25566779453080496 | validation: 0.30153458790176524]
	TIME [epoch: 88.1 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.303481862652655		[learning rate: 0.0037399]
	Learning Rate: 0.00373993
	LOSS [training: 0.303481862652655 | validation: 0.3203533698218992]
	TIME [epoch: 88.1 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2801087653054323		[learning rate: 0.0037322]
	Learning Rate: 0.00373219
	LOSS [training: 0.2801087653054323 | validation: 0.2756438105285462]
	TIME [epoch: 88.1 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26998710707403584		[learning rate: 0.0037244]
	Learning Rate: 0.00372445
	LOSS [training: 0.26998710707403584 | validation: 0.28539952812493347]
	TIME [epoch: 88.1 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2649263502056119		[learning rate: 0.0037167]
	Learning Rate: 0.00371671
	LOSS [training: 0.2649263502056119 | validation: 0.30456370586814774]
	TIME [epoch: 88.1 sec]
EPOCH 1164/2000:
	Training over batches...
