Args:
Namespace(name='model_phi2_1a_v_mmd1', outdir='out/model_training/model_phi2_1a_v_mmd1', training_data='data/training_data/data_phi2_1a/training', validation_data='data/training_data/data_phi2_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3505082701

Training model...

Saving initial model state to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.265650965816572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.265650965816572 | validation: 3.7947591605242366]
	TIME [epoch: 114 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.855314897261147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.855314897261147 | validation: 3.063801611132026]
	TIME [epoch: 7.91 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.503835909468294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.503835909468294 | validation: 2.920031438576212]
	TIME [epoch: 7.89 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.410272443939447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.410272443939447 | validation: 2.8986263869597835]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.202015570559636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.202015570559636 | validation: 2.5269209970794964]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9855338427609412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9855338427609412 | validation: 2.402902568848628]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8968544825778464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8968544825778464 | validation: 2.7728297616808097]
	TIME [epoch: 7.89 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1260556787690925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1260556787690925 | validation: 2.193966307470826]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4531948103111203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4531948103111203 | validation: 2.0696976729210648]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4386834825412427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4386834825412427 | validation: 2.0768025743139757]
	TIME [epoch: 7.84 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.234601167853558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.234601167853558 | validation: 1.7141481674986774]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9369969650414767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9369969650414767 | validation: 1.6497991720984093]
	TIME [epoch: 7.89 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8602299399956013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8602299399956013 | validation: 1.6837484536566]
	TIME [epoch: 7.85 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4918306457024344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4918306457024344 | validation: 1.3977552111967766]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.194725423535545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.194725423535545 | validation: 0.7085058275151417]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7685911384067311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7685911384067311 | validation: 0.5300525851708727]
	TIME [epoch: 7.89 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4919599236950357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4919599236950357 | validation: 0.31514851756445306]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.56764958712814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.56764958712814 | validation: 0.37821151225722144]
	TIME [epoch: 7.84 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.420582321815036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.420582321815036 | validation: 0.43091698872347645]
	TIME [epoch: 7.84 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47887756464849274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47887756464849274 | validation: 0.3042123616395357]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5052919280660686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5052919280660686 | validation: 0.35087375219698586]
	TIME [epoch: 7.89 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46287245244338876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46287245244338876 | validation: 0.3494074290410981]
	TIME [epoch: 7.85 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40649485117607587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40649485117607587 | validation: 0.24509151161672107]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4283888171557416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4283888171557416 | validation: 0.3578504916365381]
	TIME [epoch: 7.85 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42164897240610055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42164897240610055 | validation: 0.22966912724536084]
	TIME [epoch: 7.88 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38251749766498344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38251749766498344 | validation: 0.3010557246961087]
	TIME [epoch: 7.87 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40795463065533427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40795463065533427 | validation: 0.2972891284368523]
	TIME [epoch: 7.84 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3581438137722682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3581438137722682 | validation: 0.371082443187156]
	TIME [epoch: 7.84 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4503841378885385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4503841378885385 | validation: 0.2529680681652429]
	TIME [epoch: 7.84 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3765929718701035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3765929718701035 | validation: 0.20444266089165725]
	TIME [epoch: 7.89 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3491581747346518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3491581747346518 | validation: 0.3576933766693424]
	TIME [epoch: 7.84 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38571321564728256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38571321564728256 | validation: 0.29419421715077604]
	TIME [epoch: 7.84 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3414793965725897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3414793965725897 | validation: 0.2831307043741551]
	TIME [epoch: 7.84 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4171961531517247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4171961531517247 | validation: 0.40650464423100174]
	TIME [epoch: 7.86 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41637730454359395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41637730454359395 | validation: 0.2536683566517559]
	TIME [epoch: 7.88 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.335824561970092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.335824561970092 | validation: 0.25284032869962403]
	TIME [epoch: 7.84 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40130887692297545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40130887692297545 | validation: 0.19340288988108734]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27714870156007276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27714870156007276 | validation: 0.18124555956502608]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2834891732177729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2834891732177729 | validation: 0.2534660180473651]
	TIME [epoch: 7.89 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3648329802214284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3648329802214284 | validation: 0.2272783744123196]
	TIME [epoch: 7.84 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29688231321721287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29688231321721287 | validation: 0.27229107148488807]
	TIME [epoch: 7.84 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33212620668009085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33212620668009085 | validation: 0.2258154416654159]
	TIME [epoch: 7.84 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3104513143635188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3104513143635188 | validation: 0.2180751899741776]
	TIME [epoch: 7.84 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3330401192383319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3330401192383319 | validation: 0.16576039851160862]
	TIME [epoch: 7.88 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28214801299742254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28214801299742254 | validation: 0.17193978647230823]
	TIME [epoch: 7.84 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.306893628127773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.306893628127773 | validation: 0.1779675376424499]
	TIME [epoch: 7.83 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3021063608888072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3021063608888072 | validation: 0.18128580897590793]
	TIME [epoch: 7.84 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31490624706489895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31490624706489895 | validation: 0.17947866077081714]
	TIME [epoch: 7.84 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28220331651188674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28220331651188674 | validation: 0.1773095200341123]
	TIME [epoch: 7.88 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2997403935423603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2997403935423603 | validation: 0.24773133278594006]
	TIME [epoch: 7.84 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3267969103355358		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 0.3267969103355358 | validation: 0.18381707049639484]
	TIME [epoch: 7.84 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2913418590588668		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 0.2913418590588668 | validation: 0.2540277737965795]
	TIME [epoch: 7.83 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2937881073760256		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 0.2937881073760256 | validation: 0.18050124511068874]
	TIME [epoch: 7.85 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3004438639809182		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 0.3004438639809182 | validation: 0.16663560889264]
	TIME [epoch: 7.88 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28643045320446514		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 0.28643045320446514 | validation: 0.2704276575832908]
	TIME [epoch: 7.84 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31343515633521857		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 0.31343515633521857 | validation: 0.18294851319677632]
	TIME [epoch: 7.83 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27135609942147854		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 0.27135609942147854 | validation: 0.18623042108719715]
	TIME [epoch: 7.83 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27130355626439095		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 0.27130355626439095 | validation: 0.17418994081139802]
	TIME [epoch: 7.86 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29067656445048307		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 0.29067656445048307 | validation: 0.1839114655040956]
	TIME [epoch: 7.86 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2815381137125121		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 0.2815381137125121 | validation: 0.21415128492995142]
	TIME [epoch: 7.84 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3105806574028332		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 0.3105806574028332 | validation: 0.1969729904178386]
	TIME [epoch: 7.84 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27481418403803637		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 0.27481418403803637 | validation: 0.17652678854199866]
	TIME [epoch: 7.83 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26807967145682854		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 0.26807967145682854 | validation: 0.19851224157797992]
	TIME [epoch: 7.88 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2646830282453826		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 0.2646830282453826 | validation: 0.21168478909994765]
	TIME [epoch: 7.85 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29602920533853383		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 0.29602920533853383 | validation: 0.22424883990435768]
	TIME [epoch: 7.84 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2741319668970892		[learning rate: 0.0094573]
	Learning Rate: 0.00945734
	LOSS [training: 0.2741319668970892 | validation: 0.21647749851420123]
	TIME [epoch: 7.84 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.276309045986838		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 0.276309045986838 | validation: 0.1939952821517753]
	TIME [epoch: 7.81 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28959814186137345		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 0.28959814186137345 | validation: 0.1800534954540871]
	TIME [epoch: 7.88 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2547701228533419		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 0.2547701228533419 | validation: 0.1753921657410088]
	TIME [epoch: 7.85 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2576938752527421		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.2576938752527421 | validation: 0.18258229925327907]
	TIME [epoch: 7.84 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2590251063530281		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 0.2590251063530281 | validation: 0.255853568500474]
	TIME [epoch: 7.84 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.294612338030096		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 0.294612338030096 | validation: 0.15552496616673958]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25971702234924043		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 0.25971702234924043 | validation: 0.18388566212671859]
	TIME [epoch: 7.89 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2572266628844593		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 0.2572266628844593 | validation: 0.21525741009335825]
	TIME [epoch: 7.83 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2754326992261188		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.2754326992261188 | validation: 0.1913432657637528]
	TIME [epoch: 7.84 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.271170762192043		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 0.271170762192043 | validation: 0.180028287861571]
	TIME [epoch: 7.83 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24833128821288394		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 0.24833128821288394 | validation: 0.18758000558099414]
	TIME [epoch: 7.86 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26686645742420145		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.26686645742420145 | validation: 0.1570930211960211]
	TIME [epoch: 7.86 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24000115798873706		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 0.24000115798873706 | validation: 0.16325771970043135]
	TIME [epoch: 7.84 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27600425108735027		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.27600425108735027 | validation: 0.15915175405905213]
	TIME [epoch: 7.83 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.254845977319663		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.254845977319663 | validation: 0.15486638415343074]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25434399960711407		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.25434399960711407 | validation: 0.15598337949317098]
	TIME [epoch: 7.89 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2620783038626347		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.2620783038626347 | validation: 0.15475647049582364]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24899445356011135		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.24899445356011135 | validation: 0.1688979416543385]
	TIME [epoch: 7.84 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2482767499945654		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.2482767499945654 | validation: 0.16950855179481933]
	TIME [epoch: 7.83 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2599093123202318		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.2599093123202318 | validation: 0.1498341721118127]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24594586527340045		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.24594586527340045 | validation: 0.1797094748948597]
	TIME [epoch: 7.88 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26343381133128585		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.26343381133128585 | validation: 0.1692892846357938]
	TIME [epoch: 7.84 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.234337128552291		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.234337128552291 | validation: 0.15993713564989304]
	TIME [epoch: 7.83 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24821085547774224		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.24821085547774224 | validation: 0.15293721303594848]
	TIME [epoch: 7.83 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2599077566455271		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.2599077566455271 | validation: 0.1589795916529534]
	TIME [epoch: 7.86 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24159167261141248		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.24159167261141248 | validation: 0.1590468400506499]
	TIME [epoch: 7.86 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23478199688642043		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.23478199688642043 | validation: 0.1714501320786043]
	TIME [epoch: 7.83 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24547191597809923		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.24547191597809923 | validation: 0.1538617580841508]
	TIME [epoch: 7.83 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23991740682482787		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.23991740682482787 | validation: 0.1793643599792626]
	TIME [epoch: 7.83 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2448609417047224		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.2448609417047224 | validation: 0.2171232108014758]
	TIME [epoch: 7.87 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2617676533832066		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.2617676533832066 | validation: 0.16486348555532826]
	TIME [epoch: 7.84 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23208788038878247		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.23208788038878247 | validation: 0.15017126137521147]
	TIME [epoch: 7.83 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22448066984701207		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.22448066984701207 | validation: 0.1514808082420838]
	TIME [epoch: 7.83 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2317787594720287		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.2317787594720287 | validation: 0.17860558420421677]
	TIME [epoch: 7.84 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24450264271490793		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.24450264271490793 | validation: 0.15401103381442421]
	TIME [epoch: 7.89 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2238768601343626		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.2238768601343626 | validation: 0.1780488269328562]
	TIME [epoch: 7.84 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2289374630999641		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.2289374630999641 | validation: 0.1459100055079591]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22883706370113588		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.22883706370113588 | validation: 0.17123129848098761]
	TIME [epoch: 7.84 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2324451518223862		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.2324451518223862 | validation: 0.15074630997806296]
	TIME [epoch: 7.85 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22113325099719133		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.22113325099719133 | validation: 0.1638913529471508]
	TIME [epoch: 7.88 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23138788036239855		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.23138788036239855 | validation: 0.15212500214019675]
	TIME [epoch: 7.84 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2162738733959072		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.2162738733959072 | validation: 0.14955610699021027]
	TIME [epoch: 7.84 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21006954530932997		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.21006954530932997 | validation: 0.15442690195534234]
	TIME [epoch: 7.84 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21518330899865074		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.21518330899865074 | validation: 0.15825363073318083]
	TIME [epoch: 7.87 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21893169442330476		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.21893169442330476 | validation: 0.13794833782652183]
	TIME [epoch: 7.87 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1970230912623075		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.1970230912623075 | validation: 0.1441065368328843]
	TIME [epoch: 7.84 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20782335984708122		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.20782335984708122 | validation: 0.13130911007276175]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18290186132478264		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.18290186132478264 | validation: 0.19819955762135616]
	TIME [epoch: 7.85 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21250352468782807		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.21250352468782807 | validation: 0.13651430178862053]
	TIME [epoch: 7.89 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19543905517010876		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.19543905517010876 | validation: 0.1284281121912959]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1749718883946043		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.1749718883946043 | validation: 0.17741030270735722]
	TIME [epoch: 7.84 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1736791721762339		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.1736791721762339 | validation: 0.13545292464819841]
	TIME [epoch: 7.84 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17830036064778337		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.17830036064778337 | validation: 0.10407608643379332]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16628386330165815		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.16628386330165815 | validation: 0.14883264019036796]
	TIME [epoch: 7.89 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18406085894688093		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.18406085894688093 | validation: 0.102342507308645]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13511507258825414		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.13511507258825414 | validation: 0.10809954705052688]
	TIME [epoch: 7.84 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17281709065291664		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.17281709065291664 | validation: 0.09723520959425712]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1362480800031976		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.1362480800031976 | validation: 0.11962021122894757]
	TIME [epoch: 7.89 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18465594109149308		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.18465594109149308 | validation: 0.13760630661115975]
	TIME [epoch: 7.86 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14709462203503162		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.14709462203503162 | validation: 0.08739165986705509]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12771721651556464		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.12771721651556464 | validation: 0.18768010193288204]
	TIME [epoch: 7.84 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1364990397533817		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.1364990397533817 | validation: 0.09191303684817353]
	TIME [epoch: 7.82 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14038943960806272		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.14038943960806272 | validation: 0.09381003878735432]
	TIME [epoch: 7.87 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12209515227357956		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.12209515227357956 | validation: 0.11963211425232452]
	TIME [epoch: 7.84 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1423455649025946		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.1423455649025946 | validation: 0.11903195627494202]
	TIME [epoch: 7.84 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13797519137100928		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.13797519137100928 | validation: 0.08575133794252465]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_132.pth
	Model improved!!!
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09810593319703992		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.09810593319703992 | validation: 0.06221259699222834]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10901353424903497		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.10901353424903497 | validation: 0.09370756572255465]
	TIME [epoch: 7.87 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10894366641545354		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.10894366641545354 | validation: 0.08220364689440997]
	TIME [epoch: 7.84 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0990027701033285		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.0990027701033285 | validation: 0.0643633027717969]
	TIME [epoch: 7.84 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11196595786049719		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.11196595786049719 | validation: 0.05995726176532355]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_137.pth
	Model improved!!!
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11968472617638518		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 0.11968472617638518 | validation: 0.08033116705571818]
	TIME [epoch: 7.89 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09850883811552719		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.09850883811552719 | validation: 0.1020952054497116]
	TIME [epoch: 7.84 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09652248856106632		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.09652248856106632 | validation: 0.06830993406004651]
	TIME [epoch: 7.84 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10604082715973775		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.10604082715973775 | validation: 0.0687685731539399]
	TIME [epoch: 7.85 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09628673400887004		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.09628673400887004 | validation: 0.05991265309613543]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10792864233851507		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.10792864233851507 | validation: 0.0622680487179281]
	TIME [epoch: 7.88 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08787906631478198		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.08787906631478198 | validation: 0.10742759150504588]
	TIME [epoch: 7.84 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07065547801134853		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.07065547801134853 | validation: 0.06930016119302822]
	TIME [epoch: 7.83 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10368113389308717		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.10368113389308717 | validation: 0.05034399450056273]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08442979926851582		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.08442979926851582 | validation: 0.0642479007299068]
	TIME [epoch: 7.86 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06342135904142647		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.06342135904142647 | validation: 0.059415401666394474]
	TIME [epoch: 7.86 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11767011573986227		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.11767011573986227 | validation: 0.07779522270312655]
	TIME [epoch: 7.82 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07660309969506789		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.07660309969506789 | validation: 0.03648607682722946]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_150.pth
	Model improved!!!
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07658441351030214		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.07658441351030214 | validation: 0.05862569760723907]
	TIME [epoch: 7.84 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08368956854227848		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.08368956854227848 | validation: 0.05368145813468092]
	TIME [epoch: 7.89 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05460588683214442		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.05460588683214442 | validation: 0.03334848346125649]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_153.pth
	Model improved!!!
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09057542064415865		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.09057542064415865 | validation: 0.11788018346149878]
	TIME [epoch: 7.85 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10590715194565889		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.10590715194565889 | validation: 0.07575464690732647]
	TIME [epoch: 7.85 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07338625678325228		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.07338625678325228 | validation: 0.061922716475420886]
	TIME [epoch: 7.84 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06367432605909264		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.06367432605909264 | validation: 0.04653887050579618]
	TIME [epoch: 7.89 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05468276216936143		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.05468276216936143 | validation: 0.045993883346760925]
	TIME [epoch: 7.85 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050500333895672114		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.050500333895672114 | validation: 0.055261068198493885]
	TIME [epoch: 7.84 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10137168165451146		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.10137168165451146 | validation: 0.05349147228959264]
	TIME [epoch: 7.84 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041060251327885766		[learning rate: 0.0067548]
	Learning Rate: 0.00675485
	LOSS [training: 0.041060251327885766 | validation: 0.04944865829378888]
	TIME [epoch: 7.85 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09196108812613989		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.09196108812613989 | validation: 0.08901224349368478]
	TIME [epoch: 7.88 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06549278411315802		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.06549278411315802 | validation: 0.034318536243477187]
	TIME [epoch: 7.85 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03470465238185928		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.03470465238185928 | validation: 0.04788247604506538]
	TIME [epoch: 7.83 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09382276486274119		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.09382276486274119 | validation: 0.02826042112755751]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_165.pth
	Model improved!!!
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04865012833223674		[learning rate: 0.0066363]
	Learning Rate: 0.00663625
	LOSS [training: 0.04865012833223674 | validation: 0.12441341733081339]
	TIME [epoch: 7.89 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09454602670460224		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.09454602670460224 | validation: 0.06937694478136917]
	TIME [epoch: 7.86 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06676152104670237		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.06676152104670237 | validation: 0.03231738047400194]
	TIME [epoch: 7.84 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04674044264662576		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.04674044264662576 | validation: 0.08364331594833502]
	TIME [epoch: 7.84 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04237440501865259		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.04237440501865259 | validation: 0.03109316346354616]
	TIME [epoch: 7.84 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059887897485186824		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.059887897485186824 | validation: 0.04523057200548623]
	TIME [epoch: 7.89 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042650693949351996		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.042650693949351996 | validation: 0.17470372837884737]
	TIME [epoch: 7.85 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09901439145849712		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.09901439145849712 | validation: 0.049837269164388076]
	TIME [epoch: 7.84 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03900447676951211		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.03900447676951211 | validation: 0.03126570360886891]
	TIME [epoch: 7.84 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051710241647652685		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 0.051710241647652685 | validation: 0.05321293482842336]
	TIME [epoch: 7.84 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059945376963701086		[learning rate: 0.0064053]
	Learning Rate: 0.00640529
	LOSS [training: 0.059945376963701086 | validation: 0.054902603337411746]
	TIME [epoch: 7.89 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05475895960719133		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.05475895960719133 | validation: 0.07208192240474345]
	TIME [epoch: 7.85 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06208802915643329		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.06208802915643329 | validation: 0.02638382661510454]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_178.pth
	Model improved!!!
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025962482469140687		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.025962482469140687 | validation: 0.028985045708914213]
	TIME [epoch: 7.84 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08213701262614055		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.08213701262614055 | validation: 0.025075507587689227]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_180.pth
	Model improved!!!
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033140667001513524		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.033140667001513524 | validation: 0.029816782380873097]
	TIME [epoch: 7.87 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06226963768767871		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.06226963768767871 | validation: 0.05514501603768048]
	TIME [epoch: 7.83 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05336227553146711		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.05336227553146711 | validation: 0.06351449595076078]
	TIME [epoch: 7.84 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037475016818962134		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.037475016818962134 | validation: 0.026833721283537275]
	TIME [epoch: 7.84 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08283467801009221		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.08283467801009221 | validation: 0.024222366489582894]
	TIME [epoch: 7.89 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04136949710026037		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.04136949710026037 | validation: 0.018991258372178146]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_186.pth
	Model improved!!!
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033319097010401116		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.033319097010401116 | validation: 0.03825010745477399]
	TIME [epoch: 7.85 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04300180742298586		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.04300180742298586 | validation: 0.026663304511528346]
	TIME [epoch: 7.84 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02824265932873956		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.02824265932873956 | validation: 0.02359357915751712]
	TIME [epoch: 7.84 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07751398389251982		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.07751398389251982 | validation: 0.0365669005295807]
	TIME [epoch: 7.89 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03235070256317671		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.03235070256317671 | validation: 0.03725921824931386]
	TIME [epoch: 7.84 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03687639086235444		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.03687639086235444 | validation: 0.029269143506737844]
	TIME [epoch: 7.83 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05935724631090873		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.05935724631090873 | validation: 0.03305377348629595]
	TIME [epoch: 7.84 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04946481263050676		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.04946481263050676 | validation: 0.022500633377030357]
	TIME [epoch: 7.85 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03256540811392535		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.03256540811392535 | validation: 0.046577545089720146]
	TIME [epoch: 7.88 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04744322907901283		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.04744322907901283 | validation: 0.0316641583127636]
	TIME [epoch: 7.83 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050650205089393996		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.050650205089393996 | validation: 0.03248708903444003]
	TIME [epoch: 7.83 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024784355414709944		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.024784355414709944 | validation: 0.017436917187643577]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03031467621686392		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.03031467621686392 | validation: 0.07217583921166083]
	TIME [epoch: 7.89 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05038770827269203		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.05038770827269203 | validation: 0.022644071497951147]
	TIME [epoch: 7.86 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03131664589166275		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.03131664589166275 | validation: 0.04210222847079696]
	TIME [epoch: 7.84 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037916832845218636		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.037916832845218636 | validation: 0.05922509909920485]
	TIME [epoch: 7.84 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057901493363768396		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.057901493363768396 | validation: 0.025729851711161492]
	TIME [epoch: 7.84 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038573649891427676		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.038573649891427676 | validation: 0.14332759150021912]
	TIME [epoch: 7.89 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09815456849324553		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.09815456849324553 | validation: 0.026238767876719464]
	TIME [epoch: 7.84 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024279161783859637		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.024279161783859637 | validation: 0.023618247047964242]
	TIME [epoch: 7.84 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04793276955829792		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.04793276955829792 | validation: 0.020932495990334313]
	TIME [epoch: 7.79 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019188853671361698		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.019188853671361698 | validation: 0.019899262957125675]
	TIME [epoch: 7.84 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025157463270909328		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.025157463270909328 | validation: 0.028518630019913623]
	TIME [epoch: 7.88 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05643129496327017		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.05643129496327017 | validation: 0.033685570231346905]
	TIME [epoch: 7.84 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03709939521506439		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.03709939521506439 | validation: 0.015440702046950413]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01878459089410476		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.01878459089410476 | validation: 0.02103858695005989]
	TIME [epoch: 7.84 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04409341407137961		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.04409341407137961 | validation: 0.023895239077205952]
	TIME [epoch: 7.87 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03991879458990597		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.03991879458990597 | validation: 0.0966866355531789]
	TIME [epoch: 7.86 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042196637705252886		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.042196637705252886 | validation: 0.0168696646042603]
	TIME [epoch: 7.84 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016785140720109087		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.016785140720109087 | validation: 0.013300651364041075]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_216.pth
	Model improved!!!
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02559978369282427		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.02559978369282427 | validation: 0.09815384856100429]
	TIME [epoch: 7.84 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06633544474659116		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.06633544474659116 | validation: 0.02012040908447703]
	TIME [epoch: 7.89 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02290434132289544		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.02290434132289544 | validation: 0.016305777451575597]
	TIME [epoch: 7.84 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029656616741885914		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.029656616741885914 | validation: 0.031690503911612246]
	TIME [epoch: 7.84 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04160621650934776		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.04160621650934776 | validation: 0.02387507938360514]
	TIME [epoch: 7.84 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02960246699168504		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.02960246699168504 | validation: 0.04110133855181729]
	TIME [epoch: 7.84 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04286265575945293		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.04286265575945293 | validation: 0.02167069638253663]
	TIME [epoch: 7.88 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021848524740749456		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.021848524740749456 | validation: 0.01458219621544812]
	TIME [epoch: 7.84 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028109198332340744		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.028109198332340744 | validation: 0.08685540031701094]
	TIME [epoch: 7.84 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046504138616902765		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.046504138616902765 | validation: 0.04122460903793043]
	TIME [epoch: 7.84 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029321670393335705		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.029321670393335705 | validation: 0.020962128510958626]
	TIME [epoch: 7.86 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03218499078012491		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.03218499078012491 | validation: 0.022725396999553872]
	TIME [epoch: 7.88 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01938932387978685		[learning rate: 0.0053088]
	Learning Rate: 0.00530885
	LOSS [training: 0.01938932387978685 | validation: 0.043523862942870056]
	TIME [epoch: 7.84 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046881831198240784		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.046881831198240784 | validation: 0.023364584357878658]
	TIME [epoch: 7.84 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019185822205286432		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.019185822205286432 | validation: 0.019232093062178192]
	TIME [epoch: 7.84 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040049938465620195		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.040049938465620195 | validation: 0.014214545708716223]
	TIME [epoch: 7.87 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02631759376340828		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.02631759376340828 | validation: 0.03632376263104114]
	TIME [epoch: 7.86 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03608332472189352		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.03608332472189352 | validation: 0.01835302620032182]
	TIME [epoch: 7.84 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02915023205608877		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.02915023205608877 | validation: 0.015623372511190217]
	TIME [epoch: 7.84 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014900523555587178		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.014900523555587178 | validation: 0.017687562874736662]
	TIME [epoch: 7.84 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034204818633685075		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.034204818633685075 | validation: 0.1018817778451801]
	TIME [epoch: 7.88 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051354749480809876		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.051354749480809876 | validation: 0.01982125454870664]
	TIME [epoch: 7.85 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022900770861201653		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.022900770861201653 | validation: 0.02353408728193534]
	TIME [epoch: 7.84 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024580332895834004		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 0.024580332895834004 | validation: 0.018497393795726645]
	TIME [epoch: 7.84 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018394893564051624		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.018394893564051624 | validation: 0.02669546537087027]
	TIME [epoch: 7.84 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04804994869403391		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.04804994869403391 | validation: 0.015228694898377035]
	TIME [epoch: 7.89 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02275374805145427		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.02275374805145427 | validation: 0.015318718290567294]
	TIME [epoch: 7.85 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015103933304982433		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.015103933304982433 | validation: 0.020592418184632942]
	TIME [epoch: 7.84 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04547537217418708		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.04547537217418708 | validation: 0.02728087643138522]
	TIME [epoch: 7.84 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0340405049231259		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.0340405049231259 | validation: 0.015801772242051856]
	TIME [epoch: 7.85 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0165559536612955		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.0165559536612955 | validation: 0.013912409142697155]
	TIME [epoch: 7.89 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013869021089941973		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.013869021089941973 | validation: 0.014081786326666696]
	TIME [epoch: 7.84 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05213483507942061		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.05213483507942061 | validation: 0.017269456418877366]
	TIME [epoch: 7.84 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027142190875855423		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 0.027142190875855423 | validation: 0.017990187027189748]
	TIME [epoch: 7.84 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02047440640279953		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.02047440640279953 | validation: 0.013012246989584612]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_251.pth
	Model improved!!!
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017045775992983313		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.017045775992983313 | validation: 0.02483581514436168]
	TIME [epoch: 7.88 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025058219099655485		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.025058219099655485 | validation: 0.046128407859533344]
	TIME [epoch: 7.84 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029977738242656527		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.029977738242656527 | validation: 0.010576258298417383]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_254.pth
	Model improved!!!
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011915659039759448		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.011915659039759448 | validation: 0.01225705457205396]
	TIME [epoch: 7.84 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02153537805383289		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.02153537805383289 | validation: 0.0303189251313227]
	TIME [epoch: 7.88 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04088560555318317		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.04088560555318317 | validation: 0.014877210327913528]
	TIME [epoch: 7.85 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023334536234572427		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.023334536234572427 | validation: 0.02809039276790408]
	TIME [epoch: 7.84 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019262436407856343		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.019262436407856343 | validation: 0.019483979992279896]
	TIME [epoch: 7.84 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02723944775573078		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.02723944775573078 | validation: 0.02026338592462857]
	TIME [epoch: 7.84 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018454996512597067		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.018454996512597067 | validation: 0.015328601075949272]
	TIME [epoch: 7.89 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02170086636918209		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.02170086636918209 | validation: 0.022519693083530072]
	TIME [epoch: 7.84 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02954785307684188		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.02954785307684188 | validation: 0.015360906577199268]
	TIME [epoch: 7.84 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017310573905276596		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.017310573905276596 | validation: 0.012844017580537733]
	TIME [epoch: 7.84 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021653836489626665		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.021653836489626665 | validation: 0.020223309991692442]
	TIME [epoch: 7.85 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022897751724929104		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.022897751724929104 | validation: 0.022687593689793638]
	TIME [epoch: 7.88 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02052866079674893		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.02052866079674893 | validation: 0.012206215949885628]
	TIME [epoch: 7.84 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017452073419597914		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.017452073419597914 | validation: 0.07869138082928506]
	TIME [epoch: 7.84 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03922046804845983		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.03922046804845983 | validation: 0.014592621137380059]
	TIME [epoch: 7.84 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01337479883003719		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.01337479883003719 | validation: 0.021936561510842646]
	TIME [epoch: 7.86 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02999312253380064		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.02999312253380064 | validation: 0.012690284474760553]
	TIME [epoch: 7.86 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013431785136191025		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.013431785136191025 | validation: 0.022652766865066044]
	TIME [epoch: 7.84 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015348892470972191		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.015348892470972191 | validation: 0.022728253975363308]
	TIME [epoch: 7.84 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03691994283920186		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.03691994283920186 | validation: 0.010508714395919843]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_274.pth
	Model improved!!!
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02514700876125623		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.02514700876125623 | validation: 0.011611237358212166]
	TIME [epoch: 7.89 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013665823788037424		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.013665823788037424 | validation: 0.010968143108138173]
	TIME [epoch: 7.85 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026205912238485687		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.026205912238485687 | validation: 0.03300435885816607]
	TIME [epoch: 7.84 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02138639851475024		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.02138639851475024 | validation: 0.01379604703336898]
	TIME [epoch: 7.85 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022257124122308458		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.022257124122308458 | validation: 0.01816488114854711]
	TIME [epoch: 7.85 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01928543635010175		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.01928543635010175 | validation: 0.023511696009317948]
	TIME [epoch: 7.9 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022995118074755204		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.022995118074755204 | validation: 0.011791509576004833]
	TIME [epoch: 7.85 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016172960854060624		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.016172960854060624 | validation: 0.0333343421180055]
	TIME [epoch: 7.85 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027688394942838673		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.027688394942838673 | validation: 0.014518693925986328]
	TIME [epoch: 7.85 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02315683079815262		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.02315683079815262 | validation: 0.015234094826201479]
	TIME [epoch: 7.85 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014649768173814224		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.014649768173814224 | validation: 0.016607936692175723]
	TIME [epoch: 7.89 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021984139260781528		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.021984139260781528 | validation: 0.01129210177364162]
	TIME [epoch: 7.85 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01740504903886411		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.01740504903886411 | validation: 0.014247320717916779]
	TIME [epoch: 7.85 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04525080764990099		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.04525080764990099 | validation: 0.013227531281886787]
	TIME [epoch: 7.85 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015139581032004681		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.015139581032004681 | validation: 0.011702953310833921]
	TIME [epoch: 7.87 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011413425882213214		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.011413425882213214 | validation: 0.01329205426860572]
	TIME [epoch: 7.87 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018103426608423442		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.018103426608423442 | validation: 0.018250263786793636]
	TIME [epoch: 7.84 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019212126316607864		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.019212126316607864 | validation: 0.023598068474750846]
	TIME [epoch: 7.85 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020128980680307412		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.020128980680307412 | validation: 0.011611522601658842]
	TIME [epoch: 7.85 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01654044329614851		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.01654044329614851 | validation: 0.06903439285513387]
	TIME [epoch: 7.89 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04049465945858548		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.04049465945858548 | validation: 0.01571473306556452]
	TIME [epoch: 7.85 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013868538345267242		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.013868538345267242 | validation: 0.012926288168236339]
	TIME [epoch: 7.83 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015200941265607578		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.015200941265607578 | validation: 0.011904849045998712]
	TIME [epoch: 7.85 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012992238907498246		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.012992238907498246 | validation: 0.011997630344533335]
	TIME [epoch: 7.84 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011572541920021488		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.011572541920021488 | validation: 0.014064161172549546]
	TIME [epoch: 7.89 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03794166584233834		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.03794166584233834 | validation: 0.012637447652686557]
	TIME [epoch: 7.84 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023462288986174938		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.023462288986174938 | validation: 0.01563279837575802]
	TIME [epoch: 7.84 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01540848306063114		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.01540848306063114 | validation: 0.01017178097544381]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_302.pth
	Model improved!!!
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011996478847962622		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.011996478847962622 | validation: 0.017197553300399716]
	TIME [epoch: 7.85 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022571324729631168		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.022571324729631168 | validation: 0.013613267656377563]
	TIME [epoch: 7.88 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021024865544885307		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.021024865544885307 | validation: 0.055498505448443154]
	TIME [epoch: 7.84 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03578255783497685		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.03578255783497685 | validation: 0.011540815606714196]
	TIME [epoch: 7.83 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01547464663962889		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.01547464663962889 | validation: 0.009183526377852887]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_307.pth
	Model improved!!!
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010876205835254994		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.010876205835254994 | validation: 0.011168867780723887]
	TIME [epoch: 7.88 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012757124338273602		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.012757124338273602 | validation: 0.028637193371883057]
	TIME [epoch: 7.85 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017231454530871253		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.017231454530871253 | validation: 0.025421416273541633]
	TIME [epoch: 7.84 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01870551244217493		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.01870551244217493 | validation: 0.008912220296358561]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_311.pth
	Model improved!!!
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01688645799442735		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.01688645799442735 | validation: 0.037580663240277465]
	TIME [epoch: 7.84 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023147073456740642		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.023147073456740642 | validation: 0.09342042581188739]
	TIME [epoch: 7.89 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045973132434572385		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.045973132434572385 | validation: 0.015254474327532876]
	TIME [epoch: 7.84 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011529348474364182		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.011529348474364182 | validation: 0.008421649612395716]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_315.pth
	Model improved!!!
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010730702975324802		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.010730702975324802 | validation: 0.013263176134622037]
	TIME [epoch: 7.84 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011981854825519445		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.011981854825519445 | validation: 0.011164991212666085]
	TIME [epoch: 7.85 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021790597036505037		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.021790597036505037 | validation: 0.02365073471300565]
	TIME [epoch: 7.88 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014816808599259764		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.014816808599259764 | validation: 0.019127973780359377]
	TIME [epoch: 7.84 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025254999838500106		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.025254999838500106 | validation: 0.00988079185522434]
	TIME [epoch: 7.84 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011009949662703099		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.011009949662703099 | validation: 0.011459262830974943]
	TIME [epoch: 7.84 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026480478124534017		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.026480478124534017 | validation: 0.012925820076033434]
	TIME [epoch: 7.88 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012254219054241596		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.012254219054241596 | validation: 0.015432363420060775]
	TIME [epoch: 7.85 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01406542432222203		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.01406542432222203 | validation: 0.02517592267117525]
	TIME [epoch: 7.84 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015380491071275901		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.015380491071275901 | validation: 0.015349889055860644]
	TIME [epoch: 7.84 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014274067187440793		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.014274067187440793 | validation: 0.007967095235322678]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_326.pth
	Model improved!!!
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013683038119971742		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.013683038119971742 | validation: 0.027705023158590233]
	TIME [epoch: 7.89 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035212011435479335		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.035212011435479335 | validation: 0.015312073553147227]
	TIME [epoch: 7.84 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01609681118125799		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.01609681118125799 | validation: 0.010722853732324787]
	TIME [epoch: 7.84 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012772757729090374		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.012772757729090374 | validation: 0.007529085300510483]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_330.pth
	Model improved!!!
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009585178145040205		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.009585178145040205 | validation: 0.021109662885680718]
	TIME [epoch: 7.85 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02846749636851828		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.02846749636851828 | validation: 0.0214782469763308]
	TIME [epoch: 7.88 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015596976107720083		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.015596976107720083 | validation: 0.009747860036589864]
	TIME [epoch: 7.85 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00948665175736831		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.00948665175736831 | validation: 0.013592425075632015]
	TIME [epoch: 7.84 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010144609751956482		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.010144609751956482 | validation: 0.007830768727071953]
	TIME [epoch: 7.84 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014166527992515134		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.014166527992515134 | validation: 0.016642815090280295]
	TIME [epoch: 7.87 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01527133597591044		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.01527133597591044 | validation: 0.008356292266564806]
	TIME [epoch: 7.87 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014606959307110388		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.014606959307110388 | validation: 0.027091931393936952]
	TIME [epoch: 7.84 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03612697404022418		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.03612697404022418 | validation: 0.0227107009242865]
	TIME [epoch: 7.84 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017518490294659357		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.017518490294659357 | validation: 0.010056355446681115]
	TIME [epoch: 7.83 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010207172479263855		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.010207172479263855 | validation: 0.007483544021681735]
	TIME [epoch: 7.88 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_341.pth
	Model improved!!!
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008658686240333554		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.008658686240333554 | validation: 0.008158793760216575]
	TIME [epoch: 7.85 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010388042121539345		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.010388042121539345 | validation: 0.015702534433610364]
	TIME [epoch: 7.84 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014615227420074185		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.014615227420074185 | validation: 0.011993127803458107]
	TIME [epoch: 7.84 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009492522543056613		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.009492522543056613 | validation: 0.012164379509815242]
	TIME [epoch: 7.84 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03869607116011149		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.03869607116011149 | validation: 0.018926300897497028]
	TIME [epoch: 7.9 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01889653158150509		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.01889653158150509 | validation: 0.008169581688722453]
	TIME [epoch: 7.85 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009802593760808564		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.009802593760808564 | validation: 0.0074611232554636355]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_348.pth
	Model improved!!!
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00936148578826886		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.00936148578826886 | validation: 0.014967954387523744]
	TIME [epoch: 7.84 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02128506238572169		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.02128506238572169 | validation: 0.00785306695531633]
	TIME [epoch: 7.85 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008993208411407418		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.008993208411407418 | validation: 0.012409661633171393]
	TIME [epoch: 7.88 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014704431872800815		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.014704431872800815 | validation: 0.04272804188596193]
	TIME [epoch: 7.84 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02768238748569722		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.02768238748569722 | validation: 0.010425151898878481]
	TIME [epoch: 7.84 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011817863776243897		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.011817863776243897 | validation: 0.012344388941454454]
	TIME [epoch: 7.85 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012780249804659397		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.012780249804659397 | validation: 0.01037233931136596]
	TIME [epoch: 7.89 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008872515962361287		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.008872515962361287 | validation: 0.008501960778152106]
	TIME [epoch: 7.86 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009646343344233133		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.009646343344233133 | validation: 0.012495023627940395]
	TIME [epoch: 7.84 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018682191449643837		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.018682191449643837 | validation: 0.010043553786872054]
	TIME [epoch: 7.84 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01033363942198168		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.01033363942198168 | validation: 0.010832595736361385]
	TIME [epoch: 7.84 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01192853393354416		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.01192853393354416 | validation: 0.017481406422461196]
	TIME [epoch: 7.89 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023861921346229217		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.023861921346229217 | validation: 0.013037280285971575]
	TIME [epoch: 7.85 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009489816764251363		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.009489816764251363 | validation: 0.008900824888387227]
	TIME [epoch: 7.84 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011998493644599744		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.011998493644599744 | validation: 0.01084006974562612]
	TIME [epoch: 7.84 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009170437362901365		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.009170437362901365 | validation: 0.01346242700759729]
	TIME [epoch: 7.85 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016203085983564697		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.016203085983564697 | validation: 0.010265282992144421]
	TIME [epoch: 7.89 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01211375704260241		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.01211375704260241 | validation: 0.011457996838625608]
	TIME [epoch: 7.84 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009854350379666858		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.009854350379666858 | validation: 0.009591822265252452]
	TIME [epoch: 7.84 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011962056914459243		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.011962056914459243 | validation: 0.014580147723439851]
	TIME [epoch: 7.84 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025147351400023076		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.025147351400023076 | validation: 0.016315718550063564]
	TIME [epoch: 7.85 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012619187667011386		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.012619187667011386 | validation: 0.009925562289531186]
	TIME [epoch: 7.88 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007926665115987596		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.007926665115987596 | validation: 0.009555879185861847]
	TIME [epoch: 7.85 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01812073982023265		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.01812073982023265 | validation: 0.0066811531881748425]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_372.pth
	Model improved!!!
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008200553241639458		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.008200553241639458 | validation: 0.008216753989959753]
	TIME [epoch: 7.84 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00890920303724117		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.00890920303724117 | validation: 0.008398040155104617]
	TIME [epoch: 7.87 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008323582711821577		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.008323582711821577 | validation: 0.022114310156498035]
	TIME [epoch: 7.85 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017515804844470195		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.017515804844470195 | validation: 0.007012818125861051]
	TIME [epoch: 7.83 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010783414197262952		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.010783414197262952 | validation: 0.011021587993852167]
	TIME [epoch: 7.84 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01099281478739785		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.01099281478739785 | validation: 0.006822330554400727]
	TIME [epoch: 7.84 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009780544800213224		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.009780544800213224 | validation: 0.007903251525252078]
	TIME [epoch: 7.88 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015657710858209613		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.015657710858209613 | validation: 0.012339144922670691]
	TIME [epoch: 7.84 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009133047682590918		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.009133047682590918 | validation: 0.005640737428733234]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_381.pth
	Model improved!!!
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0073170427357434915		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.0073170427357434915 | validation: 0.006630532366522611]
	TIME [epoch: 7.83 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011448221970285617		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.011448221970285617 | validation: 0.015265320500103262]
	TIME [epoch: 7.84 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022367124719665506		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.022367124719665506 | validation: 0.015415162150415941]
	TIME [epoch: 7.88 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008929057620506274		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.008929057620506274 | validation: 0.005900868261483737]
	TIME [epoch: 7.84 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007655201334900549		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.007655201334900549 | validation: 0.010784421784541037]
	TIME [epoch: 7.83 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015384145878899204		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.015384145878899204 | validation: 0.017939866450175992]
	TIME [epoch: 7.84 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01383570554793647		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.01383570554793647 | validation: 0.008255377067923297]
	TIME [epoch: 7.84 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009866808802289321		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.009866808802289321 | validation: 0.006603257720483141]
	TIME [epoch: 7.88 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011993899794018388		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.011993899794018388 | validation: 0.010208822598675976]
	TIME [epoch: 7.84 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010347873907214249		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.010347873907214249 | validation: 0.010231348067440929]
	TIME [epoch: 7.84 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011898815253715125		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.011898815253715125 | validation: 0.00962044062213978]
	TIME [epoch: 7.84 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010875559161218917		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.010875559161218917 | validation: 0.008904574008372725]
	TIME [epoch: 7.88 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024256969000751406		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.024256969000751406 | validation: 0.00725579306598677]
	TIME [epoch: 7.85 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00756833820861807		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.00756833820861807 | validation: 0.007444406977282581]
	TIME [epoch: 7.83 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007590596481270305		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.007590596481270305 | validation: 0.005619001184951499]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_396.pth
	Model improved!!!
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006618365636486864		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.006618365636486864 | validation: 0.006883034238413718]
	TIME [epoch: 7.84 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014026666128405035		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.014026666128405035 | validation: 0.008302295523731573]
	TIME [epoch: 7.89 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01548071754778035		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.01548071754778035 | validation: 0.012742470860615928]
	TIME [epoch: 7.84 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008641890535057758		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.008641890535057758 | validation: 0.008368738374601366]
	TIME [epoch: 7.83 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007679961589806649		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.007679961589806649 | validation: 0.005960092256790207]
	TIME [epoch: 7.84 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02429391331130071		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.02429391331130071 | validation: 0.01465391365590452]
	TIME [epoch: 7.85 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0159565107354191		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.0159565107354191 | validation: 0.005880064383807729]
	TIME [epoch: 7.88 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0060504710066123825		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.0060504710066123825 | validation: 0.006641945329372321]
	TIME [epoch: 7.83 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014836501311569425		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.014836501311569425 | validation: 0.008186837085491568]
	TIME [epoch: 7.84 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006507135019833311		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.006507135019833311 | validation: 0.007917829271214306]
	TIME [epoch: 7.84 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008915606433173288		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.008915606433173288 | validation: 0.007092709960078713]
	TIME [epoch: 7.86 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006635991806370008		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.006635991806370008 | validation: 0.012687712702033845]
	TIME [epoch: 7.87 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013283803096567095		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.013283803096567095 | validation: 0.007093309554525705]
	TIME [epoch: 7.85 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006181462564557295		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.006181462564557295 | validation: 0.005701517776621072]
	TIME [epoch: 7.83 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006327404541026512		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.006327404541026512 | validation: 0.005133207929248273]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_411.pth
	Model improved!!!
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006440263642994104		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.006440263642994104 | validation: 0.020614370641232695]
	TIME [epoch: 7.88 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016114929460158287		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.016114929460158287 | validation: 0.011097973913317878]
	TIME [epoch: 7.84 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007558647866736604		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.007558647866736604 | validation: 0.005817778876948952]
	TIME [epoch: 7.84 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007802393040359444		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.007802393040359444 | validation: 0.012489768842368932]
	TIME [epoch: 7.84 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009444777725808554		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.009444777725808554 | validation: 0.007906208103380854]
	TIME [epoch: 7.84 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015770709032661364		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.015770709032661364 | validation: 0.01248704679061643]
	TIME [epoch: 7.88 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008974293900619323		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.008974293900619323 | validation: 0.02282192514600686]
	TIME [epoch: 7.84 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01543052159434821		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.01543052159434821 | validation: 0.008640825559917959]
	TIME [epoch: 7.83 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008384022935394569		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.008384022935394569 | validation: 0.011302645253485528]
	TIME [epoch: 7.84 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010324065737615528		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.010324065737615528 | validation: 0.009274056787185556]
	TIME [epoch: 7.86 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011219342691604139		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.011219342691604139 | validation: 0.012887919100273938]
	TIME [epoch: 7.89 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012107749034680485		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.012107749034680485 | validation: 0.008731356532171363]
	TIME [epoch: 7.84 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008631928328949184		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.008631928328949184 | validation: 0.00823386678968397]
	TIME [epoch: 7.84 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006824803259232147		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.006824803259232147 | validation: 0.008553662457021948]
	TIME [epoch: 7.85 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009091639142175359		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.009091639142175359 | validation: 0.0057814168179029166]
	TIME [epoch: 7.87 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006162840495334257		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.006162840495334257 | validation: 0.008436821087917056]
	TIME [epoch: 7.87 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007086000594287997		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.007086000594287997 | validation: 0.007102474918233756]
	TIME [epoch: 7.84 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006273314173876894		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.006273314173876894 | validation: 0.005200771689850183]
	TIME [epoch: 7.85 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007971231336821986		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.007971231336821986 | validation: 0.017800981079624044]
	TIME [epoch: 7.85 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018866937946434015		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.018866937946434015 | validation: 0.011877024562015856]
	TIME [epoch: 7.89 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009734966268640741		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.009734966268640741 | validation: 0.004446803893486978]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_432.pth
	Model improved!!!
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005679416880542736		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.005679416880542736 | validation: 0.004730329218965333]
	TIME [epoch: 7.84 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004856943510056181		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.004856943510056181 | validation: 0.003376097183018871]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_434.pth
	Model improved!!!
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007278871237308196		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.007278871237308196 | validation: 0.007817438647494868]
	TIME [epoch: 7.86 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011137314890743572		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.011137314890743572 | validation: 0.009596774047994387]
	TIME [epoch: 7.89 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009449700980171135		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.009449700980171135 | validation: 0.00466879104289851]
	TIME [epoch: 7.84 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005808647236229668		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.005808647236229668 | validation: 0.006756527301366943]
	TIME [epoch: 7.84 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007798607405572738		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.007798607405572738 | validation: 0.005130467903000453]
	TIME [epoch: 7.84 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008558672349859579		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.008558672349859579 | validation: 0.005595389012179163]
	TIME [epoch: 7.85 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005573997299107163		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.005573997299107163 | validation: 0.008603365904557297]
	TIME [epoch: 7.87 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007499607021187516		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.007499607021187516 | validation: 0.008216242446905097]
	TIME [epoch: 7.84 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009143147595555809		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.009143147595555809 | validation: 0.01030820425841571]
	TIME [epoch: 7.84 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009632247314546908		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.009632247314546908 | validation: 0.005321782181494931]
	TIME [epoch: 7.84 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00681307420913803		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.00681307420913803 | validation: 0.0077111455015859915]
	TIME [epoch: 7.88 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006723326523878078		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.006723326523878078 | validation: 0.005194211236152185]
	TIME [epoch: 7.85 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005489380875574682		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.005489380875574682 | validation: 0.005216967828610114]
	TIME [epoch: 7.83 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0064658711811721266		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.0064658711811721266 | validation: 0.009911618997775568]
	TIME [epoch: 7.84 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007581475414459096		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.007581475414459096 | validation: 0.005905768946964016]
	TIME [epoch: 7.83 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0062805807836385525		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.0062805807836385525 | validation: 0.010273509134939792]
	TIME [epoch: 7.89 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01242069518957379		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.01242069518957379 | validation: 0.012718433592415028]
	TIME [epoch: 7.84 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009567768048833416		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.009567768048833416 | validation: 0.00538099476538232]
	TIME [epoch: 7.84 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009520221286496301		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.009520221286496301 | validation: 0.005213453646777963]
	TIME [epoch: 7.84 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006965397119827646		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.006965397119827646 | validation: 0.011447009668383523]
	TIME [epoch: 7.84 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012073259139273396		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.012073259139273396 | validation: 0.010038970053596365]
	TIME [epoch: 7.89 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009106695525048211		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.009106695525048211 | validation: 0.004215488226787365]
	TIME [epoch: 7.85 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005330826243474655		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.005330826243474655 | validation: 0.006089782591811213]
	TIME [epoch: 7.84 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011125990257394311		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.011125990257394311 | validation: 0.0038439593839253135]
	TIME [epoch: 7.84 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004676787341647085		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.004676787341647085 | validation: 0.005139586393129137]
	TIME [epoch: 7.85 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011098984350815647		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.011098984350815647 | validation: 0.00894818396205516]
	TIME [epoch: 7.88 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009921383912663133		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.009921383912663133 | validation: 0.004457349118587359]
	TIME [epoch: 7.84 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004378827343313268		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.004378827343313268 | validation: 0.003610474238001254]
	TIME [epoch: 7.84 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0064023296068521154		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.0064023296068521154 | validation: 0.006212986661206609]
	TIME [epoch: 7.83 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01000579211068391		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.01000579211068391 | validation: 0.012396343250723706]
	TIME [epoch: 7.86 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010705748605505692		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.010705748605505692 | validation: 0.005575266851884289]
	TIME [epoch: 7.88 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0045319344029849445		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.0045319344029849445 | validation: 0.0041775029917852195]
	TIME [epoch: 7.84 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004456817436294577		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.004456817436294577 | validation: 0.004942484482101807]
	TIME [epoch: 7.84 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004603006010396087		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.004603006010396087 | validation: 0.00666450366584603]
	TIME [epoch: 7.83 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004668454967000635		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.004668454967000635 | validation: 0.004508833457684623]
	TIME [epoch: 7.87 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010507904467506154		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.010507904467506154 | validation: 0.005278959857207292]
	TIME [epoch: 7.84 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007162668052862461		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.007162668052862461 | validation: 0.007060730773659854]
	TIME [epoch: 7.84 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005944774974514283		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.005944774974514283 | validation: 0.003774163731663325]
	TIME [epoch: 7.83 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0060330591741373455		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.0060330591741373455 | validation: 0.006751948424437232]
	TIME [epoch: 7.83 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01111567483896608		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.01111567483896608 | validation: 0.016101925014686977]
	TIME [epoch: 7.88 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008816908789319379		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.008816908789319379 | validation: 0.0052382938426623945]
	TIME [epoch: 7.84 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006218864637322958		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.006218864637322958 | validation: 0.004932416349525267]
	TIME [epoch: 7.83 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0042509469550694005		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.0042509469550694005 | validation: 0.00294681354613514]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_477.pth
	Model improved!!!
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004426792827507098		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.004426792827507098 | validation: 0.007659222835227061]
	TIME [epoch: 7.85 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00897040175395174		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.00897040175395174 | validation: 0.006509138672573618]
	TIME [epoch: 7.88 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005658102171084625		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.005658102171084625 | validation: 0.006148135503517216]
	TIME [epoch: 7.83 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00814663865670447		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.00814663865670447 | validation: 0.0029116365838154044]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_481.pth
	Model improved!!!
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003479277173976859		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.003479277173976859 | validation: 0.009724895376862962]
	TIME [epoch: 7.83 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016697851890832845		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.016697851890832845 | validation: 0.008912795913172832]
	TIME [epoch: 7.87 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009481630340178031		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.009481630340178031 | validation: 0.005876130091855045]
	TIME [epoch: 7.84 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005555777278678483		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.005555777278678483 | validation: 0.002375192336750267]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_485.pth
	Model improved!!!
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006345258251567147		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.006345258251567147 | validation: 0.005959406006651139]
	TIME [epoch: 7.83 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011896383139732791		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.011896383139732791 | validation: 0.0034804051449428626]
	TIME [epoch: 7.83 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006958550254767445		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.006958550254767445 | validation: 0.004594386470888859]
	TIME [epoch: 7.89 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004660397639108266		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.004660397639108266 | validation: 0.004351984262491979]
	TIME [epoch: 7.84 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004836770141894449		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.004836770141894449 | validation: 0.00765444533917891]
	TIME [epoch: 7.85 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006478586064915491		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.006478586064915491 | validation: 0.004725925266744986]
	TIME [epoch: 7.84 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003150524272511687		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.003150524272511687 | validation: 0.00443286160614444]
	TIME [epoch: 7.84 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003268527506697061		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.003268527506697061 | validation: 0.0017359380306043962]
	TIME [epoch: 7.88 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_493.pth
	Model improved!!!
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003928098059010657		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.003928098059010657 | validation: 0.0031867229786852157]
	TIME [epoch: 7.86 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0076727134597238695		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.0076727134597238695 | validation: 0.006010798848864721]
	TIME [epoch: 7.85 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004073591333418315		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.004073591333418315 | validation: 0.003444503825026464]
	TIME [epoch: 7.85 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004097616288069824		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.004097616288069824 | validation: 0.0051147286404600945]
	TIME [epoch: 7.87 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00395262009296572		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.00395262009296572 | validation: 0.0035165378791734408]
	TIME [epoch: 7.87 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025179403965190313		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.0025179403965190313 | validation: 0.012197633538392996]
	TIME [epoch: 7.85 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00939887446356887		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.00939887446356887 | validation: 0.002782781902973393]
	TIME [epoch: 7.85 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00326088787539367		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.00326088787539367 | validation: 0.0034725026472348887]
	TIME [epoch: 7.85 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005980159083471527		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.005980159083471527 | validation: 0.006518575321312381]
	TIME [epoch: 7.89 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004799106153809131		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.004799106153809131 | validation: 0.002150790307480643]
	TIME [epoch: 7.85 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002604317197671591		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.002604317197671591 | validation: 0.010339442936782399]
	TIME [epoch: 7.85 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005808965113349371		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.005808965113349371 | validation: 0.004464374868806326]
	TIME [epoch: 7.85 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036231784538582673		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.0036231784538582673 | validation: 0.006491559977456733]
	TIME [epoch: 7.85 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003554254508163456		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.003554254508163456 | validation: 0.0030226565845594636]
	TIME [epoch: 7.9 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005337135586739682		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.005337135586739682 | validation: 0.0032990160623456895]
	TIME [epoch: 7.84 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028635030152587403		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.0028635030152587403 | validation: 0.0034543278745206418]
	TIME [epoch: 7.84 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0054240853156156486		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.0054240853156156486 | validation: 0.00820331925239849]
	TIME [epoch: 7.84 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01806929480868511		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.01806929480868511 | validation: 0.07066354316808737]
	TIME [epoch: 7.85 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05760878315112047		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.05760878315112047 | validation: 0.007061594776344795]
	TIME [epoch: 7.87 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008040763603545185		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.008040763603545185 | validation: 0.004536018609928206]
	TIME [epoch: 7.84 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005319908113327133		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.005319908113327133 | validation: 0.004352768983282692]
	TIME [epoch: 7.84 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004624059865029036		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.004624059865029036 | validation: 0.002636774940961166]
	TIME [epoch: 7.84 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002858506864723215		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.002858506864723215 | validation: 0.0016365833555451324]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_516.pth
	Model improved!!!
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034325636855286152		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.0034325636855286152 | validation: 0.003427797931238187]
	TIME [epoch: 7.86 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034235876653019087		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.0034235876653019087 | validation: 0.00287045629109003]
	TIME [epoch: 7.83 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002372292763423946		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.002372292763423946 | validation: 0.0012889446130790595]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_519.pth
	Model improved!!!
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020554511602183458		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.0020554511602183458 | validation: 0.0022636191621519222]
	TIME [epoch: 7.85 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030712661025103336		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.0030712661025103336 | validation: 0.0019502340827554235]
	TIME [epoch: 7.89 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00260244856374425		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.00260244856374425 | validation: 0.0014121876695571635]
	TIME [epoch: 7.86 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002511786860458642		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.002511786860458642 | validation: 0.00265264714540295]
	TIME [epoch: 7.84 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004746907971517509		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.004746907971517509 | validation: 0.003450440115694505]
	TIME [epoch: 7.85 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004768607227935429		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.004768607227935429 | validation: 0.004632548524332153]
	TIME [epoch: 7.85 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00389539086468215		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.00389539086468215 | validation: 0.0027234736505105266]
	TIME [epoch: 7.89 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038239022839299435		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.0038239022839299435 | validation: 0.003674476630861137]
	TIME [epoch: 7.85 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009198161571031482		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.009198161571031482 | validation: 0.014509768650200754]
	TIME [epoch: 7.84 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015382262569629454		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.015382262569629454 | validation: 0.0029438704206343116]
	TIME [epoch: 7.84 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002527940796980643		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.002527940796980643 | validation: 0.0008448689667572026]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_530.pth
	Model improved!!!
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008935011569847586		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.0008935011569847586 | validation: 0.0017831406499537108]
	TIME [epoch: 7.89 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010810447289386353		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.0010810447289386353 | validation: 0.0020699338748086226]
	TIME [epoch: 7.84 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002498367544652553		[learning rate: 0.0018085]
	Learning Rate: 0.00180845
	LOSS [training: 0.002498367544652553 | validation: 0.002076958941259512]
	TIME [epoch: 7.84 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021419133947025537		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.0021419133947025537 | validation: 0.0006429123596108104]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_534.pth
	Model improved!!!
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0066673356431171515		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.0066673356431171515 | validation: 0.005047494836562555]
	TIME [epoch: 7.89 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004167696775710177		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.004167696775710177 | validation: 0.0021716282355693996]
	TIME [epoch: 7.84 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016373422581678716		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.0016373422581678716 | validation: 0.0002893136237162]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_537.pth
	Model improved!!!
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004114200365461447		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.004114200365461447 | validation: 0.003552616737944321]
	TIME [epoch: 7.84 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022631113343076804		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.0022631113343076804 | validation: 0.0015065027774924262]
	TIME [epoch: 7.84 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018641185823722937		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.0018641185823722937 | validation: 0.0025419240573523244]
	TIME [epoch: 7.88 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002431239599558133		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.002431239599558133 | validation: 0.0026884913637799006]
	TIME [epoch: 7.84 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003560416496945388		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.003560416496945388 | validation: 0.002048340357091463]
	TIME [epoch: 7.84 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0063697153070181225		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.0063697153070181225 | validation: 0.005026573018816193]
	TIME [epoch: 7.84 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032357091722323914		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.0032357091722323914 | validation: 0.0011185127411924786]
	TIME [epoch: 7.85 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009635196132557365		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.0009635196132557365 | validation: 0.0020159569930221256]
	TIME [epoch: 7.88 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002483740341744453		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.002483740341744453 | validation: 0.0075165842026077845]
	TIME [epoch: 7.85 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004303717128233733		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.004303717128233733 | validation: 0.006419512617841594]
	TIME [epoch: 7.85 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003922832611469354		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.003922832611469354 | validation: 0.0007140033648272155]
	TIME [epoch: 7.84 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00205439329488703		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.00205439329488703 | validation: 0.0011671356870441199]
	TIME [epoch: 7.89 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017304650073319801		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.0017304650073319801 | validation: 0.0021619659583758705]
	TIME [epoch: 7.86 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001998901252475209		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.001998901252475209 | validation: 0.0030594281634650066]
	TIME [epoch: 7.84 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018408837853700843		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.0018408837853700843 | validation: 0.004389307646524545]
	TIME [epoch: 7.85 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005600911924132491		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.005600911924132491 | validation: 0.0050420532066268045]
	TIME [epoch: 7.84 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006366999856111612		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.006366999856111612 | validation: 0.007187045193517856]
	TIME [epoch: 7.89 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033127998479153516		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.0033127998479153516 | validation: 0.00047011819025823836]
	TIME [epoch: 7.84 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007769198069993621		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.0007769198069993621 | validation: 0.0023276354602860664]
	TIME [epoch: 7.84 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009553825068943971		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.0009553825068943971 | validation: 0.0011152558072032007]
	TIME [epoch: 7.84 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012216214559604387		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.0012216214559604387 | validation: 0.0006097472522007439]
	TIME [epoch: 7.84 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00383691212187564		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.00383691212187564 | validation: 0.004428526555348335]
	TIME [epoch: 7.89 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004840319759992109		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.004840319759992109 | validation: 0.0007317100857269412]
	TIME [epoch: 7.87 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002520743835105207		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.002520743835105207 | validation: 0.0009764984828192569]
	TIME [epoch: 7.85 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023177941512902206		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.0023177941512902206 | validation: 0.0007024035207048614]
	TIME [epoch: 7.84 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0022238491672475456		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.0022238491672475456 | validation: 0.003111433968037043]
	TIME [epoch: 7.85 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031419340512481996		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.0031419340512481996 | validation: 0.002845989032979088]
	TIME [epoch: 7.87 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003007398169341187		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.003007398169341187 | validation: 0.0017313672578869845]
	TIME [epoch: 7.84 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001781478211903719		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.001781478211903719 | validation: 0.00069759068762353]
	TIME [epoch: 7.84 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009371025276881874		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.0009371025276881874 | validation: 0.001196185031619307]
	TIME [epoch: 7.83 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033977529064694208		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.0033977529064694208 | validation: 0.003739769487133225]
	TIME [epoch: 7.86 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013197960741785418		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.0013197960741785418 | validation: 0.00025948617673801654]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_569.pth
	Model improved!!!
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001570472182570874		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.001570472182570874 | validation: 0.0015848953901709943]
	TIME [epoch: 7.84 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006015860171374446		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.006015860171374446 | validation: 0.001207365618286626]
	TIME [epoch: 7.84 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013035787536092808		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.0013035787536092808 | validation: 0.0007573431061106129]
	TIME [epoch: 7.83 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008408164183429714		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.0008408164183429714 | validation: 0.0005498496666414639]
	TIME [epoch: 7.89 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006782214691024452		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.0006782214691024452 | validation: 0.001862330715966611]
	TIME [epoch: 7.84 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016509328148535366		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.0016509328148535366 | validation: -8.206835780487331e-05]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_575.pth
	Model improved!!!
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014655713357202562		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.0014655713357202562 | validation: 0.0021740903727223097]
	TIME [epoch: 7.83 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007942256796379561		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.0007942256796379561 | validation: 0.0013549651550245523]
	TIME [epoch: 7.84 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027617994595634256		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.0027617994595634256 | validation: 0.0007827770776989343]
	TIME [epoch: 7.89 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015852680362013437		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.0015852680362013437 | validation: 0.0038489303548174714]
	TIME [epoch: 7.84 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016914903569818542		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.0016914903569818542 | validation: 0.0023736726409479742]
	TIME [epoch: 7.84 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001406709389061915		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.001406709389061915 | validation: -0.0007722078458074115]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_581.pth
	Model improved!!!
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002137963194280747		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.002137963194280747 | validation: 0.006020777897932979]
	TIME [epoch: 7.88 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003058079088744034		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.003058079088744034 | validation: 0.0039753839780728665]
	TIME [epoch: 7.87 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028214154559135867		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.0028214154559135867 | validation: 0.0047154539833745195]
	TIME [epoch: 7.85 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031237551933480665		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.0031237551933480665 | validation: -0.0002061069613602515]
	TIME [epoch: 7.85 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008192423973074265		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.0008192423973074265 | validation: 0.001203774484808879]
	TIME [epoch: 7.86 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013584923602669493		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.0013584923602669493 | validation: 0.0002207515070062742]
	TIME [epoch: 7.9 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021371353227243608		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.0021371353227243608 | validation: 0.0011213410448977675]
	TIME [epoch: 7.87 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010659833434943245		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.0010659833434943245 | validation: 0.0015708058923241017]
	TIME [epoch: 7.85 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012835053078703734		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.0012835053078703734 | validation: 0.00039945659288974605]
	TIME [epoch: 7.85 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008485153695289291		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.0008485153695289291 | validation: 0.004162775012461911]
	TIME [epoch: 7.86 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0021527478423804592		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.0021527478423804592 | validation: 0.0030911033597027042]
	TIME [epoch: 7.9 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002631838352780987		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.002631838352780987 | validation: 0.0014458443761360923]
	TIME [epoch: 7.86 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001812697865008944		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.001812697865008944 | validation: 0.0012399159264366122]
	TIME [epoch: 7.85 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004616243655536547		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.0004616243655536547 | validation: 8.007991905879575e-05]
	TIME [epoch: 7.85 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002779656050182691		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.002779656050182691 | validation: 0.0013801868613907266]
	TIME [epoch: 7.86 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024690856152772694		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.0024690856152772694 | validation: 0.0011884788854414565]
	TIME [epoch: 7.9 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001652767417924409		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.001652767417924409 | validation: 0.0002455136317082993]
	TIME [epoch: 7.85 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026333866043212824		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.0026333866043212824 | validation: 0.004065051844699386]
	TIME [epoch: 7.85 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027061546751870275		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.0027061546751870275 | validation: 0.0008032657787759305]
	TIME [epoch: 7.85 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012094089437673704		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.0012094089437673704 | validation: 0.0003628212965894324]
	TIME [epoch: 7.89 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.441784018695887e-05		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 9.441784018695887e-05 | validation: 0.0005463309129269898]
	TIME [epoch: 7.85 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005408822768360095		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.0005408822768360095 | validation: 0.0013955471066720278]
	TIME [epoch: 7.85 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025024085372737496		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.0025024085372737496 | validation: 0.0012712109467522665]
	TIME [epoch: 7.84 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025792925817963164		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.0025792925817963164 | validation: 0.0006216058350537472]
	TIME [epoch: 7.84 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00018906037982956847		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.00018906037982956847 | validation: 0.0002475939362224584]
	TIME [epoch: 7.89 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0001667798392291291		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.0001667798392291291 | validation: 0.000729645680368217]
	TIME [epoch: 7.85 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001623220924195104		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.001623220924195104 | validation: 0.0023117284209216364]
	TIME [epoch: 7.84 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013902938663098328		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.0013902938663098328 | validation: 0.006193963241787226]
	TIME [epoch: 7.84 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003053364710989861		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.003053364710989861 | validation: 0.0014936185064294834]
	TIME [epoch: 7.84 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004685619265983972		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.0004685619265983972 | validation: 0.0009616110666024002]
	TIME [epoch: 7.89 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001771968069062159		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.001771968069062159 | validation: 0.0007467667491328136]
	TIME [epoch: 7.84 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002532827965328665		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.002532827965328665 | validation: 0.00016345986125558022]
	TIME [epoch: 7.84 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001045774134673995		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.001045774134673995 | validation: 0.0012176826271138178]
	TIME [epoch: 7.84 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006258086993013441		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.0006258086993013441 | validation: 0.000917321993309622]
	TIME [epoch: 7.85 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0011531298862306884		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.0011531298862306884 | validation: 0.000819846439440894]
	TIME [epoch: 7.88 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003739718869916473		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.003739718869916473 | validation: 0.0011349197410176425]
	TIME [epoch: 7.84 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009778549877130368		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.0009778549877130368 | validation: 0.0017809628960867817]
	TIME [epoch: 7.84 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006865484458014504		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.0006865484458014504 | validation: 0.001715417561228149]
	TIME [epoch: 7.84 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009675633854009716		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.0009675633854009716 | validation: 0.0027499873287960305]
	TIME [epoch: 7.88 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000307773167334178		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.000307773167334178 | validation: 0.002467217640748633]
	TIME [epoch: 7.87 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003194997353164299		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.003194997353164299 | validation: 0.0038877878840550177]
	TIME [epoch: 7.84 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014477647511464266		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.0014477647511464266 | validation: 0.0002101292591569526]
	TIME [epoch: 7.84 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007804312119625653		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.0007804312119625653 | validation: 0.0001704236153298737]
	TIME [epoch: 7.84 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008362753499182025		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.0008362753499182025 | validation: 0.00023871742721573642]
	TIME [epoch: 7.88 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008906197732885092		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.0008906197732885092 | validation: 0.0014247483234805175]
	TIME [epoch: 7.85 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008534358761027796		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.0008534358761027796 | validation: 5.099784349685624e-06]
	TIME [epoch: 7.84 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012624073391474555		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.0012624073391474555 | validation: 0.0005155563720338692]
	TIME [epoch: 7.84 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010137687814946203		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.0010137687814946203 | validation: -0.0012003449456645541]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_629.pth
	Model improved!!!
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006225085384728631		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.0006225085384728631 | validation: 0.001670648890825598]
	TIME [epoch: 7.9 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010146908108102659		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.0010146908108102659 | validation: 0.00013209154283357673]
	TIME [epoch: 7.85 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015508197774775907		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.0015508197774775907 | validation: 0.0005375467497997603]
	TIME [epoch: 7.84 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007407290511264892		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.0007407290511264892 | validation: 4.9458573771492544e-05]
	TIME [epoch: 7.85 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014600781615287104		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.0014600781615287104 | validation: 0.0001278035634240071]
	TIME [epoch: 7.85 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016431719715605134		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.0016431719715605134 | validation: 0.005458286487934494]
	TIME [epoch: 7.88 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032925328982468067		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.0032925328982468067 | validation: 0.0018811567992296136]
	TIME [epoch: 7.84 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014799400004321818		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.0014799400004321818 | validation: 0.0011517750540231803]
	TIME [epoch: 7.84 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: -1.009268330800047e-06		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: -1.009268330800047e-06 | validation: 0.0014430288854197003]
	TIME [epoch: 7.85 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020055533274406677		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.0020055533274406677 | validation: 0.0010295786942288907]
	TIME [epoch: 7.87 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00014092192601142576		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.00014092192601142576 | validation: 0.00042304874730653586]
	TIME [epoch: 7.87 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003801305194064573		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.003801305194064573 | validation: 4.9903420267158e-05]
	TIME [epoch: 7.84 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007466414611885077		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.0007466414611885077 | validation: 0.0004836096765661897]
	TIME [epoch: 7.84 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008592710817140894		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.0008592710817140894 | validation: 0.00016249929491129838]
	TIME [epoch: 7.84 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014883553894520108		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.0014883553894520108 | validation: 0.00019205499184618]
	TIME [epoch: 7.88 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00024029813777615527		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.00024029813777615527 | validation: 0.0011562335102766291]
	TIME [epoch: 7.85 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0012718571398172558		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.0012718571398172558 | validation: 0.0018041223837053945]
	TIME [epoch: 7.84 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0044997875775151675		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.0044997875775151675 | validation: 0.0009273938052211936]
	TIME [epoch: 7.85 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00043974546800633194		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.00043974546800633194 | validation: 0.0009807336212790886]
	TIME [epoch: 7.84 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006003443489664066		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.0006003443489664066 | validation: 0.0006624205458781191]
	TIME [epoch: 7.89 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: -4.4622053429058426e-05		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: -4.4622053429058426e-05 | validation: -0.0008268083166624005]
	TIME [epoch: 7.85 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: -2.9753319053401176e-05		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: -2.9753319053401176e-05 | validation: -0.00034775838593283174]
	TIME [epoch: 7.85 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010110479473184348		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.0010110479473184348 | validation: 0.0006289181263507673]
	TIME [epoch: 7.84 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008599390841457382		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.0008599390841457382 | validation: 0.0014571930786663012]
	TIME [epoch: 7.86 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013605993235404972		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.0013605993235404972 | validation: 0.00029383223929041555]
	TIME [epoch: 7.88 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00038604574760369673		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.00038604574760369673 | validation: -0.0002778188217970983]
	TIME [epoch: 7.84 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009003119232786205		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.0009003119232786205 | validation: 0.007094213911195109]
	TIME [epoch: 7.84 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004643518677016981		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.004643518677016981 | validation: -0.0001378449529150032]
	TIME [epoch: 7.87 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.0003839503400869102		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: -0.0003839503400869102 | validation: -0.0002378051580039844]
	TIME [epoch: 7.87 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007121300371065363		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.0007121300371065363 | validation: -0.0002798985153833562]
	TIME [epoch: 7.86 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004256450378751688		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.0004256450378751688 | validation: -3.575056840438018e-05]
	TIME [epoch: 7.84 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0004157057125234858		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.0004157057125234858 | validation: 0.0012907214290100986]
	TIME [epoch: 7.84 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005199890278446724		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.0005199890278446724 | validation: 0.0005095573094199053]
	TIME [epoch: 7.84 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009805394609737244		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.0009805394609737244 | validation: 0.0005791015737789965]
	TIME [epoch: 7.88 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005047958232434532		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.0005047958232434532 | validation: 0.011907443509758]
	TIME [epoch: 7.85 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011169343781323341		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.011169343781323341 | validation: -0.0004628398024244818]
	TIME [epoch: 7.84 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000669782981070626		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.000669782981070626 | validation: 0.0010467531184838311]
	TIME [epoch: 7.84 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003000537916458228		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.0003000537916458228 | validation: -0.0007736459066655099]
	TIME [epoch: 7.85 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.00010382416611222143		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: -0.00010382416611222143 | validation: 0.0005145456873024425]
	TIME [epoch: 7.9 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: -9.529719582642815e-05		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: -9.529719582642815e-05 | validation: -0.0008556199152787048]
	TIME [epoch: 7.85 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.00010674077818801519		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: -0.00010674077818801519 | validation: 0.0008881235271077853]
	TIME [epoch: 7.85 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00022373298769524276		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.00022373298769524276 | validation: -0.0006713911106502476]
	TIME [epoch: 7.84 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.0001034885209780403		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: -0.0001034885209780403 | validation: 0.0005756709821096169]
	TIME [epoch: 7.85 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006443010298584288		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.0006443010298584288 | validation: -0.00031438564588459036]
	TIME [epoch: 7.89 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0010393183229421487		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.0010393183229421487 | validation: -0.000335643636349527]
	TIME [epoch: 7.85 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: -1.3422572231362329e-05		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: -1.3422572231362329e-05 | validation: 0.0003882466478777422]
	TIME [epoch: 7.84 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008661376775447831		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.0008661376775447831 | validation: 0.0008279782957320495]
	TIME [epoch: 7.84 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002053797479561679		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.0002053797479561679 | validation: 0.0006759504179872744]
	TIME [epoch: 7.86 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014407379923680624		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.0014407379923680624 | validation: 0.003056320497468827]
	TIME [epoch: 7.88 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002200325884811766		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.002200325884811766 | validation: 6.305845998056014e-05]
	TIME [epoch: 7.84 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.659049544559182e-05		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 5.659049544559182e-05 | validation: 0.0007369748478118013]
	TIME [epoch: 7.85 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003787389703654682		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.0003787389703654682 | validation: -0.00029941293010590406]
	TIME [epoch: 7.84 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002048085268556541		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.0002048085268556541 | validation: -0.00026042186662718515]
	TIME [epoch: 7.89 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.144744285889895e-05		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 9.144744285889895e-05 | validation: -0.0005578430594333854]
	TIME [epoch: 7.84 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008091525673295849		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.0008091525673295849 | validation: -0.0007001537770325736]
	TIME [epoch: 7.84 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.00016537775318542034		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: -0.00016537775318542034 | validation: -0.00012060889004524315]
	TIME [epoch: 7.84 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002471831471659057		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.002471831471659057 | validation: -0.000359863879222357]
	TIME [epoch: 7.84 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0023705010909926315		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.0023705010909926315 | validation: 0.0016351129549034018]
	TIME [epoch: 7.88 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006541464307465057		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.0006541464307465057 | validation: -0.00044190399703249563]
	TIME [epoch: 7.84 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.00010357931647353411		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: -0.00010357931647353411 | validation: 0.005936602276852555]
	TIME [epoch: 7.84 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0020848154981795816		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.0020848154981795816 | validation: -0.0001131209360608003]
	TIME [epoch: 7.84 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018416844366024529		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.0018416844366024529 | validation: 0.00013343199590196386]
	TIME [epoch: 7.84 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005741657328309517		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.0005741657328309517 | validation: -0.000498832392253711]
	TIME [epoch: 7.89 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.0001562606963570152		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: -0.0001562606963570152 | validation: -0.0004450711332447574]
	TIME [epoch: 7.85 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00038041982137082965		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.00038041982137082965 | validation: 0.0002917395460839156]
	TIME [epoch: 7.83 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00017189987302831987		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.00017189987302831987 | validation: -7.571480745857472e-05]
	TIME [epoch: 7.83 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: -3.0410298448769308e-05		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: -3.0410298448769308e-05 | validation: 0.0009106047667143788]
	TIME [epoch: 7.85 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017545540044879653		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.0017545540044879653 | validation: -0.00014593383779134552]
	TIME [epoch: 7.88 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006124241083943675		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.0006124241083943675 | validation: -0.000665263330663473]
	TIME [epoch: 7.84 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00028001662025615136		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.00028001662025615136 | validation: -0.0007115114609545609]
	TIME [epoch: 7.84 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.0001895561663425258		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: -0.0001895561663425258 | validation: 0.0005826831460806212]
	TIME [epoch: 7.84 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.00011806595256489571		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: -0.00011806595256489571 | validation: -0.00020840441052008752]
	TIME [epoch: 7.89 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001685506827229706		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.001685506827229706 | validation: 0.005014633492945266]
	TIME [epoch: 7.85 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036666589293819474		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.0036666589293819474 | validation: -0.0006888901890920502]
	TIME [epoch: 7.84 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: -9.101551935956586e-05		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: -9.101551935956586e-05 | validation: 0.0013789317565710204]
	TIME [epoch: 7.84 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1201152594819732e-05		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 1.1201152594819732e-05 | validation: -9.687699372653124e-05]
	TIME [epoch: 7.83 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000287637415559576		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.000287637415559576 | validation: -0.0004789354265583375]
	TIME [epoch: 7.88 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0017005570646179806		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.0017005570646179806 | validation: 0.0035081807963761626]
	TIME [epoch: 7.84 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0016863204134201124		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.0016863204134201124 | validation: -0.0014498731557256476]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_708.pth
	Model improved!!!
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.0002921756496680204		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: -0.0002921756496680204 | validation: -0.0002918484310400076]
	TIME [epoch: 7.84 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00021485828864234067		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.00021485828864234067 | validation: 0.000348273871613809]
	TIME [epoch: 7.85 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013036658332898786		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.0013036658332898786 | validation: 0.0047880095099436595]
	TIME [epoch: 7.89 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002064096106730756		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.002064096106730756 | validation: 0.00010249965820129781]
	TIME [epoch: 7.85 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.141223880295369e-05		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 9.141223880295369e-05 | validation: -5.496696256786476e-05]
	TIME [epoch: 7.84 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.0001221994868425553		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: -0.0001221994868425553 | validation: 0.00033627983328230956]
	TIME [epoch: 7.85 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00013197004442805734		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.00013197004442805734 | validation: -0.0010833546900451893]
	TIME [epoch: 7.87 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.114793627080721e-05		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 9.114793627080721e-05 | validation: 0.0015751351594522989]
	TIME [epoch: 7.87 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002292069392520368		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.0002292069392520368 | validation: 8.663633886021849e-05]
	TIME [epoch: 7.84 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.0007541010816287088		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: -0.0007541010816287088 | validation: 0.0006408058704174078]
	TIME [epoch: 7.85 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00013014174143325153		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.00013014174143325153 | validation: 0.00028256525596823145]
	TIME [epoch: 7.83 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00020318139282536474		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.00020318139282536474 | validation: -0.0005064386217601178]
	TIME [epoch: 7.88 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025831353307682325		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.0025831353307682325 | validation: -0.00028528456717409205]
	TIME [epoch: 7.84 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00039352479943109025		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.00039352479943109025 | validation: 0.0006321692948329268]
	TIME [epoch: 7.83 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00041084760397237433		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.00041084760397237433 | validation: 0.0006113409059403683]
	TIME [epoch: 7.84 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: -8.221949602639782e-05		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: -8.221949602639782e-05 | validation: -0.00040373139588355353]
	TIME [epoch: 7.84 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.00010497550567591385		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: -0.00010497550567591385 | validation: 0.0008166086107665809]
	TIME [epoch: 7.89 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.00020408723399502305		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: -0.00020408723399502305 | validation: 4.410815434455933e-05]
	TIME [epoch: 7.84 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.00027395723937373705		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: -0.00027395723937373705 | validation: 0.0005455454164234603]
	TIME [epoch: 7.84 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: -8.284353184339777e-05		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: -8.284353184339777e-05 | validation: -0.0003652495390137749]
	TIME [epoch: 7.84 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.0005237178060773177		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: -0.0005237178060773177 | validation: -0.0012285100142307832]
	TIME [epoch: 7.84 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006330472175931925		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.0006330472175931925 | validation: 0.0033548240056879217]
	TIME [epoch: 7.89 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.001453487466529106		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.001453487466529106 | validation: -0.00024306418720783008]
	TIME [epoch: 7.84 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.0004140550093712212		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: -0.0004140550093712212 | validation: -5.353149079032082e-05]
	TIME [epoch: 7.83 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.00035002490741327635		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: -0.00035002490741327635 | validation: 0.0013256419997623157]
	TIME [epoch: 7.84 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008649196783716602		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.0008649196783716602 | validation: 1.9318941329595754e-05]
	TIME [epoch: 7.85 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005022815698075457		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.005022815698075457 | validation: 0.0017306527339873284]
	TIME [epoch: 7.88 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018753450376643756		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.0018753450376643756 | validation: 0.0012726214856407059]
	TIME [epoch: 7.83 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0009906991852921354		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.0009906991852921354 | validation: -0.0006294519341605263]
	TIME [epoch: 7.83 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.0004978800506381762		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: -0.0004978800506381762 | validation: -0.0007945508350104334]
	TIME [epoch: 7.84 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.0006497107501541373		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: -0.0006497107501541373 | validation: -0.0008221832030630418]
	TIME [epoch: 7.88 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006335302039697866		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.0006335302039697866 | validation: 0.0005984469405472677]
	TIME [epoch: 7.85 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007066341574685651		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.0007066341574685651 | validation: 2.690337835443659e-05]
	TIME [epoch: 7.84 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00014452801027553817		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.00014452801027553817 | validation: 0.004185997701125951]
	TIME [epoch: 7.84 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0045135992044095154		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.0045135992044095154 | validation: -0.00011160721293997569]
	TIME [epoch: 7.83 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.0007321485954961831		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: -0.0007321485954961831 | validation: 0.00012181591885544746]
	TIME [epoch: 7.89 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.000804707177873593		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: -0.000804707177873593 | validation: -0.000637713989055254]
	TIME [epoch: 7.84 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.580118908336874e-05		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 8.580118908336874e-05 | validation: 0.00026377676940360795]
	TIME [epoch: 7.84 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009440056221394889		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.009440056221394889 | validation: 0.005419093042598462]
	TIME [epoch: 7.84 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002620222725585219		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.002620222725585219 | validation: -9.805810227875078e-05]
	TIME [epoch: 7.83 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00020669605111113044		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.00020669605111113044 | validation: -0.00035838448626212524]
	TIME [epoch: 7.88 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.0003149171028253481		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: -0.0003149171028253481 | validation: -0.0007060594029073854]
	TIME [epoch: 7.84 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.0004049662698502538		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: -0.0004049662698502538 | validation: -0.00020855854929825092]
	TIME [epoch: 7.84 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.00041120561469658014		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: -0.00041120561469658014 | validation: -0.0009155323067810413]
	TIME [epoch: 7.84 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003224339002535316		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.003224339002535316 | validation: 0.00035822170139164023]
	TIME [epoch: 7.84 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007705146077049321		[learning rate: 0.00082662]
	Learning Rate: 0.000826623
	LOSS [training: 0.0007705146077049321 | validation: 0.0011671753329307991]
	TIME [epoch: 7.88 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003959597913536155		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.0003959597913536155 | validation: 6.766641760548131e-05]
	TIME [epoch: 7.84 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.00024080272030861427		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: -0.00024080272030861427 | validation: -0.0002486364950676543]
	TIME [epoch: 7.83 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: -9.782334347128119e-05		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: -9.782334347128119e-05 | validation: 0.0005204649168236575]
	TIME [epoch: 7.83 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006911589438919727		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.0006911589438919727 | validation: -0.00014611117456685908]
	TIME [epoch: 7.87 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00045299971630091034		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.00045299971630091034 | validation: -0.0004329642830472076]
	TIME [epoch: 7.86 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.0007082024739827031		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: -0.0007082024739827031 | validation: 0.0001531679630300329]
	TIME [epoch: 7.84 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0005358255787603576		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.0005358255787603576 | validation: 0.00037838359999976714]
	TIME [epoch: 7.83 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.795889475813157e-05		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 7.795889475813157e-05 | validation: 0.0007453545426807882]
	TIME [epoch: 7.84 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.00015439953893587498		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: -0.00015439953893587498 | validation: 0.0005187939318129842]
	TIME [epoch: 7.88 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.0005950550144957288		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: -0.0005950550144957288 | validation: 0.0007101327690886251]
	TIME [epoch: 7.84 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006239187126681268		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.0006239187126681268 | validation: -8.949451922892984e-05]
	TIME [epoch: 7.83 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.00016729538524188126		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: -0.00016729538524188126 | validation: -0.0002934289301368498]
	TIME [epoch: 7.83 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00023900620446734195		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.00023900620446734195 | validation: -0.0005450088498034692]
	TIME [epoch: 7.83 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003041550751991597		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.0003041550751991597 | validation: 0.0018308085117330645]
	TIME [epoch: 7.88 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0006568811543517337		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.0006568811543517337 | validation: -0.0006841498275407116]
	TIME [epoch: 7.83 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0007464987313282388		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.0007464987313282388 | validation: -6.0773625912562864e-05]
	TIME [epoch: 7.82 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003954392296600786		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.0003954392296600786 | validation: 6.625439848141523e-05]
	TIME [epoch: 7.83 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0013534365721896183		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.0013534365721896183 | validation: -0.0004320187424732542]
	TIME [epoch: 7.85 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0015222357468687993		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.0015222357468687993 | validation: 0.0038970697074024474]
	TIME [epoch: 7.89 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036585820876589084		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.0036585820876589084 | validation: -0.000567664627922468]
	TIME [epoch: 7.85 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.0007279663449093639		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: -0.0007279663449093639 | validation: -0.001324408461163946]
	TIME [epoch: 7.83 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.0008925602440210152		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: -0.0008925602440210152 | validation: -0.0002774526491112406]
	TIME [epoch: 7.84 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.0006224484916871675		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: -0.0006224484916871675 | validation: -0.0004046926652937311]
	TIME [epoch: 7.86 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.00022404726336691707		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: -0.00022404726336691707 | validation: -0.0002618275989997661]
	TIME [epoch: 7.87 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.861001153829304e-05		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 2.861001153829304e-05 | validation: -0.0007485233928514825]
	TIME [epoch: 7.83 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.450912733152991e-05		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 5.450912733152991e-05 | validation: 0.0003152795002135496]
	TIME [epoch: 7.84 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.0007482807408562954		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: -0.0007482807408562954 | validation: -6.855168399843903e-05]
	TIME [epoch: 7.83 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.0005296483126420459		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: -0.0005296483126420459 | validation: -0.000247358790015384]
	TIME [epoch: 7.88 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002943574772718827		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.0002943574772718827 | validation: -0.00039668552133193425]
	TIME [epoch: 7.84 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0002367928439287241		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.0002367928439287241 | validation: -0.00010580537878316231]
	TIME [epoch: 7.85 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.000535239150090771		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: -0.000535239150090771 | validation: 7.85953551122329e-05]
	TIME [epoch: 7.84 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.000321165538937251		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: -0.000321165538937251 | validation: 0.0037611235737449525]
	TIME [epoch: 7.83 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002769288784081886		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.002769288784081886 | validation: 0.00012064683995681238]
	TIME [epoch: 7.88 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0018458190240212549		[learning rate: 0.00073282]
	Learning Rate: 0.000732824
	LOSS [training: 0.0018458190240212549 | validation: 0.0038202447092835655]
	TIME [epoch: 7.84 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0014237148294440301		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.0014237148294440301 | validation: -0.00015841411962685384]
	TIME [epoch: 7.84 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000447914919824701		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.000447914919824701 | validation: -0.0005435281694600574]
	TIME [epoch: 7.83 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0008663942124595779		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.0008663942124595779 | validation: -0.00015124991148252853]
	TIME [epoch: 7.84 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.485421039441916e-05		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 3.485421039441916e-05 | validation: -0.00037331830411530963]
	TIME [epoch: 7.89 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.00035171389956262635		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: -0.00035171389956262635 | validation: -0.0008978059944682825]
	TIME [epoch: 7.84 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.00023647597484336287		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: -0.00023647597484336287 | validation: -0.0011905729615865428]
	TIME [epoch: 7.85 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.00046401579278653696		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: -0.00046401579278653696 | validation: -0.0003778935462261219]
	TIME [epoch: 7.84 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.0006579822732326084		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: -0.0006579822732326084 | validation: 8.710444784528005e-05]
	TIME [epoch: 7.86 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.00019002978574515542		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: -0.00019002978574515542 | validation: 0.00046878197156757166]
	TIME [epoch: 7.88 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00038459458335345613		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.00038459458335345613 | validation: -0.0006203346942836157]
	TIME [epoch: 7.83 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.000971936146168578		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.000971936146168578 | validation: 0.0016687872544910002]
	TIME [epoch: 7.83 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00037599391762709923		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.00037599391762709923 | validation: -0.000483872034064968]
	TIME [epoch: 7.84 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.0004666541217602529		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: -0.0004666541217602529 | validation: -0.0002876323196736821]
	TIME [epoch: 7.88 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.0004030830031399571		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: -0.0004030830031399571 | validation: -0.00029348675855203464]
	TIME [epoch: 7.84 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.0002019803577415642		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: -0.0002019803577415642 | validation: 0.0009680737200855392]
	TIME [epoch: 7.85 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.0003815844668930288		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: -0.0003815844668930288 | validation: 0.0007330536589104395]
	TIME [epoch: 7.84 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: -0.00024117772576469162		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: -0.00024117772576469162 | validation: -0.00030013639067363895]
	TIME [epoch: 7.84 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.150481427895116e-05		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 6.150481427895116e-05 | validation: -0.0001287592196864407]
	TIME [epoch: 7.88 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003127659815269841		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.0003127659815269841 | validation: -0.0008406992036864654]
	TIME [epoch: 7.84 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: -9.327551823581871e-05		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: -9.327551823581871e-05 | validation: -0.000709581778303575]
	TIME [epoch: 7.84 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0003483939066943682		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.0003483939066943682 | validation: 0.00012097746316878591]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20240627_193208/states/model_phi2_1a_v_mmd1_809.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 6546.663 seconds.
