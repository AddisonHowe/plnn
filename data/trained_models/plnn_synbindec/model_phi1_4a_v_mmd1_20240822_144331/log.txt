Args:
Namespace(name='model_phi1_4a_v_mmd1', outdir='out/model_training/model_phi1_4a_v_mmd1', training_data='data/training_data/data_phi1_4a/training', validation_data='data/training_data/data_phi1_4a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3906784628

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.773522957332664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.773522957332664 | validation: 6.617273836830626]
	TIME [epoch: 48.2 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.948025441618848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.948025441618848 | validation: 6.622594844387513]
	TIME [epoch: 0.897 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.755335225587104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.755335225587104 | validation: 6.139624743914993]
	TIME [epoch: 0.882 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.896628618136916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.896628618136916 | validation: 6.584691735763562]
	TIME [epoch: 0.875 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.969422659334554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.969422659334554 | validation: 6.5310461023763615]
	TIME [epoch: 0.872 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.8270418923078795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.8270418923078795 | validation: 6.25322745775279]
	TIME [epoch: 0.872 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.263391217801527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.263391217801527 | validation: 5.876975691647887]
	TIME [epoch: 0.874 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.409214917235704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.409214917235704 | validation: 5.988549332593397]
	TIME [epoch: 0.876 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.056984718081466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.056984718081466 | validation: 6.113963617581906]
	TIME [epoch: 0.875 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.060597289351093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.060597289351093 | validation: 5.991705957884991]
	TIME [epoch: 0.879 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9477500508771683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9477500508771683 | validation: 5.856005697867983]
	TIME [epoch: 0.875 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9158750234615782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9158750234615782 | validation: 5.901168058421499]
	TIME [epoch: 0.878 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8432944463522434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8432944463522434 | validation: 5.8723563880288365]
	TIME [epoch: 0.874 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7980169404325874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7980169404325874 | validation: 5.7945779857455015]
	TIME [epoch: 0.875 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7590688330353976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7590688330353976 | validation: 5.810430259495349]
	TIME [epoch: 0.883 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.720689778861878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.720689778861878 | validation: 5.7639199955339215]
	TIME [epoch: 0.87 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6915216671974016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6915216671974016 | validation: 5.760589571690444]
	TIME [epoch: 0.889 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6696031944925127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6696031944925127 | validation: 5.7309455439208365]
	TIME [epoch: 0.874 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6627822780639154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6627822780639154 | validation: 5.787194956677828]
	TIME [epoch: 0.873 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6869386656217094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6869386656217094 | validation: 5.672187121111552]
	TIME [epoch: 0.87 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.697948390611846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.697948390611846 | validation: 5.807092918025797]
	TIME [epoch: 0.873 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.722898154717683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.722898154717683 | validation: 5.663175338895293]
	TIME [epoch: 0.871 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5738535477487563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5738535477487563 | validation: 5.622685763096714]
	TIME [epoch: 0.872 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5888172950469066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5888172950469066 | validation: 5.705755917780284]
	TIME [epoch: 0.869 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6078375706055836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6078375706055836 | validation: 5.616071817894628]
	TIME [epoch: 0.868 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.541274732266751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.541274732266751 | validation: 5.6126462115121285]
	TIME [epoch: 0.869 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.519704569304156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.519704569304156 | validation: 5.599506251663594]
	TIME [epoch: 0.876 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5050559741530023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5050559741530023 | validation: 5.564271958112507]
	TIME [epoch: 0.877 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.487226008011945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.487226008011945 | validation: 5.566564443064365]
	TIME [epoch: 0.871 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4775993519511115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4775993519511115 | validation: 5.521963242549887]
	TIME [epoch: 0.872 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.471782275534033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.471782275534033 | validation: 5.545064518305334]
	TIME [epoch: 0.874 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4785297173922833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4785297173922833 | validation: 5.499318252060125]
	TIME [epoch: 0.872 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.496892705506315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.496892705506315 | validation: 5.524453547477598]
	TIME [epoch: 0.871 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5371799803145416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5371799803145416 | validation: 5.449479341658208]
	TIME [epoch: 0.867 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4034363987285166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4034363987285166 | validation: 5.420984646830416]
	TIME [epoch: 0.869 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.393360228422679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.393360228422679 | validation: 5.418378703225447]
	TIME [epoch: 0.868 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.406629148123667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.406629148123667 | validation: 5.392805641913757]
	TIME [epoch: 0.874 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.384466157376774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.384466157376774 | validation: 5.3654342495205904]
	TIME [epoch: 0.87 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3722384049793015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3722384049793015 | validation: 5.329759738962192]
	TIME [epoch: 0.872 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3326678296054766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3326678296054766 | validation: 5.295248588377479]
	TIME [epoch: 0.878 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.320013041235834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.320013041235834 | validation: 5.2867622715337]
	TIME [epoch: 0.877 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2986308514938303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2986308514938303 | validation: 5.2495480195685875]
	TIME [epoch: 0.876 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2879862721232658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2879862721232658 | validation: 5.243805683126865]
	TIME [epoch: 0.875 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.276198613232704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.276198613232704 | validation: 5.174285532375513]
	TIME [epoch: 0.867 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2825001442259008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2825001442259008 | validation: 5.166785723338501]
	TIME [epoch: 0.866 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2367930841283608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2367930841283608 | validation: 5.025715843687647]
	TIME [epoch: 0.871 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1890011167570727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1890011167570727 | validation: 4.866882221578297]
	TIME [epoch: 0.869 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.02848028308386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.02848028308386 | validation: 4.631979239711064]
	TIME [epoch: 0.863 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.821978959974932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.821978959974932 | validation: 4.437065629831911]
	TIME [epoch: 0.871 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6524791746813254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6524791746813254 | validation: 4.2087935735813815]
	TIME [epoch: 0.868 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4678727502932065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4678727502932065 | validation: 3.6835525130915054]
	TIME [epoch: 0.869 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.264361628803402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.264361628803402 | validation: 3.0851137763627245]
	TIME [epoch: 0.867 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.320413036538943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.320413036538943 | validation: 1.4847471762398927]
	TIME [epoch: 0.868 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3640059597853074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3640059597853074 | validation: 2.0187411820782275]
	TIME [epoch: 0.867 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1997148356775664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1997148356775664 | validation: 1.2707338147918072]
	TIME [epoch: 0.865 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4168152487207348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4168152487207348 | validation: 1.5012047968536302]
	TIME [epoch: 0.869 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.787749041198757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.787749041198757 | validation: 1.3775814714725352]
	TIME [epoch: 0.868 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2600610368205158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2600610368205158 | validation: 1.2063839865308559]
	TIME [epoch: 0.867 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.29472242464037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.29472242464037 | validation: 1.0382096943298618]
	TIME [epoch: 0.869 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.16794594348725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.16794594348725 | validation: 1.328819865545449]
	TIME [epoch: 0.877 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1638977302803513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1638977302803513 | validation: 1.0158847038165884]
	TIME [epoch: 0.871 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0782827067790302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0782827067790302 | validation: 0.9921609334159357]
	TIME [epoch: 0.873 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.061533635331217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.061533635331217 | validation: 1.0201003924646268]
	TIME [epoch: 0.878 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.03392773481328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.03392773481328 | validation: 0.957951526394782]
	TIME [epoch: 0.867 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0112328957939432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0112328957939432 | validation: 0.9907472507687436]
	TIME [epoch: 0.879 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9978515065462656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9978515065462656 | validation: 0.8694236183173756]
	TIME [epoch: 0.866 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9962385304736977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9962385304736977 | validation: 1.3469491552271746]
	TIME [epoch: 0.875 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1128682296929566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1128682296929566 | validation: 1.002210672867696]
	TIME [epoch: 0.87 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2222177802844452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2222177802844452 | validation: 1.4070097634777257]
	TIME [epoch: 0.87 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1204487140462502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1204487140462502 | validation: 0.9386368311096099]
	TIME [epoch: 0.87 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9589690255303353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9589690255303353 | validation: 0.7941781918207831]
	TIME [epoch: 0.868 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0308228561856796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0308228561856796 | validation: 1.077618773948819]
	TIME [epoch: 0.869 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.007200411545902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.007200411545902 | validation: 0.8448400360864661]
	TIME [epoch: 0.872 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9364209542258588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9364209542258588 | validation: 0.7118334863169142]
	TIME [epoch: 0.865 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9496193133426755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9496193133426755 | validation: 0.8980110883649293]
	TIME [epoch: 0.865 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9284072683039548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9284072683039548 | validation: 0.7944218426114431]
	TIME [epoch: 0.867 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9102012903827622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9102012903827622 | validation: 0.7629299268514419]
	TIME [epoch: 0.863 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9114308119215976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9114308119215976 | validation: 0.8264310415247668]
	TIME [epoch: 0.865 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9108765998981011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9108765998981011 | validation: 0.787182996258018]
	TIME [epoch: 0.865 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9189669976633235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9189669976633235 | validation: 0.8642058081734177]
	TIME [epoch: 0.865 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9394848367110641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9394848367110641 | validation: 0.9861401859176934]
	TIME [epoch: 0.863 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.041668313460794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.041668313460794 | validation: 0.886208284333775]
	TIME [epoch: 0.866 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0159536669120444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0159536669120444 | validation: 1.1562202622474604]
	TIME [epoch: 0.865 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0151594811310014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0151594811310014 | validation: 0.7300773268251284]
	TIME [epoch: 0.866 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9027605689774338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9027605689774338 | validation: 0.8595301843190077]
	TIME [epoch: 0.866 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8936603283880146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8936603283880146 | validation: 0.7324259628043197]
	TIME [epoch: 0.866 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8960605310348774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8960605310348774 | validation: 0.9081025135647322]
	TIME [epoch: 0.866 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9099260951685308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9099260951685308 | validation: 0.6720681279881339]
	TIME [epoch: 0.864 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9724190367397383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9724190367397383 | validation: 0.9914780891308074]
	TIME [epoch: 0.869 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9275979635248115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9275979635248115 | validation: 0.6449215150373528]
	TIME [epoch: 0.868 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9535593852152292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9535593852152292 | validation: 1.1199550146291841]
	TIME [epoch: 0.868 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9666290854093958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9666290854093958 | validation: 0.785584398814958]
	TIME [epoch: 0.869 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9665403773289707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9665403773289707 | validation: 1.0958215487779996]
	TIME [epoch: 0.869 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0108576106534857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0108576106534857 | validation: 0.8365459628978166]
	TIME [epoch: 0.868 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9046804346714328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9046804346714328 | validation: 0.6909996778588171]
	TIME [epoch: 0.87 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8972237997454673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8972237997454673 | validation: 0.8837905494918321]
	TIME [epoch: 0.878 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8888151395738493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8888151395738493 | validation: 0.6871356507195604]
	TIME [epoch: 0.867 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8841049003876806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8841049003876806 | validation: 0.841110239586953]
	TIME [epoch: 0.866 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8710885276613306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8710885276613306 | validation: 0.6908517891263445]
	TIME [epoch: 0.876 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8740327341249747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8740327341249747 | validation: 0.8483327626664802]
	TIME [epoch: 0.873 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8757622572133973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8757622572133973 | validation: 0.6749823446332971]
	TIME [epoch: 0.876 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9130508966120214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9130508966120214 | validation: 0.9210998309133643]
	TIME [epoch: 0.874 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9315202327687012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9315202327687012 | validation: 0.8385177609929028]
	TIME [epoch: 0.871 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0122242721637844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0122242721637844 | validation: 0.835904532484602]
	TIME [epoch: 0.872 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9799526438720344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9799526438720344 | validation: 1.3279480782998156]
	TIME [epoch: 0.871 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0882237918263327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0882237918263327 | validation: 0.6517014466877508]
	TIME [epoch: 0.871 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8711331803890829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8711331803890829 | validation: 0.7566983257513059]
	TIME [epoch: 0.871 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8938617230056477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8938617230056477 | validation: 1.0062338562915567]
	TIME [epoch: 0.872 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9578010899762341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9578010899762341 | validation: 0.7678169352045248]
	TIME [epoch: 0.87 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8647127199741153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8647127199741153 | validation: 0.6685887603992691]
	TIME [epoch: 0.87 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8809477011527169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8809477011527169 | validation: 0.8893285382857421]
	TIME [epoch: 0.874 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8824123052736087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8824123052736087 | validation: 0.6334068811357096]
	TIME [epoch: 0.871 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9009122859126751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9009122859126751 | validation: 0.8557110278456291]
	TIME [epoch: 0.87 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8717997529430869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8717997529430869 | validation: 0.6647401547026256]
	TIME [epoch: 0.87 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.885760449841812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.885760449841812 | validation: 0.8156377806817731]
	TIME [epoch: 0.87 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8742515682583454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8742515682583454 | validation: 0.8238462659639705]
	TIME [epoch: 0.871 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9175134948354307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9175134948354307 | validation: 0.8493449219076226]
	TIME [epoch: 0.869 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9604589284564127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9604589284564127 | validation: 1.131257077601872]
	TIME [epoch: 0.869 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.023384103924663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.023384103924663 | validation: 0.6937160990683789]
	TIME [epoch: 0.869 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8338818478164683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8338818478164683 | validation: 0.6916529607831992]
	TIME [epoch: 0.869 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8378287272968299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8378287272968299 | validation: 0.8001087389716202]
	TIME [epoch: 0.87 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8618881225867233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8618881225867233 | validation: 0.7548811459203284]
	TIME [epoch: 0.87 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8828163631566048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8828163631566048 | validation: 0.8625170537917275]
	TIME [epoch: 0.872 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9595643891618636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9595643891618636 | validation: 0.8185675789659737]
	TIME [epoch: 0.865 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9016697071298532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9016697071298532 | validation: 0.6918662743762776]
	TIME [epoch: 0.867 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9193399727388315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9193399727388315 | validation: 0.818693812819922]
	TIME [epoch: 0.866 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8630156484620192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8630156484620192 | validation: 0.644518277060253]
	TIME [epoch: 0.865 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8692151090617078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8692151090617078 | validation: 0.8718646390737202]
	TIME [epoch: 0.867 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8556909882441861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8556909882441861 | validation: 0.6090290772056999]
	TIME [epoch: 0.867 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_129.pth
	Model improved!!!
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9141933455665796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9141933455665796 | validation: 1.0896300295955286]
	TIME [epoch: 0.869 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9518877578021367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9518877578021367 | validation: 0.6143800409269874]
	TIME [epoch: 0.869 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8760103987442154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8760103987442154 | validation: 0.8084090430479359]
	TIME [epoch: 0.88 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8368000364112319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8368000364112319 | validation: 0.7432372709384858]
	TIME [epoch: 0.869 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8314292351904896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8314292351904896 | validation: 0.7447550139573297]
	TIME [epoch: 0.87 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9003995344604089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9003995344604089 | validation: 0.8657454047252261]
	TIME [epoch: 0.879 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9430835886024568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9430835886024568 | validation: 0.8988490920912753]
	TIME [epoch: 0.868 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9962806168676668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9962806168676668 | validation: 0.7014540844256216]
	TIME [epoch: 0.868 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8350162677162193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8350162677162193 | validation: 0.7708350720901548]
	TIME [epoch: 0.871 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8167012754603766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8167012754603766 | validation: 0.6162748989578972]
	TIME [epoch: 0.867 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8358335345984313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8358335345984313 | validation: 0.9279455979583955]
	TIME [epoch: 0.867 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8853480545432921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8853480545432921 | validation: 0.5924798650689443]
	TIME [epoch: 0.866 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_141.pth
	Model improved!!!
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9343007043536173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9343007043536173 | validation: 0.8541178386368392]
	TIME [epoch: 0.872 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8446943873694519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8446943873694519 | validation: 0.6445443298612036]
	TIME [epoch: 0.869 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.801600745165955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.801600745165955 | validation: 0.6885602634819997]
	TIME [epoch: 0.87 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7979972242990545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7979972242990545 | validation: 0.733563542406974]
	TIME [epoch: 0.868 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8069671262640253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8069671262640253 | validation: 0.7306432604517663]
	TIME [epoch: 0.87 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8621189557249741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8621189557249741 | validation: 1.1797447925037277]
	TIME [epoch: 0.868 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1659551134436754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1659551134436754 | validation: 0.7741326160514768]
	TIME [epoch: 0.87 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8681912670071392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8681912670071392 | validation: 0.6055390637530706]
	TIME [epoch: 0.868 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8592144215185111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8592144215185111 | validation: 0.8098421269123786]
	TIME [epoch: 0.869 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8394223929537435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8394223929537435 | validation: 0.610624462161535]
	TIME [epoch: 0.871 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.837922799495054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.837922799495054 | validation: 0.750765356404047]
	TIME [epoch: 0.867 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7936934552207302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7936934552207302 | validation: 0.6309685737675438]
	TIME [epoch: 0.867 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7996661420331427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7996661420331427 | validation: 0.7416589163958381]
	TIME [epoch: 0.869 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7976020897083794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7976020897083794 | validation: 0.6129614225452145]
	TIME [epoch: 0.867 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8248399341644373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8248399341644373 | validation: 1.127766502015163]
	TIME [epoch: 0.867 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9552880179855332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9552880179855332 | validation: 0.6778766362526318]
	TIME [epoch: 0.867 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8874101782493629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8874101782493629 | validation: 0.870809709753446]
	TIME [epoch: 0.867 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9285120885751939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9285120885751939 | validation: 0.8856910343847194]
	TIME [epoch: 0.867 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9957273729430823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9957273729430823 | validation: 0.6097910528523198]
	TIME [epoch: 0.867 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9832436680742501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9832436680742501 | validation: 0.6520464581296276]
	TIME [epoch: 0.868 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7691274845811362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7691274845811362 | validation: 0.8327156536112338]
	TIME [epoch: 0.867 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8674539759619543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8674539759619543 | validation: 0.6410843162830928]
	TIME [epoch: 0.867 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9016456536509461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9016456536509461 | validation: 0.6627359540341344]
	TIME [epoch: 0.868 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8036211454023757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8036211454023757 | validation: 0.9949722003267303]
	TIME [epoch: 0.868 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8903620192376909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8903620192376909 | validation: 0.6286706271179544]
	TIME [epoch: 0.867 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8222006369007131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8222006369007131 | validation: 0.7553017528719967]
	TIME [epoch: 0.867 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8019461980878687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8019461980878687 | validation: 0.6925970035136402]
	TIME [epoch: 0.868 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8120757160563216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8120757160563216 | validation: 0.7102745450024042]
	TIME [epoch: 0.879 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8894507798507965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8894507798507965 | validation: 0.7733800040673113]
	TIME [epoch: 0.867 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.871878473130694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.871878473130694 | validation: 0.8785760401437845]
	TIME [epoch: 0.866 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8964882974428509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8964882974428509 | validation: 0.6681136494146381]
	TIME [epoch: 0.873 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8015462132943785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8015462132943785 | validation: 0.7085793142336704]
	TIME [epoch: 0.875 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.793390041067357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.793390041067357 | validation: 0.7120517719470933]
	TIME [epoch: 0.87 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7986626806445513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7986626806445513 | validation: 0.7466678245288816]
	TIME [epoch: 0.871 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8501712938362852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8501712938362852 | validation: 0.7633079871565075]
	TIME [epoch: 0.87 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8605062462229219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8605062462229219 | validation: 0.764608104392993]
	TIME [epoch: 0.868 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8855218849189126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8855218849189126 | validation: 0.6814961391521064]
	TIME [epoch: 0.869 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8205526893760231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8205526893760231 | validation: 0.7125311041689558]
	TIME [epoch: 0.867 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8055122414332896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8055122414332896 | validation: 0.6603475919819602]
	TIME [epoch: 0.871 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7895352768989576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7895352768989576 | validation: 0.8254294371317807]
	TIME [epoch: 0.869 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8407425847936696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8407425847936696 | validation: 0.7119483504066532]
	TIME [epoch: 0.87 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8328399087141533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8328399087141533 | validation: 0.7145418761900689]
	TIME [epoch: 0.869 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8682181876935963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8682181876935963 | validation: 0.7502923780847733]
	TIME [epoch: 0.871 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.817467638743437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.817467638743437 | validation: 0.5766785460699783]
	TIME [epoch: 0.868 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8520711335945341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8520711335945341 | validation: 0.7748822878368347]
	TIME [epoch: 0.871 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7848848144222995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7848848144222995 | validation: 0.5839368461184932]
	TIME [epoch: 0.869 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7726788437303498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7726788437303498 | validation: 0.7656465250943224]
	TIME [epoch: 0.87 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7690857239706063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7690857239706063 | validation: 0.6216225848135614]
	TIME [epoch: 0.868 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7631736386867778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7631736386867778 | validation: 0.8934750541626768]
	TIME [epoch: 0.869 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8167345638344017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8167345638344017 | validation: 0.7527517222608394]
	TIME [epoch: 0.869 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8783957627090763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8783957627090763 | validation: 0.8651732386130239]
	TIME [epoch: 0.869 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9576084630896696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9576084630896696 | validation: 0.7474141238561774]
	TIME [epoch: 0.868 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8400189804366803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8400189804366803 | validation: 0.5684510974078604]
	TIME [epoch: 0.868 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8155620780230103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8155620780230103 | validation: 0.7095583508480527]
	TIME [epoch: 0.868 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7467101831709494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7467101831709494 | validation: 0.6456798952864382]
	TIME [epoch: 0.869 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7257914214520144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7257914214520144 | validation: 0.6260352443506476]
	TIME [epoch: 0.868 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7303878456833408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7303878456833408 | validation: 0.6583383345421056]
	TIME [epoch: 0.871 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7283912415720061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7283912415720061 | validation: 0.5858243773016801]
	TIME [epoch: 0.87 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7403646899022198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7403646899022198 | validation: 0.8408512581356735]
	TIME [epoch: 0.874 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.800300288811221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.800300288811221 | validation: 0.7849122755574292]
	TIME [epoch: 47 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.927785579421562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.927785579421562 | validation: 0.9437886998142571]
	TIME [epoch: 1.73 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0900296268276137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0900296268276137 | validation: 0.705191189980941]
	TIME [epoch: 1.71 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7659041990381473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7659041990381473 | validation: 0.5243146694093664]
	TIME [epoch: 1.72 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_204.pth
	Model improved!!!
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7573008574873146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7573008574873146 | validation: 0.6807214503168089]
	TIME [epoch: 1.72 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.737765577392513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.737765577392513 | validation: 0.582705910918328]
	TIME [epoch: 1.71 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7263253583660918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7263253583660918 | validation: 0.6691457124713032]
	TIME [epoch: 1.71 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7469277152279719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7469277152279719 | validation: 0.7572308739061262]
	TIME [epoch: 1.71 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8674884066696362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8674884066696362 | validation: 0.7582027706503637]
	TIME [epoch: 1.71 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8913954601691318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8913954601691318 | validation: 0.8013370581876043]
	TIME [epoch: 1.71 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8401039099948271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8401039099948271 | validation: 0.6449867061012389]
	TIME [epoch: 1.71 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7313016005089712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7313016005089712 | validation: 0.6104544141732305]
	TIME [epoch: 1.71 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7126660067194592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7126660067194592 | validation: 0.6281668037811388]
	TIME [epoch: 1.71 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7222558582288946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7222558582288946 | validation: 0.6504330207623036]
	TIME [epoch: 1.71 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7495274417318655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7495274417318655 | validation: 0.7404182952095821]
	TIME [epoch: 1.71 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8544255811781695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8544255811781695 | validation: 0.8344434885539533]
	TIME [epoch: 1.71 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9083986265385616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9083986265385616 | validation: 0.6407311227951574]
	TIME [epoch: 1.73 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7447153348045613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7447153348045613 | validation: 0.6813459777024069]
	TIME [epoch: 1.71 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7125515976496223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7125515976496223 | validation: 0.6064831544921243]
	TIME [epoch: 1.71 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7011556830963531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7011556830963531 | validation: 0.6437232186880877]
	TIME [epoch: 1.71 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7026273638179263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7026273638179263 | validation: 0.6096822991018778]
	TIME [epoch: 1.71 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7341101404524304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7341101404524304 | validation: 0.7705532057397471]
	TIME [epoch: 1.71 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8452129269109746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8452129269109746 | validation: 0.7409261368761967]
	TIME [epoch: 1.71 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8621822165036068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8621822165036068 | validation: 0.6110475284575836]
	TIME [epoch: 1.71 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7830218808863701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7830218808863701 | validation: 0.6319154310498919]
	TIME [epoch: 1.72 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6890832911443602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6890832911443602 | validation: 0.5736481921649202]
	TIME [epoch: 1.72 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6742743900874583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6742743900874583 | validation: 0.5761433299861162]
	TIME [epoch: 1.72 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6622628698518631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6622628698518631 | validation: 0.5617191394075131]
	TIME [epoch: 1.72 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6702675866141362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6702675866141362 | validation: 0.5731798242355404]
	TIME [epoch: 1.71 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6679451252293244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6679451252293244 | validation: 0.6456110822564117]
	TIME [epoch: 1.71 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7181927247310117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7181927247310117 | validation: 0.7807601517563978]
	TIME [epoch: 1.71 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9765417745340339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9765417745340339 | validation: 0.8725228624477641]
	TIME [epoch: 1.71 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9312707980134068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9312707980134068 | validation: 0.5493533283759587]
	TIME [epoch: 1.71 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6711122767470957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6711122767470957 | validation: 0.5667765034122334]
	TIME [epoch: 1.71 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6732391305442086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6732391305442086 | validation: 0.7338249630226081]
	TIME [epoch: 1.73 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7573364865683553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7573364865683553 | validation: 0.7406672089481392]
	TIME [epoch: 1.72 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8594466875642085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8594466875642085 | validation: 0.6473607036378086]
	TIME [epoch: 1.71 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7895815117556797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7895815117556797 | validation: 0.5975657062271018]
	TIME [epoch: 1.71 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6685291593515353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6685291593515353 | validation: 0.5414221433993535]
	TIME [epoch: 1.71 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6559544103441272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6559544103441272 | validation: 0.6296995937654621]
	TIME [epoch: 1.71 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6689111167820276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6689111167820276 | validation: 0.5649915363613348]
	TIME [epoch: 1.71 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7056200829477957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7056200829477957 | validation: 0.7308033741623241]
	TIME [epoch: 1.71 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7611655876823252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7611655876823252 | validation: 0.6533738298346325]
	TIME [epoch: 1.71 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7857967145284043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7857967145284043 | validation: 0.5367178807188221]
	TIME [epoch: 1.71 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7700377486361485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7700377486361485 | validation: 0.5883459032463205]
	TIME [epoch: 1.71 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6737493763315808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6737493763315808 | validation: 0.5808507337703429]
	TIME [epoch: 1.71 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.649861654049924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.649861654049924 | validation: 0.5276183146727956]
	TIME [epoch: 1.71 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6549156784320114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6549156784320114 | validation: 0.7138551634105089]
	TIME [epoch: 1.71 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7193998913865424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7193998913865424 | validation: 0.634676725451923]
	TIME [epoch: 1.72 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7752774263663832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7752774263663832 | validation: 0.5755503526683937]
	TIME [epoch: 1.72 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7199057715997753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7199057715997753 | validation: 0.5496443095396787]
	TIME [epoch: 1.72 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.654214802171634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.654214802171634 | validation: 0.5391101205985092]
	TIME [epoch: 1.71 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6316431601003575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6316431601003575 | validation: 0.524102601159129]
	TIME [epoch: 1.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_253.pth
	Model improved!!!
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6571729859741096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6571729859741096 | validation: 0.6362704284881502]
	TIME [epoch: 1.71 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7069279711227527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7069279711227527 | validation: 0.5692522178786015]
	TIME [epoch: 1.71 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7435922842887968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7435922842887968 | validation: 0.6365062109863674]
	TIME [epoch: 1.71 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.711533720342122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.711533720342122 | validation: 0.5490939245629682]
	TIME [epoch: 1.71 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6564324378729143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6564324378729143 | validation: 0.5066870944019387]
	TIME [epoch: 1.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.619481351384138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.619481351384138 | validation: 0.502683053134872]
	TIME [epoch: 1.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_259.pth
	Model improved!!!
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6024322687370078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6024322687370078 | validation: 0.5217338207722851]
	TIME [epoch: 1.71 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6082378049450472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6082378049450472 | validation: 0.5497461820937714]
	TIME [epoch: 1.71 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7303515191779831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7303515191779831 | validation: 0.6289220065531396]
	TIME [epoch: 1.7 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7055058463376683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7055058463376683 | validation: 0.503669602557938]
	TIME [epoch: 1.71 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6540246323791697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6540246323791697 | validation: 0.6975559318582856]
	TIME [epoch: 1.7 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.703686436626333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.703686436626333 | validation: 0.6275080366755456]
	TIME [epoch: 1.7 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.715301765894188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.715301765894188 | validation: 0.496768466734253]
	TIME [epoch: 1.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_266.pth
	Model improved!!!
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6673756714302624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6673756714302624 | validation: 0.5352087345108849]
	TIME [epoch: 1.7 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6001373652135424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6001373652135424 | validation: 0.5241804823123074]
	TIME [epoch: 1.7 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6047894473499386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6047894473499386 | validation: 0.49108888943341605]
	TIME [epoch: 1.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_269.pth
	Model improved!!!
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.585514362839128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.585514362839128 | validation: 0.4924815380778215]
	TIME [epoch: 1.71 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.575795206209689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.575795206209689 | validation: 0.5397477598334754]
	TIME [epoch: 1.7 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6147436777510816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6147436777510816 | validation: 0.6302225477015758]
	TIME [epoch: 1.7 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8211477351217304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8211477351217304 | validation: 0.6930373556828289]
	TIME [epoch: 1.69 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6942784547889858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6942784547889858 | validation: 0.48177095208126447]
	TIME [epoch: 1.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_274.pth
	Model improved!!!
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5708096134393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5708096134393 | validation: 0.4728732582445607]
	TIME [epoch: 1.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_275.pth
	Model improved!!!
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.561737277224103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.561737277224103 | validation: 0.4428897603960669]
	TIME [epoch: 1.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_276.pth
	Model improved!!!
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6147375537854143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6147375537854143 | validation: 0.72339998794665]
	TIME [epoch: 1.7 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6889071100917216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6889071100917216 | validation: 0.6072632866439505]
	TIME [epoch: 1.69 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.713951926229796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.713951926229796 | validation: 0.4344603659850712]
	TIME [epoch: 1.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_279.pth
	Model improved!!!
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5900256031555722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5900256031555722 | validation: 0.5472296811345546]
	TIME [epoch: 1.71 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5654573516621958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5654573516621958 | validation: 0.47968688853497277]
	TIME [epoch: 1.71 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5673710215407404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5673710215407404 | validation: 0.5395582235445817]
	TIME [epoch: 1.71 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5805351376231443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5805351376231443 | validation: 0.5287553183035616]
	TIME [epoch: 1.71 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7181449280021414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7181449280021414 | validation: 0.5117698294089502]
	TIME [epoch: 1.71 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5733939146622477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5733939146622477 | validation: 0.5140889865510022]
	TIME [epoch: 1.71 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5283187199426292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5283187199426292 | validation: 0.5251968619855759]
	TIME [epoch: 1.71 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6887344543438959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6887344543438959 | validation: 0.7003638154950834]
	TIME [epoch: 1.71 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.684018564350672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.684018564350672 | validation: 0.5333551759219274]
	TIME [epoch: 1.7 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6274999748262869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6274999748262869 | validation: 0.42848388569642354]
	TIME [epoch: 1.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_289.pth
	Model improved!!!
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5480249590996422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5480249590996422 | validation: 0.5214412601317697]
	TIME [epoch: 1.71 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5405685035360598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5405685035360598 | validation: 0.4491894341096969]
	TIME [epoch: 1.72 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.521859408430484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.521859408430484 | validation: 0.4812803253660204]
	TIME [epoch: 1.72 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5152354818448575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5152354818448575 | validation: 0.5132438690173405]
	TIME [epoch: 1.72 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6746730469049202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6746730469049202 | validation: 0.6928146019101974]
	TIME [epoch: 1.71 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6361389663200102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6361389663200102 | validation: 0.548655443216885]
	TIME [epoch: 1.71 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6013600651090037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6013600651090037 | validation: 0.39202162805206603]
	TIME [epoch: 1.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_296.pth
	Model improved!!!
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48915981425051724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48915981425051724 | validation: 0.46623109440294547]
	TIME [epoch: 1.71 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47234595022590237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47234595022590237 | validation: 0.4374002987347354]
	TIME [epoch: 1.7 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5627338901697407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5627338901697407 | validation: 0.8280144122387768]
	TIME [epoch: 1.7 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7751567677128182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7751567677128182 | validation: 0.6007158990858781]
	TIME [epoch: 1.7 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.734495547609978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.734495547609978 | validation: 0.5156549221555818]
	TIME [epoch: 1.7 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6214449040918233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6214449040918233 | validation: 0.6017202895817835]
	TIME [epoch: 1.7 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5823876516594803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5823876516594803 | validation: 0.4985526335317933]
	TIME [epoch: 1.7 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5362176334545842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5362176334545842 | validation: 0.4216950347619996]
	TIME [epoch: 1.7 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.497057595525514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.497057595525514 | validation: 0.48746020562936015]
	TIME [epoch: 1.7 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48026557959236554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48026557959236554 | validation: 0.40807289001122504]
	TIME [epoch: 1.7 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.472166781233458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.472166781233458 | validation: 0.6790099904249385]
	TIME [epoch: 1.7 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6190867068330674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6190867068330674 | validation: 0.599700957491382]
	TIME [epoch: 1.7 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7438260550296576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7438260550296576 | validation: 0.4488302654645355]
	TIME [epoch: 1.7 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5287817517231177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5287817517231177 | validation: 0.5908031511833158]
	TIME [epoch: 1.7 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5432548963981537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5432548963981537 | validation: 0.5562774124656532]
	TIME [epoch: 1.7 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6816189818813605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6816189818813605 | validation: 0.5410206827464729]
	TIME [epoch: 1.71 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5104933321368236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5104933321368236 | validation: 0.3953239890883675]
	TIME [epoch: 1.71 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4611551424635054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4611551424635054 | validation: 0.4627992518195731]
	TIME [epoch: 1.71 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42006543861682216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42006543861682216 | validation: 0.3868110320136212]
	TIME [epoch: 1.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_315.pth
	Model improved!!!
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40834741742997693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40834741742997693 | validation: 0.4028822936074308]
	TIME [epoch: 1.7 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38212902868124643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38212902868124643 | validation: 0.3790432569533757]
	TIME [epoch: 1.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_317.pth
	Model improved!!!
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.367382925337486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.367382925337486 | validation: 0.3832809772187664]
	TIME [epoch: 1.7 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4050345615537805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4050345615537805 | validation: 1.1886056981895108]
	TIME [epoch: 1.7 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.142745935730353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.142745935730353 | validation: 0.369217441223014]
	TIME [epoch: 1.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_320.pth
	Model improved!!!
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35002635174558405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35002635174558405 | validation: 0.4127121196904888]
	TIME [epoch: 1.7 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5050844394294594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5050844394294594 | validation: 1.030990720173335]
	TIME [epoch: 1.7 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.924092313703039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.924092313703039 | validation: 0.4284101234681039]
	TIME [epoch: 1.7 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42186562654197235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42186562654197235 | validation: 0.4778949278682989]
	TIME [epoch: 1.7 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4037191486813383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4037191486813383 | validation: 0.35024545210041785]
	TIME [epoch: 1.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_325.pth
	Model improved!!!
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44598934703063103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44598934703063103 | validation: 0.8822611759511715]
	TIME [epoch: 1.7 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7055306590143616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7055306590143616 | validation: 0.6571820635903507]
	TIME [epoch: 1.7 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6504247378134835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6504247378134835 | validation: 0.3967908811320031]
	TIME [epoch: 1.7 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4388281423248258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4388281423248258 | validation: 0.4398549796496088]
	TIME [epoch: 1.7 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4270785269621138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4270785269621138 | validation: 0.4708525596949832]
	TIME [epoch: 1.7 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7443952504803776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7443952504803776 | validation: 0.4424610391694408]
	TIME [epoch: 1.7 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3841978603701424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3841978603701424 | validation: 0.5827675650985553]
	TIME [epoch: 1.7 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5003626960554141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5003626960554141 | validation: 0.5778899872817702]
	TIME [epoch: 1.7 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7229402904137311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7229402904137311 | validation: 0.42178040744759876]
	TIME [epoch: 1.71 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3796196662494345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3796196662494345 | validation: 0.5243524472414463]
	TIME [epoch: 1.7 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4649221936653812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4649221936653812 | validation: 0.5188392144089863]
	TIME [epoch: 1.71 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.641395645422365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.641395645422365 | validation: 0.42745942777445506]
	TIME [epoch: 1.7 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.372562197836338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.372562197836338 | validation: 0.4502348152517039]
	TIME [epoch: 1.7 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39883613673609347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39883613673609347 | validation: 0.4642209658351874]
	TIME [epoch: 1.7 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.589814642314914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.589814642314914 | validation: 0.519046853697091]
	TIME [epoch: 1.7 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4356338829072321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4356338829072321 | validation: 0.3955757014870775]
	TIME [epoch: 1.7 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4099042238654342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4099042238654342 | validation: 0.41618187426427844]
	TIME [epoch: 1.7 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3813867201885162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3813867201885162 | validation: 0.4084218793940778]
	TIME [epoch: 1.7 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4427553932357644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4427553932357644 | validation: 0.653785923472513]
	TIME [epoch: 1.7 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5229998299726956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5229998299726956 | validation: 0.4267114187051602]
	TIME [epoch: 1.7 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5165856608475524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5165856608475524 | validation: 0.43884775595150494]
	TIME [epoch: 1.7 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3577066176586813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3577066176586813 | validation: 0.3445077618829613]
	TIME [epoch: 1.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_347.pth
	Model improved!!!
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32501585712061276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32501585712061276 | validation: 0.3823536548610193]
	TIME [epoch: 1.7 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3199502072888843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3199502072888843 | validation: 0.3573128416192347]
	TIME [epoch: 1.7 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3168142825271437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3168142825271437 | validation: 0.4022337685641536]
	TIME [epoch: 1.7 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5301941199941166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5301941199941166 | validation: 0.6846471011482347]
	TIME [epoch: 1.7 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5989286006094737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5989286006094737 | validation: 0.4900912789027489]
	TIME [epoch: 1.7 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5237971047299377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5237971047299377 | validation: 0.5204628829584615]
	TIME [epoch: 1.7 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3950890790566844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3950890790566844 | validation: 0.33456722274022843]
	TIME [epoch: 1.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_354.pth
	Model improved!!!
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3577111511921268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3577111511921268 | validation: 0.46016638083177946]
	TIME [epoch: 1.7 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.342050047873919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.342050047873919 | validation: 0.3121084665820386]
	TIME [epoch: 1.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_356.pth
	Model improved!!!
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3676217879202976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3676217879202976 | validation: 0.5339838936567827]
	TIME [epoch: 1.72 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.372366475053383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.372366475053383 | validation: 0.4409097817522044]
	TIME [epoch: 1.71 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4757873044311253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4757873044311253 | validation: 0.48913769294952747]
	TIME [epoch: 1.7 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5444673318373575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5444673318373575 | validation: 0.3215939998191326]
	TIME [epoch: 1.71 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.436156712291456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.436156712291456 | validation: 0.7010940980074357]
	TIME [epoch: 1.71 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4715100478294519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4715100478294519 | validation: 0.46930441672721007]
	TIME [epoch: 1.71 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33801732817814656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33801732817814656 | validation: 0.31115958366698787]
	TIME [epoch: 1.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_363.pth
	Model improved!!!
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3528894652609669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3528894652609669 | validation: 0.49486530221671965]
	TIME [epoch: 1.7 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34605447843300474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34605447843300474 | validation: 0.4899005673408845]
	TIME [epoch: 1.7 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49260406397121403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49260406397121403 | validation: 0.5249972536190194]
	TIME [epoch: 1.7 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5025383492896305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5025383492896305 | validation: 0.37715703857985233]
	TIME [epoch: 1.7 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5010843026221905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5010843026221905 | validation: 0.4633845547378247]
	TIME [epoch: 1.7 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3151783396081431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3151783396081431 | validation: 0.38702393319222655]
	TIME [epoch: 1.7 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3035891454289929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3035891454289929 | validation: 0.3629602348611305]
	TIME [epoch: 1.7 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4169548065809097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4169548065809097 | validation: 0.6187832584844971]
	TIME [epoch: 1.7 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43365566055413673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43365566055413673 | validation: 0.5786897663360642]
	TIME [epoch: 1.7 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4424626142569358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4424626142569358 | validation: 0.5097496723477243]
	TIME [epoch: 1.7 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5928627260347564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5928627260347564 | validation: 0.33579646152838927]
	TIME [epoch: 1.7 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2992444504464173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2992444504464173 | validation: 0.4403331536157207]
	TIME [epoch: 1.7 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.317582321187075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.317582321187075 | validation: 0.45324798948726147]
	TIME [epoch: 1.71 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5410260296995307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5410260296995307 | validation: 0.4424675494627282]
	TIME [epoch: 1.7 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4634726165064318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4634726165064318 | validation: 0.29953323481478367]
	TIME [epoch: 1.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_378.pth
	Model improved!!!
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2987472513245607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2987472513245607 | validation: 0.6289628084830823]
	TIME [epoch: 1.72 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46827498043251825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46827498043251825 | validation: 0.5637465097819878]
	TIME [epoch: 1.72 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3865481878524035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3865481878524035 | validation: 0.39674039791129356]
	TIME [epoch: 1.71 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6136837873314533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6136837873314533 | validation: 0.33330914008240303]
	TIME [epoch: 1.7 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36121588038510355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36121588038510355 | validation: 0.7276045079730142]
	TIME [epoch: 1.71 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.565497818951588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.565497818951588 | validation: 0.38797204666565155]
	TIME [epoch: 1.71 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27773441260993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27773441260993 | validation: 0.3183372411460641]
	TIME [epoch: 1.7 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29643479653828886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29643479653828886 | validation: 0.46027211776998644]
	TIME [epoch: 1.71 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36689818275269787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36689818275269787 | validation: 0.4064006077922205]
	TIME [epoch: 1.7 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4470267410545033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4470267410545033 | validation: 0.5643909633931767]
	TIME [epoch: 1.71 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4458948351157178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4458948351157178 | validation: 0.3397731118271113]
	TIME [epoch: 1.71 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2666338219070254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2666338219070254 | validation: 0.29861260208508755]
	TIME [epoch: 1.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_390.pth
	Model improved!!!
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2396725061850379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2396725061850379 | validation: 0.3548213256512446]
	TIME [epoch: 1.7 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2336417541967705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2336417541967705 | validation: 0.2647769527083292]
	TIME [epoch: 1.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_392.pth
	Model improved!!!
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23974739082450447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23974739082450447 | validation: 0.41090050515917975]
	TIME [epoch: 1.7 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24896911981124853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24896911981124853 | validation: 0.3214072173604396]
	TIME [epoch: 1.7 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.425547942741114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.425547942741114 | validation: 0.6764582687653337]
	TIME [epoch: 1.7 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6314356325669244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6314356325669244 | validation: 0.3592670849626877]
	TIME [epoch: 1.7 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26558834768416334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26558834768416334 | validation: 0.3634658733759129]
	TIME [epoch: 1.7 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2784736589560239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2784736589560239 | validation: 0.35252068824805183]
	TIME [epoch: 1.7 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.266818813014165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.266818813014165 | validation: 0.38361681524021524]
	TIME [epoch: 1.7 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34652233783545766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34652233783545766 | validation: 0.2987319397196669]
	TIME [epoch: 1.71 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.598548590902664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.598548590902664 | validation: 0.3834355268934119]
	TIME [epoch: 1.7 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3258486258260267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3258486258260267 | validation: 0.5824790616269061]
	TIME [epoch: 1.71 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5089721000725743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5089721000725743 | validation: 0.40655777434393103]
	TIME [epoch: 1.7 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26174030364680684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26174030364680684 | validation: 0.3313532213444548]
	TIME [epoch: 1.7 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27056459647677666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27056459647677666 | validation: 0.4353313982503245]
	TIME [epoch: 1.7 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3319711895216814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3319711895216814 | validation: 0.34557637670709634]
	TIME [epoch: 1.7 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.304739723349456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.304739723349456 | validation: 0.3800419306479926]
	TIME [epoch: 1.7 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2716732486729831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2716732486729831 | validation: 0.3445860406361506]
	TIME [epoch: 1.7 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3432122655741914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3432122655741914 | validation: 0.34663030423625224]
	TIME [epoch: 1.7 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24871334400282244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24871334400282244 | validation: 0.2943481928845441]
	TIME [epoch: 1.7 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2548156364108923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2548156364108923 | validation: 0.39471997539284503]
	TIME [epoch: 1.7 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25181782813512466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25181782813512466 | validation: 0.3198746625086888]
	TIME [epoch: 1.7 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3713970702346495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3713970702346495 | validation: 0.5336673728774056]
	TIME [epoch: 1.7 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30880916075029413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30880916075029413 | validation: 0.244092258892486]
	TIME [epoch: 1.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_414.pth
	Model improved!!!
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2275321732032707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2275321732032707 | validation: 0.3653370980625582]
	TIME [epoch: 1.7 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24249342138132476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24249342138132476 | validation: 0.4369586999136039]
	TIME [epoch: 1.7 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5458853517519424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5458853517519424 | validation: 0.26624799947405975]
	TIME [epoch: 1.7 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3219117813930183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3219117813930183 | validation: 0.48391180732575556]
	TIME [epoch: 1.7 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4415370642767357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4415370642767357 | validation: 0.8026312795286927]
	TIME [epoch: 1.7 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7548897435587612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7548897435587612 | validation: 0.4911105677402766]
	TIME [epoch: 1.7 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.526863612606087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.526863612606087 | validation: 0.32527740527466453]
	TIME [epoch: 1.7 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9587687080483289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9587687080483289 | validation: 0.4390676596785592]
	TIME [epoch: 1.71 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9102245299616414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9102245299616414 | validation: 0.3296492818411596]
	TIME [epoch: 1.71 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7589075386790536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7589075386790536 | validation: 0.4101740610566167]
	TIME [epoch: 1.71 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4015778691112168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4015778691112168 | validation: 0.5733396508473317]
	TIME [epoch: 1.7 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3822087348333743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3822087348333743 | validation: 0.49685189554612735]
	TIME [epoch: 1.7 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33452687920125795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33452687920125795 | validation: 0.4086739648923587]
	TIME [epoch: 1.7 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3033494730355961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3033494730355961 | validation: 0.38264671207097384]
	TIME [epoch: 1.7 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29165934234442015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29165934234442015 | validation: 0.3935671405030274]
	TIME [epoch: 1.7 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2929499079203776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2929499079203776 | validation: 0.317098538619074]
	TIME [epoch: 1.7 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2585700005604111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2585700005604111 | validation: 0.32238341509062424]
	TIME [epoch: 1.7 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22034836969981292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22034836969981292 | validation: 0.6991372519994563]
	TIME [epoch: 1.7 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7121601479136909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7121601479136909 | validation: 0.41140990641469843]
	TIME [epoch: 1.7 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5293235904063802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5293235904063802 | validation: 0.34162021032353596]
	TIME [epoch: 1.7 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3769528081023303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3769528081023303 | validation: 0.4002878810889232]
	TIME [epoch: 1.7 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3609480746724654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3609480746724654 | validation: 0.4887565617136055]
	TIME [epoch: 1.7 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3853805026620316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3853805026620316 | validation: 0.35872123969377107]
	TIME [epoch: 1.7 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2261381111483533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2261381111483533 | validation: 0.3669645641127902]
	TIME [epoch: 1.7 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2141766159006511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2141766159006511 | validation: 0.22823317388185035]
	TIME [epoch: 1.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_439.pth
	Model improved!!!
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2234414072669381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2234414072669381 | validation: 0.41133565722379295]
	TIME [epoch: 1.69 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2258695453086118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2258695453086118 | validation: 0.27664788883273983]
	TIME [epoch: 1.69 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2549178697224614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2549178697224614 | validation: 0.48473928408621153]
	TIME [epoch: 1.69 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29813147557746317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29813147557746317 | validation: 0.283319831858959]
	TIME [epoch: 1.69 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39618943213388563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39618943213388563 | validation: 0.5159923083253601]
	TIME [epoch: 1.69 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7498480235486406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7498480235486406 | validation: 0.47224685670970523]
	TIME [epoch: 1.7 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7049983885590362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7049983885590362 | validation: 0.4641543983940428]
	TIME [epoch: 1.69 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8054172462885519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8054172462885519 | validation: 0.5271345396453202]
	TIME [epoch: 1.7 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7546255799095668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7546255799095668 | validation: 0.4807273914898602]
	TIME [epoch: 1.69 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6947432913658628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6947432913658628 | validation: 0.48382973002181634]
	TIME [epoch: 1.7 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6853297466664882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6853297466664882 | validation: 0.44435263582056017]
	TIME [epoch: 1.69 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.65625045681733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.65625045681733 | validation: 0.43226579130366966]
	TIME [epoch: 1.69 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6239304251212332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6239304251212332 | validation: 0.3745494476180826]
	TIME [epoch: 1.69 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6295870426577617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6295870426577617 | validation: 0.5588637701341058]
	TIME [epoch: 1.69 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5696537322414214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5696537322414214 | validation: 0.29356043945804017]
	TIME [epoch: 1.7 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3698128538336617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3698128538336617 | validation: 0.37540249970662476]
	TIME [epoch: 1.7 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27478202040956917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27478202040956917 | validation: 0.39569442067986565]
	TIME [epoch: 1.69 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29204433696164706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29204433696164706 | validation: 0.29082094510328815]
	TIME [epoch: 1.7 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2054873499483908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2054873499483908 | validation: 0.41540193428021316]
	TIME [epoch: 1.69 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2537256562185246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2537256562185246 | validation: 0.23071680760532384]
	TIME [epoch: 1.69 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2549215047952637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2549215047952637 | validation: 0.6803722167408539]
	TIME [epoch: 1.69 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4012749825610929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4012749825610929 | validation: 0.4196064196453721]
	TIME [epoch: 1.69 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26312876523952583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26312876523952583 | validation: 0.30832879489254456]
	TIME [epoch: 1.69 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25823031297925503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25823031297925503 | validation: 0.3279726659212177]
	TIME [epoch: 1.69 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19521763812579576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19521763812579576 | validation: 0.2492197639930462]
	TIME [epoch: 1.69 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1573303492807241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1573303492807241 | validation: 0.2405239743409062]
	TIME [epoch: 1.69 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1452614105636277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1452614105636277 | validation: 0.2612626891413683]
	TIME [epoch: 1.69 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14173930821366842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14173930821366842 | validation: 0.21153392173518967]
	TIME [epoch: 1.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_467.pth
	Model improved!!!
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19403055486199342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19403055486199342 | validation: 0.6360420715849724]
	TIME [epoch: 1.71 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43378070119533774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43378070119533774 | validation: 0.307144157634381]
	TIME [epoch: 1.71 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2815955796302278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2815955796302278 | validation: 0.4569973761550694]
	TIME [epoch: 1.7 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3644687565047688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3644687565047688 | validation: 0.3907277477780795]
	TIME [epoch: 1.7 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43339508244112634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43339508244112634 | validation: 0.28607565267527924]
	TIME [epoch: 1.7 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2113074361307614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2113074361307614 | validation: 0.4005054642361632]
	TIME [epoch: 1.7 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33112892475863687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33112892475863687 | validation: 0.3184257856433205]
	TIME [epoch: 1.7 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22036478696322295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22036478696322295 | validation: 0.2888168146456924]
	TIME [epoch: 1.7 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1568007027690116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1568007027690116 | validation: 0.27936045808818877]
	TIME [epoch: 1.7 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16880663945129637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16880663945129637 | validation: 0.27757503925445864]
	TIME [epoch: 1.7 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21428900785480756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21428900785480756 | validation: 0.260351490446103]
	TIME [epoch: 1.7 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18774015210810852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18774015210810852 | validation: 0.2958572705067591]
	TIME [epoch: 1.7 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18241288943303918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18241288943303918 | validation: 0.3051886423741881]
	TIME [epoch: 1.7 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23255493196850904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23255493196850904 | validation: 0.6643047269082127]
	TIME [epoch: 1.7 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42234198665041833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42234198665041833 | validation: 0.20095579809746528]
	TIME [epoch: 1.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_482.pth
	Model improved!!!
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15056056091551626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15056056091551626 | validation: 0.2931438586900501]
	TIME [epoch: 1.71 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20292336003716838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20292336003716838 | validation: 0.2875339706781157]
	TIME [epoch: 1.72 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22405344551678177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22405344551678177 | validation: 0.2870211736674659]
	TIME [epoch: 1.72 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15598823756814192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15598823756814192 | validation: 0.2090940789324085]
	TIME [epoch: 1.73 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22223751721404347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22223751721404347 | validation: 0.5720495354746691]
	TIME [epoch: 1.72 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31205150699228984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31205150699228984 | validation: 0.30912326747729985]
	TIME [epoch: 1.71 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22231178081314884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22231178081314884 | validation: 0.288860669051163]
	TIME [epoch: 1.72 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14823791311033174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14823791311033174 | validation: 0.2063517805169336]
	TIME [epoch: 1.72 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18146690318185357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18146690318185357 | validation: 0.3182531385628667]
	TIME [epoch: 1.72 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1601364769930467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1601364769930467 | validation: 0.23338285334434064]
	TIME [epoch: 1.72 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17850508876429438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17850508876429438 | validation: 0.4527630031914125]
	TIME [epoch: 1.71 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30148771092787485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30148771092787485 | validation: 0.4950807380678347]
	TIME [epoch: 1.71 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5362921255341301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5362921255341301 | validation: 0.2574523242462074]
	TIME [epoch: 1.71 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2271269762140319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2271269762140319 | validation: 0.38515693365520387]
	TIME [epoch: 1.73 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2582135361771559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2582135361771559 | validation: 0.30419796939648885]
	TIME [epoch: 1.71 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1952440726370915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1952440726370915 | validation: 0.2441020916430926]
	TIME [epoch: 1.71 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1309025754525856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1309025754525856 | validation: 0.28537453653519373]
	TIME [epoch: 1.71 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13259938501243768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13259938501243768 | validation: 0.22612949962533785]
	TIME [epoch: 1.71 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1466969017333936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1466969017333936 | validation: 0.47526433350483943]
	TIME [epoch: 50.3 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24519906032274225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24519906032274225 | validation: 0.280122127454887]
	TIME [epoch: 3.42 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3229503860750841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3229503860750841 | validation: 0.4894143554482181]
	TIME [epoch: 3.39 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3013326396743351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3013326396743351 | validation: 0.20445489924410631]
	TIME [epoch: 3.38 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12350131262762647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12350131262762647 | validation: 0.2166332275500131]
	TIME [epoch: 3.38 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21998284950718616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21998284950718616 | validation: 0.4321834752225863]
	TIME [epoch: 3.38 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2520990970654094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2520990970654094 | validation: 0.2988566166683113]
	TIME [epoch: 3.39 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15792667991866727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15792667991866727 | validation: 0.20211486484819974]
	TIME [epoch: 3.4 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12288086203356606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12288086203356606 | validation: 0.2516301294190604]
	TIME [epoch: 3.39 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15523410332178192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15523410332178192 | validation: 0.26053507629117667]
	TIME [epoch: 3.38 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.265475689560111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.265475689560111 | validation: 0.49610391646924135]
	TIME [epoch: 3.38 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33332857507351177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33332857507351177 | validation: 0.32742761973169815]
	TIME [epoch: 3.39 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23601388101511198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23601388101511198 | validation: 0.2805337905601662]
	TIME [epoch: 3.39 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12649899133616388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12649899133616388 | validation: 0.2276116408191535]
	TIME [epoch: 3.39 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14586758974147343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14586758974147343 | validation: 0.4431060514540864]
	TIME [epoch: 3.38 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2832707446382882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2832707446382882 | validation: 0.2651399890268408]
	TIME [epoch: 3.39 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15089966113887304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15089966113887304 | validation: 0.21304918236862191]
	TIME [epoch: 3.39 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13701538373765523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13701538373765523 | validation: 0.203240945324223]
	TIME [epoch: 3.39 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11709152536177608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11709152536177608 | validation: 0.8935497336661458]
	TIME [epoch: 3.4 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6326645432413893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6326645432413893 | validation: 1.0076609354062835]
	TIME [epoch: 3.39 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6745494156766377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6745494156766377 | validation: 0.926130629277394]
	TIME [epoch: 3.38 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6927724720823977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6927724720823977 | validation: 0.8831700292579989]
	TIME [epoch: 3.39 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6427948590938585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6427948590938585 | validation: 0.7459796851196038]
	TIME [epoch: 3.38 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6409051415725081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6409051415725081 | validation: 0.4896717189120947]
	TIME [epoch: 3.38 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2974875242229403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2974875242229403 | validation: 0.30460703582233434]
	TIME [epoch: 3.38 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23062239444007987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23062239444007987 | validation: 0.26162196822752054]
	TIME [epoch: 3.38 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18987717599178974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18987717599178974 | validation: 0.2323667287683561]
	TIME [epoch: 3.38 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14975910108195384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14975910108195384 | validation: 0.2273163319514238]
	TIME [epoch: 3.38 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12447753531126689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12447753531126689 | validation: 0.2844835050797424]
	TIME [epoch: 3.38 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12881855586836122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12881855586836122 | validation: 0.2577486995369383]
	TIME [epoch: 3.4 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1607783301579321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1607783301579321 | validation: 0.5884204926459712]
	TIME [epoch: 3.39 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31098860217904367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31098860217904367 | validation: 0.2649921559004836]
	TIME [epoch: 3.4 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29117801925175857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29117801925175857 | validation: 0.3065363622654339]
	TIME [epoch: 3.41 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17718221799008618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17718221799008618 | validation: 0.24734525059330595]
	TIME [epoch: 3.38 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17401229434862292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17401229434862292 | validation: 0.21192361409071456]
	TIME [epoch: 3.39 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16519965433791495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16519965433791495 | validation: 0.3024863386676522]
	TIME [epoch: 3.4 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17313605648153946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17313605648153946 | validation: 0.2209278845484977]
	TIME [epoch: 3.4 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14334763844882745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14334763844882745 | validation: 0.28022237869184957]
	TIME [epoch: 3.39 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1213348353212561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1213348353212561 | validation: 0.203796893128267]
	TIME [epoch: 3.38 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13097663971238074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13097663971238074 | validation: 0.38917865131061385]
	TIME [epoch: 3.39 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18848229095773195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18848229095773195 | validation: 0.2887099989853514]
	TIME [epoch: 3.4 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23135858535533138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23135858535533138 | validation: 0.3861698047608001]
	TIME [epoch: 3.39 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2283240319936284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2283240319936284 | validation: 0.37131851377610325]
	TIME [epoch: 3.38 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4192545670827027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4192545670827027 | validation: 0.18347606036977568]
	TIME [epoch: 3.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_544.pth
	Model improved!!!
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16320904405621414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16320904405621414 | validation: 0.36564232162431276]
	TIME [epoch: 3.39 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2880073682672469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2880073682672469 | validation: 0.33508914377721194]
	TIME [epoch: 3.39 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23141021298286604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23141021298286604 | validation: 0.23804830009619238]
	TIME [epoch: 3.38 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11732150868790221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11732150868790221 | validation: 0.24244769900928725]
	TIME [epoch: 3.38 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11990569408979258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11990569408979258 | validation: 0.2162700744794823]
	TIME [epoch: 3.4 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12476780623461377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12476780623461377 | validation: 0.2684552024348423]
	TIME [epoch: 3.4 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1321487239042698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1321487239042698 | validation: 0.2107383434025171]
	TIME [epoch: 3.39 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13966564395309944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13966564395309944 | validation: 0.35845781034943625]
	TIME [epoch: 3.4 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16826762008421015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16826762008421015 | validation: 0.2538595396236693]
	TIME [epoch: 3.38 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18862817834276877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18862817834276877 | validation: 0.4420604681468069]
	TIME [epoch: 3.38 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24970460791130283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24970460791130283 | validation: 0.21178409761198913]
	TIME [epoch: 3.38 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5432898440414448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5432898440414448 | validation: 0.19927069576089873]
	TIME [epoch: 3.38 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23705933073749677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23705933073749677 | validation: 0.38672986247436336]
	TIME [epoch: 3.38 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27769812087735213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27769812087735213 | validation: 0.2819467919151101]
	TIME [epoch: 3.38 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1253045844181881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1253045844181881 | validation: 0.2598894454181306]
	TIME [epoch: 3.38 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17196550681946335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17196550681946335 | validation: 0.40101144236574515]
	TIME [epoch: 3.38 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.242481701087089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.242481701087089 | validation: 0.3184581349101373]
	TIME [epoch: 3.39 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1461507298649917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1461507298649917 | validation: 0.1955523676259907]
	TIME [epoch: 3.39 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12186501159908948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12186501159908948 | validation: 0.3226073480608871]
	TIME [epoch: 3.41 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.169629332555918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.169629332555918 | validation: 0.2610195189309769]
	TIME [epoch: 3.39 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16400096517721036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16400096517721036 | validation: 0.43749433493591133]
	TIME [epoch: 3.38 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24663504546374762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24663504546374762 | validation: 0.2390652828570975]
	TIME [epoch: 3.38 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2646665468979268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2646665468979268 | validation: 0.2626785922156824]
	TIME [epoch: 3.38 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14189476602143292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14189476602143292 | validation: 0.1834504951763285]
	TIME [epoch: 3.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_568.pth
	Model improved!!!
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10487175069247777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10487175069247777 | validation: 0.19000840133589247]
	TIME [epoch: 3.38 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09038028601375661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09038028601375661 | validation: 0.19138004572217496]
	TIME [epoch: 3.38 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11368792279727812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11368792279727812 | validation: 0.4697464847183202]
	TIME [epoch: 3.38 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6382253892028251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6382253892028251 | validation: 0.3977770369788218]
	TIME [epoch: 3.39 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6525191311059547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6525191311059547 | validation: 0.6107579075301726]
	TIME [epoch: 3.39 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6872838144723646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6872838144723646 | validation: 0.34548527983129157]
	TIME [epoch: 3.4 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5307060964511645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5307060964511645 | validation: 0.28531706214113467]
	TIME [epoch: 3.39 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35305042824957694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35305042824957694 | validation: 0.25319865007680126]
	TIME [epoch: 3.38 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13958965566988008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13958965566988008 | validation: 0.2936946806153027]
	TIME [epoch: 3.39 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18211112513982117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18211112513982117 | validation: 0.32258862811654726]
	TIME [epoch: 3.38 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14322327297038887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14322327297038887 | validation: 0.22120064473886247]
	TIME [epoch: 3.38 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11474436278737164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11474436278737164 | validation: 0.22682369301823493]
	TIME [epoch: 3.39 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12743305134655583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12743305134655583 | validation: 0.28739333575583315]
	TIME [epoch: 3.39 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15913582603604678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15913582603604678 | validation: 0.41238410240909684]
	TIME [epoch: 3.39 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2996091442968191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2996091442968191 | validation: 0.32866166482012793]
	TIME [epoch: 3.39 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3122154094310162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3122154094310162 | validation: 0.4255725261750392]
	TIME [epoch: 3.39 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21984799485936107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21984799485936107 | validation: 0.18868861681347554]
	TIME [epoch: 3.41 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10677223737692253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10677223737692253 | validation: 0.18999789521379454]
	TIME [epoch: 3.42 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10229671937289699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10229671937289699 | validation: 0.2940163376237388]
	TIME [epoch: 3.39 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12458882651787619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12458882651787619 | validation: 0.2046423052268712]
	TIME [epoch: 3.39 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11584266901761976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11584266901761976 | validation: 0.2723457083990582]
	TIME [epoch: 3.39 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13828410022519566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13828410022519566 | validation: 0.2988796080692688]
	TIME [epoch: 3.38 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20238252745192298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20238252745192298 | validation: 0.24888557297263914]
	TIME [epoch: 3.39 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12106318382319323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12106318382319323 | validation: 0.15893459688232967]
	TIME [epoch: 3.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_592.pth
	Model improved!!!
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13818502379589148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13818502379589148 | validation: 0.34024514567155373]
	TIME [epoch: 3.37 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1772250442708103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1772250442708103 | validation: 0.29274340686126593]
	TIME [epoch: 3.37 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2026267602911272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2026267602911272 | validation: 0.35917679469460984]
	TIME [epoch: 3.37 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1812363520716224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1812363520716224 | validation: 0.16422412295797534]
	TIME [epoch: 3.38 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12525010284033322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12525010284033322 | validation: 0.23038426206530804]
	TIME [epoch: 3.37 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09531653460682023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09531653460682023 | validation: 0.17843220177608715]
	TIME [epoch: 3.37 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08625997448221799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08625997448221799 | validation: 0.17105979296815801]
	TIME [epoch: 3.37 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08271357365893277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08271357365893277 | validation: 0.17153044968225523]
	TIME [epoch: 3.37 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08213430935025925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08213430935025925 | validation: 0.24424093697956872]
	TIME [epoch: 3.39 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1416923560256285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1416923560256285 | validation: 0.21813686990298523]
	TIME [epoch: 3.37 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2065749575777514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2065749575777514 | validation: 0.4576190161031418]
	TIME [epoch: 3.38 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2698123808318407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2698123808318407 | validation: 0.3198806371337751]
	TIME [epoch: 3.37 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14574756187093701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14574756187093701 | validation: 0.22655178528146502]
	TIME [epoch: 3.38 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10384717663035119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10384717663035119 | validation: 0.21127259486954353]
	TIME [epoch: 3.38 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14084303454401856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14084303454401856 | validation: 0.34126920613442263]
	TIME [epoch: 3.4 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17875300395334065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17875300395334065 | validation: 0.3295129043299894]
	TIME [epoch: 3.37 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1736063936435227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1736063936435227 | validation: 0.20493438611307552]
	TIME [epoch: 3.38 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1255246028079457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1255246028079457 | validation: 0.2181991210600246]
	TIME [epoch: 3.37 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12266529712752494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12266529712752494 | validation: 0.23562906202913078]
	TIME [epoch: 3.38 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25042425956360737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25042425956360737 | validation: 0.4149912946588626]
	TIME [epoch: 3.38 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2217330911547426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2217330911547426 | validation: 0.2339479054785163]
	TIME [epoch: 3.36 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1774895898152615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1774895898152615 | validation: 0.3044512401716559]
	TIME [epoch: 3.36 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14090783189786568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14090783189786568 | validation: 0.20350516670025864]
	TIME [epoch: 3.36 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0899324035194055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0899324035194055 | validation: 0.16660091468570756]
	TIME [epoch: 3.36 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11909070520144781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11909070520144781 | validation: 0.24328193549222574]
	TIME [epoch: 3.36 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10398464211993518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10398464211993518 | validation: 0.1809260461992367]
	TIME [epoch: 3.38 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10907449974308873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10907449974308873 | validation: 0.2704133280799562]
	TIME [epoch: 3.36 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13185741949200622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13185741949200622 | validation: 0.18410911027890414]
	TIME [epoch: 3.36 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1882165841544525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1882165841544525 | validation: 0.3298315416936866]
	TIME [epoch: 3.36 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18477964290157722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18477964290157722 | validation: 0.6393489947034428]
	TIME [epoch: 3.37 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.536538810763977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.536538810763977 | validation: 0.42051103999202194]
	TIME [epoch: 3.37 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2375701164607353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2375701164607353 | validation: 0.26864890667120345]
	TIME [epoch: 3.36 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21568959030610527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21568959030610527 | validation: 0.1860942767705038]
	TIME [epoch: 3.37 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10707706109517857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10707706109517857 | validation: 0.20690124303198296]
	TIME [epoch: 3.36 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10392912182887239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10392912182887239 | validation: 0.1899302889117848]
	TIME [epoch: 3.36 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08183012290479645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08183012290479645 | validation: 0.17699759348106348]
	TIME [epoch: 3.37 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07506893720445364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07506893720445364 | validation: 0.16713750275745737]
	TIME [epoch: 3.39 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07194445747813182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07194445747813182 | validation: 0.1734401717158018]
	TIME [epoch: 3.36 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06641665911129958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06641665911129958 | validation: 0.1443000043493213]
	TIME [epoch: 3.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_631.pth
	Model improved!!!
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06513104368635797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06513104368635797 | validation: 0.251222205795712]
	TIME [epoch: 3.37 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11322083525894489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11322083525894489 | validation: 0.6284655567772415]
	TIME [epoch: 3.37 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4892637164394859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4892637164394859 | validation: 0.6710548435900383]
	TIME [epoch: 3.38 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4854581963705368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4854581963705368 | validation: 0.6071884307387627]
	TIME [epoch: 3.38 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4234503017515684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4234503017515684 | validation: 0.35531653298208526]
	TIME [epoch: 3.38 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20802183634850052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20802183634850052 | validation: 0.20243545765422263]
	TIME [epoch: 3.37 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11763037261937642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11763037261937642 | validation: 0.1720640763811188]
	TIME [epoch: 3.37 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15610093713875367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15610093713875367 | validation: 0.19170575095140957]
	TIME [epoch: 3.39 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08110811795667891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08110811795667891 | validation: 0.18258494524331323]
	TIME [epoch: 3.38 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08461481201145497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08461481201145497 | validation: 0.1886181774994209]
	TIME [epoch: 3.37 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07195927008515335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07195927008515335 | validation: 0.14049021514011772]
	TIME [epoch: 3.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_642.pth
	Model improved!!!
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10212105529818061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10212105529818061 | validation: 0.19436755946373818]
	TIME [epoch: 3.37 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08634479693688878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08634479693688878 | validation: 0.23547642905457178]
	TIME [epoch: 3.37 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1427438539931242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1427438539931242 | validation: 0.26792458343763675]
	TIME [epoch: 3.37 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14750458627201624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14750458627201624 | validation: 0.18178984213270627]
	TIME [epoch: 3.37 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14505760706225515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14505760706225515 | validation: 0.2685313019147976]
	TIME [epoch: 3.36 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16086489848065796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16086489848065796 | validation: 0.14425584773703257]
	TIME [epoch: 3.37 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11385100935828531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11385100935828531 | validation: 0.20760648591671257]
	TIME [epoch: 3.37 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10681528460582089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10681528460582089 | validation: 0.17111854742171212]
	TIME [epoch: 3.38 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15057810758809484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15057810758809484 | validation: 0.28179742462012375]
	TIME [epoch: 3.37 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25663123111598685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25663123111598685 | validation: 0.22594327695517813]
	TIME [epoch: 3.37 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15469360156676568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15469360156676568 | validation: 0.2757623316200194]
	TIME [epoch: 3.37 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11653938806688015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11653938806688015 | validation: 0.16731711634725002]
	TIME [epoch: 3.37 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07820144385980991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07820144385980991 | validation: 0.15019064094614082]
	TIME [epoch: 3.37 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07349074398583809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07349074398583809 | validation: 0.17816816015557888]
	TIME [epoch: 3.37 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07471003499856808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07471003499856808 | validation: 0.17254196624120235]
	TIME [epoch: 3.39 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08597551968696183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08597551968696183 | validation: 0.2134208091367567]
	TIME [epoch: 3.37 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08586031379287803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08586031379287803 | validation: 0.17344710476345113]
	TIME [epoch: 3.37 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08599057708803938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08599057708803938 | validation: 0.24725382277516816]
	TIME [epoch: 3.37 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10921716333431569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10921716333431569 | validation: 0.20459112028817958]
	TIME [epoch: 3.39 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16708939628801153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16708939628801153 | validation: 0.38386058952700597]
	TIME [epoch: 3.38 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2683371524315021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2683371524315021 | validation: 0.19602589554780114]
	TIME [epoch: 3.37 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2457310420587872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2457310420587872 | validation: 0.14949070901160197]
	TIME [epoch: 3.37 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11663008107387526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11663008107387526 | validation: 0.24853384288054628]
	TIME [epoch: 3.37 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11595352875741355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11595352875741355 | validation: 0.19312933728732865]
	TIME [epoch: 3.37 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12164079430306231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12164079430306231 | validation: 0.15796929134363807]
	TIME [epoch: 3.37 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08105413443896904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08105413443896904 | validation: 0.46184908172135963]
	TIME [epoch: 3.37 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25021246097147204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25021246097147204 | validation: 0.22261018301542831]
	TIME [epoch: 3.37 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17225233082732738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17225233082732738 | validation: 0.13735998274058556]
	TIME [epoch: 3.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_670.pth
	Model improved!!!
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09314942174120759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09314942174120759 | validation: 0.12074372054026919]
	TIME [epoch: 3.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_671.pth
	Model improved!!!
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08068683588148721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08068683588148721 | validation: 0.14051720422431035]
	TIME [epoch: 3.38 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0639054328447262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0639054328447262 | validation: 0.14154732184454416]
	TIME [epoch: 3.38 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05952456801941994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05952456801941994 | validation: 0.135507689221219]
	TIME [epoch: 3.38 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07330832483045842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07330832483045842 | validation: 0.18070228444336559]
	TIME [epoch: 3.37 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12883540405139166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12883540405139166 | validation: 0.27728796224689795]
	TIME [epoch: 3.37 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18258490221395385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18258490221395385 | validation: 0.24703058072130946]
	TIME [epoch: 3.36 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12978811781132035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12978811781132035 | validation: 0.2943019462652335]
	TIME [epoch: 3.36 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19638515700319764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19638515700319764 | validation: 0.3556701201837247]
	TIME [epoch: 3.36 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2504846502367728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2504846502367728 | validation: 0.19695811729239432]
	TIME [epoch: 3.36 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09516402435892658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09516402435892658 | validation: 0.20290784800735115]
	TIME [epoch: 3.36 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11200152688003558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11200152688003558 | validation: 0.1882863258990649]
	TIME [epoch: 3.37 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2435855635191394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2435855635191394 | validation: 0.28987493632353495]
	TIME [epoch: 3.37 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21363584242937542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21363584242937542 | validation: 0.26418831646904634]
	TIME [epoch: 3.37 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23393461575660404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23393461575660404 | validation: 0.30520678895559383]
	TIME [epoch: 3.36 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17366986753815142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17366986753815142 | validation: 0.19411053547041313]
	TIME [epoch: 3.36 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13944478738812527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13944478738812527 | validation: 0.18944187170353402]
	TIME [epoch: 3.34 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08655448406635471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08655448406635471 | validation: 0.14507799156952667]
	TIME [epoch: 3.34 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10073775527871635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10073775527871635 | validation: 0.17387134689482814]
	TIME [epoch: 3.35 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06918401555178733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06918401555178733 | validation: 0.1369494153006679]
	TIME [epoch: 3.36 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06346688182229784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06346688182229784 | validation: 0.1445886792674048]
	TIME [epoch: 3.36 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06372949342783824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06372949342783824 | validation: 0.12611902427287594]
	TIME [epoch: 3.36 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07189980553401845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07189980553401845 | validation: 0.2375632913454865]
	TIME [epoch: 3.36 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11859541969454167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11859541969454167 | validation: 0.3308743332479095]
	TIME [epoch: 3.38 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2545081007413926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2545081007413926 | validation: 0.36965268276831104]
	TIME [epoch: 3.37 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2025112986506783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2025112986506783 | validation: 0.11949988431459779]
	TIME [epoch: 3.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_696.pth
	Model improved!!!
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07875160765352966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07875160765352966 | validation: 0.14747287257707953]
	TIME [epoch: 3.35 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07142111228057636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07142111228057636 | validation: 0.21242085571951527]
	TIME [epoch: 3.35 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08376903397458615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08376903397458615 | validation: 0.13580099385902752]
	TIME [epoch: 3.34 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07513415118884607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07513415118884607 | validation: 0.19422762807627178]
	TIME [epoch: 3.35 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09309836949754155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09309836949754155 | validation: 0.1347304283307831]
	TIME [epoch: 3.38 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08939340915338298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08939340915338298 | validation: 0.1815733524510134]
	TIME [epoch: 3.38 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12973010754060482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12973010754060482 | validation: 0.18391982687983266]
	TIME [epoch: 3.38 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13876981448164688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13876981448164688 | validation: 0.17687709120770873]
	TIME [epoch: 3.39 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1026449734412069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1026449734412069 | validation: 0.14754203612330452]
	TIME [epoch: 3.41 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0781095965235478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0781095965235478 | validation: 0.1119832434132365]
	TIME [epoch: 3.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_706.pth
	Model improved!!!
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10222785432887736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10222785432887736 | validation: 0.24779811845055547]
	TIME [epoch: 3.38 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17003484768530075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17003484768530075 | validation: 0.21199752930671903]
	TIME [epoch: 3.38 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12270842748781073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12270842748781073 | validation: 0.26898273587024646]
	TIME [epoch: 3.38 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1289261674826639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1289261674826639 | validation: 0.18179362837647295]
	TIME [epoch: 3.38 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11803104345608571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11803104345608571 | validation: 0.20049147075992507]
	TIME [epoch: 3.37 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10304578577220001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10304578577220001 | validation: 0.12462569310360004]
	TIME [epoch: 3.38 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12777559662497157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12777559662497157 | validation: 0.2019634370576096]
	TIME [epoch: 3.38 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1086058045817901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1086058045817901 | validation: 0.12305455921346642]
	TIME [epoch: 3.38 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07591770255830615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07591770255830615 | validation: 0.12273272284801864]
	TIME [epoch: 3.39 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05279498590111399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05279498590111399 | validation: 0.12986854129852946]
	TIME [epoch: 3.39 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06126891010324654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06126891010324654 | validation: 0.18679126254916203]
	TIME [epoch: 3.38 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10175580649816958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10175580649816958 | validation: 0.17130441997770374]
	TIME [epoch: 3.38 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0833110771671472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0833110771671472 | validation: 0.19038391782747643]
	TIME [epoch: 3.38 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09972457561545857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09972457561545857 | validation: 0.23890190199307018]
	TIME [epoch: 3.38 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12451665813358816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12451665813358816 | validation: 0.26111022698171166]
	TIME [epoch: 3.38 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1245145236920789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1245145236920789 | validation: 0.13963549848578755]
	TIME [epoch: 3.38 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10260641644136338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10260641644136338 | validation: 0.1823172520649452]
	TIME [epoch: 3.38 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11648882758825815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11648882758825815 | validation: 0.16404902134184954]
	TIME [epoch: 3.39 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1552489343615568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1552489343615568 | validation: 0.19032717551995654]
	TIME [epoch: 3.39 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10032843268070554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10032843268070554 | validation: 0.11584745744070789]
	TIME [epoch: 3.4 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07363180403063085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07363180403063085 | validation: 0.2011891000646643]
	TIME [epoch: 3.39 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0921601954264193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0921601954264193 | validation: 0.17820524472725438]
	TIME [epoch: 3.38 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08615266966980642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08615266966980642 | validation: 0.2003355310262138]
	TIME [epoch: 3.38 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08830174187063243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08830174187063243 | validation: 0.16716418623090432]
	TIME [epoch: 3.37 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08394333060677678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08394333060677678 | validation: 0.1893831143436172]
	TIME [epoch: 3.37 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08637086409709195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08637086409709195 | validation: 0.11678100912768369]
	TIME [epoch: 3.38 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1046381409522731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1046381409522731 | validation: 0.19059025293874154]
	TIME [epoch: 3.38 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11252955293784897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11252955293784897 | validation: 0.1312446921028901]
	TIME [epoch: 3.37 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11836971497404135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11836971497404135 | validation: 0.20572087974705583]
	TIME [epoch: 3.38 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1405926363689486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1405926363689486 | validation: 0.30191522496895234]
	TIME [epoch: 3.38 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2226400288638019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2226400288638019 | validation: 0.18168702521503166]
	TIME [epoch: 3.4 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08750153854227578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08750153854227578 | validation: 0.1744742108003664]
	TIME [epoch: 3.39 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07694321979195426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07694321979195426 | validation: 0.14672898072838622]
	TIME [epoch: 3.37 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07087024651377866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07087024651377866 | validation: 0.1848082960647155]
	TIME [epoch: 3.37 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06975518486941322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06975518486941322 | validation: 0.16189097336171415]
	TIME [epoch: 3.38 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08691714468056017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08691714468056017 | validation: 0.24520690172965623]
	TIME [epoch: 3.37 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12150513990498858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12150513990498858 | validation: 0.16290579437083086]
	TIME [epoch: 3.37 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10594278117180689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10594278117180689 | validation: 0.16814445617691362]
	TIME [epoch: 3.37 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10059250033169718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10059250033169718 | validation: 0.09721405175151727]
	TIME [epoch: 3.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_745.pth
	Model improved!!!
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10734299913074455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10734299913074455 | validation: 0.19580575547403015]
	TIME [epoch: 3.37 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08785101791546801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08785101791546801 | validation: 0.08376305256079725]
	TIME [epoch: 3.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_747.pth
	Model improved!!!
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05514229983956846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05514229983956846 | validation: 0.13936191922375626]
	TIME [epoch: 3.38 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06300189854276672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06300189854276672 | validation: 0.1646658960562607]
	TIME [epoch: 3.37 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10139120273485543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10139120273485543 | validation: 0.22068514727379998]
	TIME [epoch: 3.37 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14154030504047505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14154030504047505 | validation: 0.18331091024710525]
	TIME [epoch: 3.36 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13806379662708887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13806379662708887 | validation: 0.17360153163895822]
	TIME [epoch: 3.36 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07456063879890448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07456063879890448 | validation: 0.11195919928643533]
	TIME [epoch: 3.36 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06459520493093861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06459520493093861 | validation: 0.12539789703153179]
	TIME [epoch: 3.36 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06198586841599112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06198586841599112 | validation: 0.0968648701202709]
	TIME [epoch: 3.36 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07115920772662136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07115920772662136 | validation: 0.25703924555823665]
	TIME [epoch: 3.37 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11977612347101206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11977612347101206 | validation: 0.14390092780616737]
	TIME [epoch: 3.36 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08160162809829505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08160162809829505 | validation: 0.23331788391359104]
	TIME [epoch: 3.37 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12876601545231847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12876601545231847 | validation: 0.22806950686149255]
	TIME [epoch: 3.38 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1552542026149674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1552542026149674 | validation: 0.2554438207437122]
	TIME [epoch: 3.38 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13391255108914984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13391255108914984 | validation: 0.10154750877401304]
	TIME [epoch: 3.36 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07065144302094015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07065144302094015 | validation: 0.1462143081093215]
	TIME [epoch: 3.37 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0703436740316491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0703436740316491 | validation: 0.13188685638184858]
	TIME [epoch: 3.41 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08977793434938704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08977793434938704 | validation: 0.16265077166770012]
	TIME [epoch: 3.37 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11210593052867324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11210593052867324 | validation: 0.14780817652698428]
	TIME [epoch: 3.36 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1059600195374746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1059600195374746 | validation: 0.16594093260526332]
	TIME [epoch: 3.37 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09172489006884664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09172489006884664 | validation: 0.20476867806048227]
	TIME [epoch: 3.36 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1058550254883356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1058550254883356 | validation: 0.19663648091674044]
	TIME [epoch: 3.37 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12406857724710735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12406857724710735 | validation: 0.22445700496497514]
	TIME [epoch: 3.37 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1111518222852737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1111518222852737 | validation: 0.1049166443875647]
	TIME [epoch: 3.38 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09174605712737685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09174605712737685 | validation: 0.1418136598194192]
	TIME [epoch: 3.38 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0642706450525548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0642706450525548 | validation: 0.08424273329483227]
	TIME [epoch: 3.36 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061954673631583315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061954673631583315 | validation: 0.1384153403874704]
	TIME [epoch: 3.36 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05222096502430172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05222096502430172 | validation: 0.08669791421398684]
	TIME [epoch: 3.36 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04157757261329852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04157757261329852 | validation: 0.1003329029765827]
	TIME [epoch: 3.36 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041194729857688336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041194729857688336 | validation: 0.09420480442528018]
	TIME [epoch: 3.36 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05035605051886678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05035605051886678 | validation: 0.12944609088000555]
	TIME [epoch: 3.36 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0819984899302516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0819984899302516 | validation: 0.13850876420601196]
	TIME [epoch: 3.36 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10487629220352314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10487629220352314 | validation: 0.19482503133476278]
	TIME [epoch: 3.37 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13729297802066237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13729297802066237 | validation: 0.22388453455339877]
	TIME [epoch: 3.37 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18905947296634368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18905947296634368 | validation: 0.23854791323141847]
	TIME [epoch: 3.39 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14315721422101477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14315721422101477 | validation: 0.199521976153737]
	TIME [epoch: 3.37 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10011665903962891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10011665903962891 | validation: 0.10367125212857992]
	TIME [epoch: 3.36 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10398551986740859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10398551986740859 | validation: 0.19937014383547658]
	TIME [epoch: 3.36 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1008811445290929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1008811445290929 | validation: 0.21290355260121366]
	TIME [epoch: 3.36 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13542062135580876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13542062135580876 | validation: 0.24012336035226373]
	TIME [epoch: 3.36 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11023094429730612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11023094429730612 | validation: 0.12728660642531495]
	TIME [epoch: 3.36 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04951381878925856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04951381878925856 | validation: 0.09767140389620817]
	TIME [epoch: 3.36 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046465517119466115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046465517119466115 | validation: 0.13020633430477285]
	TIME [epoch: 3.36 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05530012986375521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05530012986375521 | validation: 0.10821534555995281]
	TIME [epoch: 3.36 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07155836300212333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07155836300212333 | validation: 0.15916817525299476]
	TIME [epoch: 3.37 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0990571605032233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0990571605032233 | validation: 0.10279801178304666]
	TIME [epoch: 3.38 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12370171163419182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12370171163419182 | validation: 0.16084155250764987]
	TIME [epoch: 3.37 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08925206713616128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08925206713616128 | validation: 0.11389637323681692]
	TIME [epoch: 3.36 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06832870921131086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06832870921131086 | validation: 0.1331641936428607]
	TIME [epoch: 3.36 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06095982034368005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06095982034368005 | validation: 0.14992913101245822]
	TIME [epoch: 3.36 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08564214865746798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08564214865746798 | validation: 0.23711393158674643]
	TIME [epoch: 3.36 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13032860765745416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13032860765745416 | validation: 0.16801530340236526]
	TIME [epoch: 3.36 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10489425623108016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10489425623108016 | validation: 0.15274516360375243]
	TIME [epoch: 3.37 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08300695165721882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08300695165721882 | validation: 0.1034589510876765]
	TIME [epoch: 3.36 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0652972421428681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0652972421428681 | validation: 0.11081264895848891]
	TIME [epoch: 3.36 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056742520233030426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056742520233030426 | validation: 0.09136158966584328]
	TIME [epoch: 3.37 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045355708492021425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045355708492021425 | validation: 0.11340988018233991]
	TIME [epoch: 3.38 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04828619704614515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04828619704614515 | validation: 0.08758883601813844]
	TIME [epoch: 3.37 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06665730610261686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06665730610261686 | validation: 0.1568997616716784]
	TIME [epoch: 3.36 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09227115245517144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09227115245517144 | validation: 0.07595764919021543]
	TIME [epoch: 3.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_806.pth
	Model improved!!!
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09137366468939266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09137366468939266 | validation: 0.15121768318829681]
	TIME [epoch: 3.36 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07995630147998288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07995630147998288 | validation: 0.31102049982482627]
	TIME [epoch: 3.36 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1610321443939037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1610321443939037 | validation: 0.18107399720911793]
	TIME [epoch: 3.36 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10218745059709879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10218745059709879 | validation: 0.15088699239466435]
	TIME [epoch: 3.36 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13863450993817084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13863450993817084 | validation: 0.25685745390298215]
	TIME [epoch: 3.36 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11098115572037923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11098115572037923 | validation: 0.15793301359342873]
	TIME [epoch: 3.36 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09610365901657432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09610365901657432 | validation: 0.12061453816094125]
	TIME [epoch: 3.37 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06237207083381973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06237207083381973 | validation: 0.10795266400256896]
	TIME [epoch: 3.38 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09070229494817852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09070229494817852 | validation: 0.16687118410778357]
	TIME [epoch: 3.37 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09220087269151138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09220087269151138 | validation: 0.10212104060116252]
	TIME [epoch: 3.36 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06646486475252607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06646486475252607 | validation: 0.10718397071827226]
	TIME [epoch: 3.37 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0522052909940096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0522052909940096 | validation: 0.14203740328842523]
	TIME [epoch: 3.36 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0666365559755255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0666365559755255 | validation: 0.17701274131807634]
	TIME [epoch: 3.37 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10555075643375421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10555075643375421 | validation: 0.2056203279488357]
	TIME [epoch: 3.36 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11333124745759016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11333124745759016 | validation: 0.13836040846453176]
	TIME [epoch: 3.37 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07485276758500671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07485276758500671 | validation: 0.08141510106398037]
	TIME [epoch: 3.36 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045349365324285545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045349365324285545 | validation: 0.0976763135604452]
	TIME [epoch: 3.37 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04348993315785565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04348993315785565 | validation: 0.09073379323923957]
	TIME [epoch: 3.37 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05058005351824136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05058005351824136 | validation: 0.13107137631396332]
	TIME [epoch: 3.38 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07692626727872803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07692626727872803 | validation: 0.12157562531543382]
	TIME [epoch: 3.37 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09536674509812532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09536674509812532 | validation: 0.13682619101721438]
	TIME [epoch: 3.37 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07175347391800829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07175347391800829 | validation: 0.1044893487995911]
	TIME [epoch: 3.36 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10334301457165697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10334301457165697 | validation: 0.27122593521406]
	TIME [epoch: 3.37 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17927985875819702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17927985875819702 | validation: 0.26434978972287476]
	TIME [epoch: 3.36 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2291482687703659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2291482687703659 | validation: 0.15171485518916847]
	TIME [epoch: 3.37 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06600566974404021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06600566974404021 | validation: 0.12913645269889182]
	TIME [epoch: 3.36 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05311236743831467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05311236743831467 | validation: 0.09959243922650157]
	TIME [epoch: 3.37 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048098966907623594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048098966907623594 | validation: 0.10712080925293893]
	TIME [epoch: 3.36 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04474855430995852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04474855430995852 | validation: 0.10929798643077766]
	TIME [epoch: 3.41 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04312439412191776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04312439412191776 | validation: 0.11215456095643797]
	TIME [epoch: 3.38 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04881618744242932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04881618744242932 | validation: 0.13594702936732658]
	TIME [epoch: 3.37 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07951071684556349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07951071684556349 | validation: 0.15403964449507068]
	TIME [epoch: 3.36 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1255734945392408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1255734945392408 | validation: 0.14791037010768152]
	TIME [epoch: 3.36 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11954869743943086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11954869743943086 | validation: 0.10898346835103206]
	TIME [epoch: 3.36 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046068874803041754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046068874803041754 | validation: 0.08122667635556874]
	TIME [epoch: 3.36 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030065725972981978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030065725972981978 | validation: 0.08450383301978835]
	TIME [epoch: 3.36 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057992916727668396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057992916727668396 | validation: 0.11182369455300155]
	TIME [epoch: 3.37 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0681319671708984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0681319671708984 | validation: 0.15842548395504974]
	TIME [epoch: 3.36 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09508193822821358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09508193822821358 | validation: 0.2264877573107622]
	TIME [epoch: 3.36 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1612241805961802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1612241805961802 | validation: 0.2742533131765556]
	TIME [epoch: 3.37 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1732221609141311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1732221609141311 | validation: 0.10167275845952557]
	TIME [epoch: 3.38 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0861541182233851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0861541182233851 | validation: 0.10027374805194247]
	TIME [epoch: 3.37 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06087885283739091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06087885283739091 | validation: 0.154225159570677]
	TIME [epoch: 3.36 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06694855394383284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06694855394383284 | validation: 0.08964643557208099]
	TIME [epoch: 3.37 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04467028919002111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04467028919002111 | validation: 0.08806633877203568]
	TIME [epoch: 3.37 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037710750387181684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037710750387181684 | validation: 0.09389044493127963]
	TIME [epoch: 3.36 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040042419913604466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040042419913604466 | validation: 0.0979648091383884]
	TIME [epoch: 3.37 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04511882233424703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04511882233424703 | validation: 0.11299133167406868]
	TIME [epoch: 3.36 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06717664798714507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06717664798714507 | validation: 0.11032183170603226]
	TIME [epoch: 3.37 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08681320486857264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08681320486857264 | validation: 0.14396274788104416]
	TIME [epoch: 3.37 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07997502955616925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07997502955616925 | validation: 0.09658087672042603]
	TIME [epoch: 3.37 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07241260953991946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07241260953991946 | validation: 0.13578691484634806]
	TIME [epoch: 3.38 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07015814619522902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07015814619522902 | validation: 0.08189365139500464]
	TIME [epoch: 3.37 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0678186156921645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0678186156921645 | validation: 0.13157167995211358]
	TIME [epoch: 3.36 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08106467927719177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08106467927719177 | validation: 0.09984978600038281]
	TIME [epoch: 3.37 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06805297273819276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06805297273819276 | validation: 0.11167424977057183]
	TIME [epoch: 3.36 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06483720722420298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06483720722420298 | validation: 0.10209721665138108]
	TIME [epoch: 3.36 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07007745699176397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07007745699176397 | validation: 0.15012924536981884]
	TIME [epoch: 3.36 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09838629210544902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09838629210544902 | validation: 0.14365949317387]
	TIME [epoch: 3.36 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09719262327079435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09719262327079435 | validation: 0.13847472438390343]
	TIME [epoch: 3.36 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06874081054601805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06874081054601805 | validation: 0.18440421857806327]
	TIME [epoch: 3.37 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09379205187670026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09379205187670026 | validation: 0.2320574452309015]
	TIME [epoch: 3.37 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13094669725130811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13094669725130811 | validation: 0.09794503203866298]
	TIME [epoch: 3.39 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1115302224462416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1115302224462416 | validation: 0.12708449648229012]
	TIME [epoch: 3.37 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07066656052837798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07066656052837798 | validation: 0.08029804340131112]
	TIME [epoch: 3.38 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050325525717014846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050325525717014846 | validation: 0.08760028096608272]
	TIME [epoch: 3.36 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042419320476980785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042419320476980785 | validation: 0.0867272673785125]
	TIME [epoch: 3.37 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04785569019598126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04785569019598126 | validation: 0.08332693199652408]
	TIME [epoch: 3.37 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040342209108130994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040342209108130994 | validation: 0.0804479150946704]
	TIME [epoch: 3.37 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04667330785847524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04667330785847524 | validation: 0.11391946385729337]
	TIME [epoch: 3.37 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05791696746818777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05791696746818777 | validation: 0.12693369595255852]
	TIME [epoch: 3.37 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08170185015073585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08170185015073585 | validation: 0.1472745272141118]
	TIME [epoch: 3.37 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08648558920563477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08648558920563477 | validation: 0.1835517163892628]
	TIME [epoch: 3.37 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10410444257737762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10410444257737762 | validation: 0.11312769986781285]
	TIME [epoch: 3.38 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10704529383137233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10704529383137233 | validation: 0.15812862107249376]
	TIME [epoch: 3.38 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09270456356571202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09270456356571202 | validation: 0.09207890819576954]
	TIME [epoch: 3.36 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08217087245030669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08217087245030669 | validation: 0.11334968193050138]
	TIME [epoch: 3.37 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05860474515826186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05860474515826186 | validation: 0.13253808683934715]
	TIME [epoch: 3.36 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0549191937099196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0549191937099196 | validation: 0.06930771625732293]
	TIME [epoch: 3.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_885.pth
	Model improved!!!
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05085261615866169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05085261615866169 | validation: 0.10772146547477748]
	TIME [epoch: 3.35 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05877449367224151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05877449367224151 | validation: 0.16317750977323275]
	TIME [epoch: 3.35 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07946793384291381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07946793384291381 | validation: 0.09196659284344291]
	TIME [epoch: 3.35 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042713959752093696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042713959752093696 | validation: 0.08953375447290367]
	TIME [epoch: 3.35 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05946851319954998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05946851319954998 | validation: 0.1274373781029499]
	TIME [epoch: 3.35 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07985777573823452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07985777573823452 | validation: 0.07043065347472058]
	TIME [epoch: 3.36 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06296969913452072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06296969913452072 | validation: 0.1219657224869577]
	TIME [epoch: 3.36 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07472232839189351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07472232839189351 | validation: 0.14262450450274874]
	TIME [epoch: 3.35 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07273474944599558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07273474944599558 | validation: 0.10402339348707175]
	TIME [epoch: 3.35 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08268473476374766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08268473476374766 | validation: 0.2520993070725171]
	TIME [epoch: 3.36 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1670409636215105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1670409636215105 | validation: 0.15369718555890305]
	TIME [epoch: 3.36 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13819183749257719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13819183749257719 | validation: 0.11552637853855074]
	TIME [epoch: 3.36 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05634423225089259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05634423225089259 | validation: 0.06213790523583776]
	TIME [epoch: 3.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_898.pth
	Model improved!!!
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04195915839532513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04195915839532513 | validation: 0.08117740964001437]
	TIME [epoch: 3.35 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03786606325803891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03786606325803891 | validation: 0.1258037763785682]
	TIME [epoch: 3.35 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05684465658011214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05684465658011214 | validation: 0.09989333520012006]
	TIME [epoch: 3.37 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06169525312105865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06169525312105865 | validation: 0.11513417884557112]
	TIME [epoch: 3.4 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08679192758346485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08679192758346485 | validation: 0.12898650542316448]
	TIME [epoch: 3.37 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07092152768315334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07092152768315334 | validation: 0.10362917646181798]
	TIME [epoch: 3.37 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06123021183184925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06123021183184925 | validation: 0.08018064022782262]
	TIME [epoch: 3.36 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05016257133823187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05016257133823187 | validation: 0.08910347219621054]
	TIME [epoch: 3.4 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05012266575730616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05012266575730616 | validation: 0.10481483648952002]
	TIME [epoch: 3.37 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06514764576539311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06514764576539311 | validation: 0.13899187070883642]
	TIME [epoch: 3.37 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07510835008872062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07510835008872062 | validation: 0.12855059717405468]
	TIME [epoch: 3.36 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08378082669605386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08378082669605386 | validation: 0.10935121902463063]
	TIME [epoch: 3.37 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06583140213208434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06583140213208434 | validation: 0.07338359459569177]
	TIME [epoch: 3.37 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054761578979645266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054761578979645266 | validation: 0.0948561457887849]
	TIME [epoch: 3.38 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06029614946244632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06029614946244632 | validation: 0.09800429536010072]
	TIME [epoch: 3.38 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0791216865476614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0791216865476614 | validation: 0.11536247883862179]
	TIME [epoch: 3.34 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07360386135247052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07360386135247052 | validation: 0.07634247862376442]
	TIME [epoch: 3.34 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0724210241555125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0724210241555125 | validation: 0.09160656934595285]
	TIME [epoch: 3.36 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05801549626142594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05801549626142594 | validation: 0.06511843713791823]
	TIME [epoch: 3.34 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04551730581858035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04551730581858035 | validation: 0.09458555410751279]
	TIME [epoch: 3.36 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05186456451168334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05186456451168334 | validation: 0.11027395725469286]
	TIME [epoch: 3.36 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056099574324935134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056099574324935134 | validation: 0.16806672508055195]
	TIME [epoch: 3.36 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10482561662007603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10482561662007603 | validation: 0.19454125383905754]
	TIME [epoch: 3.35 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10701805422635317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10701805422635317 | validation: 0.12709069919486315]
	TIME [epoch: 3.36 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.064432431081303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.064432431081303 | validation: 0.07364222947389107]
	TIME [epoch: 3.36 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05351160171638398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05351160171638398 | validation: 0.12221783812565792]
	TIME [epoch: 3.39 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06334418958608412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06334418958608412 | validation: 0.05800688973533244]
	TIME [epoch: 3.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_925.pth
	Model improved!!!
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05627879128039875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05627879128039875 | validation: 0.09672682231862469]
	TIME [epoch: 3.36 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05053027804426161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05053027804426161 | validation: 0.07529010232933699]
	TIME [epoch: 3.36 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05930454414867986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05930454414867986 | validation: 0.15653405318823954]
	TIME [epoch: 3.37 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08697641661701126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08697641661701126 | validation: 0.15410278607507158]
	TIME [epoch: 3.36 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12352333690959641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12352333690959641 | validation: 0.12226912149179121]
	TIME [epoch: 3.36 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05672072769978092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05672072769978092 | validation: 0.07298148494248585]
	TIME [epoch: 3.36 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03424614448439489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03424614448439489 | validation: 0.07765795328505598]
	TIME [epoch: 3.36 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0455285102889682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0455285102889682 | validation: 0.10032349721478778]
	TIME [epoch: 3.36 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06856441208883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06856441208883 | validation: 0.12581559322478994]
	TIME [epoch: 3.36 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08694289026378925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08694289026378925 | validation: 0.0886716354786592]
	TIME [epoch: 3.38 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05894890944608877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05894890944608877 | validation: 0.0929544736866523]
	TIME [epoch: 3.36 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04568398622291465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04568398622291465 | validation: 0.09230568003083149]
	TIME [epoch: 3.36 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04416068253977499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04416068253977499 | validation: 0.09509964967122432]
	TIME [epoch: 3.36 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03928127255653093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03928127255653093 | validation: 0.09401482152423314]
	TIME [epoch: 3.36 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06806706119735548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06806706119735548 | validation: 0.19234207331527542]
	TIME [epoch: 3.36 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12218529615741984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12218529615741984 | validation: 0.15787922975861873]
	TIME [epoch: 3.36 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13047643289959043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13047643289959043 | validation: 0.10859764438049942]
	TIME [epoch: 3.38 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07025501518046047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07025501518046047 | validation: 0.08835041651890599]
	TIME [epoch: 3.36 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06211218422446574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06211218422446574 | validation: 0.08007659160741816]
	TIME [epoch: 3.36 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052809669256799294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052809669256799294 | validation: 0.0844575227663869]
	TIME [epoch: 3.36 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03818178548941238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03818178548941238 | validation: 0.060399011597950074]
	TIME [epoch: 3.38 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035878257665862036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035878257665862036 | validation: 0.06884266642157456]
	TIME [epoch: 3.37 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03982426495863263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03982426495863263 | validation: 0.09034404425438296]
	TIME [epoch: 3.36 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05243300257118861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05243300257118861 | validation: 0.14626519659223744]
	TIME [epoch: 3.36 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08076746731378168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08076746731378168 | validation: 0.19061064104837833]
	TIME [epoch: 3.36 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10618635221757111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10618635221757111 | validation: 0.11336021546057824]
	TIME [epoch: 3.36 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06604889508692388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06604889508692388 | validation: 0.0909412634030135]
	TIME [epoch: 3.36 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04282004337880741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04282004337880741 | validation: 0.06007115988704342]
	TIME [epoch: 3.36 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059123157590572434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059123157590572434 | validation: 0.11338183440639163]
	TIME [epoch: 3.36 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08056756148869788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08056756148869788 | validation: 0.057329172685062685]
	TIME [epoch: 3.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_955.pth
	Model improved!!!
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0657597984095377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0657597984095377 | validation: 0.06982557137361883]
	TIME [epoch: 3.37 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04011681268865409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04011681268865409 | validation: 0.052034588875712995]
	TIME [epoch: 3.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_957.pth
	Model improved!!!
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027737525403994007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027737525403994007 | validation: 0.0551601426525044]
	TIME [epoch: 3.39 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028951872721402414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028951872721402414 | validation: 0.09163765332691426]
	TIME [epoch: 3.46 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03768400861486947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03768400861486947 | validation: 0.11607274214434304]
	TIME [epoch: 3.39 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07007716747702675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07007716747702675 | validation: 0.19494834495570768]
	TIME [epoch: 3.39 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11494736555961374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11494736555961374 | validation: 0.2618604204757965]
	TIME [epoch: 3.39 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14938720880493453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14938720880493453 | validation: 0.08857580992782627]
	TIME [epoch: 3.39 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0658460800816016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0658460800816016 | validation: 0.08258570077407666]
	TIME [epoch: 3.39 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05528490154286892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05528490154286892 | validation: 0.11688120174476749]
	TIME [epoch: 3.39 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06292685638351685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06292685638351685 | validation: 0.10243220951155714]
	TIME [epoch: 3.39 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04955137709575601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04955137709575601 | validation: 0.08998869716779694]
	TIME [epoch: 3.39 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034258933119499034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034258933119499034 | validation: 0.07164511868317498]
	TIME [epoch: 3.4 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04372428340363442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04372428340363442 | validation: 0.11037620916833571]
	TIME [epoch: 3.4 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06870993275733142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06870993275733142 | validation: 0.08008188950656177]
	TIME [epoch: 3.4 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08671799087268876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08671799087268876 | validation: 0.11740764056278322]
	TIME [epoch: 3.39 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07579821668528333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07579821668528333 | validation: 0.07037912732170125]
	TIME [epoch: 3.39 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057194080595768214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057194080595768214 | validation: 0.099705595143938]
	TIME [epoch: 3.39 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0554117453020549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0554117453020549 | validation: 0.09792001867350363]
	TIME [epoch: 3.39 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05401921813497468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05401921813497468 | validation: 0.12408404667029499]
	TIME [epoch: 3.39 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06366257384305879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06366257384305879 | validation: 0.08024387592967053]
	TIME [epoch: 3.39 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03693540305729492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03693540305729492 | validation: 0.06171155344924365]
	TIME [epoch: 3.4 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03699168231016074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03699168231016074 | validation: 0.0702909413586894]
	TIME [epoch: 3.39 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039473807447744556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039473807447744556 | validation: 0.09568789814404072]
	TIME [epoch: 3.39 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0646858934002977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0646858934002977 | validation: 0.10680685910880201]
	TIME [epoch: 3.41 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08793284568102357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08793284568102357 | validation: 0.13667292718691582]
	TIME [epoch: 3.4 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08165963327012314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08165963327012314 | validation: 0.11084504785041478]
	TIME [epoch: 3.38 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.080723704579507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.080723704579507 | validation: 0.11455771694363906]
	TIME [epoch: 3.39 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05461778912960309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05461778912960309 | validation: 0.11395339664438074]
	TIME [epoch: 3.39 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05961155707628023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05961155707628023 | validation: 0.08964127329448551]
	TIME [epoch: 3.39 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054345003770302346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054345003770302346 | validation: 0.09136990537736163]
	TIME [epoch: 3.38 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04279764483689304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04279764483689304 | validation: 0.06699555632777505]
	TIME [epoch: 3.38 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03825386894727305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03825386894727305 | validation: 0.08064754605673907]
	TIME [epoch: 3.38 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03736061824535717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03736061824535717 | validation: 0.05030999933003706]
	TIME [epoch: 3.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_989.pth
	Model improved!!!
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03573897178723236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03573897178723236 | validation: 0.0699959321046956]
	TIME [epoch: 3.38 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03256892621037839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03256892621037839 | validation: 0.035511726323648754]
	TIME [epoch: 3.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_991.pth
	Model improved!!!
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034901023247893785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034901023247893785 | validation: 0.08577865528462915]
	TIME [epoch: 3.39 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06877941877069178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06877941877069178 | validation: 0.07486424003114284]
	TIME [epoch: 3.38 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09696864810192155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09696864810192155 | validation: 0.16865676942758673]
	TIME [epoch: 3.4 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09522049081440753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09522049081440753 | validation: 0.21931915787896053]
	TIME [epoch: 3.38 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11555694500771634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11555694500771634 | validation: 0.138804552884455]
	TIME [epoch: 3.38 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07223946603013069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07223946603013069 | validation: 0.057802800354356434]
	TIME [epoch: 3.38 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03407448013948088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03407448013948088 | validation: 0.0908964618277246]
	TIME [epoch: 3.38 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03525144388244807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03525144388244807 | validation: 0.04421580903543129]
	TIME [epoch: 3.38 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02961031957624248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02961031957624248 | validation: 0.06419393729332716]
	TIME [epoch: 3.38 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02590328515285625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02590328515285625 | validation: 0.059647773231506176]
	TIME [epoch: 55.5 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043227453397505604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043227453397505604 | validation: 0.11868364854380259]
	TIME [epoch: 7.38 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09373353509852607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09373353509852607 | validation: 0.14269645474147127]
	TIME [epoch: 7.34 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12541787559147893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12541787559147893 | validation: 0.10730209213613505]
	TIME [epoch: 7.34 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0697474251883932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0697474251883932 | validation: 0.11330909986396016]
	TIME [epoch: 7.34 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06274937648060497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06274937648060497 | validation: 0.10288414537135666]
	TIME [epoch: 7.33 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05812651081936183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05812651081936183 | validation: 0.05763082626687253]
	TIME [epoch: 7.35 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05650180359341959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05650180359341959 | validation: 0.09792148195229108]
	TIME [epoch: 7.37 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06815009846203475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06815009846203475 | validation: 0.11260990913116996]
	TIME [epoch: 7.35 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07557615316496552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07557615316496552 | validation: 0.13756943725972848]
	TIME [epoch: 7.34 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08382068340180665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08382068340180665 | validation: 0.08720481746074721]
	TIME [epoch: 7.34 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07112350275486433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07112350275486433 | validation: 0.07135233655215734]
	TIME [epoch: 7.37 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029501335418657643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029501335418657643 | validation: 0.05423067943300966]
	TIME [epoch: 7.35 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019477079638707543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019477079638707543 | validation: 0.03391744309807553]
	TIME [epoch: 7.35 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_1014.pth
	Model improved!!!
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020948876732295823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020948876732295823 | validation: 0.055707208323408856]
	TIME [epoch: 7.33 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02302022536341976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02302022536341976 | validation: 0.040810300105102094]
	TIME [epoch: 7.33 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03044913298578579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03044913298578579 | validation: 0.09017616011640534]
	TIME [epoch: 7.34 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040578781905999166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040578781905999166 | validation: 0.05262047880663506]
	TIME [epoch: 7.35 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044286519640607695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044286519640607695 | validation: 0.10595072433213665]
	TIME [epoch: 7.34 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062398617881267236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062398617881267236 | validation: 0.11358290500026065]
	TIME [epoch: 7.33 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10087211640480831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10087211640480831 | validation: 0.1354979553937664]
	TIME [epoch: 7.33 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.096141936468584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.096141936468584 | validation: 0.1347284478519523]
	TIME [epoch: 7.33 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08001400668662016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08001400668662016 | validation: 0.1527411623181979]
	TIME [epoch: 7.35 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0982785753823225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0982785753823225 | validation: 0.13563575432076977]
	TIME [epoch: 7.34 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09462938239908698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09462938239908698 | validation: 0.051027835551431346]
	TIME [epoch: 7.34 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05549066428041972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05549066428041972 | validation: 0.06754889985165212]
	TIME [epoch: 7.33 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026801561205938303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026801561205938303 | validation: 0.040285736674606776]
	TIME [epoch: 7.33 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023763395195875853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023763395195875853 | validation: 0.055642171150921564]
	TIME [epoch: 7.34 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028558818617431384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028558818617431384 | validation: 0.0693097343861938]
	TIME [epoch: 7.35 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03577917062423107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03577917062423107 | validation: 0.08011642912824658]
	TIME [epoch: 7.37 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05082989622875829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05082989622875829 | validation: 0.1207522548960113]
	TIME [epoch: 7.34 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0841843133325962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0841843133325962 | validation: 0.24292985770676243]
	TIME [epoch: 7.34 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14379315278571805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14379315278571805 | validation: 0.06810323079879897]
	TIME [epoch: 7.34 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03449578846621591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03449578846621591 | validation: 0.10503867782982357]
	TIME [epoch: 7.36 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05358038508954545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05358038508954545 | validation: 0.11128239853900546]
	TIME [epoch: 7.34 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054121852308887684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054121852308887684 | validation: 0.08004237545293544]
	TIME [epoch: 7.33 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0332605460570136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0332605460570136 | validation: 0.07426552246403005]
	TIME [epoch: 7.34 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055028002673140806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055028002673140806 | validation: 0.164510733865918]
	TIME [epoch: 7.34 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09138290990612177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09138290990612177 | validation: 0.11210308449317714]
	TIME [epoch: 7.36 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08778135267993249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08778135267993249 | validation: 0.12865540005000942]
	TIME [epoch: 7.34 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07674197374977754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07674197374977754 | validation: 0.0799326353850836]
	TIME [epoch: 7.34 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03654652637531284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03654652637531284 | validation: 0.0550757138990757]
	TIME [epoch: 7.34 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024489726617910424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024489726617910424 | validation: 0.03969981154839969]
	TIME [epoch: 7.34 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023789662147439328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023789662147439328 | validation: 0.06115177842884481]
	TIME [epoch: 7.35 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026313952510440934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026313952510440934 | validation: 0.04821400229536468]
	TIME [epoch: 7.34 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03763104532186414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03763104532186414 | validation: 0.11634477046039682]
	TIME [epoch: 7.34 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07448334165596643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07448334165596643 | validation: 0.09021326039025981]
	TIME [epoch: 7.34 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10207812494213762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10207812494213762 | validation: 0.09660223487877628]
	TIME [epoch: 7.35 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061477214988622006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061477214988622006 | validation: 0.06731243376425979]
	TIME [epoch: 7.35 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04126662316451928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04126662316451928 | validation: 0.04783646286002266]
	TIME [epoch: 7.36 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045116525807894166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045116525807894166 | validation: 0.07457731354622381]
	TIME [epoch: 7.33 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05091662776103888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05091662776103888 | validation: 0.09235040302964237]
	TIME [epoch: 7.34 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05960485158725403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05960485158725403 | validation: 0.15751216955252823]
	TIME [epoch: 7.33 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08174442963691736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08174442963691736 | validation: 0.1410160193920439]
	TIME [epoch: 7.35 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07675124645590707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07675124645590707 | validation: 0.061602669924242785]
	TIME [epoch: 7.35 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055757007706850964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055757007706850964 | validation: 0.06600895240857095]
	TIME [epoch: 7.34 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04354631109795403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04354631109795403 | validation: 0.034695993758130685]
	TIME [epoch: 7.33 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02748032444163324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02748032444163324 | validation: 0.05173312133146897]
	TIME [epoch: 7.34 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030597634537843883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030597634537843883 | validation: 0.09671550608779882]
	TIME [epoch: 7.34 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060856974681807764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060856974681807764 | validation: 0.07057267245664327]
	TIME [epoch: 7.36 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06741846079953617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06741846079953617 | validation: 0.08743877021643714]
	TIME [epoch: 7.33 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034414888787276034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034414888787276034 | validation: 0.07043957408932712]
	TIME [epoch: 7.34 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03172697734869374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03172697734869374 | validation: 0.08557018709954145]
	TIME [epoch: 7.33 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041921097691932196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041921097691932196 | validation: 0.1492362914464181]
	TIME [epoch: 7.34 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07314464380947064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07314464380947064 | validation: 0.14292096477272292]
	TIME [epoch: 7.35 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09308471307172121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09308471307172121 | validation: 0.1196060598063124]
	TIME [epoch: 7.34 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08075375736559938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08075375736559938 | validation: 0.04828918401373128]
	TIME [epoch: 7.33 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06538041052102245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06538041052102245 | validation: 0.07036953520112571]
	TIME [epoch: 7.34 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033791042008227364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033791042008227364 | validation: 0.043221136204962374]
	TIME [epoch: 7.34 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023514059789807088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023514059789807088 | validation: 0.05117171725177642]
	TIME [epoch: 7.34 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0275820295021149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0275820295021149 | validation: 0.08384828184927592]
	TIME [epoch: 7.35 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050802730261662876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050802730261662876 | validation: 0.08599247048363307]
	TIME [epoch: 7.33 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07084000417766424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07084000417766424 | validation: 0.06521385674975617]
	TIME [epoch: 7.33 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05615938625150633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05615938625150633 | validation: 0.09617635682891901]
	TIME [epoch: 7.33 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06616459882842557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06616459882842557 | validation: 0.1352858660413155]
	TIME [epoch: 7.34 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08788469748106341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08788469748106341 | validation: 0.07468760810873842]
	TIME [epoch: 7.35 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04294680324122881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04294680324122881 | validation: 0.06135376397844189]
	TIME [epoch: 7.33 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038241213271940716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038241213271940716 | validation: 0.07508436654427354]
	TIME [epoch: 7.33 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04015571248989788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04015571248989788 | validation: 0.10469896574559515]
	TIME [epoch: 7.33 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049724334968177575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049724334968177575 | validation: 0.1450908060300986]
	TIME [epoch: 7.35 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0849455774526807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0849455774526807 | validation: 0.1345879290579248]
	TIME [epoch: 7.36 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06456568635542705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06456568635542705 | validation: 0.04937537460323176]
	TIME [epoch: 7.34 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02988053775520709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02988053775520709 | validation: 0.041076798409856674]
	TIME [epoch: 7.33 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02508773141457548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02508773141457548 | validation: 0.05108061140289264]
	TIME [epoch: 7.36 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03694119214411253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03694119214411253 | validation: 0.07280657068667283]
	TIME [epoch: 7.33 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053853282644207086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053853282644207086 | validation: 0.07918543087763956]
	TIME [epoch: 7.37 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06839741705687363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06839741705687363 | validation: 0.10707507922264191]
	TIME [epoch: 7.34 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06827770676626047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06827770676626047 | validation: 0.09984241382181705]
	TIME [epoch: 7.34 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06138325923449551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06138325923449551 | validation: 0.12434599192949805]
	TIME [epoch: 7.33 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05781703818636072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05781703818636072 | validation: 0.06623505742117076]
	TIME [epoch: 7.37 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032762620203880884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032762620203880884 | validation: 0.06236931240771452]
	TIME [epoch: 7.35 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06191898060016028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06191898060016028 | validation: 0.12017094776688897]
	TIME [epoch: 7.33 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08082848402679307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08082848402679307 | validation: 0.0985823548562252]
	TIME [epoch: 7.34 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07725800943044374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07725800943044374 | validation: 0.0739037790230432]
	TIME [epoch: 7.33 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04159132861651678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04159132861651678 | validation: 0.054781743587782684]
	TIME [epoch: 7.33 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02765851027797736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02765851027797736 | validation: 0.05343857803381557]
	TIME [epoch: 7.34 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027063667372905505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027063667372905505 | validation: 0.08133752952863105]
	TIME [epoch: 7.35 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03345680405107517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03345680405107517 | validation: 0.09481249932365078]
	TIME [epoch: 7.34 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05050660557449973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05050660557449973 | validation: 0.09702082807503223]
	TIME [epoch: 7.33 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0534911930551667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0534911930551667 | validation: 0.0817229976823643]
	TIME [epoch: 7.33 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0505665272131796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0505665272131796 | validation: 0.056514314298213034]
	TIME [epoch: 7.35 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053347105563550934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053347105563550934 | validation: 0.06568390603985184]
	TIME [epoch: 7.4 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05325385420503112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05325385420503112 | validation: 0.04221719691243118]
	TIME [epoch: 7.34 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050351129152474236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050351129152474236 | validation: 0.06723629523050526]
	TIME [epoch: 7.35 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04318584739744408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04318584739744408 | validation: 0.06112949800803197]
	TIME [epoch: 7.35 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05356011173047033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05356011173047033 | validation: 0.12547202471692856]
	TIME [epoch: 7.35 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.069847974133691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.069847974133691 | validation: 0.08348712236691236]
	TIME [epoch: 7.36 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06757976879517477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06757976879517477 | validation: 0.1021219112252345]
	TIME [epoch: 7.36 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05561089753666997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05561089753666997 | validation: 0.08171029816399622]
	TIME [epoch: 7.34 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058648098574241636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058648098574241636 | validation: 0.1025517246176309]
	TIME [epoch: 7.36 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0597032424086383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0597032424086383 | validation: 0.0717790350364149]
	TIME [epoch: 7.35 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04658585777416236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04658585777416236 | validation: 0.04937653755899062]
	TIME [epoch: 7.36 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030416840294009304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030416840294009304 | validation: 0.05832810743626793]
	TIME [epoch: 7.36 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029913549731176874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029913549731176874 | validation: 0.05947608518273026]
	TIME [epoch: 7.34 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041278225790831076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041278225790831076 | validation: 0.0900227282983916]
	TIME [epoch: 7.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144331/states/model_phi1_4a_v_mmd1_1115.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 3521.361 seconds.
