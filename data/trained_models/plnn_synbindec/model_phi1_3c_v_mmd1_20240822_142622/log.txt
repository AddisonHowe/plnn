Args:
Namespace(name='model_phi1_3c_v_mmd1', outdir='out/model_training/model_phi1_3c_v_mmd1', training_data='data/training_data/data_phi1_3c/training', validation_data='data/training_data/data_phi1_3c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3730868055

Training model...

Saving initial model state to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.822482147398075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.822482147398075 | validation: 6.198905374924653]
	TIME [epoch: 27.4 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.022756341833899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.022756341833899 | validation: 6.1826602405176345]
	TIME [epoch: 3.69 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.190311010014673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.190311010014673 | validation: 6.003566022138153]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.015126277408796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.015126277408796 | validation: 4.879769622412339]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.738252813601728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.738252813601728 | validation: 5.928328590537827]
	TIME [epoch: 3.7 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.306117693272723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.306117693272723 | validation: 5.656075769122509]
	TIME [epoch: 3.69 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.768064212339541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.768064212339541 | validation: 4.510171580426479]
	TIME [epoch: 3.69 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.797016997955457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.797016997955457 | validation: 4.86603841062422]
	TIME [epoch: 3.69 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.824701090885121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.824701090885121 | validation: 4.530710907680653]
	TIME [epoch: 3.71 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.44227557528483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.44227557528483 | validation: 4.27079400107806]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.471259097733217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.471259097733217 | validation: 4.096541613357781]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.303284228348818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.303284228348818 | validation: 4.132065704914882]
	TIME [epoch: 3.68 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.155571038017996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.155571038017996 | validation: 4.055175879546311]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.068886884836068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.068886884836068 | validation: 3.923104497682178]
	TIME [epoch: 3.66 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.012628765854649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.012628765854649 | validation: 3.798260588486845]
	TIME [epoch: 3.66 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.924752551307626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.924752551307626 | validation: 3.7833686506528843]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8625514583022875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8625514583022875 | validation: 3.6579734636584926]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8123936657777158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8123936657777158 | validation: 3.6444103004414403]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7607345782306116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7607345782306116 | validation: 3.528299138680715]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7762855804622593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7762855804622593 | validation: 3.89127250991608]
	TIME [epoch: 3.67 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.93349277165384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.93349277165384 | validation: 3.5896540076270878]
	TIME [epoch: 3.65 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.769945978240937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.769945978240937 | validation: 3.406689003983771]
	TIME [epoch: 3.66 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.646360548621158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.646360548621158 | validation: 3.5219548334683126]
	TIME [epoch: 3.67 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.631093161926955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.631093161926955 | validation: 3.3667853648470394]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5943888338921512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5943888338921512 | validation: 3.3965871313948797]
	TIME [epoch: 3.66 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5739551717693656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5739551717693656 | validation: 3.3652410784788724]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.564224580424475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.564224580424475 | validation: 3.3797048120849094]
	TIME [epoch: 3.67 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5494236033828668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5494236033828668 | validation: 3.3187078552557794]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5827272188675177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5827272188675177 | validation: 3.5083122035117613]
	TIME [epoch: 3.67 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6130326604170437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6130326604170437 | validation: 3.2649547107404544]
	TIME [epoch: 3.66 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5294870550317468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5294870550317468 | validation: 3.3013591038379704]
	TIME [epoch: 3.68 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4588290725072377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4588290725072377 | validation: 3.2100543658781464]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4216846869334026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4216846869334026 | validation: 3.2006099363506375]
	TIME [epoch: 3.66 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.404979452931748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.404979452931748 | validation: 3.196718197214918]
	TIME [epoch: 3.66 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.386633206634992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.386633206634992 | validation: 3.156963711215159]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.373015585005096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.373015585005096 | validation: 3.1835008614272677]
	TIME [epoch: 3.67 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.362869127860684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.362869127860684 | validation: 3.123833686763414]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3628151093844454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3628151093844454 | validation: 3.2356924999966115]
	TIME [epoch: 3.67 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3804714807524476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3804714807524476 | validation: 3.13858508676911]
	TIME [epoch: 3.68 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3595465920411254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3595465920411254 | validation: 3.163863837530318]
	TIME [epoch: 3.68 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3512658058736644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3512658058736644 | validation: 3.192351580644839]
	TIME [epoch: 3.68 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3503757681792257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3503757681792257 | validation: 3.1709745074639533]
	TIME [epoch: 3.67 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4369920490418067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4369920490418067 | validation: 3.1855674428216725]
	TIME [epoch: 3.65 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3230115930501047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3230115930501047 | validation: 3.0351189028667878]
	TIME [epoch: 3.69 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.228487660742039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.228487660742039 | validation: 3.0048784604092313]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2283201586699115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2283201586699115 | validation: 3.0342891938657326]
	TIME [epoch: 3.66 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2013664070972436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2013664070972436 | validation: 2.9832177617266384]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1706862386455477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1706862386455477 | validation: 2.9724932784713864]
	TIME [epoch: 3.69 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1602678807196676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1602678807196676 | validation: 2.9666748943059598]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1493142037116035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1493142037116035 | validation: 2.946480621761373]
	TIME [epoch: 3.66 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.140298400576654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.140298400576654 | validation: 2.974931580289527]
	TIME [epoch: 3.67 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.157631517514951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.157631517514951 | validation: 3.014660904702424]
	TIME [epoch: 3.7 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.192111969074101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.192111969074101 | validation: 3.031775909008362]
	TIME [epoch: 3.66 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.207207421481758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.207207421481758 | validation: 2.8898504087450583]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.068842625384875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.068842625384875 | validation: 2.8560131533600295]
	TIME [epoch: 3.65 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0568824802155654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0568824802155654 | validation: 2.864400575604852]
	TIME [epoch: 3.69 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0530191386974934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0530191386974934 | validation: 2.8603590904323255]
	TIME [epoch: 3.69 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.04364948227438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.04364948227438 | validation: 2.848167882848107]
	TIME [epoch: 3.66 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0303343412169985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0303343412169985 | validation: 2.8104655823859463]
	TIME [epoch: 3.66 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.010185076777583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.010185076777583 | validation: 2.8048040113496633]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.003922536769211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.003922536769211 | validation: 2.7973217051824815]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9885407522561542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9885407522561542 | validation: 2.797229203746794]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9928943458544426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9928943458544426 | validation: 2.785930303732591]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9870658750349945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9870658750349945 | validation: 2.8077320001436323]
	TIME [epoch: 3.68 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9887206646534588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9887206646534588 | validation: 2.7517090412094443]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9419150068211706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9419150068211706 | validation: 2.749086076859098]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.927548119728074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.927548119728074 | validation: 2.715788361590635]
	TIME [epoch: 3.66 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.91183913378891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.91183913378891 | validation: 2.7274135559895374]
	TIME [epoch: 3.69 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9048396543873753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9048396543873753 | validation: 2.7157850897841436]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8890336553960925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8890336553960925 | validation: 2.724636601730851]
	TIME [epoch: 3.67 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.902530592119739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.902530592119739 | validation: 2.7427306330676453]
	TIME [epoch: 3.68 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9328696181261154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9328696181261154 | validation: 2.8096235581048554]
	TIME [epoch: 3.67 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9851449165350203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9851449165350203 | validation: 2.6776032755649144]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8497920115565787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8497920115565787 | validation: 2.6928412799532038]
	TIME [epoch: 3.68 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.858026944553827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.858026944553827 | validation: 2.687713615451875]
	TIME [epoch: 3.66 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.870815862646162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.870815862646162 | validation: 2.6512159504726287]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.825497392647302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.825497392647302 | validation: 2.6434336991011858]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.807449555772454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.807449555772454 | validation: 2.622408703573152]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8000173353932105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8000173353932105 | validation: 2.618624394680845]
	TIME [epoch: 3.66 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7947795107501117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7947795107501117 | validation: 2.606209324741253]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7782813396823274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7782813396823274 | validation: 2.5563237908916547]
	TIME [epoch: 3.66 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.745032501559623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.745032501559623 | validation: 2.509826444756051]
	TIME [epoch: 3.66 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6890757299948755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6890757299948755 | validation: 2.4550641715715713]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.641644204241623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.641644204241623 | validation: 2.544462463840544]
	TIME [epoch: 3.68 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6566008996213286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6566008996213286 | validation: 2.111962444728119]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2245423280041146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2245423280041146 | validation: 1.8225194219085503]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9268228260047084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9268228260047084 | validation: 1.7424645649519634]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9796119686776588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9796119686776588 | validation: 1.1893215743427263]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.32362467972822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.32362467972822 | validation: 1.0453253748496907]
	TIME [epoch: 3.69 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0898343321913198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0898343321913198 | validation: 1.255896740015621]
	TIME [epoch: 3.65 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5281198187013791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5281198187013791 | validation: 1.2811600097447955]
	TIME [epoch: 3.66 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3151763887308685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3151763887308685 | validation: 0.9446472255184446]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.961693094604615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.961693094604615 | validation: 0.931454346688494]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9668296544470178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9668296544470178 | validation: 0.8471478801389223]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8626892837284692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8626892837284692 | validation: 0.8357477897469399]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8670206320398545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8670206320398545 | validation: 0.8142142138711446]
	TIME [epoch: 3.69 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8272942235944554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8272942235944554 | validation: 0.801620579451819]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8134462773561834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8134462773561834 | validation: 0.775808832827271]
	TIME [epoch: 3.69 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8090064179244394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8090064179244394 | validation: 0.8145673395717596]
	TIME [epoch: 3.68 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8057957474872467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8057957474872467 | validation: 0.7858569993382647]
	TIME [epoch: 3.69 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8019969398021753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8019969398021753 | validation: 0.8115462779312038]
	TIME [epoch: 3.68 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8095791781410328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8095791781410328 | validation: 0.7407237864964545]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7614842435626252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7614842435626252 | validation: 0.7315689849146324]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.747860017203499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.747860017203499 | validation: 0.7491142153565807]
	TIME [epoch: 3.7 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7535182919432842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7535182919432842 | validation: 0.7297660130810801]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7715714886744816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7715714886744816 | validation: 0.9335202588983009]
	TIME [epoch: 3.65 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0019130184847602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0019130184847602 | validation: 0.7566981244525799]
	TIME [epoch: 3.66 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7685955018232289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7685955018232289 | validation: 0.8028772995268462]
	TIME [epoch: 3.68 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8354567059398106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8354567059398106 | validation: 0.7490212368124056]
	TIME [epoch: 3.67 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7451233597708585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7451233597708585 | validation: 0.7553894688241557]
	TIME [epoch: 3.66 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.719309397263178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.719309397263178 | validation: 0.7333630514935306]
	TIME [epoch: 3.67 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7142661605343449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7142661605343449 | validation: 0.7212655342699712]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7065212498771232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7065212498771232 | validation: 0.7291423838123636]
	TIME [epoch: 3.67 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6960040078308382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6960040078308382 | validation: 0.6949162721260882]
	TIME [epoch: 3.66 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.699212048117217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.699212048117217 | validation: 0.7190315743196367]
	TIME [epoch: 3.66 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6919377809199122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6919377809199122 | validation: 0.7059698733024105]
	TIME [epoch: 3.67 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6899182534897231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6899182534897231 | validation: 0.7249686978850003]
	TIME [epoch: 3.66 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6879880179596768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6879880179596768 | validation: 0.7914575866664738]
	TIME [epoch: 3.66 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7722189787312513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7722189787312513 | validation: 0.7282689138647511]
	TIME [epoch: 3.67 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7349976831508351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7349976831508351 | validation: 0.7099660359346238]
	TIME [epoch: 3.67 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7073772388218393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7073772388218393 | validation: 0.7201828804687189]
	TIME [epoch: 3.67 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6805749622478646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6805749622478646 | validation: 0.7093197998154936]
	TIME [epoch: 3.66 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.685182278316486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.685182278316486 | validation: 0.7505101022103166]
	TIME [epoch: 3.67 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7121065470806843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7121065470806843 | validation: 0.7683970696910956]
	TIME [epoch: 3.65 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7849287978432962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7849287978432962 | validation: 0.7417796954735411]
	TIME [epoch: 3.66 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7111017809858058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7111017809858058 | validation: 0.677036187323488]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6813052761743217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6813052761743217 | validation: 0.7117226419471837]
	TIME [epoch: 3.68 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6679933433691795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6679933433691795 | validation: 0.724073417390687]
	TIME [epoch: 3.67 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6961967926117499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6961967926117499 | validation: 0.7783494650805126]
	TIME [epoch: 3.67 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.825322084111533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.825322084111533 | validation: 0.7802336780811291]
	TIME [epoch: 3.66 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7796335044127025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7796335044127025 | validation: 0.8502848619055823]
	TIME [epoch: 3.67 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8309716351197277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8309716351197277 | validation: 0.6926178796058592]
	TIME [epoch: 3.65 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7245173904278636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7245173904278636 | validation: 0.6903754770415809]
	TIME [epoch: 3.68 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6896092987562679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6896092987562679 | validation: 0.73922535046887]
	TIME [epoch: 3.66 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7007416827383741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7007416827383741 | validation: 0.7073215879419046]
	TIME [epoch: 3.66 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.705584855361432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.705584855361432 | validation: 0.6831226179959761]
	TIME [epoch: 3.67 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6687270302887995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6687270302887995 | validation: 0.7239625166941281]
	TIME [epoch: 3.65 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7020981340135398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7020981340135398 | validation: 0.7263985991863419]
	TIME [epoch: 3.66 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7393955155675177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7393955155675177 | validation: 0.6868761632858255]
	TIME [epoch: 3.68 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6943913954374753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6943913954374753 | validation: 0.738454703599129]
	TIME [epoch: 3.67 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7122141846822889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7122141846822889 | validation: 0.7072332221612462]
	TIME [epoch: 3.66 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6989411630304653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6989411630304653 | validation: 0.7674650326857637]
	TIME [epoch: 3.67 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7286445353146297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7286445353146297 | validation: 0.8001590841782061]
	TIME [epoch: 3.66 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7930696600620659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7930696600620659 | validation: 0.6904172256457747]
	TIME [epoch: 3.66 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6669242840204904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6669242840204904 | validation: 0.7370295714114289]
	TIME [epoch: 3.66 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.712834705521455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.712834705521455 | validation: 0.7374362679537396]
	TIME [epoch: 3.65 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7087965668778984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7087965668778984 | validation: 0.7378670044611644]
	TIME [epoch: 3.65 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7222251455710809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7222251455710809 | validation: 0.7041280037491195]
	TIME [epoch: 3.66 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7142664660274026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7142664660274026 | validation: 0.739949668829658]
	TIME [epoch: 3.67 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6962856624699555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6962856624699555 | validation: 0.7169758857470891]
	TIME [epoch: 3.66 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6822356090483431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6822356090483431 | validation: 0.7114123357178979]
	TIME [epoch: 3.65 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6648698911607442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6648698911607442 | validation: 0.7141799539569602]
	TIME [epoch: 3.66 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6713074848265571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6713074848265571 | validation: 0.7226748686307949]
	TIME [epoch: 3.66 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6724567958442438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6724567958442438 | validation: 0.7135154338641829]
	TIME [epoch: 3.66 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6694244970739743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6694244970739743 | validation: 0.7225466641397219]
	TIME [epoch: 3.66 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7215100982029417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7215100982029417 | validation: 0.7698692224170585]
	TIME [epoch: 3.67 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7452361022116037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7452361022116037 | validation: 0.810846532114974]
	TIME [epoch: 3.65 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7863082305182877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7863082305182877 | validation: 0.6805156096072111]
	TIME [epoch: 3.65 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6572752986204013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6572752986204013 | validation: 0.7581911614266137]
	TIME [epoch: 3.64 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7136885829202205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7136885829202205 | validation: 0.7492601902229784]
	TIME [epoch: 3.65 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7349157663527635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7349157663527635 | validation: 0.7013931786007801]
	TIME [epoch: 3.65 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.695533084611233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.695533084611233 | validation: 0.7465520230523977]
	TIME [epoch: 3.66 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7302024916420546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7302024916420546 | validation: 0.691181145039363]
	TIME [epoch: 3.65 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6706047681970579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6706047681970579 | validation: 0.7568780372962384]
	TIME [epoch: 3.66 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7941813706776316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7941813706776316 | validation: 0.7368826240837338]
	TIME [epoch: 3.65 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7517241486387066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7517241486387066 | validation: 0.807753867750485]
	TIME [epoch: 3.66 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7654857067044885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7654857067044885 | validation: 0.7176358567316035]
	TIME [epoch: 3.66 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6894116410719329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6894116410719329 | validation: 0.7362252000423964]
	TIME [epoch: 3.69 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7110545770525997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7110545770525997 | validation: 0.7177010117790085]
	TIME [epoch: 3.66 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6757840028515849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6757840028515849 | validation: 0.7155262695078599]
	TIME [epoch: 3.66 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6744018062651711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6744018062651711 | validation: 0.718305396698185]
	TIME [epoch: 3.65 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6601660329917999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6601660329917999 | validation: 0.7003033024349045]
	TIME [epoch: 3.67 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.664042373925872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.664042373925872 | validation: 0.6937201522440885]
	TIME [epoch: 3.67 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6622820450278712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6622820450278712 | validation: 0.7062095960751122]
	TIME [epoch: 3.67 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6634639845748056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6634639845748056 | validation: 0.7200574111871094]
	TIME [epoch: 3.65 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7068957559496019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7068957559496019 | validation: 0.7396816572794043]
	TIME [epoch: 3.67 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6975317648080804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6975317648080804 | validation: 0.7919705460141615]
	TIME [epoch: 3.67 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7722349454981339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7722349454981339 | validation: 0.70243821920545]
	TIME [epoch: 3.66 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6661080269554622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6661080269554622 | validation: 0.680178321289604]
	TIME [epoch: 3.67 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6779780877518898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6779780877518898 | validation: 0.7231246879507716]
	TIME [epoch: 3.65 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7063346390922132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7063346390922132 | validation: 0.6748674554724219]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.686648009607665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.686648009607665 | validation: 0.7015585353079885]
	TIME [epoch: 3.71 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6775635642821706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6775635642821706 | validation: 0.7082797976742183]
	TIME [epoch: 3.66 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.671080213055626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.671080213055626 | validation: 0.7351427584774655]
	TIME [epoch: 3.67 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6977202045291514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6977202045291514 | validation: 0.746157274732551]
	TIME [epoch: 3.68 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7518483502025631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7518483502025631 | validation: 0.7154947108211912]
	TIME [epoch: 3.66 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6916880983467217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6916880983467217 | validation: 0.7068963832830035]
	TIME [epoch: 3.67 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6772236712408122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6772236712408122 | validation: 0.6726951685830771]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6518272338725644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6518272338725644 | validation: 0.7237116436838731]
	TIME [epoch: 3.68 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6782197784338433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6782197784338433 | validation: 0.7326462256806979]
	TIME [epoch: 3.66 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7038176440739414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7038176440739414 | validation: 0.7204896490664213]
	TIME [epoch: 3.67 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6833056178973514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6833056178973514 | validation: 0.720948110753373]
	TIME [epoch: 3.67 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7225786735966019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7225786735966019 | validation: 0.7073894783437112]
	TIME [epoch: 3.67 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6607179185602832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6607179185602832 | validation: 0.7154136548683]
	TIME [epoch: 3.66 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.711822060895634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.711822060895634 | validation: 0.6690995799637787]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6822341394613456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6822341394613456 | validation: 0.7022100885079431]
	TIME [epoch: 3.69 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.672089986541832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.672089986541832 | validation: 0.7365072881200598]
	TIME [epoch: 3.66 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7025152791179394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7025152791179394 | validation: 0.7563476905775461]
	TIME [epoch: 3.68 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7285048104398522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7285048104398522 | validation: 0.7370800755379213]
	TIME [epoch: 3.66 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7049846795969859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7049846795969859 | validation: 0.6962930033993664]
	TIME [epoch: 3.66 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.680951696163818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.680951696163818 | validation: 0.7071782513828275]
	TIME [epoch: 29.9 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.728713231344197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.728713231344197 | validation: 0.66317407424313]
	TIME [epoch: 7.99 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6734385722424833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6734385722424833 | validation: 0.7241619998666411]
	TIME [epoch: 7.98 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.705278225620013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.705278225620013 | validation: 0.7265682377769158]
	TIME [epoch: 7.98 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7353749142248731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7353749142248731 | validation: 0.7485735142740064]
	TIME [epoch: 7.95 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6978182754364312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6978182754364312 | validation: 0.6981141783392991]
	TIME [epoch: 7.97 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6669960558502853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6669960558502853 | validation: 0.7078540290415151]
	TIME [epoch: 7.96 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6721762559958121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6721762559958121 | validation: 0.6949590978522937]
	TIME [epoch: 8.02 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6803228120324036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6803228120324036 | validation: 0.6882981542940304]
	TIME [epoch: 7.96 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6600868785110415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6600868785110415 | validation: 0.7062274326796736]
	TIME [epoch: 7.93 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6765925475227421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6765925475227421 | validation: 0.7050705363185392]
	TIME [epoch: 7.9 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7050900391583214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7050900391583214 | validation: 0.7260869085573654]
	TIME [epoch: 7.94 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6966759901020051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6966759901020051 | validation: 0.7324515115572852]
	TIME [epoch: 7.93 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7065734911911667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7065734911911667 | validation: 0.6749007958254826]
	TIME [epoch: 7.96 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6470606673928484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6470606673928484 | validation: 0.6934154384388699]
	TIME [epoch: 7.9 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6566312182937073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6566312182937073 | validation: 0.7297353924437024]
	TIME [epoch: 7.9 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7061611981560271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7061611981560271 | validation: 0.680843754276933]
	TIME [epoch: 7.91 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6571890608777764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6571890608777764 | validation: 0.6975966714498261]
	TIME [epoch: 7.92 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6450284085989506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6450284085989506 | validation: 0.6983866269444925]
	TIME [epoch: 7.93 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6731407957570192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6731407957570192 | validation: 0.6864733184579814]
	TIME [epoch: 7.95 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7008260799754179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7008260799754179 | validation: 0.6926608487345765]
	TIME [epoch: 7.93 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6608347214487144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6608347214487144 | validation: 0.6910706507465242]
	TIME [epoch: 7.94 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6690345321363347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6690345321363347 | validation: 0.7034828266790358]
	TIME [epoch: 7.93 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6849961306677236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6849961306677236 | validation: 0.8494556142526868]
	TIME [epoch: 7.93 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7940561411573454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7940561411573454 | validation: 0.8403633843980001]
	TIME [epoch: 7.93 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8110305117735499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8110305117735499 | validation: 0.7001293007984913]
	TIME [epoch: 7.92 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.681310757318906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.681310757318906 | validation: 0.7241708657867958]
	TIME [epoch: 7.93 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6907663037353637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6907663037353637 | validation: 0.6961033357122761]
	TIME [epoch: 7.95 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6667962552585951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6667962552585951 | validation: 0.704373231308196]
	TIME [epoch: 7.91 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6706319619269463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6706319619269463 | validation: 0.683622056488885]
	TIME [epoch: 7.91 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6781965516124189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6781965516124189 | validation: 0.6842527015938441]
	TIME [epoch: 7.95 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.657255107713875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.657255107713875 | validation: 0.6953563689045679]
	TIME [epoch: 7.94 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6814725621681472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6814725621681472 | validation: 0.6873185859275148]
	TIME [epoch: 7.99 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6725020125711919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6725020125711919 | validation: 0.7085552076880351]
	TIME [epoch: 7.92 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6792976772197262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6792976772197262 | validation: 0.7584683577791937]
	TIME [epoch: 7.95 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7225231691058528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7225231691058528 | validation: 0.701695918525831]
	TIME [epoch: 7.92 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.657310732905234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.657310732905234 | validation: 0.6934970053881792]
	TIME [epoch: 7.94 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6946379922723283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6946379922723283 | validation: 0.7121246775260147]
	TIME [epoch: 7.91 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6675254251410996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6675254251410996 | validation: 0.7432546348196462]
	TIME [epoch: 7.95 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6973585128871365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6973585128871365 | validation: 0.7106656077265565]
	TIME [epoch: 7.92 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7134291952826968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7134291952826968 | validation: 0.6800141575962334]
	TIME [epoch: 7.95 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6732709863382879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6732709863382879 | validation: 0.6844819627596958]
	TIME [epoch: 7.98 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6703098482132941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6703098482132941 | validation: 0.6630105820432933]
	TIME [epoch: 7.94 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_243.pth
	Model improved!!!
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6475304759537232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6475304759537232 | validation: 0.7108775580124255]
	TIME [epoch: 7.92 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6602358682587152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6602358682587152 | validation: 0.7027147802388909]
	TIME [epoch: 7.95 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6768349336584634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6768349336584634 | validation: 0.7304478585204173]
	TIME [epoch: 7.94 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6913561735093413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6913561735093413 | validation: 0.7122100620408965]
	TIME [epoch: 7.92 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7059338755843578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7059338755843578 | validation: 0.701299850840171]
	TIME [epoch: 7.95 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6780121652897665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6780121652897665 | validation: 0.7314590735829514]
	TIME [epoch: 7.95 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.764765249105524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.764765249105524 | validation: 0.6896099074372288]
	TIME [epoch: 7.93 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6800343445860285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6800343445860285 | validation: 0.7185823250064808]
	TIME [epoch: 7.97 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7161306237640852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7161306237640852 | validation: 0.7190463122572258]
	TIME [epoch: 7.93 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6815970894661768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6815970894661768 | validation: 0.7263763121244655]
	TIME [epoch: 7.96 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.712876488201687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.712876488201687 | validation: 0.6685148266881802]
	TIME [epoch: 7.93 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6774822889102577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6774822889102577 | validation: 0.6768552609992959]
	TIME [epoch: 7.93 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6753586385869295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6753586385869295 | validation: 0.7097971813072091]
	TIME [epoch: 7.92 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6683228948291878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6683228948291878 | validation: 0.7075730472038937]
	TIME [epoch: 7.96 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6728280927421838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6728280927421838 | validation: 0.684546352692095]
	TIME [epoch: 7.92 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6567738150216761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6567738150216761 | validation: 0.7029272504239943]
	TIME [epoch: 7.95 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6580611144163057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6580611144163057 | validation: 0.7038779437731152]
	TIME [epoch: 7.92 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6584958194687128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6584958194687128 | validation: 0.7074507466033669]
	TIME [epoch: 7.96 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6586169480603461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6586169480603461 | validation: 0.7045948006721098]
	TIME [epoch: 7.93 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7005399431094098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7005399431094098 | validation: 0.7127417748882345]
	TIME [epoch: 7.95 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6699732188680687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6699732188680687 | validation: 0.705118139553861]
	TIME [epoch: 7.96 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6833896147638145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6833896147638145 | validation: 0.7061254808444035]
	TIME [epoch: 7.98 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6644183655632742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6644183655632742 | validation: 0.70330207788629]
	TIME [epoch: 7.96 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6931861777451667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6931861777451667 | validation: 0.678550879466074]
	TIME [epoch: 7.99 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6453375904532135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6453375904532135 | validation: 0.6707957930989084]
	TIME [epoch: 7.95 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.646095938001954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.646095938001954 | validation: 0.6981649186239296]
	TIME [epoch: 7.98 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6635263890274178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6635263890274178 | validation: 0.6704546455979308]
	TIME [epoch: 7.96 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6346588277025004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6346588277025004 | validation: 0.6797523836057543]
	TIME [epoch: 7.97 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6317385036080119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6317385036080119 | validation: 0.6990861031340695]
	TIME [epoch: 7.96 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6501858068533306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6501858068533306 | validation: 0.7771127598912786]
	TIME [epoch: 7.94 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.760934076485006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.760934076485006 | validation: 0.8171014983784641]
	TIME [epoch: 7.95 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7691045195740017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7691045195740017 | validation: 0.6943529690693142]
	TIME [epoch: 7.95 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6748160502366182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6748160502366182 | validation: 0.671201293936798]
	TIME [epoch: 7.95 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6340998819087704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6340998819087704 | validation: 0.6743532664877535]
	TIME [epoch: 7.94 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.641013628167868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.641013628167868 | validation: 0.6587381060900845]
	TIME [epoch: 7.91 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_278.pth
	Model improved!!!
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6529128881948912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6529128881948912 | validation: 0.6885354881527086]
	TIME [epoch: 7.96 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6468670520197725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6468670520197725 | validation: 0.6802464217614699]
	TIME [epoch: 7.97 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6592375262512583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6592375262512583 | validation: 0.6930944384448663]
	TIME [epoch: 8 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6633581325000613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6633581325000613 | validation: 0.7005508643852437]
	TIME [epoch: 7.98 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6880332180842978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6880332180842978 | validation: 0.7201362777159865]
	TIME [epoch: 7.99 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6789968510464791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6789968510464791 | validation: 0.6848019167403582]
	TIME [epoch: 7.96 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6662965986786847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6662965986786847 | validation: 0.6927900388734006]
	TIME [epoch: 7.99 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6496827151931212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6496827151931212 | validation: 0.688427415354574]
	TIME [epoch: 7.98 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6442119784956658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6442119784956658 | validation: 0.6753697177197534]
	TIME [epoch: 8.02 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6322823024949056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6322823024949056 | validation: 0.668105716400533]
	TIME [epoch: 8 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6358318452668227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6358318452668227 | validation: 0.6824979154009814]
	TIME [epoch: 7.97 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6681175868160731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6681175868160731 | validation: 0.671600608051033]
	TIME [epoch: 8 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7204943335199826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7204943335199826 | validation: 0.6606685198641906]
	TIME [epoch: 8.02 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6282850854170129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6282850854170129 | validation: 0.697436969954874]
	TIME [epoch: 7.97 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6379190315229281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6379190315229281 | validation: 0.6684681843055351]
	TIME [epoch: 8 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6187062323986241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6187062323986241 | validation: 1.1393011508503572]
	TIME [epoch: 7.97 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2865063817453462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2865063817453462 | validation: 0.6710130814959432]
	TIME [epoch: 7.97 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6288032402903525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6288032402903525 | validation: 0.6525328116037872]
	TIME [epoch: 7.96 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_296.pth
	Model improved!!!
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.616550089991467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.616550089991467 | validation: 0.6246282212778681]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_297.pth
	Model improved!!!
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5727470676764591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5727470676764591 | validation: 0.7964478612517145]
	TIME [epoch: 7.96 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9491327951235053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9491327951235053 | validation: 0.7365234438349584]
	TIME [epoch: 8.01 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7883135570097934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7883135570097934 | validation: 0.6785579213610925]
	TIME [epoch: 8.01 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7092806518094911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7092806518094911 | validation: 0.644772157567428]
	TIME [epoch: 7.98 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6572592748547429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6572592748547429 | validation: 0.6438374102011756]
	TIME [epoch: 7.97 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.66289975077495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.66289975077495 | validation: 0.6390946136380047]
	TIME [epoch: 7.96 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6300018959544965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6300018959544965 | validation: 0.6590144568020091]
	TIME [epoch: 7.97 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6418435250813628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6418435250813628 | validation: 0.6575696957044006]
	TIME [epoch: 7.98 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6151732146910809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6151732146910809 | validation: 0.6524500475936977]
	TIME [epoch: 8 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6174047299221986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6174047299221986 | validation: 0.6568286084216375]
	TIME [epoch: 7.96 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.602441384777453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.602441384777453 | validation: 0.6491324298757579]
	TIME [epoch: 7.95 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.588531676799598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.588531676799598 | validation: 0.6304713021122275]
	TIME [epoch: 7.95 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5789757220424799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5789757220424799 | validation: 0.6877906981544379]
	TIME [epoch: 7.97 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5807872107626708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5807872107626708 | validation: 5.79082002468596]
	TIME [epoch: 7.98 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.671945708897506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.671945708897506 | validation: 0.8205153983746802]
	TIME [epoch: 7.96 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7449502200385859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7449502200385859 | validation: 0.8146538082474355]
	TIME [epoch: 7.97 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8764633362388322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8764633362388322 | validation: 0.7498075457614095]
	TIME [epoch: 7.96 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7659263756751074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7659263756751074 | validation: 0.6324085528294184]
	TIME [epoch: 7.94 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6297834676132626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6297834676132626 | validation: 0.6633244066739898]
	TIME [epoch: 7.94 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6442187055322686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6442187055322686 | validation: 0.6429404852933353]
	TIME [epoch: 7.94 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6249578755754367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6249578755754367 | validation: 0.6313697845080948]
	TIME [epoch: 7.94 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6050753467578401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6050753467578401 | validation: 0.6269738091292362]
	TIME [epoch: 7.93 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5971262334375975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5971262334375975 | validation: 0.6236384503470229]
	TIME [epoch: 7.95 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_320.pth
	Model improved!!!
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.593634924090695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.593634924090695 | validation: 0.6444687679455638]
	TIME [epoch: 7.97 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.57907037108055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.57907037108055 | validation: 0.6426999204256246]
	TIME [epoch: 7.95 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5691081075094965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5691081075094965 | validation: 0.6047919982259619]
	TIME [epoch: 7.96 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_323.pth
	Model improved!!!
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5508902979996347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5508902979996347 | validation: 0.6234075553083424]
	TIME [epoch: 7.97 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5354929718750031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5354929718750031 | validation: 0.6391955393258226]
	TIME [epoch: 7.96 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5241524100353044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5241524100353044 | validation: 1.2568991034034935]
	TIME [epoch: 7.95 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5118211347405446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5118211347405446 | validation: 1.1900536984841428]
	TIME [epoch: 7.97 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.405066486808443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.405066486808443 | validation: 1.1193190114955058]
	TIME [epoch: 7.93 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2265627363788247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2265627363788247 | validation: 0.9641395067220359]
	TIME [epoch: 7.96 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9780181052030732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9780181052030732 | validation: 0.7839630131513303]
	TIME [epoch: 7.96 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.776649640331994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.776649640331994 | validation: 0.6672273605813814]
	TIME [epoch: 7.95 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7326047294923413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7326047294923413 | validation: 0.6858012788790728]
	TIME [epoch: 7.98 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.673110815497761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.673110815497761 | validation: 0.6744655398476591]
	TIME [epoch: 7.96 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6756052661998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6756052661998 | validation: 0.685923533721947]
	TIME [epoch: 7.94 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.669569903721064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.669569903721064 | validation: 0.6571459940040223]
	TIME [epoch: 7.93 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6553526630067499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6553526630067499 | validation: 0.6696192241217384]
	TIME [epoch: 7.97 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6528072651786367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6528072651786367 | validation: 0.67669096603542]
	TIME [epoch: 7.99 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6556307869588922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6556307869588922 | validation: 0.677714282895307]
	TIME [epoch: 7.97 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6382560880271242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6382560880271242 | validation: 0.69219469786501]
	TIME [epoch: 7.96 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6488605614968261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6488605614968261 | validation: 0.6728587101638143]
	TIME [epoch: 7.94 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6485982523078708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6485982523078708 | validation: 0.6647220257623584]
	TIME [epoch: 7.92 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.646714159657626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.646714159657626 | validation: 0.6719266981694573]
	TIME [epoch: 7.94 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6440193955160501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6440193955160501 | validation: 0.6618028391681267]
	TIME [epoch: 7.96 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6429534673037693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6429534673037693 | validation: 0.6789715037804722]
	TIME [epoch: 7.98 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6476911500190442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6476911500190442 | validation: 0.6558373444881834]
	TIME [epoch: 7.99 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6520373703070286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6520373703070286 | validation: 0.6752120986958966]
	TIME [epoch: 7.95 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.647833512400886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.647833512400886 | validation: 0.6546734962155688]
	TIME [epoch: 7.96 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6579729893568796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6579729893568796 | validation: 0.6916759166877973]
	TIME [epoch: 7.98 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6532442970895446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6532442970895446 | validation: 0.6643486769010389]
	TIME [epoch: 7.98 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6612109079462676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6612109079462676 | validation: 0.6585548952399087]
	TIME [epoch: 8.01 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6428996108058641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6428996108058641 | validation: 0.657122365560492]
	TIME [epoch: 7.96 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6461351214038419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6461351214038419 | validation: 0.67571412495679]
	TIME [epoch: 7.97 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6378464754953381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6378464754953381 | validation: 0.6792673664833059]
	TIME [epoch: 7.95 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6653876258497783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6653876258497783 | validation: 0.7085883787365526]
	TIME [epoch: 7.97 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6787438201216669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6787438201216669 | validation: 0.6668926617368129]
	TIME [epoch: 7.97 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6677417048191703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6677417048191703 | validation: 0.666367496381092]
	TIME [epoch: 8 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6431634697564311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6431634697564311 | validation: 0.7280555808686184]
	TIME [epoch: 7.99 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7033073246635096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7033073246635096 | validation: 0.6826724703805079]
	TIME [epoch: 8.01 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.639375730133125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.639375730133125 | validation: 0.6693097267354186]
	TIME [epoch: 7.95 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6342057036268633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6342057036268633 | validation: 0.6510329205827008]
	TIME [epoch: 7.99 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6185070450438808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6185070450438808 | validation: 0.6658745721885865]
	TIME [epoch: 8 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6199923718261691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6199923718261691 | validation: 0.6465318229491103]
	TIME [epoch: 7.98 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.61418731544363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.61418731544363 | validation: 0.7527686311837072]
	TIME [epoch: 7.96 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6999115863971318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6999115863971318 | validation: 0.8888385373662988]
	TIME [epoch: 7.97 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.914382511037098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.914382511037098 | validation: 0.7000955193538503]
	TIME [epoch: 7.93 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6371385426869912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6371385426869912 | validation: 0.6442659695714683]
	TIME [epoch: 8 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5977717372492445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5977717372492445 | validation: 0.6394319845358911]
	TIME [epoch: 7.98 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5991215749426471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5991215749426471 | validation: 0.657148645260596]
	TIME [epoch: 7.99 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5987343623175786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5987343623175786 | validation: 0.6408692220639226]
	TIME [epoch: 7.94 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5911062812602033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5911062812602033 | validation: 0.6719098584241414]
	TIME [epoch: 7.99 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5823100997083066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5823100997083066 | validation: 0.6627435017142894]
	TIME [epoch: 7.92 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6266978328929157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6266978328929157 | validation: 0.8443371897348424]
	TIME [epoch: 7.98 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8276455499506458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8276455499506458 | validation: 0.8600036228611367]
	TIME [epoch: 7.97 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9547710093887922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9547710093887922 | validation: 0.7759946134317608]
	TIME [epoch: 7.95 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8719163833814904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8719163833814904 | validation: 0.7036210541778664]
	TIME [epoch: 7.96 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7071749899873189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7071749899873189 | validation: 0.7133035112367344]
	TIME [epoch: 7.98 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7034339141825461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7034339141825461 | validation: 0.6511956259042888]
	TIME [epoch: 7.97 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6533743190227008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6533743190227008 | validation: 0.6600549442725845]
	TIME [epoch: 7.99 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6419993678266723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6419993678266723 | validation: 0.6378425087295476]
	TIME [epoch: 7.96 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6329286092143445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6329286092143445 | validation: 0.6471164290239765]
	TIME [epoch: 7.94 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6276743878516071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6276743878516071 | validation: 0.6435124130423817]
	TIME [epoch: 7.97 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6229972703752606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6229972703752606 | validation: 0.6459343114712327]
	TIME [epoch: 7.94 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.622535189758557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.622535189758557 | validation: 0.6415397774231107]
	TIME [epoch: 7.95 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6207451803643923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6207451803643923 | validation: 0.6524030364436717]
	TIME [epoch: 7.98 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.616721182278315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.616721182278315 | validation: 0.6515183976947783]
	TIME [epoch: 7.95 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6101971592053895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6101971592053895 | validation: 0.6503557255700878]
	TIME [epoch: 7.94 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6127935099774922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6127935099774922 | validation: 0.6559298664290285]
	TIME [epoch: 7.93 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6104826382810798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6104826382810798 | validation: 0.6855434063246135]
	TIME [epoch: 7.91 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6526699912679425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6526699912679425 | validation: 0.6920231001376185]
	TIME [epoch: 7.97 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.63583367858223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.63583367858223 | validation: 0.6618354130373553]
	TIME [epoch: 7.97 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6837176745070938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6837176745070938 | validation: 0.6416175613961221]
	TIME [epoch: 7.95 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6300822405421291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6300822405421291 | validation: 0.6767804626157141]
	TIME [epoch: 7.96 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6139203696352343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6139203696352343 | validation: 0.6495154362810143]
	TIME [epoch: 7.93 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5860103542230827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5860103542230827 | validation: 0.6268874174750035]
	TIME [epoch: 7.93 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5860712602041371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5860712602041371 | validation: 0.666985374550955]
	TIME [epoch: 7.98 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5932699004523293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5932699004523293 | validation: 0.6994025244597912]
	TIME [epoch: 7.95 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6887256752078903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6887256752078903 | validation: 0.8052383348267402]
	TIME [epoch: 7.95 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7414240097580503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7414240097580503 | validation: 0.7214313626101709]
	TIME [epoch: 7.96 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7162385167039408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7162385167039408 | validation: 0.6877271921601439]
	TIME [epoch: 7.96 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6235790439410369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6235790439410369 | validation: 0.6653049894529199]
	TIME [epoch: 7.93 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5774888576953207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5774888576953207 | validation: 0.6463691419747871]
	TIME [epoch: 7.95 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5366273402165561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5366273402165561 | validation: 0.6474120029679054]
	TIME [epoch: 7.96 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5183513985545044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5183513985545044 | validation: 0.6599247383678479]
	TIME [epoch: 7.96 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5668845897790348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5668845897790348 | validation: 0.9400145740946151]
	TIME [epoch: 7.95 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1997187245119236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1997187245119236 | validation: 0.6378937719767428]
	TIME [epoch: 7.95 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.643947949455051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.643947949455051 | validation: 0.7152481406525708]
	TIME [epoch: 7.93 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7256158649201987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7256158649201987 | validation: 0.6249955204900983]
	TIME [epoch: 7.95 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6056729360765308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6056729360765308 | validation: 0.5913973078337081]
	TIME [epoch: 7.94 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_408.pth
	Model improved!!!
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5693689556031335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5693689556031335 | validation: 0.5970604010156112]
	TIME [epoch: 7.97 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5534163548513794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5534163548513794 | validation: 0.6340194505542328]
	TIME [epoch: 7.96 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5292965597376872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5292965597376872 | validation: 0.6031617655902909]
	TIME [epoch: 7.95 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5295532112067504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5295532112067504 | validation: 0.7552744365580968]
	TIME [epoch: 8 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7447660621617052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7447660621617052 | validation: 0.6324793486737325]
	TIME [epoch: 7.99 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5909099798200507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5909099798200507 | validation: 0.642441127242398]
	TIME [epoch: 7.97 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6017171101725772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6017171101725772 | validation: 0.6400203346364169]
	TIME [epoch: 7.95 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6102970203419017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6102970203419017 | validation: 0.6412193523991152]
	TIME [epoch: 7.98 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5627003700605668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5627003700605668 | validation: 0.5339123283870993]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_417.pth
	Model improved!!!
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5083833373048653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5083833373048653 | validation: 0.5327984452706691]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_418.pth
	Model improved!!!
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4674542607050644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4674542607050644 | validation: 0.6824007452038127]
	TIME [epoch: 7.95 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5416925470141328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5416925470141328 | validation: 1.0325748844566456]
	TIME [epoch: 7.97 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3784551532049016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3784551532049016 | validation: 0.6386424259415779]
	TIME [epoch: 7.99 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7354864881367619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7354864881367619 | validation: 0.8171796034881972]
	TIME [epoch: 7.96 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8420333900584933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8420333900584933 | validation: 0.6236504151501551]
	TIME [epoch: 8.03 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6288000459295472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6288000459295472 | validation: 0.6229066447043702]
	TIME [epoch: 8.03 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.606564691570609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.606564691570609 | validation: 0.5745775393887455]
	TIME [epoch: 8.01 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5799576883924371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5799576883924371 | validation: 0.5733773145136267]
	TIME [epoch: 7.96 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5376558522907329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5376558522907329 | validation: 0.5681555593248567]
	TIME [epoch: 7.98 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5298143817327977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5298143817327977 | validation: 0.5273785931187952]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_428.pth
	Model improved!!!
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5072046791193328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5072046791193328 | validation: 0.5385694307373874]
	TIME [epoch: 7.94 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46870964048976943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46870964048976943 | validation: 0.5636029676984203]
	TIME [epoch: 7.93 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45132170071560895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45132170071560895 | validation: 0.6467281278892411]
	TIME [epoch: 7.92 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.58629953549595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.58629953549595 | validation: 1.2270351962563264]
	TIME [epoch: 8 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4292936849272633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4292936849272633 | validation: 0.6944657842573858]
	TIME [epoch: 8.01 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9057694118135345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9057694118135345 | validation: 0.6072706848529774]
	TIME [epoch: 8.01 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.683429559881292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.683429559881292 | validation: 0.7755454494000793]
	TIME [epoch: 8.03 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7854398710442573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7854398710442573 | validation: 0.6390537223710175]
	TIME [epoch: 8.01 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6282368012832317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6282368012832317 | validation: 0.6353867879787924]
	TIME [epoch: 8 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6468232033995195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6468232033995195 | validation: 0.5715340310711567]
	TIME [epoch: 8.02 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5566352030539113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5566352030539113 | validation: 0.5511270985189501]
	TIME [epoch: 8.02 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5555586511999931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5555586511999931 | validation: 0.5807719569903149]
	TIME [epoch: 8.01 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5318253349311047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5318253349311047 | validation: 0.564165790082772]
	TIME [epoch: 8.02 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5200405132223593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5200405132223593 | validation: 0.5500944671188932]
	TIME [epoch: 8 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5166463362267492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5166463362267492 | validation: 0.5836265929607226]
	TIME [epoch: 8 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5112205975256394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5112205975256394 | validation: 0.5543036301482603]
	TIME [epoch: 7.98 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4946889310465104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4946889310465104 | validation: 0.5625205607446926]
	TIME [epoch: 8.02 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4725118679261223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4725118679261223 | validation: 0.5556961212879927]
	TIME [epoch: 8.02 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47285876897692874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47285876897692874 | validation: 0.638347524085411]
	TIME [epoch: 8.01 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5782506856549408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5782506856549408 | validation: 0.6514727980873825]
	TIME [epoch: 8 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6058414028904141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6058414028904141 | validation: 0.6934381427847871]
	TIME [epoch: 7.99 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7852681415830597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7852681415830597 | validation: 0.4844208173642839]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_450.pth
	Model improved!!!
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4930091803590244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4930091803590244 | validation: 0.5111013090635188]
	TIME [epoch: 7.97 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5049398239680588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5049398239680588 | validation: 0.5343211551707194]
	TIME [epoch: 7.97 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4833651607910887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4833651607910887 | validation: 0.5063764364511827]
	TIME [epoch: 7.98 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43379439177157186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43379439177157186 | validation: 0.5444891945269404]
	TIME [epoch: 7.95 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.506093734502549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.506093734502549 | validation: 0.6979539305170344]
	TIME [epoch: 7.95 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6371744134012108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6371744134012108 | validation: 0.7147572861783535]
	TIME [epoch: 7.96 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9215080251466333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9215080251466333 | validation: 0.467696198186548]
	TIME [epoch: 7.96 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_457.pth
	Model improved!!!
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5214356808975991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5214356808975991 | validation: 0.6000351323465307]
	TIME [epoch: 7.98 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6034676027801349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6034676027801349 | validation: 0.5110205720550798]
	TIME [epoch: 7.95 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45618691723135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45618691723135 | validation: 0.44406523169606604]
	TIME [epoch: 7.95 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_460.pth
	Model improved!!!
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4059849867441858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4059849867441858 | validation: 0.5489330745517611]
	TIME [epoch: 7.94 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4615458797076237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4615458797076237 | validation: 0.6402202064440794]
	TIME [epoch: 7.94 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6720174852474164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6720174852474164 | validation: 0.439824392012104]
	TIME [epoch: 7.99 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_463.pth
	Model improved!!!
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40334221481813765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40334221481813765 | validation: 0.5280669557280427]
	TIME [epoch: 7.97 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47747011659262306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47747011659262306 | validation: 0.5480291306261608]
	TIME [epoch: 7.98 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5186698586422741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5186698586422741 | validation: 0.47082229914365564]
	TIME [epoch: 7.97 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37375087685654357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37375087685654357 | validation: 0.4080853040524409]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_467.pth
	Model improved!!!
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3641561817239179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3641561817239179 | validation: 0.637026478226964]
	TIME [epoch: 7.97 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5037638219504134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5037638219504134 | validation: 0.7111530552129527]
	TIME [epoch: 7.98 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9759776321745794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9759776321745794 | validation: 0.934075154609937]
	TIME [epoch: 7.97 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1369190871333448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1369190871333448 | validation: 1.0428347785726897]
	TIME [epoch: 7.98 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1480861818626071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1480861818626071 | validation: 0.5959602862942333]
	TIME [epoch: 7.96 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6185086749014679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6185086749014679 | validation: 0.4803415939632317]
	TIME [epoch: 7.98 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44258764530822975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44258764530822975 | validation: 0.5023030623347502]
	TIME [epoch: 7.98 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4132810839813108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4132810839813108 | validation: 0.4870023412803308]
	TIME [epoch: 8 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44124619534540716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44124619534540716 | validation: 0.4707831228780931]
	TIME [epoch: 7.95 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3787644681329744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3787644681329744 | validation: 0.42159811238845324]
	TIME [epoch: 7.98 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37571728518555175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37571728518555175 | validation: 0.4583308928842006]
	TIME [epoch: 7.99 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3753863510448369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3753863510448369 | validation: 0.4171811639062508]
	TIME [epoch: 7.97 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.391233134920664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.391233134920664 | validation: 0.49095177788001054]
	TIME [epoch: 8.02 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3873124062387271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3873124062387271 | validation: 0.43850406264730346]
	TIME [epoch: 7.99 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5048555934564827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5048555934564827 | validation: 0.4161662922124199]
	TIME [epoch: 7.96 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33739803961052434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33739803961052434 | validation: 0.3972639512176696]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_483.pth
	Model improved!!!
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.291231265490062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.291231265490062 | validation: 0.40439793991771805]
	TIME [epoch: 8 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2988261278914602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2988261278914602 | validation: 0.48546351240235197]
	TIME [epoch: 7.98 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3468702681941707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3468702681941707 | validation: 0.5248704952649448]
	TIME [epoch: 7.99 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6898141107620228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6898141107620228 | validation: 0.46664887474848166]
	TIME [epoch: 7.97 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40252163958057324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40252163958057324 | validation: 0.40070092139813457]
	TIME [epoch: 8.01 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3094590388290698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3094590388290698 | validation: 0.43559844689832006]
	TIME [epoch: 7.97 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30385495441121246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30385495441121246 | validation: 0.644137080079378]
	TIME [epoch: 7.99 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5483918783016709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5483918783016709 | validation: 0.9039328273371363]
	TIME [epoch: 7.98 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2212215987453798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2212215987453798 | validation: 0.8752106495281465]
	TIME [epoch: 8 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.279721362225158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.279721362225158 | validation: 0.7125250876770463]
	TIME [epoch: 7.98 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8243011303314282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8243011303314282 | validation: 0.688629424436806]
	TIME [epoch: 7.96 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7398054096157576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7398054096157576 | validation: 0.43095040136458007]
	TIME [epoch: 8 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46169629949334334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46169629949334334 | validation: 0.4686921845393435]
	TIME [epoch: 7.94 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43325667289357694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43325667289357694 | validation: 0.4224248575062534]
	TIME [epoch: 7.97 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37063403398213124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37063403398213124 | validation: 0.367620909317951]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_498.pth
	Model improved!!!
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3028526105335174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3028526105335174 | validation: 0.48268066946319066]
	TIME [epoch: 7.99 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34376724300457845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34376724300457845 | validation: 0.5404234837325371]
	TIME [epoch: 7.96 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5839375702186701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5839375702186701 | validation: 0.4552366166427536]
	TIME [epoch: 39.4 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3418542491968929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3418542491968929 | validation: 0.37116573871258884]
	TIME [epoch: 17.1 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3002532423426315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3002532423426315 | validation: 0.4584854962936426]
	TIME [epoch: 17 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3247107940391007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3247107940391007 | validation: 0.4805502493511342]
	TIME [epoch: 17 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5051172988243413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5051172988243413 | validation: 0.46568933302512616]
	TIME [epoch: 17 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34959724558119676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34959724558119676 | validation: 0.33520396352070625]
	TIME [epoch: 17 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_506.pth
	Model improved!!!
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29209732222270807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29209732222270807 | validation: 0.45949353935012455]
	TIME [epoch: 17 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33551157783261915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33551157783261915 | validation: 0.5028907741603325]
	TIME [epoch: 17 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6003678783675703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6003678783675703 | validation: 0.36595792160259566]
	TIME [epoch: 17 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31838269391615415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31838269391615415 | validation: 0.39377362841979385]
	TIME [epoch: 17 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28843400254290585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28843400254290585 | validation: 0.41134195350440134]
	TIME [epoch: 17 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35747550198053313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35747550198053313 | validation: 0.7532393262982572]
	TIME [epoch: 17 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.688221312891273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.688221312891273 | validation: 1.0077727263230531]
	TIME [epoch: 17 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.244455893764913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.244455893764913 | validation: 0.7673160612657862]
	TIME [epoch: 17 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0196921457120798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0196921457120798 | validation: 0.826681674637503]
	TIME [epoch: 17 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8473289695122991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8473289695122991 | validation: 0.5340766679787564]
	TIME [epoch: 17 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5251105160321221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5251105160321221 | validation: 0.46528606538025824]
	TIME [epoch: 17.1 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4878942414490463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4878942414490463 | validation: 0.3951984760875279]
	TIME [epoch: 17 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29810798990176923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29810798990176923 | validation: 0.49209995701600173]
	TIME [epoch: 17 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3812902451188346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3812902451188346 | validation: 0.484471289371435]
	TIME [epoch: 17.1 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5269286942247295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5269286942247295 | validation: 0.3651533066561122]
	TIME [epoch: 17.1 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2841729798385183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2841729798385183 | validation: 0.42572714284951857]
	TIME [epoch: 17 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31653831988504133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31653831988504133 | validation: 0.44954553574137546]
	TIME [epoch: 17.1 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42910548498833145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42910548498833145 | validation: 0.36783758565490016]
	TIME [epoch: 17 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25965270984303324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25965270984303324 | validation: 0.3485907400948824]
	TIME [epoch: 17.1 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2315624685097481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2315624685097481 | validation: 0.3227758557430794]
	TIME [epoch: 17 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_526.pth
	Model improved!!!
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23174145485844555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23174145485844555 | validation: 0.3619537940186226]
	TIME [epoch: 17.1 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25067262182226996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25067262182226996 | validation: 0.4358797811823649]
	TIME [epoch: 17.1 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4732702979766459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4732702979766459 | validation: 0.4601622718698812]
	TIME [epoch: 17.1 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38954908913850167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38954908913850167 | validation: 0.344290087475602]
	TIME [epoch: 17.1 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3575768405942374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3575768405942374 | validation: 0.315690854251997]
	TIME [epoch: 17 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_531.pth
	Model improved!!!
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2360719928245209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2360719928245209 | validation: 0.32076157479686157]
	TIME [epoch: 17 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20956424764741058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20956424764741058 | validation: 0.3302984646641002]
	TIME [epoch: 17.2 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20145763489761004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20145763489761004 | validation: 0.2936037799801386]
	TIME [epoch: 17 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_534.pth
	Model improved!!!
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2093054031447805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2093054031447805 | validation: 0.49538859279788805]
	TIME [epoch: 17.1 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.413117753029799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.413117753029799 | validation: 1.2605573376257404]
	TIME [epoch: 17.1 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.496848203253286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.496848203253286 | validation: 3.5968455414258744]
	TIME [epoch: 17.1 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5726414572229697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5726414572229697 | validation: 4.15222730142718]
	TIME [epoch: 17 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8408804223494015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8408804223494015 | validation: 3.9257342623838016]
	TIME [epoch: 17 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6081001662440673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6081001662440673 | validation: 3.6770685067145368]
	TIME [epoch: 17 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.387191030375126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.387191030375126 | validation: 3.511916538096526]
	TIME [epoch: 17 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2539386434193616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2539386434193616 | validation: 3.4917111558071285]
	TIME [epoch: 17 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2238075469092373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2238075469092373 | validation: 3.492470122314279]
	TIME [epoch: 17 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.200924243228818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.200924243228818 | validation: 3.305115894171635]
	TIME [epoch: 17 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1046155589004587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1046155589004587 | validation: 3.1379913492035008]
	TIME [epoch: 17 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0178052295714077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0178052295714077 | validation: 2.9008219941027793]
	TIME [epoch: 17 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9323808272999146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9323808272999146 | validation: 3.883081682601416]
	TIME [epoch: 17 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6159631099514855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6159631099514855 | validation: 3.8663162319770326]
	TIME [epoch: 17 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.599421868430094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.599421868430094 | validation: 3.8505510785562347]
	TIME [epoch: 17 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5876770652153214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5876770652153214 | validation: 3.8371044856110106]
	TIME [epoch: 17 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.580137593170652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.580137593170652 | validation: 3.8293219165733805]
	TIME [epoch: 17 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5614537338139605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5614537338139605 | validation: 3.8053105624288506]
	TIME [epoch: 17 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.549040066061887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.549040066061887 | validation: 3.792581037153978]
	TIME [epoch: 17 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5274351064304574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5274351064304574 | validation: 3.759951365684797]
	TIME [epoch: 17 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5068963653941227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5068963653941227 | validation: 3.6882728815868604]
	TIME [epoch: 17.1 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.45137794856691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.45137794856691 | validation: 3.0871111405332816]
	TIME [epoch: 17 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1033291852622695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1033291852622695 | validation: 3.003363809801538]
	TIME [epoch: 17 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.994581975599639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.994581975599639 | validation: 2.475950005842753]
	TIME [epoch: 17 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.615406142444724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.615406142444724 | validation: 2.0603652806193504]
	TIME [epoch: 17 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2123722503226624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2123722503226624 | validation: 1.5656206204722833]
	TIME [epoch: 17 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6587103185131034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6587103185131034 | validation: 0.7206862532888103]
	TIME [epoch: 17 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8318178427279116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8318178427279116 | validation: 1.1865725819738338]
	TIME [epoch: 17 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4593474140921157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4593474140921157 | validation: 0.7333102790578684]
	TIME [epoch: 17 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8609293735002601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8609293735002601 | validation: 0.96772378897039]
	TIME [epoch: 17 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0263562226509084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0263562226509084 | validation: 0.763863571292646]
	TIME [epoch: 17 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7664026350862088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7664026350862088 | validation: 0.6893766202975842]
	TIME [epoch: 17 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7887996960800088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7887996960800088 | validation: 0.565702975210503]
	TIME [epoch: 17 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5909615320217703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5909615320217703 | validation: 0.5809800134378744]
	TIME [epoch: 17 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6124420900321793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6124420900321793 | validation: 0.5191437748464294]
	TIME [epoch: 17 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5177951597036929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5177951597036929 | validation: 0.51613637117117]
	TIME [epoch: 17 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49371226759443576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49371226759443576 | validation: 0.4978753355936588]
	TIME [epoch: 17 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45275564423225545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45275564423225545 | validation: 0.5043503245639223]
	TIME [epoch: 17 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4293741103348762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4293741103348762 | validation: 0.5585476491437845]
	TIME [epoch: 17 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5506093539066104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5506093539066104 | validation: 1.0963238263496518]
	TIME [epoch: 17.1 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2748295323171634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2748295323171634 | validation: 0.6166967503127847]
	TIME [epoch: 17 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6687435158352074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6687435158352074 | validation: 0.7822082706080223]
	TIME [epoch: 17 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8047565544624634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8047565544624634 | validation: 0.6134817294429147]
	TIME [epoch: 17 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6092980876871181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6092980876871181 | validation: 0.6115905730814641]
	TIME [epoch: 17 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5870261354180524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5870261354180524 | validation: 0.501579419632611]
	TIME [epoch: 17 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4893436685642988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4893436685642988 | validation: 0.47220556849419265]
	TIME [epoch: 17 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4592149559408007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4592149559408007 | validation: 0.4681535897536481]
	TIME [epoch: 17 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4212596193742124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4212596193742124 | validation: 0.5391131306523514]
	TIME [epoch: 17.1 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4563428174868217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4563428174868217 | validation: 0.7100545212962367]
	TIME [epoch: 17 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6651139396688287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6651139396688287 | validation: 0.4792950311744171]
	TIME [epoch: 17 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43131306429535016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43131306429535016 | validation: 0.4845414377713786]
	TIME [epoch: 17 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4192711897637079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4192711897637079 | validation: 0.5182458440208587]
	TIME [epoch: 17 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43447488252917305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43447488252917305 | validation: 0.6716470742220214]
	TIME [epoch: 17 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6212606350397178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6212606350397178 | validation: 0.8492887191340013]
	TIME [epoch: 17.1 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.082535353751518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.082535353751518 | validation: 0.630824782658344]
	TIME [epoch: 17 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9197830098157849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9197830098157849 | validation: 0.7294192911076233]
	TIME [epoch: 17 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7514586015341805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7514586015341805 | validation: 0.5909637526019118]
	TIME [epoch: 17 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6045340269719262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6045340269719262 | validation: 0.5741329342249716]
	TIME [epoch: 17 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6284001578978994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6284001578978994 | validation: 0.435541211977323]
	TIME [epoch: 17.1 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47625042587116556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47625042587116556 | validation: 0.4469355511650781]
	TIME [epoch: 17 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4258761000555961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4258761000555961 | validation: 0.4633788041781749]
	TIME [epoch: 17.1 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4105539810494165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4105539810494165 | validation: 0.49805593305228363]
	TIME [epoch: 17.1 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.419731050306916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.419731050306916 | validation: 0.5237221808785814]
	TIME [epoch: 17 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46211527927471024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46211527927471024 | validation: 0.5358662284059804]
	TIME [epoch: 17.1 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4494757931454194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4494757931454194 | validation: 0.5526347862678799]
	TIME [epoch: 17.1 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5382987564355415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5382987564355415 | validation: 0.45214916378531367]
	TIME [epoch: 17 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41805746887581685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41805746887581685 | validation: 0.42657737599491485]
	TIME [epoch: 17.1 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3903768326522038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3903768326522038 | validation: 0.42048628356991746]
	TIME [epoch: 17 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3590897350320014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3590897350320014 | validation: 0.46588555955656696]
	TIME [epoch: 17 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3770609559051838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3770609559051838 | validation: 0.5531790135671435]
	TIME [epoch: 17 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5644160182508484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5644160182508484 | validation: 0.5062205312414711]
	TIME [epoch: 17 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4392386871483933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4392386871483933 | validation: 0.38019271011856043]
	TIME [epoch: 17 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30767793162452295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30767793162452295 | validation: 0.39133293249173673]
	TIME [epoch: 17 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2838887905873819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2838887905873819 | validation: 0.5948344255200622]
	TIME [epoch: 17 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48708966377943064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48708966377943064 | validation: 0.9430451835226563]
	TIME [epoch: 17 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1717672887473507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1717672887473507 | validation: 0.819700803985797]
	TIME [epoch: 17 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8036541606873602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8036541606873602 | validation: 1.3388749782785556]
	TIME [epoch: 17 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5410207176335171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5410207176335171 | validation: 0.46067298794402894]
	TIME [epoch: 17 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42365144032798085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42365144032798085 | validation: 0.5550321457249531]
	TIME [epoch: 17 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6762678597775817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6762678597775817 | validation: 0.41705070557203916]
	TIME [epoch: 17 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3917809226234084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3917809226234084 | validation: 0.49449569199086096]
	TIME [epoch: 17.1 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4485144271046507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4485144271046507 | validation: 0.39561278418059337]
	TIME [epoch: 17 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3227942909012589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3227942909012589 | validation: 0.4038070307676389]
	TIME [epoch: 17 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32458406822994734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32458406822994734 | validation: 0.436327012859773]
	TIME [epoch: 17.1 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3342919876952496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3342919876952496 | validation: 0.41176965950870453]
	TIME [epoch: 17 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3369791375607528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3369791375607528 | validation: 0.3959862048859861]
	TIME [epoch: 17 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29271141012778634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29271141012778634 | validation: 0.3642173863297056]
	TIME [epoch: 17 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28348382570547376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28348382570547376 | validation: 0.3664177290129657]
	TIME [epoch: 17 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2796697552011835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2796697552011835 | validation: 0.3682281626380362]
	TIME [epoch: 17 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.286958204433658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.286958204433658 | validation: 0.37479709483986506]
	TIME [epoch: 17 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2906565450171502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2906565450171502 | validation: 0.39674325031532226]
	TIME [epoch: 17 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3700283264574239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3700283264574239 | validation: 0.3944091925946558]
	TIME [epoch: 17.1 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3017123383543936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3017123383543936 | validation: 0.32285610428872236]
	TIME [epoch: 17 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28153768807024343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28153768807024343 | validation: 0.445393415746986]
	TIME [epoch: 17.1 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3629292103400087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3629292103400087 | validation: 0.5490149662716892]
	TIME [epoch: 17 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6468401499457709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6468401499457709 | validation: 0.49983950895335955]
	TIME [epoch: 17 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6167416300139367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6167416300139367 | validation: 1.2690080974568878]
	TIME [epoch: 17.1 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4281195116488743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4281195116488743 | validation: 0.40333364686082984]
	TIME [epoch: 17.1 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4039833730977131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4039833730977131 | validation: 0.5829755344246341]
	TIME [epoch: 17 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.817066573002756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.817066573002756 | validation: 0.40533076195852846]
	TIME [epoch: 17 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5002159941130673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5002159941130673 | validation: 0.6422481157406247]
	TIME [epoch: 17.1 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142622/states/model_phi1_3c_v_mmd1_635.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 5569.893 seconds.
