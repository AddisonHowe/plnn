Args:
Namespace(name='model_phi1_3b_v_mmd1', outdir='out/model_training/model_phi1_3b_v_mmd1', training_data='data/training_data/data_phi1_3b/training', validation_data='data/training_data/data_phi1_3b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3723538355

Training model...

Saving initial model state to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.697266238502855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.697266238502855 | validation: 4.249792742069391]
	TIME [epoch: 31 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6064313708376585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6064313708376585 | validation: 4.444345212496203]
	TIME [epoch: 1.79 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.217526131471366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.217526131471366 | validation: 2.825739792974783]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9418028365177293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9418028365177293 | validation: 3.8311176557743707]
	TIME [epoch: 1.78 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6313015920488714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6313015920488714 | validation: 3.011890599872535]
	TIME [epoch: 1.77 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7474315291968554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7474315291968554 | validation: 2.5775709537764544]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5511932586053816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5511932586053816 | validation: 2.3230795139270017]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2213316058734147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2213316058734147 | validation: 2.3330835831937664]
	TIME [epoch: 1.77 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.153876036221557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.153876036221557 | validation: 2.0297555593226155]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9225667616082032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9225667616082032 | validation: 1.8923691467798134]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.803975074117174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.803975074117174 | validation: 1.8355518814033787]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.684726773330993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.684726773330993 | validation: 1.7914738706454842]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.621635703579111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.621635703579111 | validation: 1.8153567709767229]
	TIME [epoch: 1.77 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6761508317468152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6761508317468152 | validation: 1.756055690691244]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6208839571902312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6208839571902312 | validation: 1.5870213713354322]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.45352570822249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.45352570822249 | validation: 1.4970082837612257]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3405587332111955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3405587332111955 | validation: 1.4989560545303102]
	TIME [epoch: 1.77 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3388519275121842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3388519275121842 | validation: 1.483644191711203]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3346502792430204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3346502792430204 | validation: 1.405160147236943]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2237558974700342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2237558974700342 | validation: 1.374620341986517]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1872108700675437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1872108700675437 | validation: 1.3350589313852812]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1755800087957409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1755800087957409 | validation: 1.3182619641718825]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.127705901447415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.127705901447415 | validation: 1.312181619769672]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1408579066199227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1408579066199227 | validation: 1.519859757209425]
	TIME [epoch: 1.77 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2731701321177185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2731701321177185 | validation: 1.4145486980031583]
	TIME [epoch: 1.76 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2739904242729294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2739904242729294 | validation: 1.2892848394853484]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0839317405939923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0839317405939923 | validation: 1.1489253175064342]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9942595424398256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9942595424398256 | validation: 1.151024205138899]
	TIME [epoch: 1.77 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9929653927685607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9929653927685607 | validation: 1.2155777721968222]
	TIME [epoch: 1.76 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0047604051834704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0047604051834704 | validation: 1.1326278130114955]
	TIME [epoch: 1.76 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9781283398569849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9781283398569849 | validation: 1.1607306971007822]
	TIME [epoch: 1.76 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.98076382547343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.98076382547343 | validation: 1.1152473815837767]
	TIME [epoch: 1.76 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9782756123118301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9782756123118301 | validation: 1.2079652069021227]
	TIME [epoch: 1.76 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0044424445261217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0044424445261217 | validation: 1.1514906693482287]
	TIME [epoch: 1.76 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0140945775392132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0140945775392132 | validation: 1.1519043272802172]
	TIME [epoch: 1.77 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0052913053675439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0052913053675439 | validation: 1.0540206951892583]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9003273487979934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9003273487979934 | validation: 0.9908211265044845]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.848538270888728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.848538270888728 | validation: 0.9921644782778399]
	TIME [epoch: 1.77 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8399230448295346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8399230448295346 | validation: 0.9855737321907851]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8583852952819995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8583852952819995 | validation: 1.0948744031452862]
	TIME [epoch: 1.77 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9204724655257589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9204724655257589 | validation: 1.0076075169475374]
	TIME [epoch: 1.77 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8952231942682652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8952231942682652 | validation: 1.0232529562898947]
	TIME [epoch: 1.76 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8674490677196508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8674490677196508 | validation: 0.9139698551625548]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8066973660149839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8066973660149839 | validation: 0.9579453804071261]
	TIME [epoch: 1.79 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8221529304934293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8221529304934293 | validation: 0.9880181230837558]
	TIME [epoch: 1.77 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8445574142964491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8445574142964491 | validation: 0.9855993179557292]
	TIME [epoch: 1.77 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8597534924870351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8597534924870351 | validation: 0.9027940453506926]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8084209976164577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8084209976164577 | validation: 0.9072156712930035]
	TIME [epoch: 1.77 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7982078982869234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7982078982869234 | validation: 0.8963367283858539]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8059333040272617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8059333040272617 | validation: 0.9445142286154011]
	TIME [epoch: 1.77 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8286550892554402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8286550892554402 | validation: 0.8762901101101743]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.805227431013939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.805227431013939 | validation: 0.9324943150142269]
	TIME [epoch: 1.76 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8257871930653755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8257871930653755 | validation: 0.9071535639282233]
	TIME [epoch: 1.77 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8062677888206884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8062677888206884 | validation: 0.8970045953791213]
	TIME [epoch: 1.76 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7844998054984842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7844998054984842 | validation: 0.8236294673830522]
	TIME [epoch: 1.76 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7389401603444051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7389401603444051 | validation: 0.8073875494692386]
	TIME [epoch: 1.76 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7399142486041628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7399142486041628 | validation: 0.800236689418208]
	TIME [epoch: 1.76 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7351985047093058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7351985047093058 | validation: 0.8126868445177217]
	TIME [epoch: 1.77 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7289629177954103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7289629177954103 | validation: 0.7874781729301437]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7266825348116639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7266825348116639 | validation: 0.793525209492338]
	TIME [epoch: 1.77 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.725483037256954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.725483037256954 | validation: 0.7751780335637382]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7297044563463945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7297044563463945 | validation: 0.8554623196781178]
	TIME [epoch: 1.77 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7617060446977365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7617060446977365 | validation: 1.0348965311730933]
	TIME [epoch: 1.78 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9509720549670014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9509720549670014 | validation: 1.1100080973985382]
	TIME [epoch: 1.77 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0169648521000796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0169648521000796 | validation: 0.8098784216679171]
	TIME [epoch: 1.77 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7449415987208611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7449415987208611 | validation: 0.885845532105173]
	TIME [epoch: 1.77 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8134867531384568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8134867531384568 | validation: 0.8767412216234857]
	TIME [epoch: 1.77 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.801124976975946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.801124976975946 | validation: 0.7754557258361949]
	TIME [epoch: 1.77 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7186911313086912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7186911313086912 | validation: 0.7708966304469427]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7162722165332152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7162722165332152 | validation: 0.7983060520331219]
	TIME [epoch: 1.77 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7232212939220769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7232212939220769 | validation: 0.7912224409713167]
	TIME [epoch: 1.77 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7752428322678685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7752428322678685 | validation: 1.056062390205448]
	TIME [epoch: 1.77 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9118130326470822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9118130326470822 | validation: 0.8080196875518273]
	TIME [epoch: 1.77 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7466507495607005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7466507495607005 | validation: 0.7777211719577076]
	TIME [epoch: 1.77 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7151619163461678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7151619163461678 | validation: 0.7796183357237549]
	TIME [epoch: 1.77 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7265363995035438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7265363995035438 | validation: 0.7533693214268874]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7228519158454298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7228519158454298 | validation: 0.7894330223830732]
	TIME [epoch: 1.77 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7260031771368114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7260031771368114 | validation: 0.7588140987505758]
	TIME [epoch: 1.77 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7311609746961866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7311609746961866 | validation: 0.791443526641959]
	TIME [epoch: 1.78 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7204232741837597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7204232741837597 | validation: 0.7651920962492137]
	TIME [epoch: 1.77 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.717165133966421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.717165133966421 | validation: 0.7784696747849289]
	TIME [epoch: 1.77 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7158342127867937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7158342127867937 | validation: 0.7902506118570627]
	TIME [epoch: 1.77 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7476453487457778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7476453487457778 | validation: 0.8817320064572682]
	TIME [epoch: 1.77 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.831210729108709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.831210729108709 | validation: 0.8541979513012665]
	TIME [epoch: 1.77 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8303388228513694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8303388228513694 | validation: 0.861136680352443]
	TIME [epoch: 1.77 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7883502524354341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7883502524354341 | validation: 0.7455068767292403]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.711011831927529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.711011831927529 | validation: 0.7556645082327232]
	TIME [epoch: 1.77 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7227004710771536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7227004710771536 | validation: 0.8014704223767115]
	TIME [epoch: 1.77 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7455874454059943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7455874454059943 | validation: 0.7460054560387218]
	TIME [epoch: 1.77 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7347974399089285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7347974399089285 | validation: 0.777541509567938]
	TIME [epoch: 1.77 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7159191874272525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7159191874272525 | validation: 0.7526819911718783]
	TIME [epoch: 1.77 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7221575138447162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7221575138447162 | validation: 0.7675792879220236]
	TIME [epoch: 1.77 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7175026219948504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7175026219948504 | validation: 0.7707256160837664]
	TIME [epoch: 1.77 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7259915731501624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7259915731501624 | validation: 0.8325968287754091]
	TIME [epoch: 1.76 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7643728621886499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7643728621886499 | validation: 0.8678541167543232]
	TIME [epoch: 1.77 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.827561151862879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.827561151862879 | validation: 0.82177254834791]
	TIME [epoch: 1.77 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7881613577332676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7881613577332676 | validation: 0.832306201476348]
	TIME [epoch: 1.77 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7678423475896594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7678423475896594 | validation: 0.7483680731355741]
	TIME [epoch: 1.78 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.751595286424088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.751595286424088 | validation: 0.7997018639905481]
	TIME [epoch: 1.78 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.732517307900765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.732517307900765 | validation: 0.7385685881934982]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7152365402129333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7152365402129333 | validation: 0.7465947964420367]
	TIME [epoch: 1.78 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7151483765088393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7151483765088393 | validation: 0.7367397099169415]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7006828498350239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7006828498350239 | validation: 0.7401545864760446]
	TIME [epoch: 1.77 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.711120035887908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.711120035887908 | validation: 0.7565018689702575]
	TIME [epoch: 1.77 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7215038664598569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7215038664598569 | validation: 0.8159069616117738]
	TIME [epoch: 1.77 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7516810068705941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7516810068705941 | validation: 0.8531274706887126]
	TIME [epoch: 1.77 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8483118297294329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8483118297294329 | validation: 0.9448905408367252]
	TIME [epoch: 1.77 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8773351066576075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8773351066576075 | validation: 0.7369052754028079]
	TIME [epoch: 1.77 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7094678693418183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7094678693418183 | validation: 0.7610728880097849]
	TIME [epoch: 1.78 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7528866534361524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7528866534361524 | validation: 0.8474902689271041]
	TIME [epoch: 1.77 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7811798220966619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7811798220966619 | validation: 0.801566280600893]
	TIME [epoch: 1.77 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7336777726676215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7336777726676215 | validation: 0.7390644367813668]
	TIME [epoch: 1.77 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7232740543420074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7232740543420074 | validation: 0.765488973294052]
	TIME [epoch: 1.77 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7305035849938808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7305035849938808 | validation: 0.7517454533315024]
	TIME [epoch: 1.76 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7092152245995588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7092152245995588 | validation: 0.7448817180078504]
	TIME [epoch: 1.77 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7080199828370395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7080199828370395 | validation: 0.7834192643578476]
	TIME [epoch: 1.77 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.731357129462368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.731357129462368 | validation: 0.7467632190605976]
	TIME [epoch: 1.79 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7298052908429048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7298052908429048 | validation: 0.7709810170735107]
	TIME [epoch: 1.77 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7183427568897417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7183427568897417 | validation: 0.7420524524005625]
	TIME [epoch: 1.77 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7149045541210967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7149045541210967 | validation: 0.7589328677977396]
	TIME [epoch: 1.77 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7233778039424599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7233778039424599 | validation: 0.7852659274625979]
	TIME [epoch: 1.77 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.738329303093108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.738329303093108 | validation: 0.8387134094239145]
	TIME [epoch: 1.76 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8306536784282483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8306536784282483 | validation: 0.9401198378452702]
	TIME [epoch: 1.77 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8651273799434978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8651273799434978 | validation: 0.7406902750961862]
	TIME [epoch: 1.77 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7089890926892929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7089890926892929 | validation: 0.7738632835386762]
	TIME [epoch: 1.77 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.739030045346936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.739030045346936 | validation: 0.8244698059144769]
	TIME [epoch: 1.77 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7731267363156523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7731267363156523 | validation: 0.7499854351319943]
	TIME [epoch: 1.77 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7110714695611583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7110714695611583 | validation: 0.7712090231161199]
	TIME [epoch: 1.77 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7254918659943596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7254918659943596 | validation: 0.7821886484216987]
	TIME [epoch: 1.78 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7567613358295696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7567613358295696 | validation: 0.758108713274444]
	TIME [epoch: 1.76 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7196735054372355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7196735054372355 | validation: 0.7596813612472852]
	TIME [epoch: 1.77 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7021429413596456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7021429413596456 | validation: 0.7493678068552063]
	TIME [epoch: 1.76 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7073445002952926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7073445002952926 | validation: 0.7514703298335523]
	TIME [epoch: 1.77 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7003129300337048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7003129300337048 | validation: 0.7765674144910552]
	TIME [epoch: 1.77 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7160244234258916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7160244234258916 | validation: 0.7523529483244039]
	TIME [epoch: 1.78 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7170806857838656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7170806857838656 | validation: 0.7650694470462306]
	TIME [epoch: 1.77 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7119731809829621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7119731809829621 | validation: 0.739979675473551]
	TIME [epoch: 1.78 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.700443842815306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.700443842815306 | validation: 0.7261387516148847]
	TIME [epoch: 1.76 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6973513504460545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6973513504460545 | validation: 0.7321425296496029]
	TIME [epoch: 1.77 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7004172213400338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7004172213400338 | validation: 0.8287277393184502]
	TIME [epoch: 1.76 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7706024114344808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7706024114344808 | validation: 1.0326451109186428]
	TIME [epoch: 1.77 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0400104398103616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0400104398103616 | validation: 0.8841447944313203]
	TIME [epoch: 1.76 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.819701910080037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.819701910080037 | validation: 0.7977771373614102]
	TIME [epoch: 1.77 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7499385199968315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7499385199968315 | validation: 0.7745693529618917]
	TIME [epoch: 1.76 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7690604169284834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7690604169284834 | validation: 0.7480138667919045]
	TIME [epoch: 1.77 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7035762981026352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7035762981026352 | validation: 0.75081642648214]
	TIME [epoch: 1.76 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7043700542084821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7043700542084821 | validation: 0.7418133068178714]
	TIME [epoch: 1.77 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7088383462058606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7088383462058606 | validation: 0.7547288595529548]
	TIME [epoch: 1.76 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7043333794155417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7043333794155417 | validation: 0.7509057934698624]
	TIME [epoch: 1.76 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6969415375933729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6969415375933729 | validation: 0.7547929330057364]
	TIME [epoch: 1.76 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7047069261648281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7047069261648281 | validation: 0.7952165989938464]
	TIME [epoch: 1.76 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7187618638422267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7187618638422267 | validation: 0.7722557925902588]
	TIME [epoch: 1.76 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7444119280684264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7444119280684264 | validation: 0.769842108486507]
	TIME [epoch: 1.77 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7142082743044118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7142082743044118 | validation: 0.7436148515696094]
	TIME [epoch: 1.76 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7137355250116904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7137355250116904 | validation: 0.7984748562115802]
	TIME [epoch: 1.78 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7446880309476513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7446880309476513 | validation: 0.7851787331917468]
	TIME [epoch: 1.78 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7450734351191355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7450734351191355 | validation: 0.8275983478042535]
	TIME [epoch: 1.77 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7519520301765091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7519520301765091 | validation: 0.7592115270833099]
	TIME [epoch: 1.77 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7466376912895705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7466376912895705 | validation: 0.7652659725518159]
	TIME [epoch: 1.77 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7117056399341019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7117056399341019 | validation: 0.7332012541781654]
	TIME [epoch: 1.76 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7023849594001768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7023849594001768 | validation: 0.7218675459633875]
	TIME [epoch: 1.76 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_161.pth
	Model improved!!!
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.695843133678367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.695843133678367 | validation: 0.7279731781875363]
	TIME [epoch: 1.78 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6959087687770545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6959087687770545 | validation: 0.7199411453727171]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_163.pth
	Model improved!!!
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6909043242006243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6909043242006243 | validation: 0.7345805607225026]
	TIME [epoch: 1.77 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6931722071196121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6931722071196121 | validation: 0.7233808952364931]
	TIME [epoch: 1.77 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7170505276365472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7170505276365472 | validation: 0.8160054936764429]
	TIME [epoch: 1.77 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7386372314793459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7386372314793459 | validation: 0.7398659439993108]
	TIME [epoch: 1.77 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7171653144804726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7171653144804726 | validation: 0.7306690678925071]
	TIME [epoch: 1.77 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6976616438734093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6976616438734093 | validation: 0.8822599539510982]
	TIME [epoch: 1.77 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7971964414472743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7971964414472743 | validation: 0.958594804564927]
	TIME [epoch: 1.77 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9821015258332232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9821015258332232 | validation: 0.8070589190224466]
	TIME [epoch: 1.79 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7809067796955084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7809067796955084 | validation: 0.7895032965591201]
	TIME [epoch: 1.77 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.729652083200045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.729652083200045 | validation: 0.738794537930427]
	TIME [epoch: 1.77 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7362649228719745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7362649228719745 | validation: 0.7180482628729998]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.697761280032165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.697761280032165 | validation: 0.7295175734894508]
	TIME [epoch: 1.77 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6962291648352865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6962291648352865 | validation: 0.7495363025582906]
	TIME [epoch: 1.77 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7089634295116569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7089634295116569 | validation: 0.7263441708773349]
	TIME [epoch: 1.76 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6922999701764322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6922999701764322 | validation: 0.7287893807097979]
	TIME [epoch: 1.76 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7001982038407988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7001982038407988 | validation: 0.7098422599079316]
	TIME [epoch: 1.76 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6886300387734724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6886300387734724 | validation: 0.7349529311607009]
	TIME [epoch: 1.77 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6911102048070484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6911102048070484 | validation: 0.7189730673743139]
	TIME [epoch: 1.77 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6875197707378709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6875197707378709 | validation: 0.7358324073415157]
	TIME [epoch: 1.77 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.692539996723229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.692539996723229 | validation: 0.7260996180615358]
	TIME [epoch: 1.77 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6880060351560731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6880060351560731 | validation: 0.7328056451757249]
	TIME [epoch: 1.77 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6953770111726253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6953770111726253 | validation: 0.7732971810758447]
	TIME [epoch: 1.77 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7418849367657879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7418849367657879 | validation: 0.9638560553346611]
	TIME [epoch: 1.77 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8880085993931569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8880085993931569 | validation: 0.7910785222097197]
	TIME [epoch: 1.78 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7706535931201148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7706535931201148 | validation: 0.8073197422037444]
	TIME [epoch: 1.77 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7262824840641203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7262824840641203 | validation: 0.7552423899510384]
	TIME [epoch: 1.77 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.729391981625772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.729391981625772 | validation: 0.7272198337004664]
	TIME [epoch: 1.77 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7017280698560938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7017280698560938 | validation: 0.7481407569265421]
	TIME [epoch: 1.77 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6878401993578452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6878401993578452 | validation: 0.7183601091841925]
	TIME [epoch: 1.77 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6924014023349728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6924014023349728 | validation: 0.7384102883147925]
	TIME [epoch: 1.78 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6911541493921054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6911541493921054 | validation: 0.7107209649393679]
	TIME [epoch: 1.78 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6854066720826615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6854066720826615 | validation: 0.7271771234175596]
	TIME [epoch: 1.77 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6888037458142382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6888037458142382 | validation: 0.7214553210168925]
	TIME [epoch: 1.78 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6898211846087817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6898211846087817 | validation: 0.7518758823721705]
	TIME [epoch: 1.77 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6965188853929568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6965188853929568 | validation: 0.7202425339119226]
	TIME [epoch: 1.77 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7096205280216554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7096205280216554 | validation: 0.9078739441916215]
	TIME [epoch: 1.77 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8215348562547407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8215348562547407 | validation: 0.8300097959792752]
	TIME [epoch: 1.77 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.831330078183257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.831330078183257 | validation: 0.7488627668869875]
	TIME [epoch: 30.3 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7091747776097904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7091747776097904 | validation: 0.7328587909979701]
	TIME [epoch: 3.52 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6907307106252085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6907307106252085 | validation: 0.704191419008878]
	TIME [epoch: 3.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7003405859466739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7003405859466739 | validation: 0.7635498956754851]
	TIME [epoch: 3.5 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7083378218111268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7083378218111268 | validation: 0.7144887148983947]
	TIME [epoch: 3.52 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6836924085765828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6836924085765828 | validation: 0.7315197783123559]
	TIME [epoch: 3.51 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6981793642318447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6981793642318447 | validation: 0.7323457504043358]
	TIME [epoch: 3.5 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6971965757228458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6971965757228458 | validation: 0.7043284094972582]
	TIME [epoch: 3.51 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6860735777005327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6860735777005327 | validation: 0.7471974951140729]
	TIME [epoch: 3.5 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6796598845680147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6796598845680147 | validation: 0.7096342488743398]
	TIME [epoch: 3.51 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6897320333752137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6897320333752137 | validation: 0.7456441202876148]
	TIME [epoch: 3.51 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.690444700245031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.690444700245031 | validation: 0.7393279806842563]
	TIME [epoch: 3.52 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7218933686300512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7218933686300512 | validation: 0.9291035850875391]
	TIME [epoch: 3.51 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8312780042346072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8312780042346072 | validation: 0.8006995074310972]
	TIME [epoch: 3.5 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7760964515679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7760964515679 | validation: 0.7530658567068879]
	TIME [epoch: 3.5 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7225107994325748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7225107994325748 | validation: 0.7114011233943016]
	TIME [epoch: 3.51 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6819255186327537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6819255186327537 | validation: 0.7364508127199834]
	TIME [epoch: 3.5 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.708445454474983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.708445454474983 | validation: 0.7519613454828694]
	TIME [epoch: 3.49 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7250264937377218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7250264937377218 | validation: 0.707079924805427]
	TIME [epoch: 3.5 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6842964252986508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6842964252986508 | validation: 0.720335759186305]
	TIME [epoch: 3.49 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6752187137896585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6752187137896585 | validation: 0.7156630819048494]
	TIME [epoch: 3.49 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6816147182294232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6816147182294232 | validation: 0.7078637842715237]
	TIME [epoch: 3.5 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6812037625303989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6812037625303989 | validation: 0.7170888800018447]
	TIME [epoch: 3.51 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6880323847548641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6880323847548641 | validation: 0.718860925188467]
	TIME [epoch: 3.5 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7264541354450408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7264541354450408 | validation: 0.8404052664365391]
	TIME [epoch: 3.5 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7876743282648314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7876743282648314 | validation: 0.7633635202107927]
	TIME [epoch: 3.5 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7529603490982527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7529603490982527 | validation: 0.7183200661665226]
	TIME [epoch: 3.5 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6908670983155492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6908670983155492 | validation: 0.7014336499897187]
	TIME [epoch: 3.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_228.pth
	Model improved!!!
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6827744079786927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6827744079786927 | validation: 0.7027490389634576]
	TIME [epoch: 3.5 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6846503148156049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6846503148156049 | validation: 0.7189973752874445]
	TIME [epoch: 3.5 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6842498538211932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6842498538211932 | validation: 0.6943001578494234]
	TIME [epoch: 3.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_231.pth
	Model improved!!!
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6762298411818813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6762298411818813 | validation: 0.7289203526431389]
	TIME [epoch: 3.49 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6820578175365001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6820578175365001 | validation: 0.7008515933621638]
	TIME [epoch: 3.5 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7013086689346625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7013086689346625 | validation: 0.7570470867314207]
	TIME [epoch: 3.49 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6924613114151581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6924613114151581 | validation: 0.7272694760192527]
	TIME [epoch: 3.48 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7478197211437103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7478197211437103 | validation: 0.8433763374830644]
	TIME [epoch: 3.49 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8072047262555855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8072047262555855 | validation: 0.7258317954431344]
	TIME [epoch: 3.49 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7133078474798437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7133078474798437 | validation: 0.7052564695692838]
	TIME [epoch: 3.5 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6792663671025833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6792663671025833 | validation: 0.7097984993781641]
	TIME [epoch: 3.5 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6815720228073354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6815720228073354 | validation: 0.6924818695972281]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_240.pth
	Model improved!!!
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6765045150461367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6765045150461367 | validation: 0.6977777408707833]
	TIME [epoch: 3.49 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6571848887396813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6571848887396813 | validation: 0.6479747912772873]
	TIME [epoch: 3.49 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5821612520240966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5821612520240966 | validation: 3.0721531893966594]
	TIME [epoch: 3.51 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8422702601452317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8422702601452317 | validation: 1.055288810275896]
	TIME [epoch: 3.49 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.035522641398731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.035522641398731 | validation: 0.720104014432835]
	TIME [epoch: 3.5 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7348209275519312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7348209275519312 | validation: 0.7767325676655834]
	TIME [epoch: 3.5 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7467359883141999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7467359883141999 | validation: 0.6761824226121986]
	TIME [epoch: 3.49 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6943659414916215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6943659414916215 | validation: 0.6828228911993385]
	TIME [epoch: 3.49 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6934371050532951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6934371050532951 | validation: 0.6913475110213473]
	TIME [epoch: 3.5 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.689064602559291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.689064602559291 | validation: 0.6942205645161469]
	TIME [epoch: 3.49 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6749879973964196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6749879973964196 | validation: 0.6722673821653578]
	TIME [epoch: 3.5 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.670645864043425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.670645864043425 | validation: 0.6697619107541138]
	TIME [epoch: 3.5 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6628429720492567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6628429720492567 | validation: 0.662610812535272]
	TIME [epoch: 3.51 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6367605658426864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6367605658426864 | validation: 0.6345192967348224]
	TIME [epoch: 3.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_254.pth
	Model improved!!!
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6333929488306536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6333929488306536 | validation: 0.6922966770920024]
	TIME [epoch: 3.51 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6140107093284267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6140107093284267 | validation: 0.724779827158994]
	TIME [epoch: 3.5 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6411526531994832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6411526531994832 | validation: 0.7245533179736405]
	TIME [epoch: 3.52 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7546438764187684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7546438764187684 | validation: 0.8623546494945272]
	TIME [epoch: 3.5 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7964103202275681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7964103202275681 | validation: 0.7189549706817604]
	TIME [epoch: 3.5 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7254423889742154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7254423889742154 | validation: 0.6602922817190386]
	TIME [epoch: 3.5 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6861773612593622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6861773612593622 | validation: 0.6733594950368538]
	TIME [epoch: 3.5 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6686436358661055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6686436358661055 | validation: 0.6487929245073487]
	TIME [epoch: 3.5 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6654841056721782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6654841056721782 | validation: 0.6714140948576919]
	TIME [epoch: 3.5 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6532746889799512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6532746889799512 | validation: 0.6390792966480139]
	TIME [epoch: 3.5 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6323464308836756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6323464308836756 | validation: 0.7005500122281552]
	TIME [epoch: 3.5 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6140935022620877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6140935022620877 | validation: 0.6885794014160936]
	TIME [epoch: 3.49 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6912434384343916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6912434384343916 | validation: 3.369932182585103]
	TIME [epoch: 3.49 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1282349949778734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1282349949778734 | validation: 0.6117488087208194]
	TIME [epoch: 3.49 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_268.pth
	Model improved!!!
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6027536192275208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6027536192275208 | validation: 0.6495507800117721]
	TIME [epoch: 3.5 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.70744349530662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.70744349530662 | validation: 0.672579799281616]
	TIME [epoch: 3.49 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6677228723328166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6677228723328166 | validation: 0.654943445838059]
	TIME [epoch: 3.49 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6543701081170168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6543701081170168 | validation: 0.6225533686307261]
	TIME [epoch: 3.5 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6468982096645961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6468982096645961 | validation: 0.6352776518084093]
	TIME [epoch: 3.52 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6261143268374058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6261143268374058 | validation: 0.6287884233973684]
	TIME [epoch: 3.5 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.602793561164921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.602793561164921 | validation: 0.6012432248945182]
	TIME [epoch: 3.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_275.pth
	Model improved!!!
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5739115980824444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5739115980824444 | validation: 0.8345796981071985]
	TIME [epoch: 3.49 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6375141403331293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6375141403331293 | validation: 1.2634164791553952]
	TIME [epoch: 3.5 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1292753325811251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1292753325811251 | validation: 0.6327743806311974]
	TIME [epoch: 3.49 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6813355129685349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6813355129685349 | validation: 0.7487440524984545]
	TIME [epoch: 3.49 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7403405683208979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7403405683208979 | validation: 0.6361720126867945]
	TIME [epoch: 3.49 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6655096680737013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6655096680737013 | validation: 0.6406979695422435]
	TIME [epoch: 3.49 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6744197292565062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6744197292565062 | validation: 0.6388875881670802]
	TIME [epoch: 3.5 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.650536143801371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.650536143801371 | validation: 0.616029828489792]
	TIME [epoch: 3.51 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6242739526919749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6242739526919749 | validation: 0.6000786186730145]
	TIME [epoch: 3.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_284.pth
	Model improved!!!
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5993740321507901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5993740321507901 | validation: 0.5956384007270004]
	TIME [epoch: 3.49 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_285.pth
	Model improved!!!
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5699346428292698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5699346428292698 | validation: 0.5763450830318865]
	TIME [epoch: 3.49 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_286.pth
	Model improved!!!
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5420095857115801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5420095857115801 | validation: 1.4785412913695188]
	TIME [epoch: 3.48 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1248132831367188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1248132831367188 | validation: 1.492297715754398]
	TIME [epoch: 3.49 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3781726932520382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3781726932520382 | validation: 0.7875487356619011]
	TIME [epoch: 3.48 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8241376381524351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8241376381524351 | validation: 0.7843141115816956]
	TIME [epoch: 3.49 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7962030837958876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7962030837958876 | validation: 0.7720355010217357]
	TIME [epoch: 3.49 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7795395762681006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7795395762681006 | validation: 0.6532554847999548]
	TIME [epoch: 3.49 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7023638959240859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7023638959240859 | validation: 0.671894589745895]
	TIME [epoch: 3.49 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7098684339820885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7098684339820885 | validation: 0.6428362659287932]
	TIME [epoch: 3.49 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6892068947645166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6892068947645166 | validation: 0.6640755662580786]
	TIME [epoch: 3.49 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.690690565136263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.690690565136263 | validation: 0.661067148044653]
	TIME [epoch: 3.48 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6851057542656611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6851057542656611 | validation: 0.6671340079842794]
	TIME [epoch: 3.48 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6851516795632262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6851516795632262 | validation: 0.673320940637655]
	TIME [epoch: 3.48 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6854123597379215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6854123597379215 | validation: 0.6706227458713223]
	TIME [epoch: 3.48 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6776235772993983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6776235772993983 | validation: 0.6566143999827401]
	TIME [epoch: 3.49 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.671323576019394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.671323576019394 | validation: 0.671592602598418]
	TIME [epoch: 3.5 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6686979312435358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6686979312435358 | validation: 0.657482729406849]
	TIME [epoch: 3.5 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6717210419823675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6717210419823675 | validation: 0.6579203075940843]
	TIME [epoch: 3.51 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6622093707119904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6622093707119904 | validation: 0.6415711772719807]
	TIME [epoch: 3.51 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6614459373195206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6614459373195206 | validation: 0.6433343101064534]
	TIME [epoch: 3.5 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6559687776134999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6559687776134999 | validation: 0.6607263762646519]
	TIME [epoch: 3.5 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6478004884562126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6478004884562126 | validation: 0.6454222106436721]
	TIME [epoch: 3.49 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6509923101020368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6509923101020368 | validation: 0.645199085199915]
	TIME [epoch: 3.51 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6519357046749777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6519357046749777 | validation: 0.6535372786266382]
	TIME [epoch: 3.48 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6481361522104043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6481361522104043 | validation: 0.6650702958376397]
	TIME [epoch: 3.5 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.699038734373687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.699038734373687 | validation: 0.9399902453576712]
	TIME [epoch: 3.5 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8937538969641685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8937538969641685 | validation: 0.6748961753654554]
	TIME [epoch: 3.51 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6972394379409098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6972394379409098 | validation: 0.6476901153459975]
	TIME [epoch: 3.51 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6284020362009438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6284020362009438 | validation: 0.6339625582720723]
	TIME [epoch: 3.49 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6154553191995032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6154553191995032 | validation: 0.6153713543785622]
	TIME [epoch: 3.49 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6149397280065613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6149397280065613 | validation: 0.6994014690963593]
	TIME [epoch: 3.51 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6341651328612262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6341651328612262 | validation: 0.7280356289044667]
	TIME [epoch: 3.5 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7476852624119908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7476852624119908 | validation: 1.239176080944536]
	TIME [epoch: 3.5 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.04172715025267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.04172715025267 | validation: 0.6621453330073898]
	TIME [epoch: 3.49 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7327803072771856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7327803072771856 | validation: 0.6005042001111847]
	TIME [epoch: 3.49 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6468911226869253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6468911226869253 | validation: 0.665861229822063]
	TIME [epoch: 3.5 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6700466593225202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6700466593225202 | validation: 0.6379325647432224]
	TIME [epoch: 3.5 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6438936352724588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6438936352724588 | validation: 0.6199219922142195]
	TIME [epoch: 3.5 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6486680681145139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6486680681145139 | validation: 0.6220510880147101]
	TIME [epoch: 3.5 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6311626455828891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6311626455828891 | validation: 0.6217826308231598]
	TIME [epoch: 3.5 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6293588512731545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6293588512731545 | validation: 0.5963277437763245]
	TIME [epoch: 3.5 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.61131119968479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.61131119968479 | validation: 0.5893538318137884]
	TIME [epoch: 3.49 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.594449744072779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.594449744072779 | validation: 0.5889432466347595]
	TIME [epoch: 3.49 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5811454562433339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5811454562433339 | validation: 0.5683595574038458]
	TIME [epoch: 3.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_329.pth
	Model improved!!!
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5521676549661796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5521676549661796 | validation: 0.5598790071083819]
	TIME [epoch: 3.51 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_330.pth
	Model improved!!!
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5279153395901274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5279153395901274 | validation: 0.5605196976129023]
	TIME [epoch: 3.5 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5388801978279465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5388801978279465 | validation: 1.919656436563222]
	TIME [epoch: 3.51 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5896713028238916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5896713028238916 | validation: 1.0046162761039845]
	TIME [epoch: 3.51 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0221204085565498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0221204085565498 | validation: 0.5500929768026117]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_334.pth
	Model improved!!!
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6346077312873992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6346077312873992 | validation: 0.7088147475173834]
	TIME [epoch: 3.51 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7281826939974556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7281826939974556 | validation: 0.5895645606682499]
	TIME [epoch: 3.5 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6351959156500225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6351959156500225 | validation: 0.5896028049514439]
	TIME [epoch: 3.5 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6544610976122703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6544610976122703 | validation: 0.5792531417464998]
	TIME [epoch: 3.51 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6248469046490938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6248469046490938 | validation: 0.6064842094922076]
	TIME [epoch: 3.49 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6120239281406558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6120239281406558 | validation: 0.5764222771915481]
	TIME [epoch: 3.51 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6015083967978317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6015083967978317 | validation: 0.5842331097159028]
	TIME [epoch: 3.5 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5881306086520013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5881306086520013 | validation: 0.5658848819975791]
	TIME [epoch: 3.88 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5788356340166239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5788356340166239 | validation: 0.5468990912106343]
	TIME [epoch: 3.51 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_343.pth
	Model improved!!!
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5652528966136714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5652528966136714 | validation: 0.5493413345652818]
	TIME [epoch: 3.53 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5581956236571505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5581956236571505 | validation: 0.5298182908863475]
	TIME [epoch: 3.51 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_345.pth
	Model improved!!!
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5594097225165996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5594097225165996 | validation: 0.6800716882517427]
	TIME [epoch: 3.5 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6255593262653248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6255593262653248 | validation: 0.893461042045389]
	TIME [epoch: 3.51 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9243801580230698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9243801580230698 | validation: 0.7759239713628535]
	TIME [epoch: 3.51 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6974314312712558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6974314312712558 | validation: 0.543756855121121]
	TIME [epoch: 3.5 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5542273121222185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5542273121222185 | validation: 0.5117954457619133]
	TIME [epoch: 3.51 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_350.pth
	Model improved!!!
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.510414967440965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.510414967440965 | validation: 0.5245797055698893]
	TIME [epoch: 3.5 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4960059143370846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4960059143370846 | validation: 0.5055440713353659]
	TIME [epoch: 3.51 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_352.pth
	Model improved!!!
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4949764385246471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4949764385246471 | validation: 0.8931433342060215]
	TIME [epoch: 3.53 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7162929854770347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7162929854770347 | validation: 1.2249498480478764]
	TIME [epoch: 3.5 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1170704338537223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1170704338537223 | validation: 0.5358982225894018]
	TIME [epoch: 3.5 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5953256856086514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5953256856086514 | validation: 0.7098950135000317]
	TIME [epoch: 3.5 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7254130484146214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7254130484146214 | validation: 0.541230140936227]
	TIME [epoch: 3.51 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5916867137506178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5916867137506178 | validation: 0.5595201892944108]
	TIME [epoch: 3.5 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5997651921302596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5997651921302596 | validation: 0.5259859873590713]
	TIME [epoch: 3.5 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5568719750014733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5568719750014733 | validation: 0.5137848827221255]
	TIME [epoch: 3.51 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5299124563428247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5299124563428247 | validation: 0.5041314781985111]
	TIME [epoch: 3.51 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_361.pth
	Model improved!!!
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5085800129053704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5085800129053704 | validation: 0.5202766335741217]
	TIME [epoch: 3.51 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49424797894285283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49424797894285283 | validation: 0.504550283735272]
	TIME [epoch: 3.52 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49569868660784294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49569868660784294 | validation: 0.8775093428751369]
	TIME [epoch: 3.5 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7125144568429326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7125144568429326 | validation: 1.0901916088781465]
	TIME [epoch: 3.5 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.035429193154174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.035429193154174 | validation: 0.4830102644808982]
	TIME [epoch: 3.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_366.pth
	Model improved!!!
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5327792561908475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5327792561908475 | validation: 0.6696276643619851]
	TIME [epoch: 3.51 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6747851935772163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6747851935772163 | validation: 0.49696192991017085]
	TIME [epoch: 3.51 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5465662293968765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5465662293968765 | validation: 0.49292857549318625]
	TIME [epoch: 3.51 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5274671316457326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5274671316457326 | validation: 0.5168380914561885]
	TIME [epoch: 3.5 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5163683041557466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5163683041557466 | validation: 0.4699875569993876]
	TIME [epoch: 3.51 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_371.pth
	Model improved!!!
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4833610153667442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4833610153667442 | validation: 0.5230417502584712]
	TIME [epoch: 3.51 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4771390641516143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4771390641516143 | validation: 0.5337074689289886]
	TIME [epoch: 3.51 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5519212525787731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5519212525787731 | validation: 0.8741811021392887]
	TIME [epoch: 3.52 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7310801368183101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7310801368183101 | validation: 0.6651464354542977]
	TIME [epoch: 3.51 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6904948574636895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6904948574636895 | validation: 0.4863693110589499]
	TIME [epoch: 3.51 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5154009924279679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5154009924279679 | validation: 0.5368538993116193]
	TIME [epoch: 3.55 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5376978478793915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5376978478793915 | validation: 0.47173395026911114]
	TIME [epoch: 3.52 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5085509482335594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5085509482335594 | validation: 0.4388276337306185]
	TIME [epoch: 3.51 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_379.pth
	Model improved!!!
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4662895406372249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4662895406372249 | validation: 0.43701364608049115]
	TIME [epoch: 3.49 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_380.pth
	Model improved!!!
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43455018801335776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43455018801335776 | validation: 0.4468740148059276]
	TIME [epoch: 3.49 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4131015145154801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4131015145154801 | validation: 0.4357272975441495]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_382.pth
	Model improved!!!
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4090196665037981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4090196665037981 | validation: 0.4171311954833164]
	TIME [epoch: 3.53 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_383.pth
	Model improved!!!
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3978504296219239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3978504296219239 | validation: 0.44016898470736116]
	TIME [epoch: 3.52 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3923502208160265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3923502208160265 | validation: 0.5214738018169556]
	TIME [epoch: 3.51 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5282070577516123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5282070577516123 | validation: 1.1591378165148283]
	TIME [epoch: 3.51 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9804108338903598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9804108338903598 | validation: 0.5920642676765091]
	TIME [epoch: 3.52 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5760601479334513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5760601479334513 | validation: 0.5176596536155585]
	TIME [epoch: 3.51 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5281254327518522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5281254327518522 | validation: 0.49788292545994645]
	TIME [epoch: 3.51 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5063042097728592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5063042097728592 | validation: 0.4597973414837679]
	TIME [epoch: 3.5 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4550270314399451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4550270314399451 | validation: 0.4088386219767429]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_391.pth
	Model improved!!!
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4108061707306421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4108061707306421 | validation: 0.38763396281895496]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_392.pth
	Model improved!!!
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3781628423896926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3781628423896926 | validation: 0.39548731576298346]
	TIME [epoch: 3.53 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37173727227532966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37173727227532966 | validation: 0.38239755947747445]
	TIME [epoch: 3.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_394.pth
	Model improved!!!
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38093650713260846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38093650713260846 | validation: 0.5162490346790033]
	TIME [epoch: 3.5 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4296813715615416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4296813715615416 | validation: 0.8036228806094132]
	TIME [epoch: 3.5 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7547597353225551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7547597353225551 | validation: 0.5443780833153143]
	TIME [epoch: 3.52 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.492258480106861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.492258480106861 | validation: 0.3784853979173475]
	TIME [epoch: 3.51 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_398.pth
	Model improved!!!
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3790556818744193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3790556818744193 | validation: 0.3850599458841889]
	TIME [epoch: 3.52 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3458187046549716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3458187046549716 | validation: 0.367930870285311]
	TIME [epoch: 3.51 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_400.pth
	Model improved!!!
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3633902560853459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3633902560853459 | validation: 0.5719322770614531]
	TIME [epoch: 3.51 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4289239039826477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4289239039826477 | validation: 0.6806649389139496]
	TIME [epoch: 3.52 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6203886441817489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6203886441817489 | validation: 0.5102035508796549]
	TIME [epoch: 3.53 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5008734654397401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5008734654397401 | validation: 0.4050960628436038]
	TIME [epoch: 3.54 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4152803746417699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4152803746417699 | validation: 0.3759585374772435]
	TIME [epoch: 3.5 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3747618234491165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3747618234491165 | validation: 0.4179275796820326]
	TIME [epoch: 3.5 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3781893619526624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3781893619526624 | validation: 0.45979125022537376]
	TIME [epoch: 3.5 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46961306041956447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46961306041956447 | validation: 0.7346086826853666]
	TIME [epoch: 3.5 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5510294185826866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5510294185826866 | validation: 0.6048714333646186]
	TIME [epoch: 3.5 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5573196141320651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5573196141320651 | validation: 0.47439495932933795]
	TIME [epoch: 3.5 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45278080168532925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45278080168532925 | validation: 0.42931689821073404]
	TIME [epoch: 3.47 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41550476679855763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41550476679855763 | validation: 0.4105937116174663]
	TIME [epoch: 3.53 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3877411816221997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3877411816221997 | validation: 0.3990204026745416]
	TIME [epoch: 3.54 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36215426190018885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36215426190018885 | validation: 0.3714835367956151]
	TIME [epoch: 3.5 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3797827350999197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3797827350999197 | validation: 0.48455991114285857]
	TIME [epoch: 3.52 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4069082092359576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4069082092359576 | validation: 0.5232720192446447]
	TIME [epoch: 3.52 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49050370963530676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49050370963530676 | validation: 0.4934533560076993]
	TIME [epoch: 3.51 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44991717545306315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44991717545306315 | validation: 0.32893016239426887]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_418.pth
	Model improved!!!
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30835526198115953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30835526198115953 | validation: 0.35625885610573055]
	TIME [epoch: 3.51 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32843577037700455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32843577037700455 | validation: 0.3181422892992351]
	TIME [epoch: 3.51 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_420.pth
	Model improved!!!
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28632191117542244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28632191117542244 | validation: 0.36375831979895834]
	TIME [epoch: 3.52 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29383016449543187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29383016449543187 | validation: 0.3344931530335852]
	TIME [epoch: 3.52 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3135674073633238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3135674073633238 | validation: 0.5524580961951886]
	TIME [epoch: 3.51 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4571009031987536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4571009031987536 | validation: 0.7046119550879189]
	TIME [epoch: 3.5 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6562676228477707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6562676228477707 | validation: 0.4903604398166412]
	TIME [epoch: 3.51 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44346029702419765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44346029702419765 | validation: 0.4052868557158637]
	TIME [epoch: 3.51 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3757683381007047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3757683381007047 | validation: 0.36196542579045016]
	TIME [epoch: 3.5 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3470698164192973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3470698164192973 | validation: 0.45036299880994357]
	TIME [epoch: 3.51 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3587538976074754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3587538976074754 | validation: 0.3979485450476391]
	TIME [epoch: 3.51 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38526410674853323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38526410674853323 | validation: 0.3668165213245673]
	TIME [epoch: 3.5 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31950670890219207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31950670890219207 | validation: 0.2936938465899221]
	TIME [epoch: 3.51 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_431.pth
	Model improved!!!
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2738412846215481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2738412846215481 | validation: 0.30767480988755963]
	TIME [epoch: 3.52 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26242016497365317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26242016497365317 | validation: 0.29455297783380097]
	TIME [epoch: 3.52 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26010034522530745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26010034522530745 | validation: 0.3051338248779545]
	TIME [epoch: 3.51 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2519206641561998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2519206641561998 | validation: 0.2873425172385115]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_435.pth
	Model improved!!!
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2603291908067067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2603291908067067 | validation: 0.43698870292382797]
	TIME [epoch: 3.51 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3239047824573524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3239047824573524 | validation: 0.6346632982724915]
	TIME [epoch: 3.52 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5785977973752884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5785977973752884 | validation: 0.5123260007771284]
	TIME [epoch: 3.52 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4668457030220372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4668457030220372 | validation: 0.3247460788213717]
	TIME [epoch: 3.52 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32500100688967537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32500100688967537 | validation: 0.2916506552092026]
	TIME [epoch: 3.52 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27737414087643497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27737414087643497 | validation: 0.504614075683142]
	TIME [epoch: 3.54 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3806653948882195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3806653948882195 | validation: 0.45816592965845016]
	TIME [epoch: 3.53 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4548594361959165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4548594361959165 | validation: 0.46138067981743025]
	TIME [epoch: 3.51 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4266987489175713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4266987489175713 | validation: 0.3506240770357945]
	TIME [epoch: 3.51 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3292013943904263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3292013943904263 | validation: 0.30991553518370557]
	TIME [epoch: 3.52 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.288595170444331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.288595170444331 | validation: 0.4018534162329047]
	TIME [epoch: 3.52 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34412665705023415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34412665705023415 | validation: 0.4167147261207682]
	TIME [epoch: 3.52 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4036645683342687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4036645683342687 | validation: 0.334632912561615]
	TIME [epoch: 3.52 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2667449176828247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2667449176828247 | validation: 0.271370130797242]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_449.pth
	Model improved!!!
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22188534728381612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22188534728381612 | validation: 0.2798414679353802]
	TIME [epoch: 3.52 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23866276314771664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23866276314771664 | validation: 0.2853980909587465]
	TIME [epoch: 3.53 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23107084275101855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23107084275101855 | validation: 0.26728117018219977]
	TIME [epoch: 3.54 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_452.pth
	Model improved!!!
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2454426503996885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2454426503996885 | validation: 0.387404558039937]
	TIME [epoch: 3.51 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28472103105044483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28472103105044483 | validation: 0.4354379989840624]
	TIME [epoch: 3.52 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41513766041191946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41513766041191946 | validation: 0.4016783887029222]
	TIME [epoch: 3.52 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3873443482199979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3873443482199979 | validation: 0.2558989128175574]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_456.pth
	Model improved!!!
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23238187660876009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23238187660876009 | validation: 0.31383767633982657]
	TIME [epoch: 3.51 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28595133759346214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28595133759346214 | validation: 0.2566497136108237]
	TIME [epoch: 3.52 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2298200917351891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2298200917351891 | validation: 0.3253800360609072]
	TIME [epoch: 3.51 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2499519052299857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2499519052299857 | validation: 0.28567310611400404]
	TIME [epoch: 3.52 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23935879508946817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23935879508946817 | validation: 0.3675805858057719]
	TIME [epoch: 3.53 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28533508269879115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28533508269879115 | validation: 0.4212767071280865]
	TIME [epoch: 3.53 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4009835016364332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4009835016364332 | validation: 0.3361660411505012]
	TIME [epoch: 3.52 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32794964520291575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32794964520291575 | validation: 0.24810886419656164]
	TIME [epoch: 3.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_464.pth
	Model improved!!!
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20362720651791244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20362720651791244 | validation: 0.3219410833642009]
	TIME [epoch: 3.52 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2679313409041054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2679313409041054 | validation: 0.3355141576541109]
	TIME [epoch: 3.52 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30732451190217147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30732451190217147 | validation: 0.35572634533724123]
	TIME [epoch: 3.52 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28267555081081625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28267555081081625 | validation: 0.24889045024103212]
	TIME [epoch: 3.51 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1826625396895267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1826625396895267 | validation: 0.24571828790688566]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_469.pth
	Model improved!!!
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19348334687799137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19348334687799137 | validation: 0.30071203339908786]
	TIME [epoch: 3.51 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21557822806228835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21557822806228835 | validation: 0.3253212586150871]
	TIME [epoch: 3.52 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2983599290245333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2983599290245333 | validation: 0.25753700700868287]
	TIME [epoch: 3.52 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21843331551585543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21843331551585543 | validation: 0.3789938512547464]
	TIME [epoch: 3.52 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31744477844346297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31744477844346297 | validation: 0.4611950877542481]
	TIME [epoch: 3.52 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4222751852714264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4222751852714264 | validation: 0.370595410027321]
	TIME [epoch: 3.52 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3542063581604426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3542063581604426 | validation: 0.29224308196539917]
	TIME [epoch: 3.52 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2764912373344487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2764912373344487 | validation: 0.2282862307196834]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_477.pth
	Model improved!!!
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.183935625365125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.183935625365125 | validation: 0.26838526614458663]
	TIME [epoch: 3.51 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20024766297378652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20024766297378652 | validation: 0.2528694541356264]
	TIME [epoch: 3.51 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21310085340352047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21310085340352047 | validation: 0.24927393834346423]
	TIME [epoch: 3.51 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1959209953931403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1959209953931403 | validation: 0.24900468491790947]
	TIME [epoch: 3.51 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16434083248377412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16434083248377412 | validation: 0.214702260252997]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_482.pth
	Model improved!!!
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15347272534439516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15347272534439516 | validation: 0.2661173407738809]
	TIME [epoch: 3.51 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1951817569005602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1951817569005602 | validation: 0.3813577183739038]
	TIME [epoch: 3.5 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3388927630685851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3388927630685851 | validation: 0.2264693357328556]
	TIME [epoch: 3.5 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16780222559809135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16780222559809135 | validation: 0.49781529429329824]
	TIME [epoch: 3.5 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42054336735379677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42054336735379677 | validation: 0.4978408915430844]
	TIME [epoch: 3.51 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4544754967635163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4544754967635163 | validation: 0.3675311657971152]
	TIME [epoch: 3.5 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37073763077701866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37073763077701866 | validation: 0.26620819677105634]
	TIME [epoch: 3.51 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2597513487398428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2597513487398428 | validation: 0.2700639281718867]
	TIME [epoch: 3.5 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23094917304856888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23094917304856888 | validation: 0.2362942652122222]
	TIME [epoch: 3.5 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17472473827305723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17472473827305723 | validation: 0.23007894750283714]
	TIME [epoch: 3.51 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17135715393120027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17135715393120027 | validation: 0.21881126145041643]
	TIME [epoch: 3.51 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14792732294827857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14792732294827857 | validation: 0.208019065762494]
	TIME [epoch: 3.51 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_494.pth
	Model improved!!!
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1446725453091983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1446725453091983 | validation: 0.22567938392330983]
	TIME [epoch: 3.53 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15758102902586682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15758102902586682 | validation: 0.2446227855388084]
	TIME [epoch: 3.5 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20656117079525854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20656117079525854 | validation: 0.20450279944549132]
	TIME [epoch: 3.51 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_497.pth
	Model improved!!!
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13870070809342527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13870070809342527 | validation: 0.20196731270868287]
	TIME [epoch: 3.51 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_498.pth
	Model improved!!!
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12667812839336837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12667812839336837 | validation: 0.2106380317752665]
	TIME [epoch: 3.51 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14489744705466379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14489744705466379 | validation: 0.23071243666712277]
	TIME [epoch: 3.51 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22023493383282805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22023493383282805 | validation: 0.4172339144786813]
	TIME [epoch: 37.9 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37011083940106276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37011083940106276 | validation: 0.3521308345746967]
	TIME [epoch: 7.67 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36425569665261104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36425569665261104 | validation: 0.23666264597493752]
	TIME [epoch: 7.67 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17294713022023664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17294713022023664 | validation: 0.24860376145554652]
	TIME [epoch: 7.66 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.199156273678175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.199156273678175 | validation: 0.24140776072663706]
	TIME [epoch: 7.65 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21967573835092846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21967573835092846 | validation: 0.1942583748801205]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_506.pth
	Model improved!!!
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14317684099460246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14317684099460246 | validation: 0.22747717779244458]
	TIME [epoch: 7.66 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2111489583631462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2111489583631462 | validation: 0.2627257380063712]
	TIME [epoch: 7.66 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23369964890305486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23369964890305486 | validation: 0.2127381766771487]
	TIME [epoch: 7.66 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16772554214701058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16772554214701058 | validation: 0.19749506716674248]
	TIME [epoch: 7.68 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.160085637972664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.160085637972664 | validation: 0.20089463601118027]
	TIME [epoch: 7.66 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13144040960918965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13144040960918965 | validation: 0.19458906939222065]
	TIME [epoch: 7.66 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11364821860960046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11364821860960046 | validation: 0.1812147502972649]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_513.pth
	Model improved!!!
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11211833788267254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11211833788267254 | validation: 0.17052462606107813]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_514.pth
	Model improved!!!
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1246808851936656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1246808851936656 | validation: 0.2795342938972444]
	TIME [epoch: 7.66 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24141178661730167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24141178661730167 | validation: 0.19305724243016523]
	TIME [epoch: 7.64 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13536584285499217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13536584285499217 | validation: 0.2611282134363632]
	TIME [epoch: 7.67 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.278924134922542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.278924134922542 | validation: 0.3668946043232837]
	TIME [epoch: 7.66 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35674241430267406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35674241430267406 | validation: 0.34883263839152834]
	TIME [epoch: 7.66 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32688993968126284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32688993968126284 | validation: 0.33908301269979196]
	TIME [epoch: 7.64 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33604448152103467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33604448152103467 | validation: 0.22779588243150375]
	TIME [epoch: 7.66 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18540862278273465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18540862278273465 | validation: 0.23278092410209894]
	TIME [epoch: 7.65 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18196504314620243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18196504314620243 | validation: 0.2029528791183934]
	TIME [epoch: 7.67 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1469418908725214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1469418908725214 | validation: 0.1835258253534336]
	TIME [epoch: 7.64 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1359063303888601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1359063303888601 | validation: 0.17706031316234863]
	TIME [epoch: 7.66 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10594123487300718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10594123487300718 | validation: 0.16934253031036353]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_526.pth
	Model improved!!!
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10095529430563709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10095529430563709 | validation: 0.17019429088480145]
	TIME [epoch: 7.69 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10733976145270446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10733976145270446 | validation: 0.15130847084681093]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_528.pth
	Model improved!!!
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12402240313446175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12402240313446175 | validation: 0.26741525594803744]
	TIME [epoch: 7.63 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22763580130426989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22763580130426989 | validation: 0.16413263085592855]
	TIME [epoch: 7.63 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11260160132741127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11260160132741127 | validation: 0.1569454509840554]
	TIME [epoch: 7.65 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09604134812118176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09604134812118176 | validation: 0.16320822943303956]
	TIME [epoch: 7.63 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10224362375542705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10224362375542705 | validation: 0.17666804119638677]
	TIME [epoch: 7.65 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11310379451789276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11310379451789276 | validation: 0.15583999912716195]
	TIME [epoch: 7.62 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16277713848472192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16277713848472192 | validation: 0.39967849672031347]
	TIME [epoch: 7.62 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3508969501363483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3508969501363483 | validation: 0.284948274117992]
	TIME [epoch: 7.62 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2958210117196381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2958210117196381 | validation: 0.2498848244094571]
	TIME [epoch: 7.64 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25032230453709764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25032230453709764 | validation: 0.2412187810370978]
	TIME [epoch: 7.63 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18327317289660985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18327317289660985 | validation: 0.17759571797998996]
	TIME [epoch: 7.63 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11547047619711809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11547047619711809 | validation: 0.1525675919246385]
	TIME [epoch: 7.62 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09586652758217586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09586652758217586 | validation: 0.1590853520301818]
	TIME [epoch: 7.63 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09828074440925919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09828074440925919 | validation: 0.13549308609978056]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_542.pth
	Model improved!!!
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08938237275886435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08938237275886435 | validation: 0.15234730077696154]
	TIME [epoch: 7.63 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10325479303438663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10325479303438663 | validation: 0.1619402395109647]
	TIME [epoch: 7.62 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18642643717400384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18642643717400384 | validation: 0.3511791777223218]
	TIME [epoch: 7.62 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3054644643605144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3054644643605144 | validation: 0.1547338476301214]
	TIME [epoch: 7.63 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10318491854087036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10318491854087036 | validation: 0.22231672320388443]
	TIME [epoch: 7.64 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21906554628605776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21906554628605776 | validation: 0.18342894391782444]
	TIME [epoch: 7.62 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1401707416509324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1401707416509324 | validation: 0.1671004219379953]
	TIME [epoch: 7.63 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10911376148634445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10911376148634445 | validation: 0.15760851357786557]
	TIME [epoch: 7.62 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09967380284642584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09967380284642584 | validation: 0.1428400569543892]
	TIME [epoch: 7.64 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08410763946666436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08410763946666436 | validation: 0.13186880305007584]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_552.pth
	Model improved!!!
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07637997075008077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07637997075008077 | validation: 0.12180506531760214]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_553.pth
	Model improved!!!
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09047288444322263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09047288444322263 | validation: 0.19942705201944946]
	TIME [epoch: 7.6 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1486613324208587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1486613324208587 | validation: 0.12902649176491265]
	TIME [epoch: 7.61 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09498502651662274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09498502651662274 | validation: 0.13357566492338818]
	TIME [epoch: 7.62 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07729987347161925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07729987347161925 | validation: 0.13355705047950825]
	TIME [epoch: 7.62 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0813168555679246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0813168555679246 | validation: 0.16191019136718982]
	TIME [epoch: 7.62 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10288418685623917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10288418685623917 | validation: 0.1903255424584709]
	TIME [epoch: 7.63 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23975863323583999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23975863323583999 | validation: 0.3373451486468311]
	TIME [epoch: 7.62 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31444665439364994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31444665439364994 | validation: 0.2840937036183579]
	TIME [epoch: 7.63 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2532135113942221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2532135113942221 | validation: 0.17013526866999618]
	TIME [epoch: 7.62 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1394202773512464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1394202773512464 | validation: 0.1918480400074044]
	TIME [epoch: 7.61 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1624698347136804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1624698347136804 | validation: 0.15179687020295438]
	TIME [epoch: 7.61 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09523330530027412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09523330530027412 | validation: 0.15305679493579227]
	TIME [epoch: 7.63 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08798026151689069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08798026151689069 | validation: 0.1223243993455562]
	TIME [epoch: 7.63 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07632584923704348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07632584923704348 | validation: 0.1435680319048855]
	TIME [epoch: 7.62 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08279713225985084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08279713225985084 | validation: 0.11883943787784963]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_568.pth
	Model improved!!!
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09773223628970222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09773223628970222 | validation: 0.18863912380359749]
	TIME [epoch: 7.68 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14232784934156326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14232784934156326 | validation: 0.14266436108767042]
	TIME [epoch: 7.62 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13723301913297278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13723301913297278 | validation: 0.1781503585045716]
	TIME [epoch: 7.63 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12514471952028508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12514471952028508 | validation: 0.1280365113039373]
	TIME [epoch: 7.61 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08103099555393349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08103099555393349 | validation: 0.12343572378511072]
	TIME [epoch: 7.62 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07330013325845305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07330013325845305 | validation: 0.1288345689019534]
	TIME [epoch: 7.63 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07474393981784731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07474393981784731 | validation: 0.12112446625537342]
	TIME [epoch: 7.62 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1629287507820878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1629287507820878 | validation: 0.39734321486203206]
	TIME [epoch: 7.64 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.359455544644205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.359455544644205 | validation: 0.4184954970750925]
	TIME [epoch: 7.62 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3534044136096557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3534044136096557 | validation: 0.17384215020224478]
	TIME [epoch: 7.63 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15587229185063964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15587229185063964 | validation: 0.2630058745433403]
	TIME [epoch: 7.62 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2430611088531046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2430611088531046 | validation: 0.15646272303332676]
	TIME [epoch: 7.64 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11096383928439515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11096383928439515 | validation: 0.1368698789498674]
	TIME [epoch: 7.64 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09504920689040545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09504920689040545 | validation: 0.12931532713295313]
	TIME [epoch: 7.63 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07522983267166083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07522983267166083 | validation: 0.1316369257548447]
	TIME [epoch: 7.63 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07087413399315087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07087413399315087 | validation: 0.09743938163275999]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_584.pth
	Model improved!!!
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0787084024046245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0787084024046245 | validation: 0.18547174720976853]
	TIME [epoch: 7.64 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13654084605620184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13654084605620184 | validation: 0.13256714929514166]
	TIME [epoch: 7.62 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1407912029373256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1407912029373256 | validation: 0.20828318441967478]
	TIME [epoch: 7.62 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16258257207872592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16258257207872592 | validation: 0.12498301204338867]
	TIME [epoch: 7.61 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0798045339395058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0798045339395058 | validation: 0.10194045927246442]
	TIME [epoch: 7.63 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06741553062556076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06741553062556076 | validation: 0.12847938099014486]
	TIME [epoch: 7.64 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07604313499254609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07604313499254609 | validation: 0.08492591859100007]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_591.pth
	Model improved!!!
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07703009456152081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07703009456152081 | validation: 0.18101529365022928]
	TIME [epoch: 7.62 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12835794442613405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12835794442613405 | validation: 0.11593698295416051]
	TIME [epoch: 7.62 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08486686946579639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08486686946579639 | validation: 0.13297698472147887]
	TIME [epoch: 7.62 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0865250448283826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0865250448283826 | validation: 0.11378094084682376]
	TIME [epoch: 7.63 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12015357511896012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12015357511896012 | validation: 0.252772731569622]
	TIME [epoch: 7.61 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19385533419900236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19385533419900236 | validation: 0.17542809630965744]
	TIME [epoch: 7.6 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.149668802033806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.149668802033806 | validation: 0.13176708308434235]
	TIME [epoch: 7.61 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12908198196243295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12908198196243295 | validation: 0.15421840601254858]
	TIME [epoch: 7.61 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10346237477052292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10346237477052292 | validation: 0.11111843209471078]
	TIME [epoch: 7.62 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07531891825344376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07531891825344376 | validation: 0.08721492722676655]
	TIME [epoch: 7.61 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07590249380591954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07590249380591954 | validation: 0.13283068149791552]
	TIME [epoch: 7.61 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08621970093995678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08621970093995678 | validation: 0.07462936640860224]
	TIME [epoch: 7.6 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_603.pth
	Model improved!!!
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07822988867674535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07822988867674535 | validation: 0.2273609688415162]
	TIME [epoch: 7.62 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16743288653892532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16743288653892532 | validation: 0.17839756094891226]
	TIME [epoch: 7.64 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1437267532122719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1437267532122719 | validation: 0.12206366258837464]
	TIME [epoch: 7.62 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13798195909673663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13798195909673663 | validation: 0.1464968973980922]
	TIME [epoch: 7.63 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09802051239031885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09802051239031885 | validation: 0.1531727291027988]
	TIME [epoch: 7.62 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13288516887791954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13288516887791954 | validation: 0.11037015212378867]
	TIME [epoch: 7.64 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0751009540428434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0751009540428434 | validation: 0.06731503901024596]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_610.pth
	Model improved!!!
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07964315362284011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07964315362284011 | validation: 0.19824422116063042]
	TIME [epoch: 7.63 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1533545388125431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1533545388125431 | validation: 0.18327346104699802]
	TIME [epoch: 7.62 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24250928755986162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24250928755986162 | validation: 0.09094465849531914]
	TIME [epoch: 7.62 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14649482858948065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14649482858948065 | validation: 0.24457033210959056]
	TIME [epoch: 7.64 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20403221547686612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20403221547686612 | validation: 0.13906592728927952]
	TIME [epoch: 7.62 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11082416400565176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11082416400565176 | validation: 0.12947362053445355]
	TIME [epoch: 7.62 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10568639603971144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10568639603971144 | validation: 0.13020098049770548]
	TIME [epoch: 7.62 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07427739712707136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07427739712707136 | validation: 0.09899776701001438]
	TIME [epoch: 7.62 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06488998443249497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06488998443249497 | validation: 0.09236990776695582]
	TIME [epoch: 7.64 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04970851515919895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04970851515919895 | validation: 0.08115128130842976]
	TIME [epoch: 7.63 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049672576183810636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049672576183810636 | validation: 0.07340869965456755]
	TIME [epoch: 7.62 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04787661059225771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04787661059225771 | validation: 0.09011477765837861]
	TIME [epoch: 7.61 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05296813730333701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05296813730333701 | validation: 0.07986993493154376]
	TIME [epoch: 7.63 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10477657281400138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10477657281400138 | validation: 0.33585349628592404]
	TIME [epoch: 7.63 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2788063414772769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2788063414772769 | validation: 0.20553597892088274]
	TIME [epoch: 7.62 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1580647908703553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1580647908703553 | validation: 0.14707633643722232]
	TIME [epoch: 7.62 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11623194630783638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11623194630783638 | validation: 0.10135557127108896]
	TIME [epoch: 7.62 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06860618599894998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06860618599894998 | validation: 0.09985966439680663]
	TIME [epoch: 7.63 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059507903408763545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059507903408763545 | validation: 0.07913257384649418]
	TIME [epoch: 7.64 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05132323046625862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05132323046625862 | validation: 0.08913447875316227]
	TIME [epoch: 7.64 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04720602076159156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04720602076159156 | validation: 0.08037759728808279]
	TIME [epoch: 7.62 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11395170015031457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11395170015031457 | validation: 0.3819537559204606]
	TIME [epoch: 7.61 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31745786431322237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31745786431322237 | validation: 0.22793608817392444]
	TIME [epoch: 7.63 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22329313336870285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22329313336870285 | validation: 0.1305692129106634]
	TIME [epoch: 7.64 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1631916734151771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1631916734151771 | validation: 0.15536465314777642]
	TIME [epoch: 7.62 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10942782217399372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10942782217399372 | validation: 0.11065574907358156]
	TIME [epoch: 7.62 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07864971086263642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07864971086263642 | validation: 0.08848141169366719]
	TIME [epoch: 7.62 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0626125975526499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0626125975526499 | validation: 0.09582044517394493]
	TIME [epoch: 7.63 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0520559029997439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0520559029997439 | validation: 0.0745931289138603]
	TIME [epoch: 7.63 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04811248450720179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04811248450720179 | validation: 0.06329851173922327]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_640.pth
	Model improved!!!
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04121921100477712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04121921100477712 | validation: 0.07464889977200007]
	TIME [epoch: 7.62 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04261131079042132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04261131079042132 | validation: 0.049242841238656215]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_642.pth
	Model improved!!!
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0575227231317823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0575227231317823 | validation: 0.2106416182376171]
	TIME [epoch: 7.64 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15144126232131563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15144126232131563 | validation: 0.14875433743129127]
	TIME [epoch: 7.64 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10708216769623803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10708216769623803 | validation: 0.10181167617928072]
	TIME [epoch: 7.63 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10476114636259853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10476114636259853 | validation: 0.11442819362667503]
	TIME [epoch: 7.63 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0808478152705549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0808478152705549 | validation: 0.08816935238478092]
	TIME [epoch: 7.64 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08444749149702262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08444749149702262 | validation: 0.19531537365880391]
	TIME [epoch: 7.66 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1339581534828417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1339581534828417 | validation: 0.0843914594582254]
	TIME [epoch: 7.63 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08858783648292924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08858783648292924 | validation: 0.12409667477661102]
	TIME [epoch: 7.64 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07725473062103479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07725473062103479 | validation: 0.05507123088607843]
	TIME [epoch: 7.63 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05538098508458042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05538098508458042 | validation: 0.11228345308815252]
	TIME [epoch: 7.65 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07163972798675977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07163972798675977 | validation: 0.061420441559498135]
	TIME [epoch: 7.65 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06109914368608504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06109914368608504 | validation: 0.16277219377535662]
	TIME [epoch: 7.65 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11378292978589423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11378292978589423 | validation: 0.08962616629358072]
	TIME [epoch: 7.61 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06133569222400455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06133569222400455 | validation: 0.07444011484327172]
	TIME [epoch: 7.63 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05373426408263065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05373426408263065 | validation: 0.06905517499227189]
	TIME [epoch: 7.63 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041415163444157896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041415163444157896 | validation: 0.04632616200284039]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_658.pth
	Model improved!!!
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03590575710212251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03590575710212251 | validation: 0.11536232184933466]
	TIME [epoch: 7.63 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06855305384148724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06855305384148724 | validation: 0.19515379989561954]
	TIME [epoch: 7.62 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26528377030452527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26528377030452527 | validation: 0.2825396765433606]
	TIME [epoch: 7.63 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23067213649405985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23067213649405985 | validation: 0.12453118857535067]
	TIME [epoch: 7.63 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08288820472243502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08288820472243502 | validation: 0.10675848758255789]
	TIME [epoch: 7.65 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12519540906234383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12519540906234383 | validation: 0.12790821275961353]
	TIME [epoch: 7.62 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0811268116846979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0811268116846979 | validation: 0.11933832134328942]
	TIME [epoch: 7.63 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07299341501507138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07299341501507138 | validation: 0.08578427421694063]
	TIME [epoch: 7.62 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05344191497253716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05344191497253716 | validation: 0.07139229497370808]
	TIME [epoch: 7.63 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0419077290132747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0419077290132747 | validation: 0.0706616153020274]
	TIME [epoch: 7.64 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03898137951262685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03898137951262685 | validation: 0.039501089113241954]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_669.pth
	Model improved!!!
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04907333157536991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04907333157536991 | validation: 0.137343279200507]
	TIME [epoch: 7.63 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09043877796493073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09043877796493073 | validation: 0.0818946824271215]
	TIME [epoch: 7.61 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06506153941711286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06506153941711286 | validation: 0.10551708574292173]
	TIME [epoch: 7.64 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07145369395775136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07145369395775136 | validation: 0.08812405456839134]
	TIME [epoch: 7.63 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10338463031827157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10338463031827157 | validation: 0.23127341879880922]
	TIME [epoch: 7.64 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17436261444180798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17436261444180798 | validation: 0.1308735271853123]
	TIME [epoch: 7.62 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0941256609317093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0941256609317093 | validation: 0.07166116288644281]
	TIME [epoch: 7.64 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07679740783676468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07679740783676468 | validation: 0.11923159142396225]
	TIME [epoch: 7.64 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07678637060135814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07678637060135814 | validation: 0.09069302341477073]
	TIME [epoch: 7.64 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07194133244807828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07194133244807828 | validation: 0.06674970096371373]
	TIME [epoch: 7.64 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050945423214935275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050945423214935275 | validation: 0.05978442964850629]
	TIME [epoch: 7.63 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03843691362550142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03843691362550142 | validation: 0.04251786744608152]
	TIME [epoch: 7.64 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03407786604529471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03407786604529471 | validation: 0.11449041700419987]
	TIME [epoch: 7.65 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0727286544930281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0727286544930281 | validation: 0.09136256572363331]
	TIME [epoch: 7.63 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13540481124367637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13540481124367637 | validation: 0.2954968019795857]
	TIME [epoch: 7.63 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21136596056135684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21136596056135684 | validation: 0.17568446016187744]
	TIME [epoch: 7.63 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1242534285923578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1242534285923578 | validation: 0.08362089191017269]
	TIME [epoch: 7.62 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10397243308998252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10397243308998252 | validation: 0.11606437493207511]
	TIME [epoch: 7.65 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08636955062763609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08636955062763609 | validation: 0.08995562924424244]
	TIME [epoch: 7.63 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06115039095360564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06115039095360564 | validation: 0.0768247728943683]
	TIME [epoch: 7.62 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047851498847631416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047851498847631416 | validation: 0.05171567540115956]
	TIME [epoch: 7.63 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04554020337079608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04554020337079608 | validation: 0.11844166885315453]
	TIME [epoch: 7.64 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07581848029917511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07581848029917511 | validation: 0.05211291749858319]
	TIME [epoch: 7.65 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06795350149536478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06795350149536478 | validation: 0.13587089617373876]
	TIME [epoch: 7.64 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08984765791712335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08984765791712335 | validation: 0.06880510379232947]
	TIME [epoch: 7.62 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06579313049244641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06579313049244641 | validation: 0.10282453242670396]
	TIME [epoch: 7.63 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07003280638342084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07003280638342084 | validation: 0.0598908351443789]
	TIME [epoch: 7.63 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07049002651588106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07049002651588106 | validation: 0.15621251413270298]
	TIME [epoch: 7.65 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10672902867490759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10672902867490759 | validation: 0.06629168429017528]
	TIME [epoch: 7.62 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047848417270961056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047848417270961056 | validation: 0.04659582776132483]
	TIME [epoch: 7.62 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04692959235038928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04692959235038928 | validation: 0.09844902148272215]
	TIME [epoch: 7.62 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056921406701440434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056921406701440434 | validation: 0.05140205832961064]
	TIME [epoch: 7.67 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06301989720747236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06301989720747236 | validation: 0.14564658728822563]
	TIME [epoch: 7.64 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09936712600507676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09936712600507676 | validation: 0.09044418333723114]
	TIME [epoch: 7.63 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07392039508286934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07392039508286934 | validation: 0.09304736858356649]
	TIME [epoch: 7.64 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07251220898145959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07251220898145959 | validation: 0.04875804618598972]
	TIME [epoch: 7.63 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04114320257501253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04114320257501253 | validation: 0.037626541455716526]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_706.pth
	Model improved!!!
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026329769449877603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026329769449877603 | validation: 0.03749306538137014]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_707.pth
	Model improved!!!
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02655328276847804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02655328276847804 | validation: 0.045117671241367854]
	TIME [epoch: 7.62 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034888966437234555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034888966437234555 | validation: 0.1379459721013912]
	TIME [epoch: 7.61 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09376213780801554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09376213780801554 | validation: 0.1731379571078344]
	TIME [epoch: 7.62 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24574919246116728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24574919246116728 | validation: 0.22874050825116776]
	TIME [epoch: 7.63 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1551705448557563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1551705448557563 | validation: 0.1930961299847486]
	TIME [epoch: 7.62 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12795050836386607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12795050836386607 | validation: 0.12602679723063925]
	TIME [epoch: 7.61 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09663924440730359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09663924440730359 | validation: 0.08212610030537656]
	TIME [epoch: 7.61 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06287469834501953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06287469834501953 | validation: 0.07389973952419492]
	TIME [epoch: 7.62 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041962079664913825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041962079664913825 | validation: 0.03706636623638332]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_716.pth
	Model improved!!!
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03318626779414042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03318626779414042 | validation: 0.04540019274603884]
	TIME [epoch: 7.64 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02723509301707367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02723509301707367 | validation: 0.03636595630354287]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_718.pth
	Model improved!!!
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024954988402952467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024954988402952467 | validation: 0.0420407764885361]
	TIME [epoch: 7.64 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026651913435070052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026651913435070052 | validation: 0.031210019577437544]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_720.pth
	Model improved!!!
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041788331447051415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041788331447051415 | validation: 0.22569777909827568]
	TIME [epoch: 7.66 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15982029455026156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15982029455026156 | validation: 0.17622151823086055]
	TIME [epoch: 7.66 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2383960989506739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2383960989506739 | validation: 0.16626596922928705]
	TIME [epoch: 7.64 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11635632424918957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11635632424918957 | validation: 0.06848412207873157]
	TIME [epoch: 7.64 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04746468233507184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04746468233507184 | validation: 0.06856856483878349]
	TIME [epoch: 7.64 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05474453717901943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05474453717901943 | validation: 0.06278320610793679]
	TIME [epoch: 7.66 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04306947943397219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04306947943397219 | validation: 0.04449954411569723]
	TIME [epoch: 7.64 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03537318550834415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03537318550834415 | validation: 0.0581588022845102]
	TIME [epoch: 7.65 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036327567998044596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036327567998044596 | validation: 0.032805220242764714]
	TIME [epoch: 7.64 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04507068307541676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04507068307541676 | validation: 0.14826753495620448]
	TIME [epoch: 7.67 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10764925924818677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10764925924818677 | validation: 0.0633414141432188]
	TIME [epoch: 7.65 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052831048069940725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052831048069940725 | validation: 0.05529429715521425]
	TIME [epoch: 7.65 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057491211986248986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057491211986248986 | validation: 0.08475375700422164]
	TIME [epoch: 7.64 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055163625770916054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055163625770916054 | validation: 0.034687709316817604]
	TIME [epoch: 7.65 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05589169974590063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05589169974590063 | validation: 0.16248697509422244]
	TIME [epoch: 7.67 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10518829628780296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10518829628780296 | validation: 0.08728942028094104]
	TIME [epoch: 7.65 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07468351525128288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07468351525128288 | validation: 0.0864919526036474]
	TIME [epoch: 7.64 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06431113620759155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06431113620759155 | validation: 0.033699022322106045]
	TIME [epoch: 7.65 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0402862076568774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0402862076568774 | validation: 0.09704420008681905]
	TIME [epoch: 7.65 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06376828218299475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06376828218299475 | validation: 0.04105763608069826]
	TIME [epoch: 7.67 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048383644824676095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048383644824676095 | validation: 0.09577261452817343]
	TIME [epoch: 7.65 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07341444002879496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07341444002879496 | validation: 0.0618711864108633]
	TIME [epoch: 7.65 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06335563532153467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06335563532153467 | validation: 0.1314078949170682]
	TIME [epoch: 7.64 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08645465586335778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08645465586335778 | validation: 0.05792970727103652]
	TIME [epoch: 7.65 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048306722628276864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048306722628276864 | validation: 0.04235429372682688]
	TIME [epoch: 7.66 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03364943157628097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03364943157628097 | validation: 0.06255240446196889]
	TIME [epoch: 7.65 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0386262987679317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0386262987679317 | validation: 0.035570043474401125]
	TIME [epoch: 7.64 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05959680523167009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05959680523167009 | validation: 0.20888179831882964]
	TIME [epoch: 7.65 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14651031392653477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14651031392653477 | validation: 0.10485902499342066]
	TIME [epoch: 7.66 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.089129452892291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.089129452892291 | validation: 0.0706658629051793]
	TIME [epoch: 7.66 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06312913884736282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06312913884736282 | validation: 0.07354219029426892]
	TIME [epoch: 7.66 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045975384631846555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045975384631846555 | validation: 0.043806034009275865]
	TIME [epoch: 7.64 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060090062277358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060090062277358 | validation: 0.17974202600957886]
	TIME [epoch: 7.65 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12091606935604028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12091606935604028 | validation: 0.07391663129796466]
	TIME [epoch: 7.64 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06383359825084424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06383359825084424 | validation: 0.05123910494534916]
	TIME [epoch: 7.65 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032657838897675935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032657838897675935 | validation: 0.03545109683946057]
	TIME [epoch: 7.65 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02233465699595186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02233465699595186 | validation: 0.04083278092691572]
	TIME [epoch: 7.64 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026036770281542738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026036770281542738 | validation: 0.026942509677122653]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_758.pth
	Model improved!!!
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031006037968706993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031006037968706993 | validation: 0.14169014277013794]
	TIME [epoch: 7.62 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10292171909966044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10292171909966044 | validation: 0.051821142750304985]
	TIME [epoch: 7.62 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05384455950674519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05384455950674519 | validation: 0.09389495263052525]
	TIME [epoch: 7.62 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06640301660879076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06640301660879076 | validation: 0.062436134851180713]
	TIME [epoch: 7.62 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0912599370052479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0912599370052479 | validation: 0.23326609711069013]
	TIME [epoch: 7.63 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1606093456522455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1606093456522455 | validation: 0.10471517330606091]
	TIME [epoch: 7.62 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08496635931890666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08496635931890666 | validation: 0.04018571322315615]
	TIME [epoch: 7.63 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05732139653221713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05732139653221713 | validation: 0.09537896726417515]
	TIME [epoch: 7.62 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07325896829844457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07325896829844457 | validation: 0.06173243828195445]
	TIME [epoch: 7.62 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06541151256300451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06541151256300451 | validation: 0.06665469054336674]
	TIME [epoch: 7.62 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04837066371371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04837066371371 | validation: 0.03227440341778622]
	TIME [epoch: 7.63 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04843465584786465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04843465584786465 | validation: 0.10142570283306625]
	TIME [epoch: 7.62 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0690622355200447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0690622355200447 | validation: 0.039348745619655824]
	TIME [epoch: 7.62 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04330157082553487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04330157082553487 | validation: 0.05629529350127062]
	TIME [epoch: 7.63 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03582216659549184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03582216659549184 | validation: 0.03470681622066365]
	TIME [epoch: 7.62 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0426473362029274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0426473362029274 | validation: 0.10255411694125711]
	TIME [epoch: 7.64 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06946876934386247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06946876934386247 | validation: 0.04567861064655352]
	TIME [epoch: 7.63 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048423931001312294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048423931001312294 | validation: 0.085060602110452]
	TIME [epoch: 7.67 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06096496116384965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06096496116384965 | validation: 0.0693381401193348]
	TIME [epoch: 7.62 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11419635008547385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11419635008547385 | validation: 0.20206786934059884]
	TIME [epoch: 7.63 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1349735995413159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1349735995413159 | validation: 0.13931383229692257]
	TIME [epoch: 7.64 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09669031915019945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09669031915019945 | validation: 0.057289626824362165]
	TIME [epoch: 7.64 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039564018477280576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039564018477280576 | validation: 0.047971480096084686]
	TIME [epoch: 7.61 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053934082382564946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053934082382564946 | validation: 0.05178872493045561]
	TIME [epoch: 7.61 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0428257356375252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0428257356375252 | validation: 0.04303736304311036]
	TIME [epoch: 7.63 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03301454609445381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03301454609445381 | validation: 0.026046632364258682]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_784.pth
	Model improved!!!
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032491074591499455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032491074591499455 | validation: 0.03410110002082222]
	TIME [epoch: 7.63 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02453181049996123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02453181049996123 | validation: 0.023792272660873293]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_786.pth
	Model improved!!!
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03874586231784935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03874586231784935 | validation: 0.1566229489178742]
	TIME [epoch: 7.67 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1097669812259293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1097669812259293 | validation: 0.09599038692977653]
	TIME [epoch: 7.64 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08614148143600751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08614148143600751 | validation: 0.12958541206449278]
	TIME [epoch: 7.65 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11034654224020468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11034654224020468 | validation: 0.060476497149447174]
	TIME [epoch: 7.64 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07411298211880198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07411298211880198 | validation: 0.11684518198936396]
	TIME [epoch: 7.64 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08663449439455669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08663449439455669 | validation: 0.040174233416240636]
	TIME [epoch: 7.63 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0346851729338149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0346851729338149 | validation: 0.02845944764614138]
	TIME [epoch: 7.64 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05743326650535322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05743326650535322 | validation: 0.12407966705782354]
	TIME [epoch: 7.65 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08605207536929772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08605207536929772 | validation: 0.07866841879793214]
	TIME [epoch: 7.63 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07056669903964284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07056669903964284 | validation: 0.04884329045940642]
	TIME [epoch: 7.63 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04416280690082711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04416280690082711 | validation: 0.04059838253915224]
	TIME [epoch: 7.64 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03030119931407188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03030119931407188 | validation: 0.02726853861802392]
	TIME [epoch: 7.64 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030467798941117473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030467798941117473 | validation: 0.07601357201306067]
	TIME [epoch: 7.62 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05498394537953076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05498394537953076 | validation: 0.07794691559191137]
	TIME [epoch: 7.62 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1199284831234766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1199284831234766 | validation: 0.204012264261904]
	TIME [epoch: 7.64 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15694786951935438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15694786951935438 | validation: 0.07215453976919754]
	TIME [epoch: 7.62 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05387989702472965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05387989702472965 | validation: 0.03795788080178589]
	TIME [epoch: 7.64 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04445795009285776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04445795009285776 | validation: 0.0658141075491575]
	TIME [epoch: 7.63 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05217785562324687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05217785562324687 | validation: 0.04099529020994457]
	TIME [epoch: 7.64 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043749894622982205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043749894622982205 | validation: 0.053189079248681694]
	TIME [epoch: 7.62 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035806033272294126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035806033272294126 | validation: 0.023677521687174654]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_807.pth
	Model improved!!!
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030631551672072002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030631551672072002 | validation: 0.05057277894956063]
	TIME [epoch: 7.64 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03255552952149567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03255552952149567 | validation: 0.03814999667909317]
	TIME [epoch: 7.62 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06612596545581072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06612596545581072 | validation: 0.22024823221330012]
	TIME [epoch: 7.63 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15511760356426085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15511760356426085 | validation: 0.07787216571594648]
	TIME [epoch: 7.62 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07001410239850275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07001410239850275 | validation: 0.047788763099189474]
	TIME [epoch: 7.63 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051905838574760374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051905838574760374 | validation: 0.05181125754367088]
	TIME [epoch: 7.63 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038817685117633574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038817685117633574 | validation: 0.032550145105818176]
	TIME [epoch: 7.62 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03089689964309856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03089689964309856 | validation: 0.029700271658669387]
	TIME [epoch: 7.63 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026206932362637842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026206932362637842 | validation: 0.028673895574262268]
	TIME [epoch: 7.63 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028219491170297692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028219491170297692 | validation: 0.03632656507954949]
	TIME [epoch: 7.63 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028419718039964063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028419718039964063 | validation: 0.032692187062275314]
	TIME [epoch: 7.65 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04343305063814826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04343305063814826 | validation: 0.16334716651889464]
	TIME [epoch: 7.63 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12979959571436592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12979959571436592 | validation: 0.08286867133323406]
	TIME [epoch: 7.63 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09286082093033753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09286082093033753 | validation: 0.06983839973669219]
	TIME [epoch: 7.64 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05476257868712798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05476257868712798 | validation: 0.026183274567162265]
	TIME [epoch: 7.64 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043125988226948735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043125988226948735 | validation: 0.07758784825266121]
	TIME [epoch: 7.64 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04657972034566704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04657972034566704 | validation: 0.0239432295256242]
	TIME [epoch: 7.62 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02285158612348365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02285158612348365 | validation: 0.023790736394751513]
	TIME [epoch: 7.63 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027839703301085122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027839703301085122 | validation: 0.05053163756012672]
	TIME [epoch: 7.61 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02819217491260871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02819217491260871 | validation: 0.02679305869897456]
	TIME [epoch: 7.64 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029545989160952146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029545989160952146 | validation: 0.07951883080808811]
	TIME [epoch: 7.63 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06440731248752025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06440731248752025 | validation: 0.08531887691024838]
	TIME [epoch: 7.62 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12451615409376203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12451615409376203 | validation: 0.17393834633264316]
	TIME [epoch: 7.62 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13748338362237886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13748338362237886 | validation: 0.10148552721612769]
	TIME [epoch: 7.63 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07714641948629845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07714641948629845 | validation: 0.033560564155124616]
	TIME [epoch: 7.63 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034521461283347356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034521461283347356 | validation: 0.034658479172240624]
	TIME [epoch: 7.6 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028443488830592327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028443488830592327 | validation: 0.027458503730202366]
	TIME [epoch: 7.61 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03242664604016518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03242664604016518 | validation: 0.04627363333373013]
	TIME [epoch: 7.6 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03234256300716634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03234256300716634 | validation: 0.025692039666907796]
	TIME [epoch: 7.62 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03966159443663279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03966159443663279 | validation: 0.0863717873840924]
	TIME [epoch: 7.62 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0730487244016304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0730487244016304 | validation: 0.05876208223980409]
	TIME [epoch: 7.6 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08177079115895917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08177079115895917 | validation: 0.08991567154201754]
	TIME [epoch: 7.61 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06945117932046184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06945117932046184 | validation: 0.03767550101183013]
	TIME [epoch: 7.62 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038312283246186664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038312283246186664 | validation: 0.030476120063105627]
	TIME [epoch: 7.6 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027165180630346928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027165180630346928 | validation: 0.022275362395241628]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_842.pth
	Model improved!!!
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01943385341371784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01943385341371784 | validation: 0.01698720358035959]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_843.pth
	Model improved!!!
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015826256254787303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015826256254787303 | validation: 0.01805520734633853]
	TIME [epoch: 7.63 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01651942269566855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01651942269566855 | validation: 0.02649954800472484]
	TIME [epoch: 7.65 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021712539324606612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021712539324606612 | validation: 0.03170245524436446]
	TIME [epoch: 7.6 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054769704012595746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054769704012595746 | validation: 0.21606313863153936]
	TIME [epoch: 7.66 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16055212208616737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16055212208616737 | validation: 0.12928815867684643]
	TIME [epoch: 7.6 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1287958667606503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1287958667606503 | validation: 0.05112650606176039]
	TIME [epoch: 7.61 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045314830578331515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045314830578331515 | validation: 0.05002164130421145]
	TIME [epoch: 7.6 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03680702884925929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03680702884925929 | validation: 0.03758517789874249]
	TIME [epoch: 7.61 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045448687930492775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045448687930492775 | validation: 0.08379177742549465]
	TIME [epoch: 7.63 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.070132515751526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.070132515751526 | validation: 0.04720623543096206]
	TIME [epoch: 7.6 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08164982853564454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08164982853564454 | validation: 0.09829371962891556]
	TIME [epoch: 7.59 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07280859307639723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07280859307639723 | validation: 0.02798329688142307]
	TIME [epoch: 7.6 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031293963690340314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031293963690340314 | validation: 0.018442945124209378]
	TIME [epoch: 7.62 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01875334812960793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01875334812960793 | validation: 0.01642777578053214]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_857.pth
	Model improved!!!
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01403317056602708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01403317056602708 | validation: 0.012701191557298841]
	TIME [epoch: 7.6 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_858.pth
	Model improved!!!
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017347598458774906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017347598458774906 | validation: 0.04455740464953278]
	TIME [epoch: 7.59 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031034567052629823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031034567052629823 | validation: 0.03691067530998269]
	TIME [epoch: 7.6 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07311419151511035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07311419151511035 | validation: 0.1998143017305787]
	TIME [epoch: 7.62 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14423154389386644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14423154389386644 | validation: 0.12943737595620683]
	TIME [epoch: 7.6 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11533572281015712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11533572281015712 | validation: 0.03906307181068545]
	TIME [epoch: 7.61 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04222000852171018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04222000852171018 | validation: 0.033439022609234086]
	TIME [epoch: 7.61 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03526825998609061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03526825998609061 | validation: 0.0318509042716408]
	TIME [epoch: 7.61 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02254868746969762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02254868746969762 | validation: 0.01524301038503727]
	TIME [epoch: 7.62 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023052829151847885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023052829151847885 | validation: 0.021691464312591047]
	TIME [epoch: 7.61 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019182328331674752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019182328331674752 | validation: 0.0118406393406013]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_868.pth
	Model improved!!!
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022168050540957458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022168050540957458 | validation: 0.09049396747268668]
	TIME [epoch: 7.6 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07071074889640676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07071074889640676 | validation: 0.11456019420791118]
	TIME [epoch: 7.61 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.161685401453848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.161685401453848 | validation: 0.17835695487851788]
	TIME [epoch: 7.62 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14828655076514505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14828655076514505 | validation: 0.05544859248207928]
	TIME [epoch: 7.59 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04581136205405478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04581136205405478 | validation: 0.03428626814599444]
	TIME [epoch: 7.6 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040579452474093766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040579452474093766 | validation: 0.051668965403824124]
	TIME [epoch: 7.62 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04443240696529492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04443240696529492 | validation: 0.020257423044109768]
	TIME [epoch: 7.6 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02384778087843727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02384778087843727 | validation: 0.020877338725786454]
	TIME [epoch: 7.62 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023125073423236466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023125073423236466 | validation: 0.008481139958300011]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_877.pth
	Model improved!!!
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01876319049283894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01876319049283894 | validation: 0.025654830390098193]
	TIME [epoch: 7.61 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024403221898965786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024403221898965786 | validation: 0.018761880104020846]
	TIME [epoch: 7.6 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035704527262192025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035704527262192025 | validation: 0.0919071902630646]
	TIME [epoch: 7.59 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06911339256261224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06911339256261224 | validation: 0.062022526974823945]
	TIME [epoch: 7.61 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06766556953619272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06766556953619272 | validation: 0.09466277963849286]
	TIME [epoch: 7.6 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08203649217801788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08203649217801788 | validation: 0.046063497339808926]
	TIME [epoch: 7.58 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06484646360427833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06484646360427833 | validation: 0.08631576454636483]
	TIME [epoch: 7.61 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058463615237683404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058463615237683404 | validation: 0.020079293133333544]
	TIME [epoch: 7.6 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022086446454434975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022086446454434975 | validation: 0.020622448597924573]
	TIME [epoch: 7.61 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022979817983459123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022979817983459123 | validation: 0.027384549505572255]
	TIME [epoch: 7.59 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021507022495727137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021507022495727137 | validation: 0.014588829492172785]
	TIME [epoch: 7.6 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022389153009829892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022389153009829892 | validation: 0.03836258903139527]
	TIME [epoch: 7.59 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032552611628355974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032552611628355974 | validation: 0.02580346070328098]
	TIME [epoch: 7.61 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037308387373446404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037308387373446404 | validation: 0.08103095680239474]
	TIME [epoch: 7.63 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0633823428575906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0633823428575906 | validation: 0.08198225162922596]
	TIME [epoch: 7.6 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10409915638266293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10409915638266293 | validation: 0.1312404397527207]
	TIME [epoch: 7.59 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1101422527832214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1101422527832214 | validation: 0.04181641386011529]
	TIME [epoch: 7.59 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04019524873450926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04019524873450926 | validation: 0.02183513748051893]
	TIME [epoch: 7.6 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036305719196696264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036305719196696264 | validation: 0.060286932939730725]
	TIME [epoch: 7.6 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04278032006572993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04278032006572993 | validation: 0.03344308083844632]
	TIME [epoch: 7.6 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039527514152402334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039527514152402334 | validation: 0.02736590951848901]
	TIME [epoch: 7.58 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026664351370976728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026664351370976728 | validation: 0.011782121131769786]
	TIME [epoch: 7.59 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019392270095406176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019392270095406176 | validation: 0.0361686182740607]
	TIME [epoch: 7.62 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029857017116764657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029857017116764657 | validation: 0.018483314054672472]
	TIME [epoch: 7.58 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036681491209596254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036681491209596254 | validation: 0.1003040194451927]
	TIME [epoch: 7.6 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07381369749843616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07381369749843616 | validation: 0.059358248116101235]
	TIME [epoch: 7.61 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08024175339804458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08024175339804458 | validation: 0.10365814418258501]
	TIME [epoch: 7.6 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07653811938350727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07653811938350727 | validation: 0.034988543333651735]
	TIME [epoch: 7.62 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03195559857198326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03195559857198326 | validation: 0.019001777708534697]
	TIME [epoch: 7.6 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0335887584387246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0335887584387246 | validation: 0.051483465723063795]
	TIME [epoch: 7.6 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04291543591124272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04291543591124272 | validation: 0.024629379237814154]
	TIME [epoch: 7.6 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03518965286543445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03518965286543445 | validation: 0.04670152819842626]
	TIME [epoch: 7.61 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04551850715787359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04551850715787359 | validation: 0.043623299610454074]
	TIME [epoch: 7.62 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06314358511378948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06314358511378948 | validation: 0.1044657926058972]
	TIME [epoch: 7.6 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08878593832805429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08878593832805429 | validation: 0.049276469337400264]
	TIME [epoch: 7.59 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04436643485033516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04436643485033516 | validation: 0.04355732232186506]
	TIME [epoch: 7.6 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040916406517867154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040916406517867154 | validation: 0.04584437440425652]
	TIME [epoch: 7.6 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03766275928321231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03766275928321231 | validation: 0.02765747085611694]
	TIME [epoch: 7.62 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03342498358613291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03342498358613291 | validation: 0.03457533403657209]
	TIME [epoch: 7.59 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032812550524785294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032812550524785294 | validation: 0.032060590060359384]
	TIME [epoch: 7.6 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04642488220874924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04642488220874924 | validation: 0.12003963381677796]
	TIME [epoch: 7.6 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09265798979401405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09265798979401405 | validation: 0.04977791815587071]
	TIME [epoch: 7.61 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0505538074434509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0505538074434509 | validation: 0.03538135143352215]
	TIME [epoch: 7.61 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03601607774178341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03601607774178341 | validation: 0.020879827704193877]
	TIME [epoch: 7.59 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01759839314689408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01759839314689408 | validation: 0.016349834781721917]
	TIME [epoch: 7.58 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024812001484589983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024812001484589983 | validation: 0.04055648691753733]
	TIME [epoch: 7.59 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036663086676625385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036663086676625385 | validation: 0.05223067288966247]
	TIME [epoch: 7.59 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08295641707576254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08295641707576254 | validation: 0.1463233812269326]
	TIME [epoch: 7.61 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11598862573417953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11598862573417953 | validation: 0.06769644417960985]
	TIME [epoch: 7.58 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06287017871062595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06287017871062595 | validation: 0.020812905856591915]
	TIME [epoch: 7.61 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036616839105002276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036616839105002276 | validation: 0.05101737815962957]
	TIME [epoch: 7.6 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03726342665939245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03726342665939245 | validation: 0.028675990776465033]
	TIME [epoch: 7.6 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04312389494266098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04312389494266098 | validation: 0.043418498867193414]
	TIME [epoch: 7.6 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04027475294113749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04027475294113749 | validation: 0.025879004919603002]
	TIME [epoch: 7.58 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04077265037154867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04077265037154867 | validation: 0.07169951944347161]
	TIME [epoch: 7.59 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05750726008777843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05750726008777843 | validation: 0.019095888206313628]
	TIME [epoch: 7.59 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026749829182817535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026749829182817535 | validation: 0.02018621839125434]
	TIME [epoch: 7.61 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022574084796763084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022574084796763084 | validation: 0.013745614231702131]
	TIME [epoch: 7.59 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014296928672991762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014296928672991762 | validation: 0.0202775407059296]
	TIME [epoch: 7.59 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015532038929535853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015532038929535853 | validation: 0.016208747542161873]
	TIME [epoch: 7.58 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03397331771877226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03397331771877226 | validation: 0.12879977338949572]
	TIME [epoch: 7.59 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0999989293872169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0999989293872169 | validation: 0.06883318671691972]
	TIME [epoch: 7.62 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07708575949824895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07708575949824895 | validation: 0.04847618867152586]
	TIME [epoch: 7.58 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05205456129556312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05205456129556312 | validation: 0.03908355250399612]
	TIME [epoch: 7.57 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029930800753009693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029930800753009693 | validation: 0.022613728693451553]
	TIME [epoch: 7.58 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03269360928401425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03269360928401425 | validation: 0.06373105814607204]
	TIME [epoch: 7.58 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04721654544228419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04721654544228419 | validation: 0.05581773441521313]
	TIME [epoch: 7.59 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07010192334777465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07010192334777465 | validation: 0.1025331557477089]
	TIME [epoch: 7.6 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0879593226746308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0879593226746308 | validation: 0.030331907543347393]
	TIME [epoch: 7.61 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039094924951378805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039094924951378805 | validation: 0.01678650611060888]
	TIME [epoch: 7.58 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01547181177061337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01547181177061337 | validation: 0.011179929536372714]
	TIME [epoch: 7.57 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009030152842242496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009030152842242496 | validation: 0.014620515696982795]
	TIME [epoch: 7.59 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013116749868149234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013116749868149234 | validation: 0.006258994675863595]
	TIME [epoch: 7.58 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_950.pth
	Model improved!!!
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016350855207203793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016350855207203793 | validation: 0.03802129948082941]
	TIME [epoch: 7.58 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030556964440945193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030556964440945193 | validation: 0.027976647579864778]
	TIME [epoch: 7.57 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06646397891981573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06646397891981573 | validation: 0.15784447125010198]
	TIME [epoch: 7.58 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1312686963087866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1312686963087866 | validation: 0.09680685631006976]
	TIME [epoch: 7.58 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09672602903836705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09672602903836705 | validation: 0.03925076755394389]
	TIME [epoch: 7.57 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044700630076114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044700630076114 | validation: 0.014946195589799939]
	TIME [epoch: 7.57 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02064669439605622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02064669439605622 | validation: 0.01974648178225931]
	TIME [epoch: 7.57 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0206220181291439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0206220181291439 | validation: 0.010303102030240775]
	TIME [epoch: 7.57 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01623058152334651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01623058152334651 | validation: 0.007342795448280226]
	TIME [epoch: 7.59 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01823243147837701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01823243147837701 | validation: 0.041164906005305285]
	TIME [epoch: 7.57 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03640684663727434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03640684663727434 | validation: 0.02798209475744259]
	TIME [epoch: 7.58 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043300039569455676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043300039569455676 | validation: 0.11370806709439162]
	TIME [epoch: 7.58 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09547803191944804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09547803191944804 | validation: 0.054220705931973305]
	TIME [epoch: 7.87 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06342462162282363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06342462162282363 | validation: 0.052395419448687676]
	TIME [epoch: 7.61 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04028074945627174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04028074945627174 | validation: 0.014201624785457723]
	TIME [epoch: 7.58 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018852062449083332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018852062449083332 | validation: 0.024210456761581758]
	TIME [epoch: 7.58 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01864851438464618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01864851438464618 | validation: 0.011726336422317962]
	TIME [epoch: 7.59 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020358532893176122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020358532893176122 | validation: 0.03206812864771417]
	TIME [epoch: 7.59 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025074177179919978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025074177179919978 | validation: 0.026560131664250176]
	TIME [epoch: 7.6 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041775957271978977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041775957271978977 | validation: 0.10210365544153328]
	TIME [epoch: 7.59 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07834271258124818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07834271258124818 | validation: 0.047329748012248575]
	TIME [epoch: 7.58 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04794862212130834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04794862212130834 | validation: 0.049626608322870226]
	TIME [epoch: 7.59 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050728521174072325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050728521174072325 | validation: 0.02010453607683803]
	TIME [epoch: 7.59 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02585572814880145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02585572814880145 | validation: 0.007750672436794049]
	TIME [epoch: 7.6 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013736613027896747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013736613027896747 | validation: 0.014622047849948994]
	TIME [epoch: 7.59 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017301384599105355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017301384599105355 | validation: 0.01885321110295495]
	TIME [epoch: 7.59 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03127230228933482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03127230228933482 | validation: 0.12340284419548575]
	TIME [epoch: 7.59 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09542787346095721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09542787346095721 | validation: 0.06452302566560984]
	TIME [epoch: 7.61 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06905358843300735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06905358843300735 | validation: 0.04380953441149713]
	TIME [epoch: 7.6 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040093459340783205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040093459340783205 | validation: 0.015570605715061237]
	TIME [epoch: 7.58 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029115360796341958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029115360796341958 | validation: 0.06385969124632133]
	TIME [epoch: 7.6 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05983065184886925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05983065184886925 | validation: 0.021917036674696702]
	TIME [epoch: 7.59 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026292493648839538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026292493648839538 | validation: 0.025902842014723915]
	TIME [epoch: 7.61 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02979140054618894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02979140054618894 | validation: 0.021130870887885147]
	TIME [epoch: 7.59 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01682003211237034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01682003211237034 | validation: 0.007108372497726678]
	TIME [epoch: 7.59 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012022823258627809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012022823258627809 | validation: 0.014174415069754599]
	TIME [epoch: 7.59 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01149117348101703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01149117348101703 | validation: 0.005567926091992659]
	TIME [epoch: 7.59 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_987.pth
	Model improved!!!
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015419213459205048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015419213459205048 | validation: 0.07697963241422615]
	TIME [epoch: 7.61 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06583696761546849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06583696761546849 | validation: 0.08858293204361434]
	TIME [epoch: 7.61 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11797890131538186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11797890131538186 | validation: 0.19791620671591767]
	TIME [epoch: 7.62 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17148861361740775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17148861361740775 | validation: 0.05885939995328161]
	TIME [epoch: 7.61 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051895420665866554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051895420665866554 | validation: 0.021239884548866217]
	TIME [epoch: 7.61 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03225585364196083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03225585364196083 | validation: 0.04392966702272569]
	TIME [epoch: 7.61 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03685325239473231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03685325239473231 | validation: 0.018982790711083244]
	TIME [epoch: 7.61 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028947904326860993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028947904326860993 | validation: 0.02253453796050178]
	TIME [epoch: 7.6 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022402350089113475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022402350089113475 | validation: 0.00872646342189296]
	TIME [epoch: 7.61 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014176954080050548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014176954080050548 | validation: 0.009690978721403554]
	TIME [epoch: 7.61 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013613167728874156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013613167728874156 | validation: 0.006831951329528319]
	TIME [epoch: 7.61 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016796334742811836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016796334742811836 | validation: 0.017795172375396472]
	TIME [epoch: 7.6 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02115857451161281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02115857451161281 | validation: 0.01928789024954213]
	TIME [epoch: 7.57 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040936746700265064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040936746700265064 | validation: 0.1194945296926287]
	TIME [epoch: 42.8 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09002151612741376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09002151612741376 | validation: 0.07645643469249637]
	TIME [epoch: 16.4 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0803637381392478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0803637381392478 | validation: 0.06014782649824701]
	TIME [epoch: 16.4 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06484556545518917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06484556545518917 | validation: 0.01942950759668385]
	TIME [epoch: 16.3 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0237371394663804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0237371394663804 | validation: 0.025559486252092548]
	TIME [epoch: 16.3 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022960401891591306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022960401891591306 | validation: 0.011422252846869775]
	TIME [epoch: 16.4 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023734815305367033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023734815305367033 | validation: 0.06358394694791597]
	TIME [epoch: 16.3 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0645575645584495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0645575645584495 | validation: 0.05732598515358939]
	TIME [epoch: 16.4 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07228313337226445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07228313337226445 | validation: 0.0665112781205101]
	TIME [epoch: 16.3 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06348787582552562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06348787582552562 | validation: 0.022893022035236323]
	TIME [epoch: 16.4 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023535416563536832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023535416563536832 | validation: 0.012270108808517489]
	TIME [epoch: 16.4 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01306876999136366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01306876999136366 | validation: 0.01451445287040587]
	TIME [epoch: 16.4 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012709831726852685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012709831726852685 | validation: 0.01999073302685496]
	TIME [epoch: 16.3 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024564366143085384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024564366143085384 | validation: 0.019983765026811262]
	TIME [epoch: 16.3 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03983134963912706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03983134963912706 | validation: 0.0779144567234284]
	TIME [epoch: 16.4 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0653442016970944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0653442016970944 | validation: 0.04456626493700562]
	TIME [epoch: 16.4 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05166976521328854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05166976521328854 | validation: 0.04316509533354645]
	TIME [epoch: 16.4 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04335068727187147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04335068727187147 | validation: 0.025348071755039482]
	TIME [epoch: 16.3 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025365263770234674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025365263770234674 | validation: 0.04084851995981729]
	TIME [epoch: 16.3 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03657319508777519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03657319508777519 | validation: 0.013531421291918056]
	TIME [epoch: 16.3 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017626944567608948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017626944567608948 | validation: 0.025036605241598488]
	TIME [epoch: 16.3 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021727527377527638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021727527377527638 | validation: 0.01893595161756059]
	TIME [epoch: 16.3 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03706391224164311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03706391224164311 | validation: 0.095821336993285]
	TIME [epoch: 16.3 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07934993588528792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07934993588528792 | validation: 0.046208128431399535]
	TIME [epoch: 16.3 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047556179656010736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047556179656010736 | validation: 0.038180195751911096]
	TIME [epoch: 16.4 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041025224098834995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041025224098834995 | validation: 0.011819724518251574]
	TIME [epoch: 16.3 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020889317918406206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020889317918406206 | validation: 0.01548961639652523]
	TIME [epoch: 16.3 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01780231484803662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01780231484803662 | validation: 0.019191495742380296]
	TIME [epoch: 16.3 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02092001014058924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02092001014058924 | validation: 0.03456859057455604]
	TIME [epoch: 16.3 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03497634463507993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03497634463507993 | validation: 0.03607981737728815]
	TIME [epoch: 16.3 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05618575414774947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05618575414774947 | validation: 0.13316862040046779]
	TIME [epoch: 16.3 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10068190436245388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10068190436245388 | validation: 0.07185322176338446]
	TIME [epoch: 16.3 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07537813352373382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07537813352373382 | validation: 0.03869945946514657]
	TIME [epoch: 16.3 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04678318664433086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04678318664433086 | validation: 0.015562652754403872]
	TIME [epoch: 16.3 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016320794394528826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016320794394528826 | validation: 0.007522606122936305]
	TIME [epoch: 16.3 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01691800516160546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01691800516160546 | validation: 0.02097324299865886]
	TIME [epoch: 16.3 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017474980164981613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017474980164981613 | validation: 0.017315032179508385]
	TIME [epoch: 16.3 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040048257168934155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040048257168934155 | validation: 0.10033387452403003]
	TIME [epoch: 16.3 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0908311745133683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0908311745133683 | validation: 0.04910368386594086]
	TIME [epoch: 16.3 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05557791804910516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05557791804910516 | validation: 0.022338561981514918]
	TIME [epoch: 16.3 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03826544468386211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03826544468386211 | validation: 0.025256065843014965]
	TIME [epoch: 16.3 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023483199948433234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023483199948433234 | validation: 0.025725783864177065]
	TIME [epoch: 16.3 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03636619936529632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03636619936529632 | validation: 0.05034010026301328]
	TIME [epoch: 16.3 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04598384084496096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04598384084496096 | validation: 0.043668234238413185]
	TIME [epoch: 16.3 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06638905739587393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06638905739587393 | validation: 0.10264176326932649]
	TIME [epoch: 16.3 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08603796559725313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08603796559725313 | validation: 0.03242435210014253]
	TIME [epoch: 16.3 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037518228946755085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037518228946755085 | validation: 0.019536985555893463]
	TIME [epoch: 16.3 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03446071029900195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03446071029900195 | validation: 0.03571917325992768]
	TIME [epoch: 16.3 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02798613054341879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02798613054341879 | validation: 0.023798509674095428]
	TIME [epoch: 16.3 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03667557992531587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03667557992531587 | validation: 0.03461727544536415]
	TIME [epoch: 16.3 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03511137496341187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03511137496341187 | validation: 0.023230890930312765]
	TIME [epoch: 16.3 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04556623573891841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04556623573891841 | validation: 0.05206789967775293]
	TIME [epoch: 16.4 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043544615362167756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043544615362167756 | validation: 0.01892538938819005]
	TIME [epoch: 16.3 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02320553732991762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02320553732991762 | validation: 0.010721843178599767]
	TIME [epoch: 16.3 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01744368181704574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01744368181704574 | validation: 0.013426123150728497]
	TIME [epoch: 16.3 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015112473247865776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015112473247865776 | validation: 0.008944281028043633]
	TIME [epoch: 16.3 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016790609807777794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016790609807777794 | validation: 0.019272087575319886]
	TIME [epoch: 16.3 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018422632795420273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018422632795420273 | validation: 0.01885532128492423]
	TIME [epoch: 16.3 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037047682113630284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037047682113630284 | validation: 0.09817368833819334]
	TIME [epoch: 16.3 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08351195944710961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08351195944710961 | validation: 0.08875357636924408]
	TIME [epoch: 16.3 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10354283445825015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10354283445825015 | validation: 0.05966325039016312]
	TIME [epoch: 16.3 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055632890286772714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055632890286772714 | validation: 0.010023360039537743]
	TIME [epoch: 16.4 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013962726526756106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013962726526756106 | validation: 0.007799148082226293]
	TIME [epoch: 16.3 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014688295824830168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014688295824830168 | validation: 0.009699503205999084]
	TIME [epoch: 16.3 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015089794491699005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015089794491699005 | validation: 0.009978923787532668]
	TIME [epoch: 16.4 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012692945849485998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012692945849485998 | validation: 0.008490506379912622]
	TIME [epoch: 16.3 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011329823614424557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011329823614424557 | validation: 0.010183071956870926]
	TIME [epoch: 16.4 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017972898513378025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017972898513378025 | validation: 0.04933595478586165]
	TIME [epoch: 16.4 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045827099138493595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045827099138493595 | validation: 0.03604682602223708]
	TIME [epoch: 16.4 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05585767064924668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05585767064924668 | validation: 0.10688801965148481]
	TIME [epoch: 16.4 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09569789677073529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09569789677073529 | validation: 0.04008729228911692]
	TIME [epoch: 16.4 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050778759214494845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050778759214494845 | validation: 0.018420238332911645]
	TIME [epoch: 16.3 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0200698727363583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0200698727363583 | validation: 0.004639695362692364]
	TIME [epoch: 16.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_1073.pth
	Model improved!!!
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01534667768531892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01534667768531892 | validation: 0.014388586646280222]
	TIME [epoch: 16.3 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012539756778993102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012539756778993102 | validation: 0.006978215383742082]
	TIME [epoch: 16.3 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012963922280319316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012963922280319316 | validation: 0.02181052481938899]
	TIME [epoch: 16.3 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018843543679506487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018843543679506487 | validation: 0.015123841557142715]
	TIME [epoch: 16.4 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019521103808278595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019521103808278595 | validation: 0.025812876397478937]
	TIME [epoch: 16.3 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0184011292819459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0184011292819459 | validation: 0.025217783885099633]
	TIME [epoch: 16.4 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036048839432142106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036048839432142106 | validation: 0.12856797986920623]
	TIME [epoch: 16.3 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11127356343156891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11127356343156891 | validation: 0.08174921694136987]
	TIME [epoch: 16.3 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08758370408296806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08758370408296806 | validation: 0.029294941131954546]
	TIME [epoch: 16.3 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03204377708997246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03204377708997246 | validation: 0.004895806523383218]
	TIME [epoch: 16.4 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011096383969687587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011096383969687587 | validation: 0.013465212602117317]
	TIME [epoch: 16.3 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023914351344665023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023914351344665023 | validation: 0.033908680546971375]
	TIME [epoch: 16.3 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0346920920221649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0346920920221649 | validation: 0.03053400242405102]
	TIME [epoch: 16.3 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0665906567815015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0665906567815015 | validation: 0.147846791316843]
	TIME [epoch: 16.3 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10450892411440403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10450892411440403 | validation: 0.08587997090146916]
	TIME [epoch: 16.3 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0726678392015519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0726678392015519 | validation: 0.02301965473301354]
	TIME [epoch: 16.3 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023405903199791595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023405903199791595 | validation: 0.023006818761176207]
	TIME [epoch: 16.3 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04202649941575921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04202649941575921 | validation: 0.015296719277769412]
	TIME [epoch: 16.3 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01808002169481559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01808002169481559 | validation: 0.010838220082113903]
	TIME [epoch: 16.3 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018957431003205044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018957431003205044 | validation: 0.0164821765824976]
	TIME [epoch: 16.3 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022982321762126317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022982321762126317 | validation: 0.009127371464697566]
	TIME [epoch: 16.3 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01725211146581517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01725211146581517 | validation: 0.012911528991691635]
	TIME [epoch: 16.3 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018933966266950732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018933966266950732 | validation: 0.021588372252057366]
	TIME [epoch: 16.3 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033240229336249316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033240229336249316 | validation: 0.09490776913328179]
	TIME [epoch: 16.3 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09556432334469842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09556432334469842 | validation: 0.08456645432282679]
	TIME [epoch: 16.3 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08505549152511875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08505549152511875 | validation: 0.03560879985170986]
	TIME [epoch: 16.3 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03933457041390076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03933457041390076 | validation: 0.009125437568554306]
	TIME [epoch: 16.3 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019891019312740833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019891019312740833 | validation: 0.023308523858038045]
	TIME [epoch: 16.3 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031235658414625337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031235658414625337 | validation: 0.005678941126931969]
	TIME [epoch: 16.3 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01433117534656237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01433117534656237 | validation: 0.013227612904180031]
	TIME [epoch: 16.3 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01334569682900296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01334569682900296 | validation: 0.0035976499238963623]
	TIME [epoch: 16.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_1104.pth
	Model improved!!!
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012021853767187882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012021853767187882 | validation: 0.025138504032261557]
	TIME [epoch: 16.3 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021848524400943706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021848524400943706 | validation: 0.031115516390107924]
	TIME [epoch: 16.3 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04500557283446787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04500557283446787 | validation: 0.1153077902732974]
	TIME [epoch: 16.3 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10043144473992238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10043144473992238 | validation: 0.05896517368389749]
	TIME [epoch: 16.3 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06410158404952176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06410158404952176 | validation: 0.02194386395499194]
	TIME [epoch: 16.4 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02157617316312293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02157617316312293 | validation: 0.013240213540985368]
	TIME [epoch: 16.4 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012520265554244995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012520265554244995 | validation: 0.009536139877775697]
	TIME [epoch: 16.3 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016140912632900673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016140912632900673 | validation: 0.02060210095961709]
	TIME [epoch: 16.4 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015679913833938013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015679913833938013 | validation: 0.010067780167792407]
	TIME [epoch: 16.3 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015476780226415845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015476780226415845 | validation: 0.020586501442675542]
	TIME [epoch: 16.3 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0218308770385092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0218308770385092 | validation: 0.024055868211590538]
	TIME [epoch: 16.4 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05471547329462439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05471547329462439 | validation: 0.15045357543258175]
	TIME [epoch: 16.3 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11571716600651191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11571716600651191 | validation: 0.09914322726005494]
	TIME [epoch: 16.3 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10068626200126037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10068626200126037 | validation: 0.030735742856989434]
	TIME [epoch: 16.4 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031906750419905955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031906750419905955 | validation: 0.025305053843093783]
	TIME [epoch: 16.3 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036899159742419134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036899159742419134 | validation: 0.03599756273809477]
	TIME [epoch: 16.3 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032357711394804416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032357711394804416 | validation: 0.011654678369381201]
	TIME [epoch: 16.3 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01923461982160055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01923461982160055 | validation: 0.023841736689893103]
	TIME [epoch: 16.3 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02455614940373732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02455614940373732 | validation: 0.024668269897140785]
	TIME [epoch: 16.4 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04004227678280488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04004227678280488 | validation: 0.07118357765488247]
	TIME [epoch: 16.3 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06921118960436651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06921118960436651 | validation: 0.0422024180486163]
	TIME [epoch: 16.3 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045049915835279224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045049915835279224 | validation: 0.025637782062417472]
	TIME [epoch: 16.3 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0298671317102326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0298671317102326 | validation: 0.006812306389026668]
	TIME [epoch: 16.4 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01130693690195227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01130693690195227 | validation: 0.00931344778703146]
	TIME [epoch: 16.4 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01133964505913703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01133964505913703 | validation: 0.009747864713005417]
	TIME [epoch: 16.3 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009332173382441777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009332173382441777 | validation: 0.005036468110286885]
	TIME [epoch: 16.3 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009426915873749591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009426915873749591 | validation: 0.02435165920272188]
	TIME [epoch: 16.4 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026960135199125847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026960135199125847 | validation: 0.032951169320526996]
	TIME [epoch: 16.3 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05800002820359632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05800002820359632 | validation: 0.1347868858771516]
	TIME [epoch: 16.5 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12026582093945085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12026582093945085 | validation: 0.06630442898794574]
	TIME [epoch: 16.3 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07629121124871181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07629121124871181 | validation: 0.023058930719338303]
	TIME [epoch: 21.1 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027523420725435753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027523420725435753 | validation: 0.013871131215961098]
	TIME [epoch: 16.3 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015663487498330762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015663487498330762 | validation: 0.009567323446021514]
	TIME [epoch: 16.3 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017651296630410852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017651296630410852 | validation: 0.044446472368866036]
	TIME [epoch: 16.4 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03705265331650664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03705265331650664 | validation: 0.043354843123087086]
	TIME [epoch: 16.4 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06627901654339441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06627901654339441 | validation: 0.06415519716897894]
	TIME [epoch: 25 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061526115867550725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061526115867550725 | validation: 0.021841655169947606]
	TIME [epoch: 16.4 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025293752560431315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025293752560431315 | validation: 0.00858942320785543]
	TIME [epoch: 16.4 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010135403756033057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010135403756033057 | validation: 0.009536667996808357]
	TIME [epoch: 16.3 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011654238826053804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011654238826053804 | validation: 0.01099088983462685]
	TIME [epoch: 16.3 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021640982694512846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021640982694512846 | validation: 0.03427598551798452]
	TIME [epoch: 17 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03291441536746971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03291441536746971 | validation: 0.038622475402462435]
	TIME [epoch: 16.4 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05460179259473673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05460179259473673 | validation: 0.0798892490173804]
	TIME [epoch: 16.4 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07054722660925795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07054722660925795 | validation: 0.027608252663108013]
	TIME [epoch: 16.4 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034357920956674344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034357920956674344 | validation: 0.01913431638161063]
	TIME [epoch: 16.3 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028889313432521355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028889313432521355 | validation: 0.016577081344588164]
	TIME [epoch: 22.9 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017940687333174016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017940687333174016 | validation: 0.014283593108531245]
	TIME [epoch: 16.3 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02331581352027267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02331581352027267 | validation: 0.015293539835801374]
	TIME [epoch: 16.3 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01852467133152369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01852467133152369 | validation: 0.010859524286456312]
	TIME [epoch: 16.9 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012290126327591792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012290126327591792 | validation: 0.01448657915135334]
	TIME [epoch: 16.4 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01897847038310472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01897847038310472 | validation: 0.02689573778194956]
	TIME [epoch: 16.4 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036573023489829895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036573023489829895 | validation: 0.08574888789378893]
	TIME [epoch: 16.4 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07707416736225754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07707416736225754 | validation: 0.05299956398130754]
	TIME [epoch: 16.4 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06799193698636066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06799193698636066 | validation: 0.0473062368602347]
	TIME [epoch: 24.8 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04697332682116995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04697332682116995 | validation: 0.016700896568706793]
	TIME [epoch: 16.4 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03236550708267976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03236550708267976 | validation: 0.016097200264445177]
	TIME [epoch: 16.3 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020045233480316206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020045233480316206 | validation: 0.002430201221636297]
	TIME [epoch: 16.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_1161.pth
	Model improved!!!
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008161421576563629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.008161421576563629 | validation: 0.0037109654804463824]
	TIME [epoch: 16.3 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006908513396791319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.006908513396791319 | validation: 0.003378352378395677]
	TIME [epoch: 16.3 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009095692255687517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009095692255687517 | validation: 0.012775352671746754]
	TIME [epoch: 16.3 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01064670554112692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01064670554112692 | validation: 0.010652207522415925]
	TIME [epoch: 24.8 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022694346931002612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022694346931002612 | validation: 0.07443037725120791]
	TIME [epoch: 16.3 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06872595340578942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06872595340578942 | validation: 0.09280805306499333]
	TIME [epoch: 16.3 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1202222586506334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1202222586506334 | validation: 0.0975765164126371]
	TIME [epoch: 16.3 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10484495651015519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10484495651015519 | validation: 0.03407112205856636]
	TIME [epoch: 16.3 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03788574507709426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03788574507709426 | validation: 0.016877509941113523]
	TIME [epoch: 16.4 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020469635769338072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020469635769338072 | validation: 0.016402346779166722]
	TIME [epoch: 16.3 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021413318334504768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021413318334504768 | validation: 0.00974478077593336]
	TIME [epoch: 22.7 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014358476114498163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014358476114498163 | validation: 0.005162435226330253]
	TIME [epoch: 16.6 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008679664074314482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.008679664074314482 | validation: 0.004610118789148798]
	TIME [epoch: 16.3 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009286283744085473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009286283744085473 | validation: 0.0012597209913135067]
	TIME [epoch: 16.3 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_1175.pth
	Model improved!!!
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007324368603356222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.007324368603356222 | validation: 0.005045662979771193]
	TIME [epoch: 16.5 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008168744026181124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.008168744026181124 | validation: 0.007195535961580757]
	TIME [epoch: 16.3 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012491075840478719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012491075840478719 | validation: 0.015213805344596732]
	TIME [epoch: 16.2 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026686792646132048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026686792646132048 | validation: 0.059016197599835]
	TIME [epoch: 16.3 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06072054928863424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06072054928863424 | validation: 0.06114316335725742]
	TIME [epoch: 16.2 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06781777929128645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06781777929128645 | validation: 0.07923598178796412]
	TIME [epoch: 16.2 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07936030894612194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07936030894612194 | validation: 0.03492193522590742]
	TIME [epoch: 16.2 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049382105715047406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049382105715047406 | validation: 0.017588731348161146]
	TIME [epoch: 16.3 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024523654511871777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024523654511871777 | validation: 0.005237733781461429]
	TIME [epoch: 16.2 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012850715407922446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012850715407922446 | validation: 0.009909245571588388]
	TIME [epoch: 16.2 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015757918608290913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015757918608290913 | validation: 0.003887773173619269]
	TIME [epoch: 16.2 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0160836541753348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0160836541753348 | validation: 0.012493731438169287]
	TIME [epoch: 16.2 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015667939846453066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015667939846453066 | validation: 0.010619623087265363]
	TIME [epoch: 16.2 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0243194059838319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0243194059838319 | validation: 0.07983017359023391]
	TIME [epoch: 16.2 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0701685545962248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0701685545962248 | validation: 0.05110396295631358]
	TIME [epoch: 16.2 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05455734059421207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05455734059421207 | validation: 0.0426961527926407]
	TIME [epoch: 16.2 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0421789791790704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0421789791790704 | validation: 0.00850040779816047]
	TIME [epoch: 16.2 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010421539749763063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010421539749763063 | validation: 0.007627910359143508]
	TIME [epoch: 16.3 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009490399021380654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009490399021380654 | validation: 0.003746513865118739]
	TIME [epoch: 16.3 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010854592661701105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010854592661701105 | validation: 0.01045924616068298]
	TIME [epoch: 16.2 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015174787714479932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015174787714479932 | validation: 0.014219192294021866]
	TIME [epoch: 16.2 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02562836012209178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02562836012209178 | validation: 0.08809229436371731]
	TIME [epoch: 16.3 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08101100508661782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08101100508661782 | validation: 0.06095372884486028]
	TIME [epoch: 16.3 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06715664857352986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06715664857352986 | validation: 0.05172503821147767]
	TIME [epoch: 16.2 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05367852597073146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05367852597073146 | validation: 0.02398079951016037]
	TIME [epoch: 16.3 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02660984253699223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02660984253699223 | validation: 0.021568365978662674]
	TIME [epoch: 16.3 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022816497852676306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022816497852676306 | validation: 0.0091347902134167]
	TIME [epoch: 16.3 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012526906780765148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012526906780765148 | validation: 0.01128730596135722]
	TIME [epoch: 16.2 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016264825508448308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016264825508448308 | validation: 0.01853923879228634]
	TIME [epoch: 16.3 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012542381125890185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012542381125890185 | validation: 0.004519052133855406]
	TIME [epoch: 16.2 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008639655941635615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.008639655941635615 | validation: 0.00604555637081975]
	TIME [epoch: 16.3 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0060371338173959965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0060371338173959965 | validation: 0.0029314551634925246]
	TIME [epoch: 16.3 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007978274060178824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.007978274060178824 | validation: 0.02401982869125101]
	TIME [epoch: 16.3 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029792629160622485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029792629160622485 | validation: 0.0788493253290716]
	TIME [epoch: 16.3 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11553058317190144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11553058317190144 | validation: 0.1785404180087287]
	TIME [epoch: 16.2 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15267489374730722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15267489374730722 | validation: 0.06226599151096477]
	TIME [epoch: 16.2 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05992342091595447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05992342091595447 | validation: 0.021580037157377854]
	TIME [epoch: 16.3 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02442312703649219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02442312703649219 | validation: 0.03609121314863696]
	TIME [epoch: 16.3 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03888672934747644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03888672934747644 | validation: 0.012169324269587346]
	TIME [epoch: 16.3 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016785178901131122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016785178901131122 | validation: 0.00810754699364874]
	TIME [epoch: 16.2 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015153906549377523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015153906549377523 | validation: 0.014339841025360713]
	TIME [epoch: 16.3 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015732289175602907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015732289175602907 | validation: 0.00761889405740146]
	TIME [epoch: 16.2 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01734978257390995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01734978257390995 | validation: 0.02957546709743586]
	TIME [epoch: 16.2 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031215109502773484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031215109502773484 | validation: 0.031795547885537845]
	TIME [epoch: 16.3 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060745887932170374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060745887932170374 | validation: 0.09997282751386541]
	TIME [epoch: 16.2 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08690752936182922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08690752936182922 | validation: 0.03419346611168067]
	TIME [epoch: 16.3 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04525205573440892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04525205573440892 | validation: 0.016653193382915887]
	TIME [epoch: 16.2 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020726563496414736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020726563496414736 | validation: 0.014967183471901124]
	TIME [epoch: 16.2 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016479693199826433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016479693199826433 | validation: 0.005030324622106919]
	TIME [epoch: 16.3 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012723512349772417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012723512349772417 | validation: 0.008216127980453914]
	TIME [epoch: 16.2 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010159332072495314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010159332072495314 | validation: 0.006660886625267659]
	TIME [epoch: 16.3 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015881892581978328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015881892581978328 | validation: 0.030667712539430705]
	TIME [epoch: 16.2 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03211170777745338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03211170777745338 | validation: 0.03185480038642776]
	TIME [epoch: 16.3 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04410977531997864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04410977531997864 | validation: 0.07904023360530518]
	TIME [epoch: 16.3 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07800600852541925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07800600852541925 | validation: 0.036036006036235545]
	TIME [epoch: 16.3 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04380986111619961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04380986111619961 | validation: 0.016630019057227163]
	TIME [epoch: 16.3 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016451666346465915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016451666346465915 | validation: 0.008549896416367086]
	TIME [epoch: 16.3 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011251056253843754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011251056253843754 | validation: 0.01044546131675559]
	TIME [epoch: 16.2 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017471635000761196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017471635000761196 | validation: 0.011153640080550165]
	TIME [epoch: 16.3 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018403927187893087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018403927187893087 | validation: 0.012984882018447842]
	TIME [epoch: 16.3 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018355609151656908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018355609151656908 | validation: 0.01915511239688296]
	TIME [epoch: 16.3 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0326528038006463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0326528038006463 | validation: 0.05844559035984436]
	TIME [epoch: 16.3 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05161774211083388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05161774211083388 | validation: 0.03959115358053435]
	TIME [epoch: 16.2 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043952757860881764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043952757860881764 | validation: 0.04509005727961183]
	TIME [epoch: 16.3 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04820811553604571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04820811553604571 | validation: 0.015773685058052156]
	TIME [epoch: 16.3 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02393570972302197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02393570972302197 | validation: 0.019049741426455637]
	TIME [epoch: 16.3 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018499728732928038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018499728732928038 | validation: 0.008473577555013335]
	TIME [epoch: 16.2 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02524572224100771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02524572224100771 | validation: 0.044347454577166795]
	TIME [epoch: 16.3 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048377005340518704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048377005340518704 | validation: 0.032900451786856764]
	TIME [epoch: 16.3 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04433660935154725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04433660935154725 | validation: 0.039609937223070206]
	TIME [epoch: 16.2 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038998176228045515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038998176228045515 | validation: 0.026046063673026067]
	TIME [epoch: 16.3 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038028942193447546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038028942193447546 | validation: 0.031103201825739925]
	TIME [epoch: 16.3 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033894501747489265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033894501747489265 | validation: 0.00902240161992962]
	TIME [epoch: 16.3 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012423059070873807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012423059070873807 | validation: 0.009549227194976162]
	TIME [epoch: 16.3 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013831916552540065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013831916552540065 | validation: 0.01786458146704456]
	TIME [epoch: 16.3 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017079292612457032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017079292612457032 | validation: 0.03446198936684282]
	TIME [epoch: 16.3 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03405784984428371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03405784984428371 | validation: 0.026462038266841625]
	TIME [epoch: 16.3 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049255973922429065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049255973922429065 | validation: 0.06795451903609356]
	TIME [epoch: 16.3 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06468564282381598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06468564282381598 | validation: 0.0443223771891679]
	TIME [epoch: 16.3 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0522470959854356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0522470959854356 | validation: 0.042428482605147234]
	TIME [epoch: 16.3 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03707440173525826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03707440173525826 | validation: 0.0068546564514757385]
	TIME [epoch: 16.3 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013330615854520382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013330615854520382 | validation: 0.004049079497742025]
	TIME [epoch: 16.2 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010074593066108656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010074593066108656 | validation: 0.00577850157306451]
	TIME [epoch: 16.2 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007114342350655936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.007114342350655936 | validation: 0.0036688015406108666]
	TIME [epoch: 16.2 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008349957625319418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.008349957625319418 | validation: 0.026360133276578247]
	TIME [epoch: 16.2 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0322573684873087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0322573684873087 | validation: 0.05145895213788093]
	TIME [epoch: 16.2 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08701645971760705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08701645971760705 | validation: 0.1025512336082838]
	TIME [epoch: 16.2 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09579918476408768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09579918476408768 | validation: 0.029505071482568146]
	TIME [epoch: 16.2 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036128120471838165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036128120471838165 | validation: 0.008275248623352471]
	TIME [epoch: 16.2 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022425824844539203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022425824844539203 | validation: 0.024058920674551842]
	TIME [epoch: 16.3 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033142563771832945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033142563771832945 | validation: 0.012020405275667313]
	TIME [epoch: 16.2 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02175575600256233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02175575600256233 | validation: 0.018373807885130735]
	TIME [epoch: 16.2 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018983192301349743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018983192301349743 | validation: 0.014285342578522143]
	TIME [epoch: 16.3 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017732981173633953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017732981173633953 | validation: 0.007605222939390999]
	TIME [epoch: 16.3 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01620375593029318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01620375593029318 | validation: 0.010298255391099421]
	TIME [epoch: 16.3 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013241565408446704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013241565408446704 | validation: 0.013417952371127508]
	TIME [epoch: 16.3 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019855434301182244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019855434301182244 | validation: 0.02660405846460482]
	TIME [epoch: 16.3 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039450577263053156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039450577263053156 | validation: 0.07384442733688887]
	TIME [epoch: 16.3 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07349759110179895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07349759110179895 | validation: 0.041338393588513093]
	TIME [epoch: 16.3 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045218869472004625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045218869472004625 | validation: 0.029170346422918925]
	TIME [epoch: 16.3 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022145291492405024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022145291492405024 | validation: 0.00724301181340542]
	TIME [epoch: 16.3 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_153440/states/model_phi1_3b_v_mmd1_1276.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 10019.368 seconds.
