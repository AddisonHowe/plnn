Args:
Args:
Namespace(name='model_phi1_4a_v_mmd1', outdir='out/model_training/model_phi1_4a_v_mmd1', training_data='data/training_data/data_phi1_4a/training', validation_data='data/training_data/data_phi1_4a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Namespace(name='model_phi1_4a_v_mmd1', outdir='out/model_training/model_phi1_4a_v_mmd1', training_data='data/training_data/data_phi1_4a/training', validation_data='data/training_data/data_phi1_4a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 670988347
Using seed: 3240319229

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_0.pth

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_0.pth
EPOCH 1/2000:
EPOCH 1/2000:
	Training over batches...
	Training over batches...
		[batch 1/1] avg loss: 5.0186475999693885		[learning rate: 0.01]
		[batch 1/1] avg loss: 5.182213072738375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.0186475999693885 | validation: 5.917678865507849]
	TIME [epoch: 45.3 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 5.182213072738375 | validation: 5.58229205284101]
	TIME [epoch: 46.8 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_1.pth
		[batch 1/1] avg loss: 4.33954204338142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.33954204338142 | validation: 6.116547390751329]
	TIME [epoch: 0.942 sec]
EPOCH 3/2000:
	Training over batches...
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.663647030952607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.663647030952607 | validation: 5.955421643571528]
	TIME [epoch: 0.926 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.6160730097134115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6160730097134115 | validation: 6.269675662300869]
	TIME [epoch: 0.953 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.312536309020599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.312536309020599 | validation: 5.73184227276715]
	TIME [epoch: 0.93 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_4.pth
		[batch 1/1] avg loss: 4.655011915936362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.655011915936362 | validation: 5.966068128786608]
	TIME [epoch: 0.938 sec]
EPOCH 4/2000:
	Training over batches...
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.301351294378271		[learning rate: 0.01]
		[batch 1/1] avg loss: 3.9765412927010617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.301351294378271 | validation: 5.7715140181303815]
	TIME [epoch: 0.94 sec]
EPOCH 5/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 3.9765412927010617 | validation: 5.649737791142032]
	TIME [epoch: 0.926 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.102464973921779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.102464973921779 | validation: 5.905562833764225]
	TIME [epoch: 0.938 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9113351588911303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9113351588911303 | validation: 5.563903015775942]
	TIME [epoch: 0.922 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_6.pth
		[batch 1/1] avg loss: 4.028955905072486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.028955905072486 | validation: 5.879643699170107]
	TIME [epoch: 0.94 sec]
EPOCH 7/2000:
	Training over batches...
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.897027877906603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.897027877906603 | validation: 5.803434511181912]
	TIME [epoch: 0.937 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7942374920407893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7942374920407893 | validation: 5.4818757701930085]
	TIME [epoch: 0.927 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_7.pth
		[batch 1/1] avg loss: 3.8834518119292594		[learning rate: 0.01]
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 3.8834518119292594 | validation: 5.7846996760949345]
	TIME [epoch: 0.94 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6322919329928927		[learning rate: 0.01]
		[batch 1/1] avg loss: 3.8055435283586525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6322919329928927 | validation: 5.4030456053479625]
	TIME [epoch: 0.926 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_8.pth
	Learning Rate: 0.01
	LOSS [training: 3.8055435283586525 | validation: 5.7192505153271656]
	TIME [epoch: 0.939 sec]
EPOCH 10/2000:
	Training over batches...
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7642105734437443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7642105734437443 | validation: 5.676571315711932]
	TIME [epoch: 0.932 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.485232739461154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.485232739461154 | validation: 5.219812529027158]
	TIME [epoch: 0.93 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_9.pth
		[batch 1/1] avg loss: 3.7161635968492694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7161635968492694 | validation: 5.659039422771679]
	TIME [epoch: 0.938 sec]
EPOCH 12/2000:
	Training over batches...
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.681374765014581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.681374765014581 | validation: 5.5355641501405]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_12.pth
		[batch 1/1] avg loss: 3.3052378769181883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3052378769181883 | validation: 4.958505741246826]
	TIME [epoch: 0.929 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_10.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.638899767099913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.638899767099913 | validation: 5.565547372822514]
	TIME [epoch: 0.936 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.110602053499895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.110602053499895 | validation: 4.1559593407801065]
	TIME [epoch: 0.931 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_11.pth
		[batch 1/1] avg loss: 3.6049838992290346		[learning rate: 0.01]
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 3.6049838992290346 | validation: 5.230946384513793]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8157722315749845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8157722315749845 | validation: 4.885863172833453]
	TIME [epoch: 0.93 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5754594333606042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5754594333606042 | validation: 5.408382405173139]
	TIME [epoch: 0.94 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.658570591706701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.658570591706701 | validation: 2.0289293736145098]
	TIME [epoch: 0.926 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_13.pth
		[batch 1/1] avg loss: 3.5274191112316196		[learning rate: 0.01]
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 3.5274191112316196 | validation: 4.588174685862801]
	TIME [epoch: 0.94 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1094960164981296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1094960164981296 | validation: 2.3641040662557975]
	TIME [epoch: 0.929 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.543324373474661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.543324373474661 | validation: 4.573386084366844]
	TIME [epoch: 0.932 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_17.pth
		[batch 1/1] avg loss: 2.128248663520477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.128248663520477 | validation: 3.2354935493575243]
	TIME [epoch: 0.927 sec]
EPOCH 16/2000:
	Training over batches...
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4733020468585427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4733020468585427 | validation: 2.1882858365980105]
	TIME [epoch: 0.928 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2210695628981605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2210695628981605 | validation: 4.947390541633202]
	TIME [epoch: 0.933 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6705361903367617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6705361903367617 | validation: 1.5081432195918696]
	TIME [epoch: 0.926 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_17.pth
		[batch 1/1] avg loss: 3.31789631872227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.31789631872227 | validation: 3.993398137266222]
	TIME [epoch: 0.931 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_19.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7539933794081928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7539933794081928 | validation: 1.8409528764273717]
	TIME [epoch: 0.927 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.22888835459392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.22888835459392 | validation: 3.665134151845135]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_20.pth
		[batch 1/1] avg loss: 1.6498276846252895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6498276846252895 | validation: 1.9867077525610082]
	TIME [epoch: 0.931 sec]
EPOCH 20/2000:
	Training over batches...
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7831195642299502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7831195642299502 | validation: 1.8097015304686463]
	TIME [epoch: 0.939 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0111052470851587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0111052470851587 | validation: 3.680983089123396]
	TIME [epoch: 0.937 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4949228454976389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4949228454976389 | validation: 1.6061899284413308]
	TIME [epoch: 0.924 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.760866700305485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.760866700305485 | validation: 3.5867924977695766]
	TIME [epoch: 0.933 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_22.pth
		[batch 1/1] avg loss: 1.4244154950844548		[learning rate: 0.01]
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 1.4244154950844548 | validation: 1.5524494092827572]
	TIME [epoch: 0.924 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.577085784932842		[learning rate: 0.01]
		[batch 1/1] avg loss: 1.4543388067875878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.577085784932842 | validation: 2.5407784628043313]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_23.pth
	Learning Rate: 0.01
	LOSS [training: 1.4543388067875878 | validation: 1.6123392079349814]
	TIME [epoch: 0.925 sec]
EPOCH 24/2000:
	Training over batches...
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4485546013403734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4485546013403734 | validation: 1.556790348324744]
	TIME [epoch: 0.926 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.307651418559408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.307651418559408 | validation: 1.9406607525340462]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_24.pth
		[batch 1/1] avg loss: 1.4245296676459436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4245296676459436 | validation: 1.5171479604151483]
	TIME [epoch: 0.927 sec]
EPOCH 26/2000:
	Training over batches...
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3617104527329158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3617104527329158 | validation: 1.3652102944942837]
	TIME [epoch: 0.926 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_26.pth
		[batch 1/1] avg loss: 1.703646392087098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.703646392087098 | validation: 1.6783964474568194]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_25.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3184556308777804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3184556308777804 | validation: 1.3391212576478375]
	TIME [epoch: 0.928 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_27.pth
		[batch 1/1] avg loss: 2.8570960107041845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8570960107041845 | validation: 3.0969256767712716]
	TIME [epoch: 0.937 sec]
EPOCH 27/2000:
	Training over batches...
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.485088882673937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.485088882673937 | validation: 2.4665791489992572]
	TIME [epoch: 0.956 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2912606712022183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2912606712022183 | validation: 1.2731156887623343]
	TIME [epoch: 0.929 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_28.pth
		[batch 1/1] avg loss: 2.6834350284153663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6834350284153663 | validation: 1.2694504485947984]
	TIME [epoch: 0.933 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.275959715413118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.275959715413118 | validation: 1.2825812060198887]
	TIME [epoch: 0.929 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2643982371013658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2643982371013658 | validation: 0.6648282833824182]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_29.pth
		[batch 1/1] avg loss: 1.2741760928461074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2741760928461074 | validation: 1.2009544344746843]
	TIME [epoch: 0.929 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2823790956119456		[learning rate: 0.01]
		[batch 1/1] avg loss: 1.4696365383751535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2823790956119456 | validation: 1.3680682663682682]
	TIME [epoch: 0.928 sec]
EPOCH 32/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 1.4696365383751535 | validation: 0.8253458316804884]
	TIME [epoch: 0.94 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3268922491784252		[learning rate: 0.01]
		[batch 1/1] avg loss: 1.0996672728056975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3268922491784252 | validation: 1.194082456023077]
	TIME [epoch: 0.926 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_32.pth
	Learning Rate: 0.01
	LOSS [training: 1.0996672728056975 | validation: 1.2811486570835249]
	TIME [epoch: 0.94 sec]
EPOCH 32/2000:
	Training over batches...
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2089081014376442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2089081014376442 | validation: 0.9188630197698138]
	TIME [epoch: 0.939 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3026659468725137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3026659468725137 | validation: 1.1825292906553846]
	TIME [epoch: 0.927 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_33.pth
		[batch 1/1] avg loss: 1.0562149733616988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0562149733616988 | validation: 0.7329830305866495]
	TIME [epoch: 0.944 sec]
EPOCH 34/2000:
	Training over batches...
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0799591821758479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0799591821758479 | validation: 0.9010406331882206]
	TIME [epoch: 0.937 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2561044510129311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2561044510129311 | validation: 1.1642509237166505]
	TIME [epoch: 0.926 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_34.pth
		[batch 1/1] avg loss: 1.0115503567642314		[learning rate: 0.01]
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 1.0115503567642314 | validation: 1.1069575970187338]
	TIME [epoch: 0.938 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2437606705552893		[learning rate: 0.01]
		[batch 1/1] avg loss: 1.0434135898578039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2437606705552893 | validation: 1.0682211354045907]
	TIME [epoch: 0.922 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_35.pth
	Learning Rate: 0.01
	LOSS [training: 1.0434135898578039 | validation: 0.7333665051717038]
	TIME [epoch: 0.937 sec]
EPOCH 37/2000:
	Training over batches...
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0369574218000162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0369574218000162 | validation: 0.8405746424332934]
	TIME [epoch: 0.938 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2445502452624975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2445502452624975 | validation: 1.2240511455665675]
	TIME [epoch: 0.927 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9975005243052607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9975005243052607 | validation: 1.1086207042415495]
	TIME [epoch: 0.936 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.230154605328199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.230154605328199 | validation: 0.9855903047971041]
	TIME [epoch: 0.923 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_37.pth
		[batch 1/1] avg loss: 1.025564783804117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.025564783804117 | validation: 0.6881631198352464]
	TIME [epoch: 0.942 sec]
EPOCH 40/2000:
	Training over batches...
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0284353693717112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0284353693717112 | validation: 0.8452822707256022]
	TIME [epoch: 0.939 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1833561442139933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1833561442139933 | validation: 1.0305098896929386]
	TIME [epoch: 0.928 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9766983421017185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9766983421017185 | validation: 1.1325475084778494]
	TIME [epoch: 0.937 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1554297463800693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1554297463800693 | validation: 0.951014747017265]
	TIME [epoch: 0.928 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_39.pth
		[batch 1/1] avg loss: 1.0252838956729602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0252838956729602 | validation: 0.6397679533634462]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_42.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.135043221145329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.135043221145329 | validation: 0.9468957712094679]
	TIME [epoch: 0.928 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_40.pth
		[batch 1/1] avg loss: 1.050101295470605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.050101295470605 | validation: 0.7233902096307915]
	TIME [epoch: 0.938 sec]
EPOCH 44/2000:
	Training over batches...
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.985182459079954		[learning rate: 0.01]
		[batch 1/1] avg loss: 1.1352598836253631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.985182459079954 | validation: 1.1730770350922168]
	TIME [epoch: 0.937 sec]
EPOCH 45/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 1.1352598836253631 | validation: 0.946231282862394]
	TIME [epoch: 0.929 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.034688772142619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.034688772142619 | validation: 0.7026599619139073]
	TIME [epoch: 0.937 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1289940611046216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1289940611046216 | validation: 0.9599966617003866]
	TIME [epoch: 0.927 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0390988242439954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0390988242439954 | validation: 0.9042704418048928]
	TIME [epoch: 0.938 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.13157175606426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.13157175606426 | validation: 0.9618185942132231]
	TIME [epoch: 0.928 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0983065155919625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0983065155919625 | validation: 1.0359107777828034]
	TIME [epoch: 0.937 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1508862694683841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1508862694683841 | validation: 1.0391716061997769]
	TIME [epoch: 0.928 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0076648663977652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0076648663977652 | validation: 0.8961947033202655]
	TIME [epoch: 0.937 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1749115757215456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1749115757215456 | validation: 0.9565350768932017]
	TIME [epoch: 0.927 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9562548145333601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9562548145333601 | validation: 0.7503421547857076]
	TIME [epoch: 0.937 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.17139946611592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.17139946611592 | validation: 0.9668109491530757]
	TIME [epoch: 0.928 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9395814365033616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9395814365033616 | validation: 1.000565801103131]
	TIME [epoch: 0.939 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1068545455165815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1068545455165815 | validation: 0.8609252310300282]
	TIME [epoch: 0.926 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_47.pth
		[batch 1/1] avg loss: 0.9479011438324447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9479011438324447 | validation: 0.6540965725243493]
	TIME [epoch: 0.936 sec]
EPOCH 52/2000:
	Training over batches...
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9761360904082861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9761360904082861 | validation: 0.9339263685290966]
	TIME [epoch: 0.935 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0765764504211068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0765764504211068 | validation: 0.8615063766340865]
	TIME [epoch: 0.924 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9578885482092808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9578885482092808 | validation: 0.9783996268754596]
	TIME [epoch: 0.938 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0514700569367776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0514700569367776 | validation: 0.8298040021064979]
	TIME [epoch: 0.921 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_49.pth
		[batch 1/1] avg loss: 0.977620229847851		[learning rate: 0.01]
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.977620229847851 | validation: 0.7407533606345098]
	TIME [epoch: 0.938 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0466594545012662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0466594545012662 | validation: 0.8631938323258354]
	TIME [epoch: 0.922 sec]
EPOCH 51/2000:
		[batch 1/1] avg loss: 0.9904521429689364		[learning rate: 0.01]
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.9904521429689364 | validation: 0.8940263655142893]
	TIME [epoch: 0.937 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0393767703470629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0393767703470629 | validation: 0.7625044165512997]
	TIME [epoch: 0.923 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_51.pth
		[batch 1/1] avg loss: 0.905917520164811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.905917520164811 | validation: 0.8907325172688921]
	TIME [epoch: 0.937 sec]
EPOCH 57/2000:
	Training over batches...
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.899852773655917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.899852773655917 | validation: 0.6216057264075187]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_57.pth
		[batch 1/1] avg loss: 1.0381498371324216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0381498371324216 | validation: 1.0834256935900275]
	TIME [epoch: 0.924 sec]
EPOCH 53/2000:
	Training over batches...
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.057874476533573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.057874476533573 | validation: 0.6747742606034242]
	TIME [epoch: 0.923 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_53.pth
		[batch 1/1] avg loss: 0.9321977737802376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9321977737802376 | validation: 1.215384422922332]
	TIME [epoch: 0.941 sec]
EPOCH 59/2000:
	Training over batches...
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0351652260582551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0351652260582551 | validation: 0.647010322933985]
	TIME [epoch: 0.94 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1158415138169457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1158415138169457 | validation: 1.4791125135775278]
	TIME [epoch: 0.927 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.125176410716904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.125176410716904 | validation: 1.0545607891846753]
	TIME [epoch: 0.94 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1639273570825974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1639273570825974 | validation: 0.7361796709726431]
	TIME [epoch: 0.926 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9570709907307121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9570709907307121 | validation: 0.7560151684726771]
	TIME [epoch: 0.94 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0840115174821785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0840115174821785 | validation: 0.8015347995212156]
	TIME [epoch: 0.926 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8774802230709934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8774802230709934 | validation: 0.7375248492220485]
	TIME [epoch: 0.941 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.093167832783303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.093167832783303 | validation: 1.1131390019889735]
	TIME [epoch: 0.926 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8918468817333849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8918468817333849 | validation: 0.8627276042585234]
	TIME [epoch: 0.94 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.103336255489307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.103336255489307 | validation: 0.9448075797257922]
	TIME [epoch: 0.927 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.885982507630564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.885982507630564 | validation: 0.6513136821733132]
	TIME [epoch: 0.939 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0241030097157175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0241030097157175 | validation: 0.7396302102060721]
	TIME [epoch: 0.936 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8859262025080681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8859262025080681 | validation: 0.8484360193914435]
	TIME [epoch: 0.938 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9835477883306808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9835477883306808 | validation: 0.8334140985800661]
	TIME [epoch: 0.926 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8917557070139194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8917557070139194 | validation: 0.7229157630170413]
	TIME [epoch: 0.94 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.969849714207572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.969849714207572 | validation: 0.8861975762734006]
	TIME [epoch: 0.927 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.908546771363348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.908546771363348 | validation: 0.9508947905967067]
	TIME [epoch: 0.942 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9693191921547731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9693191921547731 | validation: 0.7953111555477439]
	TIME [epoch: 0.929 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0217416047834333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0217416047834333 | validation: 0.868671829443681]
	TIME [epoch: 0.936 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9496166703498237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9496166703498237 | validation: 0.9418308139601346]
	TIME [epoch: 0.932 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9484189614048524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9484189614048524 | validation: 0.748058460376766]
	TIME [epoch: 0.937 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9515914653023928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9515914653023928 | validation: 0.806635164236522]
	TIME [epoch: 0.927 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9197920156557329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9197920156557329 | validation: 0.8085776305075142]
	TIME [epoch: 0.937 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9706097625866889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9706097625866889 | validation: 1.0042953715953917]
	TIME [epoch: 0.925 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8524162403732326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8524162403732326 | validation: 0.6488556464348125]
	TIME [epoch: 0.936 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9981912346961435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9981912346961435 | validation: 0.8530853928221284]
	TIME [epoch: 0.928 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8621572245042132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8621572245042132 | validation: 1.018922360317585]
	TIME [epoch: 0.936 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0239413678058984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0239413678058984 | validation: 1.0204359462715127]
	TIME [epoch: 0.926 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9211797676534278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9211797676534278 | validation: 0.6240896398144704]
	TIME [epoch: 0.937 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9730238187362714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9730238187362714 | validation: 0.6148304077377198]
	TIME [epoch: 0.926 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_68.pth
		[batch 1/1] avg loss: 1.176560331700613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.176560331700613 | validation: 1.2834072358983555]
	TIME [epoch: 0.936 sec]
EPOCH 75/2000:
	Training over batches...
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.092207921663867		[learning rate: 0.01]
		[batch 1/1] avg loss: 1.0817113614281029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.092207921663867 | validation: 0.6560339434825448]
	TIME [epoch: 0.95 sec]
EPOCH 76/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 1.0817113614281029 | validation: 1.3333209423531853]
	TIME [epoch: 0.926 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.886306029566766		[learning rate: 0.01]
		[batch 1/1] avg loss: 1.1232549844140665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.886306029566766 | validation: 0.6781249806161281]
	TIME [epoch: 0.935 sec]
EPOCH 77/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 1.1232549844140665 | validation: 0.7266380006189012]
	TIME [epoch: 0.925 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.89832804722574		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.9421269793915356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.89832804722574 | validation: 0.8928758156348293]
	TIME [epoch: 0.938 sec]
EPOCH 78/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.9421269793915356 | validation: 0.7555396421649515]
	TIME [epoch: 0.926 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8857855748792111		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.9500206446737249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8857855748792111 | validation: 0.7412044851240245]
	TIME [epoch: 0.941 sec]
EPOCH 79/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.9500206446737249 | validation: 0.9924650137649774]
	TIME [epoch: 0.925 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8449330783962722		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.9577458133847402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8449330783962722 | validation: 0.6317186743616212]
	TIME [epoch: 0.944 sec]
EPOCH 80/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.9577458133847402 | validation: 0.7364363764488571]
	TIME [epoch: 0.926 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8571578915955769		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.9208851429470744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8571578915955769 | validation: 0.8209443336423464]
	TIME [epoch: 0.937 sec]
EPOCH 81/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.9208851429470744 | validation: 0.746390929028155]
	TIME [epoch: 0.926 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8508468357023596		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8967070171418976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8967070171418976 | validation: 0.8686005642272807]
	TIME [epoch: 0.925 sec]
EPOCH 76/2000:
	Learning Rate: 0.01
	LOSS [training: 0.8508468357023596 | validation: 0.727375620824206]
	TIME [epoch: 0.937 sec]
	Training over batches...
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.901031878360312		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8358689189651779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.901031878360312 | validation: 0.7103974147626229]
	TIME [epoch: 0.924 sec]
EPOCH 77/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8358689189651779 | validation: 0.6834849286357776]
	TIME [epoch: 0.94 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9005734159199881		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8446638001092072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9005734159199881 | validation: 0.830269792958207]
	TIME [epoch: 0.926 sec]
EPOCH 78/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8446638001092072 | validation: 0.7457895415564301]
	TIME [epoch: 0.935 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8886230804243842		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8532331556294325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8886230804243842 | validation: 0.7526052584009814]
	TIME [epoch: 0.926 sec]
EPOCH 79/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8532331556294325 | validation: 0.770626526243789]
	TIME [epoch: 0.938 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8907790038069934		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8950129343669396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8907790038069934 | validation: 0.7886814531750739]
	TIME [epoch: 0.925 sec]
EPOCH 80/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8950129343669396 | validation: 0.8267141637897133]
	TIME [epoch: 0.938 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9040774057803194		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.9495564864113704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9040774057803194 | validation: 0.846921849526982]
	TIME [epoch: 0.926 sec]
EPOCH 81/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.9495564864113704 | validation: 0.9648124736796213]
	TIME [epoch: 0.937 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9613992658141289		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.9996195926350963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9613992658141289 | validation: 0.9824698395750096]
	TIME [epoch: 0.926 sec]
EPOCH 82/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.9996195926350963 | validation: 0.703118439734451]
	TIME [epoch: 0.937 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0188431783676484		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8268890883279899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0188431783676484 | validation: 0.7862131572776462]
	TIME [epoch: 0.926 sec]
EPOCH 83/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8268890883279899 | validation: 0.662112725078643]
	TIME [epoch: 0.937 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0321538561034012		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.850892684491162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0321538561034012 | validation: 1.0014823148627354]
	TIME [epoch: 0.925 sec]
EPOCH 84/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.850892684491162 | validation: 0.8546572041996807]
	TIME [epoch: 0.936 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.943649051427206		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8873135761439369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.943649051427206 | validation: 0.6583436879402752]
	TIME [epoch: 0.924 sec]
EPOCH 85/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8873135761439369 | validation: 0.6787731142225721]
	TIME [epoch: 0.939 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0752202175860281		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8407845875833077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0752202175860281 | validation: 1.064579784834232]
	TIME [epoch: 0.926 sec]
EPOCH 86/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8407845875833077 | validation: 0.6878625090406141]
	TIME [epoch: 0.937 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9788032678573132		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8325959003850713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9788032678573132 | validation: 0.7607697105022267]
	TIME [epoch: 0.928 sec]
EPOCH 87/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8325959003850713 | validation: 0.7400049078434227]
	TIME [epoch: 0.938 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8826065466425527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8826065466425527 | validation: 0.7182694069915522]
	TIME [epoch: 0.924 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8423041486475747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8423041486475747 | validation: 0.614434722123586]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.92362853380031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.92362853380031 | validation: 0.921965693952243]
	TIME [epoch: 0.926 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9112640608210453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9112640608210453 | validation: 1.2720233134058903]
	TIME [epoch: 0.942 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9160840971226399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9160840971226399 | validation: 0.7254120104925209]
	TIME [epoch: 0.927 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1871566342607838		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8897363863580754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1871566342607838 | validation: 0.6651455924767454]
	TIME [epoch: 0.941 sec]
EPOCH 96/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8897363863580754 | validation: 0.7256955333723459]
	TIME [epoch: 0.926 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1720147143684738		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8831547185395935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1720147143684738 | validation: 0.8988111188086165]
	TIME [epoch: 0.94 sec]
EPOCH 97/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8831547185395935 | validation: 0.8321894344816766]
	TIME [epoch: 0.926 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9204045683219202		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8882169291848598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9204045683219202 | validation: 0.8828616255521026]
	TIME [epoch: 0.939 sec]
EPOCH 98/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8882169291848598 | validation: 0.671425415505024]
	TIME [epoch: 0.925 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8839925716204573		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.9009980162462025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8839925716204573 | validation: 0.6415814775713882]
	TIME [epoch: 0.939 sec]
EPOCH 99/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.9009980162462025 | validation: 0.967766032451765]
	TIME [epoch: 0.925 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8511012806570274		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.9203072791128049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8511012806570274 | validation: 0.7528134015224386]
	TIME [epoch: 0.94 sec]
EPOCH 100/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.9203072791128049 | validation: 0.6030504492295519]
	TIME [epoch: 0.926 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8649578671815692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8649578671815692 | validation: 0.7062528271147311]
	TIME [epoch: 0.941 sec]
		[batch 1/1] avg loss: 0.9328180368497485		[learning rate: 0.01]
EPOCH 101/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.9328180368497485 | validation: 1.051015064226496]
	TIME [epoch: 0.926 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8199744633497693		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.9662800252701989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8199744633497693 | validation: 0.6931166608933118]
	TIME [epoch: 0.939 sec]
EPOCH 102/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.9662800252701989 | validation: 0.7034314238437495]
	TIME [epoch: 0.925 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8190484055223997		[learning rate: 0.01]
		[batch 1/1] avg loss: 1.0805426889119814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8190484055223997 | validation: 0.7504372542822264]
	TIME [epoch: 0.938 sec]
EPOCH 103/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 1.0805426889119814 | validation: 0.8283209330430843]
	TIME [epoch: 0.927 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8268631927764541		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.9165716219866049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8268631927764541 | validation: 0.6758036253982304]
	TIME [epoch: 0.939 sec]
EPOCH 104/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.9165716219866049 | validation: 0.8495713625666973]
	TIME [epoch: 0.927 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.818255353644596		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8890768338208748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.818255353644596 | validation: 0.7363155016406695]
	TIME [epoch: 0.938 sec]
EPOCH 105/2000:
	Learning Rate: 0.01
	LOSS [training: 0.8890768338208748 | validation: 0.6911864012600972]
	TIME [epoch: 0.926 sec]
	Training over batches...
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.891241302907182		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8246803363156685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.891241302907182 | validation: 0.8762177284820712]
	TIME [epoch: 0.925 sec]
	Learning Rate: 0.01
	LOSS [training: 0.8246803363156685 | validation: 0.6775510165348472]
	TIME [epoch: 0.938 sec]
EPOCH 106/2000:
	Training over batches...
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8249921331646792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8249921331646792 | validation: 0.7813126011259007]
	TIME [epoch: 0.938 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8974839129663309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8974839129663309 | validation: 0.5962237096947386]
	TIME [epoch: 0.924 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_101.pth
		[batch 1/1] avg loss: 0.8412683036385796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8412683036385796 | validation: 0.7175703381489411]
	TIME [epoch: 0.938 sec]
EPOCH 108/2000:
	Training over batches...
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.882762684870831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.882762684870831 | validation: 0.9856196022170904]
	TIME [epoch: 0.937 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9301546928208506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9301546928208506 | validation: 0.9837461694347432]
	TIME [epoch: 0.936 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9469019595170769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9469019595170769 | validation: 0.6492620331461809]
	TIME [epoch: 0.936 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9284594308030814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9284594308030814 | validation: 0.6120544082561865]
	TIME [epoch: 0.924 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8232611860164616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8232611860164616 | validation: 0.5860430257100171]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_110.pth
		[batch 1/1] avg loss: 0.9603342905310721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9603342905310721 | validation: 0.7766296533280612]
	TIME [epoch: 0.93 sec]
EPOCH 105/2000:
	Training over batches...
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8943622503175936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8943622503175936 | validation: 0.8729714932986559]
	TIME [epoch: 0.931 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8000707074924693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8000707074924693 | validation: 0.8737857961064819]
	TIME [epoch: 0.938 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9077348555890433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9077348555890433 | validation: 0.7005581330818056]
	TIME [epoch: 0.924 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.857432666024368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.857432666024368 | validation: 0.5339692812391439]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_112.pth
		[batch 1/1] avg loss: 0.9193880760332604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9193880760332604 | validation: 0.9123451837796348]
	TIME [epoch: 0.925 sec]
EPOCH 108/2000:
	Training over batches...
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9174986379571881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9174986379571881 | validation: 0.6983819548977855]
	TIME [epoch: 0.928 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9987468709168666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9987468709168666 | validation: 1.094905122766277]
	TIME [epoch: 0.941 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8966892642010257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8966892642010257 | validation: 0.8073105575668382]
	TIME [epoch: 0.925 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9980612246224783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9980612246224783 | validation: 0.6226624496200841]
	TIME [epoch: 0.938 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8782991645261957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8782991645261957 | validation: 0.6684849941893605]
	TIME [epoch: 0.923 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8165001385219814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8165001385219814 | validation: 0.6070719185384141]
	TIME [epoch: 0.939 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8765989736237976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8765989736237976 | validation: 0.9683157959713212]
	TIME [epoch: 0.925 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8123523094708943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8123523094708943 | validation: 0.7786875123157957]
	TIME [epoch: 0.949 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9251373999688653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9251373999688653 | validation: 0.6465668693881658]
	TIME [epoch: 0.919 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8069881660415544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8069881660415544 | validation: 0.647204244283146]
	TIME [epoch: 0.936 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.075294347971734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.075294347971734 | validation: 1.1107877418558072]
	TIME [epoch: 0.92 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7877899276423577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7877899276423577 | validation: 0.6807542258711773]
	TIME [epoch: 0.94 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.030964462699925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.030964462699925 | validation: 0.6903760715431443]
	TIME [epoch: 0.92 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7867927955087787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7867927955087787 | validation: 0.7151791923934526]
	TIME [epoch: 0.942 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9074504566063237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9074504566063237 | validation: 0.7842490528603601]
	TIME [epoch: 0.92 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.809370270447941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.809370270447941 | validation: 0.8046863750540961]
	TIME [epoch: 0.943 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9182615417280395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9182615417280395 | validation: 0.8234873460595266]
	TIME [epoch: 0.919 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.884665262208186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.884665262208186 | validation: 0.8141114836357005]
	TIME [epoch: 0.936 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8973889964881854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8973889964881854 | validation: 0.6994672429380904]
	TIME [epoch: 0.921 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9116843032903985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9116843032903985 | validation: 0.8453222558302708]
	TIME [epoch: 0.934 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8725543319297904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8725543319297904 | validation: 0.7987423226998835]
	TIME [epoch: 0.921 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8684149863704622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8684149863704622 | validation: 0.6158874706867747]
	TIME [epoch: 0.938 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8735908339069925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8735908339069925 | validation: 0.7239444929546084]
	TIME [epoch: 0.921 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7795500788780612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7795500788780612 | validation: 0.7227464006513499]
	TIME [epoch: 0.937 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8657352388264006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8657352388264006 | validation: 0.7392234310111993]
	TIME [epoch: 0.921 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7993733969197683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7993733969197683 | validation: 0.6260723944141365]
	TIME [epoch: 0.935 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8565063897018359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8565063897018359 | validation: 0.755401121270194]
	TIME [epoch: 0.922 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7918201086876587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7918201086876587 | validation: 0.7237206557513004]
	TIME [epoch: 0.933 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8519498035206029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8519498035206029 | validation: 0.7095602980536351]
	TIME [epoch: 0.924 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7836468273957189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7836468273957189 | validation: 0.6484851321901396]
	TIME [epoch: 0.934 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8646457363974074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8646457363974074 | validation: 0.8349310068826233]
	TIME [epoch: 0.923 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7883814468713152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7883814468713152 | validation: 0.7139636467065789]
	TIME [epoch: 0.934 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8969679551509776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8969679551509776 | validation: 0.7802193494549008]
	TIME [epoch: 0.921 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7810043538769805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7810043538769805 | validation: 0.7254610247502634]
	TIME [epoch: 0.938 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9708543456749257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9708543456749257 | validation: 1.0849047609613478]
	TIME [epoch: 0.921 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8048719482525598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8048719482525598 | validation: 0.7026020627293207]
	TIME [epoch: 0.935 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0280565584020347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0280565584020347 | validation: 0.5807241477365962]
	TIME [epoch: 0.92 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_126.pth
		[batch 1/1] avg loss: 0.8382824192431174		[learning rate: 0.01]
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8382824192431174 | validation: 0.9577808202237134]
	TIME [epoch: 0.938 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9014622215026395		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8879883902110993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9014622215026395 | validation: 1.0486182441590437]
	TIME [epoch: 0.929 sec]
EPOCH 128/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8879883902110993 | validation: 0.5376369161466631]
	TIME [epoch: 0.934 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0063248626633763		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8007795824340065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0063248626633763 | validation: 0.6059724981323525]
	TIME [epoch: 0.929 sec]
EPOCH 129/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8007795824340065 | validation: 0.7649363071234396]
	TIME [epoch: 0.934 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.092738779410192		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.7818144861086005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.092738779410192 | validation: 0.7952570114335948]
	TIME [epoch: 0.926 sec]
EPOCH 130/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.7818144861086005 | validation: 0.5721467212765152]
	TIME [epoch: 0.938 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8777976805511619		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.7912315708956638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8777976805511619 | validation: 0.9466468992654772]
	TIME [epoch: 0.926 sec]
EPOCH 131/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.7912315708956638 | validation: 0.780279014867904]
	TIME [epoch: 0.939 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.940015687438929		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.7952727024462029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.940015687438929 | validation: 0.604831095054613]
	TIME [epoch: 0.926 sec]
EPOCH 132/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.7952727024462029 | validation: 0.616317267280171]
	TIME [epoch: 0.939 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9194437610638029		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.7618081328834988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9194437610638029 | validation: 0.7453466436083698]
	TIME [epoch: 0.927 sec]
EPOCH 133/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.7618081328834988 | validation: 0.6427404016085334]
	TIME [epoch: 0.937 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8580732706790459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8580732706790459 | validation: 0.8004616462179777]
	TIME [epoch: 0.926 sec]
		[batch 1/1] avg loss: 0.747292369720199		[learning rate: 0.01]
EPOCH 134/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.747292369720199 | validation: 0.6736364785028578]
	TIME [epoch: 0.937 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8699926541009222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8699926541009222 | validation: 0.6552029770173031]
	TIME [epoch: 0.925 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7400701317334288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7400701317334288 | validation: 0.597786458179684]
	TIME [epoch: 0.937 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8633231090273213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8633231090273213 | validation: 0.7515873979197378]
	TIME [epoch: 0.926 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7570337288868476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7570337288868476 | validation: 0.8334136692807391]
	TIME [epoch: 0.937 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8462322669781983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8462322669781983 | validation: 0.6886130034441644]
	TIME [epoch: 0.926 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8033843536601916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8033843536601916 | validation: 0.5980304939522345]
	TIME [epoch: 0.937 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8410708765837328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8410708765837328 | validation: 0.7844570410481494]
	TIME [epoch: 0.926 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8381767819612256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8381767819612256 | validation: 0.8156130899648677]
	TIME [epoch: 0.935 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8430773246090928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8430773246090928 | validation: 0.6536114898081854]
	TIME [epoch: 0.926 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8082560645389825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8082560645389825 | validation: 0.5786791038698098]
	TIME [epoch: 0.939 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8666166855353084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8666166855353084 | validation: 0.8912663894477705]
	TIME [epoch: 0.927 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7296269497601386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7296269497601386 | validation: 0.6217583213755913]
	TIME [epoch: 0.936 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8940375246519361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8940375246519361 | validation: 0.6384822118287008]
	TIME [epoch: 0.926 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7246025809747859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7246025809747859 | validation: 0.6780164465653944]
	TIME [epoch: 0.938 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9321415663246525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9321415663246525 | validation: 1.0302526203600733]
	TIME [epoch: 0.947 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7236252143463692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7236252143463692 | validation: 0.5492779906969025]
	TIME [epoch: 0.937 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9432198793004245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9432198793004245 | validation: 0.5809442247163954]
	TIME [epoch: 0.926 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7596885405021696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7596885405021696 | validation: 0.9053305330811741]
	TIME [epoch: 0.937 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8761320038440797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8761320038440797 | validation: 0.9464023712389817]
	TIME [epoch: 0.926 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8780893048859062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8780893048859062 | validation: 0.5751768233092897]
	TIME [epoch: 0.937 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9236574673098724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9236574673098724 | validation: 0.6098965478435943]
	TIME [epoch: 0.928 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7242464913511287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7242464913511287 | validation: 0.5631880035883355]
	TIME [epoch: 0.937 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0016087601359964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0016087601359964 | validation: 0.8046284127199907]
	TIME [epoch: 0.937 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.723527059547656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.723527059547656 | validation: 0.7144831830862254]
	TIME [epoch: 0.937 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8697673497161909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8697673497161909 | validation: 0.709907729768098]
	TIME [epoch: 0.927 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.738216159611533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.738216159611533 | validation: 0.5701341366590044]
	TIME [epoch: 0.937 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8343265370569912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8343265370569912 | validation: 0.6664790935170818]
	TIME [epoch: 0.925 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7162683616026887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7162683616026887 | validation: 0.5713881902393404]
	TIME [epoch: 0.938 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8501808564736828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8501808564736828 | validation: 0.8756575173629492]
	TIME [epoch: 0.932 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7020524781329494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7020524781329494 | validation: 0.6559878444847927]
	TIME [epoch: 0.938 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8703282697456354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8703282697456354 | validation: 0.5561462204344949]
	TIME [epoch: 0.926 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_149.pth
		[batch 1/1] avg loss: 0.6990654409941681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6990654409941681 | validation: 0.5670915232068803]
	TIME [epoch: 0.937 sec]
EPOCH 155/2000:
	Training over batches...
	Model improved!!!
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7211193734223528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7211193734223528 | validation: 0.8069296131390138]
	TIME [epoch: 0.931 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9070354401212152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9070354401212152 | validation: 0.8586617938886617]
	TIME [epoch: 0.929 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8011546768142069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8011546768142069 | validation: 0.6226949749445639]
	TIME [epoch: 0.936 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8695959296054653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8695959296054653 | validation: 0.576880346629328]
	TIME [epoch: 0.932 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8147189475746643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8147189475746643 | validation: 0.6510315167574732]
	TIME [epoch: 0.934 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8794032191492949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8794032191492949 | validation: 0.7366305615305786]
	TIME [epoch: 0.928 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7123662138553172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7123662138553172 | validation: 0.5886867824468608]
	TIME [epoch: 0.933 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8493811650145661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8493811650145661 | validation: 0.874864560826444]
	TIME [epoch: 0.927 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6745966058361836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6745966058361836 | validation: 0.5639874931142584]
	TIME [epoch: 0.938 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9163098895091918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9163098895091918 | validation: 0.7160752251417638]
	TIME [epoch: 0.927 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6886817398008364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6886817398008364 | validation: 0.5771950999964943]
	TIME [epoch: 0.947 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9097182487660342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9097182487660342 | validation: 0.8656140463420425]
	TIME [epoch: 0.928 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.685447505368398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.685447505368398 | validation: 0.642944461663394]
	TIME [epoch: 0.934 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9015247938716923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9015247938716923 | validation: 0.6282214402945123]
	TIME [epoch: 0.929 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7082044262205058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7082044262205058 | validation: 0.4764035220665159]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_162.pth
		[batch 1/1] avg loss: 0.8418323034424373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8418323034424373 | validation: 0.7119607771941651]
	TIME [epoch: 0.925 sec]
EPOCH 158/2000:
	Training over batches...
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8444829121054375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8444829121054375 | validation: 0.715582820396435]
	TIME [epoch: 0.93 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6925958365449841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6925958365449841 | validation: 0.6929413764257951]
	TIME [epoch: 0.939 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8533729866199593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8533729866199593 | validation: 0.6392692122230548]
	TIME [epoch: 0.928 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6830231096256058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6830231096256058 | validation: 0.5669283359233502]
	TIME [epoch: 0.943 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8349939699810599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8349939699810599 | validation: 0.8062562706784295]
	TIME [epoch: 0.928 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6760742697210688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6760742697210688 | validation: 0.4659765004310652]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_165.pth
		[batch 1/1] avg loss: 0.8588293906973334		[learning rate: 0.01]
	Model improved!!!
EPOCH 166/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8588293906973334 | validation: 0.5274188768046548]
	TIME [epoch: 0.929 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_161.pth
	Model improved!!!
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6922362622815666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6922362622815666 | validation: 0.684322571349747]
	TIME [epoch: 0.942 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0875890014593508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0875890014593508 | validation: 0.9788070370225864]
	TIME [epoch: 0.929 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6774889655401737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6774889655401737 | validation: 0.5410948805791697]
	TIME [epoch: 0.934 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9381137895071313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9381137895071313 | validation: 0.6106330425596334]
	TIME [epoch: 0.928 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6617294797214943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6617294797214943 | validation: 0.5746284779630072]
	TIME [epoch: 0.934 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.852456055244665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.852456055244665 | validation: 0.6881993865676952]
	TIME [epoch: 0.927 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6853719557502305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6853719557502305 | validation: 0.5808083642362062]
	TIME [epoch: 0.939 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8325120487810364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8325120487810364 | validation: 0.8383229466871337]
	TIME [epoch: 0.927 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6789642207571446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6789642207571446 | validation: 0.6869253767564112]
	TIME [epoch: 0.937 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8697755268301933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8697755268301933 | validation: 0.7181793731086984]
	TIME [epoch: 0.928 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7257491205158998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7257491205158998 | validation: 0.6143992425331821]
	TIME [epoch: 0.945 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9015088289214492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9015088289214492 | validation: 0.7567091132291905]
	TIME [epoch: 0.926 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7812147150657688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7812147150657688 | validation: 0.6363038389885167]
	TIME [epoch: 0.94 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9144738473396106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9144738473396106 | validation: 0.7690284753528538]
	TIME [epoch: 0.928 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.646339673491093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.646339673491093 | validation: 0.4849311980193265]
	TIME [epoch: 0.936 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8607944939631865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8607944939631865 | validation: 0.6070319962792117]
	TIME [epoch: 0.928 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.633409301923269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.633409301923269 | validation: 0.5563792598309153]
	TIME [epoch: 0.936 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8304193093171243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8304193093171243 | validation: 0.8117800078064414]
	TIME [epoch: 0.928 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6464184298495991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6464184298495991 | validation: 0.5804100919765126]
	TIME [epoch: 0.936 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8448321841486472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8448321841486472 | validation: 0.5403404226428947]
	TIME [epoch: 0.926 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6965622964304856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6965622964304856 | validation: 0.5087528735072299]
	TIME [epoch: 0.935 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8800479850977552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8800479850977552 | validation: 0.9196518513059382]
	TIME [epoch: 0.929 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6092278701011474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6092278701011474 | validation: 0.5439073556784543]
	TIME [epoch: 0.935 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.891222154064235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.891222154064235 | validation: 0.5384937803153951]
	TIME [epoch: 0.928 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.618670065037671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.618670065037671 | validation: 0.5221259801293033]
	TIME [epoch: 0.938 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8889456241989746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8889456241989746 | validation: 0.8292249776889771]
	TIME [epoch: 0.926 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6468107153086601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6468107153086601 | validation: 0.5425394169241707]
	TIME [epoch: 0.936 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8415222164963353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8415222164963353 | validation: 0.58837542521271]
	TIME [epoch: 0.925 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6315085324525764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6315085324525764 | validation: 0.6145799260495148]
	TIME [epoch: 0.934 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8291475937935587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8291475937935587 | validation: 0.7841348264169865]
	TIME [epoch: 0.927 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6734610536689948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6734610536689948 | validation: 0.5978065795636007]
	TIME [epoch: 0.933 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8452559086895751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8452559086895751 | validation: 0.6509599359626327]
	TIME [epoch: 0.926 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6527466510632229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6527466510632229 | validation: 0.5041609045464688]
	TIME [epoch: 0.935 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8638740140250534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8638740140250534 | validation: 0.8978721170273052]
	TIME [epoch: 0.926 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6187685419191822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6187685419191822 | validation: 0.5927362606954596]
	TIME [epoch: 0.937 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9320603060149037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9320603060149037 | validation: 0.6587565042760113]
	TIME [epoch: 0.925 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6293837186935349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6293837186935349 | validation: 0.618149577185098]
	TIME [epoch: 0.935 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.009877356739876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.009877356739876 | validation: 0.7008842284251933]
	TIME [epoch: 0.927 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7617617363605185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7617617363605185 | validation: 0.5031167737515774]
	TIME [epoch: 0.934 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8345725883851906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8345725883851906 | validation: 0.7576060471212016]
	TIME [epoch: 0.925 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6007269039017031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6007269039017031 | validation: 0.6590672704042436]
	TIME [epoch: 0.937 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.833101375038506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.833101375038506 | validation: 0.5629010285659527]
	TIME [epoch: 0.927 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6653269258606985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6653269258606985 | validation: 0.4984850996338479]
	TIME [epoch: 0.936 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8748887408231955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8748887408231955 | validation: 0.96612397617882]
	TIME [epoch: 0.926 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6208868545778973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6208868545778973 | validation: 0.5000131591145719]
	TIME [epoch: 0.932 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8998003194436441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8998003194436441 | validation: 0.5582195044849707]
	TIME [epoch: 0.927 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5805545452308692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5805545452308692 | validation: 0.5799033422118743]
	TIME [epoch: 0.936 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9150878811861847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9150878811861847 | validation: 0.8341271306004009]
	TIME [epoch: 0.926 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6008346457019782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6008346457019782 | validation: 0.4424204284424573]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_190.pth
		[batch 1/1] avg loss: 0.8558593608372124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8558593608372124 | validation: 0.671289004312863]
	TIME [epoch: 0.928 sec]
EPOCH 187/2000:
	Training over batches...
	Model improved!!!
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8280877676125369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8280877676125369 | validation: 0.6967678238869133]
	TIME [epoch: 0.929 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5870217768473542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5870217768473542 | validation: 0.5573451624887608]
	TIME [epoch: 0.938 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8394750305815772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8394750305815772 | validation: 0.7231606704483728]
	TIME [epoch: 0.939 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5622826166613969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5622826166613969 | validation: 0.4459485496978113]
	TIME [epoch: 0.936 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8541384253380895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8541384253380895 | validation: 0.749781567641707]
	TIME [epoch: 0.925 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5512415033958753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5512415033958753 | validation: 0.5558184711002794]
	TIME [epoch: 0.937 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9038181543520563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9038181543520563 | validation: 0.7875215929946124]
	TIME [epoch: 0.934 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5753200917138319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5753200917138319 | validation: 0.4624283944873713]
	TIME [epoch: 0.935 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8754057015387688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8754057015387688 | validation: 0.5643027648178702]
	TIME [epoch: 0.93 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6797867600085735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6797867600085735 | validation: 0.5375394725306603]
	TIME [epoch: 0.937 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8510484484671602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8510484484671602 | validation: 0.9421998460282656]
	TIME [epoch: 0.934 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5663776259681229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5663776259681229 | validation: 0.49939998256891394]
	TIME [epoch: 0.937 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8957830854174367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8957830854174367 | validation: 0.5345352058800764]
	TIME [epoch: 0.928 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5424770379452791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5424770379452791 | validation: 0.4280305989942835]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_197.pth
		[batch 1/1] avg loss: 0.9154333453087325		[learning rate: 0.01]
	Model improved!!!
EPOCH 198/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.9154333453087325 | validation: 0.7936262166102896]
	TIME [epoch: 0.928 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5481925056315236		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8361131401975623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5481925056315236 | validation: 0.7283781186396187]
	TIME [epoch: 0.937 sec]
EPOCH 199/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8361131401975623 | validation: 0.6741119745941595]
	TIME [epoch: 0.93 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6149293744047323		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8169450268831052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6149293744047323 | validation: 0.5465678549552279]
	TIME [epoch: 0.936 sec]
EPOCH 200/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8169450268831052 | validation: 0.6379683033894736]
	TIME [epoch: 0.928 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6723414556008842		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8414319158028214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6723414556008842 | validation: 0.44683133791802815]
	TIME [epoch: 0.937 sec]
	Learning Rate: 0.01
	LOSS [training: 0.8414319158028214 | validation: 0.7397214133213161]
	TIME [epoch: 0.927 sec]
EPOCH 198/2000:
	Training over batches...
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8353700966237187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8353700966237187 | validation: 0.7059895153993118]
	TIME [epoch: 0.927 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8641114852829018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8641114852829018 | validation: 0.7087479097271108]
	TIME [epoch: 0.928 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8676449182638678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8676449182638678 | validation: 0.8089574522587079]
	TIME [epoch: 0.928 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5342918664286471		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.883817618897829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5342918664286471 | validation: 0.6295655176649539]
	TIME [epoch: 43.8 sec]
EPOCH 202/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.883817618897829 | validation: 0.6205607777769914]
	TIME [epoch: 42 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.579546523160586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.579546523160586 | validation: 0.5097411295936182]
	TIME [epoch: 1.84 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8375608052640346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8375608052640346 | validation: 0.7284718269932253]
	TIME [epoch: 1.84 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5789987984607757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5789987984607757 | validation: 0.37630839472221483]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8121658611325187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8121658611325187 | validation: 0.5391932816421954]
	TIME [epoch: 1.82 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5024327864752622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5024327864752622 | validation: 0.6583868945886993]
	TIME [epoch: 1.84 sec]
		[batch 1/1] avg loss: 0.8505765616114458		[learning rate: 0.01]
EPOCH 205/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8505765616114458 | validation: 1.0883496654154956]
	TIME [epoch: 1.82 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5416428853537578		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.9758496067463316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5416428853537578 | validation: 0.384027798385397]
	TIME [epoch: 1.84 sec]
EPOCH 206/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.9758496067463316 | validation: 0.5514785207314848]
	TIME [epoch: 1.82 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4746010543599857		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.9317626990650806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4746010543599857 | validation: 0.5040172282285202]
	TIME [epoch: 1.83 sec]
EPOCH 207/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.9317626990650806 | validation: 0.7095308912356999]
	TIME [epoch: 1.82 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46210482410447273		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8329711523028821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46210482410447273 | validation: 0.4170575330022409]
	TIME [epoch: 1.83 sec]
EPOCH 208/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8329711523028821 | validation: 0.7379936423721225]
	TIME [epoch: 1.82 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4745907028326255		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8159877842208212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4745907028326255 | validation: 0.7318283421944852]
	TIME [epoch: 1.84 sec]
EPOCH 209/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8159877842208212 | validation: 0.5655771302202726]
	TIME [epoch: 1.82 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7618062885881688		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8532704742950773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7618062885881688 | validation: 0.5182515633051226]
	TIME [epoch: 1.84 sec]
EPOCH 210/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8532704742950773 | validation: 0.8647148592484157]
	TIME [epoch: 1.82 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7382981405548726		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8629109085391438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7382981405548726 | validation: 0.4464153151825683]
	TIME [epoch: 1.84 sec]
EPOCH 211/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8629109085391438 | validation: 0.5509280950456732]
	TIME [epoch: 1.82 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5679592448816672		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8333211345314966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5679592448816672 | validation: 0.5423966575489513]
	TIME [epoch: 1.84 sec]
EPOCH 212/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8333211345314966 | validation: 0.7521547937417156]
	TIME [epoch: 1.81 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6323627277411499		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8320344150853589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6323627277411499 | validation: 0.49934734288713933]
	TIME [epoch: 1.84 sec]
EPOCH 213/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8320344150853589 | validation: 0.6052624092091642]
	TIME [epoch: 1.82 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4590035493782045		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8492534045971124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4590035493782045 | validation: 0.4030779623986523]
	TIME [epoch: 1.84 sec]
EPOCH 214/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8492534045971124 | validation: 0.7671786731456489]
	TIME [epoch: 1.82 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.470789389282584		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8929507999237265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.470789389282584 | validation: 0.4762920978480077]
	TIME [epoch: 1.83 sec]
EPOCH 215/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8929507999237265 | validation: 0.8659287269903349]
	TIME [epoch: 1.81 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4139615215855632		[learning rate: 0.01]
		[batch 1/1] avg loss: 1.015567874481039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4139615215855632 | validation: 0.41955535554389956]
	TIME [epoch: 1.83 sec]
EPOCH 216/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 1.015567874481039 | validation: 0.7185039859207167]
	TIME [epoch: 1.81 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3811380967111318		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8581289821910226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3811380967111318 | validation: 0.41333770784648255]
	TIME [epoch: 1.84 sec]
	Learning Rate: 0.01
	LOSS [training: 0.8581289821910226 | validation: 0.5977057313778235]
	TIME [epoch: 1.82 sec]
EPOCH 217/2000:
	Training over batches...
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8314461687585137		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.38340167229861877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8314461687585137 | validation: 0.8517612748625099]
	TIME [epoch: 1.82 sec]
	Learning Rate: 0.01
	LOSS [training: 0.38340167229861877 | validation: 0.5602261574749398]
	TIME [epoch: 1.83 sec]
EPOCH 218/2000:
	Training over batches...
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8503347075312087		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.6075085096146186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8503347075312087 | validation: 0.524054789619667]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_218.pth
	Learning Rate: 0.01
	LOSS [training: 0.6075085096146186 | validation: 0.5277680782006147]
	TIME [epoch: 1.84 sec]
EPOCH 219/2000:
	Training over batches...
	Model improved!!!
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6757320731450399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6757320731450399 | validation: 0.4019208433603182]
	TIME [epoch: 1.82 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.877957942150372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.877957942150372 | validation: 0.8688202191130888]
	TIME [epoch: 1.82 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37821994921666785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37821994921666785 | validation: 0.7564218118894325]
	TIME [epoch: 1.83 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8524720015882354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8524720015882354 | validation: 0.58019552122101]
	TIME [epoch: 1.82 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6540563164141132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6540563164141132 | validation: 0.48464908147380986]
	TIME [epoch: 1.84 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8220287898925568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8220287898925568 | validation: 0.7080488488781639]
	TIME [epoch: 1.82 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5890899144661553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5890899144661553 | validation: 0.45415320965318884]
	TIME [epoch: 1.83 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8065507534139488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8065507534139488 | validation: 0.659534802049818]
	TIME [epoch: 1.82 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4811230287840135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4811230287840135 | validation: 0.5221362166699743]
	TIME [epoch: 1.83 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8173132865598399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8173132865598399 | validation: 0.723236527934478]
	TIME [epoch: 1.82 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4929365907539936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4929365907539936 | validation: 0.5266547525729922]
	TIME [epoch: 1.82 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.849278312782073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.849278312782073 | validation: 0.862900446753315]
	TIME [epoch: 1.82 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42731650860848136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42731650860848136 | validation: 0.37615012937809306]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_225.pth
		[batch 1/1] avg loss: 0.970993275559064		[learning rate: 0.01]
	Model improved!!!
EPOCH 226/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.970993275559064 | validation: 0.6621140727047884]
	TIME [epoch: 1.82 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42205724029904323		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8313847669650325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42205724029904323 | validation: 0.6387288840800929]
	TIME [epoch: 1.84 sec]
EPOCH 227/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8313847669650325 | validation: 0.597269750408952]
	TIME [epoch: 1.82 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4682808882263545		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8018975990122107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4682808882263545 | validation: 0.442660253803433]
	TIME [epoch: 1.83 sec]
EPOCH 228/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8018975990122107 | validation: 0.710154038269139]
	TIME [epoch: 1.83 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4105760727811138		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8026986060140229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4105760727811138 | validation: 0.4707592624110702]
	TIME [epoch: 1.83 sec]
EPOCH 229/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8026986060140229 | validation: 0.5112477054797137]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_228.pth
	Model improved!!!
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3851859660847973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3851859660847973 | validation: 0.5302090599177394]
	TIME [epoch: 1.84 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8672646336816205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8672646336816205 | validation: 1.1505047220521982]
	TIME [epoch: 1.81 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4607451670564578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4607451670564578 | validation: 0.4631261292562332]
	TIME [epoch: 1.83 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.021415947162425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.021415947162425 | validation: 0.5502498307992928]
	TIME [epoch: 1.82 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5601665552980112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5601665552980112 | validation: 0.5017082225150671]
	TIME [epoch: 1.83 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8645277042429705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8645277042429705 | validation: 0.6839849538147229]
	TIME [epoch: 1.82 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39169377215910545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39169377215910545 | validation: 0.5099520629906688]
	TIME [epoch: 1.84 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8195037019371316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8195037019371316 | validation: 0.7888602681039699]
	TIME [epoch: 1.82 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5088027242912488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5088027242912488 | validation: 0.4054923754170077]
	TIME [epoch: 1.84 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8575051951289737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8575051951289737 | validation: 0.6192665521791647]
	TIME [epoch: 1.82 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38636646095853894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38636646095853894 | validation: 0.4114309607641663]
	TIME [epoch: 1.84 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8358869023805658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8358869023805658 | validation: 0.7438547902916671]
	TIME [epoch: 1.82 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32149252396831673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32149252396831673 | validation: 0.4285627149708308]
	TIME [epoch: 1.84 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8187591230322121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8187591230322121 | validation: 0.6582520383457984]
	TIME [epoch: 1.82 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3030220544189345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3030220544189345 | validation: 0.39599928957413955]
	TIME [epoch: 1.84 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8270812752832037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8270812752832037 | validation: 0.7711619303436481]
	TIME [epoch: 1.82 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2984713524995576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2984713524995576 | validation: 0.4756136748563904]
	TIME [epoch: 1.84 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8646499360904321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8646499360904321 | validation: 0.697542602110549]
	TIME [epoch: 1.82 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31568843112614664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31568843112614664 | validation: 0.39647499756844856]
	TIME [epoch: 1.85 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8775309112997515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8775309112997515 | validation: 0.6982050810448128]
	TIME [epoch: 1.82 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5591298009769711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5591298009769711 | validation: 0.43725849190300514]
	TIME [epoch: 1.84 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8492896800602887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8492896800602887 | validation: 0.7166579945657289]
	TIME [epoch: 1.82 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29868166881448643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29868166881448643 | validation: 0.4195943644017785]
	TIME [epoch: 1.84 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.826728806806014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.826728806806014 | validation: 0.5704883268330585]
	TIME [epoch: 1.82 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3527586264146335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3527586264146335 | validation: 0.5249732262079811]
	TIME [epoch: 1.84 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8714602955272734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8714602955272734 | validation: 0.9141144575929593]
	TIME [epoch: 1.82 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6624391999817228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6624391999817228 | validation: 0.37797801289885924]
	TIME [epoch: 1.83 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8880557997646961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8880557997646961 | validation: 0.48820871628463625]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37544267891261063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37544267891261063 | validation: 0.7229105001207602]
	TIME [epoch: 1.83 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9172522572410184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9172522572410184 | validation: 0.8243663530291381]
	TIME [epoch: 1.82 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6227001302584999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6227001302584999 | validation: 0.5557961274911859]
	TIME [epoch: 1.84 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8688139900603882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8688139900603882 | validation: 0.6135575308411956]
	TIME [epoch: 1.81 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4814849571516547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4814849571516547 | validation: 0.3747400581113769]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_245.pth
	Model improved!!!
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8035549909704344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8035549909704344 | validation: 0.7315851283404075]
	TIME [epoch: 1.81 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5886905440731397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5886905440731397 | validation: 0.442637598111355]
	TIME [epoch: 1.84 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7965410328241825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7965410328241825 | validation: 0.5466259661378688]
	TIME [epoch: 1.82 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3818297360195676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3818297360195676 | validation: 0.6355575332540795]
	TIME [epoch: 1.84 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8306399212272803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8306399212272803 | validation: 0.8229087437997097]
	TIME [epoch: 1.82 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47932537173242973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47932537173242973 | validation: 0.42910255678447495]
	TIME [epoch: 1.84 sec]
		[batch 1/1] avg loss: 0.8344700834289188		[learning rate: 0.01]
EPOCH 249/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8344700834289188 | validation: 0.542264864753629]
	TIME [epoch: 1.82 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3754778496735165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3754778496735165 | validation: 0.39866881723458525]
	TIME [epoch: 1.83 sec]
		[batch 1/1] avg loss: 0.8304782207510095		[learning rate: 0.01]
EPOCH 250/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8304782207510095 | validation: 0.759769709056486]
	TIME [epoch: 1.83 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3060660668763667		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8217265473818788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3060660668763667 | validation: 0.44691773476660634]
	TIME [epoch: 1.84 sec]
EPOCH 251/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8217265473818788 | validation: 0.5611696405241087]
	TIME [epoch: 1.82 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32688486400494965		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8693593342389653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32688486400494965 | validation: 0.4144560043479764]
	TIME [epoch: 1.83 sec]
EPOCH 252/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8693593342389653 | validation: 0.8314524417782718]
	TIME [epoch: 1.82 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33402856033007683		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8859132165018255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33402856033007683 | validation: 0.39682070265387814]
	TIME [epoch: 1.84 sec]
EPOCH 253/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8859132165018255 | validation: 0.7664454348843102]
	TIME [epoch: 1.82 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29883715523720084		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.9415433704725126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29883715523720084 | validation: 0.42531806537107536]
	TIME [epoch: 1.85 sec]
EPOCH 254/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.9415433704725126 | validation: 0.6461287691525237]
	TIME [epoch: 1.82 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29141180748859496		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8491102359986761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29141180748859496 | validation: 0.37447434604642926]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_254.pth
	Learning Rate: 0.01
	LOSS [training: 0.8491102359986761 | validation: 0.7937869342238393]
	TIME [epoch: 1.82 sec]
EPOCH 255/2000:
	Training over batches...
	Model improved!!!
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8403237098014082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8403237098014082 | validation: 0.5502502385740811]
	TIME [epoch: 1.82 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26876656911729674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26876656911729674 | validation: 0.4067159030204066]
	TIME [epoch: 1.83 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.833275309494697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.833275309494697 | validation: 0.778124678251617]
	TIME [epoch: 1.82 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2762592925622882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2762592925622882 | validation: 0.40980461047857325]
	TIME [epoch: 1.84 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8346267059654102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8346267059654102 | validation: 0.5160210551962899]
	TIME [epoch: 1.82 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3297763712545556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3297763712545556 | validation: 0.47230642664943034]
	TIME [epoch: 1.84 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8584905809237935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8584905809237935 | validation: 0.7955601412063997]
	TIME [epoch: 1.82 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5416840818211092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5416840818211092 | validation: 0.5096059444543813]
	TIME [epoch: 1.84 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8294011952588499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8294011952588499 | validation: 0.5423607808029385]
	TIME [epoch: 1.81 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3230109742315976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3230109742315976 | validation: 0.9810083341466032]
	TIME [epoch: 1.84 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8197575493444745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8197575493444745 | validation: 0.7886181786975536]
	TIME [epoch: 1.82 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8648434433583918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8648434433583918 | validation: 0.3124949082202957]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_260.pth
	Model improved!!!
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8286149430618669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8286149430618669 | validation: 0.5717142616842587]
	TIME [epoch: 1.82 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4653305196758445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4653305196758445 | validation: 0.6102572172703723]
	TIME [epoch: 1.84 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8635606741297922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8635606741297922 | validation: 0.8334281515617417]
	TIME [epoch: 1.82 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5139773196207359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5139773196207359 | validation: 0.43346619303754697]
	TIME [epoch: 1.85 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8916771900127713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8916771900127713 | validation: 0.7126481440562905]
	TIME [epoch: 1.82 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29016844186598967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29016844186598967 | validation: 0.44241707823981874]
	TIME [epoch: 1.84 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9238312336183365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9238312336183365 | validation: 0.6598729435566005]
	TIME [epoch: 1.81 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.297922678232985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.297922678232985 | validation: 0.415751186076783]
	TIME [epoch: 1.84 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8270260628900513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8270260628900513 | validation: 0.7419977368057756]
	TIME [epoch: 1.82 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3094870769872155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3094870769872155 | validation: 0.38973539872530416]
	TIME [epoch: 1.84 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8186114962213615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8186114962213615 | validation: 0.5606518483048125]
	TIME [epoch: 1.81 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2759472440852289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2759472440852289 | validation: 0.3637544852867842]
	TIME [epoch: 1.84 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.821219779634992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.821219779634992 | validation: 0.7722512758245391]
	TIME [epoch: 1.82 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25706149788476923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25706149788476923 | validation: 0.3922841832234822]
	TIME [epoch: 1.84 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.823021494915223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.823021494915223 | validation: 0.5456713354894008]
	TIME [epoch: 1.81 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24751927634651424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24751927634651424 | validation: 0.4241256515183034]
	TIME [epoch: 1.84 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8400862040690439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8400862040690439 | validation: 0.8560629056001791]
	TIME [epoch: 1.82 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29494120111066213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29494120111066213 | validation: 0.37166134570828446]
	TIME [epoch: 1.84 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8508760822530153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8508760822530153 | validation: 0.5469148438103674]
	TIME [epoch: 1.81 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3464246864763257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3464246864763257 | validation: 0.4529080717234208]
	TIME [epoch: 1.84 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8262678155905127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8262678155905127 | validation: 0.7885484416690896]
	TIME [epoch: 1.82 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3764926243419353		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8155074734804904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3764926243419353 | validation: 0.4497974753568267]
	TIME [epoch: 1.84 sec]
EPOCH 272/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8155074734804904 | validation: 0.6281219207418371]
	TIME [epoch: 1.82 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2858542490441224		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8322580659757164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2858542490441224 | validation: 0.3064777579638588]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_272.pth
	Learning Rate: 0.01
	LOSS [training: 0.8322580659757164 | validation: 0.788244411713822]
	TIME [epoch: 1.82 sec]
EPOCH 274/2000:
	Training over batches...
	Model improved!!!
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9314202418612594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9314202418612594 | validation: 0.8447894066152458]
	TIME [epoch: 1.82 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4247488577515621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4247488577515621 | validation: 0.41777367625073725]
	TIME [epoch: 1.84 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9507680459371214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9507680459371214 | validation: 0.6269760451357007]
	TIME [epoch: 1.82 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26166725192760576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26166725192760576 | validation: 0.41692461083072363]
	TIME [epoch: 1.84 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8556360742201777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8556360742201777 | validation: 0.6905063242297489]
	TIME [epoch: 1.81 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3492876964708138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3492876964708138 | validation: 0.3318214324697917]
	TIME [epoch: 1.83 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8060503229621594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8060503229621594 | validation: 0.5978248604314199]
	TIME [epoch: 1.81 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38515538371113633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38515538371113633 | validation: 0.5747031872976683]
	TIME [epoch: 1.82 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8179113209264977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8179113209264977 | validation: 0.7684490927627965]
	TIME [epoch: 1.82 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3841062808916307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3841062808916307 | validation: 0.3745026253045958]
	TIME [epoch: 1.83 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8185268232267234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8185268232267234 | validation: 0.5645157911490619]
	TIME [epoch: 1.82 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2652593876464776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2652593876464776 | validation: 0.28908565189646895]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_278.pth
	Model improved!!!
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8508108787416819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8508108787416819 | validation: 0.9190960362580599]
	TIME [epoch: 1.81 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25295031321293915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25295031321293915 | validation: 0.3540579761615424]
	TIME [epoch: 1.83 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8858678599035367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8858678599035367 | validation: 0.5473513154926603]
	TIME [epoch: 1.81 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21685550904260176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21685550904260176 | validation: 0.3639635089558702]
	TIME [epoch: 1.82 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8240793369931083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8240793369931083 | validation: 0.7097712916356883]
	TIME [epoch: 1.81 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23537416303734549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23537416303734549 | validation: 0.4476660751156845]
	TIME [epoch: 1.82 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7927666409556875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7927666409556875 | validation: 0.582793398198237]
	TIME [epoch: 1.81 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3209341817972936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3209341817972936 | validation: 0.6696470244399039]
	TIME [epoch: 1.83 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.799607903407113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.799607903407113 | validation: 0.7083608591801038]
	TIME [epoch: 1.81 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4992181860856739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4992181860856739 | validation: 0.5150866082799015]
	TIME [epoch: 1.82 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8088586673805458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8088586673805458 | validation: 0.6570986789338916]
	TIME [epoch: 1.82 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36534537331669437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36534537331669437 | validation: 0.3353115393970356]
	TIME [epoch: 1.83 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8594679852435991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8594679852435991 | validation: 0.8193800293411837]
	TIME [epoch: 1.82 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22796693010749353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22796693010749353 | validation: 0.36548202765300375]
	TIME [epoch: 1.83 sec]
		[batch 1/1] avg loss: 0.9346373810218742		[learning rate: 0.01]
EPOCH 286/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.9346373810218742 | validation: 0.746488602012805]
	TIME [epoch: 1.82 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24629557174815112		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.9070965874565344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24629557174815112 | validation: 0.4455927857241999]
	TIME [epoch: 1.84 sec]
EPOCH 287/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.9070965874565344 | validation: 0.6590489541518254]
	TIME [epoch: 1.82 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3372024471984878		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.7840315717825689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3372024471984878 | validation: 0.39955007919430746]
	TIME [epoch: 1.83 sec]
EPOCH 288/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.7840315717825689 | validation: 0.6026352656279141]
	TIME [epoch: 1.82 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4286009199821543		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8532680700061581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4286009199821543 | validation: 0.2806338479761598]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_288.pth
	Learning Rate: 0.01
	LOSS [training: 0.8532680700061581 | validation: 0.88948554879785]
	TIME [epoch: 1.82 sec]
EPOCH 291/2000:
	Training over batches...
	Model improved!!!
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8647540812000214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8647540812000214 | validation: 0.5380675670881063]
	TIME [epoch: 1.83 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20496387362694293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20496387362694293 | validation: 0.46438944456539794]
	TIME [epoch: 1.84 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8502040067817157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8502040067817157 | validation: 0.7726175073961908]
	TIME [epoch: 1.81 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.309185877334733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.309185877334733 | validation: 0.3760800419703774]
	TIME [epoch: 1.83 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8377231515214193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8377231515214193 | validation: 0.5717107633280483]
	TIME [epoch: 1.81 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.376670655972779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.376670655972779 | validation: 0.2657637994210666]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8181953104615047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8181953104615047 | validation: 0.7360237656729247]
	TIME [epoch: 1.82 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22358419547666716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22358419547666716 | validation: 0.44192581314699214]
	TIME [epoch: 1.84 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.801340481837317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.801340481837317 | validation: 0.5541084245671212]
	TIME [epoch: 1.82 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26416387411965764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26416387411965764 | validation: 0.31082067799376684]
	TIME [epoch: 1.84 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8289832599463866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8289832599463866 | validation: 0.7876393733853169]
	TIME [epoch: 1.83 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2585362188842801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2585362188842801 | validation: 0.2815719794943211]
	TIME [epoch: 1.83 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8174340290705135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8174340290705135 | validation: 0.5153704426346136]
	TIME [epoch: 1.82 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20972155176056376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20972155176056376 | validation: 0.4856578641335652]
	TIME [epoch: 1.83 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8132004980002294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8132004980002294 | validation: 0.739517270914221]
	TIME [epoch: 1.81 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24081510568756537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24081510568756537 | validation: 0.4402184278496954]
	TIME [epoch: 1.83 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8065454655516093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8065454655516093 | validation: 0.5642869388671067]
	TIME [epoch: 1.81 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24688027609904203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24688027609904203 | validation: 0.3384666752580519]
	TIME [epoch: 1.83 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7971742435055437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7971742435055437 | validation: 0.7970494944480224]
	TIME [epoch: 1.82 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22354967018969946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22354967018969946 | validation: 0.3905162989849492]
	TIME [epoch: 1.83 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.831555796847918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.831555796847918 | validation: 0.5385738463370449]
	TIME [epoch: 1.81 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24602383580350334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24602383580350334 | validation: 0.27788024639016945]
	TIME [epoch: 1.84 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8937079182545891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8937079182545891 | validation: 0.8578607633537431]
	TIME [epoch: 1.81 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24664852885063465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24664852885063465 | validation: 0.3669077799046344]
	TIME [epoch: 1.84 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.90686999436887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.90686999436887 | validation: 0.6640261928469645]
	TIME [epoch: 1.81 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20127731114795083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20127731114795083 | validation: 0.32650784456861964]
	TIME [epoch: 1.83 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9140020442849566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9140020442849566 | validation: 0.6881442978399502]
	TIME [epoch: 1.81 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3381928211925868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3381928211925868 | validation: 0.2858253139973323]
	TIME [epoch: 1.83 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8733297435662266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8733297435662266 | validation: 0.7484084216780937]
	TIME [epoch: 1.81 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3080398491321991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3080398491321991 | validation: 0.40773051116062026]
	TIME [epoch: 1.83 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8222601763204057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8222601763204057 | validation: 0.5945570008931146]
	TIME [epoch: 1.8 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3500017578865158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3500017578865158 | validation: 0.3029206472539816]
	TIME [epoch: 1.83 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8140096248526584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8140096248526584 | validation: 0.7586043900476093]
	TIME [epoch: 1.85 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2212916439530533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2212916439530533 | validation: 0.286126319733897]
	TIME [epoch: 1.83 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8142473408725514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8142473408725514 | validation: 0.5221379336813015]
	TIME [epoch: 1.81 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2436707744223828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2436707744223828 | validation: 0.41211379633473516]
	TIME [epoch: 1.83 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8346246544330299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8346246544330299 | validation: 0.709744458058835]
	TIME [epoch: 1.81 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22977170984516446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22977170984516446 | validation: 0.3777026227971587]
	TIME [epoch: 1.83 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8026618075486229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8026618075486229 | validation: 0.5860232202554163]
	TIME [epoch: 1.8 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2643229449761712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2643229449761712 | validation: 0.3313538426931145]
	TIME [epoch: 1.84 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7887730436352841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7887730436352841 | validation: 0.7266543264351628]
	TIME [epoch: 1.8 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22764552460095686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22764552460095686 | validation: 0.4151547119441065]
	TIME [epoch: 1.83 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8289355697148199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8289355697148199 | validation: 0.6761412911437881]
	TIME [epoch: 1.81 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22189583350927414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22189583350927414 | validation: 0.29377105259246844]
	TIME [epoch: 1.85 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9406588449468405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9406588449468405 | validation: 0.7775327386446858]
	TIME [epoch: 1.81 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18455340823102703		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.918110687027492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18455340823102703 | validation: 0.26928897104915267]
	TIME [epoch: 1.84 sec]
EPOCH 312/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.918110687027492 | validation: 0.8015771378433243]
	TIME [epoch: 1.81 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15884635623881402		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8634333111311758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15884635623881402 | validation: 0.36333823564128154]
	TIME [epoch: 1.84 sec]
EPOCH 313/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8634333111311758 | validation: 0.5882891293509992]
	TIME [epoch: 1.81 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19603895646586797		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.823034888007621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19603895646586797 | validation: 0.33494072715838885]
	TIME [epoch: 1.84 sec]
EPOCH 314/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.823034888007621 | validation: 0.8021120550721701]
	TIME [epoch: 1.81 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4029187853511772		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8551858973762569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4029187853511772 | validation: 0.2830600151627517]
	TIME [epoch: 1.84 sec]
EPOCH 315/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8551858973762569 | validation: 0.5681875486700833]
	TIME [epoch: 1.81 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2453207087587034		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8376184980173644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2453207087587034 | validation: 0.426206522942986]
	TIME [epoch: 1.83 sec]
EPOCH 316/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8376184980173644 | validation: 0.7360661537226345]
	TIME [epoch: 1.81 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2237922547702716		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.807443096783709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2237922547702716 | validation: 0.3645884949594862]
	TIME [epoch: 1.82 sec]
EPOCH 317/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.807443096783709 | validation: 0.5733683768898067]
	TIME [epoch: 1.82 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25171161694019206		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.7948065357496199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25171161694019206 | validation: 0.36279720693834594]
	TIME [epoch: 1.83 sec]
EPOCH 318/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.7948065357496199 | validation: 0.7963596131201519]
	TIME [epoch: 1.81 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2683401271481903		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8208937894526716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2683401271481903 | validation: 0.3623640840165029]
	TIME [epoch: 1.84 sec]
EPOCH 319/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8208937894526716 | validation: 0.563395020864685]
	TIME [epoch: 1.81 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26582053899359054		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8526785428307011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26582053899359054 | validation: 0.2792009296902222]
	TIME [epoch: 1.84 sec]
EPOCH 320/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8526785428307011 | validation: 0.7168553021206695]
	TIME [epoch: 1.81 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22065308134325512		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.802608455461295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22065308134325512 | validation: 0.2697719755834416]
	TIME [epoch: 1.83 sec]
EPOCH 321/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.802608455461295 | validation: 0.6179663564412458]
	TIME [epoch: 1.81 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1496348742140486		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.7893973547208447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1496348742140486 | validation: 0.34794336544185644]
	TIME [epoch: 1.84 sec]
EPOCH 322/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.7893973547208447 | validation: 0.6297968488835287]
	TIME [epoch: 1.81 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7802617193397866		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.22664811628440631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7802617193397866 | validation: 0.5932560724694649]
	TIME [epoch: 1.81 sec]
	Learning Rate: 0.01
	LOSS [training: 0.22664811628440631 | validation: 0.30546841338087405]
	TIME [epoch: 1.83 sec]
EPOCH 326/2000:
	Training over batches...
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7856499312598232		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.21014801942236333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7856499312598232 | validation: 0.7831215535822917]
	TIME [epoch: 1.81 sec]
EPOCH 327/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.21014801942236333 | validation: 0.2981034360751436]
	TIME [epoch: 1.84 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8268958726034555		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.24029901766448972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8268958726034555 | validation: 0.6970881360725905]
	TIME [epoch: 1.81 sec]
EPOCH 328/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.24029901766448972 | validation: 0.35374580008267253]
	TIME [epoch: 1.84 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9360415666048247		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.24433732721303267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9360415666048247 | validation: 0.9147160338405117]
	TIME [epoch: 1.81 sec]
EPOCH 329/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.24433732721303267 | validation: 0.31493613710220636]
	TIME [epoch: 1.84 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9462793116795924		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.2286761446296665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9462793116795924 | validation: 0.6724573709485399]
	TIME [epoch: 1.81 sec]
EPOCH 330/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.2286761446296665 | validation: 0.33556810666930903]
	TIME [epoch: 1.84 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7947150482894378		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.17309192613601604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7947150482894378 | validation: 0.540743319994605]
	TIME [epoch: 1.81 sec]
EPOCH 331/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.17309192613601604 | validation: 0.3221715031659027]
	TIME [epoch: 1.84 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8634693415736238		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.15890327828126596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8634693415736238 | validation: 0.9350252143969109]
	TIME [epoch: 1.81 sec]
EPOCH 332/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.15890327828126596 | validation: 0.29744399924468706]
	TIME [epoch: 1.84 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8846319850417973		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.18910443476062155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8846319850417973 | validation: 0.5480851829412832]
	TIME [epoch: 1.81 sec]
EPOCH 333/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.18910443476062155 | validation: 0.4540954585598132]
	TIME [epoch: 1.84 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8072688935334104		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.23238010335044088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8072688935334104 | validation: 0.6638864895806216]
	TIME [epoch: 1.81 sec]
EPOCH 334/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.23238010335044088 | validation: 0.2625832698787169]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_330.pth
	Model improved!!!
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7987494265885975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7987494265885975 | validation: 0.6681017896828603]
	TIME [epoch: 1.81 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23560193652516134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23560193652516134 | validation: 0.24004689243742938]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_331.pth
		[batch 1/1] avg loss: 0.8041801361545717		[learning rate: 0.01]
	Model improved!!!
EPOCH 332/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8041801361545717 | validation: 0.6663006243075544]
	TIME [epoch: 1.82 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1521268802780353		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8146976170554759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1521268802780353 | validation: 0.3102415414213541]
	TIME [epoch: 1.84 sec]
EPOCH 333/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8146976170554759 | validation: 0.6380557847204997]
	TIME [epoch: 1.81 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1761821576855401		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8775018974745785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1761821576855401 | validation: 0.21219711029161964]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_333.pth
	Learning Rate: 0.01
	LOSS [training: 0.8775018974745785 | validation: 0.7777602850386112]
	TIME [epoch: 1.81 sec]
EPOCH 338/2000:
	Training over batches...
	Model improved!!!
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8651892851224487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8651892851224487 | validation: 0.5398881558419438]
	TIME [epoch: 1.82 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2012728572168958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2012728572168958 | validation: 0.2663737497976576]
	TIME [epoch: 1.84 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8305990359228551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8305990359228551 | validation: 0.7743738408617123]
	TIME [epoch: 1.82 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1788909970378787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1788909970378787 | validation: 0.2811527565794193]
	TIME [epoch: 1.84 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8182911819476482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8182911819476482 | validation: 0.5775254378047848]
	TIME [epoch: 1.85 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1524968457235807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1524968457235807 | validation: 0.26526907830267576]
	TIME [epoch: 1.84 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7968494243175006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7968494243175006 | validation: 0.6965099381299841]
	TIME [epoch: 1.83 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1598453484022295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1598453484022295 | validation: 0.36781749563432453]
	TIME [epoch: 1.84 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7991091434302489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7991091434302489 | validation: 0.5975609994605063]
	TIME [epoch: 1.83 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24626665092898467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24626665092898467 | validation: 0.4783934380459616]
	TIME [epoch: 1.84 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8025613847307884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8025613847307884 | validation: 0.7438839975254692]
	TIME [epoch: 1.82 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4702929249558313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4702929249558313 | validation: 0.30329820613963654]
	TIME [epoch: 1.84 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8029665376265769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8029665376265769 | validation: 0.5764048668038798]
	TIME [epoch: 1.83 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2008946538484232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2008946538484232 | validation: 0.2473902263143335]
	TIME [epoch: 1.84 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8381113825973066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8381113825973066 | validation: 0.8930090713024459]
	TIME [epoch: 1.82 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20812478358447933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20812478358447933 | validation: 0.27121624619864426]
	TIME [epoch: 1.83 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8692725148546632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8692725148546632 | validation: 0.5520866126864612]
	TIME [epoch: 1.81 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18332005208624771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18332005208624771 | validation: 0.3207101730666777]
	TIME [epoch: 1.84 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8176418838732454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8176418838732454 | validation: 0.6671848042887849]
	TIME [epoch: 1.82 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1582501697511184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1582501697511184 | validation: 0.2187180751350812]
	TIME [epoch: 1.84 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7975465632235461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7975465632235461 | validation: 0.6132948049483785]
	TIME [epoch: 1.81 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11672533953952005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11672533953952005 | validation: 0.21419660760381523]
	TIME [epoch: 1.84 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8235096047095894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8235096047095894 | validation: 0.7252539756488426]
	TIME [epoch: 1.81 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12583245941667254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12583245941667254 | validation: 0.3776610784060456]
	TIME [epoch: 1.83 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8742667062200866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8742667062200866 | validation: 0.6353341991994698]
	TIME [epoch: 1.81 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18157109148618147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18157109148618147 | validation: 0.26602278364144266]
	TIME [epoch: 1.84 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9032325547103787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9032325547103787 | validation: 0.7017192260952264]
	TIME [epoch: 1.82 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22294886999108252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22294886999108252 | validation: 0.35245703810212453]
	TIME [epoch: 1.82 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8040047418715863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8040047418715863 | validation: 0.5414648537808612]
	TIME [epoch: 1.82 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17128498671146358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17128498671146358 | validation: 0.20921540296747354]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_348.pth
	Model improved!!!
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8318679802216389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8318679802216389 | validation: 0.986423186395864]
	TIME [epoch: 1.82 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.139707866127687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.139707866127687 | validation: 0.29462482552933283]
	TIME [epoch: 1.82 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9123045757331016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9123045757331016 | validation: 0.5420133339678247]
	TIME [epoch: 1.81 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30915294615785327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30915294615785327 | validation: 0.25475387260856514]
	TIME [epoch: 1.82 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8078780357808562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8078780357808562 | validation: 0.6731956442187088]
	TIME [epoch: 1.81 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16360611901047317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16360611901047317 | validation: 0.23541652591904164]
	TIME [epoch: 1.83 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7858238785634274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7858238785634274 | validation: 0.6561074471426226]
	TIME [epoch: 1.81 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17739242942042022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17739242942042022 | validation: 0.36583561703051953]
	TIME [epoch: 1.83 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8017333664541159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8017333664541159 | validation: 0.6251310931973775]
	TIME [epoch: 1.82 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2566232547476817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2566232547476817 | validation: 0.3110655198635276]
	TIME [epoch: 1.83 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7988033423945321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7988033423945321 | validation: 0.6469989358367015]
	TIME [epoch: 1.81 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.336677926355345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.336677926355345 | validation: 0.31058311276170125]
	TIME [epoch: 1.83 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.810345622594653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.810345622594653 | validation: 0.7718327696757975]
	TIME [epoch: 1.82 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15603359204307599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15603359204307599 | validation: 0.20613172681930525]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_355.pth
		[batch 1/1] avg loss: 0.8854598886183288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8854598886183288 | validation: 0.6415237465227978]
	TIME [epoch: 1.81 sec]
EPOCH 361/2000:
	Training over batches...
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8982454421139049		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.09116800664543333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8982454421139049 | validation: 0.6287316732705316]
	TIME [epoch: 1.82 sec]
EPOCH 362/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.09116800664543333 | validation: 0.18313293021920748]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_356.pth
	Model improved!!!
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7771415045452583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7771415045452583 | validation: 0.7352134933325208]
	TIME [epoch: 1.81 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10412402886143234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10412402886143234 | validation: 0.22664240206854847]
	TIME [epoch: 1.85 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7984111128234933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7984111128234933 | validation: 0.5056057144665146]
	TIME [epoch: 1.82 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10958290695518563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10958290695518563 | validation: 0.21423844575016424]
	TIME [epoch: 1.84 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8374719180026774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8374719180026774 | validation: 0.8199321128745045]
	TIME [epoch: 1.82 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12363236335932515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12363236335932515 | validation: 0.25532748842533315]
	TIME [epoch: 1.83 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8323707656117639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8323707656117639 | validation: 0.5407041033776526]
	TIME [epoch: 1.82 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15413873750593826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15413873750593826 | validation: 0.31407530630729474]
	TIME [epoch: 1.83 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8093053895472181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8093053895472181 | validation: 0.7182202409042641]
	TIME [epoch: 1.82 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22626732466167482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22626732466167482 | validation: 0.3657460214743683]
	TIME [epoch: 1.83 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8007492843072231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8007492843072231 | validation: 0.5349080377014275]
	TIME [epoch: 1.81 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2595304821417311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2595304821417311 | validation: 0.2885669765229793]
	TIME [epoch: 1.83 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8049121812166675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8049121812166675 | validation: 0.7523095127556201]
	TIME [epoch: 1.81 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4000838486202662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4000838486202662 | validation: 0.23124602674913355]
	TIME [epoch: 1.83 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8048494499628237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8048494499628237 | validation: 0.5012368274473963]
	TIME [epoch: 1.81 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14824898687579374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14824898687579374 | validation: 0.3558598499961625]
	TIME [epoch: 1.82 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8215638694584686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8215638694584686 | validation: 0.7902624549901958]
	TIME [epoch: 1.8 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22045011217549404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22045011217549404 | validation: 0.21992778718736325]
	TIME [epoch: 1.82 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8385769579261922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8385769579261922 | validation: 0.5768530563600636]
	TIME [epoch: 1.81 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20124028552722087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20124028552722087 | validation: 0.17641903678498042]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_366.pth
		[batch 1/1] avg loss: 0.8679958932185855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8679958932185855 | validation: 0.884457407762329]
	TIME [epoch: 1.82 sec]
EPOCH 373/2000:
	Training over batches...
	Model improved!!!
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9835866218399089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9835866218399089 | validation: 0.8240728292741111]
	TIME [epoch: 1.82 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12485051798597159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12485051798597159 | validation: 0.2798052380767844]
	TIME [epoch: 1.83 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9326091811860662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9326091811860662 | validation: 0.6036997287855571]
	TIME [epoch: 1.82 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12408580528313343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12408580528313343 | validation: 0.1483675529501198]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_368.pth
	Model improved!!!
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8098064740873469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8098064740873469 | validation: 0.741807879706421]
	TIME [epoch: 1.82 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12521164836289617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12521164836289617 | validation: 0.26688897947606]
	TIME [epoch: 1.83 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8604141605623845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8604141605623845 | validation: 0.6134068333251181]
	TIME [epoch: 1.82 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1304470943640361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1304470943640361 | validation: 0.2202941098972358]
	TIME [epoch: 1.83 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8192974709385547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8192974709385547 | validation: 0.63781715781718]
	TIME [epoch: 1.82 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17086616656689907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17086616656689907 | validation: 0.31696205044118014]
	TIME [epoch: 1.83 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7874517033842547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7874517033842547 | validation: 0.6317653897159334]
	TIME [epoch: 1.82 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17972788343317045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17972788343317045 | validation: 0.20246823461730054]
	TIME [epoch: 1.83 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8032528007336552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8032528007336552 | validation: 0.647161838182136]
	TIME [epoch: 1.82 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1674640967708635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1674640967708635 | validation: 0.20397087800943964]
	TIME [epoch: 1.82 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7981349954166743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7981349954166743 | validation: 0.5845319717719556]
	TIME [epoch: 1.81 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17474319565267138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17474319565267138 | validation: 0.45654443659115407]
	TIME [epoch: 1.83 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7924844718516308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7924844718516308 | validation: 0.8781938443606583]
	TIME [epoch: 1.82 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27100983110799504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27100983110799504 | validation: 0.27269665253205116]
	TIME [epoch: 1.83 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8488595705827704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8488595705827704 | validation: 0.5271581201189878]
	TIME [epoch: 1.82 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30920085558383553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30920085558383553 | validation: 0.18150777611337757]
	TIME [epoch: 1.83 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8297059585699254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8297059585699254 | validation: 0.6938588224703743]
	TIME [epoch: 1.82 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1138163539568134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1138163539568134 | validation: 0.27740790168924223]
	TIME [epoch: 1.82 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7714728980117751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7714728980117751 | validation: 0.5795472190266735]
	TIME [epoch: 1.82 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14666851191332836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14666851191332836 | validation: 0.1770381708067893]
	TIME [epoch: 1.83 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7750734366794307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7750734366794307 | validation: 0.6513221947914483]
	TIME [epoch: 1.82 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11741533071446685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11741533071446685 | validation: 0.1705110333541919]
	TIME [epoch: 1.83 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7897105907354643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7897105907354643 | validation: 0.8049843429758111]
	TIME [epoch: 1.82 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11140964803902723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11140964803902723 | validation: 0.2856053508032268]
	TIME [epoch: 1.84 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0435184936411042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0435184936411042 | validation: 0.7718822063477457]
	TIME [epoch: 1.82 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1756394702523196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1756394702523196 | validation: 0.288187142024977]
	TIME [epoch: 1.83 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9337388013059368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9337388013059368 | validation: 0.7027253805511087]
	TIME [epoch: 1.82 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2871884008442859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2871884008442859 | validation: 0.3248050091717636]
	TIME [epoch: 1.85 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8204722164756288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8204722164756288 | validation: 0.628095063829958]
	TIME [epoch: 1.83 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23396891778721565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23396891778721565 | validation: 0.2636926967291308]
	TIME [epoch: 1.83 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8110289406287217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8110289406287217 | validation: 0.703572602950173]
	TIME [epoch: 1.82 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1376616979762309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1376616979762309 | validation: 0.18011515956164914]
	TIME [epoch: 1.83 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8140480982998417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8140480982998417 | validation: 0.5845611751756526]
	TIME [epoch: 1.82 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08882524385169077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08882524385169077 | validation: 0.16193096905751903]
	TIME [epoch: 1.83 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8129201136984144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8129201136984144 | validation: 0.7101847381246391]
	TIME [epoch: 1.83 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08851573873112331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08851573873112331 | validation: 0.19384642300025934]
	TIME [epoch: 1.83 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7918145645712003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7918145645712003 | validation: 0.5388584082636673]
	TIME [epoch: 1.81 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10211912774399579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10211912774399579 | validation: 0.20546341499465065]
	TIME [epoch: 1.82 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8293267951168021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8293267951168021 | validation: 0.8054531798637776]
	TIME [epoch: 1.81 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13487181052370084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13487181052370084 | validation: 0.284531967663445]
	TIME [epoch: 1.83 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8224020770740347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8224020770740347 | validation: 0.5597388351304508]
	TIME [epoch: 1.81 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18247444838565194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18247444838565194 | validation: 0.2319143641867406]
	TIME [epoch: 1.82 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7923000023308014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7923000023308014 | validation: 0.6670587539348892]
	TIME [epoch: 1.81 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20602866029420178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20602866029420178 | validation: 0.12471037505569645]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_390.pth
		[batch 1/1] avg loss: 0.7942524074837163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7942524074837163 | validation: 0.5725546806283255]
	TIME [epoch: 1.82 sec]
EPOCH 398/2000:
	Training over batches...
	Model improved!!!
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7756744199711535		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.1467540466694066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7756744199711535 | validation: 0.6353970362711816]
	TIME [epoch: 1.82 sec]
EPOCH 399/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.1467540466694066 | validation: 0.2212407789554922]
	TIME [epoch: 1.82 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7881553137270928		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.1063170029688634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7881553137270928 | validation: 0.5519316889537551]
	TIME [epoch: 1.82 sec]
EPOCH 400/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.1063170029688634 | validation: 0.1636317694241739]
	TIME [epoch: 1.83 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7886116423066086		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.1484384208481894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7886116423066086 | validation: 0.7435637713312561]
	TIME [epoch: 1.81 sec]
	Learning Rate: 0.01
	LOSS [training: 0.1484384208481894 | validation: 0.3982027970621234]
	TIME [epoch: 1.83 sec]
EPOCH 394/2000:
	Training over batches...
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28976848788897286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28976848788897286 | validation: 0.3466298809157715]
	TIME [epoch: 1.83 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8137627584400084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8137627584400084 | validation: 0.5144798357322395]
	TIME [epoch: 1.81 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3306951301146121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3306951301146121 | validation: 0.1900068147930401]
	TIME [epoch: 1.83 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8533361702375777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8533361702375777 | validation: 0.7791349710166071]
	TIME [epoch: 1.81 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1456764986579432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1456764986579432 | validation: 0.2797297373994252]
	TIME [epoch: 1.83 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.815784427400752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.815784427400752 | validation: 0.5602278273264251]
	TIME [epoch: 1.81 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10904668124696482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10904668124696482 | validation: 0.15121318020101177]
	TIME [epoch: 1.83 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7855359364538242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7855359364538242 | validation: 0.6599039520027467]
	TIME [epoch: 1.8 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09167149507093177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09167149507093177 | validation: 0.14344459457897202]
	TIME [epoch: 1.83 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7743677290612471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7743677290612471 | validation: 0.6525471895494043]
	TIME [epoch: 1.81 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09838402622190019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09838402622190019 | validation: 0.19558713445596423]
	TIME [epoch: 1.83 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7999753493004192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7999753493004192 | validation: 0.7598621332641913]
	TIME [epoch: 1.81 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10563596970083106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10563596970083106 | validation: 0.1997345963984122]
	TIME [epoch: 1.83 sec]
		[batch 1/1] avg loss: 0.9435770411598711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9435770411598711 | validation: 0.734135643399876]
	TIME [epoch: 1.81 sec]
EPOCH 408/2000:
	Training over batches...
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9843015142725494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9843015142725494 | validation: 0.6568590640988716]
	TIME [epoch: 1.81 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1168022575529028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1168022575529028 | validation: 0.18707285973434418]
	TIME [epoch: 1.83 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7702602877098719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7702602877098719 | validation: 0.6430863599067855]
	TIME [epoch: 1.81 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1333053804841435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1333053804841435 | validation: 0.23464975051088088]
	TIME [epoch: 1.83 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8277893870529875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8277893870529875 | validation: 0.7229223743316646]
	TIME [epoch: 1.81 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1667566355266616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1667566355266616 | validation: 0.14954263373995716]
	TIME [epoch: 1.83 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8371260822147061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8371260822147061 | validation: 0.6497448383216542]
	TIME [epoch: 1.81 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17399931391143875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17399931391143875 | validation: 0.21921864188134974]
	TIME [epoch: 1.83 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7982055139062808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7982055139062808 | validation: 0.6313443438609232]
	TIME [epoch: 1.82 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14622915317851962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14622915317851962 | validation: 0.23042253714546135]
	TIME [epoch: 1.83 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7860184489822798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7860184489822798 | validation: 0.6487601135936898]
	TIME [epoch: 1.81 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16156969483095768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16156969483095768 | validation: 0.26558685858701075]
	TIME [epoch: 1.83 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7858308803990699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7858308803990699 | validation: 0.587876399785253]
	TIME [epoch: 1.81 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1475888408712906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1475888408712906 | validation: 0.1523651283512705]
	TIME [epoch: 1.84 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7856992288176755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7856992288176755 | validation: 0.7534466927103947]
	TIME [epoch: 1.81 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14037293825017896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14037293825017896 | validation: 0.19816997941724748]
	TIME [epoch: 1.83 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7982495796785029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7982495796785029 | validation: 0.5145846052920144]
	TIME [epoch: 1.81 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1535379807042684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1535379807042684 | validation: 0.15105096182078614]
	TIME [epoch: 1.83 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8916133572733159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8916133572733159 | validation: 0.739026747784816]
	TIME [epoch: 1.82 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16210086741872873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16210086741872873 | validation: 0.15130577446686064]
	TIME [epoch: 1.84 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8037709612672888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8037709612672888 | validation: 0.592240701251124]
	TIME [epoch: 1.81 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12282542910315526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12282542910315526 | validation: 0.3113158928695423]
	TIME [epoch: 1.83 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7826571637886193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7826571637886193 | validation: 0.6155341293091284]
	TIME [epoch: 1.82 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1456194221950678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1456194221950678 | validation: 0.12345908711951559]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_412.pth
	Model improved!!!
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7805852650444406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7805852650444406 | validation: 0.6922596352404677]
	TIME [epoch: 1.82 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1318350083817389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1318350083817389 | validation: 0.1733438034894752]
	TIME [epoch: 1.83 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.813882225334375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.813882225334375 | validation: 0.6828500959082079]
	TIME [epoch: 1.82 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10050541745784224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10050541745784224 | validation: 0.15074515727761825]
	TIME [epoch: 1.83 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8694591394066237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8694591394066237 | validation: 0.6617612442163737]
	TIME [epoch: 1.81 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10120722218181274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10120722218181274 | validation: 0.19143794664240457]
	TIME [epoch: 1.83 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8784801915814001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8784801915814001 | validation: 0.6646625154758323]
	TIME [epoch: 1.81 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11868569185616626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11868569185616626 | validation: 0.3008196108486884]
	TIME [epoch: 1.83 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7831234040687144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7831234040687144 | validation: 0.537667683728867]
	TIME [epoch: 1.81 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17265256763530823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17265256763530823 | validation: 0.23546231340227308]
	TIME [epoch: 1.83 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7471496126606297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7471496126606297 | validation: 0.609291933050685]
	TIME [epoch: 1.81 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15237450940240158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15237450940240158 | validation: 0.1410884865922275]
	TIME [epoch: 1.83 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7266407252505703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7266407252505703 | validation: 0.6146050995402778]
	TIME [epoch: 1.82 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13070316751823607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13070316751823607 | validation: 0.17553823325054646]
	TIME [epoch: 1.83 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.779545781475801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.779545781475801 | validation: 0.596166645497987]
	TIME [epoch: 1.82 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0952276821329641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0952276821329641 | validation: 0.141030289485557]
	TIME [epoch: 1.83 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7953243648368036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7953243648368036 | validation: 0.5801088780060971]
	TIME [epoch: 1.82 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11081189282781939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11081189282781939 | validation: 0.2771363344755668]
	TIME [epoch: 1.83 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7473303106429766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7473303106429766 | validation: 0.7786170277801244]
	TIME [epoch: 1.82 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2148295548270123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2148295548270123 | validation: 0.2678783154814318]
	TIME [epoch: 1.82 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8324557423600542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8324557423600542 | validation: 0.5781214536933665]
	TIME [epoch: 1.82 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29078265091412653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29078265091412653 | validation: 0.19162249649401786]
	TIME [epoch: 1.83 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.735127038525471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.735127038525471 | validation: 0.7041954343081077]
	TIME [epoch: 1.82 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15067966318378787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15067966318378787 | validation: 0.1935791566655471]
	TIME [epoch: 1.83 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.852259206711114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.852259206711114 | validation: 0.6727069045658457]
	TIME [epoch: 1.82 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08283623398298257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08283623398298257 | validation: 0.13298234103019277]
	TIME [epoch: 1.83 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8799358097473339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8799358097473339 | validation: 0.6602094575527173]
	TIME [epoch: 1.82 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08178707039378413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08178707039378413 | validation: 0.19861870283970812]
	TIME [epoch: 1.83 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7402911195221498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7402911195221498 | validation: 0.5618125878485664]
	TIME [epoch: 1.82 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09595330049023745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09595330049023745 | validation: 0.1604237440918675]
	TIME [epoch: 1.83 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7166775712781253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7166775712781253 | validation: 0.5436901783274676]
	TIME [epoch: 1.83 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10799507328405937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10799507328405937 | validation: 0.13614459108049282]
	TIME [epoch: 1.83 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7118594589398184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7118594589398184 | validation: 0.7036395174971671]
	TIME [epoch: 1.82 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1428163036622363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1428163036622363 | validation: 0.20742446209597964]
	TIME [epoch: 1.84 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7087198128366251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7087198128366251 | validation: 0.5915157470122233]
	TIME [epoch: 1.82 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14604572162240204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14604572162240204 | validation: 0.15212080388601434]
	TIME [epoch: 1.83 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7364806953780901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7364806953780901 | validation: 0.5403691353918316]
	TIME [epoch: 1.82 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12979404653090357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12979404653090357 | validation: 0.17033118323455676]
	TIME [epoch: 1.84 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7526704168916019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7526704168916019 | validation: 0.6962168467588299]
	TIME [epoch: 1.82 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09911282671028858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09911282671028858 | validation: 0.15423534826139554]
	TIME [epoch: 1.83 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7708553854784197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7708553854784197 | validation: 0.44754147134972105]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_440.pth
	Model improved!!!
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12602738367444422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12602738367444422 | validation: 0.2894589131081266]
	TIME [epoch: 1.83 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7598438209168853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7598438209168853 | validation: 0.585276284574287]
	TIME [epoch: 1.81 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23846515042974126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23846515042974126 | validation: 0.15693026631034812]
	TIME [epoch: 1.83 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.668510488045182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.668510488045182 | validation: 0.7036031134690282]
	TIME [epoch: 1.81 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18680424664601364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18680424664601364 | validation: 0.13343004006896905]
	TIME [epoch: 1.84 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.714919845598042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.714919845598042 | validation: 0.592791977191401]
	TIME [epoch: 1.81 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09050062482682575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09050062482682575 | validation: 0.18095771527706517]
	TIME [epoch: 1.83 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7915577198008887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7915577198008887 | validation: 0.6781011853217631]
	TIME [epoch: 1.81 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07852671398256604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07852671398256604 | validation: 0.13705030929425485]
	TIME [epoch: 1.83 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8718709131521556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8718709131521556 | validation: 0.5980753076758767]
	TIME [epoch: 1.81 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10840086824009028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10840086824009028 | validation: 0.2779972986806812]
	TIME [epoch: 1.83 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.754181455187321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.754181455187321 | validation: 0.5440695892158791]
	TIME [epoch: 1.81 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12304417989665531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12304417989665531 | validation: 0.16249893582023453]
	TIME [epoch: 1.83 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7172507454363343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7172507454363343 | validation: 0.5438393921046413]
	TIME [epoch: 1.81 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09422401956445732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09422401956445732 | validation: 0.13017264003967108]
	TIME [epoch: 1.83 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6963712680842632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6963712680842632 | validation: 0.6059749995218329]
	TIME [epoch: 1.81 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08668175278892765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08668175278892765 | validation: 0.16703205462677811]
	TIME [epoch: 1.83 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6521387808900622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6521387808900622 | validation: 0.5424885031531321]
	TIME [epoch: 1.81 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10560299006558835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10560299006558835 | validation: 0.10258198943656785]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_442.pth
		[batch 1/1] avg loss: 0.6559122424058376		[learning rate: 0.01]
	Model improved!!!
EPOCH 443/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.6559122424058376 | validation: 0.617300666804019]
	TIME [epoch: 1.81 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13514148338075024		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.6853766247330938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13514148338075024 | validation: 0.2906765920010738]
	TIME [epoch: 1.83 sec]
EPOCH 444/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.6853766247330938 | validation: 0.6149010328087601]
	TIME [epoch: 1.81 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17651973116603942		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.7069152349519209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17651973116603942 | validation: 0.26728945792450126]
	TIME [epoch: 1.83 sec]
EPOCH 445/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.7069152349519209 | validation: 0.8549069249491679]
	TIME [epoch: 1.81 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2091120374709776		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.8421257146581335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2091120374709776 | validation: 0.2073979682417801]
	TIME [epoch: 1.82 sec]
EPOCH 446/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.8421257146581335 | validation: 0.5013822268517942]
	TIME [epoch: 1.81 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18001896155076055		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.6326387739575401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18001896155076055 | validation: 0.15833187780499602]
	TIME [epoch: 1.83 sec]
EPOCH 447/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.6326387739575401 | validation: 0.5197825113401348]
	TIME [epoch: 1.81 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1361786272359683		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.6194731923628727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1361786272359683 | validation: 0.1446915156032787]
	TIME [epoch: 1.83 sec]
	Learning Rate: 0.01
	LOSS [training: 0.6194731923628727 | validation: 0.6704075257143913]
	TIME [epoch: 1.81 sec]
EPOCH 448/2000:
EPOCH 456/2000:
	Training over batches...
	Training over batches...
		[batch 1/1] avg loss: 0.662730273058534		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.1103056264799994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.662730273058534 | validation: 0.5662102045185206]
	TIME [epoch: 1.81 sec]
	Learning Rate: 0.01
	LOSS [training: 0.1103056264799994 | validation: 0.14854812672490672]
	TIME [epoch: 1.82 sec]
EPOCH 457/2000:
	Training over batches...
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7103622637203639		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.08139574212456445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7103622637203639 | validation: 0.4624274531500617]
	TIME [epoch: 1.82 sec]
EPOCH 458/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.08139574212456445 | validation: 0.084913143695729]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_449.pth
	Model improved!!!
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6795465532177184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6795465532177184 | validation: 0.6291801919998603]
	TIME [epoch: 1.82 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05826847233871793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05826847233871793 | validation: 0.09989191169108931]
	TIME [epoch: 1.83 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6160660174337407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6160660174337407 | validation: 0.5059345789624609]
	TIME [epoch: 1.81 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048595655204877145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048595655204877145 | validation: 0.07330790885512573]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_451.pth
		[batch 1/1] avg loss: 0.6365229410568836		[learning rate: 0.01]
	Model improved!!!
EPOCH 452/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.6365229410568836 | validation: 0.7794491857715485]
	TIME [epoch: 1.82 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04729625261446017		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.6869603019231428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04729625261446017 | validation: 0.10031155741026077]
	TIME [epoch: 1.84 sec]
EPOCH 453/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.6869603019231428 | validation: 0.525681121505521]
	TIME [epoch: 1.81 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06348033604795107		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.6763984789981262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06348033604795107 | validation: 0.12249971974383636]
	TIME [epoch: 1.83 sec]
EPOCH 454/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.6763984789981262 | validation: 0.4596296932409524]
	TIME [epoch: 1.81 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1310408667433803		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.6404135942824835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6404135942824835 | validation: 0.5992376678385716]
	TIME [epoch: 1.81 sec]
	Learning Rate: 0.01
	LOSS [training: 0.1310408667433803 | validation: 0.2552454460735099]
	TIME [epoch: 1.84 sec]
EPOCH 464/2000:
	Training over batches...
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6077884134978332		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.25687558131114363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6077884134978332 | validation: 0.4491959012597647]
	TIME [epoch: 1.81 sec]
EPOCH 465/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.25687558131114363 | validation: 0.37381239101748087]
	TIME [epoch: 1.85 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6264253885852906		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.23956935728621417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6264253885852906 | validation: 0.5885723028571265]
	TIME [epoch: 1.81 sec]
EPOCH 466/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.23956935728621417 | validation: 0.1284551971182145]
	TIME [epoch: 1.83 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5933526441141637		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.09583035687316187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5933526441141637 | validation: 0.6111222587426965]
	TIME [epoch: 1.81 sec]
EPOCH 467/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.09583035687316187 | validation: 0.15074841980872875]
	TIME [epoch: 1.83 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5867152578590227		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.05959656835962296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5867152578590227 | validation: 0.5530651559014652]
	TIME [epoch: 1.81 sec]
EPOCH 468/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.05959656835962296 | validation: 0.13671501938724823]
	TIME [epoch: 1.83 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7118045251469473		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.09313680677725873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7118045251469473 | validation: 0.5460534909400728]
	TIME [epoch: 1.81 sec]
EPOCH 469/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.09313680677725873 | validation: 0.198202411975542]
	TIME [epoch: 1.83 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7421921353699115		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.11103756215229796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7421921353699115 | validation: 0.4932772375433858]
	TIME [epoch: 1.81 sec]
EPOCH 470/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.11103756215229796 | validation: 0.14263663232837392]
	TIME [epoch: 1.83 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6899818991372064		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.10131664847724452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6899818991372064 | validation: 0.5674570511151016]
	TIME [epoch: 1.81 sec]
EPOCH 471/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.10131664847724452 | validation: 0.19005837369423514]
	TIME [epoch: 1.83 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6272756162216445		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.12681487934588087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6272756162216445 | validation: 0.7715732387114344]
	TIME [epoch: 1.83 sec]
EPOCH 472/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.12681487934588087 | validation: 0.22100348423332933]
	TIME [epoch: 1.83 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7001855699163889		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.2760468907754614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7001855699163889 | validation: 0.45298006130308655]
	TIME [epoch: 1.81 sec]
EPOCH 473/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.2760468907754614 | validation: 0.22690046495522367]
	TIME [epoch: 1.83 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6224839917100812		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.1834499240368344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6224839917100812 | validation: 0.5111245924770094]
	TIME [epoch: 1.82 sec]
EPOCH 474/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.1834499240368344 | validation: 0.17315441031342463]
	TIME [epoch: 1.83 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6019009303849048		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.09861649743610505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6019009303849048 | validation: 0.7009251206139816]
	TIME [epoch: 1.82 sec]
EPOCH 475/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.09861649743610505 | validation: 0.09591242447761189]
	TIME [epoch: 1.83 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.636043446814408		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.06278120511567595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.636043446814408 | validation: 0.5071457098563427]
	TIME [epoch: 1.82 sec]
EPOCH 476/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.06278120511567595 | validation: 0.1236355950218083]
	TIME [epoch: 1.83 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5971208328042665		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.054719747979617754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5971208328042665 | validation: 0.5945039885370588]
	TIME [epoch: 1.82 sec]
EPOCH 477/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.054719747979617754 | validation: 0.08709849876101865]
	TIME [epoch: 1.82 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.627511793533977		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.054827508755623956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.627511793533977 | validation: 0.4761439856870755]
	TIME [epoch: 1.82 sec]
EPOCH 478/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.054827508755623956 | validation: 0.1046873646664398]
	TIME [epoch: 1.83 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6324862008553286		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.06035149538092763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6324862008553286 | validation: 0.49396537197376883]
	TIME [epoch: 1.81 sec]
EPOCH 479/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.06035149538092763 | validation: 0.11314894195040802]
	TIME [epoch: 1.82 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5628183077098645		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.08710699340465394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5628183077098645 | validation: 0.593178682720898]
	TIME [epoch: 1.82 sec]
EPOCH 480/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.08710699340465394 | validation: 0.1706192470962393]
	TIME [epoch: 1.82 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5637436660622032		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.12929466370867781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5637436660622032 | validation: 0.47929593878475824]
	TIME [epoch: 1.82 sec]
EPOCH 481/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.12929466370867781 | validation: 0.19501035546956602]
	TIME [epoch: 1.81 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5609047332001336		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.17570770286703405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5609047332001336 | validation: 0.5909175897438917]
	TIME [epoch: 1.81 sec]
EPOCH 482/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.17570770286703405 | validation: 0.2289737429674064]
	TIME [epoch: 1.82 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5769011226823154		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.1716772529525763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5769011226823154 | validation: 0.49908671948019756]
	TIME [epoch: 1.83 sec]
EPOCH 483/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.1716772529525763 | validation: 0.17219698020998908]
	TIME [epoch: 1.83 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6242739475768838		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.17566487270528697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6242739475768838 | validation: 0.5229722913194345]
	TIME [epoch: 1.81 sec]
EPOCH 484/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.17566487270528697 | validation: 0.19507131156746402]
	TIME [epoch: 1.83 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5586244897744523		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.13464943190974357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5586244897744523 | validation: 0.763162191234113]
	TIME [epoch: 1.82 sec]
EPOCH 485/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.13464943190974357 | validation: 0.1308704373418742]
	TIME [epoch: 1.82 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7095986535993913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7095986535993913 | validation: 0.5194037928682227]
	TIME [epoch: 1.81 sec]
		[batch 1/1] avg loss: 0.10398489792898201		[learning rate: 0.01]
EPOCH 486/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.10398489792898201 | validation: 0.10995001094587682]
	TIME [epoch: 1.83 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6666368109317867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6666368109317867 | validation: 0.48851700877776233]
	TIME [epoch: 1.81 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0729238066827074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0729238066827074 | validation: 0.11124584156776202]
	TIME [epoch: 1.83 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6851303718752214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6851303718752214 | validation: 0.5385629784584488]
	TIME [epoch: 1.81 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0670303611899934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0670303611899934 | validation: 0.08422181245899454]
	TIME [epoch: 1.84 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6364001721618721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6364001721618721 | validation: 0.5844853636341013]
	TIME [epoch: 1.82 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05722146079940724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05722146079940724 | validation: 0.10659346382766566]
	TIME [epoch: 1.83 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5986537931135288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5986537931135288 | validation: 0.49640482648621165]
	TIME [epoch: 1.81 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05292108881525074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05292108881525074 | validation: 0.08484782520431682]
	TIME [epoch: 1.84 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5853282008882282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5853282008882282 | validation: 0.5379223371000601]
	TIME [epoch: 1.82 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06527926524901326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06527926524901326 | validation: 0.14511966653961686]
	TIME [epoch: 1.83 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5781563557375062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5781563557375062 | validation: 0.7295214668390702]
	TIME [epoch: 1.81 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09672283784571907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09672283784571907 | validation: 0.21314675177148842]
	TIME [epoch: 1.83 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6509800375959975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6509800375959975 | validation: 0.4919121069302304]
	TIME [epoch: 1.81 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19683238164644132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19683238164644132 | validation: 0.2662309498820531]
	TIME [epoch: 1.82 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5257294645050551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5257294645050551 | validation: 0.49813157956343085]
	TIME [epoch: 1.82 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22274505742258688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22274505742258688 | validation: 0.16307501330216495]
	TIME [epoch: 1.83 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5277662427041616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5277662427041616 | validation: 0.5553347387672366]
	TIME [epoch: 1.82 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14200689962897972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14200689962897972 | validation: 0.15543378345722458]
	TIME [epoch: 1.82 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5217707151810845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5217707151810845 | validation: 0.45509647316293456]
	TIME [epoch: 1.82 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10539108447674277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10539108447674277 | validation: 0.26592401368010027]
	TIME [epoch: 1.82 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5575310035483587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5575310035483587 | validation: 0.5475988847345938]
	TIME [epoch: 1.82 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12109085527403082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12109085527403082 | validation: 0.10702439952205972]
	TIME [epoch: 1.83 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.513609227304312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.513609227304312 | validation: 0.6504566460731455]
	TIME [epoch: 1.82 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08880831635206338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08880831635206338 | validation: 0.18706912976161552]
	TIME [epoch: 1.83 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5783752834642059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5783752834642059 | validation: 0.4725936231125877]
	TIME [epoch: 1.82 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08584040487936477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08584040487936477 | validation: 0.11129992548989445]
	TIME [epoch: 1.82 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5880623471439698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5880623471439698 | validation: 0.5489250980612211]
	TIME [epoch: 1.82 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08357235243098877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08357235243098877 | validation: 0.08931954197753861]
	TIME [epoch: 1.82 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5629630501796179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5629630501796179 | validation: 0.674916577396195]
	TIME [epoch: 1.82 sec]
		[batch 1/1] avg loss: 0.08749288372911976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08749288372911976 | validation: 0.1794750015750979]
	TIME [epoch: 1.83 sec]
EPOCH 492/2000:
	Training over batches...
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12660266078966045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12660266078966045 | validation: 0.10445413208131865]
	TIME [epoch: 1.82 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14040971223726786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14040971223726786 | validation: 0.12156256311145629]
	TIME [epoch: 1.83 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06984375171064165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06984375171064165 | validation: 0.06270139451845107]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_494.pth
	Model improved!!!
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04684202628029884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04684202628029884 | validation: 0.09655012971967457]
	TIME [epoch: 1.82 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05104202284228228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05104202284228228 | validation: 0.13637478384295793]
	TIME [epoch: 1.81 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10469126986379174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10469126986379174 | validation: 0.34033070784447744]
	TIME [epoch: 1.83 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2612439766071883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2612439766071883 | validation: 0.25189139657722526]
	TIME [epoch: 1.82 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27157579572494306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27157579572494306 | validation: 0.10166998401668939]
	TIME [epoch: 1.83 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07624262427269826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07624262427269826 | validation: 0.12494482432759231]
	TIME [epoch: 1.82 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6100635159372666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6100635159372666 | validation: 0.5563676224200808]
	TIME [epoch: 44.1 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5794906716348447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5794906716348447 | validation: 0.557645152275155]
	TIME [epoch: 3.59 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5280372365319194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5280372365319194 | validation: 0.496903810822933]
	TIME [epoch: 3.58 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5301245571560507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5301245571560507 | validation: 0.7711405155360961]
	TIME [epoch: 3.6 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052241437131909256		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.7747436333872034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7747436333872034 | validation: 0.5777547300439275]
	TIME [epoch: 3.6 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5222196172141897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5222196172141897 | validation: 0.5254906420793234]
	TIME [epoch: 3.6 sec]
EPOCH 507/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.052241437131909256 | validation: 0.10552494263472481]
	TIME [epoch: 45.6 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6746595606733569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6746595606733569 | validation: 0.4435990893036344]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_507.pth
		[batch 1/1] avg loss: 0.07282862510126376		[learning rate: 0.01]
	Model improved!!!
EPOCH 508/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.07282862510126376 | validation: 0.1324476041538549]
	TIME [epoch: 3.62 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6955592451678422		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.06223924657226447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6955592451678422 | validation: 0.4784401346571813]
	TIME [epoch: 3.6 sec]
EPOCH 509/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.06223924657226447 | validation: 0.06503401034895809]
	TIME [epoch: 3.62 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5988938764788633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5988938764788633 | validation: 0.5905111941690693]
	TIME [epoch: 3.59 sec]
		[batch 1/1] avg loss: 0.044783549322942484		[learning rate: 0.01]
EPOCH 510/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.044783549322942484 | validation: 0.08236131623560415]
	TIME [epoch: 3.61 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5823654221655205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5823654221655205 | validation: 0.5906423707917807]
	TIME [epoch: 3.59 sec]
		[batch 1/1] avg loss: 0.04014739750171548		[learning rate: 0.01]
EPOCH 511/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.04014739750171548 | validation: 0.06252407805692262]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_505.pth
	Model improved!!!
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5381656710712123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5381656710712123 | validation: 0.48379880407828274]
	TIME [epoch: 3.59 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0391863474599024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0391863474599024 | validation: 0.09839638232954678]
	TIME [epoch: 3.63 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6318420330429662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6318420330429662 | validation: 0.5309681025110395]
	TIME [epoch: 3.6 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05389214884457576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05389214884457576 | validation: 0.10404741311727408]
	TIME [epoch: 3.63 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.578888262599014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.578888262599014 | validation: 0.7532326937931711]
	TIME [epoch: 3.59 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10986956814002033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10986956814002033 | validation: 0.2868870426024855]
	TIME [epoch: 3.61 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.708228554052128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.708228554052128 | validation: 0.7045051026170935]
	TIME [epoch: 3.59 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2368812061272181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2368812061272181 | validation: 0.2612355012292695]
	TIME [epoch: 3.61 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6048771892472345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6048771892472345 | validation: 0.5072388774443557]
	TIME [epoch: 3.59 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23597734681147448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23597734681147448 | validation: 0.19003041967099196]
	TIME [epoch: 3.6 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.503112936188194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.503112936188194 | validation: 0.41009141644669883]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_516.pth
	Model improved!!!
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19142238525505628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19142238525505628 | validation: 0.32792016087845105]
	TIME [epoch: 3.61 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.582905593619575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.582905593619575 | validation: 0.5361699088315358]
	TIME [epoch: 3.59 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2127393956843096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2127393956843096 | validation: 0.1187953145039181]
	TIME [epoch: 3.61 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5334154232587035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5334154232587035 | validation: 0.5926365400112473]
	TIME [epoch: 3.6 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10245054072531151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10245054072531151 | validation: 0.1477403536926483]
	TIME [epoch: 3.61 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5701011925623463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5701011925623463 | validation: 0.47749033180207223]
	TIME [epoch: 3.6 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10718424068256223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10718424068256223 | validation: 0.13304346010173634]
	TIME [epoch: 3.61 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5295802015350073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5295802015350073 | validation: 0.4409481714276642]
	TIME [epoch: 3.6 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06512162532548992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06512162532548992 | validation: 0.07642701898485188]
	TIME [epoch: 3.61 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5022359678882586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5022359678882586 | validation: 0.4613950798490953]
	TIME [epoch: 3.59 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04036567945131663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04036567945131663 | validation: 0.0842795072873454]
	TIME [epoch: 3.61 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4916557191559192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4916557191559192 | validation: 0.4586551758186277]
	TIME [epoch: 3.6 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04825024119012134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04825024119012134 | validation: 0.10062081740748391]
	TIME [epoch: 3.61 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4961274471146642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4961274471146642 | validation: 0.5373064032914601]
	TIME [epoch: 3.59 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05350515732975284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05350515732975284 | validation: 0.10473277707769008]
	TIME [epoch: 3.61 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5068382204012376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5068382204012376 | validation: 0.49996300958108597]
	TIME [epoch: 3.6 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06365418493071655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06365418493071655 | validation: 0.0825044637252709]
	TIME [epoch: 3.61 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6096997675645315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6096997675645315 | validation: 0.8058433533220639]
	TIME [epoch: 3.59 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08256006744251151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08256006744251151 | validation: 0.10856190737114113]
	TIME [epoch: 3.62 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7126346962268636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7126346962268636 | validation: 0.5813079136627846]
	TIME [epoch: 3.59 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08506816626919565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08506816626919565 | validation: 0.13994018109577164]
	TIME [epoch: 3.62 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5540482347669642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5540482347669642 | validation: 0.48186582296219393]
	TIME [epoch: 3.59 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09951256769856558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09951256769856558 | validation: 0.14998592065210317]
	TIME [epoch: 3.61 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4974366939586987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4974366939586987 | validation: 0.48491161710036385]
	TIME [epoch: 3.6 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0930468950486843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0930468950486843 | validation: 0.16731860925001646]
	TIME [epoch: 3.61 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8528287848173558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8528287848173558 | validation: 0.7316415651475071]
	TIME [epoch: 3.59 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08408841852171804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08408841852171804 | validation: 0.08721901872513795]
	TIME [epoch: 3.61 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8292627249119042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8292627249119042 | validation: 0.7834741539531267]
	TIME [epoch: 3.6 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12937898756457858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12937898756457858 | validation: 0.22679375178656525]
	TIME [epoch: 3.61 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8269759784623258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8269759784623258 | validation: 0.5546922513933651]
	TIME [epoch: 3.6 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18569492740562524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18569492740562524 | validation: 0.2645657784765411]
	TIME [epoch: 3.61 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8587016996245581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8587016996245581 | validation: 0.7162905662110859]
	TIME [epoch: 3.61 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21281935392989965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21281935392989965 | validation: 0.1669515719859893]
	TIME [epoch: 3.61 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8161525206935926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8161525206935926 | validation: 0.6442795954417437]
	TIME [epoch: 3.59 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12563771599764398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12563771599764398 | validation: 0.12245556127637634]
	TIME [epoch: 3.61 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8092888209318291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8092888209318291 | validation: 0.644102622763483]
	TIME [epoch: 3.59 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06537644442956825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06537644442956825 | validation: 0.06646397910739058]
	TIME [epoch: 3.61 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8091392934729129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8091392934729129 | validation: 0.6573862149062516]
	TIME [epoch: 3.59 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05836926677168371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05836926677168371 | validation: 0.09263212055974372]
	TIME [epoch: 3.61 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8077179500533643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8077179500533643 | validation: 0.5632224053756011]
	TIME [epoch: 3.6 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05180625540863952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05180625540863952 | validation: 0.048176388078757994]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_531.pth
	Model improved!!!
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8022951036392465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8022951036392465 | validation: 0.785009916285015]
	TIME [epoch: 3.59 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057962309051406126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057962309051406126 | validation: 0.09885119148363353]
	TIME [epoch: 3.61 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.82871592673777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.82871592673777 | validation: 0.47685186995419715]
	TIME [epoch: 3.6 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06967167531168256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06967167531168256 | validation: 0.10833683763506421]
	TIME [epoch: 3.61 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8513518657030145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8513518657030145 | validation: 0.646193751223669]
	TIME [epoch: 3.59 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0840799207152093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0840799207152093 | validation: 0.11855798278223616]
	TIME [epoch: 3.61 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7686136883211265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7686136883211265 | validation: 0.5850587543934392]
	TIME [epoch: 3.59 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09535161889100133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09535161889100133 | validation: 0.2074157498384478]
	TIME [epoch: 3.62 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.704211742714474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.704211742714474 | validation: 0.4094167326765347]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_541.pth
	Model improved!!!
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08833230247528459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08833230247528459 | validation: 0.14627002493865798]
	TIME [epoch: 3.6 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6205661653922298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6205661653922298 | validation: 0.6358858136897138]
	TIME [epoch: 3.59 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09316483188440274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09316483188440274 | validation: 0.15989302280588144]
	TIME [epoch: 3.6 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6450652548788355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6450652548788355 | validation: 0.52117625804621]
	TIME [epoch: 3.59 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13787960884223097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13787960884223097 | validation: 0.19008152276310453]
	TIME [epoch: 3.59 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5924171242538457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5924171242538457 | validation: 0.6467826958180491]
	TIME [epoch: 3.6 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.190754242276578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.190754242276578 | validation: 0.16748482000219522]
	TIME [epoch: 3.61 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6728294170533954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6728294170533954 | validation: 0.8535289706765359]
	TIME [epoch: 3.6 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14973322896315067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14973322896315067 | validation: 0.21486496343161698]
	TIME [epoch: 3.6 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8051480686707245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8051480686707245 | validation: 0.5968018529913268]
	TIME [epoch: 3.6 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12399493531565764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12399493531565764 | validation: 0.1179127738906251]
	TIME [epoch: 3.6 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5699964715890148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5699964715890148 | validation: 0.419690688213916]
	TIME [epoch: 3.59 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07269606458930822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07269606458930822 | validation: 0.09108764271274049]
	TIME [epoch: 3.61 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.614308099595903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.614308099595903 | validation: 0.40829133114253446]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_548.pth
	Model improved!!!
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04804391041075574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04804391041075574 | validation: 0.04694566410017839]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_543.pth
	Model improved!!!
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5831989813698565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5831989813698565 | validation: 0.4580317284740061]
	TIME [epoch: 3.58 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0551027842845354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0551027842845354 | validation: 0.08633231400240958]
	TIME [epoch: 3.62 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5240802806709032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5240802806709032 | validation: 0.533529944506545]
	TIME [epoch: 3.59 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.068519671613345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.068519671613345 | validation: 0.07367484588695787]
	TIME [epoch: 3.61 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5147454056129481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5147454056129481 | validation: 0.443473159338676]
	TIME [epoch: 3.58 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0886651879433504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0886651879433504 | validation: 0.0702564735610655]
	TIME [epoch: 3.62 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5303329013287698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5303329013287698 | validation: 0.6426879682666473]
	TIME [epoch: 3.59 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05715801369154054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05715801369154054 | validation: 0.06469970002485866]
	TIME [epoch: 3.62 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5850832061120316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5850832061120316 | validation: 0.5286022863898406]
	TIME [epoch: 3.59 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04061979895365788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04061979895365788 | validation: 0.06018766697235112]
	TIME [epoch: 3.62 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.535150266618368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.535150266618368 | validation: 0.4776963591397485]
	TIME [epoch: 3.59 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04739687325460131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04739687325460131 | validation: 0.19854325959516533]
	TIME [epoch: 3.6 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5592187663138125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5592187663138125 | validation: 0.5402123565092046]
	TIME [epoch: 3.59 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10519678430898921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10519678430898921 | validation: 0.19754383700974174]
	TIME [epoch: 3.61 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.557177450624359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.557177450624359 | validation: 0.536458448930206]
	TIME [epoch: 3.61 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14526601043886364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14526601043886364 | validation: 0.17635741670329017]
	TIME [epoch: 3.6 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5270585776594879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5270585776594879 | validation: 0.41528427476907437]
	TIME [epoch: 3.61 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1418454517775091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1418454517775091 | validation: 0.13721414151124078]
	TIME [epoch: 3.61 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5268731119604382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5268731119604382 | validation: 0.47976443854280126]
	TIME [epoch: 3.6 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09961865559622267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09961865559622267 | validation: 0.12321464899483255]
	TIME [epoch: 3.61 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5023059796431316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5023059796431316 | validation: 0.48813156749567344]
	TIME [epoch: 3.59 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.128071971285387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.128071971285387 | validation: 0.26406946862776387]
	TIME [epoch: 3.61 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5007574898884145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5007574898884145 | validation: 0.5072879388643052]
	TIME [epoch: 3.6 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15438359043108882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15438359043108882 | validation: 0.13904714126966009]
	TIME [epoch: 3.61 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5109502616408694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5109502616408694 | validation: 0.4678667935014265]
	TIME [epoch: 3.59 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1161830810118527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1161830810118527 | validation: 0.13723313758544614]
	TIME [epoch: 3.61 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5769378082164146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5769378082164146 | validation: 0.6356058367750053]
	TIME [epoch: 3.6 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14853514360096262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14853514360096262 | validation: 0.1271947552960306]
	TIME [epoch: 3.61 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5664371681716128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5664371681716128 | validation: 0.48496761532380295]
	TIME [epoch: 3.59 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09220017423337584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09220017423337584 | validation: 0.08818099229764136]
	TIME [epoch: 3.6 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.524541448065523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.524541448065523 | validation: 0.4464695612914562]
	TIME [epoch: 3.6 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05621892755613361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05621892755613361 | validation: 0.1004422970618652]
	TIME [epoch: 3.61 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5189113378008696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5189113378008696 | validation: 0.6647807626210188]
	TIME [epoch: 3.59 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07119170541694908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07119170541694908 | validation: 0.0793416608127312]
	TIME [epoch: 3.62 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6272063289029123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6272063289029123 | validation: 0.6031569492223582]
	TIME [epoch: 3.6 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061315939289866145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061315939289866145 | validation: 0.07740486613344574]
	TIME [epoch: 3.63 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5366961949595056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5366961949595056 | validation: 0.3917567090036269]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_567.pth
		[batch 1/1] avg loss: 0.046520195312536636		[learning rate: 0.01]
	Model improved!!!
EPOCH 568/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.046520195312536636 | validation: 0.07925557034330116]
	TIME [epoch: 3.61 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.531613435068735		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.060430436510587146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.531613435068735 | validation: 0.48297175010663934]
	TIME [epoch: 3.59 sec]
EPOCH 569/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.060430436510587146 | validation: 0.12289684982234461]
	TIME [epoch: 3.6 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4735293771828702		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.08356689333423584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4735293771828702 | validation: 0.4952269376581575]
	TIME [epoch: 3.6 sec]
EPOCH 570/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.08356689333423584 | validation: 0.16861866357709035]
	TIME [epoch: 3.61 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.477272525741044		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.13512617102475433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.477272525741044 | validation: 0.4993014930375297]
	TIME [epoch: 3.6 sec]
EPOCH 571/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.13512617102475433 | validation: 0.16451621544046163]
	TIME [epoch: 3.6 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4803141183548131		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.16281959788979006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4803141183548131 | validation: 0.4288580835471925]
	TIME [epoch: 3.6 sec]
EPOCH 572/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.16281959788979006 | validation: 0.15192921990091363]
	TIME [epoch: 3.62 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6018022021187702		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.10988787896623854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6018022021187702 | validation: 0.5311540619255489]
	TIME [epoch: 3.59 sec]
EPOCH 573/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.10988787896623854 | validation: 0.08100755322462581]
	TIME [epoch: 3.6 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6647043901103183		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.06636513009352517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6647043901103183 | validation: 0.7526484194896794]
	TIME [epoch: 3.6 sec]
EPOCH 574/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.06636513009352517 | validation: 0.1511788509048825]
	TIME [epoch: 3.62 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7403368033757071		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.06850229367065175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7403368033757071 | validation: 0.5728526775218636]
	TIME [epoch: 3.59 sec]
EPOCH 575/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.06850229367065175 | validation: 0.0905148441377861]
	TIME [epoch: 3.61 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5292800065729396		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.08594382623269269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5292800065729396 | validation: 0.4629928084635399]
	TIME [epoch: 3.59 sec]
EPOCH 576/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.08594382623269269 | validation: 0.1389626539163839]
	TIME [epoch: 3.61 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5983190733419618		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.10586937613952742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5983190733419618 | validation: 0.4624459120356601]
	TIME [epoch: 3.6 sec]
EPOCH 577/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.10586937613952742 | validation: 0.10180596751606047]
	TIME [epoch: 3.61 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5864978026037152		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.11438109730535018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5864978026037152 | validation: 0.4649678448990111]
	TIME [epoch: 3.59 sec]
EPOCH 578/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.11438109730535018 | validation: 0.07906720622788069]
	TIME [epoch: 3.61 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47070873444474004		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.06082565198678571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47070873444474004 | validation: 0.4514092747590547]
	TIME [epoch: 3.59 sec]
EPOCH 579/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.06082565198678571 | validation: 0.15013561476673326]
	TIME [epoch: 3.61 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4834363712098555		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.07220949769734909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4834363712098555 | validation: 0.4579084196734211]
	TIME [epoch: 3.6 sec]
EPOCH 580/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.07220949769734909 | validation: 0.07832031575909419]
	TIME [epoch: 3.62 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48615887292753096		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.05270866790682661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48615887292753096 | validation: 0.426775806895195]
	TIME [epoch: 3.6 sec]
EPOCH 581/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.05270866790682661 | validation: 0.0927880675530605]
	TIME [epoch: 3.63 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.482666494419735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.482666494419735 | validation: 0.7202081291092397]
	TIME [epoch: 3.6 sec]
		[batch 1/1] avg loss: 0.041072248880965424		[learning rate: 0.01]
EPOCH 582/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.041072248880965424 | validation: 0.055735066440708296]
	TIME [epoch: 3.62 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6945524515335296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6945524515335296 | validation: 0.7281651753533889]
	TIME [epoch: 3.6 sec]
		[batch 1/1] avg loss: 0.03288360199557034		[learning rate: 0.01]
EPOCH 583/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.03288360199557034 | validation: 0.054107292046816684]
	TIME [epoch: 3.62 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6491761646976335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6491761646976335 | validation: 0.4666524067384911]
	TIME [epoch: 3.61 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033800085197394594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033800085197394594 | validation: 0.06852888085625684]
	TIME [epoch: 3.62 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47223457378367095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47223457378367095 | validation: 0.39345414021036507]
	TIME [epoch: 3.6 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047196177328691746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047196177328691746 | validation: 0.13351755126745762]
	TIME [epoch: 3.62 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5399032343259488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5399032343259488 | validation: 0.4299286692444258]
	TIME [epoch: 3.6 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1279017098815725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1279017098815725 | validation: 0.3090672905186354]
	TIME [epoch: 3.62 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6160689102457056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6160689102457056 | validation: 0.43655714755469754]
	TIME [epoch: 3.6 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2863931245106176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2863931245106176 | validation: 0.21335695921729655]
	TIME [epoch: 3.62 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5934335341220855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5934335341220855 | validation: 0.6746245332830261]
	TIME [epoch: 3.59 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2043799992239406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2043799992239406 | validation: 0.14261845660115063]
	TIME [epoch: 3.61 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6312066689394122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6312066689394122 | validation: 0.6027360197886096]
	TIME [epoch: 3.6 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14895454306678077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14895454306678077 | validation: 0.10642297995171193]
	TIME [epoch: 3.61 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5904440930898674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5904440930898674 | validation: 0.4930243460483034]
	TIME [epoch: 3.6 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09319782416615631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09319782416615631 | validation: 0.10144234820570197]
	TIME [epoch: 3.61 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.480423714715717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.480423714715717 | validation: 0.4729893935697684]
	TIME [epoch: 3.59 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053977377797207494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053977377797207494 | validation: 0.06478334977671311]
	TIME [epoch: 3.61 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4986875850510136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4986875850510136 | validation: 0.4280570647064608]
	TIME [epoch: 3.6 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04168753211782189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04168753211782189 | validation: 0.07632402010354117]
	TIME [epoch: 3.62 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5608637376673666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5608637376673666 | validation: 0.43023380734122585]
	TIME [epoch: 3.59 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0440921186352893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0440921186352893 | validation: 0.06834105144529763]
	TIME [epoch: 3.62 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49567313838943305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49567313838943305 | validation: 0.46442879931995773]
	TIME [epoch: 3.6 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04131094169335023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04131094169335023 | validation: 0.06976244880468195]
	TIME [epoch: 3.62 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48889008184481947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48889008184481947 | validation: 0.4408438596906944]
	TIME [epoch: 3.6 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037718978899777725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037718978899777725 | validation: 0.06570108005326257]
	TIME [epoch: 3.62 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5034859574887293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5034859574887293 | validation: 0.3886905354624693]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_595.pth
		[batch 1/1] avg loss: 0.0423011143990092		[learning rate: 0.01]
	Model improved!!!
EPOCH 596/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.0423011143990092 | validation: 0.07471409206101347]
	TIME [epoch: 3.62 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4866325686100247		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.05881842431055228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4866325686100247 | validation: 0.5989526337320318]
	TIME [epoch: 3.6 sec]
EPOCH 597/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.05881842431055228 | validation: 0.10254351346772231]
	TIME [epoch: 3.61 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5326879994349947		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.08859017717736781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5326879994349947 | validation: 0.5356593687595114]
	TIME [epoch: 3.6 sec]
EPOCH 598/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.08859017717736781 | validation: 0.10341489661159838]
	TIME [epoch: 3.61 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48746214611690336		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.10102601095428923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48746214611690336 | validation: 0.4979055097085258]
	TIME [epoch: 3.59 sec]
EPOCH 599/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.10102601095428923 | validation: 0.08852882749481332]
	TIME [epoch: 3.61 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5251917661321209		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.06964096613903023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5251917661321209 | validation: 0.5299060641364383]
	TIME [epoch: 3.59 sec]
EPOCH 600/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.06964096613903023 | validation: 0.14926650027008465]
	TIME [epoch: 3.62 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6157019222497531		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.08443352300607122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6157019222497531 | validation: 0.5467515654844414]
	TIME [epoch: 3.59 sec]
	Learning Rate: 0.01
	LOSS [training: 0.08443352300607122 | validation: 0.23257078706142797]
	TIME [epoch: 3.61 sec]
EPOCH 596/2000:
	Training over batches...
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20354379401793993		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.5262266089296413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20354379401793993 | validation: 0.20916543425732775]
	TIME [epoch: 3.61 sec]
EPOCH 597/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.5262266089296413 | validation: 0.42129621222758246]
	TIME [epoch: 3.58 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21732097134963596		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.48678292548903374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21732097134963596 | validation: 0.11007318218558573]
	TIME [epoch: 3.61 sec]
EPOCH 598/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.48678292548903374 | validation: 0.44888643171743187]
	TIME [epoch: 3.58 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09846166065334717		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.45696669408414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09846166065334717 | validation: 0.06562102800918464]
	TIME [epoch: 3.61 sec]
EPOCH 599/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.45696669408414 | validation: 0.495887261155034]
	TIME [epoch: 3.58 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03833395924288251		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.46114874960233776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03833395924288251 | validation: 0.1626103382167536]
	TIME [epoch: 3.61 sec]
EPOCH 600/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.46114874960233776 | validation: 0.4069143740015186]
	TIME [epoch: 3.58 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07532159837131679		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.5140251165124985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07532159837131679 | validation: 0.14209045823679026]
	TIME [epoch: 3.61 sec]
	Learning Rate: 0.01
	LOSS [training: 0.5140251165124985 | validation: 0.6267787427178093]
	TIME [epoch: 3.58 sec]
EPOCH 606/2000:
	Training over batches...
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5837540092261156		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.10234434078467794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5837540092261156 | validation: 0.688954433998]
	TIME [epoch: 3.59 sec]
EPOCH 607/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.10234434078467794 | validation: 0.16678060336846504]
	TIME [epoch: 3.62 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.621114084134058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.621114084134058 | validation: 0.5590962336306109]
	TIME [epoch: 3.6 sec]
		[batch 1/1] avg loss: 0.08378588972815361		[learning rate: 0.01]
EPOCH 608/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.08378588972815361 | validation: 0.06524079643955609]
	TIME [epoch: 3.64 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49655285577005887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49655285577005887 | validation: 0.41405167245101604]
	TIME [epoch: 3.61 sec]
		[batch 1/1] avg loss: 0.08454774231808594		[learning rate: 0.01]
EPOCH 609/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.08454774231808594 | validation: 0.12622032424620097]
	TIME [epoch: 3.64 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4752653200958739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4752653200958739 | validation: 0.48697383250073045]
	TIME [epoch: 3.6 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09552832584685779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09552832584685779 | validation: 0.09545469543600398]
	TIME [epoch: 3.63 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5025349420786662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5025349420786662 | validation: 0.5699436780153109]
	TIME [epoch: 3.6 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08004639121673136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08004639121673136 | validation: 0.09926788283839724]
	TIME [epoch: 3.62 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5793093830847396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5793093830847396 | validation: 0.6218701023003199]
	TIME [epoch: 3.6 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07171058285386522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07171058285386522 | validation: 0.13555707610229903]
	TIME [epoch: 3.63 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.573524669687239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.573524669687239 | validation: 0.4935604719431037]
	TIME [epoch: 3.59 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08951159531002652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08951159531002652 | validation: 0.10495015233534391]
	TIME [epoch: 3.63 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5048268940419957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5048268940419957 | validation: 0.37782659062459295]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_613.pth
		[batch 1/1] avg loss: 0.12178040356671174		[learning rate: 0.01]
	Model improved!!!
EPOCH 614/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.12178040356671174 | validation: 0.09536135946073787]
	TIME [epoch: 3.62 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5496326221174133		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.092033163810583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5496326221174133 | validation: 0.37293722440031407]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_614.pth
	Learning Rate: 0.01
	LOSS [training: 0.092033163810583 | validation: 0.0936361555944544]
	TIME [epoch: 3.62 sec]
EPOCH 610/2000:
	Training over batches...
	Model improved!!!
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08500143497610846		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.5298740905229996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08500143497610846 | validation: 0.06745936963512872]
	TIME [epoch: 3.63 sec]
EPOCH 611/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.5298740905229996 | validation: 0.4373833261782215]
	TIME [epoch: 3.6 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0440629899312072		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.4534422915762798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0440629899312072 | validation: 0.07916707183226679]
	TIME [epoch: 3.63 sec]
EPOCH 612/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.4534422915762798 | validation: 0.511711775771088]
	TIME [epoch: 3.6 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03383155084950034		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.47356935285744817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03383155084950034 | validation: 0.0499269562379874]
	TIME [epoch: 3.62 sec]
EPOCH 613/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.47356935285744817 | validation: 0.4236165053154022]
	TIME [epoch: 3.6 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033881959746720224		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.4621844985813786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033881959746720224 | validation: 0.09429056822737977]
	TIME [epoch: 3.62 sec]
EPOCH 614/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.4621844985813786 | validation: 0.42703850177337827]
	TIME [epoch: 3.6 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04810139181743152		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.63481053396315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04810139181743152 | validation: 0.0984497847958996]
	TIME [epoch: 3.63 sec]
EPOCH 615/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.63481053396315 | validation: 0.46440477318258433]
	TIME [epoch: 3.6 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06852254323673124		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.6529722219516143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06852254323673124 | validation: 0.12136035013572877]
	TIME [epoch: 3.63 sec]
EPOCH 616/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.6529722219516143 | validation: 0.5575981517783458]
	TIME [epoch: 3.61 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08265374810621001		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.629193469850813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08265374810621001 | validation: 0.11735591787052227]
	TIME [epoch: 3.64 sec]
EPOCH 617/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.629193469850813 | validation: 0.5297531087458323]
	TIME [epoch: 3.6 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10130774271773732		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.5524014118302061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10130774271773732 | validation: 0.12184936482604125]
	TIME [epoch: 3.63 sec]
EPOCH 618/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.5524014118302061 | validation: 0.5438563947966126]
	TIME [epoch: 3.6 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15087232290152072		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.47820285168634186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15087232290152072 | validation: 0.2417470213947259]
	TIME [epoch: 3.63 sec]
EPOCH 619/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.47820285168634186 | validation: 0.44541674941666765]
	TIME [epoch: 3.6 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19026722831534149		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.4647272079339557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19026722831534149 | validation: 0.15916843928185861]
	TIME [epoch: 3.61 sec]
EPOCH 620/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.4647272079339557 | validation: 0.4101829373031122]
	TIME [epoch: 3.6 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13278314944903333		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.4995266165419225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13278314944903333 | validation: 0.11580601373081029]
	TIME [epoch: 3.62 sec]
EPOCH 621/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.4995266165419225 | validation: 0.4601240683203436]
	TIME [epoch: 3.6 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05861569720564397		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.47725966390448304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05861569720564397 | validation: 0.04999580893844553]
	TIME [epoch: 3.62 sec]
EPOCH 622/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.47725966390448304 | validation: 0.4837739404615229]
	TIME [epoch: 3.61 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04579433101670282		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.48520415941940526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04579433101670282 | validation: 0.09703842074226827]
	TIME [epoch: 3.62 sec]
EPOCH 623/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.48520415941940526 | validation: 0.4386487246209167]
	TIME [epoch: 3.6 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05923663744146157		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.45259898245302965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05923663744146157 | validation: 0.08157194887396166]
	TIME [epoch: 3.62 sec]
EPOCH 624/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.45259898245302965 | validation: 0.38030297301700455]
	TIME [epoch: 3.6 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08540520558285891		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.5079094474687189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08540520558285891 | validation: 0.09353626052642489]
	TIME [epoch: 3.63 sec]
	Learning Rate: 0.01
	LOSS [training: 0.5079094474687189 | validation: 0.45690126838266903]
	TIME [epoch: 3.6 sec]
EPOCH 625/2000:
	Training over batches...
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08910059977653319		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.4683913201050523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4683913201050523 | validation: 0.5238417634406926]
	TIME [epoch: 3.6 sec]
	Learning Rate: 0.01
	LOSS [training: 0.08910059977653319 | validation: 0.1461204289157929]
	TIME [epoch: 3.62 sec]
EPOCH 631/2000:
	Training over batches...
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5236518746255199		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.07870777168195894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5236518746255199 | validation: 0.5451034366844058]
	TIME [epoch: 3.6 sec]
	Learning Rate: 0.01
	LOSS [training: 0.07870777168195894 | validation: 0.055175044215985784]
	TIME [epoch: 3.63 sec]
EPOCH 627/2000:
EPOCH 632/2000:
	Training over batches...
	Training over batches...
		[batch 1/1] avg loss: 0.5577766012960242		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.04931767545700127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5577766012960242 | validation: 0.4432087153654954]
	TIME [epoch: 3.6 sec]
	Learning Rate: 0.01
	LOSS [training: 0.04931767545700127 | validation: 0.07769151256379465]
	TIME [epoch: 3.62 sec]
EPOCH 633/2000:
	Training over batches...
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.522163219611426		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.04100091347870669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.522163219611426 | validation: 0.5629195843115056]
	TIME [epoch: 3.61 sec]
EPOCH 634/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.04100091347870669 | validation: 0.06725999616313064]
	TIME [epoch: 3.63 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5114295668367282		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.04763686180296288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5114295668367282 | validation: 0.4623547819247109]
	TIME [epoch: 3.6 sec]
EPOCH 635/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.04763686180296288 | validation: 0.11891860154052032]
	TIME [epoch: 3.64 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45388498097989854		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.06188142708719974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45388498097989854 | validation: 0.3711141417012187]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_635.pth
	Learning Rate: 0.01
	LOSS [training: 0.06188142708719974 | validation: 0.09690957908925481]
	TIME [epoch: 3.63 sec]
EPOCH 631/2000:
	Training over batches...
	Model improved!!!
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08112463264239732		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.4629955439236788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08112463264239732 | validation: 0.12090691354021915]
	TIME [epoch: 3.63 sec]
EPOCH 632/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.4629955439236788 | validation: 0.4905830934709854]
	TIME [epoch: 3.59 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11453396035195951		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.45349976712436874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11453396035195951 | validation: 0.13164604240453134]
	TIME [epoch: 3.62 sec]
EPOCH 633/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.45349976712436874 | validation: 0.5215702871564764]
	TIME [epoch: 3.59 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12327071159714854		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.4795878271879414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12327071159714854 | validation: 0.14492785471494837]
	TIME [epoch: 3.63 sec]
EPOCH 634/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.4795878271879414 | validation: 0.41079829202569407]
	TIME [epoch: 3.59 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1596624926943349		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.46644224497944037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1596624926943349 | validation: 0.17849198655684972]
	TIME [epoch: 3.63 sec]
EPOCH 635/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.46644224497944037 | validation: 0.4338656388828918]
	TIME [epoch: 3.59 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1341749126802769		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.544743934278997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1341749126802769 | validation: 0.1259603006000327]
	TIME [epoch: 3.63 sec]
EPOCH 636/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.544743934278997 | validation: 0.5843213906098594]
	TIME [epoch: 3.59 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10921480204453646		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.568819470125663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10921480204453646 | validation: 0.05212198053516315]
	TIME [epoch: 3.61 sec]
EPOCH 637/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.568819470125663 | validation: 0.7335020566364702]
	TIME [epoch: 3.59 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06984090082027127		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.7543416709768941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06984090082027127 | validation: 0.11236844596426646]
	TIME [epoch: 3.63 sec]
EPOCH 638/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.7543416709768941 | validation: 0.7245556425266613]
	TIME [epoch: 3.59 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04449572393490823		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.683711091837383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04449572393490823 | validation: 0.06604765302690097]
	TIME [epoch: 3.62 sec]
EPOCH 639/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.683711091837383 | validation: 0.5630153307140153]
	TIME [epoch: 3.59 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04783269486119146		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.5084599394006293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04783269486119146 | validation: 0.10918074086686987]
	TIME [epoch: 3.62 sec]
	Learning Rate: 0.01
	LOSS [training: 0.5084599394006293 | validation: 0.3988079365158892]
	TIME [epoch: 3.59 sec]
EPOCH 640/2000:
	Training over batches...
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05856161890846707		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.5120177887985391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05856161890846707 | validation: 0.07546441852010248]
	TIME [epoch: 3.71 sec]
	Learning Rate: 0.01
	LOSS [training: 0.5120177887985391 | validation: 0.4653507798062806]
	TIME [epoch: 3.7 sec]
EPOCH 646/2000:
EPOCH 641/2000:
	Training over batches...
	Training over batches...
		[batch 1/1] avg loss: 0.06367257747628446		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.5311287583210197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5311287583210197 | validation: 0.49729225364487606]
	TIME [epoch: 3.6 sec]
	Learning Rate: 0.01
	LOSS [training: 0.06367257747628446 | validation: 0.09483199498110628]
	TIME [epoch: 3.61 sec]
EPOCH 647/2000:
	Training over batches...
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5267716783735714		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.058081993968457785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5267716783735714 | validation: 0.4468981936139164]
	TIME [epoch: 3.59 sec]
	Learning Rate: 0.01
	LOSS [training: 0.058081993968457785 | validation: 0.07789162267389231]
	TIME [epoch: 3.63 sec]
EPOCH 648/2000:
	Training over batches...
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4586880015922711		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.06843430907178713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4586880015922711 | validation: 0.4624261031889761]
	TIME [epoch: 3.59 sec]
EPOCH 649/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.06843430907178713 | validation: 0.11324651930452992]
	TIME [epoch: 3.62 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4560904784990194		[learning rate: 0.01]
		[batch 1/1] avg loss: 0.0890237852994257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4560904784990194 | validation: 0.4419066653001165]
	TIME [epoch: 3.6 sec]
EPOCH 650/2000:
	Training over batches...
	Learning Rate: 0.01
	LOSS [training: 0.0890237852994257 | validation: 0.14725880712204384]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_644.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 1441.020 seconds.
		[batch 1/1] avg loss: 0.4381626275002763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4381626275002763 | validation: 0.39547297907719514]
	TIME [epoch: 3.59 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46703197022152615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46703197022152615 | validation: 0.4108774904315056]
	TIME [epoch: 3.59 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4619025065627071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4619025065627071 | validation: 0.5377442979819577]
	TIME [epoch: 3.59 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5136048171434424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5136048171434424 | validation: 0.5333051640014903]
	TIME [epoch: 3.59 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5740864387620961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5740864387620961 | validation: 0.41242203213868134]
	TIME [epoch: 3.58 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5296859875473001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5296859875473001 | validation: 0.45374562688920117]
	TIME [epoch: 3.59 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44803928768570883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44803928768570883 | validation: 0.48594796938347495]
	TIME [epoch: 3.59 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4519904177346965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4519904177346965 | validation: 0.4426785222007931]
	TIME [epoch: 3.59 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4556858273199287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4556858273199287 | validation: 0.404082849152955]
	TIME [epoch: 3.6 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4614422554071099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4614422554071099 | validation: 0.47931337079075337]
	TIME [epoch: 3.59 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45431002496482636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45431002496482636 | validation: 0.4493712471398672]
	TIME [epoch: 3.59 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.438820134539109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.438820134539109 | validation: 0.4048023899273373]
	TIME [epoch: 3.59 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4416527824309522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4416527824309522 | validation: 0.45694642492163223]
	TIME [epoch: 3.59 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4592067231407171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4592067231407171 | validation: 0.5147562950709256]
	TIME [epoch: 3.59 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4614653439147419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4614653439147419 | validation: 0.4234881971578941]
	TIME [epoch: 3.59 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5445152445742886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5445152445742886 | validation: 0.6412438829582037]
	TIME [epoch: 3.59 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5800154126868527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5800154126868527 | validation: 0.549218899159985]
	TIME [epoch: 3.59 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5154576119321146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5154576119321146 | validation: 0.38787582081462757]
	TIME [epoch: 3.59 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43318949820281405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43318949820281405 | validation: 0.40079805643465927]
	TIME [epoch: 3.59 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4324387612049052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4324387612049052 | validation: 0.44723781190110157]
	TIME [epoch: 3.62 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43558188113532953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43558188113532953 | validation: 0.3871897986013108]
	TIME [epoch: 3.6 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4385319991143293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4385319991143293 | validation: 0.4820849798081406]
	TIME [epoch: 3.6 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4494395399050749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4494395399050749 | validation: 0.42440770155657614]
	TIME [epoch: 3.59 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42247983457439164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42247983457439164 | validation: 0.37218952262758953]
	TIME [epoch: 3.59 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4290028697750828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4290028697750828 | validation: 0.4180870430295853]
	TIME [epoch: 3.59 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41366069672467504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41366069672467504 | validation: 0.4360159273221686]
	TIME [epoch: 3.57 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43443491310170695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43443491310170695 | validation: 0.4758925849847508]
	TIME [epoch: 3.59 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5401377278460752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5401377278460752 | validation: 0.7036499846292747]
	TIME [epoch: 3.59 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6849656964910926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6849656964910926 | validation: 0.41129843399490335]
	TIME [epoch: 3.59 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5027282779867093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5027282779867093 | validation: 0.47649237866648875]
	TIME [epoch: 3.59 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43098938432494693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43098938432494693 | validation: 0.3987419425686128]
	TIME [epoch: 3.59 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6008603401624286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6008603401624286 | validation: 0.35735234671680893]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_681.pth
	Model improved!!!
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6343368461549134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6343368461549134 | validation: 0.36829858587865916]
	TIME [epoch: 3.6 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.53968037458108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.53968037458108 | validation: 0.4138203037950204]
	TIME [epoch: 3.6 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42812487933033366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42812487933033366 | validation: 0.5038248555861343]
	TIME [epoch: 3.6 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4662670356961774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4662670356961774 | validation: 0.37853665681929893]
	TIME [epoch: 3.6 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4369479670755962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4369479670755962 | validation: 0.41179487118455893]
	TIME [epoch: 3.6 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4583600188594125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4583600188594125 | validation: 0.49139694980883547]
	TIME [epoch: 3.59 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4875522565710962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4875522565710962 | validation: 0.49124888622667007]
	TIME [epoch: 3.59 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5139687874626107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5139687874626107 | validation: 0.5507617176811203]
	TIME [epoch: 3.6 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5483894421335488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5483894421335488 | validation: 0.49330585744922645]
	TIME [epoch: 3.59 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44703568895741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44703568895741 | validation: 0.38086049351372275]
	TIME [epoch: 3.59 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45223554389269616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45223554389269616 | validation: 0.34844460063888305]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_692.pth
	Model improved!!!
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4193996760242766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4193996760242766 | validation: 0.43230476101429255]
	TIME [epoch: 3.6 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41905914673279443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41905914673279443 | validation: 0.4643173330225181]
	TIME [epoch: 3.6 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.467911880715447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.467911880715447 | validation: 0.5766272718804187]
	TIME [epoch: 3.6 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5548488008026472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5548488008026472 | validation: 0.4287022682742378]
	TIME [epoch: 3.61 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5551021776201912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5551021776201912 | validation: 0.4126026091556521]
	TIME [epoch: 3.61 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4246192215590161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4246192215590161 | validation: 0.4202578450146365]
	TIME [epoch: 3.6 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4050319002926591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4050319002926591 | validation: 0.4133992873973349]
	TIME [epoch: 3.6 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4096954849604306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4096954849604306 | validation: 0.4066724501894987]
	TIME [epoch: 3.6 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4054429266505866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4054429266505866 | validation: 0.3560760007094921]
	TIME [epoch: 3.57 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47375032528579925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47375032528579925 | validation: 0.42306709549439736]
	TIME [epoch: 3.57 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43541283387827173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43541283387827173 | validation: 0.4533519988066401]
	TIME [epoch: 3.57 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47258492925921725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47258492925921725 | validation: 0.49206944785986906]
	TIME [epoch: 3.57 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4835947618282754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4835947618282754 | validation: 0.41420312878645404]
	TIME [epoch: 3.57 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4539803150665732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4539803150665732 | validation: 0.44587005416953396]
	TIME [epoch: 3.57 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41464261768146954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41464261768146954 | validation: 0.3715921985713659]
	TIME [epoch: 3.58 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39329569985504004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39329569985504004 | validation: 0.3495808394541888]
	TIME [epoch: 3.57 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3963402185612673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3963402185612673 | validation: 0.38987732382671003]
	TIME [epoch: 3.59 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3894498191811805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3894498191811805 | validation: 0.38400569543600516]
	TIME [epoch: 3.57 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3833828709338586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3833828709338586 | validation: 0.34643943650476844]
	TIME [epoch: 3.57 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_711.pth
	Model improved!!!
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39643674293621284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39643674293621284 | validation: 0.6065939763264444]
	TIME [epoch: 3.6 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5528839353128296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5528839353128296 | validation: 0.5532826615163069]
	TIME [epoch: 3.6 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5402112607676892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5402112607676892 | validation: 0.49991840539641885]
	TIME [epoch: 3.59 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7541865683162192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7541865683162192 | validation: 0.39751857216747943]
	TIME [epoch: 3.6 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6332005241463456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6332005241463456 | validation: 0.4206808026374411]
	TIME [epoch: 3.59 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5672676400347826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5672676400347826 | validation: 0.5505986553823572]
	TIME [epoch: 3.6 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48834813974696917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48834813974696917 | validation: 0.47445207926430916]
	TIME [epoch: 3.6 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4319400051026242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4319400051026242 | validation: 0.4051030956849063]
	TIME [epoch: 3.6 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4370770630449001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4370770630449001 | validation: 0.3971122148767611]
	TIME [epoch: 3.61 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4124552909384406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4124552909384406 | validation: 0.3520933648978743]
	TIME [epoch: 3.61 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3933364500735659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3933364500735659 | validation: 0.3746160110896184]
	TIME [epoch: 3.6 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.381646666478227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.381646666478227 | validation: 0.3568179702593867]
	TIME [epoch: 3.6 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3758636733486877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3758636733486877 | validation: 0.3737320435470203]
	TIME [epoch: 3.6 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3639966674151968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3639966674151968 | validation: 0.3267456525830532]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_725.pth
	Model improved!!!
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3784476436177262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3784476436177262 | validation: 0.5766652604099558]
	TIME [epoch: 3.58 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5018550758080041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5018550758080041 | validation: 0.5751321995246177]
	TIME [epoch: 3.58 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5803314066741064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5803314066741064 | validation: 0.45859865676661615]
	TIME [epoch: 3.58 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4385362969487242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4385362969487242 | validation: 0.33172464856322237]
	TIME [epoch: 3.58 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3937333932681331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3937333932681331 | validation: 0.331441927897826]
	TIME [epoch: 3.58 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3920717762759437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3920717762759437 | validation: 0.4239707701006776]
	TIME [epoch: 3.57 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3974635430472284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3974635430472284 | validation: 0.3861426301272391]
	TIME [epoch: 3.58 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3879883051153665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3879883051153665 | validation: 0.4181721301486099]
	TIME [epoch: 3.59 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4046401437745618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4046401437745618 | validation: 0.3503318044101341]
	TIME [epoch: 3.59 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4195386879004117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4195386879004117 | validation: 0.4664046210723746]
	TIME [epoch: 3.58 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42845758710706705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42845758710706705 | validation: 0.3914401067681976]
	TIME [epoch: 3.57 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4348498470912393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4348498470912393 | validation: 0.5853068092677266]
	TIME [epoch: 3.57 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5117688156101902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5117688156101902 | validation: 0.3882345508069413]
	TIME [epoch: 3.59 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37623028151225485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37623028151225485 | validation: 0.3273047872426824]
	TIME [epoch: 3.57 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39683158925644463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39683158925644463 | validation: 0.3799288149033873]
	TIME [epoch: 3.57 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3691701044137165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3691701044137165 | validation: 0.35888132649686644]
	TIME [epoch: 3.6 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4027362950943899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4027362950943899 | validation: 0.5189114553293814]
	TIME [epoch: 3.6 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46904326165145227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46904326165145227 | validation: 0.34242482087484305]
	TIME [epoch: 3.6 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39337167154515057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39337167154515057 | validation: 0.3637445934774739]
	TIME [epoch: 3.6 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35372103237338104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35372103237338104 | validation: 0.33581832787749166]
	TIME [epoch: 3.6 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3500910247450838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3500910247450838 | validation: 0.39643200361589215]
	TIME [epoch: 3.6 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35928208485636204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35928208485636204 | validation: 0.32209105505811214]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_747.pth
	Model improved!!!
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39762216699714303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39762216699714303 | validation: 0.4494717181565192]
	TIME [epoch: 3.6 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4158922867393426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4158922867393426 | validation: 0.36985428141131943]
	TIME [epoch: 3.6 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45796485301263845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45796485301263845 | validation: 0.44058823183028617]
	TIME [epoch: 3.59 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3932225312678612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3932225312678612 | validation: 0.3434913433876953]
	TIME [epoch: 3.6 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33988082442670914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33988082442670914 | validation: 0.3531277899196124]
	TIME [epoch: 3.6 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32467318861212535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32467318861212535 | validation: 0.3074313933246031]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_753.pth
	Model improved!!!
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3299375306973873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3299375306973873 | validation: 0.39322726607054126]
	TIME [epoch: 3.59 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34322138185661105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34322138185661105 | validation: 0.30401042043315185]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_755.pth
	Model improved!!!
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36812981136564804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36812981136564804 | validation: 0.6319900743387246]
	TIME [epoch: 3.59 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5441851464567387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5441851464567387 | validation: 0.4292548347993555]
	TIME [epoch: 3.59 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5016807237597568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5016807237597568 | validation: 0.6283268790870818]
	TIME [epoch: 3.6 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5298627758527655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5298627758527655 | validation: 0.41720644413832625]
	TIME [epoch: 3.61 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35754820018337413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35754820018337413 | validation: 0.31117023741661937]
	TIME [epoch: 3.6 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3689374959848447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3689374959848447 | validation: 0.3471507399428587]
	TIME [epoch: 3.59 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3307659041666264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3307659041666264 | validation: 0.3579832556057789]
	TIME [epoch: 3.59 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33225912252867396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33225912252867396 | validation: 0.325215313387564]
	TIME [epoch: 3.59 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3092793825652632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3092793825652632 | validation: 0.29458079045628527]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_764.pth
	Model improved!!!
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3057800564419094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3057800564419094 | validation: 0.2964551207679087]
	TIME [epoch: 3.59 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2985808865823162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2985808865823162 | validation: 0.37241594204961265]
	TIME [epoch: 3.59 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.319097740586035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.319097740586035 | validation: 0.3198855178006559]
	TIME [epoch: 3.59 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36403494069361286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36403494069361286 | validation: 0.6212456338223846]
	TIME [epoch: 3.59 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5534195575481905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5534195575481905 | validation: 0.3352914164366081]
	TIME [epoch: 3.59 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5125372699828512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5125372699828512 | validation: 0.3693441613506423]
	TIME [epoch: 3.6 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3041339352919272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3041339352919272 | validation: 0.3072003379450099]
	TIME [epoch: 3.61 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28992371307733306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28992371307733306 | validation: 0.3049909101112815]
	TIME [epoch: 3.6 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3021886327868475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3021886327868475 | validation: 0.39736979764737507]
	TIME [epoch: 3.59 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3102158528061004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3102158528061004 | validation: 0.2762905913544927]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_774.pth
	Model improved!!!
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30348723345543543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30348723345543543 | validation: 0.4842965745905124]
	TIME [epoch: 3.59 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38694652299271254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38694652299271254 | validation: 0.2981241190302681]
	TIME [epoch: 3.59 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3580598905880732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3580598905880732 | validation: 0.39428724414966293]
	TIME [epoch: 3.59 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3220007054988186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3220007054988186 | validation: 0.31379484233379884]
	TIME [epoch: 3.59 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28282624048584826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28282624048584826 | validation: 0.30118431318117]
	TIME [epoch: 3.59 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25986722904017406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25986722904017406 | validation: 0.289870349457954]
	TIME [epoch: 3.59 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25427307591246345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25427307591246345 | validation: 0.27018218996519233]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_781.pth
	Model improved!!!
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23875178981813203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23875178981813203 | validation: 0.36104312367695385]
	TIME [epoch: 3.6 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2700026861746417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2700026861746417 | validation: 0.33230345663155103]
	TIME [epoch: 3.6 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5455038910374704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5455038910374704 | validation: 0.6243428008092436]
	TIME [epoch: 3.61 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5351446455169752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5351446455169752 | validation: 0.25278400447723337]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_785.pth
	Model improved!!!
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3185968335253183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3185968335253183 | validation: 0.32883926005930386]
	TIME [epoch: 3.6 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26578374590751463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26578374590751463 | validation: 0.23831649601367577]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_787.pth
	Model improved!!!
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2509128878247226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2509128878247226 | validation: 0.32828629852173674]
	TIME [epoch: 3.6 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26237008493228486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26237008493228486 | validation: 0.22603855032263054]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_789.pth
	Model improved!!!
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28410452529774183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28410452529774183 | validation: 0.5195164928777544]
	TIME [epoch: 3.58 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42924437370523094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42924437370523094 | validation: 0.29740270370188154]
	TIME [epoch: 3.58 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3722913272277569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3722913272277569 | validation: 0.41837018174598345]
	TIME [epoch: 3.57 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33290705264647913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33290705264647913 | validation: 0.2282251585819206]
	TIME [epoch: 3.58 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.224539738824779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.224539738824779 | validation: 0.2439252432751917]
	TIME [epoch: 3.58 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20901193695183473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20901193695183473 | validation: 0.26832970968487296]
	TIME [epoch: 3.58 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20434668813391788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20434668813391788 | validation: 0.22644760318881044]
	TIME [epoch: 3.59 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20724715704043525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20724715704043525 | validation: 0.29434291049349154]
	TIME [epoch: 3.59 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2176089858550439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2176089858550439 | validation: 0.2807309213230524]
	TIME [epoch: 3.58 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3825515827889913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3825515827889913 | validation: 0.6650393101601402]
	TIME [epoch: 3.57 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6236875126183995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6236875126183995 | validation: 0.23770554628708043]
	TIME [epoch: 3.58 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2404284959187713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2404284959187713 | validation: 0.22870343211117344]
	TIME [epoch: 3.59 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21996274043414515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21996274043414515 | validation: 0.3977817370061498]
	TIME [epoch: 3.59 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30231299577469883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30231299577469883 | validation: 0.26286132214945607]
	TIME [epoch: 3.59 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2559335429973483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2559335429973483 | validation: 0.3433226919101531]
	TIME [epoch: 3.59 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2620137815270082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2620137815270082 | validation: 0.22871728526895813]
	TIME [epoch: 3.59 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21936371730302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21936371730302 | validation: 0.2865761854923293]
	TIME [epoch: 3.59 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21814116577758452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21814116577758452 | validation: 0.2128372719828021]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_807.pth
	Model improved!!!
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.196352550613507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.196352550613507 | validation: 0.2826686326965015]
	TIME [epoch: 3.6 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2164067287823515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2164067287823515 | validation: 0.22684155367695824]
	TIME [epoch: 3.6 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26870252896312447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26870252896312447 | validation: 0.47474004185660484]
	TIME [epoch: 3.59 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3747527785483786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3747527785483786 | validation: 0.27505962706282433]
	TIME [epoch: 3.59 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32438693223113396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32438693223113396 | validation: 0.4307042953219553]
	TIME [epoch: 3.59 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32467224686101814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32467224686101814 | validation: 0.21066634304794626]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_813.pth
	Model improved!!!
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17354809563601808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17354809563601808 | validation: 0.21513292706543782]
	TIME [epoch: 3.6 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17415198643624094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17415198643624094 | validation: 0.2846535214698467]
	TIME [epoch: 3.59 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1962599165115987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1962599165115987 | validation: 0.20059227351413364]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_816.pth
	Model improved!!!
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21916487095315995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21916487095315995 | validation: 0.39738109531242616]
	TIME [epoch: 3.6 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3077001573854814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3077001573854814 | validation: 0.24373155255062443]
	TIME [epoch: 3.63 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2487471436539687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2487471436539687 | validation: 0.278154972704242]
	TIME [epoch: 3.62 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21069599142860906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21069599142860906 | validation: 0.19156791883026375]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_820.pth
	Model improved!!!
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15531118009223352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15531118009223352 | validation: 0.17156942714023082]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_821.pth
	Model improved!!!
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13118114618517723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13118114618517723 | validation: 0.20828706387165422]
	TIME [epoch: 3.6 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13482527752102205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13482527752102205 | validation: 0.24917930902438573]
	TIME [epoch: 3.6 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2864885967274767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2864885967274767 | validation: 0.6135164580758373]
	TIME [epoch: 3.59 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5535086451424076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5535086451424076 | validation: 0.24461310461461072]
	TIME [epoch: 3.59 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2791202803708939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2791202803708939 | validation: 0.24355499274240444]
	TIME [epoch: 3.59 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17874318374865517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17874318374865517 | validation: 0.18417807949990492]
	TIME [epoch: 3.6 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13470289545589467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13470289545589467 | validation: 0.17182822263832667]
	TIME [epoch: 3.59 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13625508217203117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13625508217203117 | validation: 0.21452657685959275]
	TIME [epoch: 3.59 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14386447820926687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14386447820926687 | validation: 0.18737564025792286]
	TIME [epoch: 3.59 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17217688918930912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17217688918930912 | validation: 0.422894370565653]
	TIME [epoch: 3.6 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33147357772221525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33147357772221525 | validation: 0.2889455064731911]
	TIME [epoch: 3.6 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34531548940371987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34531548940371987 | validation: 0.3358028227370975]
	TIME [epoch: 3.61 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2792014623277916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2792014623277916 | validation: 0.15418076277688925]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_834.pth
	Model improved!!!
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1309635512425773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1309635512425773 | validation: 0.17778119673500445]
	TIME [epoch: 3.6 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14290284424364547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14290284424364547 | validation: 0.27127137776709864]
	TIME [epoch: 3.6 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18558640309373114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18558640309373114 | validation: 0.18703903980198053]
	TIME [epoch: 3.59 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18147813879807856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18147813879807856 | validation: 0.31921831872061945]
	TIME [epoch: 3.6 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23486262708470818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23486262708470818 | validation: 0.23065909061541845]
	TIME [epoch: 3.6 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21737402127669694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21737402127669694 | validation: 0.28570426773672736]
	TIME [epoch: 3.6 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2159734548939872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2159734548939872 | validation: 0.16288244344491956]
	TIME [epoch: 3.6 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1452040065416824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1452040065416824 | validation: 0.1660584510098167]
	TIME [epoch: 3.59 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12474060734172569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12474060734172569 | validation: 0.14012301884252873]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_843.pth
	Model improved!!!
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11199888114509678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11199888114509678 | validation: 0.17355298000246594]
	TIME [epoch: 3.6 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11477017750537968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11477017750537968 | validation: 0.18817590184341781]
	TIME [epoch: 3.6 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15021430378535489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15021430378535489 | validation: 0.4360058834907459]
	TIME [epoch: 3.61 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3673118598959071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3673118598959071 | validation: 0.35037063442900096]
	TIME [epoch: 3.6 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5016481608876359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5016481608876359 | validation: 0.3198083549820576]
	TIME [epoch: 3.6 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2619314167083182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2619314167083182 | validation: 0.18279221068884]
	TIME [epoch: 3.6 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12632362660590804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12632362660590804 | validation: 0.20704232326101565]
	TIME [epoch: 3.59 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16079513941160137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16079513941160137 | validation: 0.22572404498508547]
	TIME [epoch: 3.59 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16653947507713088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16653947507713088 | validation: 0.16037778508236145]
	TIME [epoch: 3.6 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12987833655418468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12987833655418468 | validation: 0.15332778950599055]
	TIME [epoch: 3.6 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10829852419734605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10829852419734605 | validation: 0.15395463532119694]
	TIME [epoch: 3.59 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09404512110992379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09404512110992379 | validation: 0.13038195288482837]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_855.pth
	Model improved!!!
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09091066796341288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09091066796341288 | validation: 0.13214921644573555]
	TIME [epoch: 3.6 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09725589524665154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09725589524665154 | validation: 0.2138020624037488]
	TIME [epoch: 3.59 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14903191169784932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14903191169784932 | validation: 0.31346124595616953]
	TIME [epoch: 3.6 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4346898364948079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4346898364948079 | validation: 0.6485601958650973]
	TIME [epoch: 3.61 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6076619993076916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6076619993076916 | validation: 0.15427896358114088]
	TIME [epoch: 3.6 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1194352695954396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1194352695954396 | validation: 0.24382358410530003]
	TIME [epoch: 3.59 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.255821396206978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.255821396206978 | validation: 0.3162148636213499]
	TIME [epoch: 3.59 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25533360929135573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25533360929135573 | validation: 0.17249515469347665]
	TIME [epoch: 3.59 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12142229258892431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12142229258892431 | validation: 0.1619326464429696]
	TIME [epoch: 3.59 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11532973270563486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11532973270563486 | validation: 0.19431945051250515]
	TIME [epoch: 3.59 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1256844425001073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1256844425001073 | validation: 0.16492036310201472]
	TIME [epoch: 3.59 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11817922561037154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11817922561037154 | validation: 0.1657742334758866]
	TIME [epoch: 3.59 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10631571634320285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10631571634320285 | validation: 0.13704130374989543]
	TIME [epoch: 3.59 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09520894062903278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09520894062903278 | validation: 0.1672267829626697]
	TIME [epoch: 3.59 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11876432602266133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11876432602266133 | validation: 0.17457216434663578]
	TIME [epoch: 3.59 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15585229909195675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15585229909195675 | validation: 0.3699517015281971]
	TIME [epoch: 3.6 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2857703172045767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2857703172045767 | validation: 0.26449202975384944]
	TIME [epoch: 3.61 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3187200155981538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3187200155981538 | validation: 0.32496507187323775]
	TIME [epoch: 3.6 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26233121457482234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26233121457482234 | validation: 0.15577618670173946]
	TIME [epoch: 3.59 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10282231714842528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10282231714842528 | validation: 0.17312223968611998]
	TIME [epoch: 3.59 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12946862456259994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12946862456259994 | validation: 0.2078301219315812]
	TIME [epoch: 3.59 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1466561720773573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1466561720773573 | validation: 0.14455534576267515]
	TIME [epoch: 3.59 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1203299007582453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1203299007582453 | validation: 0.15933747485610247]
	TIME [epoch: 3.6 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10179311550098706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10179311550098706 | validation: 0.14089186372906246]
	TIME [epoch: 3.59 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09212930158731168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09212930158731168 | validation: 0.133020923174859]
	TIME [epoch: 3.59 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08726902409707453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08726902409707453 | validation: 0.13676535789247568]
	TIME [epoch: 3.59 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09457171858882807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09457171858882807 | validation: 0.2179182494009304]
	TIME [epoch: 3.6 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17034112584001476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17034112584001476 | validation: 0.32259292396079137]
	TIME [epoch: 3.6 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4496517417394856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4496517417394856 | validation: 0.5002143294525817]
	TIME [epoch: 3.6 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41713836603644494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41713836603644494 | validation: 0.14269953463951654]
	TIME [epoch: 3.61 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10925304986208194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10925304986208194 | validation: 0.20321704206652955]
	TIME [epoch: 3.6 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16237363489525586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16237363489525586 | validation: 0.22924241542724985]
	TIME [epoch: 3.62 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16807382624508052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16807382624508052 | validation: 0.15842065642702288]
	TIME [epoch: 3.6 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10770365352110217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10770365352110217 | validation: 0.1340382257206425]
	TIME [epoch: 3.6 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08191140019299203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08191140019299203 | validation: 0.12646781209195518]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_890.pth
	Model improved!!!
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07180443720720793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07180443720720793 | validation: 0.11805722703420085]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_891.pth
	Model improved!!!
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07026496922459001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07026496922459001 | validation: 0.10778213682503264]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_892.pth
	Model improved!!!
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06932138921304958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06932138921304958 | validation: 0.10545546501826678]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_893.pth
	Model improved!!!
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0678717467371147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0678717467371147 | validation: 0.11174069784694228]
	TIME [epoch: 3.59 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07632558165182907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07632558165182907 | validation: 0.21455020168859035]
	TIME [epoch: 3.59 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15166434910606066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15166434910606066 | validation: 0.31016533133443547]
	TIME [epoch: 3.6 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5116569457450545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5116569457450545 | validation: 0.6047845515682818]
	TIME [epoch: 3.61 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5148545019483416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5148545019483416 | validation: 0.19722424028210617]
	TIME [epoch: 3.6 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1459043931152596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1459043931152596 | validation: 0.24289187316369973]
	TIME [epoch: 3.58 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18877015031524708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18877015031524708 | validation: 0.3172377317979376]
	TIME [epoch: 3.57 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24317188689544214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24317188689544214 | validation: 0.14910026675821136]
	TIME [epoch: 3.6 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10835798237518227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10835798237518227 | validation: 0.14662304835306925]
	TIME [epoch: 3.59 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07714737390573347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07714737390573347 | validation: 0.13655591107521656]
	TIME [epoch: 3.59 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08103078014102398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08103078014102398 | validation: 0.1374311236269625]
	TIME [epoch: 3.59 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09205575108796982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09205575108796982 | validation: 0.15773249409283807]
	TIME [epoch: 3.59 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10655760125965306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10655760125965306 | validation: 0.167272936831246]
	TIME [epoch: 3.59 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1396483033582174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1396483033582174 | validation: 0.28261739130949726]
	TIME [epoch: 3.59 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2181549554272193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2181549554272193 | validation: 0.232047390811975]
	TIME [epoch: 3.59 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22908767038366612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22908767038366612 | validation: 0.28918862588427285]
	TIME [epoch: 3.6 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21661117415226558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21661117415226558 | validation: 0.15963502823096387]
	TIME [epoch: 3.6 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10452479110106701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10452479110106701 | validation: 0.13891450909825626]
	TIME [epoch: 3.59 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07127509210191435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07127509210191435 | validation: 0.13970272859894445]
	TIME [epoch: 3.59 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08516081849481127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08516081849481127 | validation: 0.13217307987668409]
	TIME [epoch: 3.6 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08746252106732694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08746252106732694 | validation: 0.14113694068180074]
	TIME [epoch: 3.59 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09364078461298327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09364078461298327 | validation: 0.15022955192042847]
	TIME [epoch: 3.59 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10805681672704305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10805681672704305 | validation: 0.22211216514638804]
	TIME [epoch: 3.59 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15610192134182277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15610192134182277 | validation: 0.2615187605576582]
	TIME [epoch: 3.59 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3071019381531497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3071019381531497 | validation: 0.5129212240950932]
	TIME [epoch: 3.59 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4794721767695745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4794721767695745 | validation: 0.12878860737370315]
	TIME [epoch: 3.59 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09779728938188591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09779728938188591 | validation: 0.17678937119966032]
	TIME [epoch: 3.59 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15028361460132353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15028361460132353 | validation: 0.22271673907922485]
	TIME [epoch: 3.59 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.174940886108122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.174940886108122 | validation: 0.15196240281209106]
	TIME [epoch: 3.6 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0902395817434734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0902395817434734 | validation: 0.1348707366107572]
	TIME [epoch: 3.6 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07557159620761089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07557159620761089 | validation: 0.12330276766184407]
	TIME [epoch: 3.59 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07132320841410976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07132320841410976 | validation: 0.12395305164412761]
	TIME [epoch: 3.59 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07725253400578388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07725253400578388 | validation: 0.15063908538956083]
	TIME [epoch: 3.59 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0937318627999456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0937318627999456 | validation: 0.19073275833852665]
	TIME [epoch: 3.59 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1481032522215243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1481032522215243 | validation: 0.31824885583100926]
	TIME [epoch: 3.59 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2531426903282491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2531426903282491 | validation: 0.24226229850595013]
	TIME [epoch: 3.59 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26332844962016294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26332844962016294 | validation: 0.23992378382394228]
	TIME [epoch: 3.59 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19499715028765757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19499715028765757 | validation: 0.13781545422874455]
	TIME [epoch: 3.59 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08250216327743747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08250216327743747 | validation: 0.14411530510614357]
	TIME [epoch: 3.59 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08012836087160484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08012836087160484 | validation: 0.16352610857159486]
	TIME [epoch: 3.65 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10548121059173607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10548121059173607 | validation: 0.14520202249152409]
	TIME [epoch: 3.59 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10846241320639653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10846241320639653 | validation: 0.15926987388304378]
	TIME [epoch: 3.6 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11613145789625431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11613145789625431 | validation: 0.16590016191679874]
	TIME [epoch: 3.6 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1309791242036641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1309791242036641 | validation: 0.2498306812627279]
	TIME [epoch: 3.59 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1792666407134432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1792666407134432 | validation: 0.20184208605236217]
	TIME [epoch: 3.59 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16501515932818403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16501515932818403 | validation: 0.21392873156944126]
	TIME [epoch: 3.59 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15684362867894253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15684362867894253 | validation: 0.13694161160756144]
	TIME [epoch: 3.59 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0860610940115636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0860610940115636 | validation: 0.13095817768305204]
	TIME [epoch: 3.59 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0630696432040283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0630696432040283 | validation: 0.11766171034597983]
	TIME [epoch: 3.59 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05919894830861759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05919894830861759 | validation: 0.11015405703679076]
	TIME [epoch: 3.59 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05881056978406444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05881056978406444 | validation: 0.11743343172944898]
	TIME [epoch: 3.59 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05971588735542273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05971588735542273 | validation: 0.145153910189262]
	TIME [epoch: 3.59 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13045274300524112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13045274300524112 | validation: 0.4203228867782398]
	TIME [epoch: 3.59 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4219670304867903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4219670304867903 | validation: 0.2928593709732722]
	TIME [epoch: 3.59 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32533748901804826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32533748901804826 | validation: 0.30636288155419766]
	TIME [epoch: 3.6 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23162559523819448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23162559523819448 | validation: 0.17611150165781908]
	TIME [epoch: 3.6 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09430667718247328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09430667718247328 | validation: 0.17212219951926783]
	TIME [epoch: 3.6 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10274049846483037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10274049846483037 | validation: 0.1911725799732972]
	TIME [epoch: 3.59 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1142937745402281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1142937745402281 | validation: 0.13448410428818292]
	TIME [epoch: 3.59 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08702047859238342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08702047859238342 | validation: 0.13501292586443722]
	TIME [epoch: 3.59 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062173316143024855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062173316143024855 | validation: 0.11245991890297989]
	TIME [epoch: 3.59 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05845208200119927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05845208200119927 | validation: 0.10608960828365217]
	TIME [epoch: 3.61 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05855652419407962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05855652419407962 | validation: 0.11321202246214401]
	TIME [epoch: 3.59 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05482711604853284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05482711604853284 | validation: 0.10886881457451325]
	TIME [epoch: 3.58 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056868987276976146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056868987276976146 | validation: 0.10887983560561665]
	TIME [epoch: 3.59 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06837588376956562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06837588376956562 | validation: 0.22762813614581878]
	TIME [epoch: 3.61 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17778785142608172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17778785142608172 | validation: 0.46073685401004527]
	TIME [epoch: 3.59 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6544799024969771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6544799024969771 | validation: 0.31046598392852315]
	TIME [epoch: 3.6 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2160110694247659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2160110694247659 | validation: 0.17466739351699276]
	TIME [epoch: 3.6 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11372304435040828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11372304435040828 | validation: 0.23417628054368725]
	TIME [epoch: 3.57 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18271635411255027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18271635411255027 | validation: 0.20137458095705008]
	TIME [epoch: 3.59 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13453835942012912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13453835942012912 | validation: 0.1479771207943927]
	TIME [epoch: 3.59 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0971936955405517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0971936955405517 | validation: 0.13665858232227862]
	TIME [epoch: 3.59 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060157854548593924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060157854548593924 | validation: 0.11940872345823657]
	TIME [epoch: 3.59 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05972904570641038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05972904570641038 | validation: 0.11586549314775102]
	TIME [epoch: 3.59 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05563232130445101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05563232130445101 | validation: 0.10157550055071192]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_969.pth
	Model improved!!!
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05900227605028263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05900227605028263 | validation: 0.13338622162853314]
	TIME [epoch: 3.6 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07141646207168453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07141646207168453 | validation: 0.20471772467565025]
	TIME [epoch: 3.59 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16883255207235365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16883255207235365 | validation: 0.6315932744287066]
	TIME [epoch: 3.59 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6325148856654604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6325148856654604 | validation: 0.16598599290234073]
	TIME [epoch: 3.6 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1364638554566237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1364638554566237 | validation: 0.14490308687747258]
	TIME [epoch: 3.6 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07217355397475897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07217355397475897 | validation: 0.1542728339637286]
	TIME [epoch: 3.6 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08464717756675023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08464717756675023 | validation: 0.14632319405345715]
	TIME [epoch: 3.59 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07715532268363517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07715532268363517 | validation: 0.13551093404899223]
	TIME [epoch: 3.6 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06344375031176082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06344375031176082 | validation: 0.13124479773318343]
	TIME [epoch: 3.6 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05973735554120316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05973735554120316 | validation: 0.11710786210183254]
	TIME [epoch: 3.59 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06524412570967023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06524412570967023 | validation: 0.1477114681201515]
	TIME [epoch: 3.59 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09230436474719265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09230436474719265 | validation: 0.25974995660983136]
	TIME [epoch: 3.59 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20169478624438836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20169478624438836 | validation: 0.277438449039387]
	TIME [epoch: 3.59 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30259281081254225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30259281081254225 | validation: 0.32814500040913636]
	TIME [epoch: 3.59 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2745179567874146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2745179567874146 | validation: 0.13820986600790716]
	TIME [epoch: 3.6 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07260244489911487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07260244489911487 | validation: 0.17325306315625788]
	TIME [epoch: 3.6 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10523632435951497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10523632435951497 | validation: 0.18371639186723698]
	TIME [epoch: 3.61 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11764328350823586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11764328350823586 | validation: 0.13089559154094083]
	TIME [epoch: 3.6 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07295619039560461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07295619039560461 | validation: 0.11305306449501823]
	TIME [epoch: 3.59 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05555049402017494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05555049402017494 | validation: 0.11066696150009175]
	TIME [epoch: 3.59 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05281465298600572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05281465298600572 | validation: 0.11557545727491136]
	TIME [epoch: 3.6 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0504785016057855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0504785016057855 | validation: 0.10446261967297812]
	TIME [epoch: 3.6 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050271290735487535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050271290735487535 | validation: 0.10717903564046652]
	TIME [epoch: 3.6 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05390334159007539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05390334159007539 | validation: 0.12006300128846666]
	TIME [epoch: 3.59 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07300039185973432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07300039185973432 | validation: 0.16880193371021168]
	TIME [epoch: 3.59 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09791829093411433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09791829093411433 | validation: 0.144131894855527]
	TIME [epoch: 3.59 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10610457405506876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10610457405506876 | validation: 0.13079245489867594]
	TIME [epoch: 3.59 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07449292969860828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07449292969860828 | validation: 0.15895053435956696]
	TIME [epoch: 3.59 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10616359955855542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10616359955855542 | validation: 0.34433650630864143]
	TIME [epoch: 3.6 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4373468126928124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4373468126928124 | validation: 0.599420239290548]
	TIME [epoch: 3.61 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5166280592065776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5166280592065776 | validation: 0.14655391930270759]
	TIME [epoch: 3.6 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07507093023004811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07507093023004811 | validation: 0.23170218367620082]
	TIME [epoch: 47.4 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19644829764672253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19644829764672253 | validation: 0.20243827214691612]
	TIME [epoch: 7.8 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14876090550447368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14876090550447368 | validation: 0.1327124957411536]
	TIME [epoch: 7.77 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06065133122383155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06065133122383155 | validation: 0.15317700756277997]
	TIME [epoch: 7.77 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07489303655739062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07489303655739062 | validation: 0.13683976116502894]
	TIME [epoch: 7.76 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07538519097011379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07538519097011379 | validation: 0.14615123721648315]
	TIME [epoch: 7.78 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09504575757130802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09504575757130802 | validation: 0.1929320727938268]
	TIME [epoch: 7.76 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1479238246110778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1479238246110778 | validation: 0.1685008177320837]
	TIME [epoch: 7.77 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10399877679099684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10399877679099684 | validation: 0.14939861595229492]
	TIME [epoch: 7.79 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09021648141537264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09021648141537264 | validation: 0.1393100361941777]
	TIME [epoch: 7.77 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07624328572875465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07624328572875465 | validation: 0.1244592096163244]
	TIME [epoch: 7.77 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07049054097519755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07049054097519755 | validation: 0.131084262341664]
	TIME [epoch: 7.76 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07449794364592323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07449794364592323 | validation: 0.13209913126780679]
	TIME [epoch: 7.78 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07932561541940286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07932561541940286 | validation: 0.15461745616808253]
	TIME [epoch: 7.77 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1217411277322129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1217411277322129 | validation: 0.28585626290252497]
	TIME [epoch: 7.79 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.242922738031138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.242922738031138 | validation: 0.21439943371180292]
	TIME [epoch: 7.78 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20814710803909373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20814710803909373 | validation: 0.20117089029723845]
	TIME [epoch: 7.77 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11829780773700761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11829780773700761 | validation: 0.1272343835102454]
	TIME [epoch: 7.78 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060448616459123417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060448616459123417 | validation: 0.14194024874649053]
	TIME [epoch: 7.77 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061222257802313676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061222257802313676 | validation: 0.1319747747706706]
	TIME [epoch: 7.77 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06736942454912774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06736942454912774 | validation: 0.1276897781378008]
	TIME [epoch: 7.78 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06945382511804704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06945382511804704 | validation: 0.13572000397383904]
	TIME [epoch: 7.78 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1052825956398804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1052825956398804 | validation: 0.19293310998423496]
	TIME [epoch: 7.77 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17234742807796508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17234742807796508 | validation: 0.3472791283778134]
	TIME [epoch: 7.77 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2648735210029146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2648735210029146 | validation: 0.1733082550963583]
	TIME [epoch: 7.78 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1156097341300972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1156097341300972 | validation: 0.132390031503276]
	TIME [epoch: 7.77 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062437733301674396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062437733301674396 | validation: 0.13640379410555775]
	TIME [epoch: 7.77 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055834697467734354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055834697467734354 | validation: 0.11533613008481579]
	TIME [epoch: 7.79 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059949501064466844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059949501064466844 | validation: 0.1211484841891348]
	TIME [epoch: 7.78 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06281860399270026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06281860399270026 | validation: 0.1375116694200955]
	TIME [epoch: 7.77 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08812606402813636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08812606402813636 | validation: 0.21660077481574752]
	TIME [epoch: 7.77 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15895841951632253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15895841951632253 | validation: 0.2177563788218839]
	TIME [epoch: 7.77 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21301592033054703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21301592033054703 | validation: 0.3102018181454301]
	TIME [epoch: 7.78 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2201993683188789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2201993683188789 | validation: 0.16296354926531853]
	TIME [epoch: 7.78 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09648759231442754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09648759231442754 | validation: 0.145960916972684]
	TIME [epoch: 7.78 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06853337383614896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06853337383614896 | validation: 0.13072852691995207]
	TIME [epoch: 7.78 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07168425281523769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07168425281523769 | validation: 0.1406930651850014]
	TIME [epoch: 7.77 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07214622491686237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07214622491686237 | validation: 0.13967871119035344]
	TIME [epoch: 7.85 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08414595046829419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08414595046829419 | validation: 0.15545652068163346]
	TIME [epoch: 7.77 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09326587572547161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09326587572547161 | validation: 0.174012812675297]
	TIME [epoch: 7.77 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10787521261685307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10787521261685307 | validation: 0.1702552498912559]
	TIME [epoch: 7.79 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11729842311885838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11729842311885838 | validation: 0.2404233459230489]
	TIME [epoch: 7.77 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1780967313713936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1780967313713936 | validation: 0.1865270731469339]
	TIME [epoch: 7.77 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17058223473723913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17058223473723913 | validation: 0.19837589780916218]
	TIME [epoch: 7.77 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1402190152689542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1402190152689542 | validation: 0.12260306932484047]
	TIME [epoch: 7.77 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06567793927203987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06567793927203987 | validation: 0.12292445565625945]
	TIME [epoch: 7.77 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05294822668381819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05294822668381819 | validation: 0.11285638939887446]
	TIME [epoch: 7.8 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053344998174269895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053344998174269895 | validation: 0.10984693877288793]
	TIME [epoch: 7.78 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05433511363946777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05433511363946777 | validation: 0.1058674297276189]
	TIME [epoch: 7.77 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05782484359981636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05782484359981636 | validation: 0.1334683627932686]
	TIME [epoch: 7.77 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08640493626128848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08640493626128848 | validation: 0.2525859599540629]
	TIME [epoch: 7.77 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19571668469327846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19571668469327846 | validation: 0.3016208259420436]
	TIME [epoch: 7.77 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2883331236540343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2883331236540343 | validation: 0.29755352986077765]
	TIME [epoch: 7.77 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2343680589787188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2343680589787188 | validation: 0.1718892313971125]
	TIME [epoch: 7.78 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10081455857433541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10081455857433541 | validation: 0.1504261155503348]
	TIME [epoch: 7.77 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08295849572670294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08295849572670294 | validation: 0.15422891818254347]
	TIME [epoch: 7.77 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0940549994032074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0940549994032074 | validation: 0.13500607225211006]
	TIME [epoch: 7.76 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06721134965580243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06721134965580243 | validation: 0.11141655786964866]
	TIME [epoch: 7.77 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05051492080952661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05051492080952661 | validation: 0.09399399046382935]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_1059.pth
	Model improved!!!
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050112705877293814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050112705877293814 | validation: 0.10483778855031188]
	TIME [epoch: 7.79 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046131298259330294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046131298259330294 | validation: 0.10387477511556997]
	TIME [epoch: 7.76 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0427153213989803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0427153213989803 | validation: 0.09019466473353864]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_1062.pth
	Model improved!!!
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040319933061261685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040319933061261685 | validation: 0.10188264431260341]
	TIME [epoch: 7.76 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05650045498449677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05650045498449677 | validation: 0.1850320810338783]
	TIME [epoch: 7.76 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16184385918989677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16184385918989677 | validation: 0.6109218956622517]
	TIME [epoch: 7.76 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5695150979770809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5695150979770809 | validation: 0.14745338376362543]
	TIME [epoch: 7.78 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11226181559297498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11226181559297498 | validation: 0.13456770717853989]
	TIME [epoch: 7.77 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07639540820295174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07639540820295174 | validation: 0.16765433704002095]
	TIME [epoch: 7.76 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09348149910356107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09348149910356107 | validation: 0.14835454255897007]
	TIME [epoch: 7.76 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07887619440969693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07887619440969693 | validation: 0.12113306420819564]
	TIME [epoch: 7.76 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05943106185185727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05943106185185727 | validation: 0.13113965515177864]
	TIME [epoch: 7.76 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05764294271310799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05764294271310799 | validation: 0.12414514459709813]
	TIME [epoch: 7.77 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07452736421126592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07452736421126592 | validation: 0.14591083318848003]
	TIME [epoch: 7.79 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10492608596251671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10492608596251671 | validation: 0.24334422940053937]
	TIME [epoch: 7.77 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17444257146333858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17444257146333858 | validation: 0.18942375312354415]
	TIME [epoch: 7.76 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1494545557604721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1494545557604721 | validation: 0.17159839410976752]
	TIME [epoch: 7.76 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10370318709405606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10370318709405606 | validation: 0.11203247422448095]
	TIME [epoch: 7.76 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056482877331567906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056482877331567906 | validation: 0.10723296299912095]
	TIME [epoch: 7.77 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044170315093589096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044170315093589096 | validation: 0.10997002436197707]
	TIME [epoch: 7.79 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04364812402498769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04364812402498769 | validation: 0.09856280411162766]
	TIME [epoch: 7.77 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04574822357299957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04574822357299957 | validation: 0.0984519922538838]
	TIME [epoch: 7.77 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0632562058907743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0632562058907743 | validation: 0.16963563119965347]
	TIME [epoch: 7.77 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14905808249618105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14905808249618105 | validation: 0.3644525842242062]
	TIME [epoch: 7.77 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31366529239024266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31366529239024266 | validation: 0.22212046868668317]
	TIME [epoch: 7.77 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16399650044334718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16399650044334718 | validation: 0.20037827672150216]
	TIME [epoch: 7.78 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13118894121773855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13118894121773855 | validation: 0.15010765594504305]
	TIME [epoch: 7.78 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07473131803646423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07473131803646423 | validation: 0.10639669226394749]
	TIME [epoch: 7.76 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04353044360020796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04353044360020796 | validation: 0.1192731206607546]
	TIME [epoch: 7.77 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053750851905346446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053750851905346446 | validation: 0.103294859314201]
	TIME [epoch: 7.76 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050873842107222955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050873842107222955 | validation: 0.09892973758191086]
	TIME [epoch: 7.77 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04880326832739689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04880326832739689 | validation: 0.11504276741401383]
	TIME [epoch: 7.76 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05436609756780893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05436609756780893 | validation: 0.13560581976540417]
	TIME [epoch: 7.79 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0949647627341157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0949647627341157 | validation: 0.2357805870110281]
	TIME [epoch: 7.76 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26562009361841443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26562009361841443 | validation: 0.4645937838801724]
	TIME [epoch: 7.77 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3600850692306515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3600850692306515 | validation: 0.1320336903570429]
	TIME [epoch: 7.76 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07259666066180553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07259666066180553 | validation: 0.1709419349773679]
	TIME [epoch: 7.77 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10039905858549858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10039905858549858 | validation: 0.1622375878129032]
	TIME [epoch: 7.77 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0917847613353463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0917847613353463 | validation: 0.11448331977361799]
	TIME [epoch: 7.78 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06381458472479702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06381458472479702 | validation: 0.1115860555362815]
	TIME [epoch: 7.77 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04571128175441622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04571128175441622 | validation: 0.11384801392560706]
	TIME [epoch: 7.77 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04701733459630862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04701733459630862 | validation: 0.10599063453452096]
	TIME [epoch: 7.77 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04808957589977288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04808957589977288 | validation: 0.09656793057469107]
	TIME [epoch: 7.79 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04678648039548133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04678648039548133 | validation: 0.11630621808518296]
	TIME [epoch: 7.77 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05950882674185165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05950882674185165 | validation: 0.16537636440023445]
	TIME [epoch: 7.78 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11050690893667917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11050690893667917 | validation: 0.2286561453744369]
	TIME [epoch: 7.78 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20944607633415474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20944607633415474 | validation: 0.35352690381001906]
	TIME [epoch: 7.76 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28799500906667796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28799500906667796 | validation: 0.13522845280694828]
	TIME [epoch: 7.77 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07548396896729036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07548396896729036 | validation: 0.131815436334905]
	TIME [epoch: 7.77 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06379159831817632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06379159831817632 | validation: 0.13409918297411805]
	TIME [epoch: 7.76 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07052963817960065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07052963817960065 | validation: 0.12464444345752607]
	TIME [epoch: 7.77 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06527743111240895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06527743111240895 | validation: 0.10517852698078473]
	TIME [epoch: 7.78 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05372778985960012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05372778985960012 | validation: 0.10904741907837019]
	TIME [epoch: 7.78 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05584759029592836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05584759029592836 | validation: 0.12564114882106311]
	TIME [epoch: 7.77 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07109304634669768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07109304634669768 | validation: 0.16773316356209478]
	TIME [epoch: 7.76 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10658568534353538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10658568534353538 | validation: 0.23086826837180935]
	TIME [epoch: 7.76 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17741671864225636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17741671864225636 | validation: 0.1621155022187054]
	TIME [epoch: 7.77 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13718964621345467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13718964621345467 | validation: 0.15399470005741903]
	TIME [epoch: 7.77 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09877867403662728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09877867403662728 | validation: 0.13350673417574682]
	TIME [epoch: 7.78 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07232091424981872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07232091424981872 | validation: 0.1049656471556743]
	TIME [epoch: 7.76 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05575694943063513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05575694943063513 | validation: 0.11469652359761473]
	TIME [epoch: 7.77 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043179812732710994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043179812732710994 | validation: 0.08693700956068129]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_1121.pth
	Model improved!!!
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0388615007839644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0388615007839644 | validation: 0.08453532834295428]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_1122.pth
	Model improved!!!
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03459764290507139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03459764290507139 | validation: 0.08185978747550393]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_1123.pth
	Model improved!!!
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03625666834859605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03625666834859605 | validation: 0.08408400766097684]
	TIME [epoch: 7.79 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049410738827557595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049410738827557595 | validation: 0.17811127164221882]
	TIME [epoch: 7.77 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19569345894254114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19569345894254114 | validation: 0.6622340935187441]
	TIME [epoch: 7.76 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6190845858525503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6190845858525503 | validation: 0.15073306551727]
	TIME [epoch: 7.77 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12461023248473171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12461023248473171 | validation: 0.20397144925718447]
	TIME [epoch: 7.77 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1758892066224206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1758892066224206 | validation: 0.2287464898836991]
	TIME [epoch: 7.77 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13183031303806822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13183031303806822 | validation: 0.12587952909677247]
	TIME [epoch: 7.79 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07840230536932284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07840230536932284 | validation: 0.1271364749882715]
	TIME [epoch: 7.76 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0525688632501474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0525688632501474 | validation: 0.11926367656973041]
	TIME [epoch: 7.76 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06255368778699283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06255368778699283 | validation: 0.13319943835492373]
	TIME [epoch: 7.76 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06080106042417466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06080106042417466 | validation: 0.12520891107688628]
	TIME [epoch: 7.77 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060100755580634796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060100755580634796 | validation: 0.11812219479812643]
	TIME [epoch: 7.76 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05544962272350272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05544962272350272 | validation: 0.11879889021881472]
	TIME [epoch: 7.78 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05744583874784124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05744583874784124 | validation: 0.1330589907435031]
	TIME [epoch: 7.77 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06795487048583203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06795487048583203 | validation: 0.13868991808627157]
	TIME [epoch: 7.76 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0915525041740183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0915525041740183 | validation: 0.17501973662994152]
	TIME [epoch: 7.76 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12015120371990136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12015120371990136 | validation: 0.24668226654313125]
	TIME [epoch: 7.76 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19446061320726707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19446061320726707 | validation: 0.15332810160131163]
	TIME [epoch: 7.76 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10058485135313752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10058485135313752 | validation: 0.11613125458673657]
	TIME [epoch: 7.77 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06026244862904914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06026244862904914 | validation: 0.09972443335150706]
	TIME [epoch: 7.78 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04322589904324653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04322589904324653 | validation: 0.09454969598610576]
	TIME [epoch: 7.77 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037698552591378526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037698552591378526 | validation: 0.08814566150792416]
	TIME [epoch: 7.76 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035942157155496555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035942157155496555 | validation: 0.08611675148779702]
	TIME [epoch: 7.77 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035575098012283396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035575098012283396 | validation: 0.10634058017574864]
	TIME [epoch: 7.77 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0395432414443963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0395432414443963 | validation: 0.12743975422890033]
	TIME [epoch: 7.76 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07366147004087507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07366147004087507 | validation: 0.2602862894145927]
	TIME [epoch: 7.79 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22475126901494313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22475126901494313 | validation: 0.46960590178293177]
	TIME [epoch: 7.77 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4067089218462421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4067089218462421 | validation: 0.16009634205274859]
	TIME [epoch: 7.77 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09840272995932946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09840272995932946 | validation: 0.14090484107212295]
	TIME [epoch: 7.76 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07109731404627331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07109731404627331 | validation: 0.13618590560172414]
	TIME [epoch: 7.76 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08180702831309487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08180702831309487 | validation: 0.1292141772541224]
	TIME [epoch: 7.77 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05964305081038402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05964305081038402 | validation: 0.10141353685827899]
	TIME [epoch: 7.77 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04010396524220115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04010396524220115 | validation: 0.09299469853134407]
	TIME [epoch: 7.79 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03651894396163336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03651894396163336 | validation: 0.09222198225514978]
	TIME [epoch: 7.76 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0351311956351296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0351311956351296 | validation: 0.07712915818970091]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_1158.pth
	Model improved!!!
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036128657678914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036128657678914 | validation: 0.09495073477889779]
	TIME [epoch: 7.8 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03979640285684815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03979640285684815 | validation: 0.11025336217844509]
	TIME [epoch: 7.8 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07603348481734966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07603348481734966 | validation: 0.23408091743977222]
	TIME [epoch: 7.8 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2311106977462736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2311106977462736 | validation: 0.504388314519239]
	TIME [epoch: 7.81 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43468689239907493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43468689239907493 | validation: 0.11370755018926162]
	TIME [epoch: 7.79 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06889699375582746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06889699375582746 | validation: 0.20111259221952915]
	TIME [epoch: 7.79 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15535090531156381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15535090531156381 | validation: 0.18758242642715156]
	TIME [epoch: 7.8 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10489941453469989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10489941453469989 | validation: 0.12082628031435544]
	TIME [epoch: 7.8 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05286015573598617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05286015573598617 | validation: 0.12270824256936629]
	TIME [epoch: 7.79 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057474701340057596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057474701340057596 | validation: 0.10288963659521096]
	TIME [epoch: 7.81 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047695150538672716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047695150538672716 | validation: 0.09701205061519122]
	TIME [epoch: 7.8 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03914908557244467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03914908557244467 | validation: 0.09094906334220411]
	TIME [epoch: 7.8 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044823430550517004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044823430550517004 | validation: 0.11065545412982965]
	TIME [epoch: 7.79 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05557865876457147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05557865876457147 | validation: 0.15006282223266595]
	TIME [epoch: 7.79 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07764601145400273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07764601145400273 | validation: 0.15868456907648265]
	TIME [epoch: 7.79 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10442097400337513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10442097400337513 | validation: 0.18437935940791947]
	TIME [epoch: 7.79 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.139937685368084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.139937685368084 | validation: 0.17732633902506884]
	TIME [epoch: 7.81 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18267815274945676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18267815274945676 | validation: 0.20036064287239516]
	TIME [epoch: 7.78 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1334106313904422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1334106313904422 | validation: 0.11701640273086969]
	TIME [epoch: 7.79 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05013012342196037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05013012342196037 | validation: 0.11313977878823143]
	TIME [epoch: 7.79 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044502711041177964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044502711041177964 | validation: 0.1016353616578106]
	TIME [epoch: 7.79 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04707111343592001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04707111343592001 | validation: 0.10240228917630052]
	TIME [epoch: 7.79 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04685830081488352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04685830081488352 | validation: 0.10061616091726254]
	TIME [epoch: 7.8 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05601830486148679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05601830486148679 | validation: 0.14581462822694827]
	TIME [epoch: 7.8 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09187674048278811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09187674048278811 | validation: 0.2129408971361782]
	TIME [epoch: 7.79 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17307591637970307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17307591637970307 | validation: 0.19412355412189616]
	TIME [epoch: 7.79 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17096219522259404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17096219522259404 | validation: 0.1730021960274956]
	TIME [epoch: 7.79 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11934745478873542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11934745478873542 | validation: 0.11834467553749968]
	TIME [epoch: 7.79 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05784932186373133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05784932186373133 | validation: 0.11308014919870613]
	TIME [epoch: 7.8 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046548670629491085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046548670629491085 | validation: 0.10660924563381309]
	TIME [epoch: 7.8 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048928055255274716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048928055255274716 | validation: 0.11495439578645397]
	TIME [epoch: 7.8 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0505957375467435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0505957375467435 | validation: 0.0978114119197766]
	TIME [epoch: 7.79 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060157072227667044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060157072227667044 | validation: 0.12391834604216281]
	TIME [epoch: 7.8 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07509772727648954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07509772727648954 | validation: 0.16199023229895632]
	TIME [epoch: 7.76 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12776857203215305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12776857203215305 | validation: 0.19404509031137965]
	TIME [epoch: 7.77 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16912817269092162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16912817269092162 | validation: 0.21430030170119305]
	TIME [epoch: 7.78 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16033204332850176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16033204332850176 | validation: 0.1289611478248318]
	TIME [epoch: 7.77 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05151914803495879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05151914803495879 | validation: 0.10654217633522262]
	TIME [epoch: 7.77 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042932996263188715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042932996263188715 | validation: 0.12049478254677559]
	TIME [epoch: 7.77 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051041221259963784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051041221259963784 | validation: 0.12476684086346479]
	TIME [epoch: 7.85 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057403208441554575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057403208441554575 | validation: 0.12325345234474422]
	TIME [epoch: 7.76 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07225349987384362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07225349987384362 | validation: 0.1341536084099828]
	TIME [epoch: 7.78 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07997333874686967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07997333874686967 | validation: 0.13250724252536802]
	TIME [epoch: 7.81 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09363798289508561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09363798289508561 | validation: 0.13764521860386958]
	TIME [epoch: 7.8 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0934186476587503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0934186476587503 | validation: 0.12965099874937677]
	TIME [epoch: 7.8 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08387259604460472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08387259604460472 | validation: 0.12299331182866205]
	TIME [epoch: 7.8 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07166224575337676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07166224575337676 | validation: 0.10108613175190616]
	TIME [epoch: 7.8 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0470386905315257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0470386905315257 | validation: 0.07989385009968836]
	TIME [epoch: 7.8 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0341219130435232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0341219130435232 | validation: 0.08242044560032426]
	TIME [epoch: 7.82 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028553471368870688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028553471368870688 | validation: 0.07414326389420835]
	TIME [epoch: 7.8 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_1208.pth
	Model improved!!!
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027548863907471195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027548863907471195 | validation: 0.06997577753159996]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_1209.pth
	Model improved!!!
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028273338827985734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028273338827985734 | validation: 0.07112606096173951]
	TIME [epoch: 7.79 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048879161293017996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048879161293017996 | validation: 0.21707071349646237]
	TIME [epoch: 7.79 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2653042325113826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2653042325113826 | validation: 0.5826281980873043]
	TIME [epoch: 7.79 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48213701928377817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48213701928377817 | validation: 0.0926912813458508]
	TIME [epoch: 7.81 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06048023494223907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06048023494223907 | validation: 0.21641303619447708]
	TIME [epoch: 7.79 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16943537645890522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16943537645890522 | validation: 0.17043474291752256]
	TIME [epoch: 7.79 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08376841871140357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08376841871140357 | validation: 0.11852728156875952]
	TIME [epoch: 7.79 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06525900293463373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06525900293463373 | validation: 0.13817043964080958]
	TIME [epoch: 7.79 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05293332757345289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05293332757345289 | validation: 0.10279002981030012]
	TIME [epoch: 7.79 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04202914438380066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04202914438380066 | validation: 0.10071120720397025]
	TIME [epoch: 7.8 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04141778372261998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04141778372261998 | validation: 0.10824156327709154]
	TIME [epoch: 7.8 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04172656012049364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04172656012049364 | validation: 0.10699860346637435]
	TIME [epoch: 7.8 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04545209725217928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04545209725217928 | validation: 0.12656133744306217]
	TIME [epoch: 7.79 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07287187187778287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07287187187778287 | validation: 0.138710920255197]
	TIME [epoch: 7.79 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08824106856109687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08824106856109687 | validation: 0.15274082392533583]
	TIME [epoch: 7.79 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11299771464418105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11299771464418105 | validation: 0.22067475018748245]
	TIME [epoch: 7.8 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16609011822276157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16609011822276157 | validation: 0.14982230377977288]
	TIME [epoch: 7.81 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10531754180305368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10531754180305368 | validation: 0.10647154855187729]
	TIME [epoch: 7.79 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05565129571562852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05565129571562852 | validation: 0.08852398486769175]
	TIME [epoch: 7.8 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0343550419360423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0343550419360423 | validation: 0.08336042015924637]
	TIME [epoch: 7.79 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03490123672146481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03490123672146481 | validation: 0.09221670734997167]
	TIME [epoch: 7.79 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03202858527491266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03202858527491266 | validation: 0.08289304241461831]
	TIME [epoch: 7.79 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03005202415025937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03005202415025937 | validation: 0.06959582170949638]
	TIME [epoch: 7.8 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_1232.pth
	Model improved!!!
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030399060113495534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030399060113495534 | validation: 0.07974604532042466]
	TIME [epoch: 7.76 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03338422227341696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03338422227341696 | validation: 0.09563407631634536]
	TIME [epoch: 7.75 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060116750305370364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060116750305370364 | validation: 0.25045543471239595]
	TIME [epoch: 7.75 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22790960117993142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22790960117993142 | validation: 0.47709352096405216]
	TIME [epoch: 7.75 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40732686719069977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40732686719069977 | validation: 0.10938383431838283]
	TIME [epoch: 7.76 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05834512671146026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05834512671146026 | validation: 0.17244694528507865]
	TIME [epoch: 7.76 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1209329676714311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1209329676714311 | validation: 0.18081323471838587]
	TIME [epoch: 7.77 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10938140931878305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10938140931878305 | validation: 0.11033711649202496]
	TIME [epoch: 7.75 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04783859666533591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04783859666533591 | validation: 0.10629600886521895]
	TIME [epoch: 7.75 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04589659650380976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04589659650380976 | validation: 0.10335437197874181]
	TIME [epoch: 7.75 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04540298793569163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04540298793569163 | validation: 0.09326601750013065]
	TIME [epoch: 7.76 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045577226330739004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045577226330739004 | validation: 0.09701456341826303]
	TIME [epoch: 7.76 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04281792038474148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04281792038474148 | validation: 0.10566608923087042]
	TIME [epoch: 7.78 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051618537951732296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051618537951732296 | validation: 0.11448517917755359]
	TIME [epoch: 7.75 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07189216039769156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07189216039769156 | validation: 0.16650678275820252]
	TIME [epoch: 7.75 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1125644143916072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1125644143916072 | validation: 0.22009771781948057]
	TIME [epoch: 7.75 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1686212468927964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1686212468927964 | validation: 0.15049505253771422]
	TIME [epoch: 7.75 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10444045372434788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10444045372434788 | validation: 0.10056873701875464]
	TIME [epoch: 7.76 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05239702487591487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05239702487591487 | validation: 0.09039695323090018]
	TIME [epoch: 7.77 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03381398820441214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03381398820441214 | validation: 0.09497594612297734]
	TIME [epoch: 7.77 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0341494298518817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0341494298518817 | validation: 0.08644002051038147]
	TIME [epoch: 7.75 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03668514602287814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03668514602287814 | validation: 0.10159276032520048]
	TIME [epoch: 7.75 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050306153818390244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050306153818390244 | validation: 0.16641681837868585]
	TIME [epoch: 7.75 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11945072558873018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11945072558873018 | validation: 0.2331720919891141]
	TIME [epoch: 7.75 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17929923001029546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17929923001029546 | validation: 0.28720182138586375]
	TIME [epoch: 7.76 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19811638124102474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19811638124102474 | validation: 0.11939102278745867]
	TIME [epoch: 7.78 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061170007422339856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061170007422339856 | validation: 0.10625714441079377]
	TIME [epoch: 7.75 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04915865137624966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04915865137624966 | validation: 0.10760481627888237]
	TIME [epoch: 7.75 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05319544755500721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05319544755500721 | validation: 0.10988855525960994]
	TIME [epoch: 7.76 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055455029372333514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055455029372333514 | validation: 0.0970605960024069]
	TIME [epoch: 7.76 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049871983772684744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049871983772684744 | validation: 0.09886768456591705]
	TIME [epoch: 7.76 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039874469540222945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039874469540222945 | validation: 0.07689508101854357]
	TIME [epoch: 7.77 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03774112652093224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03774112652093224 | validation: 0.09721210467883534]
	TIME [epoch: 7.77 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04974991824073948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04974991824073948 | validation: 0.1352917724325586]
	TIME [epoch: 7.77 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09400856691341215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09400856691341215 | validation: 0.17492757210061816]
	TIME [epoch: 7.76 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13942384039840355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13942384039840355 | validation: 0.19781332918981748]
	TIME [epoch: 7.75 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14739968932682826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14739968932682826 | validation: 0.15098819658652163]
	TIME [epoch: 7.76 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09927002554454305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09927002554454305 | validation: 0.11236578526853847]
	TIME [epoch: 7.75 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07339217770379969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07339217770379969 | validation: 0.10117987363301709]
	TIME [epoch: 7.77 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04165524038405211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04165524038405211 | validation: 0.08398435820335269]
	TIME [epoch: 7.75 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03229654891431256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03229654891431256 | validation: 0.07817142372456598]
	TIME [epoch: 7.76 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039634220965080943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039634220965080943 | validation: 0.09565787035944975]
	TIME [epoch: 7.75 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04292591505131732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04292591505131732 | validation: 0.09057821129991345]
	TIME [epoch: 7.76 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061247567112245965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061247567112245965 | validation: 0.18788852700096076]
	TIME [epoch: 7.75 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13848183893658753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13848183893658753 | validation: 0.30977916355698226]
	TIME [epoch: 7.76 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2099754690715156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2099754690715156 | validation: 0.12036959096643672]
	TIME [epoch: 7.76 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07119273264111582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07119273264111582 | validation: 0.10575763549083916]
	TIME [epoch: 7.76 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050388888534527665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050388888534527665 | validation: 0.09216655544576109]
	TIME [epoch: 7.75 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04343041610129911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04343041610129911 | validation: 0.09957246827719984]
	TIME [epoch: 7.75 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04361983651604142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04361983651604142 | validation: 0.09678764993191305]
	TIME [epoch: 7.77 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05157150123966741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05157150123966741 | validation: 0.11330236225208057]
	TIME [epoch: 7.77 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054451480466378675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054451480466378675 | validation: 0.11242561884266983]
	TIME [epoch: 7.77 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06751448490754879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06751448490754879 | validation: 0.13509764099067728]
	TIME [epoch: 7.76 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0826456080989468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0826456080989468 | validation: 0.18448527218943853]
	TIME [epoch: 7.75 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13068056173074333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13068056173074333 | validation: 0.1420014284340547]
	TIME [epoch: 7.76 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09551603896569873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09551603896569873 | validation: 0.10779083047772353]
	TIME [epoch: 7.76 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061525010824977446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061525010824977446 | validation: 0.08851689810998936]
	TIME [epoch: 7.75 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03931053704017983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03931053704017983 | validation: 0.08337221536106011]
	TIME [epoch: 7.76 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0334889439762425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0334889439762425 | validation: 0.06175480339127848]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_1291.pth
	Model improved!!!
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037661461996234234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037661461996234234 | validation: 0.10725078736065939]
	TIME [epoch: 7.81 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053755044107864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053755044107864 | validation: 0.13097366365578755]
	TIME [epoch: 7.81 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09489700469088669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09489700469088669 | validation: 0.1726180081016475]
	TIME [epoch: 7.8 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1405187913897453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1405187913897453 | validation: 0.2648316831162898]
	TIME [epoch: 7.81 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17710398510715283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17710398510715283 | validation: 0.127164196097175]
	TIME [epoch: 7.82 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08450089810981569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08450089810981569 | validation: 0.10057965246236061]
	TIME [epoch: 7.81 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0496166076651347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0496166076651347 | validation: 0.09239938693740013]
	TIME [epoch: 7.81 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03552472193549109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03552472193549109 | validation: 0.08184776103486463]
	TIME [epoch: 7.81 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03852823422535364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03852823422535364 | validation: 0.09462927780980301]
	TIME [epoch: 7.8 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03932499981229995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03932499981229995 | validation: 0.0839496874631559]
	TIME [epoch: 7.8 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03961357745606835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03961357745606835 | validation: 0.09046957933098593]
	TIME [epoch: 7.81 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06066333432516702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06066333432516702 | validation: 0.16299221199195482]
	TIME [epoch: 7.82 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1259074901007984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1259074901007984 | validation: 0.3313877092214809]
	TIME [epoch: 7.81 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2621308014201054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2621308014201054 | validation: 0.10655531691301301]
	TIME [epoch: 7.8 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05136084630271037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05136084630271037 | validation: 0.11361435634608252]
	TIME [epoch: 7.8 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04751753789912552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04751753789912552 | validation: 0.1052985867597466]
	TIME [epoch: 7.8 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057060957432608216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057060957432608216 | validation: 0.09825551959739195]
	TIME [epoch: 7.8 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040670332958921786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040670332958921786 | validation: 0.08294573708154707]
	TIME [epoch: 7.81 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031716819749027875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031716819749027875 | validation: 0.07713010955346164]
	TIME [epoch: 7.8 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031529738941007385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031529738941007385 | validation: 0.08026609852253179]
	TIME [epoch: 7.8 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0355408464439543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0355408464439543 | validation: 0.10332321912489324]
	TIME [epoch: 7.8 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05780050631599088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05780050631599088 | validation: 0.15552106217736772]
	TIME [epoch: 7.8 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11984925550078433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11984925550078433 | validation: 0.21133650759429454]
	TIME [epoch: 7.8 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1798938872232938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1798938872232938 | validation: 0.23664338922580677]
	TIME [epoch: 7.81 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1668784461851884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1668784461851884 | validation: 0.11861330379704765]
	TIME [epoch: 7.82 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06282675660903977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06282675660903977 | validation: 0.08743230255707633]
	TIME [epoch: 7.81 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040146780043528556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040146780043528556 | validation: 0.08171466663405792]
	TIME [epoch: 7.81 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03951752261772717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03951752261772717 | validation: 0.09545365011445106]
	TIME [epoch: 7.81 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04190388454672523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04190388454672523 | validation: 0.07013941494427771]
	TIME [epoch: 7.8 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038285250492962976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038285250492962976 | validation: 0.08179695455674789]
	TIME [epoch: 7.81 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037847342961340166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037847342961340166 | validation: 0.06891350665429301]
	TIME [epoch: 7.83 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04564535644302231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04564535644302231 | validation: 0.11505432886540451]
	TIME [epoch: 7.8 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06806801773768453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06806801773768453 | validation: 0.15729426151905362]
	TIME [epoch: 7.82 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11408397944637151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11408397944637151 | validation: 0.1690642868607365]
	TIME [epoch: 7.81 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1330199103809316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1330199103809316 | validation: 0.2061576808706906]
	TIME [epoch: 7.81 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1308997151247266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1308997151247266 | validation: 0.1095430227376036]
	TIME [epoch: 7.81 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06419511213861162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06419511213861162 | validation: 0.07178407104088214]
	TIME [epoch: 7.82 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03723139228137283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03723139228137283 | validation: 0.09037494386895066]
	TIME [epoch: 7.82 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03448268158437477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03448268158437477 | validation: 0.08013671106233354]
	TIME [epoch: 7.81 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037809012641556894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037809012641556894 | validation: 0.06859644247760109]
	TIME [epoch: 7.8 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03921347774685023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03921347774685023 | validation: 0.09902436133157615]
	TIME [epoch: 7.81 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045507686161698466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045507686161698466 | validation: 0.13238353215254514]
	TIME [epoch: 7.81 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09237751948386133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09237751948386133 | validation: 0.16507532215707768]
	TIME [epoch: 7.81 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1311670257941732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1311670257941732 | validation: 0.2251086115500912]
	TIME [epoch: 7.82 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1458589355533654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1458589355533654 | validation: 0.09664559405607558]
	TIME [epoch: 7.81 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041273661707140155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041273661707140155 | validation: 0.09062066178096671]
	TIME [epoch: 7.81 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0328869427610256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0328869427610256 | validation: 0.08851140091455342]
	TIME [epoch: 7.81 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04440120566651278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04440120566651278 | validation: 0.10179652959014965]
	TIME [epoch: 7.8 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04512860112821509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04512860112821509 | validation: 0.07637780805510108]
	TIME [epoch: 7.81 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04186371676964968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04186371676964968 | validation: 0.08571544463396133]
	TIME [epoch: 7.81 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03874592526664834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03874592526664834 | validation: 0.0814475135101439]
	TIME [epoch: 7.82 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049277366759760265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049277366759760265 | validation: 0.12291660804432669]
	TIME [epoch: 7.8 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06494490023565729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06494490023565729 | validation: 0.1702232233465134]
	TIME [epoch: 7.8 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12849227445806052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12849227445806052 | validation: 0.16294230567652604]
	TIME [epoch: 7.79 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13185607439910269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13185607439910269 | validation: 0.13538088870797924]
	TIME [epoch: 7.81 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1136707756339932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1136707756339932 | validation: 0.0930205290188547]
	TIME [epoch: 7.8 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06075195210217654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06075195210217654 | validation: 0.0905841107603735]
	TIME [epoch: 7.81 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027273722417137335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027273722417137335 | validation: 0.07329342869308696]
	TIME [epoch: 7.8 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0383358045944337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0383358045944337 | validation: 0.10312731901231394]
	TIME [epoch: 7.8 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05236582237527282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05236582237527282 | validation: 0.16094962185429953]
	TIME [epoch: 7.8 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09978010445004802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09978010445004802 | validation: 0.1459258290737808]
	TIME [epoch: 7.81 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09942562773468881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09942562773468881 | validation: 0.1361365848104986]
	TIME [epoch: 7.82 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0816095253670217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0816095253670217 | validation: 0.08396080563486795]
	TIME [epoch: 7.82 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041808439835597434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041808439835597434 | validation: 0.0792420859469587]
	TIME [epoch: 7.82 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033530761328586525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033530761328586525 | validation: 0.07651634477487368]
	TIME [epoch: 7.81 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03501817664883763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03501817664883763 | validation: 0.08715646383309736]
	TIME [epoch: 7.8 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03695244691545803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03695244691545803 | validation: 0.06557206629568091]
	TIME [epoch: 7.8 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03634629908275649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03634629908275649 | validation: 0.08810488187208178]
	TIME [epoch: 7.79 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039496981037713326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039496981037713326 | validation: 0.06448993048626551]
	TIME [epoch: 7.81 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047122583139613906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047122583139613906 | validation: 0.1334742428324218]
	TIME [epoch: 7.8 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07934627911688188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07934627911688188 | validation: 0.3104127427512392]
	TIME [epoch: 7.8 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2170316277344967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2170316277344967 | validation: 0.19214445152593657]
	TIME [epoch: 7.8 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14405363154087572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14405363154087572 | validation: 0.13437637033578204]
	TIME [epoch: 7.81 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07253433983182551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07253433983182551 | validation: 0.08255166456610497]
	TIME [epoch: 7.81 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03290695060757412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03290695060757412 | validation: 0.08429380888558281]
	TIME [epoch: 7.81 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03110415415819458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03110415415819458 | validation: 0.07070084419733227]
	TIME [epoch: 7.83 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03637867149608781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03637867149608781 | validation: 0.09159533592183566]
	TIME [epoch: 7.8 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04377529389093341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04377529389093341 | validation: 0.11628706903365149]
	TIME [epoch: 7.81 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07988354636738247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07988354636738247 | validation: 0.1401329756081449]
	TIME [epoch: 7.81 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0892480185237988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0892480185237988 | validation: 0.14714795371981357]
	TIME [epoch: 7.82 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11084544251180088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11084544251180088 | validation: 0.1017875275753918]
	TIME [epoch: 7.81 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0550818000810914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0550818000810914 | validation: 0.07198706559780302]
	TIME [epoch: 7.82 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03743062256049338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03743062256049338 | validation: 0.07586726154438135]
	TIME [epoch: 7.82 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03916297090366631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03916297090366631 | validation: 0.08913380175927986]
	TIME [epoch: 7.81 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04451229410105244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04451229410105244 | validation: 0.06480465549525342]
	TIME [epoch: 7.81 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03075760507895029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03075760507895029 | validation: 0.0651018344403261]
	TIME [epoch: 7.76 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025459544451784475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025459544451784475 | validation: 0.052989493681119215]
	TIME [epoch: 7.8 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_1378.pth
	Model improved!!!
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023951623626603035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023951623626603035 | validation: 0.06690021299873876]
	TIME [epoch: 7.75 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029388218379941627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029388218379941627 | validation: 0.10675033185709702]
	TIME [epoch: 7.76 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07480253030385893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07480253030385893 | validation: 0.23494754001159956]
	TIME [epoch: 7.75 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19833437297206288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19833437297206288 | validation: 0.3780126808747504]
	TIME [epoch: 7.75 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.250057595900698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.250057595900698 | validation: 0.08293776661188701]
	TIME [epoch: 7.77 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05323782394450354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05323782394450354 | validation: 0.178171721017693]
	TIME [epoch: 7.75 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11693605912758909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11693605912758909 | validation: 0.10245700045712983]
	TIME [epoch: 7.76 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0460159496071941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0460159496071941 | validation: 0.09461904291962449]
	TIME [epoch: 7.8 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05823546721575174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05823546721575174 | validation: 0.09938580381267287]
	TIME [epoch: 7.75 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03478644606059592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03478644606059592 | validation: 0.0898035759037462]
	TIME [epoch: 7.75 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026554059002619274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026554059002619274 | validation: 0.0744247742348927]
	TIME [epoch: 7.75 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028156427255968862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028156427255968862 | validation: 0.07648981721958428]
	TIME [epoch: 7.76 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026118713055656252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026118713055656252 | validation: 0.07853668590736215]
	TIME [epoch: 7.76 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029782265341346398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029782265341346398 | validation: 0.08289600078374704]
	TIME [epoch: 7.77 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0536640918584606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0536640918584606 | validation: 0.18101785145427945]
	TIME [epoch: 7.76 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12647475783286266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12647475783286266 | validation: 0.2508796438413263]
	TIME [epoch: 7.75 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19164383376818514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19164383376818514 | validation: 0.10550072945939709]
	TIME [epoch: 7.77 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061246886352855905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061246886352855905 | validation: 0.08602384002173454]
	TIME [epoch: 7.75 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03558335201980362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03558335201980362 | validation: 0.07342880138360212]
	TIME [epoch: 7.76 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041839971666143255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041839971666143255 | validation: 0.09356969834044554]
	TIME [epoch: 7.76 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039759153128215416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039759153128215416 | validation: 0.07524602605180813]
	TIME [epoch: 7.78 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04046681747429461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04046681747429461 | validation: 0.08615614979971313]
	TIME [epoch: 7.75 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04158796210731653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04158796210731653 | validation: 0.08725850846965814]
	TIME [epoch: 7.79 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05267580115356734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05267580115356734 | validation: 0.11858955497186192]
	TIME [epoch: 7.79 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07766188062200036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07766188062200036 | validation: 0.15012716442881868]
	TIME [epoch: 7.79 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10523379757840318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10523379757840318 | validation: 0.11164754235900101]
	TIME [epoch: 7.76 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07965728381939138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07965728381939138 | validation: 0.10154884802680353]
	TIME [epoch: 7.77 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07073139650370525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07073139650370525 | validation: 0.10254234608573641]
	TIME [epoch: 7.77 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05798352262421554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05798352262421554 | validation: 0.06439677823653173]
	TIME [epoch: 7.76 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028987897012036367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028987897012036367 | validation: 0.06461620708370137]
	TIME [epoch: 7.75 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023524448473411983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023524448473411983 | validation: 0.07743207601152371]
	TIME [epoch: 7.76 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029535194006360704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029535194006360704 | validation: 0.07035463195058593]
	TIME [epoch: 7.75 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03846744773959698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03846744773959698 | validation: 0.13656512932913034]
	TIME [epoch: 7.77 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07538572875565519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07538572875565519 | validation: 0.2714361027734875]
	TIME [epoch: 7.77 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18130613001984472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18130613001984472 | validation: 0.1519916120580637]
	TIME [epoch: 7.76 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10690836653237933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10690836653237933 | validation: 0.10948698497836072]
	TIME [epoch: 7.75 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06548959977330009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06548959977330009 | validation: 0.08301105681826887]
	TIME [epoch: 7.76 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04378368236491666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04378368236491666 | validation: 0.08439457357732352]
	TIME [epoch: 7.76 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033154938085349674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033154938085349674 | validation: 0.0689517782134461]
	TIME [epoch: 7.8 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03468946420285597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03468946420285597 | validation: 0.09521188153037785]
	TIME [epoch: 7.77 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03907904922099102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03907904922099102 | validation: 0.07823158243742971]
	TIME [epoch: 7.77 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05965843837440394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05965843837440394 | validation: 0.1210774189745496]
	TIME [epoch: 7.76 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07074326283007669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07074326283007669 | validation: 0.13545948820449935]
	TIME [epoch: 7.76 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09580827766997935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09580827766997935 | validation: 0.12035664861680778]
	TIME [epoch: 7.76 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06859520247799182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06859520247799182 | validation: 0.07413950122125132]
	TIME [epoch: 7.76 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04375255411792274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04375255411792274 | validation: 0.06987230287366235]
	TIME [epoch: 7.77 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027841175381381315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027841175381381315 | validation: 0.06293593306136396]
	TIME [epoch: 7.8 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02333960296607364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02333960296607364 | validation: 0.05562600125991373]
	TIME [epoch: 7.79 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02326144452698854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02326144452698854 | validation: 0.057931775433689464]
	TIME [epoch: 7.79 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02901617753399437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02901617753399437 | validation: 0.06441118218240775]
	TIME [epoch: 7.79 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04285973069550529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04285973069550529 | validation: 0.10344218806670631]
	TIME [epoch: 7.8 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07280245908755462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07280245908755462 | validation: 0.13670982682440624]
	TIME [epoch: 7.8 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10046428726774637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10046428726774637 | validation: 0.3000146534697719]
	TIME [epoch: 7.78 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1974659267581493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1974659267581493 | validation: 0.14654143741074815]
	TIME [epoch: 7.76 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13173938469152405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13173938469152405 | validation: 0.08557275523552069]
	TIME [epoch: 7.76 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045073108661268464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045073108661268464 | validation: 0.06213401383150773]
	TIME [epoch: 7.75 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026246315282585923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026246315282585923 | validation: 0.0665122878423167]
	TIME [epoch: 7.77 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027647308245696892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027647308245696892 | validation: 0.047790558325835654]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_1436.pth
	Model improved!!!
EPOCH 1437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023398495377944088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023398495377944088 | validation: 0.06542494754534987]
	TIME [epoch: 7.82 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020116886315377124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020116886315377124 | validation: 0.054483379205928495]
	TIME [epoch: 7.8 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026305973682591858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026305973682591858 | validation: 0.07433448808937289]
	TIME [epoch: 7.81 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039869748065051384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039869748065051384 | validation: 0.10837366417766735]
	TIME [epoch: 7.78 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07391225463964228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07391225463964228 | validation: 0.14482637843153273]
	TIME [epoch: 7.79 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11862746742694517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11862746742694517 | validation: 0.2680812873203915]
	TIME [epoch: 7.78 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17258028029571854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17258028029571854 | validation: 0.1139992333420758]
	TIME [epoch: 7.81 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06825429894730815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06825429894730815 | validation: 0.06159340249976064]
	TIME [epoch: 7.81 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03331705438016045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03331705438016045 | validation: 0.06418365934315715]
	TIME [epoch: 7.78 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030642960086121575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030642960086121575 | validation: 0.061514193995778094]
	TIME [epoch: 7.79 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030547263703607594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030547263703607594 | validation: 0.05552073892329989]
	TIME [epoch: 7.79 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030513243942000435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030513243942000435 | validation: 0.09740673858484902]
	TIME [epoch: 7.79 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04467478147231798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04467478147231798 | validation: 0.10262362409522226]
	TIME [epoch: 7.8 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07410834821867217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07410834821867217 | validation: 0.15267019523994352]
	TIME [epoch: 7.81 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0977936080922523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0977936080922523 | validation: 0.15466755366626395]
	TIME [epoch: 7.79 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09659724024768643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09659724024768643 | validation: 0.08186053077539054]
	TIME [epoch: 7.8 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03841522740738389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03841522740738389 | validation: 0.07364737343751453]
	TIME [epoch: 7.79 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026551564848615792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026551564848615792 | validation: 0.07608840832993918]
	TIME [epoch: 7.8 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036907864965808716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036907864965808716 | validation: 0.08611660569953233]
	TIME [epoch: 7.8 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04279497209521149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04279497209521149 | validation: 0.07279687480311434]
	TIME [epoch: 7.81 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04024001166914813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04024001166914813 | validation: 0.08054181807466854]
	TIME [epoch: 7.81 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03373779566577089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03373779566577089 | validation: 0.04870835282004591]
	TIME [epoch: 7.8 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02804504421353726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02804504421353726 | validation: 0.0840372314994213]
	TIME [epoch: 7.8 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03397142945304609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03397142945304609 | validation: 0.0965952308092946]
	TIME [epoch: 7.8 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06914380013788286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06914380013788286 | validation: 0.21520989636447335]
	TIME [epoch: 7.8 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1551767909922499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1551767909922499 | validation: 0.24101145590041542]
	TIME [epoch: 7.81 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1705871470113395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1705871470113395 | validation: 0.07011811619184971]
	TIME [epoch: 7.81 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044213357000461195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044213357000461195 | validation: 0.10561735989820978]
	TIME [epoch: 7.8 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048377033115406726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048377033115406726 | validation: 0.10049509582720405]
	TIME [epoch: 7.8 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05527103790262653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05527103790262653 | validation: 0.07638425015780931]
	TIME [epoch: 7.8 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038339503955232335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038339503955232335 | validation: 0.06182041871086839]
	TIME [epoch: 7.8 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02868307842486252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02868307842486252 | validation: 0.06999639063629672]
	TIME [epoch: 7.8 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021615266296322252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021615266296322252 | validation: 0.051548741707148585]
	TIME [epoch: 7.81 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0212640358501951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0212640358501951 | validation: 0.056172895181135135]
	TIME [epoch: 7.81 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023122331804536014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023122331804536014 | validation: 0.05892441367508821]
	TIME [epoch: 7.81 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03467606940049195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03467606940049195 | validation: 0.12388282967481623]
	TIME [epoch: 7.81 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07839390202471586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07839390202471586 | validation: 0.2748888263240183]
	TIME [epoch: 7.8 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1783017519305935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1783017519305935 | validation: 0.11419503859293685]
	TIME [epoch: 7.81 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0672959359410251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0672959359410251 | validation: 0.08741370437533658]
	TIME [epoch: 7.8 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0412587157023355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0412587157023355 | validation: 0.07285324659574219]
	TIME [epoch: 7.82 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038039987497143164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038039987497143164 | validation: 0.08367986050421268]
	TIME [epoch: 7.81 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04779029509879479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04779029509879479 | validation: 0.07653643613563904]
	TIME [epoch: 7.88 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051741784806678535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051741784806678535 | validation: 0.08201557573359991]
	TIME [epoch: 7.8 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04134097560425708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04134097560425708 | validation: 0.0882081814041778]
	TIME [epoch: 7.8 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04841891760556561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04841891760556561 | validation: 0.09910990118714086]
	TIME [epoch: 7.8 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07558668655093868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07558668655093868 | validation: 0.13500427883253227]
	TIME [epoch: 7.83 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09849934988707539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09849934988707539 | validation: 0.15025272334535356]
	TIME [epoch: 7.79 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09949723745167433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09949723745167433 | validation: 0.06931917869600485]
	TIME [epoch: 7.8 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03579547023892027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03579547023892027 | validation: 0.06248618677577326]
	TIME [epoch: 7.8 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02257573073507471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02257573073507471 | validation: 0.05274371137999765]
	TIME [epoch: 7.81 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023032224856871154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023032224856871154 | validation: 0.06722927534253674]
	TIME [epoch: 7.8 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024756908856903365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024756908856903365 | validation: 0.049550507986872244]
	TIME [epoch: 7.81 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026675309662133467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026675309662133467 | validation: 0.06780806235681487]
	TIME [epoch: 7.81 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03353188146602535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03353188146602535 | validation: 0.05470835576508737]
	TIME [epoch: 7.81 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04087458773938667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04087458773938667 | validation: 0.12460974163127753]
	TIME [epoch: 7.8 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07080019543007751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07080019543007751 | validation: 0.1987783460662096]
	TIME [epoch: 7.8 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1253527907686877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1253527907686877 | validation: 0.11616315302749432]
	TIME [epoch: 7.8 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08892824811451906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08892824811451906 | validation: 0.07502404966648697]
	TIME [epoch: 7.81 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06121027385715468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06121027385715468 | validation: 0.05637577610568202]
	TIME [epoch: 7.81 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030414081474005792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030414081474005792 | validation: 0.07156017713505256]
	TIME [epoch: 7.8 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023597486856002662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023597486856002662 | validation: 0.043751941889270146]
	TIME [epoch: 7.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_1497.pth
	Model improved!!!
EPOCH 1498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021143725039063272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021143725039063272 | validation: 0.05762338466884699]
	TIME [epoch: 7.79 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02173974014244444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02173974014244444 | validation: 0.03419593578021921]
	TIME [epoch: 7.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_1499.pth
	Model improved!!!
EPOCH 1500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02398000960414078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02398000960414078 | validation: 0.06515868038786869]
	TIME [epoch: 7.79 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028613751077095583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028613751077095583 | validation: 0.054608905784033025]
	TIME [epoch: 7.8 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04474341209359377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04474341209359377 | validation: 0.12218768600365917]
	TIME [epoch: 7.8 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07922538762042382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07922538762042382 | validation: 0.3177984508124331]
	TIME [epoch: 7.78 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19273125675117805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19273125675117805 | validation: 0.17088631883423983]
	TIME [epoch: 7.8 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11309448561346581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11309448561346581 | validation: 0.13403715134035543]
	TIME [epoch: 7.79 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08654051126633068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08654051126633068 | validation: 0.05626457883542373]
	TIME [epoch: 7.8 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030181386389907656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030181386389907656 | validation: 0.07152003135899763]
	TIME [epoch: 7.81 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02761789454828205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02761789454828205 | validation: 0.05812896475692969]
	TIME [epoch: 7.79 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02315933131507994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02315933131507994 | validation: 0.05831980478955288]
	TIME [epoch: 7.79 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021373249483852403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021373249483852403 | validation: 0.046576262126810056]
	TIME [epoch: 7.8 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01828493549970835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01828493549970835 | validation: 0.047706494095178464]
	TIME [epoch: 7.78 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017081604225901395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017081604225901395 | validation: 0.04358668371122198]
	TIME [epoch: 7.79 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027294777430882548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027294777430882548 | validation: 0.12184121876827946]
	TIME [epoch: 7.81 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07763816885382632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07763816885382632 | validation: 0.3079280046750843]
	TIME [epoch: 7.79 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2151444052754025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2151444052754025 | validation: 0.11935934566931183]
	TIME [epoch: 7.79 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07547390413087723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07547390413087723 | validation: 0.09253960761577618]
	TIME [epoch: 7.77 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04002478804294284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04002478804294284 | validation: 0.05568114422538848]
	TIME [epoch: 7.78 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02976503332191054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02976503332191054 | validation: 0.07380414727274649]
	TIME [epoch: 7.78 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022550438066555066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022550438066555066 | validation: 0.04964030975612539]
	TIME [epoch: 7.8 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021502105913656262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021502105913656262 | validation: 0.059362019730623966]
	TIME [epoch: 7.79 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02186966645234853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02186966645234853 | validation: 0.04808076901533828]
	TIME [epoch: 7.8 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025743371472852737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025743371472852737 | validation: 0.08813723538822232]
	TIME [epoch: 7.79 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03914751868992779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03914751868992779 | validation: 0.12148489280678812]
	TIME [epoch: 7.8 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08803135782905304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08803135782905304 | validation: 0.16125105294944786]
	TIME [epoch: 7.79 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11341039486380805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11341039486380805 | validation: 0.13463312780822032]
	TIME [epoch: 7.8 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09756114992198354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09756114992198354 | validation: 0.058276183882422866]
	TIME [epoch: 7.8 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03725326670896264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03725326670896264 | validation: 0.0864214654825043]
	TIME [epoch: 7.8 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04148931375130291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04148931375130291 | validation: 0.06874530469436792]
	TIME [epoch: 7.79 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03699347647632294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03699347647632294 | validation: 0.06564851824058825]
	TIME [epoch: 7.83 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0327500254407425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0327500254407425 | validation: 0.048361064373629616]
	TIME [epoch: 7.79 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03513647169630831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03513647169630831 | validation: 0.08907932636956484]
	TIME [epoch: 7.81 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03816186750613794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03816186750613794 | validation: 0.08649585641073382]
	TIME [epoch: 7.8 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05535064014094593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05535064014094593 | validation: 0.12492828035946918]
	TIME [epoch: 7.8 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07723226876022579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07723226876022579 | validation: 0.10236168471786616]
	TIME [epoch: 7.79 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06608209360082289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06608209360082289 | validation: 0.06476002736971233]
	TIME [epoch: 7.8 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03509566576740889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03509566576740889 | validation: 0.043143881465546]
	TIME [epoch: 7.78 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024520819788989918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024520819788989918 | validation: 0.04337744261793191]
	TIME [epoch: 7.8 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019042650269762318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019042650269762318 | validation: 0.034901342631993605]
	TIME [epoch: 7.79 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01511310048577787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01511310048577787 | validation: 0.03378882684536277]
	TIME [epoch: 7.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_1539.pth
	Model improved!!!
EPOCH 1540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015084937915375699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015084937915375699 | validation: 0.023779732949096345]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_1540.pth
	Model improved!!!
EPOCH 1541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01723465805776417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01723465805776417 | validation: 0.08382717654604283]
	TIME [epoch: 7.79 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03889573520355521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03889573520355521 | validation: 0.16538237638536799]
	TIME [epoch: 7.79 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12788102455561884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12788102455561884 | validation: 0.18877567692597802]
	TIME [epoch: 7.79 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1537299172680336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1537299172680336 | validation: 0.23267562771447967]
	TIME [epoch: 7.82 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12064634909147845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12064634909147845 | validation: 0.15091649726728687]
	TIME [epoch: 7.79 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10688400979873962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10688400979873962 | validation: 0.1335467483793895]
	TIME [epoch: 7.79 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07136469487181142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07136469487181142 | validation: 0.06784707056567932]
	TIME [epoch: 7.79 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027593893791877423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027593893791877423 | validation: 0.06364683503164338]
	TIME [epoch: 7.78 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031024981821101943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031024981821101943 | validation: 0.06112825328424198]
	TIME [epoch: 7.8 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021213456815555656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021213456815555656 | validation: 0.05982592575355905]
	TIME [epoch: 7.79 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019611950146271945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019611950146271945 | validation: 0.04493216885263605]
	TIME [epoch: 7.78 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018475400843900376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018475400843900376 | validation: 0.05178415325963113]
	TIME [epoch: 7.78 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016330283199577404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016330283199577404 | validation: 0.04169307600354852]
	TIME [epoch: 7.78 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022174854986591903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022174854986591903 | validation: 0.0812971671881509]
	TIME [epoch: 7.78 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04208843142760907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04208843142760907 | validation: 0.17405520504184124]
	TIME [epoch: 7.79 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13297867749646133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13297867749646133 | validation: 0.17132211994159666]
	TIME [epoch: 7.79 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13621283269545092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13621283269545092 | validation: 0.09806909621711896]
	TIME [epoch: 7.78 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06290241125617442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06290241125617442 | validation: 0.04341839001304235]
	TIME [epoch: 7.78 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0211478044125986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0211478044125986 | validation: 0.0627556597546096]
	TIME [epoch: 7.78 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025574642576700335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025574642576700335 | validation: 0.052469776207920406]
	TIME [epoch: 7.78 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03038536827324183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03038536827324183 | validation: 0.06282205967669473]
	TIME [epoch: 7.79 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02763735275945123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02763735275945123 | validation: 0.046436527437486524]
	TIME [epoch: 7.8 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027127781875442132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027127781875442132 | validation: 0.056529554592822745]
	TIME [epoch: 7.79 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02791288804756805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02791288804756805 | validation: 0.04542791124538659]
	TIME [epoch: 7.78 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033612499848168866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033612499848168866 | validation: 0.07893389051122918]
	TIME [epoch: 7.78 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05287771396128614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05287771396128614 | validation: 0.13856297618397945]
	TIME [epoch: 7.78 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09728417481743858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09728417481743858 | validation: 0.11460693227950035]
	TIME [epoch: 7.79 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07754850960772623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07754850960772623 | validation: 0.08418174811231813]
	TIME [epoch: 7.8 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05425234235227104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05425234235227104 | validation: 0.06284787842758865]
	TIME [epoch: 7.78 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033391131101708556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033391131101708556 | validation: 0.059737494481482384]
	TIME [epoch: 7.78 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041756208157728104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041756208157728104 | validation: 0.06892531813880866]
	TIME [epoch: 7.78 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04390771810694537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04390771810694537 | validation: 0.0912073160327108]
	TIME [epoch: 7.78 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048194793237649504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048194793237649504 | validation: 0.067073711146058]
	TIME [epoch: 7.79 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0413629682102159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0413629682102159 | validation: 0.07037289331719584]
	TIME [epoch: 7.8 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037604388861689146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037604388861689146 | validation: 0.07578866289912577]
	TIME [epoch: 7.79 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05562696340386174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05562696340386174 | validation: 0.11532418058704899]
	TIME [epoch: 7.78 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05424818381120531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05424818381120531 | validation: 0.08403071226846488]
	TIME [epoch: 7.78 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05263134622868985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05263134622868985 | validation: 0.05770510045247908]
	TIME [epoch: 7.78 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03160800763741751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03160800763741751 | validation: 0.057380878989413756]
	TIME [epoch: 7.79 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027112455296992896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027112455296992896 | validation: 0.053395902455502145]
	TIME [epoch: 7.79 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026101256096163154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026101256096163154 | validation: 0.05999338437799903]
	TIME [epoch: 7.78 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03233165853902202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03233165853902202 | validation: 0.04628860548577153]
	TIME [epoch: 7.79 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03768950976022386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03768950976022386 | validation: 0.046770652319093844]
	TIME [epoch: 7.8 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03639781071908801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03639781071908801 | validation: 0.11035894871965085]
	TIME [epoch: 7.79 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06748824110888041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06748824110888041 | validation: 0.2445864359379115]
	TIME [epoch: 7.8 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1638599685791226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1638599685791226 | validation: 0.11337593179371164]
	TIME [epoch: 7.8 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05273865873783008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05273865873783008 | validation: 0.05139131908222671]
	TIME [epoch: 7.79 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025525139419414446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025525139419414446 | validation: 0.04028119550049635]
	TIME [epoch: 7.79 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021460384257922748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021460384257922748 | validation: 0.05083412245530788]
	TIME [epoch: 7.8 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02433194989537938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02433194989537938 | validation: 0.03640072116011887]
	TIME [epoch: 7.78 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026028935628165807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026028935628165807 | validation: 0.05532553935180377]
	TIME [epoch: 7.8 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023142575004428897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023142575004428897 | validation: 0.025889277210963714]
	TIME [epoch: 7.8 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020663403094197452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020663403094197452 | validation: 0.05742157185534304]
	TIME [epoch: 7.79 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023938871878726193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023938871878726193 | validation: 0.03376360320174304]
	TIME [epoch: 7.79 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027308505373691856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027308505373691856 | validation: 0.07212385533904762]
	TIME [epoch: 7.79 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034075021042691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034075021042691 | validation: 0.05515237139392083]
	TIME [epoch: 7.79 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03676339263171168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03676339263171168 | validation: 0.07164529294069938]
	TIME [epoch: 7.79 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03423441957145213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03423441957145213 | validation: 0.0436052467986027]
	TIME [epoch: 7.81 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022275885486874116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022275885486874116 | validation: 0.030142222571081456]
	TIME [epoch: 7.79 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021635112439932006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021635112439932006 | validation: 0.08201558499164398]
	TIME [epoch: 7.79 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05006223750524374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05006223750524374 | validation: 0.3449634627233721]
	TIME [epoch: 7.79 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24252111816816707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24252111816816707 | validation: 0.16418224462078365]
	TIME [epoch: 7.79 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09829877074715797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09829877074715797 | validation: 0.08797819837635219]
	TIME [epoch: 7.79 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038905805003695816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038905805003695816 | validation: 0.046140785143693325]
	TIME [epoch: 7.81 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02570756019527983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02570756019527983 | validation: 0.07345327633818886]
	TIME [epoch: 7.78 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02629196444790071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02629196444790071 | validation: 0.042946746930537294]
	TIME [epoch: 7.79 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02121738728848494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02121738728848494 | validation: 0.05984923419524928]
	TIME [epoch: 7.79 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020261493197532204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020261493197532204 | validation: 0.024708523041296504]
	TIME [epoch: 7.79 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023146306163626857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023146306163626857 | validation: 0.06541189831926358]
	TIME [epoch: 7.79 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025435409080820176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025435409080820176 | validation: 0.05269564124969787]
	TIME [epoch: 7.82 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045260500603599214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045260500603599214 | validation: 0.1246657763310552]
	TIME [epoch: 7.79 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0756423418529024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0756423418529024 | validation: 0.16998141768751288]
	TIME [epoch: 7.8 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11210574946272112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11210574946272112 | validation: 0.07311380028905742]
	TIME [epoch: 7.79 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034059830553816166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034059830553816166 | validation: 0.09290060998072491]
	TIME [epoch: 7.79 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0424668373059968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0424668373059968 | validation: 0.07173842501731356]
	TIME [epoch: 7.79 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047354432192781605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047354432192781605 | validation: 0.05764036842944806]
	TIME [epoch: 7.82 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023047247106211755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023047247106211755 | validation: 0.03490981155897438]
	TIME [epoch: 7.79 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01335144047071287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01335144047071287 | validation: 0.03678641576760664]
	TIME [epoch: 7.79 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014714232386563483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014714232386563483 | validation: 0.03616632017206822]
	TIME [epoch: 7.79 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013606929356674763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013606929356674763 | validation: 0.026005019135830178]
	TIME [epoch: 7.79 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01655460930965521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01655460930965521 | validation: 0.060252767102582955]
	TIME [epoch: 7.79 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03189330732577272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03189330732577272 | validation: 0.11904362489442555]
	TIME [epoch: 7.8 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08901019213453744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08901019213453744 | validation: 0.1743509547731855]
	TIME [epoch: 7.8 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11673470279157044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11673470279157044 | validation: 0.14604208288401346]
	TIME [epoch: 7.79 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1144385142378086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1144385142378086 | validation: 0.05701460792716211]
	TIME [epoch: 7.79 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03875235875124918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03875235875124918 | validation: 0.07104613002450033]
	TIME [epoch: 7.79 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029890049891185048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029890049891185048 | validation: 0.04694934848673182]
	TIME [epoch: 7.79 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036431144342951756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036431144342951756 | validation: 0.05653872531371477]
	TIME [epoch: 7.8 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02319610447842932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02319610447842932 | validation: 0.02782117497038743]
	TIME [epoch: 7.8 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01731555911568573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01731555911568573 | validation: 0.04394480510307737]
	TIME [epoch: 7.8 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016434928363988653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016434928363988653 | validation: 0.025424867431711684]
	TIME [epoch: 7.79 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017508843651157426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017508843651157426 | validation: 0.052763759921101216]
	TIME [epoch: 7.8 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020660016573596573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020660016573596573 | validation: 0.03428506892531569]
	TIME [epoch: 7.78 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026693527890418154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026693527890418154 | validation: 0.08487616017711226]
	TIME [epoch: 7.8 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0375324687253023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0375324687253023 | validation: 0.07579735610821947]
	TIME [epoch: 7.8 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045350600732698795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045350600732698795 | validation: 0.08442957207660332]
	TIME [epoch: 7.79 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039912920297328375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039912920297328375 | validation: 0.05965486119302086]
	TIME [epoch: 7.79 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030957251268059167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030957251268059167 | validation: 0.11893202010075746]
	TIME [epoch: 7.79 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08383745291583447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08383745291583447 | validation: 0.1828726289900691]
	TIME [epoch: 7.79 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15234569705084403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15234569705084403 | validation: 0.20202309327246112]
	TIME [epoch: 7.8 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11913157605604159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11913157605604159 | validation: 0.0632659161034878]
	TIME [epoch: 7.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240823_171538/states/model_phi1_4a_v_mmd1_1641.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 7830.927 seconds.
