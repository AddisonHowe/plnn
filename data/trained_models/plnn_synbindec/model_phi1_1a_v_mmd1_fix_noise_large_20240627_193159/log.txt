Args:
Namespace(name='model_phi1_1a_v_mmd1_fix_noise_large', outdir='out/model_training/model_phi1_1a_v_mmd1_fix_noise_large', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.5, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 437302489

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.969816573280274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.969816573280274 | validation: 2.9102544675096857]
	TIME [epoch: 159 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.357145391283162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.357145391283162 | validation: 2.7526522635070347]
	TIME [epoch: 7.92 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.249542262187474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.249542262187474 | validation: 2.735689449845461]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.302237751726648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.302237751726648 | validation: 2.6408711371518634]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1943201179610217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1943201179610217 | validation: 2.598579224217543]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.15237703510991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.15237703510991 | validation: 2.6406032534039126]
	TIME [epoch: 7.8 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.216950508282117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.216950508282117 | validation: 2.5509225539096394]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1918105516414244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1918105516414244 | validation: 2.484377870298081]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.127839819467883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.127839819467883 | validation: 2.4682190192522793]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1132926623509523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1132926623509523 | validation: 2.4804530398552025]
	TIME [epoch: 7.77 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1809316406958583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1809316406958583 | validation: 2.430620962230216]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.124775849208424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.124775849208424 | validation: 2.39609111217909]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0939797871306443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0939797871306443 | validation: 2.5057395727354677]
	TIME [epoch: 7.77 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.161160028080891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.161160028080891 | validation: 2.402045875766717]
	TIME [epoch: 7.77 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1457667508195035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1457667508195035 | validation: 2.380231439051217]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0445713857542938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0445713857542938 | validation: 2.3615228711682352]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1554650698029043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1554650698029043 | validation: 2.3222598895083735]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0619972879336403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0619972879336403 | validation: 2.2848025084661447]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9377055188600498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9377055188600498 | validation: 2.161374130640911]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0282600961168034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0282600961168034 | validation: 2.7553570106963665]
	TIME [epoch: 7.81 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2672721398615288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2672721398615288 | validation: 1.8565144501253739]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7410303307089372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7410303307089372 | validation: 1.7415272967891364]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6693059781665984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6693059781665984 | validation: 1.9150021004634061]
	TIME [epoch: 7.77 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7204935677830187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7204935677830187 | validation: 1.6577820538872046]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6246378199356575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6246378199356575 | validation: 1.877094036458404]
	TIME [epoch: 7.82 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6280775485094492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6280775485094492 | validation: 1.6071167568522244]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6466242495626453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6466242495626453 | validation: 1.8050942891621649]
	TIME [epoch: 7.77 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.704408761927053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.704408761927053 | validation: 1.7315094518883183]
	TIME [epoch: 7.76 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.607551140460616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.607551140460616 | validation: 1.5801488114667772]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5647537585783422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5647537585783422 | validation: 1.5389267592638416]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5812419633292352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5812419633292352 | validation: 1.546055660668261]
	TIME [epoch: 7.78 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4960282691044466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4960282691044466 | validation: 1.5574516364064692]
	TIME [epoch: 7.76 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5548982193820309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5548982193820309 | validation: 1.5042141168868732]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5214387292645841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5214387292645841 | validation: 1.52690684151484]
	TIME [epoch: 7.79 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4553458598382587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4553458598382587 | validation: 1.414768430751928]
	TIME [epoch: 7.79 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5347949961986813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5347949961986813 | validation: 1.4948778975595558]
	TIME [epoch: 7.77 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4218812229467634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4218812229467634 | validation: 1.3938112670805674]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3906297185222547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3906297185222547 | validation: 1.3391527783168578]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3136087245641699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3136087245641699 | validation: 1.2501999664742862]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1978228313027643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1978228313027643 | validation: 2.314888665561675]
	TIME [epoch: 7.77 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7780783608928936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7780783608928936 | validation: 1.4791793925606378]
	TIME [epoch: 7.76 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2513023490118949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2513023490118949 | validation: 1.0941632560216261]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.225642301781666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.225642301781666 | validation: 1.0533659601478815]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0367703704559257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0367703704559257 | validation: 1.0190155987535574]
	TIME [epoch: 7.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3806064866990932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3806064866990932 | validation: 1.05602999489163]
	TIME [epoch: 7.77 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3843614603815177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3843614603815177 | validation: 2.1947715117025366]
	TIME [epoch: 7.77 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5821000504972829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5821000504972829 | validation: 2.1425903134395496]
	TIME [epoch: 7.77 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5380930555106156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5380930555106156 | validation: 2.145845647187216]
	TIME [epoch: 7.78 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5652130626552623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5652130626552623 | validation: 2.100546422105012]
	TIME [epoch: 7.8 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8191100187462137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8191100187462137 | validation: 2.8095374854650315]
	TIME [epoch: 7.77 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.623185828050345		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 2.623185828050345 | validation: 2.5119612564505447]
	TIME [epoch: 7.78 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.268010862014364		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 2.268010862014364 | validation: 2.3761097801826083]
	TIME [epoch: 7.77 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8084950914584315		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 1.8084950914584315 | validation: 2.2026990594514673]
	TIME [epoch: 7.8 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5769208720047447		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 1.5769208720047447 | validation: 2.149465259563358]
	TIME [epoch: 7.81 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4901789683689384		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 1.4901789683689384 | validation: 2.0033577683863912]
	TIME [epoch: 7.77 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.400934399359863		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 1.400934399359863 | validation: 1.9768986607590342]
	TIME [epoch: 7.77 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3643057107017764		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 1.3643057107017764 | validation: 2.3258424881748097]
	TIME [epoch: 7.78 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3874708000536324		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 1.3874708000536324 | validation: 1.073450405219785]
	TIME [epoch: 7.8 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8233220685912884		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 0.8233220685912884 | validation: 0.6971149921306488]
	TIME [epoch: 7.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6571372137091487		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 0.6571372137091487 | validation: 0.6908338987142404]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6737902206564831		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 0.6737902206564831 | validation: 0.6869875097660467]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.639766925318609		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 0.639766925318609 | validation: 0.6742945273361765]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.657447234769122		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 0.657447234769122 | validation: 0.6452404300728525]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6163515094511163		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 0.6163515094511163 | validation: 0.6148210266697763]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5696336466057035		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 0.5696336466057035 | validation: 0.6331908718759851]
	TIME [epoch: 7.77 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.80718182524706		[learning rate: 0.0094573]
	Learning Rate: 0.00945734
	LOSS [training: 0.80718182524706 | validation: 0.6817969517677236]
	TIME [epoch: 7.77 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5765544338976765		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 0.5765544338976765 | validation: 0.49073003840662244]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5731773225947874		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 0.5731773225947874 | validation: 0.7339819037692189]
	TIME [epoch: 7.81 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5571809225564193		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 0.5571809225564193 | validation: 0.5967129577847601]
	TIME [epoch: 7.77 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5119172986800815		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.5119172986800815 | validation: 0.46831915018499404]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44260978574949683		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 0.44260978574949683 | validation: 0.532177857260135]
	TIME [epoch: 7.77 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4621534489586823		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 0.4621534489586823 | validation: 0.6211585522149677]
	TIME [epoch: 7.79 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5259489750780745		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 0.5259489750780745 | validation: 0.43653069862333854]
	TIME [epoch: 7.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43211873193049255		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 0.43211873193049255 | validation: 0.4516834354356529]
	TIME [epoch: 7.78 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6426759818861059		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.6426759818861059 | validation: 0.7006787822091548]
	TIME [epoch: 7.77 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5228816913013838		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 0.5228816913013838 | validation: 0.3781352574946024]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3215048191357753		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 0.3215048191357753 | validation: 0.353079090567185]
	TIME [epoch: 7.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3780339474328546		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.3780339474328546 | validation: 0.4044528247072403]
	TIME [epoch: 7.79 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4510433559792444		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 0.4510433559792444 | validation: 0.43510139673346726]
	TIME [epoch: 7.77 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34392993593684756		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.34392993593684756 | validation: 0.3160722498661078]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3555969542989023		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.3555969542989023 | validation: 0.39784344752130407]
	TIME [epoch: 7.77 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3532931887909412		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.3532931887909412 | validation: 0.4220381610900692]
	TIME [epoch: 7.83 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3584222377456771		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.3584222377456771 | validation: 0.3473107408371773]
	TIME [epoch: 7.77 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3091766009204132		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.3091766009204132 | validation: 0.40127471270094384]
	TIME [epoch: 7.76 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38732902615386655		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.38732902615386655 | validation: 0.3232671240308417]
	TIME [epoch: 7.76 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2967175682286059		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.2967175682286059 | validation: 0.27575021903225827]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3030854514275583		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.3030854514275583 | validation: 0.36878887310694697]
	TIME [epoch: 7.82 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3143599902570199		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.3143599902570199 | validation: 0.42411587813824203]
	TIME [epoch: 7.75 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36602797133699333		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.36602797133699333 | validation: 0.3282309062208554]
	TIME [epoch: 7.76 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28996827568603956		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.28996827568603956 | validation: 0.27166053889860564]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25810956849660066		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.25810956849660066 | validation: 0.3343592442813167]
	TIME [epoch: 7.77 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37238644935690857		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.37238644935690857 | validation: 0.35941741160104645]
	TIME [epoch: 7.81 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32057577906460694		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.32057577906460694 | validation: 0.2964888492468374]
	TIME [epoch: 7.76 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25326424584446866		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.25326424584446866 | validation: 0.29183194226084846]
	TIME [epoch: 7.77 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3641271576106633		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.3641271576106633 | validation: 0.6863457023608259]
	TIME [epoch: 7.77 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38291884836033907		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.38291884836033907 | validation: 0.42318083806686757]
	TIME [epoch: 7.78 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26772298604584094		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.26772298604584094 | validation: 0.2801820508017614]
	TIME [epoch: 7.82 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28168056559459637		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.28168056559459637 | validation: 0.27643739586361904]
	TIME [epoch: 7.78 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26205981065540823		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.26205981065540823 | validation: 0.322569284505167]
	TIME [epoch: 7.76 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29160661197918025		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.29160661197918025 | validation: 0.25281039281215734]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2423538876759911		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.2423538876759911 | validation: 0.29050261375149533]
	TIME [epoch: 7.78 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27997277305873136		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.27997277305873136 | validation: 0.24391452485973247]
	TIME [epoch: 7.79 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19507490177405729		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.19507490177405729 | validation: 0.2704087096485043]
	TIME [epoch: 7.77 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3162107912183221		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.3162107912183221 | validation: 0.31448561750394477]
	TIME [epoch: 7.78 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2582636357448125		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.2582636357448125 | validation: 0.2571231924993469]
	TIME [epoch: 7.77 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25500240185179285		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.25500240185179285 | validation: 0.22902344957429543]
	TIME [epoch: 7.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24999580368714386		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.24999580368714386 | validation: 0.3430108150000639]
	TIME [epoch: 7.8 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26206382813013923		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.26206382813013923 | validation: 0.2671967937815879]
	TIME [epoch: 7.77 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2262333046348702		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.2262333046348702 | validation: 0.19812427323019505]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32022841684152187		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.32022841684152187 | validation: 0.3376414882168213]
	TIME [epoch: 7.77 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2511982582093199		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.2511982582093199 | validation: 0.20374951016817844]
	TIME [epoch: 7.81 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24104652811813504		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.24104652811813504 | validation: 0.23265137733809316]
	TIME [epoch: 7.78 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25619430025451356		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.25619430025451356 | validation: 0.2149817516506539]
	TIME [epoch: 7.77 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1996012370187605		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.1996012370187605 | validation: 0.4089224632179137]
	TIME [epoch: 7.77 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.305074473009489		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.305074473009489 | validation: 0.2447676361939168]
	TIME [epoch: 7.77 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2262772553655946		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.2262772553655946 | validation: 0.2106167494608286]
	TIME [epoch: 7.82 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22579527851628545		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.22579527851628545 | validation: 0.348902244721575]
	TIME [epoch: 7.77 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24920938548948599		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.24920938548948599 | validation: 0.21015558746943314]
	TIME [epoch: 7.77 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21838456646054563		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.21838456646054563 | validation: 0.22771907923843826]
	TIME [epoch: 7.77 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19250134437750566		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.19250134437750566 | validation: 0.17477072004026842]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21198053049865226		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.21198053049865226 | validation: 0.1702008194801376]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21947604431721246		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.21947604431721246 | validation: 0.22243438404366334]
	TIME [epoch: 7.77 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2185696074390847		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.2185696074390847 | validation: 0.17745096639308355]
	TIME [epoch: 7.76 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2599814755992253		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.2599814755992253 | validation: 0.2292549176942243]
	TIME [epoch: 7.76 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21922079234176264		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.21922079234176264 | validation: 0.2570844912483099]
	TIME [epoch: 7.77 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2058411316495694		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.2058411316495694 | validation: 0.22522436785301198]
	TIME [epoch: 7.81 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21523251449074174		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.21523251449074174 | validation: 0.16649066027559434]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16441012840658525		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.16441012840658525 | validation: 0.22741592395875743]
	TIME [epoch: 7.77 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22486069328497066		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.22486069328497066 | validation: 0.16933063572824958]
	TIME [epoch: 7.76 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15141269585083758		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.15141269585083758 | validation: 0.5065855158942558]
	TIME [epoch: 7.78 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27825697511589387		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.27825697511589387 | validation: 0.23060827239032092]
	TIME [epoch: 7.81 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19939966888911126		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.19939966888911126 | validation: 0.18594421848434012]
	TIME [epoch: 7.77 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17428427235983493		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.17428427235983493 | validation: 0.21185420490920198]
	TIME [epoch: 7.77 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18373774726498773		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.18373774726498773 | validation: 0.14597403103362944]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16295872479652612		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.16295872479652612 | validation: 0.17403207253599365]
	TIME [epoch: 7.79 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2118472827467539		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.2118472827467539 | validation: 0.21452288350961302]
	TIME [epoch: 7.81 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20606908912870728		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.20606908912870728 | validation: 0.20526232818497542]
	TIME [epoch: 7.77 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18502810747537624		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 0.18502810747537624 | validation: 0.17872678118921742]
	TIME [epoch: 7.77 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17227969663525936		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.17227969663525936 | validation: 0.1792978637767992]
	TIME [epoch: 7.77 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2187580574964173		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.2187580574964173 | validation: 0.23115909038277485]
	TIME [epoch: 7.79 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1922858717325038		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.1922858717325038 | validation: 0.16314064447002302]
	TIME [epoch: 7.79 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16276365814448596		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.16276365814448596 | validation: 0.1892361642381274]
	TIME [epoch: 7.77 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19214573706206872		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.19214573706206872 | validation: 0.15625847745886334]
	TIME [epoch: 7.77 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16027331178333734		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.16027331178333734 | validation: 0.2295858717099336]
	TIME [epoch: 7.76 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1971804200295851		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.1971804200295851 | validation: 0.15027986494214446]
	TIME [epoch: 7.8 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14036860822013714		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.14036860822013714 | validation: 0.19325525977529406]
	TIME [epoch: 7.79 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1864466088120757		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.1864466088120757 | validation: 0.14937897907385228]
	TIME [epoch: 7.76 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1636842738683355		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.1636842738683355 | validation: 0.1549676356800176]
	TIME [epoch: 7.77 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13664012520945543		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.13664012520945543 | validation: 0.16722557096688828]
	TIME [epoch: 7.76 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16529955504815397		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.16529955504815397 | validation: 0.19716931330170823]
	TIME [epoch: 7.8 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18299587585984628		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.18299587585984628 | validation: 0.13005933723594776]
	TIME [epoch: 7.79 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_151.pth
	Model improved!!!
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16985459472313383		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.16985459472313383 | validation: 0.2416241409370254]
	TIME [epoch: 7.77 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16384505676113603		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.16384505676113603 | validation: 0.1463868863676967]
	TIME [epoch: 7.77 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17830042172946609		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.17830042172946609 | validation: 0.28641077863691794]
	TIME [epoch: 7.77 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1903404904606942		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.1903404904606942 | validation: 0.19604687421210726]
	TIME [epoch: 7.82 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1995959969803063		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.1995959969803063 | validation: 0.25264350607695824]
	TIME [epoch: 7.78 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18360669284538494		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.18360669284538494 | validation: 0.17830358843608246]
	TIME [epoch: 7.77 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1620165278945966		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.1620165278945966 | validation: 0.13840000264566513]
	TIME [epoch: 7.77 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14686197608554702		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.14686197608554702 | validation: 0.21047439495899856]
	TIME [epoch: 7.78 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18960769185969112		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.18960769185969112 | validation: 0.1908267643018745]
	TIME [epoch: 7.81 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1871706096279242		[learning rate: 0.0067548]
	Learning Rate: 0.00675485
	LOSS [training: 0.1871706096279242 | validation: 0.16936933351743855]
	TIME [epoch: 7.78 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15422976920038645		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.15422976920038645 | validation: 0.12510869548259201]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14406227126073806		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.14406227126073806 | validation: 0.13429730605212886]
	TIME [epoch: 7.77 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15533189580562495		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.15533189580562495 | validation: 0.21388077817121387]
	TIME [epoch: 7.76 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1565399278097549		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.1565399278097549 | validation: 0.15268184307399624]
	TIME [epoch: 7.81 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17936180730487222		[learning rate: 0.0066363]
	Learning Rate: 0.00663625
	LOSS [training: 0.17936180730487222 | validation: 0.14632470040929652]
	TIME [epoch: 7.78 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1289822552903278		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.1289822552903278 | validation: 0.11416264140606755]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_167.pth
	Model improved!!!
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13105311670740757		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.13105311670740757 | validation: 0.19588977353613957]
	TIME [epoch: 7.77 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15103919553948658		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.15103919553948658 | validation: 0.1411080830112008]
	TIME [epoch: 7.77 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15946961209347715		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.15946961209347715 | validation: 0.15363936546162765]
	TIME [epoch: 7.81 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13946483921411837		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.13946483921411837 | validation: 0.20428236204216327]
	TIME [epoch: 7.77 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17774314480823275		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.17774314480823275 | validation: 0.11734554218103035]
	TIME [epoch: 7.77 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1259230321025232		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.1259230321025232 | validation: 0.1483972812397255]
	TIME [epoch: 7.77 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.168931662244537		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.168931662244537 | validation: 0.1961120359495414]
	TIME [epoch: 7.78 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17023110033382846		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 0.17023110033382846 | validation: 0.13532248786878193]
	TIME [epoch: 7.81 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.131137316334439		[learning rate: 0.0064053]
	Learning Rate: 0.00640529
	LOSS [training: 0.131137316334439 | validation: 0.11703645301594842]
	TIME [epoch: 7.77 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11356117476146999		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.11356117476146999 | validation: 0.14416688578762366]
	TIME [epoch: 7.77 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15385964286882114		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.15385964286882114 | validation: 0.1259474744815222]
	TIME [epoch: 7.77 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14936664475662845		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.14936664475662845 | validation: 0.252771617895247]
	TIME [epoch: 7.77 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15949795131859126		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.15949795131859126 | validation: 0.17883102172530096]
	TIME [epoch: 7.81 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1560545340702998		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.1560545340702998 | validation: 0.14826092676696634]
	TIME [epoch: 7.77 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1479644544986663		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.1479644544986663 | validation: 0.21391352817443027]
	TIME [epoch: 7.77 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16136726403655924		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.16136726403655924 | validation: 0.12537105905130572]
	TIME [epoch: 7.77 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12321352666185532		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.12321352666185532 | validation: 0.1874150581908216]
	TIME [epoch: 7.77 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15299160715751828		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.15299160715751828 | validation: 0.11123003477629897]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12826349288091243		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.12826349288091243 | validation: 0.11410833116039411]
	TIME [epoch: 7.77 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12204006657496212		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.12204006657496212 | validation: 0.14888007723204127]
	TIME [epoch: 7.77 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14363646739651528		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.14363646739651528 | validation: 0.1568635284638369]
	TIME [epoch: 7.77 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13017519007297831		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.13017519007297831 | validation: 0.17462368728333305]
	TIME [epoch: 7.79 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1405300021759952		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.1405300021759952 | validation: 0.17716983262763714]
	TIME [epoch: 7.81 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15142826414320612		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.15142826414320612 | validation: 0.13531715499439423]
	TIME [epoch: 7.77 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12213660080042987		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.12213660080042987 | validation: 0.13788526283689018]
	TIME [epoch: 7.77 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17286798287002733		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.17286798287002733 | validation: 0.18183305957656123]
	TIME [epoch: 7.77 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16519326842212376		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.16519326842212376 | validation: 0.13886747300337535]
	TIME [epoch: 7.79 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12117290204289491		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.12117290204289491 | validation: 0.11318462548803286]
	TIME [epoch: 7.8 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10684248705519804		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.10684248705519804 | validation: 0.10976888681248625]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12969616806920764		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.12969616806920764 | validation: 0.12230377556828437]
	TIME [epoch: 7.77 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14893611857596487		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.14893611857596487 | validation: 0.20968266275709757]
	TIME [epoch: 7.77 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13710816063739142		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.13710816063739142 | validation: 0.11968902346774167]
	TIME [epoch: 7.8 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12080745018027852		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.12080745018027852 | validation: 0.13254848448221804]
	TIME [epoch: 7.8 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10757016809621336		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.10757016809621336 | validation: 0.1200237678894414]
	TIME [epoch: 7.77 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13288407104135563		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.13288407104135563 | validation: 0.17166711757849606]
	TIME [epoch: 7.77 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12495199120340061		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.12495199120340061 | validation: 0.10356215748005798]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11291662136017727		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.11291662136017727 | validation: 0.16292558875132884]
	TIME [epoch: 7.82 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13546269772046154		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.13546269772046154 | validation: 0.1455481769270966]
	TIME [epoch: 7.79 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12505855121104723		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.12505855121104723 | validation: 0.11989655071542238]
	TIME [epoch: 7.77 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12244234167132587		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.12244234167132587 | validation: 0.14488150198477817]
	TIME [epoch: 7.77 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13551363335646568		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.13551363335646568 | validation: 0.10917464842402808]
	TIME [epoch: 7.77 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10725219106973628		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.10725219106973628 | validation: 0.12126111947009342]
	TIME [epoch: 7.82 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1015331945497388		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.1015331945497388 | validation: 0.1245073311772446]
	TIME [epoch: 7.77 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14500902529331564		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.14500902529331564 | validation: 0.18832015134877655]
	TIME [epoch: 7.77 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13148059325891212		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.13148059325891212 | validation: 0.12271711822696674]
	TIME [epoch: 7.77 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11150228182045287		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.11150228182045287 | validation: 0.12273571052864617]
	TIME [epoch: 7.77 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12510102809421408		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.12510102809421408 | validation: 0.10497505485691944]
	TIME [epoch: 7.81 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10938097206299272		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.10938097206299272 | validation: 0.19696405565769054]
	TIME [epoch: 7.77 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14060079922896151		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.14060079922896151 | validation: 0.10546313259619045]
	TIME [epoch: 7.77 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10665137470234126		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.10665137470234126 | validation: 0.13055177320324526]
	TIME [epoch: 7.76 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11405486625468576		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.11405486625468576 | validation: 0.17467997009963881]
	TIME [epoch: 7.77 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12887388050802462		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.12887388050802462 | validation: 0.12799791890985202]
	TIME [epoch: 7.82 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11844875351612774		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.11844875351612774 | validation: 0.14135461964125146]
	TIME [epoch: 7.77 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12660076092773606		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.12660076092773606 | validation: 0.14541405296533938]
	TIME [epoch: 7.77 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11722880371696248		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.11722880371696248 | validation: 0.11165743952574084]
	TIME [epoch: 7.77 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11913930199960843		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.11913930199960843 | validation: 0.12005281392832601]
	TIME [epoch: 7.77 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11305822142927519		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.11305822142927519 | validation: 0.11676678685329592]
	TIME [epoch: 7.82 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13904652623749583		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.13904652623749583 | validation: 0.10550046106168019]
	TIME [epoch: 7.77 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11331714869223918		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.11331714869223918 | validation: 0.10981450631365788]
	TIME [epoch: 7.77 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11680288458471189		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.11680288458471189 | validation: 0.10453548396744691]
	TIME [epoch: 7.77 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21675974129298545		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.21675974129298545 | validation: 0.17181311496375296]
	TIME [epoch: 7.78 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14821711048124694		[learning rate: 0.0053088]
	Learning Rate: 0.00530885
	LOSS [training: 0.14821711048124694 | validation: 0.13698676240720165]
	TIME [epoch: 7.82 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11340466870037512		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.11340466870037512 | validation: 0.14309210363765584]
	TIME [epoch: 7.78 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1461119415786652		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.1461119415786652 | validation: 0.14506704241565066]
	TIME [epoch: 7.77 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12548124161971755		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.12548124161971755 | validation: 0.12898624977509213]
	TIME [epoch: 7.78 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1152868764689393		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.1152868764689393 | validation: 0.18521201040410196]
	TIME [epoch: 7.78 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15047643159407054		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.15047643159407054 | validation: 0.14440309021384734]
	TIME [epoch: 7.82 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13191144223734091		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.13191144223734091 | validation: 0.1365870557635154]
	TIME [epoch: 7.78 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11793447411592141		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.11793447411592141 | validation: 0.14025315841593444]
	TIME [epoch: 7.78 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10919189425411382		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.10919189425411382 | validation: 0.11629494414544148]
	TIME [epoch: 7.77 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1248800413528888		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.1248800413528888 | validation: 0.14569718800656462]
	TIME [epoch: 7.79 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11596224251754238		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.11596224251754238 | validation: 0.10963324270122352]
	TIME [epoch: 7.82 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.111907377352754		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 0.111907377352754 | validation: 0.12498962890946538]
	TIME [epoch: 7.77 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12943345933184897		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.12943345933184897 | validation: 0.1255994647022765]
	TIME [epoch: 7.77 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12441031177566625		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.12441031177566625 | validation: 0.09999942181614008]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0959777999289002		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.0959777999289002 | validation: 0.11252937474285613]
	TIME [epoch: 7.79 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11531269662395793		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.11531269662395793 | validation: 0.13791839505634468]
	TIME [epoch: 7.8 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11935473864878465		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.11935473864878465 | validation: 0.1023892420276501]
	TIME [epoch: 7.77 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09992566013210735		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.09992566013210735 | validation: 0.13892844628767592]
	TIME [epoch: 7.77 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1113171663314074		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.1113171663314074 | validation: 0.09566166383478093]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09890217967657583		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.09890217967657583 | validation: 0.149049485462514]
	TIME [epoch: 7.79 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13771733384585877		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.13771733384585877 | validation: 0.13756006496518425]
	TIME [epoch: 7.79 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12216158418856587		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 0.12216158418856587 | validation: 0.10580695619985227]
	TIME [epoch: 7.77 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1159495455721154		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.1159495455721154 | validation: 0.10349126627027215]
	TIME [epoch: 7.77 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11749871248553119		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.11749871248553119 | validation: 0.11765992274053655]
	TIME [epoch: 7.77 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11240787992948799		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.11240787992948799 | validation: 0.14886633041900776]
	TIME [epoch: 7.8 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11041806558469394		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.11041806558469394 | validation: 0.12577749216904432]
	TIME [epoch: 7.8 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11333568690137935		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.11333568690137935 | validation: 0.10471331455967511]
	TIME [epoch: 7.77 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10284880400136709		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.10284880400136709 | validation: 0.11289195712272157]
	TIME [epoch: 7.77 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09549951801722023		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.09549951801722023 | validation: 0.09091664125432258]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_257.pth
	Model improved!!!
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09991667773561883		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.09991667773561883 | validation: 0.1383141205188477]
	TIME [epoch: 7.81 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13188884425557154		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.13188884425557154 | validation: 0.12591933172239816]
	TIME [epoch: 7.78 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1011214921053217		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.1011214921053217 | validation: 0.11253114330637952]
	TIME [epoch: 7.76 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11347342895226512		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.11347342895226512 | validation: 0.14738571485888552]
	TIME [epoch: 7.76 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1430444932580799		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.1430444932580799 | validation: 0.12097475956680756]
	TIME [epoch: 7.77 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10337720652185492		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.10337720652185492 | validation: 0.09234690305729817]
	TIME [epoch: 7.81 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10992104288053225		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.10992104288053225 | validation: 0.11368877565068575]
	TIME [epoch: 7.78 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13428724987425986		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.13428724987425986 | validation: 0.11855136350793928]
	TIME [epoch: 7.77 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10375513298376088		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.10375513298376088 | validation: 0.105699205572374]
	TIME [epoch: 7.77 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10619263351199286		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.10619263351199286 | validation: 0.09886186750884829]
	TIME [epoch: 7.76 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13039200637416976		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.13039200637416976 | validation: 0.10074947000300082]
	TIME [epoch: 7.82 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09748797141550153		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.09748797141550153 | validation: 0.09629417472620708]
	TIME [epoch: 7.77 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10422403630844367		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.10422403630844367 | validation: 0.08943707725479422]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_270.pth
	Model improved!!!
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09777282181521776		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.09777282181521776 | validation: 0.10085321646959938]
	TIME [epoch: 7.77 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10021683793926243		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.10021683793926243 | validation: 0.09976297796409789]
	TIME [epoch: 7.76 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.098014636790962		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.098014636790962 | validation: 0.11237392453309052]
	TIME [epoch: 7.81 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1224627263405861		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.1224627263405861 | validation: 0.11056625707711179]
	TIME [epoch: 7.78 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1456075654973311		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.1456075654973311 | validation: 0.17266728719288105]
	TIME [epoch: 7.77 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13890479443973902		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.13890479443973902 | validation: 0.12100343657158022]
	TIME [epoch: 7.76 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11902431806056936		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.11902431806056936 | validation: 0.10739430539746647]
	TIME [epoch: 7.77 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11107653470129301		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.11107653470129301 | validation: 0.11618330076823892]
	TIME [epoch: 7.82 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11471557443111133		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.11471557443111133 | validation: 0.11533891923290435]
	TIME [epoch: 7.77 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10880577889823488		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.10880577889823488 | validation: 0.1027872061799056]
	TIME [epoch: 7.77 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11221605012431621		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.11221605012431621 | validation: 0.11361795351548709]
	TIME [epoch: 7.77 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09567959312424987		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.09567959312424987 | validation: 0.0959995371991467]
	TIME [epoch: 7.77 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1090065678941899		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.1090065678941899 | validation: 0.10658218636791558]
	TIME [epoch: 7.82 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1037693054895409		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.1037693054895409 | validation: 0.10757929195801164]
	TIME [epoch: 7.77 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09817971237371077		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.09817971237371077 | validation: 0.10000368935479659]
	TIME [epoch: 7.76 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16232736203793197		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.16232736203793197 | validation: 0.14749905380554218]
	TIME [epoch: 7.76 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14514463674319927		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.14514463674319927 | validation: 0.12619582838430934]
	TIME [epoch: 7.77 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1257557546592375		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.1257557546592375 | validation: 0.10505572939314245]
	TIME [epoch: 7.81 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10808253186090407		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.10808253186090407 | validation: 0.1001176985834681]
	TIME [epoch: 7.78 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11125931695172843		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.11125931695172843 | validation: 0.09960077140572851]
	TIME [epoch: 7.77 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11123955220148823		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.11123955220148823 | validation: 0.13303525000184563]
	TIME [epoch: 7.77 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11749434848723928		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.11749434848723928 | validation: 0.10040167742247752]
	TIME [epoch: 7.78 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09530928629217666		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.09530928629217666 | validation: 0.0849706214803817]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_293.pth
	Model improved!!!
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0958849672316766		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.0958849672316766 | validation: 0.126759863194344]
	TIME [epoch: 7.76 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10492915361985894		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.10492915361985894 | validation: 0.09272769442181758]
	TIME [epoch: 7.77 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09455383628166195		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.09455383628166195 | validation: 0.10707373769690215]
	TIME [epoch: 7.76 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11994632840825677		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.11994632840825677 | validation: 0.0954428219769177]
	TIME [epoch: 7.78 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09943452512754607		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.09943452512754607 | validation: 0.10156338672904858]
	TIME [epoch: 7.8 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10206488804327579		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.10206488804327579 | validation: 0.12136504475322638]
	TIME [epoch: 7.77 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09834237804853599		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.09834237804853599 | validation: 0.10094226057045588]
	TIME [epoch: 7.77 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10697137817009145		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.10697137817009145 | validation: 0.10894787864274004]
	TIME [epoch: 7.77 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09396414394730877		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.09396414394730877 | validation: 0.09604093064687783]
	TIME [epoch: 7.8 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09164689022037259		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.09164689022037259 | validation: 0.0994216850127033]
	TIME [epoch: 7.79 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09949659639065331		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.09949659639065331 | validation: 0.10059152898849236]
	TIME [epoch: 7.77 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10102816293474876		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.10102816293474876 | validation: 0.1050009886156329]
	TIME [epoch: 7.76 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10570125772085166		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.10570125772085166 | validation: 0.10318459562756761]
	TIME [epoch: 7.76 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09229489593525089		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.09229489593525089 | validation: 0.09626676083889141]
	TIME [epoch: 7.79 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08531739271637795		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.08531739271637795 | validation: 0.10347149875243888]
	TIME [epoch: 7.8 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.100977885210877		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.100977885210877 | validation: 0.11359105254708364]
	TIME [epoch: 7.77 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09971220301193832		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.09971220301193832 | validation: 0.10113844784866656]
	TIME [epoch: 7.77 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09677022077199801		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.09677022077199801 | validation: 0.09117714576860148]
	TIME [epoch: 7.77 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08918918633033528		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.08918918633033528 | validation: 0.09830901646013836]
	TIME [epoch: 7.81 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09033252559125163		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.09033252559125163 | validation: 0.11939063300165989]
	TIME [epoch: 7.79 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10815558214828017		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.10815558214828017 | validation: 0.11197713411485372]
	TIME [epoch: 7.77 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09381455276798253		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.09381455276798253 | validation: 0.10233652276677689]
	TIME [epoch: 7.77 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09252106839118346		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.09252106839118346 | validation: 0.11287993449630106]
	TIME [epoch: 7.77 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09869150582924045		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.09869150582924045 | validation: 0.12124634576144772]
	TIME [epoch: 7.8 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09875671466146323		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.09875671466146323 | validation: 0.11955772898551328]
	TIME [epoch: 7.79 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09488849721068777		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.09488849721068777 | validation: 0.10739551045936788]
	TIME [epoch: 7.77 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09778664633674199		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.09778664633674199 | validation: 0.12216071361149367]
	TIME [epoch: 7.77 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09130002101030854		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.09130002101030854 | validation: 0.09621588253670252]
	TIME [epoch: 7.76 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09265699007027421		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.09265699007027421 | validation: 0.10675514542304781]
	TIME [epoch: 7.81 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08796711094233568		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.08796711094233568 | validation: 0.11449288672884628]
	TIME [epoch: 7.78 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10619607794676234		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.10619607794676234 | validation: 0.08484457633862838]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_324.pth
	Model improved!!!
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08788921955581554		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.08788921955581554 | validation: 0.09771819112090038]
	TIME [epoch: 7.75 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0887542309384804		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.0887542309384804 | validation: 0.10966830196264428]
	TIME [epoch: 7.77 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08491131288523288		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.08491131288523288 | validation: 0.09425378226649173]
	TIME [epoch: 7.82 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.095714533212789		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.095714533212789 | validation: 0.12067746991120615]
	TIME [epoch: 7.78 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10070911651776976		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.10070911651776976 | validation: 0.09663168265569327]
	TIME [epoch: 7.77 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08944058245179243		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.08944058245179243 | validation: 0.10205065081273748]
	TIME [epoch: 7.77 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10991834344425486		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.10991834344425486 | validation: 0.10373574234058033]
	TIME [epoch: 7.77 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09201309516453549		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.09201309516453549 | validation: 0.10344085035350237]
	TIME [epoch: 7.82 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08527874722570392		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.08527874722570392 | validation: 0.08414836598817817]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_333.pth
	Model improved!!!
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08573094649435423		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.08573094649435423 | validation: 0.12852280395296306]
	TIME [epoch: 7.77 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09904968327879367		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.09904968327879367 | validation: 0.0868739510468575]
	TIME [epoch: 7.77 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08666214337514863		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.08666214337514863 | validation: 0.11456790354754559]
	TIME [epoch: 7.77 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09614935165980837		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.09614935165980837 | validation: 0.10730885525014836]
	TIME [epoch: 7.82 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09279511549442351		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.09279511549442351 | validation: 0.09346564145694838]
	TIME [epoch: 7.76 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09074531783481302		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.09074531783481302 | validation: 0.09578344404764962]
	TIME [epoch: 7.77 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09197080302371888		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.09197080302371888 | validation: 0.08766660725322467]
	TIME [epoch: 7.76 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09644790224820082		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.09644790224820082 | validation: 0.08591295398598386]
	TIME [epoch: 7.78 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08785517993722523		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.08785517993722523 | validation: 0.09377343919342387]
	TIME [epoch: 7.82 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08092915493017656		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.08092915493017656 | validation: 0.1375157410973171]
	TIME [epoch: 7.77 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10860993315884865		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.10860993315884865 | validation: 0.0883860265693544]
	TIME [epoch: 7.77 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08805034009256363		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.08805034009256363 | validation: 0.08805544864560898]
	TIME [epoch: 7.76 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0915967719392279		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.0915967719392279 | validation: 0.08341762647906664]
	TIME [epoch: 7.79 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_346.pth
	Model improved!!!
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08316123152681339		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.08316123152681339 | validation: 0.08909925397229819]
	TIME [epoch: 7.81 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10285440648073893		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.10285440648073893 | validation: 0.08514421039985458]
	TIME [epoch: 7.76 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0856073446386505		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.0856073446386505 | validation: 0.08979771887970264]
	TIME [epoch: 7.76 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08500994088906817		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.08500994088906817 | validation: 0.08709426367128661]
	TIME [epoch: 7.77 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08922576150949255		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.08922576150949255 | validation: 0.09186742491420875]
	TIME [epoch: 7.77 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09101547727691708		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.09101547727691708 | validation: 0.09599767662390472]
	TIME [epoch: 7.81 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09078390818273305		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.09078390818273305 | validation: 0.10399034461342951]
	TIME [epoch: 7.77 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08467773783663726		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.08467773783663726 | validation: 0.08455830646526319]
	TIME [epoch: 7.77 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08599837303436347		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.08599837303436347 | validation: 0.09659579024063342]
	TIME [epoch: 7.77 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08662370771300017		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.08662370771300017 | validation: 0.08605217989128312]
	TIME [epoch: 7.79 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08146292629047142		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.08146292629047142 | validation: 0.104459315137586]
	TIME [epoch: 7.8 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10253800563956578		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.10253800563956578 | validation: 0.10909735246835557]
	TIME [epoch: 7.77 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08953988833948905		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.08953988833948905 | validation: 0.10165833239777783]
	TIME [epoch: 7.76 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08017529771541113		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.08017529771541113 | validation: 0.09082917973063057]
	TIME [epoch: 7.77 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08702036585648835		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.08702036585648835 | validation: 0.0930151852986296]
	TIME [epoch: 7.79 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09264590750362402		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.09264590750362402 | validation: 0.085767452180046]
	TIME [epoch: 7.8 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08398157126889802		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.08398157126889802 | validation: 0.10229539139282223]
	TIME [epoch: 7.77 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08550554772159945		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.08550554772159945 | validation: 0.09055634327394904]
	TIME [epoch: 7.77 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09094765421130152		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.09094765421130152 | validation: 0.0827270494048384]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_365.pth
	Model improved!!!
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0801827444573125		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.0801827444573125 | validation: 0.08611579405293054]
	TIME [epoch: 7.8 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08490451905519555		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.08490451905519555 | validation: 0.10520626532604029]
	TIME [epoch: 7.79 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0858099774181591		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.0858099774181591 | validation: 0.09684162170742444]
	TIME [epoch: 7.77 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08676470496006482		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.08676470496006482 | validation: 0.09196540838765133]
	TIME [epoch: 7.77 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08374030642555032		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.08374030642555032 | validation: 0.09015137626280031]
	TIME [epoch: 7.76 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0879991657117675		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.0879991657117675 | validation: 0.10392721735103053]
	TIME [epoch: 7.79 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09202772798431906		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.09202772798431906 | validation: 0.11196841174011275]
	TIME [epoch: 7.79 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08623967028018677		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.08623967028018677 | validation: 0.1019041073539201]
	TIME [epoch: 7.77 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09266303488971095		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.09266303488971095 | validation: 0.08520860802525776]
	TIME [epoch: 7.76 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08592339313513736		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.08592339313513736 | validation: 0.11711752443718505]
	TIME [epoch: 7.77 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09061212423138687		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.09061212423138687 | validation: 0.09294322281536896]
	TIME [epoch: 7.8 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08081040104593923		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.08081040104593923 | validation: 0.09616489130537309]
	TIME [epoch: 7.78 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09538460246259473		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.09538460246259473 | validation: 0.0953006814557255]
	TIME [epoch: 7.76 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08282668653009377		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.08282668653009377 | validation: 0.08091324333428157]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_379.pth
	Model improved!!!
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08233606920699485		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.08233606920699485 | validation: 0.08581449743576486]
	TIME [epoch: 7.77 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08858577766710599		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.08858577766710599 | validation: 0.08402839927053127]
	TIME [epoch: 7.81 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08027807587521654		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.08027807587521654 | validation: 0.08270617793474727]
	TIME [epoch: 7.77 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08489151358130989		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.08489151358130989 | validation: 0.08127844841854051]
	TIME [epoch: 7.76 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07977556977822135		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.07977556977822135 | validation: 0.08652244595077475]
	TIME [epoch: 7.76 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08262581127869113		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.08262581127869113 | validation: 0.07977480479918883]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_385.pth
	Model improved!!!
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08514193026973048		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.08514193026973048 | validation: 0.10359775437214357]
	TIME [epoch: 7.81 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08402190318248712		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.08402190318248712 | validation: 0.11056320865392649]
	TIME [epoch: 7.77 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08484037435356094		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.08484037435356094 | validation: 0.08738420471520295]
	TIME [epoch: 7.76 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08232422320016462		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.08232422320016462 | validation: 0.08536621211956583]
	TIME [epoch: 7.76 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08604261857704748		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.08604261857704748 | validation: 0.1311346522319845]
	TIME [epoch: 7.77 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09503367146351702		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.09503367146351702 | validation: 0.0964450090633519]
	TIME [epoch: 7.8 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08180730769586494		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.08180730769586494 | validation: 0.07784387500734598]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_392.pth
	Model improved!!!
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13709217587989303		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.13709217587989303 | validation: 0.12914752330341683]
	TIME [epoch: 7.76 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10873324177020166		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.10873324177020166 | validation: 0.10715594291867885]
	TIME [epoch: 7.76 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09167453083652072		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.09167453083652072 | validation: 0.10484718502583981]
	TIME [epoch: 7.77 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10827784010982877		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.10827784010982877 | validation: 0.13838605781476043]
	TIME [epoch: 7.8 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09953815111010463		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.09953815111010463 | validation: 0.0939344154479653]
	TIME [epoch: 7.76 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09380019634594991		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.09380019634594991 | validation: 0.0838382284717073]
	TIME [epoch: 7.76 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0899863330496398		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.0899863330496398 | validation: 0.08456886534670066]
	TIME [epoch: 7.76 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09767137901088041		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.09767137901088041 | validation: 0.1579891322345074]
	TIME [epoch: 7.77 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10780307902934089		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.10780307902934089 | validation: 0.0974285063557056]
	TIME [epoch: 7.8 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09585945650019548		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.09585945650019548 | validation: 0.0980860007590203]
	TIME [epoch: 7.76 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09001344835312565		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.09001344835312565 | validation: 0.08730871788765035]
	TIME [epoch: 7.76 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08416245287550522		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.08416245287550522 | validation: 0.0788696202447293]
	TIME [epoch: 7.76 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09514328374775367		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.09514328374775367 | validation: 0.08098086373355379]
	TIME [epoch: 7.77 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08302186276316703		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.08302186276316703 | validation: 0.08688282476005851]
	TIME [epoch: 7.79 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08265731153885439		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.08265731153885439 | validation: 0.08477242981675556]
	TIME [epoch: 7.76 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08111206249315196		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.08111206249315196 | validation: 0.08588079078231971]
	TIME [epoch: 7.76 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08299549166044641		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.08299549166044641 | validation: 0.08209858293853309]
	TIME [epoch: 7.76 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08435748424917519		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.08435748424917519 | validation: 0.09926439941139188]
	TIME [epoch: 7.78 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08491099199944885		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.08491099199944885 | validation: 0.09338986983819644]
	TIME [epoch: 7.79 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08784945128631974		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.08784945128631974 | validation: 0.08344391067804145]
	TIME [epoch: 7.76 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10131120894533742		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.10131120894533742 | validation: 0.098045997106257]
	TIME [epoch: 7.76 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08533125617692314		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.08533125617692314 | validation: 0.09533633419934415]
	TIME [epoch: 7.76 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08126353095462911		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.08126353095462911 | validation: 0.09144713440827029]
	TIME [epoch: 7.79 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13567278559062423		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.13567278559062423 | validation: 0.11001802084833655]
	TIME [epoch: 7.79 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11787461029209823		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.11787461029209823 | validation: 0.08664018148635594]
	TIME [epoch: 7.76 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09089296257797744		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.09089296257797744 | validation: 0.07852430807830954]
	TIME [epoch: 7.76 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08216859861484149		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.08216859861484149 | validation: 0.0775327137174246]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_419.pth
	Model improved!!!
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07924845240417935		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.07924845240417935 | validation: 0.08035103979743106]
	TIME [epoch: 7.79 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0789949455828264		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.0789949455828264 | validation: 0.07475388028322377]
	TIME [epoch: 7.79 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_421.pth
	Model improved!!!
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07919737069098345		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.07919737069098345 | validation: 0.07796763815915468]
	TIME [epoch: 7.76 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09019533054010848		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.09019533054010848 | validation: 0.07597738923023206]
	TIME [epoch: 7.76 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.078041226385708		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.078041226385708 | validation: 0.0765314060609814]
	TIME [epoch: 7.76 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07663996607076519		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.07663996607076519 | validation: 0.09542476296051366]
	TIME [epoch: 7.81 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10208502399725103		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.10208502399725103 | validation: 0.08148939507908096]
	TIME [epoch: 7.78 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0802336641937043		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.0802336641937043 | validation: 0.09093383875369945]
	TIME [epoch: 7.77 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08373913922759368		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.08373913922759368 | validation: 0.07889639049548638]
	TIME [epoch: 7.76 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08099311730272687		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.08099311730272687 | validation: 0.09056457610722513]
	TIME [epoch: 7.76 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08772209270100047		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.08772209270100047 | validation: 0.09292787119985081]
	TIME [epoch: 7.82 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07932025556991265		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.07932025556991265 | validation: 0.08508261478034403]
	TIME [epoch: 7.77 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08042640329035411		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.08042640329035411 | validation: 0.10411303943513794]
	TIME [epoch: 7.77 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08925327736062072		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.08925327736062072 | validation: 0.07875452071592928]
	TIME [epoch: 7.77 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07508866583118305		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.07508866583118305 | validation: 0.07567594783175982]
	TIME [epoch: 7.77 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0774503206677527		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.0774503206677527 | validation: 0.08951202285293725]
	TIME [epoch: 7.82 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0803213461248096		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.0803213461248096 | validation: 0.08557878585035233]
	TIME [epoch: 7.77 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07834477579473358		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.07834477579473358 | validation: 0.0923932996763194]
	TIME [epoch: 7.77 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08237037934195408		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.08237037934195408 | validation: 0.08161872407930165]
	TIME [epoch: 7.77 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08193972394790518		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.08193972394790518 | validation: 0.0961278765879657]
	TIME [epoch: 7.75 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08409232017285184		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.08409232017285184 | validation: 0.07956948626907567]
	TIME [epoch: 7.82 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07749858421315595		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.07749858421315595 | validation: 0.08510921271226277]
	TIME [epoch: 7.77 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08227419743578368		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.08227419743578368 | validation: 0.0948844096209549]
	TIME [epoch: 7.77 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08824404152371332		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.08824404152371332 | validation: 0.09137222334522592]
	TIME [epoch: 7.77 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08445299496118558		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.08445299496118558 | validation: 0.07930054273815391]
	TIME [epoch: 7.77 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07617310781989521		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.07617310781989521 | validation: 0.0899876592525539]
	TIME [epoch: 7.8 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08378536559374453		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.08378536559374453 | validation: 0.07538354443986586]
	TIME [epoch: 7.77 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0823780954667936		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.0823780954667936 | validation: 0.08181487398857366]
	TIME [epoch: 7.76 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07556451302186361		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.07556451302186361 | validation: 0.08811231844697234]
	TIME [epoch: 7.76 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08175724118445127		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.08175724118445127 | validation: 0.08279521647600621]
	TIME [epoch: 7.77 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0839055356525795		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.0839055356525795 | validation: 0.07839535568934156]
	TIME [epoch: 7.81 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08263954086651203		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.08263954086651203 | validation: 0.08440777978775132]
	TIME [epoch: 7.76 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08243058926810354		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.08243058926810354 | validation: 0.08672169804796681]
	TIME [epoch: 7.76 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08402057403073022		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.08402057403073022 | validation: 0.09890303218012776]
	TIME [epoch: 7.76 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08566552172474795		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.08566552172474795 | validation: 0.08332229456175752]
	TIME [epoch: 7.77 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07824718168247052		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.07824718168247052 | validation: 0.07859248914531888]
	TIME [epoch: 7.8 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07628157174846359		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.07628157174846359 | validation: 0.07487209860707657]
	TIME [epoch: 7.77 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07787197354026268		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.07787197354026268 | validation: 0.08404336296556261]
	TIME [epoch: 7.77 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07673433557351425		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.07673433557351425 | validation: 0.07793594010658211]
	TIME [epoch: 7.76 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08218511722966983		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.08218511722966983 | validation: 0.0897930289716032]
	TIME [epoch: 7.78 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08099459010950849		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.08099459010950849 | validation: 0.077577842105619]
	TIME [epoch: 7.81 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07437318082095228		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.07437318082095228 | validation: 0.0868029347705766]
	TIME [epoch: 7.77 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07539111117681531		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.07539111117681531 | validation: 0.08143992598388464]
	TIME [epoch: 7.76 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08012778106915552		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.08012778106915552 | validation: 0.11087274285714602]
	TIME [epoch: 7.76 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07862411542452385		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.07862411542452385 | validation: 0.0731035563598969]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_464.pth
	Model improved!!!
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07349346468591925		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.07349346468591925 | validation: 0.07546190604024636]
	TIME [epoch: 8.11 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07747663975121093		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.07747663975121093 | validation: 0.08230484925736753]
	TIME [epoch: 7.78 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07535326037034219		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.07535326037034219 | validation: 0.07962961129641107]
	TIME [epoch: 7.78 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07330664453304514		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.07330664453304514 | validation: 0.08125026178431277]
	TIME [epoch: 7.78 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07266073833533716		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.07266073833533716 | validation: 0.07449510032333015]
	TIME [epoch: 7.8 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08343921468687365		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.08343921468687365 | validation: 0.08136449430533871]
	TIME [epoch: 7.81 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08264344489023953		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.08264344489023953 | validation: 0.07521441724632025]
	TIME [epoch: 7.78 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0762759221983451		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.0762759221983451 | validation: 0.07693745415157499]
	TIME [epoch: 7.78 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07564641138962905		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.07564641138962905 | validation: 0.07933525002219807]
	TIME [epoch: 7.78 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08239228018949495		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.08239228018949495 | validation: 0.07397797996315003]
	TIME [epoch: 7.81 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07734273903662113		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.07734273903662113 | validation: 0.07791008473437162]
	TIME [epoch: 7.81 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07469949424836492		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.07469949424836492 | validation: 0.08498837649884651]
	TIME [epoch: 7.78 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07324811641631324		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.07324811641631324 | validation: 0.07234615108018225]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_477.pth
	Model improved!!!
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07419026742702134		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.07419026742702134 | validation: 0.07242575266668329]
	TIME [epoch: 7.77 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07516303681793246		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.07516303681793246 | validation: 0.08254323309523198]
	TIME [epoch: 7.81 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07734917597393431		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.07734917597393431 | validation: 0.06981311246647003]
	TIME [epoch: 7.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_480.pth
	Model improved!!!
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07249043752628113		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.07249043752628113 | validation: 0.07204765360941448]
	TIME [epoch: 7.78 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07492947483893342		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.07492947483893342 | validation: 0.10781328830535479]
	TIME [epoch: 7.77 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08064449767062656		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.08064449767062656 | validation: 0.07053105501157628]
	TIME [epoch: 7.77 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0736128938313542		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.0736128938313542 | validation: 0.07371914778491366]
	TIME [epoch: 7.82 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0754980379479964		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.0754980379479964 | validation: 0.15906706501020182]
	TIME [epoch: 7.79 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09580685414709834		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.09580685414709834 | validation: 0.07413076410305272]
	TIME [epoch: 7.77 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07438219574428909		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.07438219574428909 | validation: 0.06972938136451212]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_487.pth
	Model improved!!!
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07235302061450573		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.07235302061450573 | validation: 0.08796110202326343]
	TIME [epoch: 7.78 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08719849166578861		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.08719849166578861 | validation: 0.09147467481497158]
	TIME [epoch: 7.81 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0741075455386111		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.0741075455386111 | validation: 0.07615954382035497]
	TIME [epoch: 7.77 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07257528841404		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.07257528841404 | validation: 0.08651535357815121]
	TIME [epoch: 7.78 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07474414616771552		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.07474414616771552 | validation: 0.07116490532887802]
	TIME [epoch: 7.77 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0724725319379681		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.0724725319379681 | validation: 0.0789214380187052]
	TIME [epoch: 7.77 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07244716651497152		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.07244716651497152 | validation: 0.07758069206267527]
	TIME [epoch: 7.81 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07564782280495386		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.07564782280495386 | validation: 0.0741688167209553]
	TIME [epoch: 7.78 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0752725484946337		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.0752725484946337 | validation: 0.10561883462722874]
	TIME [epoch: 7.77 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08173192034990126		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.08173192034990126 | validation: 0.08245945303975562]
	TIME [epoch: 7.77 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07642148103114434		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.07642148103114434 | validation: 0.07568659590124188]
	TIME [epoch: 7.78 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07102196933659663		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.07102196933659663 | validation: 0.07007516292568505]
	TIME [epoch: 7.82 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07132040078961367		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.07132040078961367 | validation: 0.07974946553625664]
	TIME [epoch: 7.77 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08350636324747483		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.08350636324747483 | validation: 0.08975636200689947]
	TIME [epoch: 7.77 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07527153985899811		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.07527153985899811 | validation: 0.07413039131222585]
	TIME [epoch: 7.77 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07321154702870536		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.07321154702870536 | validation: 0.08375636193142627]
	TIME [epoch: 7.78 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08464932619856275		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.08464932619856275 | validation: 0.08345133006323924]
	TIME [epoch: 7.81 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07650622668402485		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.07650622668402485 | validation: 0.0774180604740775]
	TIME [epoch: 7.76 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07052307595072461		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.07052307595072461 | validation: 0.0843507475130105]
	TIME [epoch: 7.77 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0733043781015307		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.0733043781015307 | validation: 0.07547057596031435]
	TIME [epoch: 7.76 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07641741606751708		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.07641741606751708 | validation: 0.08497640457673075]
	TIME [epoch: 7.78 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07703263828470128		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.07703263828470128 | validation: 0.10424173837489957]
	TIME [epoch: 7.81 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07862751034954339		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.07862751034954339 | validation: 0.0801872813578581]
	TIME [epoch: 7.77 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07340847784630103		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.07340847784630103 | validation: 0.07949327807244294]
	TIME [epoch: 7.77 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07620590490539622		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.07620590490539622 | validation: 0.08656143727458909]
	TIME [epoch: 7.76 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07957489819016682		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.07957489819016682 | validation: 0.07990688385207714]
	TIME [epoch: 7.79 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0722772397978		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.0722772397978 | validation: 0.07367814784998661]
	TIME [epoch: 7.8 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07487313763398673		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.07487313763398673 | validation: 0.0822405445147609]
	TIME [epoch: 7.76 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07596974773761261		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.07596974773761261 | validation: 0.08657999289557755]
	TIME [epoch: 7.77 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0731799414099322		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.0731799414099322 | validation: 0.07961260839675194]
	TIME [epoch: 7.76 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07404864593343095		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.07404864593343095 | validation: 0.08469312730672034]
	TIME [epoch: 7.78 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0849491081544653		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.0849491081544653 | validation: 0.09347058361421265]
	TIME [epoch: 7.8 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08203273964508788		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.08203273964508788 | validation: 0.07443478679945306]
	TIME [epoch: 7.77 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07087788571750137		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.07087788571750137 | validation: 0.07159943049877263]
	TIME [epoch: 7.76 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07128144217721805		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.07128144217721805 | validation: 0.07377526749056866]
	TIME [epoch: 7.76 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07014628486042612		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.07014628486042612 | validation: 0.08544771319601294]
	TIME [epoch: 7.78 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0750170176091128		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.0750170176091128 | validation: 0.07561994326472059]
	TIME [epoch: 7.8 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06949548117663575		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.06949548117663575 | validation: 0.07693572443881032]
	TIME [epoch: 7.77 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0758959673493459		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.0758959673493459 | validation: 0.07133539694215987]
	TIME [epoch: 7.77 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0687528608140041		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.0687528608140041 | validation: 0.09410804027163241]
	TIME [epoch: 7.76 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08222507067299686		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.08222507067299686 | validation: 0.08188406424414114]
	TIME [epoch: 7.8 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07298204142895126		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.07298204142895126 | validation: 0.0730312587874026]
	TIME [epoch: 7.79 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07133800700587713		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.07133800700587713 | validation: 0.06737672173165457]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_530.pth
	Model improved!!!
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06960061502615536		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.06960061502615536 | validation: 0.07380203571972258]
	TIME [epoch: 7.77 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07281954444304391		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.07281954444304391 | validation: 0.07052480995960186]
	TIME [epoch: 7.76 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10445986893552114		[learning rate: 0.0018085]
	Learning Rate: 0.00180845
	LOSS [training: 0.10445986893552114 | validation: 0.28472417028936015]
	TIME [epoch: 7.8 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1889916361955443		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.1889916361955443 | validation: 0.0980825938909287]
	TIME [epoch: 7.78 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.091430985769169		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.091430985769169 | validation: 0.08162819143862253]
	TIME [epoch: 7.76 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08325866591844157		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.08325866591844157 | validation: 0.08423636007105172]
	TIME [epoch: 7.76 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08382582253037561		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.08382582253037561 | validation: 0.08449458512529268]
	TIME [epoch: 7.77 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08130303988578648		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.08130303988578648 | validation: 0.0765278562743942]
	TIME [epoch: 7.8 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07745532485412594		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.07745532485412594 | validation: 0.07477809041104576]
	TIME [epoch: 7.78 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07562772364297811		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.07562772364297811 | validation: 0.07455093107306315]
	TIME [epoch: 7.76 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0772014249808716		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.0772014249808716 | validation: 0.07627250970487841]
	TIME [epoch: 7.77 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07446945146040154		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.07446945146040154 | validation: 0.0780372308283209]
	TIME [epoch: 7.77 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07952327973510694		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.07952327973510694 | validation: 0.07311503015605618]
	TIME [epoch: 7.8 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07306493917070571		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.07306493917070571 | validation: 0.07684799868255202]
	TIME [epoch: 7.77 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07395368314516604		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.07395368314516604 | validation: 0.07913637204783108]
	TIME [epoch: 7.76 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07538498723465066		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.07538498723465066 | validation: 0.09858910408436869]
	TIME [epoch: 7.76 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07877607955017266		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.07877607955017266 | validation: 0.06775248197464864]
	TIME [epoch: 7.76 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07018642095615406		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.07018642095615406 | validation: 0.08479696037845309]
	TIME [epoch: 7.81 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08052443874505405		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.08052443874505405 | validation: 0.08320521999076544]
	TIME [epoch: 7.76 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07705995497041117		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.07705995497041117 | validation: 0.07800591412773278]
	TIME [epoch: 7.76 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07408851678907542		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.07408851678907542 | validation: 0.07725097574159083]
	TIME [epoch: 7.76 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07239850660767969		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.07239850660767969 | validation: 0.07899945835351084]
	TIME [epoch: 7.76 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07900698093736758		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.07900698093736758 | validation: 0.08064148713181518]
	TIME [epoch: 7.81 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07258812989960763		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.07258812989960763 | validation: 0.08363887137283194]
	TIME [epoch: 7.77 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07983203732390089		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.07983203732390089 | validation: 0.07830291816774035]
	TIME [epoch: 7.76 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07355980098359899		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.07355980098359899 | validation: 0.07706354734266727]
	TIME [epoch: 7.76 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07214850016524657		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.07214850016524657 | validation: 0.07229938852498886]
	TIME [epoch: 7.76 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06863640920253955		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.06863640920253955 | validation: 0.07904058674358366]
	TIME [epoch: 7.81 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06794054705928652		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.06794054705928652 | validation: 0.08615224266475427]
	TIME [epoch: 7.77 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0748898974621994		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.0748898974621994 | validation: 0.09538815597095379]
	TIME [epoch: 7.76 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07471056908649533		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.07471056908649533 | validation: 0.08008086787758624]
	TIME [epoch: 7.76 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07552774144219396		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.07552774144219396 | validation: 0.08633707409931324]
	TIME [epoch: 7.76 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07452813710254833		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.07452813710254833 | validation: 0.071524473601359]
	TIME [epoch: 7.81 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06653269616051906		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.06653269616051906 | validation: 0.06937702656565319]
	TIME [epoch: 7.76 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0703057575912544		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.0703057575912544 | validation: 0.07422964809218537]
	TIME [epoch: 7.76 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06862310804000862		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.06862310804000862 | validation: 0.072880139212361]
	TIME [epoch: 7.76 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06982430501720732		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.06982430501720732 | validation: 0.06659092899093329]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_567.pth
	Model improved!!!
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06972075955097283		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.06972075955097283 | validation: 0.07956520904870429]
	TIME [epoch: 7.82 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06960611443059611		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.06960611443059611 | validation: 0.08959676627673077]
	TIME [epoch: 7.76 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07256661990004652		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.07256661990004652 | validation: 0.0800255164629621]
	TIME [epoch: 7.75 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07197558621315414		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.07197558621315414 | validation: 0.07656625206105275]
	TIME [epoch: 7.76 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07119460019755136		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.07119460019755136 | validation: 0.07815151150880914]
	TIME [epoch: 7.77 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06989709046565706		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.06989709046565706 | validation: 0.08033960396748759]
	TIME [epoch: 7.8 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07685879425396636		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.07685879425396636 | validation: 0.07474776005666528]
	TIME [epoch: 7.76 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07229969346488563		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.07229969346488563 | validation: 0.07510203736046478]
	TIME [epoch: 7.76 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07016797073494956		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.07016797073494956 | validation: 0.07652190826549495]
	TIME [epoch: 7.76 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0771299307720834		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.0771299307720834 | validation: 0.07580898191431465]
	TIME [epoch: 7.77 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0700322708604365		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.0700322708604365 | validation: 0.08221384116026245]
	TIME [epoch: 7.8 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07324976249870116		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.07324976249870116 | validation: 0.07715000846775474]
	TIME [epoch: 7.77 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0713421815374817		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.0713421815374817 | validation: 0.07230563124047643]
	TIME [epoch: 7.76 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06993442613565956		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.06993442613565956 | validation: 0.07277491872738913]
	TIME [epoch: 7.76 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07563334796414524		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.07563334796414524 | validation: 0.07879985010759354]
	TIME [epoch: 7.77 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07108841785892951		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.07108841785892951 | validation: 0.0760064099281112]
	TIME [epoch: 7.81 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07708055482000069		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.07708055482000069 | validation: 0.06958830847637104]
	TIME [epoch: 7.77 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07038683681863699		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.07038683681863699 | validation: 0.06713635618031487]
	TIME [epoch: 7.77 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06641747931890389		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.06641747931890389 | validation: 0.0726148332587279]
	TIME [epoch: 7.77 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06820002375775322		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.06820002375775322 | validation: 0.07881567691382008]
	TIME [epoch: 7.78 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07137879127377855		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.07137879127377855 | validation: 0.07114909076199707]
	TIME [epoch: 7.8 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06981743931171584		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.06981743931171584 | validation: 0.07260334258154429]
	TIME [epoch: 7.77 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06938151810955649		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.06938151810955649 | validation: 0.06810962697076763]
	TIME [epoch: 7.77 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07085512574014646		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.07085512574014646 | validation: 0.07248060718688695]
	TIME [epoch: 7.77 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07204861781808856		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.07204861781808856 | validation: 0.0837518619388049]
	TIME [epoch: 7.79 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07289864417642268		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.07289864417642268 | validation: 0.07334347072133673]
	TIME [epoch: 7.8 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0709065246366707		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.0709065246366707 | validation: 0.06807760869321161]
	TIME [epoch: 7.77 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06887234134336864		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.06887234134336864 | validation: 0.07010726718218228]
	TIME [epoch: 7.77 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07170541827053553		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.07170541827053553 | validation: 0.07081157067478061]
	TIME [epoch: 7.77 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06770426779653782		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.06770426779653782 | validation: 0.07251416403345237]
	TIME [epoch: 7.8 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06909258035127748		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.06909258035127748 | validation: 0.09244867483871341]
	TIME [epoch: 7.79 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07596698599059881		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.07596698599059881 | validation: 0.08556975726095933]
	TIME [epoch: 7.76 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0764167455147522		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.0764167455147522 | validation: 0.07544205881419802]
	TIME [epoch: 7.76 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07091131977880324		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.07091131977880324 | validation: 0.07681312231849041]
	TIME [epoch: 7.77 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0751539876076602		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.0751539876076602 | validation: 0.07943053469236107]
	TIME [epoch: 7.81 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07092983395530877		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.07092983395530877 | validation: 0.07034570695453468]
	TIME [epoch: 7.79 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.069230639387977		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.069230639387977 | validation: 0.0755712704444495]
	TIME [epoch: 7.76 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07298935612617441		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.07298935612617441 | validation: 0.07341032670780609]
	TIME [epoch: 7.76 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06853900473511114		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.06853900473511114 | validation: 0.07123280423705944]
	TIME [epoch: 7.76 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0740297686490187		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.0740297686490187 | validation: 0.07409908249029354]
	TIME [epoch: 7.79 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07012769329491944		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.07012769329491944 | validation: 0.06912814324170627]
	TIME [epoch: 7.79 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06746223912411946		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.06746223912411946 | validation: 0.07427476690484924]
	TIME [epoch: 7.76 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06869895382970047		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.06869895382970047 | validation: 0.07935449966001872]
	TIME [epoch: 7.77 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07015595359631503		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.07015595359631503 | validation: 0.0793295944672387]
	TIME [epoch: 7.77 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07004648153675543		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.07004648153675543 | validation: 0.0732666414153972]
	TIME [epoch: 7.81 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07780188194956329		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.07780188194956329 | validation: 0.07808317588597381]
	TIME [epoch: 7.78 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06856337193634882		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.06856337193634882 | validation: 0.07414908857151238]
	TIME [epoch: 7.77 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06864683001074103		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.06864683001074103 | validation: 0.0744277997113406]
	TIME [epoch: 7.77 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07028285288013636		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.07028285288013636 | validation: 0.07967122081907044]
	TIME [epoch: 7.76 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0732129950783443		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.0732129950783443 | validation: 0.07027805445443969]
	TIME [epoch: 7.81 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0664909372616125		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.0664909372616125 | validation: 0.07109486897091752]
	TIME [epoch: 7.78 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06962451157428057		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.06962451157428057 | validation: 0.0706197966622631]
	TIME [epoch: 7.77 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07533504059962835		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.07533504059962835 | validation: 0.08042303028816564]
	TIME [epoch: 7.77 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07204919947923148		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.07204919947923148 | validation: 0.07323689251869281]
	TIME [epoch: 7.77 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06606305848621173		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.06606305848621173 | validation: 0.06778696686482882]
	TIME [epoch: 7.82 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.072731962561428		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.072731962561428 | validation: 0.07341056155640721]
	TIME [epoch: 7.77 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06991189309638801		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.06991189309638801 | validation: 0.07297695171495788]
	TIME [epoch: 7.77 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0673652654329245		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.0673652654329245 | validation: 0.067455140144394]
	TIME [epoch: 7.76 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06494846895725793		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.06494846895725793 | validation: 0.08593976331247657]
	TIME [epoch: 7.77 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07241931070822957		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.07241931070822957 | validation: 0.0717023123778054]
	TIME [epoch: 7.82 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06626912221981168		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.06626912221981168 | validation: 0.06869757063337312]
	TIME [epoch: 7.77 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07104922495486177		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.07104922495486177 | validation: 0.06656974380247939]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_629.pth
	Model improved!!!
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06879709509857622		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.06879709509857622 | validation: 0.06661520546353276]
	TIME [epoch: 7.77 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06693025358148055		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.06693025358148055 | validation: 0.06787015727095828]
	TIME [epoch: 7.77 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06882980093763023		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.06882980093763023 | validation: 0.07097277652739992]
	TIME [epoch: 7.82 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06762840774721537		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.06762840774721537 | validation: 0.06767223645023981]
	TIME [epoch: 7.77 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06909192942932686		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.06909192942932686 | validation: 0.06817437316289943]
	TIME [epoch: 7.77 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06785498209491411		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.06785498209491411 | validation: 0.0774743227308674]
	TIME [epoch: 7.77 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06910324651316468		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.06910324651316468 | validation: 0.076044063318467]
	TIME [epoch: 7.78 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07308160760957894		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.07308160760957894 | validation: 0.07042631075989123]
	TIME [epoch: 7.81 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06762049899098194		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.06762049899098194 | validation: 0.06531719260682455]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_638.pth
	Model improved!!!
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06611115842433134		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.06611115842433134 | validation: 0.0653911771258838]
	TIME [epoch: 7.76 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06724796943342859		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.06724796943342859 | validation: 0.08593166088110925]
	TIME [epoch: 7.76 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07222995283019856		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.07222995283019856 | validation: 0.07132265038543287]
	TIME [epoch: 7.78 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0671062737523924		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.0671062737523924 | validation: 0.059025888615182806]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_642.pth
	Model improved!!!
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08259770595812943		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.08259770595812943 | validation: 0.09464230125982231]
	TIME [epoch: 7.77 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08596671410700636		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.08596671410700636 | validation: 0.07907399839345514]
	TIME [epoch: 7.77 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07659849436705578		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.07659849436705578 | validation: 0.06758646862534393]
	TIME [epoch: 7.77 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06958947140644595		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.06958947140644595 | validation: 0.07049733654155241]
	TIME [epoch: 7.79 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06913672830426586		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.06913672830426586 | validation: 0.06553695558457936]
	TIME [epoch: 7.8 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06977598901000995		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.06977598901000995 | validation: 0.06933308634410743]
	TIME [epoch: 7.78 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06737199089579553		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.06737199089579553 | validation: 0.06553749961701699]
	TIME [epoch: 7.77 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06669911333632755		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.06669911333632755 | validation: 0.0675479834230397]
	TIME [epoch: 7.77 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06609136842072176		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.06609136842072176 | validation: 0.07362356468688026]
	TIME [epoch: 7.8 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06642963984132708		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.06642963984132708 | validation: 0.0679930938908009]
	TIME [epoch: 7.81 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06522923833037325		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.06522923833037325 | validation: 0.0681284759452214]
	TIME [epoch: 7.77 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06583892179122502		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.06583892179122502 | validation: 0.06416296713468163]
	TIME [epoch: 7.77 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07005322085692381		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.07005322085692381 | validation: 0.06797849232931964]
	TIME [epoch: 7.77 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06460462135066038		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.06460462135066038 | validation: 0.06253527797383007]
	TIME [epoch: 7.81 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06422012095245962		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.06422012095245962 | validation: 0.062371781366213463]
	TIME [epoch: 7.79 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06546448085315335		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.06546448085315335 | validation: 0.0671869447815466]
	TIME [epoch: 7.77 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06870196946620871		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.06870196946620871 | validation: 0.0814907996849354]
	TIME [epoch: 7.77 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07197736052745163		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.07197736052745163 | validation: 0.07323602137231143]
	TIME [epoch: 7.77 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06605001399366958		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.06605001399366958 | validation: 0.06820483578652599]
	TIME [epoch: 7.8 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06716505582494084		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.06716505582494084 | validation: 0.06844390544990267]
	TIME [epoch: 7.8 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06601482012345725		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.06601482012345725 | validation: 0.06984015920094759]
	TIME [epoch: 7.78 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06661295902326553		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.06661295902326553 | validation: 0.0739051535399043]
	TIME [epoch: 7.76 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06914841223967033		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.06914841223967033 | validation: 0.07481551225235898]
	TIME [epoch: 7.77 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06748421123927452		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.06748421123927452 | validation: 0.05911485271122184]
	TIME [epoch: 7.81 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06458198414072071		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.06458198414072071 | validation: 0.06749218862182974]
	TIME [epoch: 7.78 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0660972296306535		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.0660972296306535 | validation: 0.07021851478776882]
	TIME [epoch: 7.78 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06615259901015394		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.06615259901015394 | validation: 0.07118096801238304]
	TIME [epoch: 7.77 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06720492433514155		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.06720492433514155 | validation: 0.06897326615081587]
	TIME [epoch: 7.77 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0652767556047527		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.0652767556047527 | validation: 0.0838880470390336]
	TIME [epoch: 7.82 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06783135415316217		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.06783135415316217 | validation: 0.07551177907807949]
	TIME [epoch: 7.79 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06728673064381926		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.06728673064381926 | validation: 0.06897725849151876]
	TIME [epoch: 7.76 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06993793141945558		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.06993793141945558 | validation: 0.06794845305582031]
	TIME [epoch: 7.76 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06605381757737218		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.06605381757737218 | validation: 0.06590529840210893]
	TIME [epoch: 7.77 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06454903268650561		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.06454903268650561 | validation: 0.07998840273878813]
	TIME [epoch: 7.81 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06658746435519955		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.06658746435519955 | validation: 0.06733015553864163]
	TIME [epoch: 7.77 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06726670153798821		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.06726670153798821 | validation: 0.06986193989025961]
	TIME [epoch: 7.76 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0661299237496358		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.0661299237496358 | validation: 0.06407183857905902]
	TIME [epoch: 7.77 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06406847478307771		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.06406847478307771 | validation: 0.06313985723521343]
	TIME [epoch: 7.76 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06605217586842684		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.06605217586842684 | validation: 0.06982327653311993]
	TIME [epoch: 7.82 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06434866251497036		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.06434866251497036 | validation: 0.07581236552476406]
	TIME [epoch: 7.76 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06684591758774493		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.06684591758774493 | validation: 0.06571560040091563]
	TIME [epoch: 7.76 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06775962939272728		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.06775962939272728 | validation: 0.059506194736954995]
	TIME [epoch: 7.76 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06314851242056574		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.06314851242056574 | validation: 0.06005396483648142]
	TIME [epoch: 7.76 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06445366084326745		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.06445366084326745 | validation: 0.06254392827942097]
	TIME [epoch: 7.82 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06552812357421296		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.06552812357421296 | validation: 0.06625678306276683]
	TIME [epoch: 7.78 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06515410496195756		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.06515410496195756 | validation: 0.061402865333399545]
	TIME [epoch: 7.76 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06498930620431362		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.06498930620431362 | validation: 0.06740095402097593]
	TIME [epoch: 7.76 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06590603331939171		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.06590603331939171 | validation: 0.0736419705023629]
	TIME [epoch: 7.76 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06780796704985922		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.06780796704985922 | validation: 0.06918755814943145]
	TIME [epoch: 7.81 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0671008813794545		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.0671008813794545 | validation: 0.08001099837281114]
	TIME [epoch: 7.77 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06681302978428		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.06681302978428 | validation: 0.06623056431687842]
	TIME [epoch: 7.76 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06665969030913096		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.06665969030913096 | validation: 0.06454277276504054]
	TIME [epoch: 7.76 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06809865670536148		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.06809865670536148 | validation: 0.07323758366883361]
	TIME [epoch: 7.76 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0875092892215763		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.0875092892215763 | validation: 0.08119119906573213]
	TIME [epoch: 7.81 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07117232609479657		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.07117232609479657 | validation: 0.07085020266789675]
	TIME [epoch: 7.77 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06712434914197526		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.06712434914197526 | validation: 0.07216301568689495]
	TIME [epoch: 7.77 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06915033534656734		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.06915033534656734 | validation: 0.07122169481363581]
	TIME [epoch: 7.76 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06936636746158081		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.06936636746158081 | validation: 0.06999296192951611]
	TIME [epoch: 7.77 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06624408717601973		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.06624408717601973 | validation: 0.06884004663337316]
	TIME [epoch: 7.81 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06877900668772319		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.06877900668772319 | validation: 0.07779777282983977]
	TIME [epoch: 7.76 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06817050656149148		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.06817050656149148 | validation: 0.07433470138800838]
	TIME [epoch: 7.76 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0665251311987439		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.0665251311987439 | validation: 0.06824571255921191]
	TIME [epoch: 7.76 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06853593737918248		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.06853593737918248 | validation: 0.06980337083192524]
	TIME [epoch: 7.77 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0711282849739857		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.0711282849739857 | validation: 0.08922886136782052]
	TIME [epoch: 7.81 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07175648451294997		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.07175648451294997 | validation: 0.07517816325000551]
	TIME [epoch: 7.77 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06882328324707404		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.06882328324707404 | validation: 0.07839436250541182]
	TIME [epoch: 7.76 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07177105943388444		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.07177105943388444 | validation: 0.0781909233643791]
	TIME [epoch: 7.76 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07117731723519313		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.07117731723519313 | validation: 0.08291333210085189]
	TIME [epoch: 7.78 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0707090531948484		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.0707090531948484 | validation: 0.07707655612762951]
	TIME [epoch: 7.8 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06934708171006189		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.06934708171006189 | validation: 0.07824346075780017]
	TIME [epoch: 7.76 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.073292129771791		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.073292129771791 | validation: 0.08883497324575344]
	TIME [epoch: 7.76 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07163118058753917		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.07163118058753917 | validation: 0.08083222074078988]
	TIME [epoch: 7.76 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07094822523755749		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.07094822523755749 | validation: 0.08130851755437964]
	TIME [epoch: 7.78 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08812648736254167		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.08812648736254167 | validation: 0.08009134008542654]
	TIME [epoch: 7.79 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0731892654744743		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.0731892654744743 | validation: 0.07846713270363369]
	TIME [epoch: 7.77 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06754546005011648		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.06754546005011648 | validation: 0.06641866991114725]
	TIME [epoch: 7.76 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06655759002758634		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.06655759002758634 | validation: 0.06441441749813231]
	TIME [epoch: 7.76 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06499544261194014		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.06499544261194014 | validation: 0.06733697209698157]
	TIME [epoch: 7.78 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06344975793183401		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.06344975793183401 | validation: 0.06876441907136482]
	TIME [epoch: 7.8 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06330146764388957		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.06330146764388957 | validation: 0.0686235247003684]
	TIME [epoch: 7.77 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06415993253812635		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.06415993253812635 | validation: 0.06982731417541961]
	TIME [epoch: 7.76 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06657820658334097		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.06657820658334097 | validation: 0.07008055707662314]
	TIME [epoch: 7.76 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06614651919598988		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.06614651919598988 | validation: 0.06820397181995519]
	TIME [epoch: 7.8 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06818895600128079		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.06818895600128079 | validation: 0.06659764256953205]
	TIME [epoch: 7.8 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06738614013448606		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.06738614013448606 | validation: 0.06885905505248133]
	TIME [epoch: 7.76 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0642361165245925		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.0642361165245925 | validation: 0.07192185542379015]
	TIME [epoch: 7.76 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06696406247146583		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.06696406247146583 | validation: 0.07181783086047358]
	TIME [epoch: 7.76 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06746465087657162		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.06746465087657162 | validation: 0.07187261692449853]
	TIME [epoch: 7.8 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07007794621047794		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.07007794621047794 | validation: 0.06212637509574544]
	TIME [epoch: 7.79 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06627486240180236		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.06627486240180236 | validation: 0.06384891610725069]
	TIME [epoch: 7.77 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06565431957913939		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.06565431957913939 | validation: 0.09925749403713738]
	TIME [epoch: 7.76 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07236127830247781		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.07236127830247781 | validation: 0.06674034512698224]
	TIME [epoch: 7.76 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06390799700315583		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.06390799700315583 | validation: 0.05987818344729236]
	TIME [epoch: 7.79 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06170115916019886		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.06170115916019886 | validation: 0.06076406220113449]
	TIME [epoch: 7.79 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06269017691916298		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.06269017691916298 | validation: 0.06183856446912771]
	TIME [epoch: 7.76 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06847089530351055		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.06847089530351055 | validation: 0.08973016202609327]
	TIME [epoch: 7.77 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0865752176939384		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.0865752176939384 | validation: 0.07460874403312309]
	TIME [epoch: 7.76 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06917326720465261		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.06917326720465261 | validation: 0.06218026142936914]
	TIME [epoch: 7.8 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06393688072409204		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.06393688072409204 | validation: 0.06299678403199689]
	TIME [epoch: 7.78 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06470605711817252		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.06470605711817252 | validation: 0.06335771787089223]
	TIME [epoch: 7.76 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06353446578289679		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.06353446578289679 | validation: 0.05887034335222342]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_large_20240627_193159/states/model_phi1_1a_v_mmd1_fix_noise_large_743.pth
	Model improved!!!
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 6010.938 seconds.
