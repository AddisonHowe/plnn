Args:
Namespace(name='model_phi1_2a_v_mmd1', outdir='out/model_training/model_phi1_2a_v_mmd1', training_data='data/training_data/data_phi1_2a/training', validation_data='data/training_data/data_phi1_2a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 4037743501

Training model...

Saving initial model state to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.819159891040683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.819159891040683 | validation: 5.530748202884716]
	TIME [epoch: 104 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 2/2] avg loss: 5.037986027810621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.037986027810621 | validation: 4.594424194357463]
	TIME [epoch: 1.76 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.394645188625524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.394645188625524 | validation: 3.700422443385469]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.512621028738185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.512621028738185 | validation: 3.5320260761658435]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.4342208342124927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4342208342124927 | validation: 3.107421945256533]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.9669696490871544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9669696490871544 | validation: 2.7243076012636447]
	TIME [epoch: 1.69 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.805335450575418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.805335450575418 | validation: 2.810341217377483]
	TIME [epoch: 1.67 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.746276341841962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.746276341841962 | validation: 2.579961799952285]
	TIME [epoch: 1.67 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.722477861990509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.722477861990509 | validation: 2.650765177593246]
	TIME [epoch: 1.66 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.6339388196305418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6339388196305418 | validation: 2.4965223059407884]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.568809321182343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.568809321182343 | validation: 2.5349226538511007]
	TIME [epoch: 1.66 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.5914835206787386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5914835206787386 | validation: 2.5275600206316486]
	TIME [epoch: 1.66 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.563203278244841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.563203278244841 | validation: 2.471704142777396]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.5017548732033035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5017548732033035 | validation: 2.4771843971032155]
	TIME [epoch: 1.66 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.53181972355208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.53181972355208 | validation: 2.477436966277681]
	TIME [epoch: 1.66 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.5060824995668165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5060824995668165 | validation: 2.469937248558556]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.4909239114265853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4909239114265853 | validation: 2.461006036040473]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.5253367522945167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5253367522945167 | validation: 2.451103320358651]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.5209763534336407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5209763534336407 | validation: 2.3904351908571466]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.438163248498773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.438163248498773 | validation: 2.4934046546656123]
	TIME [epoch: 1.66 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.514670501465875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.514670501465875 | validation: 2.3831818587577946]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.478595063822965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.478595063822965 | validation: 2.388861121725213]
	TIME [epoch: 1.67 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.3888899110632194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3888899110632194 | validation: 2.3344521843506967]
	TIME [epoch: 1.67 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.398635254845707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.398635254845707 | validation: 2.325986332484122]
	TIME [epoch: 1.68 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.307266350226482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.307266350226482 | validation: 2.3306723606168873]
	TIME [epoch: 1.68 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.336363975908453		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 2.336363975908453 | validation: 2.304766533982615]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.3588512957439454		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 2.3588512957439454 | validation: 2.1695544848744612]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.199075975207128		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 2.199075975207128 | validation: 2.3411394590352153]
	TIME [epoch: 1.66 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.2245072438680724		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 2.2245072438680724 | validation: 2.542498363383277]
	TIME [epoch: 1.66 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.2782172563352105		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 2.2782172563352105 | validation: 2.113763757195109]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.0359117243869735		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 2.0359117243869735 | validation: 1.9560669149798582]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.8961584320413167		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.8961584320413167 | validation: 1.9168383711000183]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.8956891577189354		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.8956891577189354 | validation: 1.874991698343611]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.7882827691336014		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.7882827691336014 | validation: 1.9607448919147288]
	TIME [epoch: 1.66 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.9084943271047976		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.9084943271047976 | validation: 1.798347343165942]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.709916723771078		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.709916723771078 | validation: 1.783595678101859]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.7436128147195076		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.7436128147195076 | validation: 1.8463379787863177]
	TIME [epoch: 1.66 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.727972467228482		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.727972467228482 | validation: 1.6916307678677525]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.6268958611896935		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.6268958611896935 | validation: 1.6635739485352499]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.6217347324731128		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.6217347324731128 | validation: 1.692508747555915]
	TIME [epoch: 1.66 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.7466742328251486		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.7466742328251486 | validation: 1.6376965445727039]
	TIME [epoch: 1.67 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.6248442669513663		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.6248442669513663 | validation: 1.6502885547060178]
	TIME [epoch: 1.68 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.492598320012962		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.492598320012962 | validation: 1.4294765828223757]
	TIME [epoch: 1.68 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4639584171932924		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.4639584171932924 | validation: 1.3109804955101934]
	TIME [epoch: 1.67 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2959473483986494		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.2959473483986494 | validation: 1.2375267226945201]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2728141256762038		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.2728141256762038 | validation: 1.2423172143144112]
	TIME [epoch: 1.66 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3852841056139438		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.3852841056139438 | validation: 1.4251706798227446]
	TIME [epoch: 1.66 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.381593034769732		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.381593034769732 | validation: 1.1661484405068776]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.205170566693656		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.205170566693656 | validation: 1.1481584838144632]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.180273205014723		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.180273205014723 | validation: 1.1856961470263296]
	TIME [epoch: 1.97 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.266419435731878		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.266419435731878 | validation: 1.5462486141313934]
	TIME [epoch: 1.67 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3866418045575557		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.3866418045575557 | validation: 1.151377875791123]
	TIME [epoch: 1.66 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1860644876210074		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.1860644876210074 | validation: 1.043661000441681]
	TIME [epoch: 1.67 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.120219905647731		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.120219905647731 | validation: 1.0186156383270064]
	TIME [epoch: 1.67 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.089789689921195		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.089789689921195 | validation: 1.1671066330258817]
	TIME [epoch: 1.66 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4212629596377837		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.4212629596377837 | validation: 1.1052203279503108]
	TIME [epoch: 1.66 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.145808279828768		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.145808279828768 | validation: 1.1829569180544066]
	TIME [epoch: 1.66 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.151947809829739		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.151947809829739 | validation: 0.9562758445009742]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.023079996833828		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.023079996833828 | validation: 0.9493980504829637]
	TIME [epoch: 1.69 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0047675460979129		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.0047675460979129 | validation: 1.2339161233355425]
	TIME [epoch: 1.68 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1658257513025292		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.1658257513025292 | validation: 0.9252363885797857]
	TIME [epoch: 1.67 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9741468834772067		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.9741468834772067 | validation: 1.0773679652079895]
	TIME [epoch: 1.66 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2956710186437226		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.2956710186437226 | validation: 0.9457874393426442]
	TIME [epoch: 1.66 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0396213045000717		[learning rate: 0.0076127]
	Learning Rate: 0.00761269
	LOSS [training: 1.0396213045000717 | validation: 0.9472718343490961]
	TIME [epoch: 1.66 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0482260821314022		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.0482260821314022 | validation: 0.9197709761586034]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0116338814801646		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.0116338814801646 | validation: 1.0402161913001087]
	TIME [epoch: 1.66 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0930729504142882		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.0930729504142882 | validation: 0.8897813904714479]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9184148550361391		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.9184148550361391 | validation: 0.861921801551336]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9062671437545698		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.9062671437545698 | validation: 0.7465592806606994]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8756516113832438		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.8756516113832438 | validation: 1.015422883159092]
	TIME [epoch: 1.66 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0965626532982142		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.0965626532982142 | validation: 0.7129617381225428]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8012103505560766		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.8012103505560766 | validation: 0.6729075151929642]
	TIME [epoch: 1.67 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7789395337097613		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.7789395337097613 | validation: 0.6861659598939811]
	TIME [epoch: 1.66 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8509749252486418		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.8509749252486418 | validation: 1.202983106128634]
	TIME [epoch: 1.66 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9851082939634586		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.9851082939634586 | validation: 0.7691334838229044]
	TIME [epoch: 1.67 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9221684300448072		[learning rate: 0.0069922]
	Learning Rate: 0.00699222
	LOSS [training: 0.9221684300448072 | validation: 0.8012707899333841]
	TIME [epoch: 1.67 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8197127785385628		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.8197127785385628 | validation: 0.6731224311793296]
	TIME [epoch: 1.68 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8927610195634208		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.8927610195634208 | validation: 0.7022604366340708]
	TIME [epoch: 1.68 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7989463802184321		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.7989463802184321 | validation: 0.616014205909299]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6795449672342722		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.6795449672342722 | validation: 0.6324398656145072]
	TIME [epoch: 1.66 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.810754347494101		[learning rate: 0.0067489]
	Learning Rate: 0.00674887
	LOSS [training: 0.810754347494101 | validation: 0.8763206084309619]
	TIME [epoch: 1.66 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7996227207628362		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.7996227207628362 | validation: 0.5906588015062805]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6626581230318433		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.6626581230318433 | validation: 0.5031719767620405]
	TIME [epoch: 1.7 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6230770260996747		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.6230770260996747 | validation: 0.5574045729689797]
	TIME [epoch: 1.66 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6925616086762443		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.6925616086762443 | validation: 0.9346933452403321]
	TIME [epoch: 1.66 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8288224990748388		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.8288224990748388 | validation: 0.5112704698458995]
	TIME [epoch: 1.66 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.578392675356336		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.578392675356336 | validation: 0.47023532819795]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5379298492041307		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.5379298492041307 | validation: 0.7200481558930856]
	TIME [epoch: 1.66 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7299435055070234		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.7299435055070234 | validation: 0.709888661837968]
	TIME [epoch: 1.66 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6448223804217761		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.6448223804217761 | validation: 0.5649846810028643]
	TIME [epoch: 1.66 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7185409688441463		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.7185409688441463 | validation: 0.8338401645434713]
	TIME [epoch: 1.66 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7819559258792499		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.7819559258792499 | validation: 0.5359492006313277]
	TIME [epoch: 1.66 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5149839187844897		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.5149839187844897 | validation: 0.4806036645121461]
	TIME [epoch: 1.66 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7575422135987543		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.7575422135987543 | validation: 0.4751415881026814]
	TIME [epoch: 1.66 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5735198233434031		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.5735198233434031 | validation: 0.6044626016816728]
	TIME [epoch: 1.67 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5398426024328652		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.5398426024328652 | validation: 0.4406995623508886]
	TIME [epoch: 1.67 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.548385431907409		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.548385431907409 | validation: 0.44820335813880235]
	TIME [epoch: 1.68 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.43820173829614684		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.43820173829614684 | validation: 0.5191543555684197]
	TIME [epoch: 1.66 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6155467528404324		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.6155467528404324 | validation: 0.5768454631661166]
	TIME [epoch: 1.66 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5221349964640991		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.5221349964640991 | validation: 0.3667425915886156]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.40472790844326606		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.40472790844326606 | validation: 0.44059007406367456]
	TIME [epoch: 1.66 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.48687786202285843		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.48687786202285843 | validation: 0.6024269047405859]
	TIME [epoch: 1.66 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6013999315313483		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.6013999315313483 | validation: 0.5488622091562669]
	TIME [epoch: 1.66 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4887659588562845		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.4887659588562845 | validation: 0.43179299857444225]
	TIME [epoch: 1.66 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4854325099727868		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.4854325099727868 | validation: 0.5153061487622498]
	TIME [epoch: 1.66 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6279594709166374		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.6279594709166374 | validation: 0.5104376701091108]
	TIME [epoch: 1.66 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.45956580382457246		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.45956580382457246 | validation: 0.3593208859531912]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4810352853551708		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.4810352853551708 | validation: 0.6030700221168254]
	TIME [epoch: 1.67 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5165717979173508		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.5165717979173508 | validation: 0.47306870946738394]
	TIME [epoch: 1.67 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4456169259964039		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.4456169259964039 | validation: 0.34590121404713875]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4451086182533061		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.4451086182533061 | validation: 0.4267702692772565]
	TIME [epoch: 1.67 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.46752905610217677		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.46752905610217677 | validation: 0.38837585413509246]
	TIME [epoch: 1.66 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.477145961817787		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.477145961817787 | validation: 0.40645833620656147]
	TIME [epoch: 1.66 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3987216591951771		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.3987216591951771 | validation: 0.5055658303126751]
	TIME [epoch: 1.66 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4776927466721334		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.4776927466721334 | validation: 0.38389205203881804]
	TIME [epoch: 1.68 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.45751575380266024		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.45751575380266024 | validation: 0.7218805532849766]
	TIME [epoch: 1.68 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5563761517479245		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.5563761517479245 | validation: 0.340319379927132]
	TIME [epoch: 1.67 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.39529708435661054		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.39529708435661054 | validation: 0.4760896071044082]
	TIME [epoch: 1.66 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.523830414991292		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.523830414991292 | validation: 0.3502101698213316]
	TIME [epoch: 1.66 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.44448267081839954		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.44448267081839954 | validation: 0.6335582650810965]
	TIME [epoch: 1.66 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5331459627931134		[learning rate: 0.0050834]
	Learning Rate: 0.00508339
	LOSS [training: 0.5331459627931134 | validation: 0.3199793997019109]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3656085651528177		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.3656085651528177 | validation: 0.3845793373320072]
	TIME [epoch: 1.66 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.46646689378752737		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.46646689378752737 | validation: 0.3267037736760265]
	TIME [epoch: 1.65 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.36168046555544764		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.36168046555544764 | validation: 0.32089781469794193]
	TIME [epoch: 1.65 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.41733816820467395		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.41733816820467395 | validation: 0.3163899116180904]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.36713451822337606		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.36713451822337606 | validation: 0.37680758499057565]
	TIME [epoch: 1.66 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.38552719162916255		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.38552719162916255 | validation: 0.3473425362140816]
	TIME [epoch: 1.65 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.451200141217564		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.451200141217564 | validation: 0.3368902026973949]
	TIME [epoch: 1.65 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.36011655195236136		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.36011655195236136 | validation: 0.321870493072052]
	TIME [epoch: 1.65 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3783798249543636		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.3783798249543636 | validation: 0.4421691552592643]
	TIME [epoch: 1.66 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.43977560080242584		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.43977560080242584 | validation: 0.5053666580753543]
	TIME [epoch: 1.65 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4286981867116968		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.4286981867116968 | validation: 0.3166282403730019]
	TIME [epoch: 1.65 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3233206589317861		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.3233206589317861 | validation: 0.33590879708173377]
	TIME [epoch: 1.65 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.32814599088561724		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.32814599088561724 | validation: 0.33981124094778986]
	TIME [epoch: 1.66 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3597638601432384		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.3597638601432384 | validation: 0.5341502835170351]
	TIME [epoch: 1.67 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.427049988068941		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.427049988068941 | validation: 0.30597573853763294]
	TIME [epoch: 1.67 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.41079388057936184		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.41079388057936184 | validation: 0.33181044613752]
	TIME [epoch: 1.66 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.33651567561057555		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.33651567561057555 | validation: 0.32085819059678666]
	TIME [epoch: 1.66 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.29888287495783783		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.29888287495783783 | validation: 0.33791322464090606]
	TIME [epoch: 1.66 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3276679610791724		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.3276679610791724 | validation: 0.2960281607670993]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2870653087770156		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.2870653087770156 | validation: 0.25247708700557203]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_141.pth
	Model improved!!!
EPOCH 142/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.28484223335723147		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.28484223335723147 | validation: 0.5740985256610369]
	TIME [epoch: 1.66 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.45377001644127035		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.45377001644127035 | validation: 0.28752515056000094]
	TIME [epoch: 1.66 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.29816373982348543		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.29816373982348543 | validation: 0.5595783416894923]
	TIME [epoch: 1.66 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5481601802019211		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.5481601802019211 | validation: 0.31378326576299576]
	TIME [epoch: 1.66 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.32003451491896506		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.32003451491896506 | validation: 0.36597548503452293]
	TIME [epoch: 1.66 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3043788792430025		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.3043788792430025 | validation: 0.23226207318676018]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24889792180944764		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.24889792180944764 | validation: 0.27255411425485043]
	TIME [epoch: 1.66 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2943555228119338		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.2943555228119338 | validation: 0.4670514342330062]
	TIME [epoch: 1.66 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.384726510511327		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.384726510511327 | validation: 0.25542370414124105]
	TIME [epoch: 1.66 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.28475460726068214		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.28475460726068214 | validation: 0.32509861637280335]
	TIME [epoch: 1.66 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.31041778438769274		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.31041778438769274 | validation: 0.45407884389329045]
	TIME [epoch: 1.66 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3737888022177848		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.3737888022177848 | validation: 0.2659755398395149]
	TIME [epoch: 1.66 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2775035161871954		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.2775035161871954 | validation: 0.3605735853156704]
	TIME [epoch: 1.67 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3493594585439912		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.3493594585439912 | validation: 0.31857006692857737]
	TIME [epoch: 1.67 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2875344171197258		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.2875344171197258 | validation: 0.25129880796611787]
	TIME [epoch: 1.66 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.27604646957716317		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.27604646957716317 | validation: 0.3138941288341972]
	TIME [epoch: 1.66 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2606598418428533		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.2606598418428533 | validation: 0.2094822894495632]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_158.pth
	Model improved!!!
EPOCH 159/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22923883858517868		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.22923883858517868 | validation: 0.21988864871798364]
	TIME [epoch: 1.66 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23645904738789658		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.23645904738789658 | validation: 0.32010840829064463]
	TIME [epoch: 1.66 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3276733981767239		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.3276733981767239 | validation: 0.31659000386793723]
	TIME [epoch: 1.67 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.28911191811321413		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.28911191811321413 | validation: 0.2208083763759087]
	TIME [epoch: 1.66 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21164642603626835		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.21164642603626835 | validation: 0.18324660938985257]
	TIME [epoch: 1.67 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_163.pth
	Model improved!!!
EPOCH 164/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23054424370275745		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.23054424370275745 | validation: 0.2586675471085562]
	TIME [epoch: 1.67 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.296197789205752		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.296197789205752 | validation: 0.2077667903502758]
	TIME [epoch: 1.66 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21651752875708055		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.21651752875708055 | validation: 0.20720650036783203]
	TIME [epoch: 1.67 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24720177777601499		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.24720177777601499 | validation: 0.518842256703996]
	TIME [epoch: 1.66 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4174854483476603		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.4174854483476603 | validation: 0.29890145360555864]
	TIME [epoch: 1.66 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.30193346439654123		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.30193346439654123 | validation: 0.22701729279588242]
	TIME [epoch: 1.67 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2684516014318409		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.2684516014318409 | validation: 0.3415564092423981]
	TIME [epoch: 1.66 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3162827894077099		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.3162827894077099 | validation: 0.21247840626658118]
	TIME [epoch: 1.66 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23681547459568622		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.23681547459568622 | validation: 0.19214935054923032]
	TIME [epoch: 1.66 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21543110799081938		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.21543110799081938 | validation: 0.187432340154195]
	TIME [epoch: 1.68 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19193813373460683		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.19193813373460683 | validation: 0.3904969891535116]
	TIME [epoch: 1.68 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2740411174191144		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.2740411174191144 | validation: 0.23133002036746655]
	TIME [epoch: 1.68 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2223986030758358		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.2223986030758358 | validation: 0.25111537376059945]
	TIME [epoch: 1.67 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22692486127415123		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.22692486127415123 | validation: 0.19920636237152756]
	TIME [epoch: 1.66 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19432674973117905		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.19432674973117905 | validation: 0.21163595915706726]
	TIME [epoch: 1.66 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2089503465442787		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.2089503465442787 | validation: 0.22322242425165398]
	TIME [epoch: 1.66 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1884442433635485		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.1884442433635485 | validation: 0.17610428028524353]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_180.pth
	Model improved!!!
EPOCH 181/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1972817439338267		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.1972817439338267 | validation: 0.6765346109585525]
	TIME [epoch: 1.66 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7538947715409519		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.7538947715409519 | validation: 0.3450025324354504]
	TIME [epoch: 1.66 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3403332966350119		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.3403332966350119 | validation: 0.2258017617508477]
	TIME [epoch: 1.66 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23306068766865878		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.23306068766865878 | validation: 0.19037137779438607]
	TIME [epoch: 1.66 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19276229354212274		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.19276229354212274 | validation: 0.1601797209964707]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1674053329483513		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.1674053329483513 | validation: 0.14614595214251144]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_186.pth
	Model improved!!!
EPOCH 187/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15980629999761653		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.15980629999761653 | validation: 0.16197899323900156]
	TIME [epoch: 1.66 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18662233347221607		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.18662233347221607 | validation: 0.3748294409357652]
	TIME [epoch: 1.66 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2652035141930643		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.2652035141930643 | validation: 0.17671652294039092]
	TIME [epoch: 1.66 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19299323492500645		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.19299323492500645 | validation: 0.17364921976223144]
	TIME [epoch: 1.66 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19818960339985597		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.19818960339985597 | validation: 0.18035502708021112]
	TIME [epoch: 1.66 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17263840355713628		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.17263840355713628 | validation: 0.20129785246865303]
	TIME [epoch: 1.66 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18323322386103694		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.18323322386103694 | validation: 0.1679654793387048]
	TIME [epoch: 1.67 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15149888191229646		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.15149888191229646 | validation: 0.155151431158346]
	TIME [epoch: 1.68 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1885426012917079		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.1885426012917079 | validation: 0.17391746351165127]
	TIME [epoch: 1.68 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5300670374302563		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.5300670374302563 | validation: 0.5528875896209536]
	TIME [epoch: 1.66 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5515740476625873		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.5515740476625873 | validation: 0.2112180735797277]
	TIME [epoch: 1.66 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24724351170422176		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.24724351170422176 | validation: 0.2450470745432977]
	TIME [epoch: 1.66 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19698751027050293		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.19698751027050293 | validation: 0.17530924551290206]
	TIME [epoch: 1.66 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16896640996264078		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.16896640996264078 | validation: 0.19946258563218303]
	TIME [epoch: 1.65 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16921338853394904		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.16921338853394904 | validation: 0.16119853551227192]
	TIME [epoch: 103 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14535848044979738		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.14535848044979738 | validation: 0.14581347432303643]
	TIME [epoch: 3.28 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14958760017705042		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.14958760017705042 | validation: 0.19685985442034998]
	TIME [epoch: 3.24 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19212932132248317		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.19212932132248317 | validation: 0.16366883632085782]
	TIME [epoch: 3.25 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15737799734423527		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.15737799734423527 | validation: 0.2909067355076432]
	TIME [epoch: 3.24 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.27876462368402016		[learning rate: 0.0027837]
	Learning Rate: 0.00278366
	LOSS [training: 0.27876462368402016 | validation: 0.20170634652205602]
	TIME [epoch: 3.24 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17579775465822656		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.17579775465822656 | validation: 0.14007684403192888]
	TIME [epoch: 3.24 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_207.pth
	Model improved!!!
EPOCH 208/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14313223408611267		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.14313223408611267 | validation: 0.12793718961582953]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13425363343052354		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.13425363343052354 | validation: 0.1528797864430731]
	TIME [epoch: 3.26 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15489367178471136		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.15489367178471136 | validation: 0.1363555782173769]
	TIME [epoch: 3.26 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14196142643988635		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.14196142643988635 | validation: 0.1428249334485612]
	TIME [epoch: 3.26 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16034846233257982		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.16034846233257982 | validation: 0.18170448945822038]
	TIME [epoch: 3.24 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1797462910589644		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.1797462910589644 | validation: 0.24080062646731634]
	TIME [epoch: 3.25 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19868334503380816		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.19868334503380816 | validation: 0.141794360887686]
	TIME [epoch: 3.25 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1355568717147348		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.1355568717147348 | validation: 0.11563811484028085]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_215.pth
	Model improved!!!
EPOCH 216/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13742469555668407		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.13742469555668407 | validation: 0.16881100721186934]
	TIME [epoch: 3.25 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15859288863325965		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.15859288863325965 | validation: 0.11251956453288217]
	TIME [epoch: 3.24 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_217.pth
	Model improved!!!
EPOCH 218/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11964560663255587		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.11964560663255587 | validation: 0.13785998191940477]
	TIME [epoch: 3.24 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14903308354727007		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.14903308354727007 | validation: 0.14881724258959825]
	TIME [epoch: 3.25 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16566234289257487		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.16566234289257487 | validation: 0.14476999797888151]
	TIME [epoch: 3.26 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15337765809331239		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.15337765809331239 | validation: 0.12565291136219375]
	TIME [epoch: 3.26 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12075281788552375		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.12075281788552375 | validation: 0.124450311248625]
	TIME [epoch: 3.25 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12752117988189907		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.12752117988189907 | validation: 0.12572439595731053]
	TIME [epoch: 3.24 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14184844603412783		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.14184844603412783 | validation: 0.16428724355140534]
	TIME [epoch: 3.24 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17207907289127178		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.17207907289127178 | validation: 0.13187807422107467]
	TIME [epoch: 3.24 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13304352205004716		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.13304352205004716 | validation: 0.12866141180854077]
	TIME [epoch: 3.24 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1906825752819583		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.1906825752819583 | validation: 0.13685581006053024]
	TIME [epoch: 3.24 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12068682458574971		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.12068682458574971 | validation: 0.12124996618423785]
	TIME [epoch: 3.24 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1213158643091776		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.1213158643091776 | validation: 0.11285172700433317]
	TIME [epoch: 3.25 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12186449538856964		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.12186449538856964 | validation: 0.1516075966453997]
	TIME [epoch: 3.25 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13502972485199372		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.13502972485199372 | validation: 0.12065619844231042]
	TIME [epoch: 3.27 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11522823434588156		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.11522823434588156 | validation: 0.15564251799080964]
	TIME [epoch: 3.26 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12318267292123655		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.12318267292123655 | validation: 0.1098977806685833]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_233.pth
	Model improved!!!
EPOCH 234/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1567621899964304		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.1567621899964304 | validation: 0.13894015622483763]
	TIME [epoch: 3.25 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1423143789518047		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.1423143789518047 | validation: 0.11246890002713535]
	TIME [epoch: 3.25 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11828169038465536		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.11828169038465536 | validation: 0.21408439880306618]
	TIME [epoch: 3.25 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1582569626232377		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.1582569626232377 | validation: 0.12729070949041893]
	TIME [epoch: 3.24 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10754233254770601		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.10754233254770601 | validation: 0.12360866693558342]
	TIME [epoch: 3.25 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1376844976533267		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.1376844976533267 | validation: 0.1557646113465191]
	TIME [epoch: 3.25 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1661533419541779		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.1661533419541779 | validation: 0.11942570686761417]
	TIME [epoch: 3.25 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11064266209335152		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.11064266209335152 | validation: 0.1098963339608231]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_241.pth
	Model improved!!!
EPOCH 242/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10566232251772562		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.10566232251772562 | validation: 0.107515341013008]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09993222341383368		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.09993222341383368 | validation: 0.11620239215229718]
	TIME [epoch: 3.26 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11096104503670659		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.11096104503670659 | validation: 0.11206844756611932]
	TIME [epoch: 3.24 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10662898432038434		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.10662898432038434 | validation: 0.1357199139400908]
	TIME [epoch: 3.25 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12476240194952122		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.12476240194952122 | validation: 0.11565571420744787]
	TIME [epoch: 3.24 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11511324545480284		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.11511324545480284 | validation: 0.09202547809477468]
	TIME [epoch: 3.24 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0903006809394444		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.0903006809394444 | validation: 0.10040345509991311]
	TIME [epoch: 3.24 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09858368580682714		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.09858368580682714 | validation: 0.11142279545675166]
	TIME [epoch: 3.25 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1023953033783691		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.1023953033783691 | validation: 0.10337834602198384]
	TIME [epoch: 3.25 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.136082859155448		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.136082859155448 | validation: 0.20360649335836684]
	TIME [epoch: 3.25 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13334732238937255		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.13334732238937255 | validation: 0.1413859353430197]
	TIME [epoch: 3.25 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13781676052763084		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.13781676052763084 | validation: 0.08713130299137457]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_253.pth
	Model improved!!!
EPOCH 254/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09471970908839825		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.09471970908839825 | validation: 0.1305687049965303]
	TIME [epoch: 3.25 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10737795143773944		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.10737795143773944 | validation: 0.09594767081639638]
	TIME [epoch: 3.24 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12993761973837586		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.12993761973837586 | validation: 0.11443079930659766]
	TIME [epoch: 3.25 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10674290315609869		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.10674290315609869 | validation: 0.09181875478112977]
	TIME [epoch: 3.25 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09070154736474093		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.09070154736474093 | validation: 0.16288953117709837]
	TIME [epoch: 3.24 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11968503292942329		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.11968503292942329 | validation: 0.12842686733917597]
	TIME [epoch: 3.24 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12661742092535244		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.12661742092535244 | validation: 0.09323347101948559]
	TIME [epoch: 3.24 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08933189930232902		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.08933189930232902 | validation: 0.09934419168928457]
	TIME [epoch: 3.24 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08826382240315038		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.08826382240315038 | validation: 0.08736906161651266]
	TIME [epoch: 3.24 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09325157984288587		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.09325157984288587 | validation: 0.10655868766385082]
	TIME [epoch: 3.27 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09981447645409884		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.09981447645409884 | validation: 0.08963411790461229]
	TIME [epoch: 3.26 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09211777290366419		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.09211777290366419 | validation: 0.13544463212216137]
	TIME [epoch: 3.25 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12320288579269711		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.12320288579269711 | validation: 0.09502017008940052]
	TIME [epoch: 3.23 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08981748466559586		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.08981748466559586 | validation: 0.15132001631509073]
	TIME [epoch: 3.24 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13046686008077502		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.13046686008077502 | validation: 0.08123257689727992]
	TIME [epoch: 3.24 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_268.pth
	Model improved!!!
EPOCH 269/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0832028264110833		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.0832028264110833 | validation: 0.1001360355701845]
	TIME [epoch: 3.24 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09634154554445193		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.09634154554445193 | validation: 0.0917486740700037]
	TIME [epoch: 3.23 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08835633402476481		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.08835633402476481 | validation: 0.08023621435270636]
	TIME [epoch: 3.24 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_271.pth
	Model improved!!!
EPOCH 272/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07904542861335542		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.07904542861335542 | validation: 0.08222241785368965]
	TIME [epoch: 3.24 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0829321323811345		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.0829321323811345 | validation: 0.08859310170213537]
	TIME [epoch: 3.24 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08746938075627735		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.08746938075627735 | validation: 0.08959610492806269]
	TIME [epoch: 3.26 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09127988285843525		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.09127988285843525 | validation: 0.10937163052539296]
	TIME [epoch: 3.25 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09505529306666512		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.09505529306666512 | validation: 0.17618116477543894]
	TIME [epoch: 3.24 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1285595241191712		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.1285595241191712 | validation: 0.0781746283726001]
	TIME [epoch: 3.23 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07720350210476956		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.07720350210476956 | validation: 0.08009043360943825]
	TIME [epoch: 3.24 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07762498324185127		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.07762498324185127 | validation: 0.08803757427605437]
	TIME [epoch: 3.23 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08692544213699105		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.08692544213699105 | validation: 0.11504351181197699]
	TIME [epoch: 3.23 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10090323865891754		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.10090323865891754 | validation: 0.0783038989673835]
	TIME [epoch: 3.24 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08764282132541579		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.08764282132541579 | validation: 0.1265963961294308]
	TIME [epoch: 3.23 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09352310832410222		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.09352310832410222 | validation: 0.1164717560998092]
	TIME [epoch: 3.24 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0981219699539162		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.0981219699539162 | validation: 0.08233164547008291]
	TIME [epoch: 3.26 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07498444096488863		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.07498444096488863 | validation: 0.07772637388574632]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_285.pth
	Model improved!!!
EPOCH 286/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08014558826496562		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.08014558826496562 | validation: 0.10010195411695673]
	TIME [epoch: 3.26 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0929230806707792		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.0929230806707792 | validation: 0.08466893466377995]
	TIME [epoch: 3.24 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07564447103154151		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.07564447103154151 | validation: 0.08455497491384541]
	TIME [epoch: 3.24 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08049573727632718		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.08049573727632718 | validation: 0.08505124339145663]
	TIME [epoch: 3.24 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0736685729002107		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.0736685729002107 | validation: 0.06510098007006827]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_290.pth
	Model improved!!!
EPOCH 291/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07143320458087722		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.07143320458087722 | validation: 0.07628020649274095]
	TIME [epoch: 3.25 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08193948770502615		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.08193948770502615 | validation: 0.09881119043204022]
	TIME [epoch: 3.23 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0905657011472458		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.0905657011472458 | validation: 0.07526712507568294]
	TIME [epoch: 3.24 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07003559844807938		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.07003559844807938 | validation: 0.08271247286666715]
	TIME [epoch: 3.25 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08085616970763376		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.08085616970763376 | validation: 0.07698455350159834]
	TIME [epoch: 3.26 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07374086266286031		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.07374086266286031 | validation: 0.06855747421326575]
	TIME [epoch: 3.25 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07004137759504867		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.07004137759504867 | validation: 0.07558482838595204]
	TIME [epoch: 3.25 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.073454010094999		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.073454010094999 | validation: 0.1374024451654658]
	TIME [epoch: 3.25 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1002579810563723		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.1002579810563723 | validation: 0.07011743983010676]
	TIME [epoch: 3.25 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06844426766625789		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.06844426766625789 | validation: 0.07667931268901373]
	TIME [epoch: 3.25 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08265047139192183		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.08265047139192183 | validation: 0.07694056651102982]
	TIME [epoch: 3.25 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0665865617614405		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.0665865617614405 | validation: 0.06805908461092673]
	TIME [epoch: 3.25 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07466155252133036		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.07466155252133036 | validation: 0.09478996866669956]
	TIME [epoch: 3.25 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08261212775623494		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.08261212775623494 | validation: 0.07582726699531543]
	TIME [epoch: 3.25 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06332693844318762		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.06332693844318762 | validation: 0.06514797356609552]
	TIME [epoch: 3.24 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07047931973551376		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.07047931973551376 | validation: 0.1526766936619665]
	TIME [epoch: 3.26 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10285670764991603		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.10285670764991603 | validation: 0.09532454747227603]
	TIME [epoch: 3.27 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08511826080671062		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.08511826080671062 | validation: 0.06085512664432542]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_308.pth
	Model improved!!!
EPOCH 309/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06833381879449271		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.06833381879449271 | validation: 0.06795773161784684]
	TIME [epoch: 3.25 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06685733034279942		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.06685733034279942 | validation: 0.06839772527094107]
	TIME [epoch: 3.25 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06070268399699731		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.06070268399699731 | validation: 0.07232219952794025]
	TIME [epoch: 3.25 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06904069510592767		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.06904069510592767 | validation: 0.07355564271385856]
	TIME [epoch: 3.25 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07031632932871697		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.07031632932871697 | validation: 0.07518435186494049]
	TIME [epoch: 3.25 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0646501461862859		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.0646501461862859 | validation: 0.06551772671030204]
	TIME [epoch: 3.25 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06170087978338476		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.06170087978338476 | validation: 0.07744730226145458]
	TIME [epoch: 3.25 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07380922010726984		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.07380922010726984 | validation: 0.084522549214057]
	TIME [epoch: 3.25 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07225556007054817		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.07225556007054817 | validation: 0.07268670965718758]
	TIME [epoch: 3.26 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.070470138621936		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.070470138621936 | validation: 0.06596563955500637]
	TIME [epoch: 3.27 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05692043321375155		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.05692043321375155 | validation: 0.06396742118581333]
	TIME [epoch: 3.27 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05894428941708253		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.05894428941708253 | validation: 0.07284637737119538]
	TIME [epoch: 3.25 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06929305313469356		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.06929305313469356 | validation: 0.08210061988675406]
	TIME [epoch: 3.25 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07601000052458212		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.07601000052458212 | validation: 0.06275613020388136]
	TIME [epoch: 3.25 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06076692268387816		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.06076692268387816 | validation: 0.0655000779547568]
	TIME [epoch: 3.25 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06037429033006532		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.06037429033006532 | validation: 0.06613151795314702]
	TIME [epoch: 3.25 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.060508800582878566		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.060508800582878566 | validation: 0.06460085174878859]
	TIME [epoch: 3.24 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.059926928525172576		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.059926928525172576 | validation: 0.07066918862215224]
	TIME [epoch: 3.25 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06075621390288653		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.06075621390288653 | validation: 0.064746433450479]
	TIME [epoch: 3.25 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06316571717401488		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.06316571717401488 | validation: 0.0660484325148701]
	TIME [epoch: 3.26 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05509686907144638		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.05509686907144638 | validation: 0.05525050826855036]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_329.pth
	Model improved!!!
EPOCH 330/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0547735833625758		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.0547735833625758 | validation: 0.10802318963136398]
	TIME [epoch: 3.27 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06921157030141295		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.06921157030141295 | validation: 0.07398530476167592]
	TIME [epoch: 3.24 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06729736129967537		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.06729736129967537 | validation: 0.07064146875637567]
	TIME [epoch: 3.25 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06655554061574101		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.06655554061574101 | validation: 0.06524530918134717]
	TIME [epoch: 3.25 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06612000022297718		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.06612000022297718 | validation: 0.07074824476923341]
	TIME [epoch: 3.25 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06320984449439981		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.06320984449439981 | validation: 0.06550352117834739]
	TIME [epoch: 3.25 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06070400343789652		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.06070400343789652 | validation: 0.05755043458285696]
	TIME [epoch: 3.25 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05407213919127178		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.05407213919127178 | validation: 0.056598372530161756]
	TIME [epoch: 3.25 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05143734562091644		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.05143734562091644 | validation: 0.057589009359068535]
	TIME [epoch: 3.25 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05556883221201225		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.05556883221201225 | validation: 0.08605853534793724]
	TIME [epoch: 3.26 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06553518746365788		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.06553518746365788 | validation: 0.0709225500013848]
	TIME [epoch: 3.28 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06650000435717877		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.06650000435717877 | validation: 0.054840318559080385]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_341.pth
	Model improved!!!
EPOCH 342/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051299023104164836		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.051299023104164836 | validation: 0.05451242049238162]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_342.pth
	Model improved!!!
EPOCH 343/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054212566618584354		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.054212566618584354 | validation: 0.058977250075333214]
	TIME [epoch: 3.25 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05651992788142335		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.05651992788142335 | validation: 0.059472089406966494]
	TIME [epoch: 3.25 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051909871181870036		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.051909871181870036 | validation: 0.06643972432957218]
	TIME [epoch: 3.25 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05420623003841078		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.05420623003841078 | validation: 0.05321093611589946]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_346.pth
	Model improved!!!
EPOCH 347/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06166518452566013		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.06166518452566013 | validation: 0.06575495176206507]
	TIME [epoch: 3.24 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05602550018485557		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.05602550018485557 | validation: 0.052789892485569624]
	TIME [epoch: 3.24 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_348.pth
	Model improved!!!
EPOCH 349/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048261525290611895		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.048261525290611895 | validation: 0.05215989673521912]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_349.pth
	Model improved!!!
EPOCH 350/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050826842971440106		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.050826842971440106 | validation: 0.05509298108515701]
	TIME [epoch: 3.27 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0567928858241996		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.0567928858241996 | validation: 0.11027558705793086]
	TIME [epoch: 3.25 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06870005846320462		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.06870005846320462 | validation: 0.0630723934076111]
	TIME [epoch: 3.24 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.055798457805834076		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.055798457805834076 | validation: 0.057337789067986644]
	TIME [epoch: 3.24 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05327414538744039		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.05327414538744039 | validation: 0.05461090941616771]
	TIME [epoch: 3.24 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051672237837095		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.051672237837095 | validation: 0.05092612914842958]
	TIME [epoch: 3.24 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05025439911269399		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.05025439911269399 | validation: 0.055371708944511266]
	TIME [epoch: 3.24 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04845296354613976		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.04845296354613976 | validation: 0.059747789658138366]
	TIME [epoch: 3.24 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050150861578908676		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.050150861578908676 | validation: 0.05769222743489182]
	TIME [epoch: 3.24 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04802570701234243		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.04802570701234243 | validation: 0.05735102958186536]
	TIME [epoch: 3.25 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05147286528665239		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.05147286528665239 | validation: 0.048855179298288726]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_360.pth
	Model improved!!!
EPOCH 361/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04952388643535449		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.04952388643535449 | validation: 0.05636513657500031]
	TIME [epoch: 3.26 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04700941313681167		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.04700941313681167 | validation: 0.05369544002704592]
	TIME [epoch: 3.25 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048048956810960546		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.048048956810960546 | validation: 0.06277929552163801]
	TIME [epoch: 3.24 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05348945389496444		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.05348945389496444 | validation: 0.054432825802877075]
	TIME [epoch: 3.24 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04985546194827939		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.04985546194827939 | validation: 0.05772721220336419]
	TIME [epoch: 3.24 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05266822766573838		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.05266822766573838 | validation: 0.05352358783002667]
	TIME [epoch: 3.24 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05170351392321275		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.05170351392321275 | validation: 0.04586533807036583]
	TIME [epoch: 3.24 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_367.pth
	Model improved!!!
EPOCH 368/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047751773865739686		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.047751773865739686 | validation: 0.05213364466064219]
	TIME [epoch: 3.25 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04602292380329183		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.04602292380329183 | validation: 0.052760521263814066]
	TIME [epoch: 3.25 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04498201781272103		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.04498201781272103 | validation: 0.056131693989028286]
	TIME [epoch: 3.26 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05497270233017448		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.05497270233017448 | validation: 0.057671786557924876]
	TIME [epoch: 3.27 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050267178294699566		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.050267178294699566 | validation: 0.053961968364909456]
	TIME [epoch: 3.27 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04557348131336003		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.04557348131336003 | validation: 0.05447961148467944]
	TIME [epoch: 3.24 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04959268097991085		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.04959268097991085 | validation: 0.0526898774270054]
	TIME [epoch: 3.25 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04831635913902855		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.04831635913902855 | validation: 0.04952544728400018]
	TIME [epoch: 3.25 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04373699523702619		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.04373699523702619 | validation: 0.05159489613120569]
	TIME [epoch: 3.25 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04589180837224906		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.04589180837224906 | validation: 0.04629083837680596]
	TIME [epoch: 3.25 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0415739830248535		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.0415739830248535 | validation: 0.0523179370603265]
	TIME [epoch: 3.25 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053288744009821504		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.053288744009821504 | validation: 0.05237923126585442]
	TIME [epoch: 3.25 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044754532007384974		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.044754532007384974 | validation: 0.048059403375154325]
	TIME [epoch: 3.25 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042075790949708626		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.042075790949708626 | validation: 0.04757931326113308]
	TIME [epoch: 3.25 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04694936152732563		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.04694936152732563 | validation: 0.05843289511623975]
	TIME [epoch: 3.26 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0489999109293397		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.0489999109293397 | validation: 0.047710148593724744]
	TIME [epoch: 3.28 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03954212858996363		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.03954212858996363 | validation: 0.04809487484773492]
	TIME [epoch: 3.25 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04550329640105094		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.04550329640105094 | validation: 0.048842116980936684]
	TIME [epoch: 3.25 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04172833804686106		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.04172833804686106 | validation: 0.04445434462391352]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_386.pth
	Model improved!!!
EPOCH 387/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04249696221830539		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.04249696221830539 | validation: 0.05497789895641262]
	TIME [epoch: 3.25 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0531299702180703		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.0531299702180703 | validation: 0.05514010764520643]
	TIME [epoch: 3.25 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046371573382607924		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.046371573382607924 | validation: 0.04673447172318004]
	TIME [epoch: 3.25 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04142204402155919		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.04142204402155919 | validation: 0.04544341146146202]
	TIME [epoch: 3.25 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04404513915088121		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.04404513915088121 | validation: 0.048186028825743125]
	TIME [epoch: 3.25 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041791263865179146		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.041791263865179146 | validation: 0.04232243679461289]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_392.pth
	Model improved!!!
EPOCH 393/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039261841253311336		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.039261841253311336 | validation: 0.04447556746962978]
	TIME [epoch: 3.26 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04334623640527758		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.04334623640527758 | validation: 0.058422344975633526]
	TIME [epoch: 3.27 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05131507577569748		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.05131507577569748 | validation: 0.05066693298973244]
	TIME [epoch: 3.25 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043432470017713255		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.043432470017713255 | validation: 0.04623859273545325]
	TIME [epoch: 3.25 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038882886860001964		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.038882886860001964 | validation: 0.040169520803394365]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_397.pth
	Model improved!!!
EPOCH 398/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03966864496928223		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.03966864496928223 | validation: 0.04447764608329806]
	TIME [epoch: 3.25 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043377447186707746		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.043377447186707746 | validation: 0.0513381129352687]
	TIME [epoch: 3.25 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05844719780416092		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.05844719780416092 | validation: 0.06107440888117264]
	TIME [epoch: 3.25 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04717465754167992		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.04717465754167992 | validation: 0.043619153831010456]
	TIME [epoch: 3.25 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04087859994985096		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.04087859994985096 | validation: 0.04201702448210731]
	TIME [epoch: 3.25 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03886488407512019		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.03886488407512019 | validation: 0.0461643643858064]
	TIME [epoch: 3.24 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03990174661982812		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.03990174661982812 | validation: 0.050909332548923686]
	TIME [epoch: 3.27 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044174775843886536		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.044174775843886536 | validation: 0.04308752806520476]
	TIME [epoch: 3.27 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038028967280487144		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.038028967280487144 | validation: 0.04307086831493414]
	TIME [epoch: 3.26 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03812676654021952		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.03812676654021952 | validation: 0.03983121463897085]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_407.pth
	Model improved!!!
EPOCH 408/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03776940441348661		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.03776940441348661 | validation: 0.043534254293780085]
	TIME [epoch: 3.25 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038334517356052145		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.038334517356052145 | validation: 0.047665011894443714]
	TIME [epoch: 3.25 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04125294968761421		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.04125294968761421 | validation: 0.041324176758838]
	TIME [epoch: 3.25 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03709246262665668		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.03709246262665668 | validation: 0.04426110264584318]
	TIME [epoch: 3.24 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038321322120921805		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.038321322120921805 | validation: 0.04312203341566592]
	TIME [epoch: 3.24 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0384666840772558		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.0384666840772558 | validation: 0.0454171551702115]
	TIME [epoch: 3.25 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04102854724033474		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.04102854724033474 | validation: 0.04609074663681156]
	TIME [epoch: 3.25 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044281490822413214		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.044281490822413214 | validation: 0.046525311373191484]
	TIME [epoch: 3.26 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03954884614614833		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.03954884614614833 | validation: 0.041155215322926514]
	TIME [epoch: 3.26 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.037114001551686746		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.037114001551686746 | validation: 0.04342558522671771]
	TIME [epoch: 3.25 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.037497327821945536		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.037497327821945536 | validation: 0.04465341137925939]
	TIME [epoch: 3.24 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03784422064090016		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.03784422064090016 | validation: 0.047134404205427846]
	TIME [epoch: 3.24 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03754180460163119		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.03754180460163119 | validation: 0.0405243452665965]
	TIME [epoch: 3.25 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03825906752348213		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.03825906752348213 | validation: 0.04317147309220252]
	TIME [epoch: 3.25 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03737027956046415		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.03737027956046415 | validation: 0.04377442753987592]
	TIME [epoch: 3.25 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040339451614232066		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.040339451614232066 | validation: 0.04386086790576431]
	TIME [epoch: 3.24 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03563370227351112		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.03563370227351112 | validation: 0.04323284736712506]
	TIME [epoch: 3.24 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03673642765472204		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.03673642765472204 | validation: 0.04302389070112889]
	TIME [epoch: 3.25 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040743462996960425		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.040743462996960425 | validation: 0.04113769318502778]
	TIME [epoch: 3.26 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03684875534930801		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.03684875534930801 | validation: 0.039498232826943064]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_427.pth
	Model improved!!!
EPOCH 428/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03630683492345584		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.03630683492345584 | validation: 0.041320942003167535]
	TIME [epoch: 3.25 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.036258743753759155		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.036258743753759155 | validation: 0.03989419045213695]
	TIME [epoch: 3.24 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03624946430118621		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.03624946430118621 | validation: 0.037733572421258244]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_430.pth
	Model improved!!!
EPOCH 431/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0330071155067304		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.0330071155067304 | validation: 0.04032773785482538]
	TIME [epoch: 3.24 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.037941581858892875		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.037941581858892875 | validation: 0.03983336948928434]
	TIME [epoch: 3.24 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03699740140229789		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.03699740140229789 | validation: 0.051586265073863186]
	TIME [epoch: 3.24 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03768894407355232		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.03768894407355232 | validation: 0.03968641521561402]
	TIME [epoch: 3.24 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03483375634494889		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.03483375634494889 | validation: 0.040287766631485825]
	TIME [epoch: 3.25 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.032309241923850075		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.032309241923850075 | validation: 0.039314976152674624]
	TIME [epoch: 3.26 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03574576954953781		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.03574576954953781 | validation: 0.038802232453288545]
	TIME [epoch: 3.26 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03626168003034286		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.03626168003034286 | validation: 0.04734100283680319]
	TIME [epoch: 3.26 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03648436120269931		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.03648436120269931 | validation: 0.03650956188308859]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_439.pth
	Model improved!!!
EPOCH 440/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03287977266909052		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.03287977266909052 | validation: 0.03956046509374977]
	TIME [epoch: 3.25 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.034176600242213084		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.034176600242213084 | validation: 0.042822443960617976]
	TIME [epoch: 3.25 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.034482707760951976		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.034482707760951976 | validation: 0.039684586411914656]
	TIME [epoch: 3.24 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0347800977821454		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.0347800977821454 | validation: 0.03876833619074564]
	TIME [epoch: 3.25 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03458766413217032		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.03458766413217032 | validation: 0.04456226409731917]
	TIME [epoch: 3.25 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03372196839669158		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.03372196839669158 | validation: 0.04090817702424673]
	TIME [epoch: 3.24 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03578032794266805		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.03578032794266805 | validation: 0.03790599961543346]
	TIME [epoch: 3.25 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03334063791877824		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.03334063791877824 | validation: 0.03728430203683626]
	TIME [epoch: 3.26 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03330237124849225		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.03330237124849225 | validation: 0.03640667451138288]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_448.pth
	Model improved!!!
EPOCH 449/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.035111359081656024		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.035111359081656024 | validation: 0.037481741687179736]
	TIME [epoch: 3.26 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.032215787087958914		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.032215787087958914 | validation: 0.03694676623016236]
	TIME [epoch: 3.24 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03126505110274785		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.03126505110274785 | validation: 0.039123164058076774]
	TIME [epoch: 3.24 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03310161724159945		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.03310161724159945 | validation: 0.03828222891025426]
	TIME [epoch: 3.24 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03112620990567234		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.03112620990567234 | validation: 0.03551244100252264]
	TIME [epoch: 3.24 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_453.pth
	Model improved!!!
EPOCH 454/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03135997721759708		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.03135997721759708 | validation: 0.044817721460245846]
	TIME [epoch: 3.24 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.035234509080200795		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.035234509080200795 | validation: 0.05667440398884485]
	TIME [epoch: 3.24 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03965694171205332		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.03965694171205332 | validation: 0.03882644103031264]
	TIME [epoch: 3.24 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03645772588540851		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.03645772588540851 | validation: 0.03559460693011506]
	TIME [epoch: 3.25 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.032541816372653964		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.032541816372653964 | validation: 0.03494271648526208]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_458.pth
	Model improved!!!
EPOCH 459/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03240883121167311		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.03240883121167311 | validation: 0.033535875007543334]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_459.pth
	Model improved!!!
EPOCH 460/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03088721040638854		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.03088721040638854 | validation: 0.03451682207373518]
	TIME [epoch: 3.25 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03236017171667984		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.03236017171667984 | validation: 0.03760628132141737]
	TIME [epoch: 3.24 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03246464968973621		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.03246464968973621 | validation: 0.03358694446188706]
	TIME [epoch: 3.24 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02961760321006863		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.02961760321006863 | validation: 0.03679214770356214]
	TIME [epoch: 3.24 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.031871360543930465		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.031871360543930465 | validation: 0.03408385077405598]
	TIME [epoch: 3.24 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.031776976871368226		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.031776976871368226 | validation: 0.03581285606881426]
	TIME [epoch: 3.24 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02974172347389505		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.02974172347389505 | validation: 0.03405150513450252]
	TIME [epoch: 3.24 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03007669858490816		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.03007669858490816 | validation: 0.03330300880880892]
	TIME [epoch: 3.24 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_467.pth
	Model improved!!!
EPOCH 468/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.032051899367002534		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.032051899367002534 | validation: 0.037874810375456036]
	TIME [epoch: 3.27 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03396942449832322		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.03396942449832322 | validation: 0.03449727073586957]
	TIME [epoch: 3.26 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.031107685894369116		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.031107685894369116 | validation: 0.034614483730770265]
	TIME [epoch: 3.26 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030331037762132328		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.030331037762132328 | validation: 0.03992524342940032]
	TIME [epoch: 3.24 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030746296271225097		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.030746296271225097 | validation: 0.03484177742261188]
	TIME [epoch: 3.25 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02897694657739526		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.02897694657739526 | validation: 0.03422379726172872]
	TIME [epoch: 3.25 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03070348499563127		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.03070348499563127 | validation: 0.03346325257275709]
	TIME [epoch: 3.25 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029388712502702037		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.029388712502702037 | validation: 0.03200919048142325]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_475.pth
	Model improved!!!
EPOCH 476/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029297425078527402		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.029297425078527402 | validation: 0.035801033115991435]
	TIME [epoch: 3.25 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02836460232096044		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.02836460232096044 | validation: 0.03118295399707756]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_477.pth
	Model improved!!!
EPOCH 478/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028131127259246422		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.028131127259246422 | validation: 0.03287220968519284]
	TIME [epoch: 3.25 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029143752464823515		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.029143752464823515 | validation: 0.03492167090725188]
	TIME [epoch: 3.26 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02998788056583624		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.02998788056583624 | validation: 0.03258977147676492]
	TIME [epoch: 3.27 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028489597516818663		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.028489597516818663 | validation: 0.03374365351715757]
	TIME [epoch: 3.27 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02798271895405395		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.02798271895405395 | validation: 0.03544737135551662]
	TIME [epoch: 3.23 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030002286098316497		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.030002286098316497 | validation: 0.03427592460207981]
	TIME [epoch: 3.25 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027672036663970608		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.027672036663970608 | validation: 0.0330898319285883]
	TIME [epoch: 3.25 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029006862956242607		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.029006862956242607 | validation: 0.03233842603793175]
	TIME [epoch: 3.25 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02800034138617249		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.02800034138617249 | validation: 0.04199133186205154]
	TIME [epoch: 3.25 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.033055078060131984		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.033055078060131984 | validation: 0.032482081480264255]
	TIME [epoch: 3.25 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029211114692398937		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.029211114692398937 | validation: 0.03338540389258759]
	TIME [epoch: 3.25 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02896655046773869		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.02896655046773869 | validation: 0.0316355050848867]
	TIME [epoch: 3.25 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029436360318332484		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.029436360318332484 | validation: 0.03053059890088715]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_490.pth
	Model improved!!!
EPOCH 491/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027068920945552098		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.027068920945552098 | validation: 0.033549160941458316]
	TIME [epoch: 3.27 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02630207771497267		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.02630207771497267 | validation: 0.0415869737956809]
	TIME [epoch: 3.26 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.035869547938603016		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.035869547938603016 | validation: 0.04135042101887407]
	TIME [epoch: 3.25 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03030883778099651		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.03030883778099651 | validation: 0.0303959817520065]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_494.pth
	Model improved!!!
EPOCH 495/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030799095594624723		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.030799095594624723 | validation: 0.033520098053379355]
	TIME [epoch: 3.25 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027809023847249002		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.027809023847249002 | validation: 0.030688996413380278]
	TIME [epoch: 3.25 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02693088033902296		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.02693088033902296 | validation: 0.029873186385278055]
	TIME [epoch: 3.25 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_497.pth
	Model improved!!!
EPOCH 498/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027331912301269738		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.027331912301269738 | validation: 0.03199414886104172]
	TIME [epoch: 3.25 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028495971605362325		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.028495971605362325 | validation: 0.03170679572162923]
	TIME [epoch: 3.24 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02614604050901413		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.02614604050901413 | validation: 0.03180823743329458]
	TIME [epoch: 3.24 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027305190739194525		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.027305190739194525 | validation: 0.03029702641324014]
	TIME [epoch: 107 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027242010872469302		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.027242010872469302 | validation: 0.03385614665710337]
	TIME [epoch: 6.41 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027138503381766738		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.027138503381766738 | validation: 0.03313125190182887]
	TIME [epoch: 6.41 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027779645992055478		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.027779645992055478 | validation: 0.03403319226625526]
	TIME [epoch: 6.43 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028874720143836383		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.028874720143836383 | validation: 0.02958868465706581]
	TIME [epoch: 6.39 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_505.pth
	Model improved!!!
EPOCH 506/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025808928858741503		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.025808928858741503 | validation: 0.029596138416810153]
	TIME [epoch: 6.39 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02611622303745448		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.02611622303745448 | validation: 0.02822774400811698]
	TIME [epoch: 6.39 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_507.pth
	Model improved!!!
EPOCH 508/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025630075029686172		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.025630075029686172 | validation: 0.02524775604603068]
	TIME [epoch: 6.39 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_508.pth
	Model improved!!!
EPOCH 509/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028265187839165807		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.028265187839165807 | validation: 0.03284120672963226]
	TIME [epoch: 6.39 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026496085387301783		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.026496085387301783 | validation: 0.027739997814960112]
	TIME [epoch: 6.43 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026063429255849528		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.026063429255849528 | validation: 0.028820231483459205]
	TIME [epoch: 6.42 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026269126509706135		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.026269126509706135 | validation: 0.03007899283824372]
	TIME [epoch: 6.41 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02708931053053721		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.02708931053053721 | validation: 0.036510962478778025]
	TIME [epoch: 6.4 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029832728992995407		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.029832728992995407 | validation: 0.025898618689847122]
	TIME [epoch: 6.4 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026089408402255015		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.026089408402255015 | validation: 0.029583783120692198]
	TIME [epoch: 6.4 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025779442497766702		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.025779442497766702 | validation: 0.027857285005744987]
	TIME [epoch: 6.43 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026173521851351002		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.026173521851351002 | validation: 0.03632032753924537]
	TIME [epoch: 6.42 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025855561248863797		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.025855561248863797 | validation: 0.030257617263380155]
	TIME [epoch: 6.39 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02895037999787327		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.02895037999787327 | validation: 0.028797111596763004]
	TIME [epoch: 6.39 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028202869518188758		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.028202869518188758 | validation: 0.027699385844000998]
	TIME [epoch: 6.39 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025775712324225278		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.025775712324225278 | validation: 0.027450100111859856]
	TIME [epoch: 6.4 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026498568644724288		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.026498568644724288 | validation: 0.028202671802250306]
	TIME [epoch: 6.4 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02532547510787815		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.02532547510787815 | validation: 0.025208946693455376]
	TIME [epoch: 6.43 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_523.pth
	Model improved!!!
EPOCH 524/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024481232386270292		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.024481232386270292 | validation: 0.0284715625365111]
	TIME [epoch: 6.4 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023935028431050366		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.023935028431050366 | validation: 0.03196367341179444]
	TIME [epoch: 6.4 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026711850234992673		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.026711850234992673 | validation: 0.029196348664751726]
	TIME [epoch: 6.4 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026114032278076786		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.026114032278076786 | validation: 0.0273005716567365]
	TIME [epoch: 6.4 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025672320185975026		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.025672320185975026 | validation: 0.027035184337946407]
	TIME [epoch: 6.41 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02547707763954352		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.02547707763954352 | validation: 0.02822977958821579]
	TIME [epoch: 6.43 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025946586880676765		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.025946586880676765 | validation: 0.027663904021675713]
	TIME [epoch: 6.39 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024906609076073213		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.024906609076073213 | validation: 0.030658765644395472]
	TIME [epoch: 6.4 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02604346410048842		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.02604346410048842 | validation: 0.029408228986703717]
	TIME [epoch: 6.37 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024974010368177986		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.024974010368177986 | validation: 0.02747463885530358]
	TIME [epoch: 6.39 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023650278442872562		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.023650278442872562 | validation: 0.023869677442231976]
	TIME [epoch: 6.41 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_534.pth
	Model improved!!!
EPOCH 535/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024316520080145582		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.024316520080145582 | validation: 0.028484944689535226]
	TIME [epoch: 6.4 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024218618517799655		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.024218618517799655 | validation: 0.027334377997069344]
	TIME [epoch: 6.4 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024021524490918968		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.024021524490918968 | validation: 0.030440781743454177]
	TIME [epoch: 6.38 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02430434393537769		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.02430434393537769 | validation: 0.02783825255909087]
	TIME [epoch: 6.4 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02378220988574939		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.02378220988574939 | validation: 0.02657343649182993]
	TIME [epoch: 6.4 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02418585385868177		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.02418585385868177 | validation: 0.02797220001337374]
	TIME [epoch: 6.4 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02482866051734976		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.02482866051734976 | validation: 0.02607753203227746]
	TIME [epoch: 6.43 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024261214378714632		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.024261214378714632 | validation: 0.027952071409995668]
	TIME [epoch: 6.41 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023055685081715304		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.023055685081715304 | validation: 0.024163049082716093]
	TIME [epoch: 6.38 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0249858667760684		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.0249858667760684 | validation: 0.026036266286324362]
	TIME [epoch: 6.4 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025416850027214513		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.025416850027214513 | validation: 0.02658248182466677]
	TIME [epoch: 6.39 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024438205306652847		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.024438205306652847 | validation: 0.029062403218759653]
	TIME [epoch: 6.39 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024276289067244626		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.024276289067244626 | validation: 0.02766310173903701]
	TIME [epoch: 6.4 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023436868470620806		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.023436868470620806 | validation: 0.02732573374193715]
	TIME [epoch: 6.43 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02445608584270853		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.02445608584270853 | validation: 0.027135360670696936]
	TIME [epoch: 6.4 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02236106556282788		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.02236106556282788 | validation: 0.027694486633937594]
	TIME [epoch: 6.4 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021899469017972482		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.021899469017972482 | validation: 0.024246766873942796]
	TIME [epoch: 6.39 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02330498735737825		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.02330498735737825 | validation: 0.02600664132586943]
	TIME [epoch: 6.38 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023510172253236025		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.023510172253236025 | validation: 0.027209113041666164]
	TIME [epoch: 6.41 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02228856712587999		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.02228856712587999 | validation: 0.02595921243774145]
	TIME [epoch: 6.44 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022652539568194165		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.022652539568194165 | validation: 0.02698259811776284]
	TIME [epoch: 6.41 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022063638657578655		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.022063638657578655 | validation: 0.027216855362305628]
	TIME [epoch: 6.41 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021721170885397417		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.021721170885397417 | validation: 0.02565790484787064]
	TIME [epoch: 6.36 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022394958506821234		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.022394958506821234 | validation: 0.024960183068007288]
	TIME [epoch: 6.4 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023387370196974913		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.023387370196974913 | validation: 0.02879428490721768]
	TIME [epoch: 6.39 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02385462811108481		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.02385462811108481 | validation: 0.027019085736472027]
	TIME [epoch: 6.42 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022296512553050478		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.022296512553050478 | validation: 0.02374229079872487]
	TIME [epoch: 6.42 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_561.pth
	Model improved!!!
EPOCH 562/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02184767561567163		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.02184767561567163 | validation: 0.026673891263305688]
	TIME [epoch: 6.41 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0227070023923436		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.0227070023923436 | validation: 0.025968123276222145]
	TIME [epoch: 6.4 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02201370803997965		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.02201370803997965 | validation: 0.024045484165400643]
	TIME [epoch: 6.4 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02439381008741865		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.02439381008741865 | validation: 0.02657643691625742]
	TIME [epoch: 6.4 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022594300828292908		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.022594300828292908 | validation: 0.02555209924255581]
	TIME [epoch: 6.42 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022301026817671564		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.022301026817671564 | validation: 0.024714017683853107]
	TIME [epoch: 6.42 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022171157921305466		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.022171157921305466 | validation: 0.029463529088099627]
	TIME [epoch: 6.4 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022878981052178168		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.022878981052178168 | validation: 0.026533036709012078]
	TIME [epoch: 6.4 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022032813714140544		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.022032813714140544 | validation: 0.022640513482895537]
	TIME [epoch: 6.4 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_570.pth
	Model improved!!!
EPOCH 571/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021520078374937922		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.021520078374937922 | validation: 0.023501065995825368]
	TIME [epoch: 6.4 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022169791928484645		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.022169791928484645 | validation: 0.029177971006387127]
	TIME [epoch: 6.41 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02234678297250612		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.02234678297250612 | validation: 0.023801179606736063]
	TIME [epoch: 6.44 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02142985885618283		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.02142985885618283 | validation: 0.026121282494867698]
	TIME [epoch: 6.41 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02140159365365928		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.02140159365365928 | validation: 0.025485792906723814]
	TIME [epoch: 6.4 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02168470275003294		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.02168470275003294 | validation: 0.02649706978891202]
	TIME [epoch: 6.4 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021726432349219046		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.021726432349219046 | validation: 0.02286893995392958]
	TIME [epoch: 6.41 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021165775502661972		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.021165775502661972 | validation: 0.023653004185375248]
	TIME [epoch: 6.41 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020367069121984126		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.020367069121984126 | validation: 0.024505702588712392]
	TIME [epoch: 6.44 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02119633828920566		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.02119633828920566 | validation: 0.02513565282324147]
	TIME [epoch: 6.42 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021720195662517622		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.021720195662517622 | validation: 0.02540740082330183]
	TIME [epoch: 6.4 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021346048532851842		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.021346048532851842 | validation: 0.02343890932639128]
	TIME [epoch: 6.4 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020012391221141894		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.020012391221141894 | validation: 0.02571960485821837]
	TIME [epoch: 6.4 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02089254372543163		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.02089254372543163 | validation: 0.027075683945552216]
	TIME [epoch: 6.4 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020988618118883473		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.020988618118883473 | validation: 0.02500618841953139]
	TIME [epoch: 6.43 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02044304129999465		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.02044304129999465 | validation: 0.025128462038603673]
	TIME [epoch: 6.4 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02196843040574232		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.02196843040574232 | validation: 0.026275657790199455]
	TIME [epoch: 6.4 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02091985614263694		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.02091985614263694 | validation: 0.023089007049210533]
	TIME [epoch: 6.38 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019469786322716932		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.019469786322716932 | validation: 0.0242614757300689]
	TIME [epoch: 6.39 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02038385044163054		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.02038385044163054 | validation: 0.023593470098855762]
	TIME [epoch: 6.4 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022451125833425324		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.022451125833425324 | validation: 0.02496435824112569]
	TIME [epoch: 6.42 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020415901955271765		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.020415901955271765 | validation: 0.02548166224670047]
	TIME [epoch: 6.42 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019659932480964155		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.019659932480964155 | validation: 0.022500505833170842]
	TIME [epoch: 6.39 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_593.pth
	Model improved!!!
EPOCH 594/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021438492540748266		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.021438492540748266 | validation: 0.022708063943808393]
	TIME [epoch: 6.4 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020213138777116144		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.020213138777116144 | validation: 0.024644001497886273]
	TIME [epoch: 6.39 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019243006625893233		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.019243006625893233 | validation: 0.022113288445507404]
	TIME [epoch: 6.4 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_596.pth
	Model improved!!!
EPOCH 597/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021047454103678257		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.021047454103678257 | validation: 0.025679570893541066]
	TIME [epoch: 6.42 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021028379585190667		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.021028379585190667 | validation: 0.02317051657282981]
	TIME [epoch: 6.42 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02069241430357061		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.02069241430357061 | validation: 0.023682778165162396]
	TIME [epoch: 6.39 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020914635850757125		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.020914635850757125 | validation: 0.02376449123848435]
	TIME [epoch: 6.39 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02022618085558943		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.02022618085558943 | validation: 0.023782127816737803]
	TIME [epoch: 6.39 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019162261888472247		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.019162261888472247 | validation: 0.02214006652963396]
	TIME [epoch: 6.39 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020179406779941976		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.020179406779941976 | validation: 0.02526191747551251]
	TIME [epoch: 6.38 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0224307795655434		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.0224307795655434 | validation: 0.02125396609631964]
	TIME [epoch: 6.43 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_604.pth
	Model improved!!!
EPOCH 605/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01916124787770783		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.01916124787770783 | validation: 0.022097665231367726]
	TIME [epoch: 6.42 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019694435669742515		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.019694435669742515 | validation: 0.021634197206464263]
	TIME [epoch: 6.4 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019868877751476645		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.019868877751476645 | validation: 0.023920293883963994]
	TIME [epoch: 6.39 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019829752087812527		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.019829752087812527 | validation: 0.024974641173401226]
	TIME [epoch: 6.41 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019485403669309592		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.019485403669309592 | validation: 0.023315870831488374]
	TIME [epoch: 6.4 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01888034151650348		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.01888034151650348 | validation: 0.024151809167314527]
	TIME [epoch: 6.42 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01978324298390375		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.01978324298390375 | validation: 0.021339242958441848]
	TIME [epoch: 6.4 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019342298712740992		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.019342298712740992 | validation: 0.023594368029796583]
	TIME [epoch: 6.39 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01931815116890212		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.01931815116890212 | validation: 0.02233779562676518]
	TIME [epoch: 6.38 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018623840771080938		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.018623840771080938 | validation: 0.0226248015797351]
	TIME [epoch: 6.39 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019250478664449837		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.019250478664449837 | validation: 0.02011505251431441]
	TIME [epoch: 6.39 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_615.pth
	Model improved!!!
EPOCH 616/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018992724992123733		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.018992724992123733 | validation: 0.02083952479500835]
	TIME [epoch: 6.43 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019127304168235305		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.019127304168235305 | validation: 0.022335875419733656]
	TIME [epoch: 6.41 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019229942739767455		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.019229942739767455 | validation: 0.023068871294017235]
	TIME [epoch: 6.39 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01983222797699913		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.01983222797699913 | validation: 0.022453852298131044]
	TIME [epoch: 6.4 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019370269984423132		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.019370269984423132 | validation: 0.023985807395417183]
	TIME [epoch: 6.4 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01905753503192669		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.01905753503192669 | validation: 0.02236956182404065]
	TIME [epoch: 6.4 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01950394583464103		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.01950394583464103 | validation: 0.023669410527916074]
	TIME [epoch: 6.41 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018734085869243154		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.018734085869243154 | validation: 0.021964394663025147]
	TIME [epoch: 6.42 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018374400969919172		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.018374400969919172 | validation: 0.019978005835935415]
	TIME [epoch: 6.4 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_624.pth
	Model improved!!!
EPOCH 625/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01783296944147588		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.01783296944147588 | validation: 0.02144307540072897]
	TIME [epoch: 6.39 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019196078871102036		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.019196078871102036 | validation: 0.021029935689451093]
	TIME [epoch: 6.39 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01909243596960874		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.01909243596960874 | validation: 0.02000633519187807]
	TIME [epoch: 6.39 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01790047626851351		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.01790047626851351 | validation: 0.021921610517024917]
	TIME [epoch: 6.42 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018762103843810676		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.018762103843810676 | validation: 0.021884248413098484]
	TIME [epoch: 6.43 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01767992053666289		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.01767992053666289 | validation: 0.020720281031894463]
	TIME [epoch: 6.4 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018536912640907587		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.018536912640907587 | validation: 0.023229565828202958]
	TIME [epoch: 6.39 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018802897038162805		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.018802897038162805 | validation: 0.022568934479293146]
	TIME [epoch: 6.39 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019066369884729227		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.019066369884729227 | validation: 0.02398564936234368]
	TIME [epoch: 6.39 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019029794308305793		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.019029794308305793 | validation: 0.019655172713401472]
	TIME [epoch: 6.39 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_634.pth
	Model improved!!!
EPOCH 635/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018334253379737642		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.018334253379737642 | validation: 0.020887915278266535]
	TIME [epoch: 6.44 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016944984260920327		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.016944984260920327 | validation: 0.021648683513180045]
	TIME [epoch: 6.39 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01792749219656605		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.01792749219656605 | validation: 0.022843729543652876]
	TIME [epoch: 6.4 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018313072915910084		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.018313072915910084 | validation: 0.023162619114038097]
	TIME [epoch: 6.39 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01783604503963976		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.01783604503963976 | validation: 0.02010954582444724]
	TIME [epoch: 6.39 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017986726857668472		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.017986726857668472 | validation: 0.020323734513009974]
	TIME [epoch: 6.4 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018400417350677056		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.018400417350677056 | validation: 0.021000008768704604]
	TIME [epoch: 6.43 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018167679567483476		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.018167679567483476 | validation: 0.021376557856587153]
	TIME [epoch: 6.4 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018566361424683632		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.018566361424683632 | validation: 0.020205509262856653]
	TIME [epoch: 6.4 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017863586222896376		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.017863586222896376 | validation: 0.020918129511140568]
	TIME [epoch: 6.39 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018402257719166957		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.018402257719166957 | validation: 0.020514502276182095]
	TIME [epoch: 6.39 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017581580742021347		[learning rate: 0.00012324]
	Learning Rate: 0.000123245
	LOSS [training: 0.017581580742021347 | validation: 0.022348660521059632]
	TIME [epoch: 6.4 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018144630586232884		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.018144630586232884 | validation: 0.020264609411019454]
	TIME [epoch: 6.41 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017483777640067893		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.017483777640067893 | validation: 0.01898965634756807]
	TIME [epoch: 6.42 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_648.pth
	Model improved!!!
EPOCH 649/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017042864174016067		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.017042864174016067 | validation: 0.019813256844459223]
	TIME [epoch: 6.39 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017028482427196102		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.017028482427196102 | validation: 0.019788114350754882]
	TIME [epoch: 6.39 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016533038263062353		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.016533038263062353 | validation: 0.021294531022225918]
	TIME [epoch: 6.39 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0177928225796829		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.0177928225796829 | validation: 0.02033038680078056]
	TIME [epoch: 6.39 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01881643611346804		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.01881643611346804 | validation: 0.01965768743615824]
	TIME [epoch: 6.42 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017441099935730328		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.017441099935730328 | validation: 0.01982165032064739]
	TIME [epoch: 6.43 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017192471091479672		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.017192471091479672 | validation: 0.022263519902425245]
	TIME [epoch: 6.39 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018113967476849516		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.018113967476849516 | validation: 0.022771206523084855]
	TIME [epoch: 6.39 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017184983137657935		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.017184983137657935 | validation: 0.019230038768883996]
	TIME [epoch: 6.4 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016889310297959547		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.016889310297959547 | validation: 0.020077985898899]
	TIME [epoch: 6.4 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018128851597484417		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.018128851597484417 | validation: 0.018504782078503052]
	TIME [epoch: 6.41 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_659.pth
	Model improved!!!
EPOCH 660/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016665066801699255		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.016665066801699255 | validation: 0.020057547346695744]
	TIME [epoch: 6.44 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01720839380490633		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.01720839380490633 | validation: 0.020897202026253114]
	TIME [epoch: 6.4 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01715766456978783		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.01715766456978783 | validation: 0.01975104043675633]
	TIME [epoch: 6.4 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016828292241951003		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.016828292241951003 | validation: 0.020233802324368496]
	TIME [epoch: 6.4 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018316941277702656		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.018316941277702656 | validation: 0.01930914810618516]
	TIME [epoch: 6.4 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017669836081288214		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.017669836081288214 | validation: 0.01922444014325769]
	TIME [epoch: 6.41 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0171770684167721		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.0171770684167721 | validation: 0.01834618525798879]
	TIME [epoch: 6.44 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_666.pth
	Model improved!!!
EPOCH 667/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01710654129978568		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.01710654129978568 | validation: 0.020546530812892247]
	TIME [epoch: 6.4 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017110916011693032		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.017110916011693032 | validation: 0.020387011256720357]
	TIME [epoch: 6.4 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016719478643302005		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.016719478643302005 | validation: 0.020002505867010068]
	TIME [epoch: 6.39 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01648372663284646		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.01648372663284646 | validation: 0.01896840461143783]
	TIME [epoch: 6.4 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017376441464807525		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.017376441464807525 | validation: 0.018387497825691376]
	TIME [epoch: 6.4 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017548604887134174		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.017548604887134174 | validation: 0.017481734176845926]
	TIME [epoch: 6.44 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_672.pth
	Model improved!!!
EPOCH 673/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016764191020508572		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.016764191020508572 | validation: 0.018745177730060902]
	TIME [epoch: 6.41 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016465074019535026		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.016465074019535026 | validation: 0.02073921845946308]
	TIME [epoch: 6.4 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016995660540212267		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.016995660540212267 | validation: 0.019482713206579447]
	TIME [epoch: 6.4 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016678246847404522		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.016678246847404522 | validation: 0.020169357884502855]
	TIME [epoch: 6.41 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016921787432137497		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.016921787432137497 | validation: 0.018412350246188376]
	TIME [epoch: 6.41 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017719978228079553		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.017719978228079553 | validation: 0.019852046515031432]
	TIME [epoch: 6.43 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016350005133964313		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.016350005133964313 | validation: 0.01875526614660198]
	TIME [epoch: 6.43 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01581851596737371		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.01581851596737371 | validation: 0.018596316250553746]
	TIME [epoch: 6.4 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01648528596712398		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.01648528596712398 | validation: 0.020179614411815146]
	TIME [epoch: 6.41 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016360748636542022		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.016360748636542022 | validation: 0.020445401939208768]
	TIME [epoch: 6.4 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017744044958152093		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.017744044958152093 | validation: 0.018478116463970795]
	TIME [epoch: 6.41 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015424816296116282		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.015424816296116282 | validation: 0.019969500566702637]
	TIME [epoch: 6.42 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015849247162755493		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.015849247162755493 | validation: 0.019173949704355965]
	TIME [epoch: 6.43 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01638220282890969		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.01638220282890969 | validation: 0.019646739928243614]
	TIME [epoch: 6.41 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01597530210359275		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.01597530210359275 | validation: 0.019026080595384666]
	TIME [epoch: 6.4 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01594127811358554		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.01594127811358554 | validation: 0.017710584113004895]
	TIME [epoch: 6.41 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015988775304227737		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.015988775304227737 | validation: 0.016507228557074965]
	TIME [epoch: 6.4 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_689.pth
	Model improved!!!
EPOCH 690/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01677932384832964		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.01677932384832964 | validation: 0.020044614702323556]
	TIME [epoch: 6.42 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016060703909339426		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.016060703909339426 | validation: 0.018233285925810805]
	TIME [epoch: 6.42 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01535095787649609		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.01535095787649609 | validation: 0.021164400624785952]
	TIME [epoch: 6.37 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015478716731966815		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.015478716731966815 | validation: 0.01805133222986533]
	TIME [epoch: 6.4 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015489038628050216		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.015489038628050216 | validation: 0.01854660909675478]
	TIME [epoch: 6.4 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016080075644713934		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.016080075644713934 | validation: 0.016522855204915827]
	TIME [epoch: 6.4 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013953675148232852		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.013953675148232852 | validation: 0.01882459214738187]
	TIME [epoch: 6.41 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016148561207307996		[learning rate: 8.5871e-05]
	Learning Rate: 8.58709e-05
	LOSS [training: 0.016148561207307996 | validation: 0.017985052711216126]
	TIME [epoch: 6.42 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01580103919221907		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.01580103919221907 | validation: 0.01893429521156071]
	TIME [epoch: 6.4 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014662668368828069		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.014662668368828069 | validation: 0.019859116661130688]
	TIME [epoch: 6.4 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015275703739208408		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.015275703739208408 | validation: 0.018814484338514494]
	TIME [epoch: 6.4 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01593315414789747		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.01593315414789747 | validation: 0.018920317481973703]
	TIME [epoch: 6.39 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016165183463210976		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.016165183463210976 | validation: 0.01869889180294946]
	TIME [epoch: 6.42 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015205653445600063		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.015205653445600063 | validation: 0.01778386699439834]
	TIME [epoch: 6.44 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015556118822523765		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.015556118822523765 | validation: 0.018321596095246334]
	TIME [epoch: 6.41 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015310616464217556		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.015310616464217556 | validation: 0.017784077048619857]
	TIME [epoch: 6.41 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01602364387154441		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.01602364387154441 | validation: 0.018525873800629932]
	TIME [epoch: 6.4 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016388826595229083		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.016388826595229083 | validation: 0.01933052234799633]
	TIME [epoch: 6.4 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015795464174759487		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.015795464174759487 | validation: 0.020207587611850177]
	TIME [epoch: 6.39 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015092764661074835		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.015092764661074835 | validation: 0.01745904607248021]
	TIME [epoch: 6.42 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01570403017369808		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.01570403017369808 | validation: 0.017682717090283268]
	TIME [epoch: 6.41 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015302661381021113		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.015302661381021113 | validation: 0.01890614721052208]
	TIME [epoch: 6.4 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01584572372970404		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.01584572372970404 | validation: 0.016384388821931408]
	TIME [epoch: 6.4 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_712.pth
	Model improved!!!
EPOCH 713/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015079192166869943		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.015079192166869943 | validation: 0.019597473811912187]
	TIME [epoch: 6.4 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01491800751240955		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.01491800751240955 | validation: 0.019441840415427375]
	TIME [epoch: 6.41 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015114741307208697		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.015114741307208697 | validation: 0.01778875563935604]
	TIME [epoch: 6.42 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014901606773903054		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.014901606773903054 | validation: 0.018374449862677136]
	TIME [epoch: 6.42 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015465817270192889		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.015465817270192889 | validation: 0.01621470268134288]
	TIME [epoch: 6.41 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_717.pth
	Model improved!!!
EPOCH 718/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014758304323820328		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.014758304323820328 | validation: 0.018455354629393846]
	TIME [epoch: 6.4 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01507682023226278		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.01507682023226278 | validation: 0.018335674912838218]
	TIME [epoch: 6.4 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016105720396263033		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.016105720396263033 | validation: 0.018177035082736003]
	TIME [epoch: 6.39 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015626912972094047		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.015626912972094047 | validation: 0.016988396097323468]
	TIME [epoch: 6.42 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01569360505451692		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.01569360505451692 | validation: 0.016986475833273135]
	TIME [epoch: 6.43 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014591596619039294		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.014591596619039294 | validation: 0.018245800678964287]
	TIME [epoch: 6.4 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015393890605907769		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.015393890605907769 | validation: 0.016903587798980085]
	TIME [epoch: 6.39 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015852485752765114		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.015852485752765114 | validation: 0.016932866977333674]
	TIME [epoch: 6.4 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015989138894872766		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.015989138894872766 | validation: 0.017659402010746596]
	TIME [epoch: 6.4 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01546296356178212		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.01546296356178212 | validation: 0.01624824555951701]
	TIME [epoch: 6.4 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014766357004529199		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.014766357004529199 | validation: 0.018392050966576097]
	TIME [epoch: 6.43 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016038692447557096		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.016038692447557096 | validation: 0.017111847540854205]
	TIME [epoch: 6.4 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01536492791788268		[learning rate: 6.7968e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.01536492791788268 | validation: 0.017374281099376693]
	TIME [epoch: 6.4 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014993099922027962		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.014993099922027962 | validation: 0.017325274758501524]
	TIME [epoch: 6.4 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013990577802106545		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.013990577802106545 | validation: 0.017284602661118382]
	TIME [epoch: 6.4 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014894482385633435		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.014894482385633435 | validation: 0.019348670583088687]
	TIME [epoch: 6.4 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014562047060906799		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.014562047060906799 | validation: 0.016610403136566327]
	TIME [epoch: 6.43 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014988088319287537		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.014988088319287537 | validation: 0.01627604243677187]
	TIME [epoch: 6.42 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015322424239959775		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.015322424239959775 | validation: 0.01650507053277973]
	TIME [epoch: 6.39 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015761450365074783		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.015761450365074783 | validation: 0.017730372253258686]
	TIME [epoch: 6.39 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014969833744913261		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.014969833744913261 | validation: 0.018122229777175453]
	TIME [epoch: 6.39 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015322871696139845		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.015322871696139845 | validation: 0.01856349265320692]
	TIME [epoch: 6.4 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014708180400238465		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.014708180400238465 | validation: 0.017644467372221025]
	TIME [epoch: 6.41 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015142653253469023		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.015142653253469023 | validation: 0.01914356623510033]
	TIME [epoch: 6.42 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015301016225068475		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.015301016225068475 | validation: 0.01523786284686849]
	TIME [epoch: 6.4 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_742.pth
	Model improved!!!
EPOCH 743/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01497643593250726		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.01497643593250726 | validation: 0.016395703441394]
	TIME [epoch: 6.4 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013967122970564173		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.013967122970564173 | validation: 0.01684454401249377]
	TIME [epoch: 6.39 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014960250360376267		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.014960250360376267 | validation: 0.018952659204577945]
	TIME [epoch: 6.41 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014868550518927466		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.014868550518927466 | validation: 0.016303378251440626]
	TIME [epoch: 6.41 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014679993581252307		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.014679993581252307 | validation: 0.016850331086693687]
	TIME [epoch: 6.43 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015143776799501166		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.015143776799501166 | validation: 0.015400778054167114]
	TIME [epoch: 6.4 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014704899138446978		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.014704899138446978 | validation: 0.015637016572092165]
	TIME [epoch: 6.4 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014466075390285563		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.014466075390285563 | validation: 0.01554802830744123]
	TIME [epoch: 6.41 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014544957554359802		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.014544957554359802 | validation: 0.016921548004154963]
	TIME [epoch: 6.4 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014744373157725784		[learning rate: 5.8159e-05]
	Learning Rate: 5.81588e-05
	LOSS [training: 0.014744373157725784 | validation: 0.016935710919913617]
	TIME [epoch: 6.41 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014930930654644091		[learning rate: 5.7748e-05]
	Learning Rate: 5.77482e-05
	LOSS [training: 0.014930930654644091 | validation: 0.017979821413221648]
	TIME [epoch: 6.43 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014168137423645085		[learning rate: 5.7341e-05]
	Learning Rate: 5.73405e-05
	LOSS [training: 0.014168137423645085 | validation: 0.015256358009203764]
	TIME [epoch: 6.41 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014715379996025521		[learning rate: 5.6936e-05]
	Learning Rate: 5.69357e-05
	LOSS [training: 0.014715379996025521 | validation: 0.01978438682937968]
	TIME [epoch: 6.41 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01407279239881409		[learning rate: 5.6534e-05]
	Learning Rate: 5.65337e-05
	LOSS [training: 0.01407279239881409 | validation: 0.017430245726159057]
	TIME [epoch: 6.4 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01405027503467756		[learning rate: 5.6135e-05]
	Learning Rate: 5.61346e-05
	LOSS [training: 0.01405027503467756 | validation: 0.015682289671454516]
	TIME [epoch: 6.39 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014087610871127317		[learning rate: 5.5738e-05]
	Learning Rate: 5.57383e-05
	LOSS [training: 0.014087610871127317 | validation: 0.017480668256773243]
	TIME [epoch: 6.41 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014321667902473311		[learning rate: 5.5345e-05]
	Learning Rate: 5.53448e-05
	LOSS [training: 0.014321667902473311 | validation: 0.016082829738708032]
	TIME [epoch: 6.43 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014000384865534787		[learning rate: 5.4954e-05]
	Learning Rate: 5.49541e-05
	LOSS [training: 0.014000384865534787 | validation: 0.01824572391288043]
	TIME [epoch: 6.4 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013304579341812952		[learning rate: 5.4566e-05]
	Learning Rate: 5.45661e-05
	LOSS [training: 0.013304579341812952 | validation: 0.01925853104907478]
	TIME [epoch: 6.39 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01493018585200048		[learning rate: 5.4181e-05]
	Learning Rate: 5.41809e-05
	LOSS [training: 0.01493018585200048 | validation: 0.01789406818537765]
	TIME [epoch: 6.39 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013756938558770726		[learning rate: 5.3798e-05]
	Learning Rate: 5.37984e-05
	LOSS [training: 0.013756938558770726 | validation: 0.01719345216990675]
	TIME [epoch: 6.39 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014056424277237252		[learning rate: 5.3419e-05]
	Learning Rate: 5.34186e-05
	LOSS [training: 0.014056424277237252 | validation: 0.016053389511766186]
	TIME [epoch: 6.4 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013733188551041579		[learning rate: 5.3041e-05]
	Learning Rate: 5.30415e-05
	LOSS [training: 0.013733188551041579 | validation: 0.01683506779832449]
	TIME [epoch: 6.41 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015064427517259598		[learning rate: 5.2667e-05]
	Learning Rate: 5.2667e-05
	LOSS [training: 0.015064427517259598 | validation: 0.01640614242198998]
	TIME [epoch: 6.43 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013037461538056114		[learning rate: 5.2295e-05]
	Learning Rate: 5.22952e-05
	LOSS [training: 0.013037461538056114 | validation: 0.017234980403303242]
	TIME [epoch: 6.39 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014162754237285297		[learning rate: 5.1926e-05]
	Learning Rate: 5.1926e-05
	LOSS [training: 0.014162754237285297 | validation: 0.017961365203046777]
	TIME [epoch: 6.39 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014031180496918719		[learning rate: 5.1559e-05]
	Learning Rate: 5.15594e-05
	LOSS [training: 0.014031180496918719 | validation: 0.016572121970087225]
	TIME [epoch: 6.4 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013710478049072019		[learning rate: 5.1195e-05]
	Learning Rate: 5.11954e-05
	LOSS [training: 0.013710478049072019 | validation: 0.016034719784378683]
	TIME [epoch: 6.4 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013809643996232326		[learning rate: 5.0834e-05]
	Learning Rate: 5.0834e-05
	LOSS [training: 0.013809643996232326 | validation: 0.017635272134186242]
	TIME [epoch: 6.41 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014109196367881997		[learning rate: 5.0475e-05]
	Learning Rate: 5.04751e-05
	LOSS [training: 0.014109196367881997 | validation: 0.01523229456408804]
	TIME [epoch: 6.44 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_772.pth
	Model improved!!!
EPOCH 773/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014035132056048394		[learning rate: 5.0119e-05]
	Learning Rate: 5.01187e-05
	LOSS [training: 0.014035132056048394 | validation: 0.015646173243333682]
	TIME [epoch: 6.41 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013982054039273979		[learning rate: 4.9765e-05]
	Learning Rate: 4.97649e-05
	LOSS [training: 0.013982054039273979 | validation: 0.016059901251708277]
	TIME [epoch: 6.4 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01318178351644977		[learning rate: 4.9414e-05]
	Learning Rate: 4.94136e-05
	LOSS [training: 0.01318178351644977 | validation: 0.015793142834331763]
	TIME [epoch: 6.4 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014498029974367194		[learning rate: 4.9065e-05]
	Learning Rate: 4.90647e-05
	LOSS [training: 0.014498029974367194 | validation: 0.018378388690919494]
	TIME [epoch: 6.4 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014112575565198788		[learning rate: 4.8718e-05]
	Learning Rate: 4.87183e-05
	LOSS [training: 0.014112575565198788 | validation: 0.015987510001340657]
	TIME [epoch: 6.41 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013423350136257377		[learning rate: 4.8374e-05]
	Learning Rate: 4.83744e-05
	LOSS [training: 0.013423350136257377 | validation: 0.016052866696289997]
	TIME [epoch: 6.44 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013913772093214871		[learning rate: 4.8033e-05]
	Learning Rate: 4.80329e-05
	LOSS [training: 0.013913772093214871 | validation: 0.015132315841002964]
	TIME [epoch: 6.41 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_779.pth
	Model improved!!!
EPOCH 780/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014361389517277941		[learning rate: 4.7694e-05]
	Learning Rate: 4.76938e-05
	LOSS [training: 0.014361389517277941 | validation: 0.014827970912116396]
	TIME [epoch: 6.41 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_780.pth
	Model improved!!!
EPOCH 781/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014337145003104741		[learning rate: 4.7357e-05]
	Learning Rate: 4.73571e-05
	LOSS [training: 0.014337145003104741 | validation: 0.014789497270917579]
	TIME [epoch: 6.4 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_781.pth
	Model improved!!!
EPOCH 782/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012958390958384908		[learning rate: 4.7023e-05]
	Learning Rate: 4.70227e-05
	LOSS [training: 0.012958390958384908 | validation: 0.015811447959165253]
	TIME [epoch: 6.4 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013567689264963986		[learning rate: 4.6691e-05]
	Learning Rate: 4.66907e-05
	LOSS [training: 0.013567689264963986 | validation: 0.015633841020915286]
	TIME [epoch: 6.41 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013924711376565764		[learning rate: 4.6361e-05]
	Learning Rate: 4.63611e-05
	LOSS [training: 0.013924711376565764 | validation: 0.0170230071352646]
	TIME [epoch: 6.43 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013904215531048349		[learning rate: 4.6034e-05]
	Learning Rate: 4.60338e-05
	LOSS [training: 0.013904215531048349 | validation: 0.016678652972455157]
	TIME [epoch: 6.4 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013370218255312268		[learning rate: 4.5709e-05]
	Learning Rate: 4.57088e-05
	LOSS [training: 0.013370218255312268 | validation: 0.017313423005757552]
	TIME [epoch: 6.39 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01359462166806467		[learning rate: 4.5386e-05]
	Learning Rate: 4.53861e-05
	LOSS [training: 0.01359462166806467 | validation: 0.015533449574838532]
	TIME [epoch: 6.4 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01326083793800006		[learning rate: 4.5066e-05]
	Learning Rate: 4.50657e-05
	LOSS [training: 0.01326083793800006 | validation: 0.015520290401522441]
	TIME [epoch: 6.39 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013463445588544913		[learning rate: 4.4748e-05]
	Learning Rate: 4.47476e-05
	LOSS [training: 0.013463445588544913 | validation: 0.01483000253907113]
	TIME [epoch: 6.39 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013524919789312568		[learning rate: 4.4432e-05]
	Learning Rate: 4.44316e-05
	LOSS [training: 0.013524919789312568 | validation: 0.016354704123889015]
	TIME [epoch: 6.42 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013508098116914957		[learning rate: 4.4118e-05]
	Learning Rate: 4.4118e-05
	LOSS [training: 0.013508098116914957 | validation: 0.01662925714575621]
	TIME [epoch: 6.41 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015093220375043195		[learning rate: 4.3807e-05]
	Learning Rate: 4.38065e-05
	LOSS [training: 0.015093220375043195 | validation: 0.017057387223080922]
	TIME [epoch: 6.4 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01362203147877639		[learning rate: 4.3497e-05]
	Learning Rate: 4.34972e-05
	LOSS [training: 0.01362203147877639 | validation: 0.016049934120937717]
	TIME [epoch: 6.4 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013305273884473414		[learning rate: 4.319e-05]
	Learning Rate: 4.31902e-05
	LOSS [training: 0.013305273884473414 | validation: 0.01744630010313616]
	TIME [epoch: 6.39 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013668409185395785		[learning rate: 4.2885e-05]
	Learning Rate: 4.28852e-05
	LOSS [training: 0.013668409185395785 | validation: 0.015500339450199518]
	TIME [epoch: 6.4 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014161594142226713		[learning rate: 4.2582e-05]
	Learning Rate: 4.25825e-05
	LOSS [training: 0.014161594142226713 | validation: 0.017294399971073043]
	TIME [epoch: 6.42 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013215928031060335		[learning rate: 4.2282e-05]
	Learning Rate: 4.22819e-05
	LOSS [training: 0.013215928031060335 | validation: 0.014894029564868561]
	TIME [epoch: 6.43 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013620074280075632		[learning rate: 4.1983e-05]
	Learning Rate: 4.19833e-05
	LOSS [training: 0.013620074280075632 | validation: 0.015176848862324517]
	TIME [epoch: 6.4 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01372515570961216		[learning rate: 4.1687e-05]
	Learning Rate: 4.16869e-05
	LOSS [training: 0.01372515570961216 | validation: 0.014905915553118411]
	TIME [epoch: 6.4 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013168421302109844		[learning rate: 4.1393e-05]
	Learning Rate: 4.13926e-05
	LOSS [training: 0.013168421302109844 | validation: 0.015937681327077746]
	TIME [epoch: 6.4 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012857603188223349		[learning rate: 4.11e-05]
	Learning Rate: 4.11004e-05
	LOSS [training: 0.012857603188223349 | validation: 0.01580625509224747]
	TIME [epoch: 6.4 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013109433123925821		[learning rate: 4.081e-05]
	Learning Rate: 4.08103e-05
	LOSS [training: 0.013109433123925821 | validation: 0.015964270797312365]
	TIME [epoch: 6.42 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01348310061100505		[learning rate: 4.0522e-05]
	Learning Rate: 4.05221e-05
	LOSS [training: 0.01348310061100505 | validation: 0.015171835189215582]
	TIME [epoch: 6.43 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014215134236475059		[learning rate: 4.0236e-05]
	Learning Rate: 4.02361e-05
	LOSS [training: 0.014215134236475059 | validation: 0.014728268939945562]
	TIME [epoch: 6.4 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_804.pth
	Model improved!!!
EPOCH 805/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013470652073716794		[learning rate: 3.9952e-05]
	Learning Rate: 3.9952e-05
	LOSS [training: 0.013470652073716794 | validation: 0.015498240413713744]
	TIME [epoch: 6.4 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012943598267850883		[learning rate: 3.967e-05]
	Learning Rate: 3.967e-05
	LOSS [training: 0.012943598267850883 | validation: 0.017177132543001378]
	TIME [epoch: 6.39 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014064229094280206		[learning rate: 3.939e-05]
	Learning Rate: 3.93899e-05
	LOSS [training: 0.014064229094280206 | validation: 0.016103079491985563]
	TIME [epoch: 6.4 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013317251179402741		[learning rate: 3.9112e-05]
	Learning Rate: 3.91118e-05
	LOSS [training: 0.013317251179402741 | validation: 0.015893811299034166]
	TIME [epoch: 6.41 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013563397716040301		[learning rate: 3.8836e-05]
	Learning Rate: 3.88357e-05
	LOSS [training: 0.013563397716040301 | validation: 0.017017845665381788]
	TIME [epoch: 6.44 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013872715444078974		[learning rate: 3.8561e-05]
	Learning Rate: 3.85615e-05
	LOSS [training: 0.013872715444078974 | validation: 0.014246091021489172]
	TIME [epoch: 6.4 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_810.pth
	Model improved!!!
EPOCH 811/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014153301428477857		[learning rate: 3.8289e-05]
	Learning Rate: 3.82893e-05
	LOSS [training: 0.014153301428477857 | validation: 0.01644172056969146]
	TIME [epoch: 6.4 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013341681662713637		[learning rate: 3.8019e-05]
	Learning Rate: 3.8019e-05
	LOSS [training: 0.013341681662713637 | validation: 0.01626700925703451]
	TIME [epoch: 6.39 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013592674226511992		[learning rate: 3.7751e-05]
	Learning Rate: 3.77505e-05
	LOSS [training: 0.013592674226511992 | validation: 0.01601697005311607]
	TIME [epoch: 6.4 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013428580504345769		[learning rate: 3.7484e-05]
	Learning Rate: 3.7484e-05
	LOSS [training: 0.013428580504345769 | validation: 0.015798389856877575]
	TIME [epoch: 6.4 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013624034249997034		[learning rate: 3.7219e-05]
	Learning Rate: 3.72194e-05
	LOSS [training: 0.013624034249997034 | validation: 0.016175456900664503]
	TIME [epoch: 6.43 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01374625251285332		[learning rate: 3.6957e-05]
	Learning Rate: 3.69566e-05
	LOSS [training: 0.01374625251285332 | validation: 0.016534827559709787]
	TIME [epoch: 6.4 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013473925631184077		[learning rate: 3.6696e-05]
	Learning Rate: 3.66957e-05
	LOSS [training: 0.013473925631184077 | validation: 0.018076358610417193]
	TIME [epoch: 6.4 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013210415745637222		[learning rate: 3.6437e-05]
	Learning Rate: 3.64367e-05
	LOSS [training: 0.013210415745637222 | validation: 0.017201656409338153]
	TIME [epoch: 6.4 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012849803785147438		[learning rate: 3.6179e-05]
	Learning Rate: 3.61794e-05
	LOSS [training: 0.012849803785147438 | validation: 0.016310795842874853]
	TIME [epoch: 6.39 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012927584659604278		[learning rate: 3.5924e-05]
	Learning Rate: 3.5924e-05
	LOSS [training: 0.012927584659604278 | validation: 0.014016946765396561]
	TIME [epoch: 6.4 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_820.pth
	Model improved!!!
EPOCH 821/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013460348726657528		[learning rate: 3.567e-05]
	Learning Rate: 3.56704e-05
	LOSS [training: 0.013460348726657528 | validation: 0.015094159096153687]
	TIME [epoch: 6.44 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013381250259769646		[learning rate: 3.5419e-05]
	Learning Rate: 3.54186e-05
	LOSS [training: 0.013381250259769646 | validation: 0.014598044209961449]
	TIME [epoch: 6.41 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013683290563884617		[learning rate: 3.5169e-05]
	Learning Rate: 3.51685e-05
	LOSS [training: 0.013683290563884617 | validation: 0.015528345347923068]
	TIME [epoch: 6.4 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013767880595301027		[learning rate: 3.492e-05]
	Learning Rate: 3.49202e-05
	LOSS [training: 0.013767880595301027 | validation: 0.016049982574926177]
	TIME [epoch: 6.4 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012934853335797313		[learning rate: 3.4674e-05]
	Learning Rate: 3.46737e-05
	LOSS [training: 0.012934853335797313 | validation: 0.014983074622076743]
	TIME [epoch: 6.4 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013054493046312224		[learning rate: 3.4429e-05]
	Learning Rate: 3.44289e-05
	LOSS [training: 0.013054493046312224 | validation: 0.01781102582547488]
	TIME [epoch: 6.4 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013380312779422119		[learning rate: 3.4186e-05]
	Learning Rate: 3.41858e-05
	LOSS [training: 0.013380312779422119 | validation: 0.015452554600110231]
	TIME [epoch: 6.41 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013506065504285509		[learning rate: 3.3944e-05]
	Learning Rate: 3.39445e-05
	LOSS [training: 0.013506065504285509 | validation: 0.014785956955345753]
	TIME [epoch: 6.43 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012673739820113097		[learning rate: 3.3705e-05]
	Learning Rate: 3.37049e-05
	LOSS [training: 0.012673739820113097 | validation: 0.01785741830766725]
	TIME [epoch: 6.4 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012307161667325586		[learning rate: 3.3467e-05]
	Learning Rate: 3.34669e-05
	LOSS [training: 0.012307161667325586 | validation: 0.014107374716295064]
	TIME [epoch: 6.4 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01313930278698858		[learning rate: 3.3231e-05]
	Learning Rate: 3.32306e-05
	LOSS [training: 0.01313930278698858 | validation: 0.015988487673590547]
	TIME [epoch: 6.39 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013116701222412125		[learning rate: 3.2996e-05]
	Learning Rate: 3.2996e-05
	LOSS [training: 0.013116701222412125 | validation: 0.015977741530439637]
	TIME [epoch: 6.4 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013924370525495496		[learning rate: 3.2763e-05]
	Learning Rate: 3.27631e-05
	LOSS [training: 0.013924370525495496 | validation: 0.015005537009891507]
	TIME [epoch: 6.41 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013512549058777548		[learning rate: 3.2532e-05]
	Learning Rate: 3.25318e-05
	LOSS [training: 0.013512549058777548 | validation: 0.015023382815850762]
	TIME [epoch: 6.43 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013524742803276485		[learning rate: 3.2302e-05]
	Learning Rate: 3.23021e-05
	LOSS [training: 0.013524742803276485 | validation: 0.015651585861391842]
	TIME [epoch: 6.39 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013714030136202787		[learning rate: 3.2074e-05]
	Learning Rate: 3.20741e-05
	LOSS [training: 0.013714030136202787 | validation: 0.017971169326427734]
	TIME [epoch: 6.39 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013909980271524		[learning rate: 3.1848e-05]
	Learning Rate: 3.18476e-05
	LOSS [training: 0.013909980271524 | validation: 0.015091482295762943]
	TIME [epoch: 6.39 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013033623628592832		[learning rate: 3.1623e-05]
	Learning Rate: 3.16228e-05
	LOSS [training: 0.013033623628592832 | validation: 0.016015346703409702]
	TIME [epoch: 6.4 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013697719184598955		[learning rate: 3.14e-05]
	Learning Rate: 3.13995e-05
	LOSS [training: 0.013697719184598955 | validation: 0.015660238151390865]
	TIME [epoch: 6.39 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013505994306226538		[learning rate: 3.1178e-05]
	Learning Rate: 3.11778e-05
	LOSS [training: 0.013505994306226538 | validation: 0.01570255840003621]
	TIME [epoch: 6.43 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013442816040509521		[learning rate: 3.0958e-05]
	Learning Rate: 3.09577e-05
	LOSS [training: 0.013442816040509521 | validation: 0.01627160127558356]
	TIME [epoch: 6.39 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013790170097596883		[learning rate: 3.0739e-05]
	Learning Rate: 3.07392e-05
	LOSS [training: 0.013790170097596883 | validation: 0.01580880082672182]
	TIME [epoch: 6.4 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012898698396071155		[learning rate: 3.0522e-05]
	Learning Rate: 3.05222e-05
	LOSS [training: 0.012898698396071155 | validation: 0.015510944690655632]
	TIME [epoch: 6.4 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012760506895311826		[learning rate: 3.0307e-05]
	Learning Rate: 3.03067e-05
	LOSS [training: 0.012760506895311826 | validation: 0.016032946492839407]
	TIME [epoch: 6.39 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012233019531386287		[learning rate: 3.0093e-05]
	Learning Rate: 3.00927e-05
	LOSS [training: 0.012233019531386287 | validation: 0.014608929508418717]
	TIME [epoch: 6.4 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013177353369832081		[learning rate: 2.988e-05]
	Learning Rate: 2.98803e-05
	LOSS [training: 0.013177353369832081 | validation: 0.01655547842437029]
	TIME [epoch: 6.43 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012826636295953188		[learning rate: 2.9669e-05]
	Learning Rate: 2.96693e-05
	LOSS [training: 0.012826636295953188 | validation: 0.015646623904940506]
	TIME [epoch: 6.4 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013566846689632667		[learning rate: 2.946e-05]
	Learning Rate: 2.94599e-05
	LOSS [training: 0.013566846689632667 | validation: 0.016758403502556064]
	TIME [epoch: 6.39 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013924576632573746		[learning rate: 2.9252e-05]
	Learning Rate: 2.92519e-05
	LOSS [training: 0.013924576632573746 | validation: 0.014267712811233901]
	TIME [epoch: 6.39 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012829692424083777		[learning rate: 2.9045e-05]
	Learning Rate: 2.90454e-05
	LOSS [training: 0.012829692424083777 | validation: 0.014766826344779927]
	TIME [epoch: 6.4 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013486707089299912		[learning rate: 2.884e-05]
	Learning Rate: 2.88403e-05
	LOSS [training: 0.013486707089299912 | validation: 0.016890770337836435]
	TIME [epoch: 6.39 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012961191918768983		[learning rate: 2.8637e-05]
	Learning Rate: 2.86367e-05
	LOSS [training: 0.012961191918768983 | validation: 0.012973439124817077]
	TIME [epoch: 6.42 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_852.pth
	Model improved!!!
EPOCH 853/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013464109870686025		[learning rate: 2.8435e-05]
	Learning Rate: 2.84345e-05
	LOSS [training: 0.013464109870686025 | validation: 0.014708080889555393]
	TIME [epoch: 6.43 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012775566934052338		[learning rate: 2.8234e-05]
	Learning Rate: 2.82338e-05
	LOSS [training: 0.012775566934052338 | validation: 0.01646912939405152]
	TIME [epoch: 6.4 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012226239440676228		[learning rate: 2.8034e-05]
	Learning Rate: 2.80345e-05
	LOSS [training: 0.012226239440676228 | validation: 0.01636500601167787]
	TIME [epoch: 6.39 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012891436867803923		[learning rate: 2.7837e-05]
	Learning Rate: 2.78366e-05
	LOSS [training: 0.012891436867803923 | validation: 0.015656564611538727]
	TIME [epoch: 6.4 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013406669497148607		[learning rate: 2.764e-05]
	Learning Rate: 2.764e-05
	LOSS [training: 0.013406669497148607 | validation: 0.015266161119580585]
	TIME [epoch: 6.4 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01215731598789562		[learning rate: 2.7445e-05]
	Learning Rate: 2.74449e-05
	LOSS [training: 0.01215731598789562 | validation: 0.015256634779388484]
	TIME [epoch: 6.41 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012152052520893422		[learning rate: 2.7251e-05]
	Learning Rate: 2.72511e-05
	LOSS [training: 0.012152052520893422 | validation: 0.015316142689136404]
	TIME [epoch: 6.44 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013468305615823063		[learning rate: 2.7059e-05]
	Learning Rate: 2.70587e-05
	LOSS [training: 0.013468305615823063 | validation: 0.016453692287665928]
	TIME [epoch: 6.4 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0129412664492945		[learning rate: 2.6868e-05]
	Learning Rate: 2.68677e-05
	LOSS [training: 0.0129412664492945 | validation: 0.01606734433503299]
	TIME [epoch: 6.39 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013846456624685293		[learning rate: 2.6678e-05]
	Learning Rate: 2.6678e-05
	LOSS [training: 0.013846456624685293 | validation: 0.015181362554044016]
	TIME [epoch: 6.4 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013028227630636154		[learning rate: 2.649e-05]
	Learning Rate: 2.64897e-05
	LOSS [training: 0.013028227630636154 | validation: 0.014182957209373143]
	TIME [epoch: 6.4 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012310697211896027		[learning rate: 2.6303e-05]
	Learning Rate: 2.63027e-05
	LOSS [training: 0.012310697211896027 | validation: 0.015989949871244993]
	TIME [epoch: 6.4 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013009017774930024		[learning rate: 2.6117e-05]
	Learning Rate: 2.6117e-05
	LOSS [training: 0.013009017774930024 | validation: 0.014917040697026691]
	TIME [epoch: 6.43 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013425032927513621		[learning rate: 2.5933e-05]
	Learning Rate: 2.59326e-05
	LOSS [training: 0.013425032927513621 | validation: 0.014636656587537423]
	TIME [epoch: 6.4 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012415247089413217		[learning rate: 2.575e-05]
	Learning Rate: 2.57495e-05
	LOSS [training: 0.012415247089413217 | validation: 0.01478012425301329]
	TIME [epoch: 6.39 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012181053599170991		[learning rate: 2.5568e-05]
	Learning Rate: 2.55677e-05
	LOSS [training: 0.012181053599170991 | validation: 0.015854543668406402]
	TIME [epoch: 6.4 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013100023716900324		[learning rate: 2.5387e-05]
	Learning Rate: 2.53872e-05
	LOSS [training: 0.013100023716900324 | validation: 0.014412792241595569]
	TIME [epoch: 6.4 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012927195535704426		[learning rate: 2.5208e-05]
	Learning Rate: 2.5208e-05
	LOSS [training: 0.012927195535704426 | validation: 0.01666603015307531]
	TIME [epoch: 6.4 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013401025453017027		[learning rate: 2.503e-05]
	Learning Rate: 2.503e-05
	LOSS [training: 0.013401025453017027 | validation: 0.015732646241905436]
	TIME [epoch: 6.42 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012472710006058377		[learning rate: 2.4853e-05]
	Learning Rate: 2.48533e-05
	LOSS [training: 0.012472710006058377 | validation: 0.015672302142031042]
	TIME [epoch: 6.4 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01301717603706684		[learning rate: 2.4678e-05]
	Learning Rate: 2.46779e-05
	LOSS [training: 0.01301717603706684 | validation: 0.014510949161004805]
	TIME [epoch: 6.4 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013799686757967266		[learning rate: 2.4504e-05]
	Learning Rate: 2.45037e-05
	LOSS [training: 0.013799686757967266 | validation: 0.015886607060373644]
	TIME [epoch: 6.4 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012769771641080358		[learning rate: 2.4331e-05]
	Learning Rate: 2.43307e-05
	LOSS [training: 0.012769771641080358 | validation: 0.01591846698096267]
	TIME [epoch: 6.4 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01238704695831903		[learning rate: 2.4159e-05]
	Learning Rate: 2.41589e-05
	LOSS [training: 0.01238704695831903 | validation: 0.014898788432999145]
	TIME [epoch: 6.4 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013282086378682365		[learning rate: 2.3988e-05]
	Learning Rate: 2.39883e-05
	LOSS [training: 0.013282086378682365 | validation: 0.013748946000484846]
	TIME [epoch: 6.42 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013127961200800585		[learning rate: 2.3819e-05]
	Learning Rate: 2.3819e-05
	LOSS [training: 0.013127961200800585 | validation: 0.014484514570760627]
	TIME [epoch: 6.42 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0119666411160958		[learning rate: 2.3651e-05]
	Learning Rate: 2.36508e-05
	LOSS [training: 0.0119666411160958 | validation: 0.014305092829728467]
	TIME [epoch: 6.4 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012140232286638705		[learning rate: 2.3484e-05]
	Learning Rate: 2.34838e-05
	LOSS [training: 0.012140232286638705 | validation: 0.015826953189296136]
	TIME [epoch: 6.4 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012457022165511915		[learning rate: 2.3318e-05]
	Learning Rate: 2.33181e-05
	LOSS [training: 0.012457022165511915 | validation: 0.01447700440951218]
	TIME [epoch: 6.4 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011873610778703938		[learning rate: 2.3153e-05]
	Learning Rate: 2.31534e-05
	LOSS [training: 0.011873610778703938 | validation: 0.01670140876772625]
	TIME [epoch: 6.39 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012917159940841026		[learning rate: 2.299e-05]
	Learning Rate: 2.299e-05
	LOSS [training: 0.012917159940841026 | validation: 0.013933316669862757]
	TIME [epoch: 6.41 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012458869850373408		[learning rate: 2.2828e-05]
	Learning Rate: 2.28277e-05
	LOSS [training: 0.012458869850373408 | validation: 0.014807560249933094]
	TIME [epoch: 6.43 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01289845447924972		[learning rate: 2.2667e-05]
	Learning Rate: 2.26665e-05
	LOSS [training: 0.01289845447924972 | validation: 0.015506168982965607]
	TIME [epoch: 6.4 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013470664328608207		[learning rate: 2.2506e-05]
	Learning Rate: 2.25065e-05
	LOSS [training: 0.013470664328608207 | validation: 0.01410661901157031]
	TIME [epoch: 6.39 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013120171549188257		[learning rate: 2.2348e-05]
	Learning Rate: 2.23476e-05
	LOSS [training: 0.013120171549188257 | validation: 0.014721828073090177]
	TIME [epoch: 6.4 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012438910867208771		[learning rate: 2.219e-05]
	Learning Rate: 2.21898e-05
	LOSS [training: 0.012438910867208771 | validation: 0.014702208473352713]
	TIME [epoch: 6.39 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011533966543196354		[learning rate: 2.2033e-05]
	Learning Rate: 2.20332e-05
	LOSS [training: 0.011533966543196354 | validation: 0.01480042486664811]
	TIME [epoch: 6.4 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.014247745530858481		[learning rate: 2.1878e-05]
	Learning Rate: 2.18776e-05
	LOSS [training: 0.014247745530858481 | validation: 0.014030187689429803]
	TIME [epoch: 6.43 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012327250228509303		[learning rate: 2.1723e-05]
	Learning Rate: 2.17232e-05
	LOSS [training: 0.012327250228509303 | validation: 0.015433863920540958]
	TIME [epoch: 6.41 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012830995228453362		[learning rate: 2.157e-05]
	Learning Rate: 2.15698e-05
	LOSS [training: 0.012830995228453362 | validation: 0.01521859669856659]
	TIME [epoch: 6.39 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01188386290234641		[learning rate: 2.1418e-05]
	Learning Rate: 2.14175e-05
	LOSS [training: 0.01188386290234641 | validation: 0.01647919606429616]
	TIME [epoch: 6.4 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012079129490832018		[learning rate: 2.1266e-05]
	Learning Rate: 2.12663e-05
	LOSS [training: 0.012079129490832018 | validation: 0.013356568118574991]
	TIME [epoch: 6.39 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012256764039866716		[learning rate: 2.1116e-05]
	Learning Rate: 2.11162e-05
	LOSS [training: 0.012256764039866716 | validation: 0.016438099744075573]
	TIME [epoch: 6.4 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012586918151991202		[learning rate: 2.0967e-05]
	Learning Rate: 2.09671e-05
	LOSS [training: 0.012586918151991202 | validation: 0.014260507165734343]
	TIME [epoch: 6.41 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012010011632586855		[learning rate: 2.0819e-05]
	Learning Rate: 2.08191e-05
	LOSS [training: 0.012010011632586855 | validation: 0.014600482222993878]
	TIME [epoch: 6.42 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011619514235550597		[learning rate: 2.0672e-05]
	Learning Rate: 2.06721e-05
	LOSS [training: 0.011619514235550597 | validation: 0.015593693595378029]
	TIME [epoch: 6.4 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01233178801590682		[learning rate: 2.0526e-05]
	Learning Rate: 2.05262e-05
	LOSS [training: 0.01233178801590682 | validation: 0.015149305662145424]
	TIME [epoch: 6.39 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012371568947256738		[learning rate: 2.0381e-05]
	Learning Rate: 2.03813e-05
	LOSS [training: 0.012371568947256738 | validation: 0.016043738515480067]
	TIME [epoch: 6.39 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0125577592280059		[learning rate: 2.0237e-05]
	Learning Rate: 2.02374e-05
	LOSS [training: 0.0125577592280059 | validation: 0.014330897351334738]
	TIME [epoch: 6.4 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012659004495800751		[learning rate: 2.0094e-05]
	Learning Rate: 2.00945e-05
	LOSS [training: 0.012659004495800751 | validation: 0.012300994819703537]
	TIME [epoch: 6.42 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_902.pth
	Model improved!!!
EPOCH 903/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012716096937462024		[learning rate: 1.9953e-05]
	Learning Rate: 1.99526e-05
	LOSS [training: 0.012716096937462024 | validation: 0.016022462773186315]
	TIME [epoch: 6.4 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012624078051864884		[learning rate: 1.9812e-05]
	Learning Rate: 1.98118e-05
	LOSS [training: 0.012624078051864884 | validation: 0.014373373589017968]
	TIME [epoch: 6.38 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012726638292189084		[learning rate: 1.9672e-05]
	Learning Rate: 1.96719e-05
	LOSS [training: 0.012726638292189084 | validation: 0.01377254340530858]
	TIME [epoch: 6.37 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012472484320826725		[learning rate: 1.9533e-05]
	Learning Rate: 1.9533e-05
	LOSS [training: 0.012472484320826725 | validation: 0.013302359543967269]
	TIME [epoch: 6.4 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012561025489562536		[learning rate: 1.9395e-05]
	Learning Rate: 1.93951e-05
	LOSS [training: 0.012561025489562536 | validation: 0.013647786381281862]
	TIME [epoch: 6.4 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012168679435011626		[learning rate: 1.9258e-05]
	Learning Rate: 1.92582e-05
	LOSS [training: 0.012168679435011626 | validation: 0.0157561778810886]
	TIME [epoch: 6.4 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012590535341532667		[learning rate: 1.9122e-05]
	Learning Rate: 1.91222e-05
	LOSS [training: 0.012590535341532667 | validation: 0.015862790991580656]
	TIME [epoch: 6.42 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011596075793447416		[learning rate: 1.8987e-05]
	Learning Rate: 1.89872e-05
	LOSS [training: 0.011596075793447416 | validation: 0.016715769745998945]
	TIME [epoch: 6.39 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012575445205146576		[learning rate: 1.8853e-05]
	Learning Rate: 1.88532e-05
	LOSS [training: 0.012575445205146576 | validation: 0.01464888471450353]
	TIME [epoch: 6.4 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013088861786898592		[learning rate: 1.872e-05]
	Learning Rate: 1.87201e-05
	LOSS [training: 0.013088861786898592 | validation: 0.014595960154810655]
	TIME [epoch: 6.4 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01235981559091854		[learning rate: 1.8588e-05]
	Learning Rate: 1.85879e-05
	LOSS [training: 0.01235981559091854 | validation: 0.01645275364621871]
	TIME [epoch: 6.42 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011746562155491636		[learning rate: 1.8457e-05]
	Learning Rate: 1.84567e-05
	LOSS [training: 0.011746562155491636 | validation: 0.014419150994609038]
	TIME [epoch: 6.4 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012285562774169714		[learning rate: 1.8326e-05]
	Learning Rate: 1.83264e-05
	LOSS [training: 0.012285562774169714 | validation: 0.013586861765335835]
	TIME [epoch: 6.42 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011622786960185212		[learning rate: 1.8197e-05]
	Learning Rate: 1.8197e-05
	LOSS [training: 0.011622786960185212 | validation: 0.014731284703320469]
	TIME [epoch: 6.38 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012428273642898006		[learning rate: 1.8069e-05]
	Learning Rate: 1.80685e-05
	LOSS [training: 0.012428273642898006 | validation: 0.015287379102185573]
	TIME [epoch: 6.39 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012685032176303523		[learning rate: 1.7941e-05]
	Learning Rate: 1.7941e-05
	LOSS [training: 0.012685032176303523 | validation: 0.013937932964129091]
	TIME [epoch: 6.41 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012513428748159694		[learning rate: 1.7814e-05]
	Learning Rate: 1.78143e-05
	LOSS [training: 0.012513428748159694 | validation: 0.015127240513600832]
	TIME [epoch: 6.4 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012091623731012729		[learning rate: 1.7689e-05]
	Learning Rate: 1.76886e-05
	LOSS [training: 0.012091623731012729 | validation: 0.012248523142299562]
	TIME [epoch: 6.37 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_920.pth
	Model improved!!!
EPOCH 921/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012825332055464557		[learning rate: 1.7564e-05]
	Learning Rate: 1.75637e-05
	LOSS [training: 0.012825332055464557 | validation: 0.014958658588569007]
	TIME [epoch: 6.44 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012443093546983489		[learning rate: 1.744e-05]
	Learning Rate: 1.74397e-05
	LOSS [training: 0.012443093546983489 | validation: 0.015128893705580971]
	TIME [epoch: 6.41 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011844881396847105		[learning rate: 1.7317e-05]
	Learning Rate: 1.73166e-05
	LOSS [training: 0.011844881396847105 | validation: 0.016848857405743267]
	TIME [epoch: 6.41 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012663268438280282		[learning rate: 1.7194e-05]
	Learning Rate: 1.71943e-05
	LOSS [training: 0.012663268438280282 | validation: 0.015160011633650484]
	TIME [epoch: 6.41 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012485915666163978		[learning rate: 1.7073e-05]
	Learning Rate: 1.70729e-05
	LOSS [training: 0.012485915666163978 | validation: 0.014753609076948105]
	TIME [epoch: 6.4 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012582802805658707		[learning rate: 1.6952e-05]
	Learning Rate: 1.69524e-05
	LOSS [training: 0.012582802805658707 | validation: 0.01423089109293298]
	TIME [epoch: 6.39 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01249119141023793		[learning rate: 1.6833e-05]
	Learning Rate: 1.68327e-05
	LOSS [training: 0.01249119141023793 | validation: 0.012995162760916014]
	TIME [epoch: 6.42 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012306538398745279		[learning rate: 1.6714e-05]
	Learning Rate: 1.67139e-05
	LOSS [training: 0.012306538398745279 | validation: 0.01600113686898882]
	TIME [epoch: 6.42 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012056089524729216		[learning rate: 1.6596e-05]
	Learning Rate: 1.65959e-05
	LOSS [training: 0.012056089524729216 | validation: 0.014974064079202681]
	TIME [epoch: 6.4 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012147234647887178		[learning rate: 1.6479e-05]
	Learning Rate: 1.64787e-05
	LOSS [training: 0.012147234647887178 | validation: 0.014166530308799491]
	TIME [epoch: 6.4 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012271403243677747		[learning rate: 1.6362e-05]
	Learning Rate: 1.63624e-05
	LOSS [training: 0.012271403243677747 | validation: 0.014244502020701127]
	TIME [epoch: 6.4 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011900984554386429		[learning rate: 1.6247e-05]
	Learning Rate: 1.62469e-05
	LOSS [training: 0.011900984554386429 | validation: 0.014726193068038768]
	TIME [epoch: 6.39 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012891991510634805		[learning rate: 1.6132e-05]
	Learning Rate: 1.61322e-05
	LOSS [training: 0.012891991510634805 | validation: 0.013301359754936754]
	TIME [epoch: 6.43 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012861807753408654		[learning rate: 1.6018e-05]
	Learning Rate: 1.60183e-05
	LOSS [training: 0.012861807753408654 | validation: 0.014230547772738989]
	TIME [epoch: 6.41 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01190509964722862		[learning rate: 1.5905e-05]
	Learning Rate: 1.59052e-05
	LOSS [training: 0.01190509964722862 | validation: 0.015466169882447612]
	TIME [epoch: 6.4 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012171871092558007		[learning rate: 1.5793e-05]
	Learning Rate: 1.57929e-05
	LOSS [training: 0.012171871092558007 | validation: 0.013172899402196382]
	TIME [epoch: 6.41 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011607784195804691		[learning rate: 1.5681e-05]
	Learning Rate: 1.56814e-05
	LOSS [training: 0.011607784195804691 | validation: 0.014141579125029653]
	TIME [epoch: 6.4 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011683181421544522		[learning rate: 1.5571e-05]
	Learning Rate: 1.55707e-05
	LOSS [training: 0.011683181421544522 | validation: 0.013686844648199803]
	TIME [epoch: 6.4 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012114683317972642		[learning rate: 1.5461e-05]
	Learning Rate: 1.54608e-05
	LOSS [training: 0.012114683317972642 | validation: 0.012608460739956685]
	TIME [epoch: 6.41 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.013091121038129719		[learning rate: 1.5352e-05]
	Learning Rate: 1.53516e-05
	LOSS [training: 0.013091121038129719 | validation: 0.014860126657853124]
	TIME [epoch: 6.44 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011910525138185885		[learning rate: 1.5243e-05]
	Learning Rate: 1.52432e-05
	LOSS [training: 0.011910525138185885 | validation: 0.01444981352379926]
	TIME [epoch: 6.41 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012424175848632727		[learning rate: 1.5136e-05]
	Learning Rate: 1.51356e-05
	LOSS [training: 0.012424175848632727 | validation: 0.016046964936897234]
	TIME [epoch: 6.41 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012207336008964502		[learning rate: 1.5029e-05]
	Learning Rate: 1.50288e-05
	LOSS [training: 0.012207336008964502 | validation: 0.015329420222373281]
	TIME [epoch: 6.4 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01183038724667377		[learning rate: 1.4923e-05]
	Learning Rate: 1.49227e-05
	LOSS [training: 0.01183038724667377 | validation: 0.01411211482456684]
	TIME [epoch: 6.4 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01163623856047082		[learning rate: 1.4817e-05]
	Learning Rate: 1.48173e-05
	LOSS [training: 0.01163623856047082 | validation: 0.014662050087660673]
	TIME [epoch: 6.4 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012197376541023505		[learning rate: 1.4713e-05]
	Learning Rate: 1.47127e-05
	LOSS [training: 0.012197376541023505 | validation: 0.014390810504682784]
	TIME [epoch: 6.43 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011577177886868087		[learning rate: 1.4609e-05]
	Learning Rate: 1.46088e-05
	LOSS [training: 0.011577177886868087 | validation: 0.015118237403986701]
	TIME [epoch: 6.41 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012007074584054749		[learning rate: 1.4506e-05]
	Learning Rate: 1.45057e-05
	LOSS [training: 0.012007074584054749 | validation: 0.014905167736430653]
	TIME [epoch: 6.39 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012110081113501345		[learning rate: 1.4403e-05]
	Learning Rate: 1.44033e-05
	LOSS [training: 0.012110081113501345 | validation: 0.015288740940961172]
	TIME [epoch: 6.4 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01187070590240193		[learning rate: 1.4302e-05]
	Learning Rate: 1.43016e-05
	LOSS [training: 0.01187070590240193 | validation: 0.01445675149951532]
	TIME [epoch: 6.39 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011842658830853205		[learning rate: 1.4201e-05]
	Learning Rate: 1.42006e-05
	LOSS [training: 0.011842658830853205 | validation: 0.014175449556720655]
	TIME [epoch: 6.4 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0123354959646854		[learning rate: 1.41e-05]
	Learning Rate: 1.41004e-05
	LOSS [training: 0.0123354959646854 | validation: 0.01641223564762496]
	TIME [epoch: 6.42 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012267352989465114		[learning rate: 1.4001e-05]
	Learning Rate: 1.40008e-05
	LOSS [training: 0.012267352989465114 | validation: 0.015037706070384745]
	TIME [epoch: 6.43 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011529842945779425		[learning rate: 1.3902e-05]
	Learning Rate: 1.3902e-05
	LOSS [training: 0.011529842945779425 | validation: 0.013977182767975395]
	TIME [epoch: 6.4 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012526885793022564		[learning rate: 1.3804e-05]
	Learning Rate: 1.38038e-05
	LOSS [training: 0.012526885793022564 | validation: 0.014890655186007424]
	TIME [epoch: 6.39 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011775934160256313		[learning rate: 1.3706e-05]
	Learning Rate: 1.37064e-05
	LOSS [training: 0.011775934160256313 | validation: 0.015980043841942928]
	TIME [epoch: 6.39 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012086195703824801		[learning rate: 1.361e-05]
	Learning Rate: 1.36096e-05
	LOSS [training: 0.012086195703824801 | validation: 0.014468428601338307]
	TIME [epoch: 6.4 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01128362187474704		[learning rate: 1.3514e-05]
	Learning Rate: 1.35135e-05
	LOSS [training: 0.01128362187474704 | validation: 0.014010728419447772]
	TIME [epoch: 6.42 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012536876530176119		[learning rate: 1.3418e-05]
	Learning Rate: 1.34181e-05
	LOSS [training: 0.012536876530176119 | validation: 0.015043842484120879]
	TIME [epoch: 6.43 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012378213038962822		[learning rate: 1.3323e-05]
	Learning Rate: 1.33234e-05
	LOSS [training: 0.012378213038962822 | validation: 0.013690916906890827]
	TIME [epoch: 6.4 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01256477743323829		[learning rate: 1.3229e-05]
	Learning Rate: 1.32294e-05
	LOSS [training: 0.01256477743323829 | validation: 0.014393586831684952]
	TIME [epoch: 6.4 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011721336292558695		[learning rate: 1.3136e-05]
	Learning Rate: 1.3136e-05
	LOSS [training: 0.011721336292558695 | validation: 0.013475083914539466]
	TIME [epoch: 6.4 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012380185910230507		[learning rate: 1.3043e-05]
	Learning Rate: 1.30432e-05
	LOSS [training: 0.012380185910230507 | validation: 0.014040326479165845]
	TIME [epoch: 6.41 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01251976353856881		[learning rate: 1.2951e-05]
	Learning Rate: 1.29511e-05
	LOSS [training: 0.01251976353856881 | validation: 0.014747818870562852]
	TIME [epoch: 6.41 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012500431720477934		[learning rate: 1.286e-05]
	Learning Rate: 1.28597e-05
	LOSS [training: 0.012500431720477934 | validation: 0.014170855532910549]
	TIME [epoch: 6.43 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012243865634119173		[learning rate: 1.2769e-05]
	Learning Rate: 1.27689e-05
	LOSS [training: 0.012243865634119173 | validation: 0.014098050718390953]
	TIME [epoch: 6.4 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011498220521332566		[learning rate: 1.2679e-05]
	Learning Rate: 1.26788e-05
	LOSS [training: 0.011498220521332566 | validation: 0.013957728654459206]
	TIME [epoch: 6.41 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012325149986473828		[learning rate: 1.2589e-05]
	Learning Rate: 1.25893e-05
	LOSS [training: 0.012325149986473828 | validation: 0.012376418235413578]
	TIME [epoch: 6.4 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012577502403723212		[learning rate: 1.25e-05]
	Learning Rate: 1.25004e-05
	LOSS [training: 0.012577502403723212 | validation: 0.013488647619531136]
	TIME [epoch: 6.4 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012857279852815977		[learning rate: 1.2412e-05]
	Learning Rate: 1.24121e-05
	LOSS [training: 0.012857279852815977 | validation: 0.014029461342204364]
	TIME [epoch: 6.4 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012930794023338285		[learning rate: 1.2325e-05]
	Learning Rate: 1.23245e-05
	LOSS [training: 0.012930794023338285 | validation: 0.01367576867926188]
	TIME [epoch: 6.44 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011425587112082854		[learning rate: 1.2237e-05]
	Learning Rate: 1.22375e-05
	LOSS [training: 0.011425587112082854 | validation: 0.014492846595374281]
	TIME [epoch: 6.41 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012226245961144145		[learning rate: 1.2151e-05]
	Learning Rate: 1.21511e-05
	LOSS [training: 0.012226245961144145 | validation: 0.013397931002645282]
	TIME [epoch: 6.4 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011685118961946593		[learning rate: 1.2065e-05]
	Learning Rate: 1.20653e-05
	LOSS [training: 0.011685118961946593 | validation: 0.013844705638631516]
	TIME [epoch: 6.41 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011418169546576623		[learning rate: 1.198e-05]
	Learning Rate: 1.19801e-05
	LOSS [training: 0.011418169546576623 | validation: 0.013889008863995562]
	TIME [epoch: 6.4 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011918543424429643		[learning rate: 1.1896e-05]
	Learning Rate: 1.18956e-05
	LOSS [training: 0.011918543424429643 | validation: 0.014964773375896046]
	TIME [epoch: 6.4 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012904708053855402		[learning rate: 1.1812e-05]
	Learning Rate: 1.18116e-05
	LOSS [training: 0.012904708053855402 | validation: 0.013923689186054146]
	TIME [epoch: 6.41 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012667395717360003		[learning rate: 1.1728e-05]
	Learning Rate: 1.17282e-05
	LOSS [training: 0.012667395717360003 | validation: 0.015507273715355863]
	TIME [epoch: 6.42 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012360188573286655		[learning rate: 1.1645e-05]
	Learning Rate: 1.16454e-05
	LOSS [training: 0.012360188573286655 | validation: 0.014789091850170716]
	TIME [epoch: 6.4 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012331539745737117		[learning rate: 1.1563e-05]
	Learning Rate: 1.15632e-05
	LOSS [training: 0.012331539745737117 | validation: 0.014420337968037811]
	TIME [epoch: 6.4 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0120293307302187		[learning rate: 1.1482e-05]
	Learning Rate: 1.14815e-05
	LOSS [training: 0.0120293307302187 | validation: 0.015625641796982066]
	TIME [epoch: 6.4 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011936766677626337		[learning rate: 1.14e-05]
	Learning Rate: 1.14005e-05
	LOSS [training: 0.011936766677626337 | validation: 0.011358008864017994]
	TIME [epoch: 6.39 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_982.pth
	Model improved!!!
EPOCH 983/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012570100393158559		[learning rate: 1.132e-05]
	Learning Rate: 1.132e-05
	LOSS [training: 0.012570100393158559 | validation: 0.013754446191956843]
	TIME [epoch: 6.42 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012406454665031654		[learning rate: 1.124e-05]
	Learning Rate: 1.12401e-05
	LOSS [training: 0.012406454665031654 | validation: 0.013385253328092352]
	TIME [epoch: 6.43 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012501197892540065		[learning rate: 1.1161e-05]
	Learning Rate: 1.11607e-05
	LOSS [training: 0.012501197892540065 | validation: 0.015399232280492265]
	TIME [epoch: 6.4 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011962189459677305		[learning rate: 1.1082e-05]
	Learning Rate: 1.10819e-05
	LOSS [training: 0.011962189459677305 | validation: 0.01404044666697693]
	TIME [epoch: 6.4 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012143418979878428		[learning rate: 1.1004e-05]
	Learning Rate: 1.10037e-05
	LOSS [training: 0.012143418979878428 | validation: 0.014206964862350952]
	TIME [epoch: 6.4 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011961406108025303		[learning rate: 1.0926e-05]
	Learning Rate: 1.0926e-05
	LOSS [training: 0.011961406108025303 | validation: 0.013344051117764395]
	TIME [epoch: 6.4 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01220297169737585		[learning rate: 1.0849e-05]
	Learning Rate: 1.08489e-05
	LOSS [training: 0.01220297169737585 | validation: 0.015137819213660654]
	TIME [epoch: 6.42 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011950284183627237		[learning rate: 1.0772e-05]
	Learning Rate: 1.07723e-05
	LOSS [training: 0.011950284183627237 | validation: 0.014318262781535597]
	TIME [epoch: 6.43 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011902745569372885		[learning rate: 1.0696e-05]
	Learning Rate: 1.06962e-05
	LOSS [training: 0.011902745569372885 | validation: 0.013997060800653695]
	TIME [epoch: 6.41 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011860258451213445		[learning rate: 1.0621e-05]
	Learning Rate: 1.06207e-05
	LOSS [training: 0.011860258451213445 | validation: 0.013288515324610218]
	TIME [epoch: 6.4 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011676210900794038		[learning rate: 1.0546e-05]
	Learning Rate: 1.05457e-05
	LOSS [training: 0.011676210900794038 | validation: 0.015369581922668641]
	TIME [epoch: 6.41 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012730947599626827		[learning rate: 1.0471e-05]
	Learning Rate: 1.04713e-05
	LOSS [training: 0.012730947599626827 | validation: 0.014576437301123108]
	TIME [epoch: 6.39 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011296989009884299		[learning rate: 1.0397e-05]
	Learning Rate: 1.03974e-05
	LOSS [training: 0.011296989009884299 | validation: 0.01299130102326272]
	TIME [epoch: 6.41 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011817954511472889		[learning rate: 1.0324e-05]
	Learning Rate: 1.0324e-05
	LOSS [training: 0.011817954511472889 | validation: 0.014684374345556833]
	TIME [epoch: 6.44 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012509558043587207		[learning rate: 1.0251e-05]
	Learning Rate: 1.02511e-05
	LOSS [training: 0.012509558043587207 | validation: 0.013282955983423678]
	TIME [epoch: 6.41 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011710483315134659		[learning rate: 1.0179e-05]
	Learning Rate: 1.01787e-05
	LOSS [training: 0.011710483315134659 | validation: 0.012761546857610218]
	TIME [epoch: 6.4 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011305281555079197		[learning rate: 1.0107e-05]
	Learning Rate: 1.01068e-05
	LOSS [training: 0.011305281555079197 | validation: 0.014477282918840463]
	TIME [epoch: 6.4 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01231156873820724		[learning rate: 1.0035e-05]
	Learning Rate: 1.00355e-05
	LOSS [training: 0.01231156873820724 | validation: 0.013429928749274796]
	TIME [epoch: 6.4 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012220116651339643		[learning rate: 9.9646e-06]
	Learning Rate: 9.96464e-06
	LOSS [training: 0.012220116651339643 | validation: 0.013764643501501118]
	TIME [epoch: 113 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01219657073559201		[learning rate: 9.8943e-06]
	Learning Rate: 9.89429e-06
	LOSS [training: 0.01219657073559201 | validation: 0.014297764133545632]
	TIME [epoch: 14 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012098319276074077		[learning rate: 9.8244e-06]
	Learning Rate: 9.82444e-06
	LOSS [training: 0.012098319276074077 | validation: 0.014814265261394783]
	TIME [epoch: 14 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012244123774881412		[learning rate: 9.7551e-06]
	Learning Rate: 9.75508e-06
	LOSS [training: 0.012244123774881412 | validation: 0.014068172536833096]
	TIME [epoch: 14 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011980315861832722		[learning rate: 9.6862e-06]
	Learning Rate: 9.68621e-06
	LOSS [training: 0.011980315861832722 | validation: 0.01551932866330113]
	TIME [epoch: 14 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012454410317977054		[learning rate: 9.6178e-06]
	Learning Rate: 9.61783e-06
	LOSS [training: 0.012454410317977054 | validation: 0.014411956846422231]
	TIME [epoch: 14 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012420921273851382		[learning rate: 9.5499e-06]
	Learning Rate: 9.54993e-06
	LOSS [training: 0.012420921273851382 | validation: 0.014756358836083406]
	TIME [epoch: 14 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011589729333840034		[learning rate: 9.4825e-06]
	Learning Rate: 9.48251e-06
	LOSS [training: 0.011589729333840034 | validation: 0.014583890085998275]
	TIME [epoch: 14 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012505590368428396		[learning rate: 9.4156e-06]
	Learning Rate: 9.41556e-06
	LOSS [training: 0.012505590368428396 | validation: 0.013769145617470658]
	TIME [epoch: 13.9 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01162169299832293		[learning rate: 9.3491e-06]
	Learning Rate: 9.34909e-06
	LOSS [training: 0.01162169299832293 | validation: 0.014821061245698203]
	TIME [epoch: 14 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012153893253044515		[learning rate: 9.2831e-06]
	Learning Rate: 9.28308e-06
	LOSS [training: 0.012153893253044515 | validation: 0.014314007601621798]
	TIME [epoch: 14 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01191685623420961		[learning rate: 9.2176e-06]
	Learning Rate: 9.21755e-06
	LOSS [training: 0.01191685623420961 | validation: 0.013415735344824582]
	TIME [epoch: 14 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012144430634814195		[learning rate: 9.1525e-06]
	Learning Rate: 9.15248e-06
	LOSS [training: 0.012144430634814195 | validation: 0.012301111458460402]
	TIME [epoch: 14 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011710628917701568		[learning rate: 9.0879e-06]
	Learning Rate: 9.08786e-06
	LOSS [training: 0.011710628917701568 | validation: 0.013745784217765178]
	TIME [epoch: 14 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01207704568882149		[learning rate: 9.0237e-06]
	Learning Rate: 9.0237e-06
	LOSS [training: 0.01207704568882149 | validation: 0.01414127710221968]
	TIME [epoch: 14 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012093563989605122		[learning rate: 8.96e-06]
	Learning Rate: 8.96e-06
	LOSS [training: 0.012093563989605122 | validation: 0.013907073118272351]
	TIME [epoch: 14 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011811317967256254		[learning rate: 8.8967e-06]
	Learning Rate: 8.89674e-06
	LOSS [training: 0.011811317967256254 | validation: 0.01351218584922705]
	TIME [epoch: 14 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011509245572202363		[learning rate: 8.8339e-06]
	Learning Rate: 8.83393e-06
	LOSS [training: 0.011509245572202363 | validation: 0.013569415498873805]
	TIME [epoch: 14 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01219613632599422		[learning rate: 8.7716e-06]
	Learning Rate: 8.77157e-06
	LOSS [training: 0.01219613632599422 | validation: 0.013619280092436188]
	TIME [epoch: 14 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011386745116355648		[learning rate: 8.7096e-06]
	Learning Rate: 8.70964e-06
	LOSS [training: 0.011386745116355648 | validation: 0.015287342679825245]
	TIME [epoch: 14 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011600627164408024		[learning rate: 8.6481e-06]
	Learning Rate: 8.64815e-06
	LOSS [training: 0.011600627164408024 | validation: 0.013411548057191209]
	TIME [epoch: 14 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011899027175608723		[learning rate: 8.5871e-06]
	Learning Rate: 8.5871e-06
	LOSS [training: 0.011899027175608723 | validation: 0.013581370847336127]
	TIME [epoch: 14 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01185707430290326		[learning rate: 8.5265e-06]
	Learning Rate: 8.52647e-06
	LOSS [training: 0.01185707430290326 | validation: 0.012633154252454804]
	TIME [epoch: 14 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012275516831402402		[learning rate: 8.4663e-06]
	Learning Rate: 8.46627e-06
	LOSS [training: 0.012275516831402402 | validation: 0.014571023259266336]
	TIME [epoch: 14 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011547081758683716		[learning rate: 8.4065e-06]
	Learning Rate: 8.40651e-06
	LOSS [training: 0.011547081758683716 | validation: 0.013194401530335333]
	TIME [epoch: 14 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011867835619140808		[learning rate: 8.3472e-06]
	Learning Rate: 8.34716e-06
	LOSS [training: 0.011867835619140808 | validation: 0.01392663755525097]
	TIME [epoch: 14 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011768506904582303		[learning rate: 8.2882e-06]
	Learning Rate: 8.28823e-06
	LOSS [training: 0.011768506904582303 | validation: 0.014057854748448935]
	TIME [epoch: 14 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011406929259215909		[learning rate: 8.2297e-06]
	Learning Rate: 8.22972e-06
	LOSS [training: 0.011406929259215909 | validation: 0.015267413073053855]
	TIME [epoch: 14 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01218817234759014		[learning rate: 8.1716e-06]
	Learning Rate: 8.17162e-06
	LOSS [training: 0.01218817234759014 | validation: 0.013275063709347208]
	TIME [epoch: 14 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.010886944173913352		[learning rate: 8.1139e-06]
	Learning Rate: 8.11392e-06
	LOSS [training: 0.010886944173913352 | validation: 0.014055653514888633]
	TIME [epoch: 14 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012491622501261644		[learning rate: 8.0566e-06]
	Learning Rate: 8.05664e-06
	LOSS [training: 0.012491622501261644 | validation: 0.01640036922773886]
	TIME [epoch: 14 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011390688178761293		[learning rate: 7.9998e-06]
	Learning Rate: 7.99976e-06
	LOSS [training: 0.011390688178761293 | validation: 0.01207014660266554]
	TIME [epoch: 14 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.010976591169499367		[learning rate: 7.9433e-06]
	Learning Rate: 7.94328e-06
	LOSS [training: 0.010976591169499367 | validation: 0.016175266893840298]
	TIME [epoch: 14 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011766702816768232		[learning rate: 7.8872e-06]
	Learning Rate: 7.8872e-06
	LOSS [training: 0.011766702816768232 | validation: 0.012908007173538422]
	TIME [epoch: 14 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01179141040258828		[learning rate: 7.8315e-06]
	Learning Rate: 7.83153e-06
	LOSS [training: 0.01179141040258828 | validation: 0.013544465941250356]
	TIME [epoch: 14 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011762773122137048		[learning rate: 7.7762e-06]
	Learning Rate: 7.77624e-06
	LOSS [training: 0.011762773122137048 | validation: 0.013536839321813038]
	TIME [epoch: 14 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011954091357463125		[learning rate: 7.7213e-06]
	Learning Rate: 7.72133e-06
	LOSS [training: 0.011954091357463125 | validation: 0.013083231135896562]
	TIME [epoch: 14 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012215075645297783		[learning rate: 7.6668e-06]
	Learning Rate: 7.66683e-06
	LOSS [training: 0.012215075645297783 | validation: 0.012755792913591912]
	TIME [epoch: 14 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011941485702055797		[learning rate: 7.6127e-06]
	Learning Rate: 7.6127e-06
	LOSS [training: 0.011941485702055797 | validation: 0.013821172241648007]
	TIME [epoch: 14 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011959352929400046		[learning rate: 7.559e-06]
	Learning Rate: 7.55895e-06
	LOSS [training: 0.011959352929400046 | validation: 0.015492086129942058]
	TIME [epoch: 14 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011895942772694776		[learning rate: 7.5056e-06]
	Learning Rate: 7.50559e-06
	LOSS [training: 0.011895942772694776 | validation: 0.013662212983437028]
	TIME [epoch: 14 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011823754941707891		[learning rate: 7.4526e-06]
	Learning Rate: 7.4526e-06
	LOSS [training: 0.011823754941707891 | validation: 0.013764229682080943]
	TIME [epoch: 14 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011271106275387986		[learning rate: 7.4e-06]
	Learning Rate: 7.39998e-06
	LOSS [training: 0.011271106275387986 | validation: 0.014500358660875979]
	TIME [epoch: 14 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012110539107033389		[learning rate: 7.3477e-06]
	Learning Rate: 7.34774e-06
	LOSS [training: 0.012110539107033389 | validation: 0.013302335963111145]
	TIME [epoch: 14 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012374095679837806		[learning rate: 7.2959e-06]
	Learning Rate: 7.29587e-06
	LOSS [training: 0.012374095679837806 | validation: 0.014537058503412276]
	TIME [epoch: 14 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011710247841113755		[learning rate: 7.2444e-06]
	Learning Rate: 7.24436e-06
	LOSS [training: 0.011710247841113755 | validation: 0.014641447856128705]
	TIME [epoch: 14 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01129738720475869		[learning rate: 7.1932e-06]
	Learning Rate: 7.19322e-06
	LOSS [training: 0.01129738720475869 | validation: 0.014753128539488958]
	TIME [epoch: 14 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011806257108605249		[learning rate: 7.1424e-06]
	Learning Rate: 7.14244e-06
	LOSS [training: 0.011806257108605249 | validation: 0.012940784138822265]
	TIME [epoch: 14 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011978189306813616		[learning rate: 7.092e-06]
	Learning Rate: 7.09201e-06
	LOSS [training: 0.011978189306813616 | validation: 0.01369211702036895]
	TIME [epoch: 14 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012662660151537716		[learning rate: 7.0419e-06]
	Learning Rate: 7.04194e-06
	LOSS [training: 0.012662660151537716 | validation: 0.014819485084046236]
	TIME [epoch: 14 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01177574836649968		[learning rate: 6.9922e-06]
	Learning Rate: 6.99222e-06
	LOSS [training: 0.01177574836649968 | validation: 0.014026260598583301]
	TIME [epoch: 14 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011762277521010802		[learning rate: 6.9429e-06]
	Learning Rate: 6.94286e-06
	LOSS [training: 0.011762277521010802 | validation: 0.013501990980594675]
	TIME [epoch: 14 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011638065753722646		[learning rate: 6.8938e-06]
	Learning Rate: 6.89385e-06
	LOSS [training: 0.011638065753722646 | validation: 0.011426618503001085]
	TIME [epoch: 14 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011874281383172341		[learning rate: 6.8452e-06]
	Learning Rate: 6.84518e-06
	LOSS [training: 0.011874281383172341 | validation: 0.01327713966806393]
	TIME [epoch: 14 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012283379346119377		[learning rate: 6.7969e-06]
	Learning Rate: 6.79685e-06
	LOSS [training: 0.012283379346119377 | validation: 0.015936402913916015]
	TIME [epoch: 14 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011932030178520172		[learning rate: 6.7489e-06]
	Learning Rate: 6.74887e-06
	LOSS [training: 0.011932030178520172 | validation: 0.012656745477717758]
	TIME [epoch: 14 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012059522825875812		[learning rate: 6.7012e-06]
	Learning Rate: 6.70122e-06
	LOSS [training: 0.012059522825875812 | validation: 0.013426521487296056]
	TIME [epoch: 14 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011417517968185208		[learning rate: 6.6539e-06]
	Learning Rate: 6.65391e-06
	LOSS [training: 0.011417517968185208 | validation: 0.013605859916111008]
	TIME [epoch: 14 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012400071856391229		[learning rate: 6.6069e-06]
	Learning Rate: 6.60694e-06
	LOSS [training: 0.012400071856391229 | validation: 0.0152948591435361]
	TIME [epoch: 14 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011843964898027469		[learning rate: 6.5603e-06]
	Learning Rate: 6.56029e-06
	LOSS [training: 0.011843964898027469 | validation: 0.013141101422842805]
	TIME [epoch: 14 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0118471244987664		[learning rate: 6.514e-06]
	Learning Rate: 6.51398e-06
	LOSS [training: 0.0118471244987664 | validation: 0.014449547581543765]
	TIME [epoch: 14 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011222373467596625		[learning rate: 6.468e-06]
	Learning Rate: 6.46799e-06
	LOSS [training: 0.011222373467596625 | validation: 0.013441518256028497]
	TIME [epoch: 14 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011266304491246257		[learning rate: 6.4223e-06]
	Learning Rate: 6.42233e-06
	LOSS [training: 0.011266304491246257 | validation: 0.015296318695033851]
	TIME [epoch: 14 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01141878041642252		[learning rate: 6.377e-06]
	Learning Rate: 6.37698e-06
	LOSS [training: 0.01141878041642252 | validation: 0.01434178015804581]
	TIME [epoch: 14 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01153584458064001		[learning rate: 6.332e-06]
	Learning Rate: 6.33197e-06
	LOSS [training: 0.01153584458064001 | validation: 0.013806916626245704]
	TIME [epoch: 14 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01172158799900057		[learning rate: 6.2873e-06]
	Learning Rate: 6.28726e-06
	LOSS [training: 0.01172158799900057 | validation: 0.012886491502933773]
	TIME [epoch: 14 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011645733221874344		[learning rate: 6.2429e-06]
	Learning Rate: 6.24288e-06
	LOSS [training: 0.011645733221874344 | validation: 0.014037053686416667]
	TIME [epoch: 14 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011844567892562474		[learning rate: 6.1988e-06]
	Learning Rate: 6.1988e-06
	LOSS [training: 0.011844567892562474 | validation: 0.014649270733286403]
	TIME [epoch: 13.9 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01199853325652366		[learning rate: 6.155e-06]
	Learning Rate: 6.15504e-06
	LOSS [training: 0.01199853325652366 | validation: 0.014870267971605861]
	TIME [epoch: 14 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.010983885956280266		[learning rate: 6.1116e-06]
	Learning Rate: 6.11159e-06
	LOSS [training: 0.010983885956280266 | validation: 0.013020407206934093]
	TIME [epoch: 14 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01254476443619923		[learning rate: 6.0684e-06]
	Learning Rate: 6.06844e-06
	LOSS [training: 0.01254476443619923 | validation: 0.013126834961559398]
	TIME [epoch: 14 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01188806164096173		[learning rate: 6.0256e-06]
	Learning Rate: 6.0256e-06
	LOSS [training: 0.01188806164096173 | validation: 0.01316087643372701]
	TIME [epoch: 14 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0118436836241641		[learning rate: 5.9831e-06]
	Learning Rate: 5.98306e-06
	LOSS [training: 0.0118436836241641 | validation: 0.013876106145809265]
	TIME [epoch: 14 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011140274350340377		[learning rate: 5.9408e-06]
	Learning Rate: 5.94082e-06
	LOSS [training: 0.011140274350340377 | validation: 0.01322866902022582]
	TIME [epoch: 14 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011440914482389535		[learning rate: 5.8989e-06]
	Learning Rate: 5.89888e-06
	LOSS [training: 0.011440914482389535 | validation: 0.01549604038739899]
	TIME [epoch: 14 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012067011432567658		[learning rate: 5.8572e-06]
	Learning Rate: 5.85723e-06
	LOSS [training: 0.012067011432567658 | validation: 0.014202538761353934]
	TIME [epoch: 14 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011131275621240163		[learning rate: 5.8159e-06]
	Learning Rate: 5.81588e-06
	LOSS [training: 0.011131275621240163 | validation: 0.013774221883015343]
	TIME [epoch: 14 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011605852338128227		[learning rate: 5.7748e-06]
	Learning Rate: 5.77482e-06
	LOSS [training: 0.011605852338128227 | validation: 0.01413159720952219]
	TIME [epoch: 14 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012216256642111584		[learning rate: 5.7341e-06]
	Learning Rate: 5.73405e-06
	LOSS [training: 0.012216256642111584 | validation: 0.012241647536199569]
	TIME [epoch: 14 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011602765451763131		[learning rate: 5.6936e-06]
	Learning Rate: 5.69357e-06
	LOSS [training: 0.011602765451763131 | validation: 0.013755993542227618]
	TIME [epoch: 14 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012072609822108567		[learning rate: 5.6534e-06]
	Learning Rate: 5.65337e-06
	LOSS [training: 0.012072609822108567 | validation: 0.012628115931771084]
	TIME [epoch: 14 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.012670677700662745		[learning rate: 5.6135e-06]
	Learning Rate: 5.61346e-06
	LOSS [training: 0.012670677700662745 | validation: 0.014001823470828229]
	TIME [epoch: 14 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.011693164647841509		[learning rate: 5.5738e-06]
	Learning Rate: 5.57383e-06
	LOSS [training: 0.011693164647841509 | validation: 0.014596314058423787]
	TIME [epoch: 14 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194028/states/model_phi1_2a_v_mmd1_1083.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 6215.257 seconds.
