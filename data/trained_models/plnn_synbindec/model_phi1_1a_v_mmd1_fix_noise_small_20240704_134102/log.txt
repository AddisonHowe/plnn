Args:
Namespace(name='model_phi1_1a_v_mmd1_fix_noise_small', outdir='out/model_training/model_phi1_1a_v_mmd1_fix_noise_small', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 526865431

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.665837965359751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.665837965359751 | validation: 5.851698726792373]
	TIME [epoch: 165 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.030304776060772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.030304776060772 | validation: 5.2180366345045055]
	TIME [epoch: 7.49 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.997257731061623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.997257731061623 | validation: 4.5226300539476085]
	TIME [epoch: 7.44 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.430569560426593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.430569560426593 | validation: 4.607127296709226]
	TIME [epoch: 7.46 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.212823167241583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.212823167241583 | validation: 4.158168024333172]
	TIME [epoch: 7.44 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.061784948113917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.061784948113917 | validation: 3.9957242683110805]
	TIME [epoch: 7.46 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9167370736299745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9167370736299745 | validation: 3.844834342802345]
	TIME [epoch: 7.44 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.100248510261314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.100248510261314 | validation: 3.8638104510036335]
	TIME [epoch: 7.47 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.717313337510145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.717313337510145 | validation: 3.7708926213505354]
	TIME [epoch: 7.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.642891940069819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.642891940069819 | validation: 3.4966434841514875]
	TIME [epoch: 7.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5927523745492356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5927523745492356 | validation: 3.563103886250757]
	TIME [epoch: 7.47 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.70653989301261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.70653989301261 | validation: 3.483550536786938]
	TIME [epoch: 7.47 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.340167933868957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.340167933868957 | validation: 3.278431602966476]
	TIME [epoch: 7.47 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.295045541163503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.295045541163503 | validation: 3.232222426355815]
	TIME [epoch: 7.52 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.393301914805271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.393301914805271 | validation: 3.17782032705639]
	TIME [epoch: 7.45 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1824231842283526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1824231842283526 | validation: 3.480786105332913]
	TIME [epoch: 7.46 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.244466115870953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.244466115870953 | validation: 3.6718774098884284]
	TIME [epoch: 7.44 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2277882396737274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2277882396737274 | validation: 4.134613350281862]
	TIME [epoch: 7.46 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8171022314924308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8171022314924308 | validation: 3.172575250772714]
	TIME [epoch: 7.49 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2358234487108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2358234487108 | validation: 2.89146975871928]
	TIME [epoch: 7.45 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.09460587141969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.09460587141969 | validation: 2.9608333913648215]
	TIME [epoch: 7.46 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.956048521544688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.956048521544688 | validation: 2.768948788878279]
	TIME [epoch: 7.46 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8399243117234207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8399243117234207 | validation: 2.7765519432712407]
	TIME [epoch: 7.48 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4156402367965972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4156402367965972 | validation: 4.0159283145929265]
	TIME [epoch: 7.52 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.185383244939063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.185383244939063 | validation: 3.072117444248391]
	TIME [epoch: 7.48 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9057618143001127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9057618143001127 | validation: 3.7830703960744554]
	TIME [epoch: 7.49 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.473395595249146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.473395595249146 | validation: 2.6949132665501505]
	TIME [epoch: 7.48 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.114911711304866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.114911711304866 | validation: 2.7576453768735587]
	TIME [epoch: 7.48 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.881294293635337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.881294293635337 | validation: 2.723565881690099]
	TIME [epoch: 7.5 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8177512901878607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8177512901878607 | validation: 2.7821961972384712]
	TIME [epoch: 7.46 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.860522166183057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.860522166183057 | validation: 2.525557869544589]
	TIME [epoch: 7.47 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7077213837045893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7077213837045893 | validation: 3.5796446218527835]
	TIME [epoch: 7.48 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.281264378008905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.281264378008905 | validation: 3.6063724985055665]
	TIME [epoch: 7.47 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1299270639669996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1299270639669996 | validation: 2.601599655170907]
	TIME [epoch: 7.53 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5272437096754112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5272437096754112 | validation: 3.377505133173587]
	TIME [epoch: 7.48 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7070739064686227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7070739064686227 | validation: 2.0677441058300765]
	TIME [epoch: 7.47 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5480757430130905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5480757430130905 | validation: 3.75651603917965]
	TIME [epoch: 7.48 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1191902249616272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1191902249616272 | validation: 2.472230421968005]
	TIME [epoch: 7.49 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2734287130981574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2734287130981574 | validation: 3.8845824417772525]
	TIME [epoch: 7.5 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9511328529808587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9511328529808587 | validation: 1.8363229156625365]
	TIME [epoch: 7.47 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.15230136559319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.15230136559319 | validation: 2.4653861242619275]
	TIME [epoch: 7.48 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0231413304935724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0231413304935724 | validation: 3.313603607521245]
	TIME [epoch: 7.46 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.560560915136584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.560560915136584 | validation: 2.0932459972854547]
	TIME [epoch: 7.47 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4012836575928145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4012836575928145 | validation: 2.4869176464072336]
	TIME [epoch: 7.54 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8691921601837955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8691921601837955 | validation: 3.539043517283077]
	TIME [epoch: 7.47 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.395421121365898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.395421121365898 | validation: 1.1160569938363238]
	TIME [epoch: 7.49 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.407063641798838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.407063641798838 | validation: 1.6315920459322633]
	TIME [epoch: 7.45 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6615216198341527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6615216198341527 | validation: 1.8932466108672399]
	TIME [epoch: 7.41 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7738694840379745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7738694840379745 | validation: 0.9387050129552761]
	TIME [epoch: 7.46 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2502503080647451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2502503080647451 | validation: 0.9364206018877446]
	TIME [epoch: 7.45 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.328141165474101		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 1.328141165474101 | validation: 0.9701876191448142]
	TIME [epoch: 7.45 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.082051393800422		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 1.082051393800422 | validation: 1.2543404901049855]
	TIME [epoch: 7.45 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1121680357666914		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 1.1121680357666914 | validation: 1.0481019462638037]
	TIME [epoch: 7.45 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9377034494925863		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 0.9377034494925863 | validation: 1.4151293102328943]
	TIME [epoch: 7.49 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0884671730609472		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 1.0884671730609472 | validation: 0.8124265451220478]
	TIME [epoch: 7.44 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0202746695835834		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 1.0202746695835834 | validation: 0.859843954394582]
	TIME [epoch: 7.45 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9702007736192535		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 0.9702007736192535 | validation: 1.4453887485072223]
	TIME [epoch: 7.46 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0797634440912964		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 1.0797634440912964 | validation: 0.8093980340042027]
	TIME [epoch: 7.46 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8295044703815747		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 0.8295044703815747 | validation: 1.2043387040730766]
	TIME [epoch: 7.5 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9389249880258144		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 0.9389249880258144 | validation: 0.8360441383935763]
	TIME [epoch: 7.47 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.993017394166771		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 0.993017394166771 | validation: 0.8139594838859836]
	TIME [epoch: 7.46 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0574579649958495		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 1.0574579649958495 | validation: 0.689930551407466]
	TIME [epoch: 7.47 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9304872648539835		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 0.9304872648539835 | validation: 1.326765838102566]
	TIME [epoch: 7.48 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8311906539729814		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 0.8311906539729814 | validation: 1.1515771027285118]
	TIME [epoch: 7.51 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9137445778704361		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 0.9137445778704361 | validation: 0.8742796961011403]
	TIME [epoch: 7.46 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6666440486939021		[learning rate: 0.0094573]
	Learning Rate: 0.00945734
	LOSS [training: 0.6666440486939021 | validation: 0.9911986187391123]
	TIME [epoch: 7.47 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2702816615049957		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 1.2702816615049957 | validation: 1.1310627136239764]
	TIME [epoch: 7.46 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8030078285507228		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 0.8030078285507228 | validation: 0.6030120318840915]
	TIME [epoch: 7.47 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7591794165596656		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 0.7591794165596656 | validation: 0.5728619688962409]
	TIME [epoch: 7.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6418107790498624		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.6418107790498624 | validation: 0.4786578268908624]
	TIME [epoch: 7.51 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7594226080024271		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 0.7594226080024271 | validation: 0.6560593225087945]
	TIME [epoch: 7.43 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7530227641363453		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 0.7530227641363453 | validation: 1.1999489807924544]
	TIME [epoch: 7.43 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0493309116614213		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 1.0493309116614213 | validation: 0.6695960321577168]
	TIME [epoch: 7.43 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9389502665369849		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 0.9389502665369849 | validation: 0.6625097913722018]
	TIME [epoch: 7.48 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6362843235420718		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.6362843235420718 | validation: 0.4933751228440176]
	TIME [epoch: 7.44 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5531342185177245		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 0.5531342185177245 | validation: 0.9780535072400895]
	TIME [epoch: 7.43 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.045729174918478		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 1.045729174918478 | validation: 0.5005910518560276]
	TIME [epoch: 7.44 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5848814408143164		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.5848814408143164 | validation: 2.2124513869985085]
	TIME [epoch: 7.43 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0270062697187212		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 1.0270062697187212 | validation: 0.4930191923970173]
	TIME [epoch: 7.46 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5601678469611369		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.5601678469611369 | validation: 0.4363171076222161]
	TIME [epoch: 7.45 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9923301273900569		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.9923301273900569 | validation: 1.0430860536234945]
	TIME [epoch: 7.44 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8094019715188121		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.8094019715188121 | validation: 0.7077106212932607]
	TIME [epoch: 7.45 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9064253166451959		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.9064253166451959 | validation: 0.8973249212065231]
	TIME [epoch: 7.44 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7680475709300842		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.7680475709300842 | validation: 0.5988010501682091]
	TIME [epoch: 7.47 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5837114194765046		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.5837114194765046 | validation: 0.5416155404954666]
	TIME [epoch: 7.47 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6076465472104396		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.6076465472104396 | validation: 0.5056665871398919]
	TIME [epoch: 7.46 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6539744025042963		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.6539744025042963 | validation: 1.4941239855420452]
	TIME [epoch: 7.44 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9666631932744431		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.9666631932744431 | validation: 0.5237101734070189]
	TIME [epoch: 7.45 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.842305004425716		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.842305004425716 | validation: 0.4447212072362583]
	TIME [epoch: 7.46 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7815726285521503		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.7815726285521503 | validation: 0.8479142362361001]
	TIME [epoch: 7.48 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6526345516955899		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.6526345516955899 | validation: 0.9903051587237621]
	TIME [epoch: 7.44 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1744053307254516		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 1.1744053307254516 | validation: 1.0847783149747263]
	TIME [epoch: 7.45 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8417747941181892		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.8417747941181892 | validation: 0.521175528223168]
	TIME [epoch: 7.44 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6294378731979838		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.6294378731979838 | validation: 0.5817304139848103]
	TIME [epoch: 7.45 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5892025733573135		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.5892025733573135 | validation: 0.7902757885646348]
	TIME [epoch: 7.48 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7436943645546824		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.7436943645546824 | validation: 0.5558627349412573]
	TIME [epoch: 7.45 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9672131920997757		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.9672131920997757 | validation: 0.9478248803094047]
	TIME [epoch: 7.44 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7143618728985152		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.7143618728985152 | validation: 0.5087737242206769]
	TIME [epoch: 7.44 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5702760056232216		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.5702760056232216 | validation: 0.4618046669809853]
	TIME [epoch: 7.44 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6782894764711429		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.6782894764711429 | validation: 0.7066769464012083]
	TIME [epoch: 7.49 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6773059259901285		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.6773059259901285 | validation: 0.5521563120276656]
	TIME [epoch: 7.47 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8011449016307355		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.8011449016307355 | validation: 0.7927108095348819]
	TIME [epoch: 7.47 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8436685387219941		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.8436685387219941 | validation: 0.6087044675130018]
	TIME [epoch: 7.46 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5818363630082434		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.5818363630082434 | validation: 0.4875310929301022]
	TIME [epoch: 7.46 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5563874081414999		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.5563874081414999 | validation: 0.660474626169633]
	TIME [epoch: 7.51 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5581765686975578		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.5581765686975578 | validation: 0.6767665534123748]
	TIME [epoch: 7.45 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.896532413663658		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.896532413663658 | validation: 0.9179539197886946]
	TIME [epoch: 7.46 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6597171467917218		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.6597171467917218 | validation: 1.0238281840296946]
	TIME [epoch: 7.45 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8505589876340137		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.8505589876340137 | validation: 0.7371815269258177]
	TIME [epoch: 7.47 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6495939530815036		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.6495939530815036 | validation: 0.5719511906717858]
	TIME [epoch: 7.5 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5643835555954888		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.5643835555954888 | validation: 0.561769644668606]
	TIME [epoch: 7.48 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5923322422453823		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.5923322422453823 | validation: 0.3972763730154487]
	TIME [epoch: 7.46 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7872693988062235		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.7872693988062235 | validation: 0.9535631836869769]
	TIME [epoch: 7.44 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6893462530033846		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.6893462530033846 | validation: 0.5003153364898135]
	TIME [epoch: 7.44 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.724499382065719		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.724499382065719 | validation: 0.5934454603754624]
	TIME [epoch: 7.49 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.621969351597826		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.621969351597826 | validation: 0.4581590535182339]
	TIME [epoch: 7.49 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49758933206594097		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.49758933206594097 | validation: 0.3847032877217297]
	TIME [epoch: 7.47 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7625669231640143		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.7625669231640143 | validation: 0.9308804063479483]
	TIME [epoch: 7.46 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8087944042046871		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.8087944042046871 | validation: 0.6296518257515653]
	TIME [epoch: 7.47 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6287661429368965		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.6287661429368965 | validation: 0.6212740820078957]
	TIME [epoch: 7.5 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.589662326973569		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.589662326973569 | validation: 0.8438451171760168]
	TIME [epoch: 7.53 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9482176431260827		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.9482176431260827 | validation: 0.8054650316246348]
	TIME [epoch: 7.51 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6599787636195973		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.6599787636195973 | validation: 0.5224379878967613]
	TIME [epoch: 7.49 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6234180454890711		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.6234180454890711 | validation: 0.5010206777322465]
	TIME [epoch: 7.5 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5743371759281298		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.5743371759281298 | validation: 0.4122106121471873]
	TIME [epoch: 7.53 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6748134242701838		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.6748134242701838 | validation: 0.8072700429036133]
	TIME [epoch: 7.5 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7176162161127648		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.7176162161127648 | validation: 0.5868364104960739]
	TIME [epoch: 7.5 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8119115478545025		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.8119115478545025 | validation: 0.700254819472059]
	TIME [epoch: 7.47 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6294938721534016		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.6294938721534016 | validation: 0.633625227316332]
	TIME [epoch: 7.49 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6131092623405685		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.6131092623405685 | validation: 0.6563402466760332]
	TIME [epoch: 7.49 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6937071703287621		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.6937071703287621 | validation: 0.7508795044000566]
	TIME [epoch: 7.52 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6705083160680209		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.6705083160680209 | validation: 0.5610346861801093]
	TIME [epoch: 7.49 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5417727207841202		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.5417727207841202 | validation: 0.5403216877536828]
	TIME [epoch: 7.47 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7220963179782568		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.7220963179782568 | validation: 0.7008989227820704]
	TIME [epoch: 7.5 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5703042774730354		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.5703042774730354 | validation: 0.8179802609276169]
	TIME [epoch: 7.52 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.624820551018845		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.624820551018845 | validation: 0.4498991665306641]
	TIME [epoch: 7.53 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5038726729156001		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.5038726729156001 | validation: 0.7582583550868724]
	TIME [epoch: 7.49 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.701134241162929		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 0.701134241162929 | validation: 0.6803573708077157]
	TIME [epoch: 7.48 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6849804906987781		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.6849804906987781 | validation: 0.47813910593302433]
	TIME [epoch: 7.48 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.847684235505492		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.847684235505492 | validation: 0.625441750958619]
	TIME [epoch: 7.51 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7536228575451839		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.7536228575451839 | validation: 0.8782974316603848]
	TIME [epoch: 7.52 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7096776984871317		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.7096776984871317 | validation: 0.641709465416733]
	TIME [epoch: 7.49 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6836578035789719		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.6836578035789719 | validation: 0.6244063836980762]
	TIME [epoch: 7.5 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6508590280319597		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.6508590280319597 | validation: 0.5233363570960317]
	TIME [epoch: 7.49 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5932814102422648		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.5932814102422648 | validation: 0.5891174490706821]
	TIME [epoch: 7.5 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47829517432449675		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.47829517432449675 | validation: 0.7828106818013272]
	TIME [epoch: 7.54 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7897268011834582		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.7897268011834582 | validation: 0.6941578379867062]
	TIME [epoch: 7.5 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.628539852959575		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.628539852959575 | validation: 0.44730405047438637]
	TIME [epoch: 7.51 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6108631774106809		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.6108631774106809 | validation: 0.9149124081545316]
	TIME [epoch: 7.47 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.64516127748707		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.64516127748707 | validation: 0.5254976842565002]
	TIME [epoch: 7.48 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4932139861785715		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.4932139861785715 | validation: 1.030286081211573]
	TIME [epoch: 7.53 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6659516801684636		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.6659516801684636 | validation: 0.5791666046518715]
	TIME [epoch: 7.52 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7301451141365255		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.7301451141365255 | validation: 0.5806984985789663]
	TIME [epoch: 7.48 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5419701004146861		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.5419701004146861 | validation: 0.6675769022989515]
	TIME [epoch: 7.48 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5783869595946005		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.5783869595946005 | validation: 0.6183140810288809]
	TIME [epoch: 7.51 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5517032560476327		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.5517032560476327 | validation: 0.44587956191681644]
	TIME [epoch: 7.52 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5395542079203873		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.5395542079203873 | validation: 0.6666906177647587]
	TIME [epoch: 7.53 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6123699907971977		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.6123699907971977 | validation: 0.48260390565681033]
	TIME [epoch: 7.5 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6847192214833536		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.6847192214833536 | validation: 0.7156287895213528]
	TIME [epoch: 7.51 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5460094451070001		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.5460094451070001 | validation: 0.49881932132256435]
	TIME [epoch: 7.51 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5384703951038157		[learning rate: 0.0067548]
	Learning Rate: 0.00675485
	LOSS [training: 0.5384703951038157 | validation: 0.3827552318808514]
	TIME [epoch: 7.54 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_161.pth
	Model improved!!!
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.579318876337108		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.579318876337108 | validation: 0.707830061907073]
	TIME [epoch: 7.52 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5647003282930805		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.5647003282930805 | validation: 0.5655078292625102]
	TIME [epoch: 7.5 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6225573482318895		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.6225573482318895 | validation: 0.46317524747670835]
	TIME [epoch: 7.5 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4276796488017329		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.4276796488017329 | validation: 0.3972640657457662]
	TIME [epoch: 7.49 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.481878056717456		[learning rate: 0.0066363]
	Learning Rate: 0.00663625
	LOSS [training: 0.481878056717456 | validation: 0.5352947335228426]
	TIME [epoch: 7.51 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5085394460052445		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.5085394460052445 | validation: 0.6604008662424274]
	TIME [epoch: 7.53 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6420086304951981		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.6420086304951981 | validation: 0.44173519201343187]
	TIME [epoch: 7.5 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41655063648128454		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.41655063648128454 | validation: 0.8485660154303734]
	TIME [epoch: 7.48 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6255586426122259		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.6255586426122259 | validation: 0.43513748275124664]
	TIME [epoch: 7.51 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4550403656108711		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.4550403656108711 | validation: 0.507143339056277]
	TIME [epoch: 7.5 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5946725467840513		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.5946725467840513 | validation: 0.5591002028817393]
	TIME [epoch: 7.52 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4585124930923734		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.4585124930923734 | validation: 0.8510291469049499]
	TIME [epoch: 7.48 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5019829328548029		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.5019829328548029 | validation: 0.7978370215900268]
	TIME [epoch: 7.48 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.617228030518377		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 0.617228030518377 | validation: 0.49376789499768525]
	TIME [epoch: 7.46 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4893746248026544		[learning rate: 0.0064053]
	Learning Rate: 0.00640529
	LOSS [training: 0.4893746248026544 | validation: 0.6751539167602534]
	TIME [epoch: 7.5 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5052011709119043		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.5052011709119043 | validation: 0.37554066124166163]
	TIME [epoch: 7.52 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_177.pth
	Model improved!!!
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7162651024881569		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.7162651024881569 | validation: 0.5695177149027009]
	TIME [epoch: 7.48 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44044999563010706		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.44044999563010706 | validation: 0.5552914881587538]
	TIME [epoch: 7.47 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46626256031284485		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.46626256031284485 | validation: 0.7698518618578674]
	TIME [epoch: 7.48 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5433559146664613		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.5433559146664613 | validation: 0.513379338206962]
	TIME [epoch: 7.49 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41979908193936416		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.41979908193936416 | validation: 0.592921125091847]
	TIME [epoch: 7.53 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5447915515644329		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.5447915515644329 | validation: 0.5464333010369083]
	TIME [epoch: 7.47 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5031540902640641		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.5031540902640641 | validation: 0.46360926831455496]
	TIME [epoch: 7.48 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39389752240383086		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.39389752240383086 | validation: 0.5219839282578631]
	TIME [epoch: 7.5 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5712294009753909		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.5712294009753909 | validation: 0.6642574397445951]
	TIME [epoch: 7.48 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4831454754830944		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.4831454754830944 | validation: 0.5189178946519435]
	TIME [epoch: 7.56 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4853227125273322		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.4853227125273322 | validation: 0.5792821994205406]
	TIME [epoch: 7.49 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46293381049526466		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.46293381049526466 | validation: 0.5433232134264123]
	TIME [epoch: 7.49 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36522402604235943		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.36522402604235943 | validation: 0.9653836811025114]
	TIME [epoch: 7.49 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5803410374785587		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.5803410374785587 | validation: 0.5672243480705679]
	TIME [epoch: 7.49 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4584348930870532		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.4584348930870532 | validation: 0.41005213375196325]
	TIME [epoch: 7.54 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47094070930719933		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.47094070930719933 | validation: 0.5372317951113993]
	TIME [epoch: 7.48 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38282427137560715		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.38282427137560715 | validation: 0.34962674164998725]
	TIME [epoch: 7.48 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4936723888482586		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.4936723888482586 | validation: 0.5020040943492368]
	TIME [epoch: 7.46 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4526738657350823		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.4526738657350823 | validation: 0.5108878352447775]
	TIME [epoch: 7.43 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.453771050741371		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.453771050741371 | validation: 0.5091965684543592]
	TIME [epoch: 7.48 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3912312226788144		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.3912312226788144 | validation: 0.8606126045569824]
	TIME [epoch: 7.44 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49976926740156624		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.49976926740156624 | validation: 0.37460502295272635]
	TIME [epoch: 7.43 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43131789934047343		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.43131789934047343 | validation: 0.684685638927125]
	TIME [epoch: 7.43 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7246805058621285		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.7246805058621285 | validation: 0.628030994097323]
	TIME [epoch: 120 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.431533559612268		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.431533559612268 | validation: 0.4767588071162787]
	TIME [epoch: 14.7 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4249772032278161		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.4249772032278161 | validation: 0.646706067082769]
	TIME [epoch: 14.7 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4984773800590121		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.4984773800590121 | validation: 0.47291507274172195]
	TIME [epoch: 14.7 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40253903960934734		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.40253903960934734 | validation: 0.41447760748162527]
	TIME [epoch: 14.7 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3700059222407278		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.3700059222407278 | validation: 0.5608372393457868]
	TIME [epoch: 14.7 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34863805794302877		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.34863805794302877 | validation: 0.4946657070520585]
	TIME [epoch: 14.7 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46337457476169663		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.46337457476169663 | validation: 0.7222812591004482]
	TIME [epoch: 14.7 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5760562987482941		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.5760562987482941 | validation: 0.4188233793939875]
	TIME [epoch: 14.7 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.489260893369708		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.489260893369708 | validation: 0.6364321181152944]
	TIME [epoch: 14.7 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41449467943956997		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.41449467943956997 | validation: 0.3748533269374311]
	TIME [epoch: 14.7 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46149442476483526		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.46149442476483526 | validation: 0.3338140404705618]
	TIME [epoch: 14.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4515853337736896		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.4515853337736896 | validation: 0.7193486412436991]
	TIME [epoch: 14.6 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4476073468229461		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.4476073468229461 | validation: 0.34626487953064927]
	TIME [epoch: 14.6 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33698596487945864		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.33698596487945864 | validation: 0.4103817857163796]
	TIME [epoch: 14.7 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4590636743741235		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.4590636743741235 | validation: 0.4990239203922441]
	TIME [epoch: 14.7 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4468554626552808		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.4468554626552808 | validation: 0.4661065760846653]
	TIME [epoch: 14.6 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48829493710851757		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.48829493710851757 | validation: 0.46263453735147564]
	TIME [epoch: 14.7 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4524919680821953		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.4524919680821953 | validation: 0.5897340627396004]
	TIME [epoch: 14.6 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4077542048402393		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.4077542048402393 | validation: 0.38793195706748956]
	TIME [epoch: 14.7 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4382365378907733		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.4382365378907733 | validation: 0.4856927024877275]
	TIME [epoch: 14.7 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.416965809378931		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.416965809378931 | validation: 0.49715278979120103]
	TIME [epoch: 14.7 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4489064561768139		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.4489064561768139 | validation: 0.3715523606028296]
	TIME [epoch: 14.7 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3440260497645785		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.3440260497645785 | validation: 0.35251765894992404]
	TIME [epoch: 14.7 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37536700993710037		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.37536700993710037 | validation: 0.5412657337902224]
	TIME [epoch: 14.6 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3969691671219829		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.3969691671219829 | validation: 0.3899578440466365]
	TIME [epoch: 14.7 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38580887567986066		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.38580887567986066 | validation: 0.7330096629342784]
	TIME [epoch: 14.7 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.522361694252734		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.522361694252734 | validation: 0.417461449492461]
	TIME [epoch: 14.7 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3312650934292605		[learning rate: 0.0053088]
	Learning Rate: 0.00530885
	LOSS [training: 0.3312650934292605 | validation: 0.3581878237040216]
	TIME [epoch: 14.7 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3427389731184305		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.3427389731184305 | validation: 0.37032869328791296]
	TIME [epoch: 14.7 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3796317914448767		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.3796317914448767 | validation: 0.6112746175120055]
	TIME [epoch: 14.7 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38714551721789214		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.38714551721789214 | validation: 0.3172750381807947]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_232.pth
	Model improved!!!
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4230345687177969		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.4230345687177969 | validation: 0.4328295033879713]
	TIME [epoch: 14.6 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.406773826872016		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.406773826872016 | validation: 0.640842743671793]
	TIME [epoch: 14.7 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.48212245996142766		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.48212245996142766 | validation: 0.391818268196101]
	TIME [epoch: 14.6 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.403043305048564		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.403043305048564 | validation: 0.34882996705206004]
	TIME [epoch: 14.7 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3577917104884069		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.3577917104884069 | validation: 0.38563230883726046]
	TIME [epoch: 14.7 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3636993418061224		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.3636993418061224 | validation: 0.3491492878739324]
	TIME [epoch: 14.6 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4047431280965168		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.4047431280965168 | validation: 0.4244527197425324]
	TIME [epoch: 14.7 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3456075624044471		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 0.3456075624044471 | validation: 0.3675531443928848]
	TIME [epoch: 14.7 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34800176435022867		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.34800176435022867 | validation: 0.572253473075853]
	TIME [epoch: 14.7 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4847474701609769		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.4847474701609769 | validation: 0.355016711774727]
	TIME [epoch: 14.7 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3747326186965189		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.3747326186965189 | validation: 0.3593229473939209]
	TIME [epoch: 14.7 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31978743464100323		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.31978743464100323 | validation: 0.5056015162260046]
	TIME [epoch: 14.7 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3674819189104764		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.3674819189104764 | validation: 0.3377956195640578]
	TIME [epoch: 14.7 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.355182734067484		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.355182734067484 | validation: 0.3267308753803932]
	TIME [epoch: 14.7 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.288296384591099		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.288296384591099 | validation: 0.36160639534808425]
	TIME [epoch: 14.7 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3499952067702987		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.3499952067702987 | validation: 0.3138447299011115]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2973919038937482		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.2973919038937482 | validation: 0.5175199142907851]
	TIME [epoch: 14.6 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4011682630026078		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 0.4011682630026078 | validation: 0.3703120358357612]
	TIME [epoch: 14.7 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38331679861377		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.38331679861377 | validation: 0.5602600072989214]
	TIME [epoch: 14.6 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3843589998694878		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.3843589998694878 | validation: 0.30972769063825045]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_252.pth
	Model improved!!!
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3401991017260707		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.3401991017260707 | validation: 0.36762796681314935]
	TIME [epoch: 14.7 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3081963896579996		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.3081963896579996 | validation: 0.42082192324008794]
	TIME [epoch: 14.7 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3339678259835456		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.3339678259835456 | validation: 0.3633188894454245]
	TIME [epoch: 14.7 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36072231093794205		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.36072231093794205 | validation: 0.3129800175362685]
	TIME [epoch: 14.7 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38395747805897407		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.38395747805897407 | validation: 0.6283833599724895]
	TIME [epoch: 14.6 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45979794219429665		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.45979794219429665 | validation: 0.3261073391682782]
	TIME [epoch: 14.7 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28784622280864774		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.28784622280864774 | validation: 0.3543234011453892]
	TIME [epoch: 14.6 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38068642397155905		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.38068642397155905 | validation: 0.4842179451069995]
	TIME [epoch: 14.6 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3347722816174664		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.3347722816174664 | validation: 0.30477754661710865]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_261.pth
	Model improved!!!
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2856435258911795		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.2856435258911795 | validation: 0.3654355619075287]
	TIME [epoch: 14.6 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.322531098977353		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.322531098977353 | validation: 0.3234854500664922]
	TIME [epoch: 14.7 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3632617341255938		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.3632617341255938 | validation: 0.4932538910192604]
	TIME [epoch: 14.7 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3612678126402748		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.3612678126402748 | validation: 0.3036124032828062]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_265.pth
	Model improved!!!
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3088049557638722		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.3088049557638722 | validation: 0.3022182188673608]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_266.pth
	Model improved!!!
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29404262175226004		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.29404262175226004 | validation: 0.2803518404150856]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3346480921019324		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.3346480921019324 | validation: 0.3050221155845061]
	TIME [epoch: 14.7 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29553420196928104		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.29553420196928104 | validation: 0.34635515623968116]
	TIME [epoch: 14.8 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2859811175106069		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.2859811175106069 | validation: 0.4368136075392784]
	TIME [epoch: 14.7 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27434633354321525		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.27434633354321525 | validation: 0.28457972370990314]
	TIME [epoch: 14.8 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34286475652784043		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.34286475652784043 | validation: 0.4321143243753063]
	TIME [epoch: 14.7 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34316638626194773		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.34316638626194773 | validation: 0.4128086960019139]
	TIME [epoch: 14.7 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2884576072958852		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.2884576072958852 | validation: 0.28005451329609743]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_274.pth
	Model improved!!!
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30193294578283		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.30193294578283 | validation: 0.3018185056667422]
	TIME [epoch: 14.7 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3382167000433742		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.3382167000433742 | validation: 0.32828554424828327]
	TIME [epoch: 14.8 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.303315279973026		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.303315279973026 | validation: 0.26975797420619724]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2662642913422546		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.2662642913422546 | validation: 0.34666283909800805]
	TIME [epoch: 14.7 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28698580316060973		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.28698580316060973 | validation: 0.3062936174891625]
	TIME [epoch: 14.7 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2859982235633672		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.2859982235633672 | validation: 0.2789191542042722]
	TIME [epoch: 14.7 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2739872033960733		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.2739872033960733 | validation: 0.26838429182854234]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_281.pth
	Model improved!!!
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26464709846050083		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.26464709846050083 | validation: 0.33154069709636974]
	TIME [epoch: 14.8 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27364475406298955		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.27364475406298955 | validation: 0.26454723533902336]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_283.pth
	Model improved!!!
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27354616583871405		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.27354616583871405 | validation: 0.25945503889922794]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_284.pth
	Model improved!!!
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3356043927527921		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.3356043927527921 | validation: 0.2578690942150906]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_285.pth
	Model improved!!!
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2677368079960041		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.2677368079960041 | validation: 0.32034498951979096]
	TIME [epoch: 14.7 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3431836474746017		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.3431836474746017 | validation: 0.3168624905941563]
	TIME [epoch: 14.7 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2867408004801454		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.2867408004801454 | validation: 0.2872160464718591]
	TIME [epoch: 14.7 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2721609222151916		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.2721609222151916 | validation: 0.2948816147687774]
	TIME [epoch: 14.7 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28900253213466487		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.28900253213466487 | validation: 0.44731567910488734]
	TIME [epoch: 14.7 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3247504437628052		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.3247504437628052 | validation: 0.2932747909642516]
	TIME [epoch: 14.7 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.287466503669496		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.287466503669496 | validation: 0.3107580293364048]
	TIME [epoch: 14.7 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26182650652037576		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.26182650652037576 | validation: 0.252631539902996]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_293.pth
	Model improved!!!
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26198952497749556		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.26198952497749556 | validation: 0.47504254841050747]
	TIME [epoch: 14.7 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2868770920575108		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.2868770920575108 | validation: 0.32385041957142663]
	TIME [epoch: 14.7 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3142688374818376		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.3142688374818376 | validation: 0.23670951915736138]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_296.pth
	Model improved!!!
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35783958265862714		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.35783958265862714 | validation: 0.2612266315600096]
	TIME [epoch: 14.7 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2885398255606971		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.2885398255606971 | validation: 0.3014535378414612]
	TIME [epoch: 14.7 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27661707830617144		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.27661707830617144 | validation: 0.2693877617331324]
	TIME [epoch: 14.8 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30809330229996357		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.30809330229996357 | validation: 0.26903725775130455]
	TIME [epoch: 14.7 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24995781028775263		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.24995781028775263 | validation: 0.264346854491871]
	TIME [epoch: 14.8 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24936735824664205		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.24936735824664205 | validation: 0.3456337229617421]
	TIME [epoch: 14.7 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2911178250682305		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.2911178250682305 | validation: 0.3848142004005358]
	TIME [epoch: 14.7 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30247985121402937		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.30247985121402937 | validation: 0.35561689085490106]
	TIME [epoch: 14.8 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2754908830357752		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.2754908830357752 | validation: 0.3072494827279527]
	TIME [epoch: 14.7 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30309137483757675		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.30309137483757675 | validation: 0.3058219766044756]
	TIME [epoch: 14.7 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26718692574021013		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.26718692574021013 | validation: 0.24502273345800035]
	TIME [epoch: 14.8 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2564415314673376		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.2564415314673376 | validation: 0.32252963645653504]
	TIME [epoch: 14.7 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2617922426628197		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.2617922426628197 | validation: 0.262696941435067]
	TIME [epoch: 14.7 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2648244352664		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.2648244352664 | validation: 0.2596133125965764]
	TIME [epoch: 14.7 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24275990104931078		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.24275990104931078 | validation: 0.24110954055952755]
	TIME [epoch: 14.7 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22588326574865608		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.22588326574865608 | validation: 0.2218378306396189]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_312.pth
	Model improved!!!
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26194499540430827		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.26194499540430827 | validation: 0.270340321015317]
	TIME [epoch: 14.7 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23580877890857949		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.23580877890857949 | validation: 0.2503526120582754]
	TIME [epoch: 14.7 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2629314808123562		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.2629314808123562 | validation: 0.29513957247979405]
	TIME [epoch: 14.8 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3152100426529435		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.3152100426529435 | validation: 0.3114538010628204]
	TIME [epoch: 14.7 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26665877334673344		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.26665877334673344 | validation: 0.257009040353161]
	TIME [epoch: 14.7 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2561565673906882		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.2561565673906882 | validation: 0.23742940460020134]
	TIME [epoch: 14.7 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2198115526591831		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.2198115526591831 | validation: 0.2655433882470375]
	TIME [epoch: 14.7 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27393257753104716		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.27393257753104716 | validation: 0.3097241877954522]
	TIME [epoch: 14.7 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28076261846359435		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.28076261846359435 | validation: 0.31777372935279474]
	TIME [epoch: 14.8 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2555662565212731		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.2555662565212731 | validation: 0.2511007356030721]
	TIME [epoch: 14.7 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2641397811246905		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.2641397811246905 | validation: 0.30751784296927687]
	TIME [epoch: 14.7 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24813149564143888		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.24813149564143888 | validation: 0.2948879675483951]
	TIME [epoch: 14.7 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23738323444839837		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.23738323444839837 | validation: 0.30234955016350484]
	TIME [epoch: 14.7 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27646181195540087		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.27646181195540087 | validation: 0.2518417765934373]
	TIME [epoch: 14.8 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29081962728366945		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.29081962728366945 | validation: 0.27219735773034165]
	TIME [epoch: 14.7 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25499676153429696		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.25499676153429696 | validation: 0.28230759749288553]
	TIME [epoch: 14.7 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2371576488443787		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.2371576488443787 | validation: 0.24906986775016698]
	TIME [epoch: 14.8 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22021586522009418		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.22021586522009418 | validation: 0.2638988196702666]
	TIME [epoch: 14.7 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25461878263194104		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.25461878263194104 | validation: 0.22637391855702876]
	TIME [epoch: 14.8 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2779548027282948		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.2779548027282948 | validation: 0.2220398418180649]
	TIME [epoch: 14.7 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25786572826496285		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.25786572826496285 | validation: 0.23006772761350203]
	TIME [epoch: 14.7 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22763235736878568		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.22763235736878568 | validation: 0.27684535196409465]
	TIME [epoch: 14.8 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2263273209282893		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.2263273209282893 | validation: 0.21623945270480155]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_335.pth
	Model improved!!!
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20752768784099432		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.20752768784099432 | validation: 0.21126076378681657]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_336.pth
	Model improved!!!
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3398618280633239		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.3398618280633239 | validation: 0.2401482074441244]
	TIME [epoch: 14.8 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2620511040942716		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.2620511040942716 | validation: 0.24000801314414671]
	TIME [epoch: 14.7 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26131772255809604		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.26131772255809604 | validation: 0.2534957991064508]
	TIME [epoch: 14.7 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23126632119461046		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.23126632119461046 | validation: 0.21618629239544118]
	TIME [epoch: 14.8 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25391271629102935		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.25391271629102935 | validation: 0.2277493308129469]
	TIME [epoch: 14.7 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2103623348428753		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.2103623348428753 | validation: 0.21283629486201788]
	TIME [epoch: 14.8 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2397880376818971		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.2397880376818971 | validation: 0.2366317322376855]
	TIME [epoch: 14.7 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24852106380970723		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.24852106380970723 | validation: 0.25011251949154006]
	TIME [epoch: 14.7 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27080902842953114		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.27080902842953114 | validation: 0.23900009556662288]
	TIME [epoch: 14.8 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2210113215328397		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.2210113215328397 | validation: 0.2048469393774433]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_346.pth
	Model improved!!!
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24000873406877984		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.24000873406877984 | validation: 0.2181604792939965]
	TIME [epoch: 14.7 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22202436516476937		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.22202436516476937 | validation: 0.22826049299233891]
	TIME [epoch: 14.7 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30374756129005576		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.30374756129005576 | validation: 0.26024962055601947]
	TIME [epoch: 14.7 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23999057380429362		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.23999057380429362 | validation: 0.23283239315802495]
	TIME [epoch: 14.7 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20612634474619373		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.20612634474619373 | validation: 0.2607601804661918]
	TIME [epoch: 14.7 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24008426122785909		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.24008426122785909 | validation: 0.23777423050616128]
	TIME [epoch: 14.7 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2841665295061131		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.2841665295061131 | validation: 0.26713033030097477]
	TIME [epoch: 14.7 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23638827327237544		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.23638827327237544 | validation: 0.23623889526949587]
	TIME [epoch: 14.7 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20894935445373702		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.20894935445373702 | validation: 0.20757640557184698]
	TIME [epoch: 14.7 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22025808439704195		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.22025808439704195 | validation: 0.2116494119956327]
	TIME [epoch: 14.7 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23470068859361606		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.23470068859361606 | validation: 0.20039254952731694]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_357.pth
	Model improved!!!
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2120019175596871		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.2120019175596871 | validation: 0.2041726938367366]
	TIME [epoch: 14.7 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22746846769320167		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.22746846769320167 | validation: 0.24680242613990788]
	TIME [epoch: 14.7 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22169344646992944		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.22169344646992944 | validation: 0.3463726462025811]
	TIME [epoch: 14.7 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2630060646781835		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.2630060646781835 | validation: 0.19223228359427275]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_361.pth
	Model improved!!!
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20344676606838036		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.20344676606838036 | validation: 0.25666332950549664]
	TIME [epoch: 14.7 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20413906353442993		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.20413906353442993 | validation: 0.24820893868479998]
	TIME [epoch: 14.7 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20164294922140044		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.20164294922140044 | validation: 0.19267792197368327]
	TIME [epoch: 14.7 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20045047170658384		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.20045047170658384 | validation: 0.23966115263519608]
	TIME [epoch: 14.8 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25516599648926913		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.25516599648926913 | validation: 0.2722991131647709]
	TIME [epoch: 14.7 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21841880852220394		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.21841880852220394 | validation: 0.174391869204833]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_367.pth
	Model improved!!!
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20951582232867047		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.20951582232867047 | validation: 0.1796247308113019]
	TIME [epoch: 14.7 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19604752887868035		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.19604752887868035 | validation: 0.23949349382949853]
	TIME [epoch: 14.7 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21215585982523377		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.21215585982523377 | validation: 0.19315527932439297]
	TIME [epoch: 14.7 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2029619270728528		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.2029619270728528 | validation: 0.18854993399744122]
	TIME [epoch: 14.7 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18348668929412354		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.18348668929412354 | validation: 0.17145256250301566]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_372.pth
	Model improved!!!
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18556842765617582		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.18556842765617582 | validation: 0.30941513050596275]
	TIME [epoch: 14.7 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27167383358603986		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.27167383358603986 | validation: 0.25663474423523436]
	TIME [epoch: 14.7 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24294073145828224		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.24294073145828224 | validation: 0.21662008386860046]
	TIME [epoch: 14.7 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17924776930855643		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.17924776930855643 | validation: 0.17859301238279002]
	TIME [epoch: 14.7 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2015938478397808		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.2015938478397808 | validation: 0.1666705878377468]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_377.pth
	Model improved!!!
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17071760698403143		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.17071760698403143 | validation: 0.17799267809862745]
	TIME [epoch: 14.7 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17450378333207908		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.17450378333207908 | validation: 0.16751681289599823]
	TIME [epoch: 14.7 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18226209797560872		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.18226209797560872 | validation: 0.1572233442192854]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_380.pth
	Model improved!!!
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17623149451215797		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.17623149451215797 | validation: 0.19089925586583828]
	TIME [epoch: 14.7 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1853501370652016		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.1853501370652016 | validation: 0.3444424900403219]
	TIME [epoch: 14.8 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21948057039842744		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.21948057039842744 | validation: 0.16724388755650604]
	TIME [epoch: 14.7 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18137669314608273		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.18137669314608273 | validation: 0.19571165191058107]
	TIME [epoch: 14.7 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19761555769494848		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.19761555769494848 | validation: 0.23196168201557976]
	TIME [epoch: 14.8 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20056404032893926		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.20056404032893926 | validation: 0.20007964674569456]
	TIME [epoch: 14.7 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16791956953832177		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.16791956953832177 | validation: 0.21017175183857154]
	TIME [epoch: 14.7 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1728165598330882		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.1728165598330882 | validation: 0.16678198242668824]
	TIME [epoch: 14.8 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1545848881823401		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.1545848881823401 | validation: 0.17392012133692342]
	TIME [epoch: 14.7 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15526174515962665		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.15526174515962665 | validation: 0.18637392067112807]
	TIME [epoch: 14.7 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18958808960925966		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.18958808960925966 | validation: 0.20827080131633569]
	TIME [epoch: 14.7 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18427254006783278		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.18427254006783278 | validation: 0.1711884933456541]
	TIME [epoch: 14.7 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1652953409016577		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.1652953409016577 | validation: 0.16215470492165535]
	TIME [epoch: 14.7 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15747127331206978		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.15747127331206978 | validation: 0.15969974215548388]
	TIME [epoch: 14.7 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19694679683217867		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.19694679683217867 | validation: 0.14359751190222772]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_395.pth
	Model improved!!!
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16234723160115924		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.16234723160115924 | validation: 0.13987138386764775]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_396.pth
	Model improved!!!
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16689087919445528		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.16689087919445528 | validation: 0.29226025584248383]
	TIME [epoch: 14.7 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2388597107685711		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.2388597107685711 | validation: 0.17180642650884037]
	TIME [epoch: 14.7 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17133535195792338		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.17133535195792338 | validation: 0.14320051898833064]
	TIME [epoch: 14.8 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15529539097538614		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.15529539097538614 | validation: 0.16872238240166543]
	TIME [epoch: 14.7 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15860444311266575		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.15860444311266575 | validation: 0.16025764451920596]
	TIME [epoch: 14.7 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15187932806852006		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.15187932806852006 | validation: 0.18991135135752346]
	TIME [epoch: 14.8 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20624850434032585		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.20624850434032585 | validation: 0.16089728315063348]
	TIME [epoch: 14.7 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21378805752603358		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.21378805752603358 | validation: 0.21707077581321313]
	TIME [epoch: 14.7 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1738829672772908		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.1738829672772908 | validation: 0.146979940446344]
	TIME [epoch: 14.7 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15415569308948815		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.15415569308948815 | validation: 0.20530393193904517]
	TIME [epoch: 14.7 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20853589082031515		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.20853589082031515 | validation: 0.20211058646482016]
	TIME [epoch: 14.7 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18767609259723894		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.18767609259723894 | validation: 0.14297279757111198]
	TIME [epoch: 14.7 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1377171830663267		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.1377171830663267 | validation: 0.13511399089361117]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_409.pth
	Model improved!!!
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1407661747487482		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.1407661747487482 | validation: 0.24250215634709613]
	TIME [epoch: 14.7 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2167366397101511		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.2167366397101511 | validation: 0.13983617178152963]
	TIME [epoch: 14.7 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1437677250943659		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.1437677250943659 | validation: 0.19003549674177111]
	TIME [epoch: 14.7 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15707269672529786		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.15707269672529786 | validation: 0.14847278599218017]
	TIME [epoch: 14.7 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13342563913889563		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.13342563913889563 | validation: 0.12850748368224946]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_414.pth
	Model improved!!!
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15969498402601268		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.15969498402601268 | validation: 0.1431289581600649]
	TIME [epoch: 14.7 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15143126451149194		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.15143126451149194 | validation: 0.23129667093120398]
	TIME [epoch: 14.7 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21545483790593947		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.21545483790593947 | validation: 0.1800766586060096]
	TIME [epoch: 14.7 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15468626102191085		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.15468626102191085 | validation: 0.15175353894048052]
	TIME [epoch: 14.7 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13782657046734398		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.13782657046734398 | validation: 0.12744986371141706]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_419.pth
	Model improved!!!
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13720238435121382		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.13720238435121382 | validation: 0.13237839580335176]
	TIME [epoch: 14.7 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15753403009237257		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.15753403009237257 | validation: 0.15223067374905402]
	TIME [epoch: 14.7 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16481041408600886		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.16481041408600886 | validation: 0.14936334404903506]
	TIME [epoch: 14.7 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1352286336592743		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.1352286336592743 | validation: 0.149186847163478]
	TIME [epoch: 14.7 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13423144886142913		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.13423144886142913 | validation: 0.1371006902380194]
	TIME [epoch: 14.7 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19742708488401067		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.19742708488401067 | validation: 0.19479546502817263]
	TIME [epoch: 14.7 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16661262826094725		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.16661262826094725 | validation: 0.14601166518931535]
	TIME [epoch: 14.7 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16349346524179026		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.16349346524179026 | validation: 0.19533159025781327]
	TIME [epoch: 14.7 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14833365038995353		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.14833365038995353 | validation: 0.1386798669631224]
	TIME [epoch: 14.7 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12899644176907157		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.12899644176907157 | validation: 0.13715334983004515]
	TIME [epoch: 14.7 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13229222146949168		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.13229222146949168 | validation: 0.17548485418515458]
	TIME [epoch: 14.8 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14610034467375788		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.14610034467375788 | validation: 0.11960191569648881]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_431.pth
	Model improved!!!
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.150756507746768		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.150756507746768 | validation: 0.14456009434283923]
	TIME [epoch: 14.7 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12676810106323325		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.12676810106323325 | validation: 0.11415877001521586]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_433.pth
	Model improved!!!
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14379195423390606		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.14379195423390606 | validation: 0.12621352302476258]
	TIME [epoch: 14.7 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1599955954071356		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.1599955954071356 | validation: 0.12434048708315035]
	TIME [epoch: 14.7 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12641898753449304		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.12641898753449304 | validation: 0.10872045454064909]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_436.pth
	Model improved!!!
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11509760045727377		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.11509760045727377 | validation: 0.1385340072324706]
	TIME [epoch: 14.7 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15818781231757073		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.15818781231757073 | validation: 0.15484722269508017]
	TIME [epoch: 14.7 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16041061942689788		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.16041061942689788 | validation: 0.156493320080213]
	TIME [epoch: 14.7 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1538236783606222		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.1538236783606222 | validation: 0.17631570167241198]
	TIME [epoch: 14.7 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12622376085152787		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.12622376085152787 | validation: 0.1365449757731032]
	TIME [epoch: 14.7 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12446067204853423		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.12446067204853423 | validation: 0.2356322643775526]
	TIME [epoch: 14.7 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16614413721553645		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.16614413721553645 | validation: 0.147208169492615]
	TIME [epoch: 14.7 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13825242086722472		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.13825242086722472 | validation: 0.11841645681163204]
	TIME [epoch: 14.7 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1594621264446066		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.1594621264446066 | validation: 0.1350563752099116]
	TIME [epoch: 14.7 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11845594082480165		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.11845594082480165 | validation: 0.19947879304094493]
	TIME [epoch: 14.7 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14519874509809244		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.14519874509809244 | validation: 0.1378447746306447]
	TIME [epoch: 14.7 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11987272283825767		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.11987272283825767 | validation: 0.2333024784744303]
	TIME [epoch: 14.7 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16683364222208705		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.16683364222208705 | validation: 0.14354994733690774]
	TIME [epoch: 14.7 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16735267912208523		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.16735267912208523 | validation: 0.1354509749602893]
	TIME [epoch: 14.7 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1293103417382232		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.1293103417382232 | validation: 0.12851485293634532]
	TIME [epoch: 14.7 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16521236388177032		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.16521236388177032 | validation: 0.14563588528788618]
	TIME [epoch: 14.7 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13118022247722463		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.13118022247722463 | validation: 0.11626203257576312]
	TIME [epoch: 14.7 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13398260869210565		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.13398260869210565 | validation: 0.12405798144067529]
	TIME [epoch: 14.7 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14527399782123238		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.14527399782123238 | validation: 0.14030913457795605]
	TIME [epoch: 14.7 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14309223122386838		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.14309223122386838 | validation: 0.12099257757465043]
	TIME [epoch: 14.7 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11191568507630247		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.11191568507630247 | validation: 0.13118067974693393]
	TIME [epoch: 14.7 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12058861478615754		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.12058861478615754 | validation: 0.12709703341466771]
	TIME [epoch: 14.7 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13060095145008638		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.13060095145008638 | validation: 0.17910494023576415]
	TIME [epoch: 14.8 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17349800740510768		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.17349800740510768 | validation: 0.2626877679660566]
	TIME [epoch: 14.6 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22227301387026266		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.22227301387026266 | validation: 0.13948654280834483]
	TIME [epoch: 14.7 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11801120708573123		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.11801120708573123 | validation: 0.1458681843326398]
	TIME [epoch: 14.7 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12995158189421235		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.12995158189421235 | validation: 0.1344364928619376]
	TIME [epoch: 14.7 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13342796302448928		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.13342796302448928 | validation: 0.142264889935226]
	TIME [epoch: 14.7 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11037573379845325		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.11037573379845325 | validation: 0.11636286910331231]
	TIME [epoch: 14.7 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12298755633175878		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.12298755633175878 | validation: 0.21420530175503355]
	TIME [epoch: 14.7 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16795439640830637		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.16795439640830637 | validation: 0.2092712045835986]
	TIME [epoch: 14.7 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14682269013034852		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.14682269013034852 | validation: 0.1154852091322047]
	TIME [epoch: 14.7 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12898891742242688		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.12898891742242688 | validation: 0.13270956667660339]
	TIME [epoch: 14.6 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12193103954364032		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.12193103954364032 | validation: 0.1148924432234806]
	TIME [epoch: 14.7 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13353774114952366		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.13353774114952366 | validation: 0.12751486598553446]
	TIME [epoch: 14.7 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1332621903234323		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.1332621903234323 | validation: 0.114114139073732]
	TIME [epoch: 14.7 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12093748485078622		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.12093748485078622 | validation: 0.17043639904282687]
	TIME [epoch: 14.7 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1572092940524832		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.1572092940524832 | validation: 0.13430861729980434]
	TIME [epoch: 14.7 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12474737342850564		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.12474737342850564 | validation: 0.12035270536839168]
	TIME [epoch: 14.7 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11878755561091854		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.11878755561091854 | validation: 0.16230538964070718]
	TIME [epoch: 14.7 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14318947078438213		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.14318947078438213 | validation: 0.10925137034400452]
	TIME [epoch: 14.7 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11842827216178031		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.11842827216178031 | validation: 0.1097940935714557]
	TIME [epoch: 14.7 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12460634773432355		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.12460634773432355 | validation: 0.1122699445654951]
	TIME [epoch: 14.7 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10798776459634613		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.10798776459634613 | validation: 0.1062057735892077]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_480.pth
	Model improved!!!
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10893238523452864		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.10893238523452864 | validation: 0.19571355476121666]
	TIME [epoch: 14.7 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1438309661690934		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.1438309661690934 | validation: 0.1062100835303137]
	TIME [epoch: 14.8 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11367248087338773		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.11367248087338773 | validation: 0.119971315684234]
	TIME [epoch: 14.7 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12264606829216211		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.12264606829216211 | validation: 0.10483929736489776]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_484.pth
	Model improved!!!
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10742773081451616		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.10742773081451616 | validation: 0.10495392867625623]
	TIME [epoch: 14.7 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10950181489019867		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.10950181489019867 | validation: 0.19483346168036403]
	TIME [epoch: 14.7 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13775483109893175		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.13775483109893175 | validation: 0.17400470503152304]
	TIME [epoch: 14.7 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1320488970967463		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.1320488970967463 | validation: 0.12166727587911444]
	TIME [epoch: 14.7 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11620170598249263		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.11620170598249263 | validation: 0.12016499359609738]
	TIME [epoch: 14.7 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10220481150866		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.10220481150866 | validation: 0.19219070548153644]
	TIME [epoch: 14.7 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15256406195729913		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.15256406195729913 | validation: 0.13670503624552707]
	TIME [epoch: 14.7 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12423887888953952		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.12423887888953952 | validation: 0.10586110928478]
	TIME [epoch: 14.7 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12306194953744917		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.12306194953744917 | validation: 0.09935611496105715]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_493.pth
	Model improved!!!
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1557882492836306		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.1557882492836306 | validation: 0.1463625730616856]
	TIME [epoch: 14.7 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15207107152330468		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.15207107152330468 | validation: 0.13663261127077142]
	TIME [epoch: 14.7 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10169414242852359		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.10169414242852359 | validation: 0.10133138810940806]
	TIME [epoch: 14.7 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10080684207088092		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.10080684207088092 | validation: 0.1147702458206942]
	TIME [epoch: 14.7 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1223841694759871		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.1223841694759871 | validation: 0.11054446879185587]
	TIME [epoch: 14.7 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10852797337637696		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.10852797337637696 | validation: 0.09648037616257439]
	TIME [epoch: 14.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_499.pth
	Model improved!!!
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11198417022669163		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.11198417022669163 | validation: 0.10908365230443778]
	TIME [epoch: 14.7 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11572487611704624		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.11572487611704624 | validation: 0.12283992365278809]
	TIME [epoch: 138 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10217365125148281		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.10217365125148281 | validation: 0.11993139320986035]
	TIME [epoch: 31.9 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10214441584301964		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.10214441584301964 | validation: 0.11385407524508377]
	TIME [epoch: 32 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14032243511267609		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.14032243511267609 | validation: 0.16082681942132346]
	TIME [epoch: 32.1 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12264888939611518		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.12264888939611518 | validation: 0.12317310321571116]
	TIME [epoch: 32.1 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10994133540244058		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.10994133540244058 | validation: 0.08889147246718127]
	TIME [epoch: 32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_506.pth
	Model improved!!!
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10377428398274624		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.10377428398274624 | validation: 0.11597129082137672]
	TIME [epoch: 32 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10165393347443061		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.10165393347443061 | validation: 0.14377561198289507]
	TIME [epoch: 32 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12163443495783159		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.12163443495783159 | validation: 0.11251788556039388]
	TIME [epoch: 32 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11961436083425342		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.11961436083425342 | validation: 0.11110695098427797]
	TIME [epoch: 32 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11123892372033657		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.11123892372033657 | validation: 0.1096850932038246]
	TIME [epoch: 32 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11399950140335323		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.11399950140335323 | validation: 0.13194123980674108]
	TIME [epoch: 32 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11360919652875602		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.11360919652875602 | validation: 0.09794041673289494]
	TIME [epoch: 32 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1064434959822845		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.1064434959822845 | validation: 0.1097675266691591]
	TIME [epoch: 32 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09447959125113417		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.09447959125113417 | validation: 0.10578456997773897]
	TIME [epoch: 32 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10221461574595522		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.10221461574595522 | validation: 0.0916273620974782]
	TIME [epoch: 31.9 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08806797607438724		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.08806797607438724 | validation: 0.10627113735823278]
	TIME [epoch: 31.8 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10752310477852364		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.10752310477852364 | validation: 0.196524304595102]
	TIME [epoch: 31.8 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12762979660813722		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.12762979660813722 | validation: 0.12088072108409426]
	TIME [epoch: 31.9 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10489635655491933		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.10489635655491933 | validation: 0.09067886424251598]
	TIME [epoch: 31.8 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08903500443941298		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.08903500443941298 | validation: 0.09410010086513493]
	TIME [epoch: 31.8 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08685648011015079		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.08685648011015079 | validation: 0.12658902733575877]
	TIME [epoch: 31.8 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12378493034162581		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.12378493034162581 | validation: 0.09034921703847286]
	TIME [epoch: 31.8 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09569318552442989		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.09569318552442989 | validation: 0.11849209065237097]
	TIME [epoch: 31.8 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09212025884905149		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.09212025884905149 | validation: 0.13130205297286238]
	TIME [epoch: 31.8 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09645669391476683		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.09645669391476683 | validation: 0.11946568590361879]
	TIME [epoch: 31.8 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10989284126867357		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.10989284126867357 | validation: 0.10464130138596539]
	TIME [epoch: 31.8 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08002150469646621		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.08002150469646621 | validation: 0.08129513119184367]
	TIME [epoch: 31.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_528.pth
	Model improved!!!
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09304998792911336		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.09304998792911336 | validation: 0.08489125789800887]
	TIME [epoch: 31.9 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08455509700622683		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.08455509700622683 | validation: 0.09174218176240367]
	TIME [epoch: 31.9 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11264790859935302		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.11264790859935302 | validation: 0.0905882211552435]
	TIME [epoch: 32 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12208042520627499		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.12208042520627499 | validation: 0.1279795332603661]
	TIME [epoch: 32 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09910966483758385		[learning rate: 0.0018085]
	Learning Rate: 0.00180845
	LOSS [training: 0.09910966483758385 | validation: 0.08759630321695983]
	TIME [epoch: 31.9 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09941468474799647		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.09941468474799647 | validation: 0.0794488447750206]
	TIME [epoch: 32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_534.pth
	Model improved!!!
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0862696806852486		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.0862696806852486 | validation: 0.07444338137954334]
	TIME [epoch: 31.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_535.pth
	Model improved!!!
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07763432640676671		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.07763432640676671 | validation: 0.10080748173258289]
	TIME [epoch: 31.9 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10572952869110007		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.10572952869110007 | validation: 0.10657537401953994]
	TIME [epoch: 32 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0877215258002346		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.0877215258002346 | validation: 0.08849015793745546]
	TIME [epoch: 31.9 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0860326245909148		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.0860326245909148 | validation: 0.10678810643807857]
	TIME [epoch: 32 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07992905951672566		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.07992905951672566 | validation: 0.09273139340991146]
	TIME [epoch: 32 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09967250420768224		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.09967250420768224 | validation: 0.10273605599778961]
	TIME [epoch: 32 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08947382407157607		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.08947382407157607 | validation: 0.07437373475331355]
	TIME [epoch: 32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_542.pth
	Model improved!!!
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08071323546931695		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.08071323546931695 | validation: 0.07333701948420802]
	TIME [epoch: 32.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_543.pth
	Model improved!!!
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07779223653748073		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.07779223653748073 | validation: 0.09409660237142817]
	TIME [epoch: 32.1 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08837523814489107		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.08837523814489107 | validation: 0.09076502752641305]
	TIME [epoch: 32.1 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10782423768389028		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.10782423768389028 | validation: 0.10323346451207543]
	TIME [epoch: 32.1 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0946940588362929		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.0946940588362929 | validation: 0.12399694759366314]
	TIME [epoch: 32.1 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11456865351196627		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.11456865351196627 | validation: 0.09693882923642297]
	TIME [epoch: 32.1 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0728440800923518		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.0728440800923518 | validation: 0.07919654973653936]
	TIME [epoch: 32.1 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08093238052552837		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.08093238052552837 | validation: 0.10545890483440912]
	TIME [epoch: 32.1 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09592056450290123		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.09592056450290123 | validation: 0.09874337246638265]
	TIME [epoch: 32 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08622384141508066		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.08622384141508066 | validation: 0.09126319618555867]
	TIME [epoch: 32.1 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07813377022442147		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.07813377022442147 | validation: 0.07664838474600802]
	TIME [epoch: 32.1 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08031195923368184		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.08031195923368184 | validation: 0.08376864126728321]
	TIME [epoch: 32.1 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07661310793876408		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.07661310793876408 | validation: 0.07449625708070043]
	TIME [epoch: 32 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07887557101476542		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.07887557101476542 | validation: 0.16503342526533896]
	TIME [epoch: 32 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11647943579799802		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.11647943579799802 | validation: 0.08081059398395593]
	TIME [epoch: 32 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08207113007373867		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.08207113007373867 | validation: 0.06811817724110891]
	TIME [epoch: 32.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_558.pth
	Model improved!!!
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07680046665108528		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.07680046665108528 | validation: 0.07970257302447702]
	TIME [epoch: 32.1 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07997960821684624		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.07997960821684624 | validation: 0.07945331864311271]
	TIME [epoch: 32 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07668287159264763		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.07668287159264763 | validation: 0.06905337193030162]
	TIME [epoch: 32 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0730370959659702		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.0730370959659702 | validation: 0.08702889506017208]
	TIME [epoch: 32.1 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08080370194993726		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.08080370194993726 | validation: 0.08171281173376496]
	TIME [epoch: 32.2 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10460768438370588		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.10460768438370588 | validation: 0.08366591784307366]
	TIME [epoch: 32.1 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07246170191579031		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.07246170191579031 | validation: 0.07416589689313709]
	TIME [epoch: 32 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06470788388558804		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.06470788388558804 | validation: 0.09801512490866204]
	TIME [epoch: 32.1 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09954407620231365		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.09954407620231365 | validation: 0.1435726831135268]
	TIME [epoch: 32.1 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12610356340260304		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.12610356340260304 | validation: 0.07233764967151654]
	TIME [epoch: 32.1 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07386686359273865		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.07386686359273865 | validation: 0.07774509399715515]
	TIME [epoch: 32 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08196849474901205		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.08196849474901205 | validation: 0.0819488117955299]
	TIME [epoch: 32 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07448079053198133		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.07448079053198133 | validation: 0.0899026727786907]
	TIME [epoch: 32.1 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08452142586269099		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.08452142586269099 | validation: 0.07085878584415414]
	TIME [epoch: 32.1 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08024465362529651		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.08024465362529651 | validation: 0.07153806215023348]
	TIME [epoch: 32.1 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07200708676555805		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.07200708676555805 | validation: 0.12611097574569685]
	TIME [epoch: 32.1 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10198000693600542		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.10198000693600542 | validation: 0.06803619759656018]
	TIME [epoch: 32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_575.pth
	Model improved!!!
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07569272505023254		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.07569272505023254 | validation: 0.08489043097038637]
	TIME [epoch: 32.1 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07015339143020065		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.07015339143020065 | validation: 0.07807680228522487]
	TIME [epoch: 32.1 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07134955883545203		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.07134955883545203 | validation: 0.0658456902855004]
	TIME [epoch: 32.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_578.pth
	Model improved!!!
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06597572508925817		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.06597572508925817 | validation: 0.10018817398377985]
	TIME [epoch: 32 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07845699626055298		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.07845699626055298 | validation: 0.07340956660666377]
	TIME [epoch: 32 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06480701503536629		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.06480701503536629 | validation: 0.0798861609546754]
	TIME [epoch: 32.2 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08187063158854944		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.08187063158854944 | validation: 0.10712136329710713]
	TIME [epoch: 32.1 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0781253097407589		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.0781253097407589 | validation: 0.0753803490676872]
	TIME [epoch: 32 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11021169515186419		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.11021169515186419 | validation: 0.07458616743948232]
	TIME [epoch: 32.2 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06728725021485608		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.06728725021485608 | validation: 0.11140470796435092]
	TIME [epoch: 32 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07528426589004697		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.07528426589004697 | validation: 0.07867006516739428]
	TIME [epoch: 32.1 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06701029591515355		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.06701029591515355 | validation: 0.08478008519306554]
	TIME [epoch: 32.1 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08092010283826089		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.08092010283826089 | validation: 0.0633895624380296]
	TIME [epoch: 32.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_588.pth
	Model improved!!!
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06279984289168604		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.06279984289168604 | validation: 0.07095046909781064]
	TIME [epoch: 32.1 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10288431068953915		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.10288431068953915 | validation: 0.07551282287867762]
	TIME [epoch: 32 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07587010079594392		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.07587010079594392 | validation: 0.09671290073713637]
	TIME [epoch: 32.1 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07196645290324168		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.07196645290324168 | validation: 0.07033783824002722]
	TIME [epoch: 32.1 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06564534371028329		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.06564534371028329 | validation: 0.06884095366019387]
	TIME [epoch: 32.1 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0652498156759926		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.0652498156759926 | validation: 0.08151800075890883]
	TIME [epoch: 32 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09012320051784764		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.09012320051784764 | validation: 0.08567473817115184]
	TIME [epoch: 32 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07875382410817802		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.07875382410817802 | validation: 0.06251567062413102]
	TIME [epoch: 32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_596.pth
	Model improved!!!
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06731041723397507		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.06731041723397507 | validation: 0.08981971631019053]
	TIME [epoch: 32 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07310203905697947		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.07310203905697947 | validation: 0.06522461602319388]
	TIME [epoch: 31.9 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07836921072902728		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.07836921072902728 | validation: 0.08413724841459612]
	TIME [epoch: 31.8 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07982047083890326		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.07982047083890326 | validation: 0.06379838787948143]
	TIME [epoch: 32 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05591705035175864		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.05591705035175864 | validation: 0.06440454000075044]
	TIME [epoch: 32 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05703770501082033		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.05703770501082033 | validation: 0.06373292165233133]
	TIME [epoch: 31.9 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06954338346664385		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.06954338346664385 | validation: 0.06176074874124014]
	TIME [epoch: 32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_603.pth
	Model improved!!!
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07991279371843259		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.07991279371843259 | validation: 0.06326271681880838]
	TIME [epoch: 31.8 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.064617545904822		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.064617545904822 | validation: 0.07936764993816528]
	TIME [epoch: 31.9 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0884883539203239		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.0884883539203239 | validation: 0.10416296258646647]
	TIME [epoch: 31.9 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07378524298683106		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.07378524298683106 | validation: 0.06383278625202492]
	TIME [epoch: 32 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06996353507347418		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.06996353507347418 | validation: 0.06018557365091469]
	TIME [epoch: 31.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_608.pth
	Model improved!!!
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06670712022444197		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.06670712022444197 | validation: 0.06101867813272624]
	TIME [epoch: 32 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05685560063938736		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.05685560063938736 | validation: 0.058557056577905345]
	TIME [epoch: 32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_610.pth
	Model improved!!!
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06933817196560045		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.06933817196560045 | validation: 0.12877528260161852]
	TIME [epoch: 31.9 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1011427982871823		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.1011427982871823 | validation: 0.06812856951249927]
	TIME [epoch: 32 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06975255623378909		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.06975255623378909 | validation: 0.06347185043722245]
	TIME [epoch: 32 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11883045112454348		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.11883045112454348 | validation: 0.1507864518356713]
	TIME [epoch: 32 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11669314183215646		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.11669314183215646 | validation: 0.0765793573360912]
	TIME [epoch: 31.9 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0642416586905007		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.0642416586905007 | validation: 0.07033845207183728]
	TIME [epoch: 32 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05685410497632609		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.05685410497632609 | validation: 0.07181710638454676]
	TIME [epoch: 32 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07097467339211475		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.07097467339211475 | validation: 0.10953987304770131]
	TIME [epoch: 32.1 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08757407699980617		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.08757407699980617 | validation: 0.13222506119734956]
	TIME [epoch: 32 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11305025975767502		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.11305025975767502 | validation: 0.07410401938166647]
	TIME [epoch: 32 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0766230221212001		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.0766230221212001 | validation: 0.09162645729055449]
	TIME [epoch: 31.9 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06817350260535575		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.06817350260535575 | validation: 0.06110719202490915]
	TIME [epoch: 32 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05939329173669609		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.05939329173669609 | validation: 0.07324580754049267]
	TIME [epoch: 32 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05888410257878891		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.05888410257878891 | validation: 0.06570433259345168]
	TIME [epoch: 31.9 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06711579048270179		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.06711579048270179 | validation: 0.1066532321286581]
	TIME [epoch: 31.9 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22164823981915527		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.22164823981915527 | validation: 0.1394235670256359]
	TIME [epoch: 32 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1143937258642095		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.1143937258642095 | validation: 0.06460293634315481]
	TIME [epoch: 32 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09110032014310679		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.09110032014310679 | validation: 0.06820172284602496]
	TIME [epoch: 32 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06561429199091087		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.06561429199091087 | validation: 0.05815008853646564]
	TIME [epoch: 32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_629.pth
	Model improved!!!
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054063574813244655		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.054063574813244655 | validation: 0.05183468859053236]
	TIME [epoch: 31.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_630.pth
	Model improved!!!
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06224229540912647		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.06224229540912647 | validation: 0.08391771335455235]
	TIME [epoch: 32.1 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060460840698327994		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.060460840698327994 | validation: 0.06227537175513951]
	TIME [epoch: 32.1 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054552185608657405		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.054552185608657405 | validation: 0.062215247145649105]
	TIME [epoch: 31.9 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0799207535956289		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.0799207535956289 | validation: 0.1071533248212491]
	TIME [epoch: 32 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06624426759861551		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.06624426759861551 | validation: 0.061154148528703184]
	TIME [epoch: 32.1 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06548633186532876		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.06548633186532876 | validation: 0.07084761433278575]
	TIME [epoch: 32 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0593944195655165		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.0593944195655165 | validation: 0.060807300589827605]
	TIME [epoch: 31.9 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055362868674788436		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.055362868674788436 | validation: 0.09093034831735974]
	TIME [epoch: 32 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06668480662681169		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.06668480662681169 | validation: 0.061049684987647644]
	TIME [epoch: 32 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07113307184223873		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.07113307184223873 | validation: 0.0759298230045832]
	TIME [epoch: 31.9 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06640289770779029		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.06640289770779029 | validation: 0.05862041965291662]
	TIME [epoch: 32 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05804989379542235		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.05804989379542235 | validation: 0.1121468844957377]
	TIME [epoch: 32 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07766306704411698		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.07766306704411698 | validation: 0.06028616694121286]
	TIME [epoch: 32 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05618628715449573		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.05618628715449573 | validation: 0.07223095920058381]
	TIME [epoch: 32.1 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06294300612803366		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.06294300612803366 | validation: 0.09986510196281229]
	TIME [epoch: 32.1 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06955269027981315		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.06955269027981315 | validation: 0.061213268190413235]
	TIME [epoch: 31.9 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05105892310761706		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.05105892310761706 | validation: 0.06674410449284521]
	TIME [epoch: 32 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06095402040691732		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.06095402040691732 | validation: 0.06303505538797696]
	TIME [epoch: 31.9 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05373797729451292		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.05373797729451292 | validation: 0.05943892312456581]
	TIME [epoch: 32 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05864430566007152		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.05864430566007152 | validation: 0.08404146144719826]
	TIME [epoch: 31.9 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06316805739284612		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.06316805739284612 | validation: 0.056007964212664255]
	TIME [epoch: 31.9 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05972210285936389		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.05972210285936389 | validation: 0.13087377267479017]
	TIME [epoch: 32 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08172745705895117		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.08172745705895117 | validation: 0.0643177537933225]
	TIME [epoch: 32 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054791174669466314		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.054791174669466314 | validation: 0.0786840461531895]
	TIME [epoch: 32 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05645422894589948		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.05645422894589948 | validation: 0.06102553366083374]
	TIME [epoch: 32 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05395524059046259		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.05395524059046259 | validation: 0.06202302626035383]
	TIME [epoch: 31.9 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05158608840411622		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.05158608840411622 | validation: 0.057337337667808325]
	TIME [epoch: 32.1 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0632442190925438		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.0632442190925438 | validation: 0.05758415698452085]
	TIME [epoch: 32 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055888848569966475		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.055888848569966475 | validation: 0.06311455927109158]
	TIME [epoch: 32 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07369803828146954		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.07369803828146954 | validation: 0.06461357071723794]
	TIME [epoch: 32 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05053279819499192		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.05053279819499192 | validation: 0.06571576341723867]
	TIME [epoch: 32.1 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051999577338137926		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.051999577338137926 | validation: 0.07881799023310657]
	TIME [epoch: 32 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05570161302243662		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.05570161302243662 | validation: 0.06107408797158323]
	TIME [epoch: 32 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.049982944822846105		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.049982944822846105 | validation: 0.051816329254433746]
	TIME [epoch: 32.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_664.pth
	Model improved!!!
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05542186697598234		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.05542186697598234 | validation: 0.06586848863532106]
	TIME [epoch: 32 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07330480556770748		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.07330480556770748 | validation: 0.07636441203707235]
	TIME [epoch: 31.9 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05605398780778746		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.05605398780778746 | validation: 0.06694422845442577]
	TIME [epoch: 32.1 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058528441960881736		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.058528441960881736 | validation: 0.06763437116561369]
	TIME [epoch: 32.1 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059335212626774075		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.059335212626774075 | validation: 0.05230912377694963]
	TIME [epoch: 32 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05392658525891156		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.05392658525891156 | validation: 0.06157376418375136]
	TIME [epoch: 32 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05010027811379264		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.05010027811379264 | validation: 0.059926476671835434]
	TIME [epoch: 32 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05405217309487492		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.05405217309487492 | validation: 0.0571724454526244]
	TIME [epoch: 32 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04736528589966651		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.04736528589966651 | validation: 0.055603519997057635]
	TIME [epoch: 32.1 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0652436407842635		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.0652436407842635 | validation: 0.08105768878695546]
	TIME [epoch: 32 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06025487591619338		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.06025487591619338 | validation: 0.05319622258237878]
	TIME [epoch: 32 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.049800679882300185		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.049800679882300185 | validation: 0.05025709123144146]
	TIME [epoch: 31.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_676.pth
	Model improved!!!
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05832559666081659		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.05832559666081659 | validation: 0.054479981647600546]
	TIME [epoch: 32 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06520136466250513		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.06520136466250513 | validation: 0.05442562615410711]
	TIME [epoch: 32 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0474306142700001		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.0474306142700001 | validation: 0.06394965186218554]
	TIME [epoch: 31.9 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05156452487062276		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.05156452487062276 | validation: 0.057246511272848286]
	TIME [epoch: 32 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05799973608174877		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.05799973608174877 | validation: 0.06128650179952093]
	TIME [epoch: 32 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05984831128098767		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.05984831128098767 | validation: 0.06407168438576726]
	TIME [epoch: 31.9 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0498330603061263		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.0498330603061263 | validation: 0.05638703410881618]
	TIME [epoch: 32 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0477101492948061		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.0477101492948061 | validation: 0.12448363625275821]
	TIME [epoch: 32 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07116266216688798		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.07116266216688798 | validation: 0.0771629964751701]
	TIME [epoch: 31.9 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05675817868547377		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.05675817868547377 | validation: 0.05604485406323616]
	TIME [epoch: 32 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04940214643764934		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.04940214643764934 | validation: 0.06351807203739808]
	TIME [epoch: 32 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05284214145710256		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.05284214145710256 | validation: 0.05401159185144473]
	TIME [epoch: 32 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05124123166233711		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.05124123166233711 | validation: 0.05899052515546185]
	TIME [epoch: 32 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05808134462865263		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.05808134462865263 | validation: 0.10155025712035291]
	TIME [epoch: 32.1 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05574193960899525		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.05574193960899525 | validation: 0.051729349239138044]
	TIME [epoch: 32 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051941327605042276		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.051941327605042276 | validation: 0.050787518092186006]
	TIME [epoch: 32 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04928083887721592		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.04928083887721592 | validation: 0.05433534592705924]
	TIME [epoch: 32 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04827713135939239		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.04827713135939239 | validation: 0.0522416798799101]
	TIME [epoch: 32.1 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047908737621494724		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.047908737621494724 | validation: 0.09162413552123072]
	TIME [epoch: 31.9 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05888988862587812		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.05888988862587812 | validation: 0.05675131120807417]
	TIME [epoch: 31.9 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09337813695454301		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.09337813695454301 | validation: 0.07731055784718499]
	TIME [epoch: 32 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05882933834828647		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.05882933834828647 | validation: 0.05045413590322597]
	TIME [epoch: 31.8 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04678156712150772		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.04678156712150772 | validation: 0.05066219161168195]
	TIME [epoch: 32.1 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0557221347514605		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.0557221347514605 | validation: 0.0527774793778948]
	TIME [epoch: 32.1 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04933685473046876		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.04933685473046876 | validation: 0.05437688761281659]
	TIME [epoch: 32.1 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04987878831408084		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.04987878831408084 | validation: 0.05885928249691037]
	TIME [epoch: 32.2 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04578886569095799		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.04578886569095799 | validation: 0.05097349221249384]
	TIME [epoch: 32.2 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045180863153528456		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.045180863153528456 | validation: 0.04994742887176936]
	TIME [epoch: 32.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_704.pth
	Model improved!!!
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05043789006707926		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.05043789006707926 | validation: 0.05169831893080474]
	TIME [epoch: 32.1 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050749768903350194		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.050749768903350194 | validation: 0.06852315100889993]
	TIME [epoch: 32.1 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061796261461978766		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.061796261461978766 | validation: 0.06124259020960501]
	TIME [epoch: 32.1 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05029833423799621		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.05029833423799621 | validation: 0.0509971411178165]
	TIME [epoch: 32.1 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04616579233334401		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.04616579233334401 | validation: 0.05388342733675329]
	TIME [epoch: 32.1 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04828887667512602		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.04828887667512602 | validation: 0.05642020154145777]
	TIME [epoch: 32 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05353112934715496		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.05353112934715496 | validation: 0.05049767285474377]
	TIME [epoch: 31.9 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05910337115620963		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.05910337115620963 | validation: 0.05689636000036782]
	TIME [epoch: 31.9 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08552620823827493		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.08552620823827493 | validation: 0.0801547557628608]
	TIME [epoch: 32 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056215824472777925		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.056215824472777925 | validation: 0.04792027184024156]
	TIME [epoch: 31.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_714.pth
	Model improved!!!
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04933906879994754		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.04933906879994754 | validation: 0.09267668111262281]
	TIME [epoch: 31.9 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.126242194024238		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.126242194024238 | validation: 0.052981937199115384]
	TIME [epoch: 32 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05502460591422663		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.05502460591422663 | validation: 0.06116254132584702]
	TIME [epoch: 32 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051064615104175204		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.051064615104175204 | validation: 0.046163268163537614]
	TIME [epoch: 32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_718.pth
	Model improved!!!
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04218820681245898		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.04218820681245898 | validation: 0.04669125239581936]
	TIME [epoch: 31.9 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04265269046481283		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.04265269046481283 | validation: 0.049482974757552284]
	TIME [epoch: 31.9 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04979446738284873		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.04979446738284873 | validation: 0.06312592908163756]
	TIME [epoch: 31.9 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04773835707143123		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.04773835707143123 | validation: 0.04665566107043398]
	TIME [epoch: 31.9 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04673811144406878		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.04673811144406878 | validation: 0.04653026545196899]
	TIME [epoch: 31.9 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04274107313372346		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.04274107313372346 | validation: 0.053952254163535954]
	TIME [epoch: 31.9 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05145897469777589		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.05145897469777589 | validation: 0.04863587024386455]
	TIME [epoch: 31.9 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04564838171714777		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.04564838171714777 | validation: 0.054432353703553316]
	TIME [epoch: 31.9 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04423550406930442		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.04423550406930442 | validation: 0.048846636431097976]
	TIME [epoch: 32 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04241631528280333		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.04241631528280333 | validation: 0.060562529770668394]
	TIME [epoch: 32 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06021001293489285		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.06021001293489285 | validation: 0.07330458163165014]
	TIME [epoch: 32 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053079553901702756		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.053079553901702756 | validation: 0.058022461041928916]
	TIME [epoch: 32 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04307657313160696		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.04307657313160696 | validation: 0.0458721514780906]
	TIME [epoch: 32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_731.pth
	Model improved!!!
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04659431240702498		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.04659431240702498 | validation: 0.05410369380216692]
	TIME [epoch: 31.8 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04563314624280647		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.04563314624280647 | validation: 0.0662665421727204]
	TIME [epoch: 31.8 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04720553235676819		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.04720553235676819 | validation: 0.04749964576773344]
	TIME [epoch: 31.9 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04828050170569795		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.04828050170569795 | validation: 0.04865863374484211]
	TIME [epoch: 31.9 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045114130438563674		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.045114130438563674 | validation: 0.05263905628602657]
	TIME [epoch: 31.9 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04101523164187704		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.04101523164187704 | validation: 0.0483615249590408]
	TIME [epoch: 31.8 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04418165791438556		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.04418165791438556 | validation: 0.051605136093799936]
	TIME [epoch: 31.9 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04454589218983537		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.04454589218983537 | validation: 0.07366784647507216]
	TIME [epoch: 31.9 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05795647046506375		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.05795647046506375 | validation: 0.050974953125558525]
	TIME [epoch: 31.8 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04295997203431085		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.04295997203431085 | validation: 0.051603716869012675]
	TIME [epoch: 31.8 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04937538373547272		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.04937538373547272 | validation: 0.049314061781137966]
	TIME [epoch: 31.9 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040668308602957746		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.040668308602957746 | validation: 0.04901974906174652]
	TIME [epoch: 32 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04709891123038065		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.04709891123038065 | validation: 0.06081448987257673]
	TIME [epoch: 31.9 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04869630282634052		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.04869630282634052 | validation: 0.04973397580454343]
	TIME [epoch: 31.9 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0476446961313432		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.0476446961313432 | validation: 0.05437808987459872]
	TIME [epoch: 31.8 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04505742704370633		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.04505742704370633 | validation: 0.0613425517019013]
	TIME [epoch: 32 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04614706961457613		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.04614706961457613 | validation: 0.056314813140385875]
	TIME [epoch: 31.9 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044267087862197574		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.044267087862197574 | validation: 0.05339752676800584]
	TIME [epoch: 32 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046157604059884255		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.046157604059884255 | validation: 0.048183463141684424]
	TIME [epoch: 31.9 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04030383668631859		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.04030383668631859 | validation: 0.051529917531482054]
	TIME [epoch: 32 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04218715529792895		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.04218715529792895 | validation: 0.049395802590057616]
	TIME [epoch: 31.9 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04005023879091076		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.04005023879091076 | validation: 0.07589497371495475]
	TIME [epoch: 32 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0514865409500173		[learning rate: 0.00082662]
	Learning Rate: 0.000826623
	LOSS [training: 0.0514865409500173 | validation: 0.04603984248937094]
	TIME [epoch: 31.9 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04160453197649988		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.04160453197649988 | validation: 0.04765078501742038]
	TIME [epoch: 32 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04161016410362772		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.04161016410362772 | validation: 0.045951265491437486]
	TIME [epoch: 31.9 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0442490004698827		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.0442490004698827 | validation: 0.049932434381726495]
	TIME [epoch: 32 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04218832317142104		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.04218832317142104 | validation: 0.05052941072461816]
	TIME [epoch: 32 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04898635854246665		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.04898635854246665 | validation: 0.09596148034566171]
	TIME [epoch: 31.9 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06766899656693819		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.06766899656693819 | validation: 0.05374727576527884]
	TIME [epoch: 32 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041945334260968234		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.041945334260968234 | validation: 0.04418456201794567]
	TIME [epoch: 32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_761.pth
	Model improved!!!
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04925137281796374		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.04925137281796374 | validation: 0.043846002879224195]
	TIME [epoch: 32.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_762.pth
	Model improved!!!
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04534038411638504		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.04534038411638504 | validation: 0.05412607203534928]
	TIME [epoch: 32.1 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04589612502586176		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.04589612502586176 | validation: 0.04582214418438944]
	TIME [epoch: 32 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039321874049364054		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.039321874049364054 | validation: 0.053158229272984484]
	TIME [epoch: 32 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04532761074366608		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.04532761074366608 | validation: 0.04866539149062667]
	TIME [epoch: 32 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042820183897694084		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.042820183897694084 | validation: 0.04326912444850642]
	TIME [epoch: 32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_767.pth
	Model improved!!!
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046270207033106504		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.046270207033106504 | validation: 0.06334953416544724]
	TIME [epoch: 32 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048343555042563664		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.048343555042563664 | validation: 0.04402273585447799]
	TIME [epoch: 32 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037632619368524564		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.037632619368524564 | validation: 0.07331530546335725]
	TIME [epoch: 32.1 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04515004219359021		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.04515004219359021 | validation: 0.046144604270218506]
	TIME [epoch: 32.1 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03879523704934421		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.03879523704934421 | validation: 0.06726740301911101]
	TIME [epoch: 32 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05474760724284286		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.05474760724284286 | validation: 0.05191883486770686]
	TIME [epoch: 32 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04302332936171687		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.04302332936171687 | validation: 0.04610446656499678]
	TIME [epoch: 32.1 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04137663038670242		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.04137663038670242 | validation: 0.044863088545900945]
	TIME [epoch: 32.1 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04741466224712273		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.04741466224712273 | validation: 0.0515966660017317]
	TIME [epoch: 32.1 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03901167319494909		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.03901167319494909 | validation: 0.04137592993346464]
	TIME [epoch: 32.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_777.pth
	Model improved!!!
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041738698270830085		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.041738698270830085 | validation: 0.06354687521644956]
	TIME [epoch: 32.1 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05498612110695305		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.05498612110695305 | validation: 0.06186117053921877]
	TIME [epoch: 32.2 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05053597693960236		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.05053597693960236 | validation: 0.05062767992538364]
	TIME [epoch: 32.1 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04175198745147193		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.04175198745147193 | validation: 0.04468229930282534]
	TIME [epoch: 32.1 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04497327941902115		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.04497327941902115 | validation: 0.04265861515440513]
	TIME [epoch: 32.1 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04259912903469635		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.04259912903469635 | validation: 0.05364368637663432]
	TIME [epoch: 32.1 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0640275104798282		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.0640275104798282 | validation: 0.0461662236449564]
	TIME [epoch: 32.1 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04086682542967683		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.04086682542967683 | validation: 0.040497054975867086]
	TIME [epoch: 32.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_785.pth
	Model improved!!!
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03662587077361862		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.03662587077361862 | validation: 0.04327748250123615]
	TIME [epoch: 32.1 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04231868934597187		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.04231868934597187 | validation: 0.04286770805753113]
	TIME [epoch: 32 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0417611646254821		[learning rate: 0.00073282]
	Learning Rate: 0.000732824
	LOSS [training: 0.0417611646254821 | validation: 0.04513053354573131]
	TIME [epoch: 32.1 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038393329945872495		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.038393329945872495 | validation: 0.04150420363969988]
	TIME [epoch: 32.1 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03817066562081175		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.03817066562081175 | validation: 0.05963465463791695]
	TIME [epoch: 32.1 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04381269014664073		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.04381269014664073 | validation: 0.06200531026944882]
	TIME [epoch: 32.1 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04554261204496675		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.04554261204496675 | validation: 0.06336943095840145]
	TIME [epoch: 32.1 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04293475999437496		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.04293475999437496 | validation: 0.04294834744707336]
	TIME [epoch: 32.1 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03856255387702891		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.03856255387702891 | validation: 0.04446688200251933]
	TIME [epoch: 32.1 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03876538446392072		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.03876538446392072 | validation: 0.04256213960474546]
	TIME [epoch: 32.1 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03736351003630464		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.03736351003630464 | validation: 0.06386866988368131]
	TIME [epoch: 32.1 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04383523448096743		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.04383523448096743 | validation: 0.05064015188601946]
	TIME [epoch: 32.1 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03691236903886387		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.03691236903886387 | validation: 0.047244271434960926]
	TIME [epoch: 32.2 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04946685195347139		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.04946685195347139 | validation: 0.0896878809349444]
	TIME [epoch: 32.1 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05759405177626409		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.05759405177626409 | validation: 0.04458790987186839]
	TIME [epoch: 32.1 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058729788024696215		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.058729788024696215 | validation: 0.05869051957070885]
	TIME [epoch: 32.2 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04239155498304157		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.04239155498304157 | validation: 0.04298253079181077]
	TIME [epoch: 32.2 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037129758659054955		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.037129758659054955 | validation: 0.05804098914836009]
	TIME [epoch: 32.1 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04242090697265144		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.04242090697265144 | validation: 0.043670464436061346]
	TIME [epoch: 32.1 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03676565488966177		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.03676565488966177 | validation: 0.046711080185901525]
	TIME [epoch: 32.1 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04234000234020484		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.04234000234020484 | validation: 0.04970267316833738]
	TIME [epoch: 32.1 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039599654333373986		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.039599654333373986 | validation: 0.040993310828400006]
	TIME [epoch: 32.2 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03690852487898985		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.03690852487898985 | validation: 0.04234637423341697]
	TIME [epoch: 32.2 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03594722781135049		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.03594722781135049 | validation: 0.045009323739444836]
	TIME [epoch: 32.2 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037786112464036965		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.037786112464036965 | validation: 0.04806650250510662]
	TIME [epoch: 32.1 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047557242308475696		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.047557242308475696 | validation: 0.04980807229696957]
	TIME [epoch: 32.2 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044327235341603106		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.044327235341603106 | validation: 0.040812737797504786]
	TIME [epoch: 32.2 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03620371732700013		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.03620371732700013 | validation: 0.04392087303524675]
	TIME [epoch: 33.3 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04200942781891753		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.04200942781891753 | validation: 0.05967228843737561]
	TIME [epoch: 32.1 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046824857974566474		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.046824857974566474 | validation: 0.0670541286724829]
	TIME [epoch: 32.2 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04045125421459998		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.04045125421459998 | validation: 0.04099869716038028]
	TIME [epoch: 32.2 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03605574881938341		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.03605574881938341 | validation: 0.05210602173081694]
	TIME [epoch: 32.2 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038584650841777805		[learning rate: 0.00065894]
	Learning Rate: 0.000658941
	LOSS [training: 0.038584650841777805 | validation: 0.04083899060693086]
	TIME [epoch: 32.2 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04369826658147855		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.04369826658147855 | validation: 0.042853341298468905]
	TIME [epoch: 32.1 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04041736853623101		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.04041736853623101 | validation: 0.058853542738378714]
	TIME [epoch: 32.2 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042449422516577906		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.042449422516577906 | validation: 0.04211231927523026]
	TIME [epoch: 32.1 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034932489257733866		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.034932489257733866 | validation: 0.039528843484938]
	TIME [epoch: 32.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_822.pth
	Model improved!!!
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03535311855315959		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.03535311855315959 | validation: 0.044371000538871205]
	TIME [epoch: 32.1 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04093330893135086		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.04093330893135086 | validation: 0.044834579297568344]
	TIME [epoch: 32.2 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04275208111098661		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.04275208111098661 | validation: 0.04324960243459229]
	TIME [epoch: 32.1 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04150847369959693		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.04150847369959693 | validation: 0.04164295666193105]
	TIME [epoch: 32.1 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03606129360807046		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.03606129360807046 | validation: 0.053773346013504864]
	TIME [epoch: 32.1 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04144190056514458		[learning rate: 0.00063601]
	Learning Rate: 0.000636006
	LOSS [training: 0.04144190056514458 | validation: 0.04427996963749921]
	TIME [epoch: 32.1 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03906981448308696		[learning rate: 0.00063376]
	Learning Rate: 0.000633757
	LOSS [training: 0.03906981448308696 | validation: 0.06746270889844888]
	TIME [epoch: 32.1 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046055174179645164		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.046055174179645164 | validation: 0.05021790619158937]
	TIME [epoch: 32.1 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035754570218405515		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.035754570218405515 | validation: 0.041690117299209566]
	TIME [epoch: 32.1 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034341692744023676		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.034341692744023676 | validation: 0.04394311568864685]
	TIME [epoch: 32 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041591525490710816		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.041591525490710816 | validation: 0.049119971140918525]
	TIME [epoch: 32.1 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041301994897607344		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.041301994897607344 | validation: 0.06994740632298425]
	TIME [epoch: 32.1 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042628822211558934		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.042628822211558934 | validation: 0.043715651537804406]
	TIME [epoch: 32.1 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035695357231207495		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.035695357231207495 | validation: 0.07126578699799982]
	TIME [epoch: 32.2 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04943154657523864		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.04943154657523864 | validation: 0.04682943099151379]
	TIME [epoch: 32.1 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035143589873226015		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.035143589873226015 | validation: 0.044157775695532954]
	TIME [epoch: 32.2 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03569025241861639		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.03569025241861639 | validation: 0.0563712522267324]
	TIME [epoch: 32.2 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037468948243791884		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.037468948243791884 | validation: 0.04863762652468119]
	TIME [epoch: 32.2 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035064954305724025		[learning rate: 0.00060738]
	Learning Rate: 0.000607381
	LOSS [training: 0.035064954305724025 | validation: 0.040762621905425414]
	TIME [epoch: 32.1 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05007772304276941		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.05007772304276941 | validation: 0.04352869488048561]
	TIME [epoch: 32.1 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03923686428167952		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.03923686428167952 | validation: 0.04996027508990771]
	TIME [epoch: 32.1 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036587871949853815		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.036587871949853815 | validation: 0.04236408892420608]
	TIME [epoch: 32.2 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03402978863293347		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.03402978863293347 | validation: 0.04215204453177612]
	TIME [epoch: 32.1 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03592798946143778		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.03592798946143778 | validation: 0.04003055775745888]
	TIME [epoch: 32.1 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03473801747396546		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.03473801747396546 | validation: 0.04539766436451466]
	TIME [epoch: 32.1 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0384433559853972		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.0384433559853972 | validation: 0.04281374026466696]
	TIME [epoch: 32.2 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03359174919105988		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.03359174919105988 | validation: 0.037493617076695226]
	TIME [epoch: 32.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_849.pth
	Model improved!!!
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039478734830969454		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.039478734830969454 | validation: 0.04512929467468163]
	TIME [epoch: 32.1 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03758381125917434		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.03758381125917434 | validation: 0.05473139175165573]
	TIME [epoch: 32.1 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03848928467406337		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.03848928467406337 | validation: 0.045102064564185244]
	TIME [epoch: 32.2 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03714375111931349		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.03714375111931349 | validation: 0.04185692773075608]
	TIME [epoch: 32.2 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03789222613517475		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.03789222613517475 | validation: 0.044132240254607394]
	TIME [epoch: 32.3 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04376900738877165		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.04376900738877165 | validation: 0.03956660863816938]
	TIME [epoch: 32.1 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03534009089675682		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.03534009089675682 | validation: 0.043735826039614234]
	TIME [epoch: 32.1 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03413459411057717		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.03413459411057717 | validation: 0.037456446090956934]
	TIME [epoch: 32.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_857.pth
	Model improved!!!
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03364371778698552		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.03364371778698552 | validation: 0.04175345850908867]
	TIME [epoch: 32.2 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034916885862074254		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.034916885862074254 | validation: 0.03766784555307935]
	TIME [epoch: 32.1 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07232233416647565		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.07232233416647565 | validation: 0.06809771830084273]
	TIME [epoch: 32.1 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04661936337331981		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.04661936337331981 | validation: 0.04020952887220038]
	TIME [epoch: 32.2 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035771926489492456		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.035771926489492456 | validation: 0.03891822952717476]
	TIME [epoch: 32.1 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033485030801473775		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.033485030801473775 | validation: 0.035746352145322804]
	TIME [epoch: 32.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_863.pth
	Model improved!!!
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033880545482993636		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.033880545482993636 | validation: 0.03631231210851642]
	TIME [epoch: 32.2 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03318918899380979		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.03318918899380979 | validation: 0.037970608145195706]
	TIME [epoch: 32.1 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03291478722621074		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.03291478722621074 | validation: 0.038293491508860994]
	TIME [epoch: 32.3 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03601328298102808		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.03601328298102808 | validation: 0.048150654753923855]
	TIME [epoch: 32.2 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0395449769431146		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.0395449769431146 | validation: 0.04002398953643284]
	TIME [epoch: 32.2 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0329396899019352		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.0329396899019352 | validation: 0.052545218601683624]
	TIME [epoch: 32.1 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03675308978999426		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.03675308978999426 | validation: 0.041098269673214054]
	TIME [epoch: 32.2 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03699578695592311		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.03699578695592311 | validation: 0.039062673428124015]
	TIME [epoch: 32.2 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041951178489985065		[learning rate: 0.00054421]
	Learning Rate: 0.000544213
	LOSS [training: 0.041951178489985065 | validation: 0.04038487848830967]
	TIME [epoch: 32.2 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033876061568520056		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.033876061568520056 | validation: 0.04376256425930473]
	TIME [epoch: 32.1 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05074232136763958		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.05074232136763958 | validation: 0.04359379509303893]
	TIME [epoch: 32.2 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03674676208800276		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.03674676208800276 | validation: 0.037472452197625176]
	TIME [epoch: 32.2 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033048156197444147		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.033048156197444147 | validation: 0.05316486392399813]
	TIME [epoch: 32.2 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03776440409666089		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.03776440409666089 | validation: 0.03900229404899684]
	TIME [epoch: 32.3 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0334076960592008		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.0334076960592008 | validation: 0.039012728913820606]
	TIME [epoch: 32.2 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03398523376117106		[learning rate: 0.00053088]
	Learning Rate: 0.000530884
	LOSS [training: 0.03398523376117106 | validation: 0.03784922207691971]
	TIME [epoch: 32.2 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03266598107372567		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.03266598107372567 | validation: 0.04645273569632163]
	TIME [epoch: 32.3 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04238881357309332		[learning rate: 0.00052714]
	Learning Rate: 0.000527136
	LOSS [training: 0.04238881357309332 | validation: 0.043726856849980004]
	TIME [epoch: 32.2 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034121400871278684		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.034121400871278684 | validation: 0.03935800683825952]
	TIME [epoch: 32.2 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03553455379029675		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.03553455379029675 | validation: 0.06460137374713446]
	TIME [epoch: 32.2 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04070923041541579		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.04070923041541579 | validation: 0.03898170376940838]
	TIME [epoch: 32.2 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03412499634763537		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.03412499634763537 | validation: 0.0403507564756309]
	TIME [epoch: 32.2 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03298935790555657		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.03298935790555657 | validation: 0.03715561094051828]
	TIME [epoch: 32.2 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039516802672228915		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.039516802672228915 | validation: 0.04021838143440926]
	TIME [epoch: 32.1 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03210159515291398		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.03210159515291398 | validation: 0.040732700230561456]
	TIME [epoch: 32.1 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03752421264515554		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.03752421264515554 | validation: 0.055901099445408095]
	TIME [epoch: 32.1 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03804016322737668		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.03804016322737668 | validation: 0.03702113597238613]
	TIME [epoch: 32.2 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03200477884351067		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.03200477884351067 | validation: 0.039676265682953446]
	TIME [epoch: 32.2 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03211019218820454		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.03211019218820454 | validation: 0.04226498147861762]
	TIME [epoch: 32.2 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035284837431924385		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.035284837431924385 | validation: 0.03675523682856469]
	TIME [epoch: 32.2 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040525390868431835		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.040525390868431835 | validation: 0.04219941813283566]
	TIME [epoch: 32.2 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03824043074692831		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.03824043074692831 | validation: 0.048729080667886504]
	TIME [epoch: 32.1 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0339815134296925		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.0339815134296925 | validation: 0.036242739473798365]
	TIME [epoch: 32.2 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03144008792848392		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.03144008792848392 | validation: 0.03790938721922448]
	TIME [epoch: 32.1 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032800249481980626		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.032800249481980626 | validation: 0.04034210073349196]
	TIME [epoch: 32.2 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03323440405887727		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.03323440405887727 | validation: 0.037744884707706206]
	TIME [epoch: 32.2 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03701751210297854		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.03701751210297854 | validation: 0.0396890683730724]
	TIME [epoch: 32.2 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037943588821519325		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.037943588821519325 | validation: 0.039047917123524435]
	TIME [epoch: 32.1 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032734234607986315		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.032734234607986315 | validation: 0.05775872747669679]
	TIME [epoch: 32.2 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037560682246991314		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.037560682246991314 | validation: 0.039481693057863525]
	TIME [epoch: 32.2 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033945052273114486		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.033945052273114486 | validation: 0.03939849925849844]
	TIME [epoch: 32.2 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03264213085565604		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.03264213085565604 | validation: 0.04088696605772461]
	TIME [epoch: 32 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039406893430775095		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.039406893430775095 | validation: 0.03860653519875023]
	TIME [epoch: 32.1 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030289674582915982		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.030289674582915982 | validation: 0.03827592086198448]
	TIME [epoch: 32.1 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032339335806795905		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.032339335806795905 | validation: 0.0365474169050716]
	TIME [epoch: 32.1 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031736511943244144		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.031736511943244144 | validation: 0.03663480771413815]
	TIME [epoch: 32.1 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031263704887159544		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.031263704887159544 | validation: 0.0354200560223346]
	TIME [epoch: 32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_910.pth
	Model improved!!!
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03323182885074075		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.03323182885074075 | validation: 0.04023675911260884]
	TIME [epoch: 32.1 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031840601057362		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.031840601057362 | validation: 0.03691852877884773]
	TIME [epoch: 32.2 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03464890451755248		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.03464890451755248 | validation: 0.03769504769495226]
	TIME [epoch: 32.1 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060468107787223876		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.060468107787223876 | validation: 0.04282631663421098]
	TIME [epoch: 32.2 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03520887244300988		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.03520887244300988 | validation: 0.03964579938632621]
	TIME [epoch: 32.2 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032382472797252704		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.032382472797252704 | validation: 0.03427320703419968]
	TIME [epoch: 32.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_916.pth
	Model improved!!!
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03448162144259372		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.03448162144259372 | validation: 0.03701322597327519]
	TIME [epoch: 32 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03248159613967634		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.03248159613967634 | validation: 0.037066402738099254]
	TIME [epoch: 32 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03159401438087665		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.03159401438087665 | validation: 0.037098248507696226]
	TIME [epoch: 32 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03224312490726588		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.03224312490726588 | validation: 0.03491442477283582]
	TIME [epoch: 32 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033020034109533594		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.033020034109533594 | validation: 0.04768012414157738]
	TIME [epoch: 32 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0378170214982534		[learning rate: 0.00045588]
	Learning Rate: 0.000455875
	LOSS [training: 0.0378170214982534 | validation: 0.036364393567338164]
	TIME [epoch: 32.1 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031584545682548856		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.031584545682548856 | validation: 0.034743878149600775]
	TIME [epoch: 32 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03139254233381812		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.03139254233381812 | validation: 0.04096585258256577]
	TIME [epoch: 32 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03396942585343861		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.03396942585343861 | validation: 0.03827166117346574]
	TIME [epoch: 32 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035665756880030014		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.035665756880030014 | validation: 0.03486826967954165]
	TIME [epoch: 32 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03245034704297888		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.03245034704297888 | validation: 0.03509925393799818]
	TIME [epoch: 32 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03164018373016945		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.03164018373016945 | validation: 0.03825431472263516]
	TIME [epoch: 32 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03782738330504956		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.03782738330504956 | validation: 0.04724337962925551]
	TIME [epoch: 32 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03263133800117281		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.03263133800117281 | validation: 0.03743168060327199]
	TIME [epoch: 32.1 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03167750549811591		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 0.03167750549811591 | validation: 0.036681461473241424]
	TIME [epoch: 32 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04647805394231656		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 0.04647805394231656 | validation: 0.03805148791440385]
	TIME [epoch: 32.1 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03347760439573056		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 0.03347760439573056 | validation: 0.03683077282432055]
	TIME [epoch: 32 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030298912049231772		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 0.030298912049231772 | validation: 0.04463355027130937]
	TIME [epoch: 32 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03191298934418573		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 0.03191298934418573 | validation: 0.03840397573941905]
	TIME [epoch: 32 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030905986904674852		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 0.030905986904674852 | validation: 0.038532155412968755]
	TIME [epoch: 32.1 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03064209873273461		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 0.03064209873273461 | validation: 0.036751576440457664]
	TIME [epoch: 32 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033146453843463305		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 0.033146453843463305 | validation: 0.038085293052475495]
	TIME [epoch: 32 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03214646090040382		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 0.03214646090040382 | validation: 0.038761975424840556]
	TIME [epoch: 32 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03228429653135114		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 0.03228429653135114 | validation: 0.038926394974840385]
	TIME [epoch: 32.1 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03314460103554487		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 0.03314460103554487 | validation: 0.03931576509018528]
	TIME [epoch: 32 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0329281577593568		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 0.0329281577593568 | validation: 0.051263726560832994]
	TIME [epoch: 32 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03488824962674447		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 0.03488824962674447 | validation: 0.03652903998334378]
	TIME [epoch: 32 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029624849519331715		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 0.029624849519331715 | validation: 0.03491987561833594]
	TIME [epoch: 32 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03174765545567447		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 0.03174765545567447 | validation: 0.05045429963573432]
	TIME [epoch: 32.1 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04130134460473822		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 0.04130134460473822 | validation: 0.03798220194488593]
	TIME [epoch: 32.1 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03722764017388473		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 0.03722764017388473 | validation: 0.04216668608682818]
	TIME [epoch: 32 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03386584301840945		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 0.03386584301840945 | validation: 0.04576207087654691]
	TIME [epoch: 32.1 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031915944561978216		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 0.031915944561978216 | validation: 0.03379421020076849]
	TIME [epoch: 32.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_949.pth
	Model improved!!!
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029347074463100607		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 0.029347074463100607 | validation: 0.034160216105561275]
	TIME [epoch: 32.1 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029834670203576896		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 0.029834670203576896 | validation: 0.033658343139820704]
	TIME [epoch: 32.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_951.pth
	Model improved!!!
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02895684819388537		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 0.02895684819388537 | validation: 0.0411985369843831]
	TIME [epoch: 32.1 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03023092728279607		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 0.03023092728279607 | validation: 0.04149542358004102]
	TIME [epoch: 32.1 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03243330578828016		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 0.03243330578828016 | validation: 0.03342625166113056]
	TIME [epoch: 32.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_954.pth
	Model improved!!!
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030564768002720524		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 0.030564768002720524 | validation: 0.031938916229462176]
	TIME [epoch: 32.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_955.pth
	Model improved!!!
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03088207302940891		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 0.03088207302940891 | validation: 0.03646494094284033]
	TIME [epoch: 32.1 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03130754753433756		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.03130754753433756 | validation: 0.03280807107871928]
	TIME [epoch: 32 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029961098254466405		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 0.029961098254466405 | validation: 0.037892327248023966]
	TIME [epoch: 32.1 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03485597541702534		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 0.03485597541702534 | validation: 0.04120281768579869]
	TIME [epoch: 32 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03115669138687381		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 0.03115669138687381 | validation: 0.042667214868747624]
	TIME [epoch: 32.1 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03164891393132044		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 0.03164891393132044 | validation: 0.03674459856862562]
	TIME [epoch: 32 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029170400446339156		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 0.029170400446339156 | validation: 0.03844608929757308]
	TIME [epoch: 32.1 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02876965108746556		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 0.02876965108746556 | validation: 0.035934633189561103]
	TIME [epoch: 32 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029561584734550735		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 0.029561584734550735 | validation: 0.03222996949102455]
	TIME [epoch: 32 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03145395104953095		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 0.03145395104953095 | validation: 0.031040521452427326]
	TIME [epoch: 32 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_965.pth
	Model improved!!!
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033275652647265605		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 0.033275652647265605 | validation: 0.03450947531106923]
	TIME [epoch: 32.2 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029502269788318762		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 0.029502269788318762 | validation: 0.036018025380139235]
	TIME [epoch: 32.2 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030487527868392268		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 0.030487527868392268 | validation: 0.03463532517729013]
	TIME [epoch: 32.2 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03127684178409721		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 0.03127684178409721 | validation: 0.036940780251928707]
	TIME [epoch: 32.2 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02997849887648327		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 0.02997849887648327 | validation: 0.03380161117962111]
	TIME [epoch: 32.2 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032429608699551034		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 0.032429608699551034 | validation: 0.032329813811168125]
	TIME [epoch: 32.2 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029648669420081437		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 0.029648669420081437 | validation: 0.03294882037238027]
	TIME [epoch: 32.2 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030100226912851094		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 0.030100226912851094 | validation: 0.0322883420385411]
	TIME [epoch: 32.1 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032502587972773236		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 0.032502587972773236 | validation: 0.03399827811715199]
	TIME [epoch: 32.1 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03320461579402131		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 0.03320461579402131 | validation: 0.03998013156517828]
	TIME [epoch: 32.2 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03037125010469632		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 0.03037125010469632 | validation: 0.03772606337210059]
	TIME [epoch: 32.2 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03030361018296924		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 0.03030361018296924 | validation: 0.033289336789690864]
	TIME [epoch: 32.2 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030124750196322832		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 0.030124750196322832 | validation: 0.03286051488991039]
	TIME [epoch: 32.2 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030769646917437637		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 0.030769646917437637 | validation: 0.04818502995850391]
	TIME [epoch: 32.2 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03251400776069171		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 0.03251400776069171 | validation: 0.04264773656555062]
	TIME [epoch: 32.1 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032724502981424354		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 0.032724502981424354 | validation: 0.03279938362179656]
	TIME [epoch: 32.1 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029267813724973196		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 0.029267813724973196 | validation: 0.03449003630868938]
	TIME [epoch: 32.2 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028977205823664972		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 0.028977205823664972 | validation: 0.037158630372333176]
	TIME [epoch: 32.1 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029585124050176118		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 0.029585124050176118 | validation: 0.03864044297059868]
	TIME [epoch: 32.2 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03158769866127095		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 0.03158769866127095 | validation: 0.03403783429707504]
	TIME [epoch: 32.2 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030226892612217792		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 0.030226892612217792 | validation: 0.03670425313859687]
	TIME [epoch: 32.1 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030627342140758063		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 0.030627342140758063 | validation: 0.036496948857125226]
	TIME [epoch: 32.1 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03396933088221809		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 0.03396933088221809 | validation: 0.03176275284981031]
	TIME [epoch: 32.2 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029408489711040382		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 0.029408489711040382 | validation: 0.036758936191029525]
	TIME [epoch: 32.2 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028808135180415542		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 0.028808135180415542 | validation: 0.0360814523542938]
	TIME [epoch: 32.2 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03001478929012542		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 0.03001478929012542 | validation: 0.03187779639262265]
	TIME [epoch: 32.2 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02914872796061928		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 0.02914872796061928 | validation: 0.033754910525432016]
	TIME [epoch: 32.1 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06726242959065169		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 0.06726242959065169 | validation: 0.08305389457771638]
	TIME [epoch: 32.1 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07930043887074989		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 0.07930043887074989 | validation: 0.052020918747458386]
	TIME [epoch: 32.1 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03785812379477924		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 0.03785812379477924 | validation: 0.03505366522572226]
	TIME [epoch: 32.1 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02995326038730148		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.02995326038730148 | validation: 0.03219492109994701]
	TIME [epoch: 32.2 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03006333119868303		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 0.03006333119868303 | validation: 0.03628684522520348]
	TIME [epoch: 32.1 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03006137993642894		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 0.03006137993642894 | validation: 0.03110381146916539]
	TIME [epoch: 32.2 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029358548203437772		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 0.029358548203437772 | validation: 0.047708440194958485]
	TIME [epoch: 32.1 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032393978607659024		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 0.032393978607659024 | validation: 0.0310078544649194]
	TIME [epoch: 32.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_1000.pth
	Model improved!!!
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02829031581720975		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 0.02829031581720975 | validation: 0.03360906313513242]
	TIME [epoch: 174 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028575352849392097		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 0.028575352849392097 | validation: 0.032051638941441724]
	TIME [epoch: 68.9 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028725307400836363		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 0.028725307400836363 | validation: 0.031126193904658903]
	TIME [epoch: 69 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0296830425998869		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 0.0296830425998869 | validation: 0.031517410944203156]
	TIME [epoch: 69 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0288715776989535		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 0.0288715776989535 | validation: 0.0333908682747454]
	TIME [epoch: 69 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02795414557422734		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 0.02795414557422734 | validation: 0.03321498453680039]
	TIME [epoch: 68.9 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0675516226125559		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 0.0675516226125559 | validation: 0.06716452202555763]
	TIME [epoch: 69 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05153873434987152		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 0.05153873434987152 | validation: 0.040220783076366104]
	TIME [epoch: 69 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029955522986120354		[learning rate: 0.00033497]
	Learning Rate: 0.000334965
	LOSS [training: 0.029955522986120354 | validation: 0.03092856994551215]
	TIME [epoch: 68.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_1009.pth
	Model improved!!!
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027651196857551262		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 0.027651196857551262 | validation: 0.032348050125976247]
	TIME [epoch: 68.8 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03157079927441976		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 0.03157079927441976 | validation: 0.03166624470974002]
	TIME [epoch: 68.9 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029590603197642937		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 0.029590603197642937 | validation: 0.03223860153491402]
	TIME [epoch: 68.8 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02864882278923916		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 0.02864882278923916 | validation: 0.03018329239647788]
	TIME [epoch: 68.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_1013.pth
	Model improved!!!
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027805627980283774		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 0.027805627980283774 | validation: 0.03163685793277024]
	TIME [epoch: 68.7 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028298626876317116		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 0.028298626876317116 | validation: 0.030925515926341154]
	TIME [epoch: 68.9 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02964388294151837		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 0.02964388294151837 | validation: 0.03524010787168144]
	TIME [epoch: 68.9 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029163304213549797		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 0.029163304213549797 | validation: 0.03225723735969543]
	TIME [epoch: 69 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028845110259813407		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 0.028845110259813407 | validation: 0.031239067945182837]
	TIME [epoch: 68.9 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028359488206532855		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 0.028359488206532855 | validation: 0.03125671673937176]
	TIME [epoch: 68.9 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029634599896089974		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 0.029634599896089974 | validation: 0.030854070846709193]
	TIME [epoch: 68.8 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029263334607233116		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 0.029263334607233116 | validation: 0.03332068221732306]
	TIME [epoch: 68.9 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029003687335073563		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 0.029003687335073563 | validation: 0.03203999396278948]
	TIME [epoch: 68.9 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028903437185959165		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 0.028903437185959165 | validation: 0.03407940557489484]
	TIME [epoch: 68.7 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029716163297061664		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 0.029716163297061664 | validation: 0.0323229981633038]
	TIME [epoch: 68.7 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028799892688401253		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 0.028799892688401253 | validation: 0.0325625649943107]
	TIME [epoch: 68.9 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02704619910286311		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 0.02704619910286311 | validation: 0.03229085717461807]
	TIME [epoch: 68.7 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028962736074258675		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 0.028962736074258675 | validation: 0.03136051965068604]
	TIME [epoch: 68.8 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02816912419731612		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 0.02816912419731612 | validation: 0.031037143964316585]
	TIME [epoch: 68.8 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030000220553456483		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 0.030000220553456483 | validation: 0.033961791770374714]
	TIME [epoch: 68.8 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028935381293086742		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 0.028935381293086742 | validation: 0.032221576768087665]
	TIME [epoch: 68.7 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027632882813776453		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 0.027632882813776453 | validation: 0.030022353156633482]
	TIME [epoch: 68.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_1031.pth
	Model improved!!!
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027595757572249795		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 0.027595757572249795 | validation: 0.0317852567808556]
	TIME [epoch: 68.8 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028813282665319146		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 0.028813282665319146 | validation: 0.03342100583899834]
	TIME [epoch: 68.8 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02776041020214469		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 0.02776041020214469 | validation: 0.032245057476681216]
	TIME [epoch: 68.9 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03157803050089083		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.03157803050089083 | validation: 0.03345763260202112]
	TIME [epoch: 68.8 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030407922228530934		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 0.030407922228530934 | validation: 0.033643500917211216]
	TIME [epoch: 68.9 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027706018854547234		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 0.027706018854547234 | validation: 0.03143494075475458]
	TIME [epoch: 68.8 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035051438679532525		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 0.035051438679532525 | validation: 0.04302255303846017]
	TIME [epoch: 68.9 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03150299783195927		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 0.03150299783195927 | validation: 0.03229749536878749]
	TIME [epoch: 68.9 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027466707042979974		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 0.027466707042979974 | validation: 0.030097155829631076]
	TIME [epoch: 68.9 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02878897730620591		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 0.02878897730620591 | validation: 0.03404144101076019]
	TIME [epoch: 68.9 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029726334387980523		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 0.029726334387980523 | validation: 0.0430366879626149]
	TIME [epoch: 68.9 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03474033470481548		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 0.03474033470481548 | validation: 0.0322353090448361]
	TIME [epoch: 68.8 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028483947568282854		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 0.028483947568282854 | validation: 0.03640399120049905]
	TIME [epoch: 68.9 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027020266799169686		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 0.027020266799169686 | validation: 0.03140746220503525]
	TIME [epoch: 68.8 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028600825692268697		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 0.028600825692268697 | validation: 0.033942311923985606]
	TIME [epoch: 68.9 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02904659174968873		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 0.02904659174968873 | validation: 0.031223116649368735]
	TIME [epoch: 68.9 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02837218587953184		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 0.02837218587953184 | validation: 0.02943232893797475]
	TIME [epoch: 68.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_1048.pth
	Model improved!!!
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026615148801908052		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 0.026615148801908052 | validation: 0.03079247147761916]
	TIME [epoch: 69 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02740102959493216		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 0.02740102959493216 | validation: 0.0335468929655359]
	TIME [epoch: 69 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027288246691986554		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 0.027288246691986554 | validation: 0.03212230778298009]
	TIME [epoch: 69 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03349209462180271		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 0.03349209462180271 | validation: 0.04027819449864986]
	TIME [epoch: 69 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029419872978272876		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 0.029419872978272876 | validation: 0.03313412867416336]
	TIME [epoch: 68.9 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027610798423059253		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 0.027610798423059253 | validation: 0.03785227263333965]
	TIME [epoch: 68.9 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030430461638761716		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 0.030430461638761716 | validation: 0.03150400701272048]
	TIME [epoch: 69 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02717184977744656		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 0.02717184977744656 | validation: 0.031753980152853536]
	TIME [epoch: 69 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028753468929367546		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 0.028753468929367546 | validation: 0.031342463962429165]
	TIME [epoch: 69 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028907192808478956		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 0.028907192808478956 | validation: 0.03093008901033266]
	TIME [epoch: 69.1 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02766354173551793		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 0.02766354173551793 | validation: 0.03444071802936316]
	TIME [epoch: 69.1 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030738866579120356		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 0.030738866579120356 | validation: 0.030850372491262282]
	TIME [epoch: 68.9 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02722954918545383		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 0.02722954918545383 | validation: 0.030778288143426816]
	TIME [epoch: 68.9 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027649473306444912		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 0.027649473306444912 | validation: 0.030898019581512597]
	TIME [epoch: 68.9 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027216470999677684		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 0.027216470999677684 | validation: 0.03176569127523516]
	TIME [epoch: 68.7 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02650317256693522		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 0.02650317256693522 | validation: 0.033302323661185504]
	TIME [epoch: 68.9 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027666977849753648		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 0.027666977849753648 | validation: 0.03139258013101192]
	TIME [epoch: 68.8 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034610617718334936		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 0.034610617718334936 | validation: 0.07165466500886399]
	TIME [epoch: 69 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04280760351472547		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 0.04280760351472547 | validation: 0.03499150266005099]
	TIME [epoch: 68.7 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027960678614603968		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 0.027960678614603968 | validation: 0.029744769267848706]
	TIME [epoch: 68.9 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026192563276003313		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 0.026192563276003313 | validation: 0.03047585666244884]
	TIME [epoch: 68.9 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028255386634845506		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 0.028255386634845506 | validation: 0.051074335203699917]
	TIME [epoch: 68.9 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034290572095720256		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 0.034290572095720256 | validation: 0.0331741094518488]
	TIME [epoch: 68.8 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0274891927848992		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 0.0274891927848992 | validation: 0.029104275475977603]
	TIME [epoch: 68.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_1072.pth
	Model improved!!!
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027789541492084013		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 0.027789541492084013 | validation: 0.03054302522426603]
	TIME [epoch: 69 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026512409122330757		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.026512409122330757 | validation: 0.029753251240837723]
	TIME [epoch: 69 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027416056370994833		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 0.027416056370994833 | validation: 0.02973843914684344]
	TIME [epoch: 69.1 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02595635997186324		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 0.02595635997186324 | validation: 0.03236237205106938]
	TIME [epoch: 69.1 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02742683052695385		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 0.02742683052695385 | validation: 0.03505093305208448]
	TIME [epoch: 69.1 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027627137340309336		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 0.027627137340309336 | validation: 0.03090550561227868]
	TIME [epoch: 68.9 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027604279270364344		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 0.027604279270364344 | validation: 0.029855100243461698]
	TIME [epoch: 68.9 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026487176166289025		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 0.026487176166289025 | validation: 0.030970243015095166]
	TIME [epoch: 68.9 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028573012657189013		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 0.028573012657189013 | validation: 0.035923858610817976]
	TIME [epoch: 68.9 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027845244444458125		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 0.027845244444458125 | validation: 0.03377675463421598]
	TIME [epoch: 68.9 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026313455673963707		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 0.026313455673963707 | validation: 0.031470612423001545]
	TIME [epoch: 68.9 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026640044208400465		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 0.026640044208400465 | validation: 0.032873069028775695]
	TIME [epoch: 68.9 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02775839810446528		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 0.02775839810446528 | validation: 0.030094938271339626]
	TIME [epoch: 68.9 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027780324309150565		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 0.027780324309150565 | validation: 0.028558174465476198]
	TIME [epoch: 68.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_1086.pth
	Model improved!!!
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026508975462131372		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 0.026508975462131372 | validation: 0.03401433601553461]
	TIME [epoch: 68.8 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026737949873686415		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 0.026737949873686415 | validation: 0.03373249730761514]
	TIME [epoch: 68.8 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02783322353308742		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 0.02783322353308742 | validation: 0.03394024153999097]
	TIME [epoch: 68.9 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029094452846799644		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 0.029094452846799644 | validation: 0.038022860826350244]
	TIME [epoch: 68.7 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02885881356868235		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 0.02885881356868235 | validation: 0.029437449331752154]
	TIME [epoch: 68.9 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026487444289335006		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 0.026487444289335006 | validation: 0.03286182950073738]
	TIME [epoch: 68.8 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028694572311860574		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 0.028694572311860574 | validation: 0.030936675386838648]
	TIME [epoch: 69 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026728523549898367		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 0.026728523549898367 | validation: 0.030202072641223578]
	TIME [epoch: 68.9 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02714091188736497		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 0.02714091188736497 | validation: 0.029383602580022816]
	TIME [epoch: 68.8 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03535681744194078		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 0.03535681744194078 | validation: 0.034946117655117195]
	TIME [epoch: 68.7 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03030556579880433		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 0.03030556579880433 | validation: 0.03701439535047529]
	TIME [epoch: 69 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028470823657472024		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 0.028470823657472024 | validation: 0.03176109146558484]
	TIME [epoch: 68.6 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02761190858741299		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 0.02761190858741299 | validation: 0.032300769049676346]
	TIME [epoch: 69 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02692024773031459		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 0.02692024773031459 | validation: 0.02807980373125866]
	TIME [epoch: 68.7 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_1100.pth
	Model improved!!!
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026429289962313675		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 0.026429289962313675 | validation: 0.02850646953171853]
	TIME [epoch: 68.8 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026728294629294776		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 0.026728294629294776 | validation: 0.029258389811203447]
	TIME [epoch: 68.8 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02648834215947121		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 0.02648834215947121 | validation: 0.029170304597288706]
	TIME [epoch: 68.9 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02666405185624228		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 0.02666405185624228 | validation: 0.02978697104333921]
	TIME [epoch: 68.8 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026216256515681338		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 0.026216256515681338 | validation: 0.03147651630379692]
	TIME [epoch: 68.9 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027094161686655367		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 0.027094161686655367 | validation: 0.03200216232515762]
	TIME [epoch: 68.7 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026432392686013557		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 0.026432392686013557 | validation: 0.03022897949387715]
	TIME [epoch: 68.9 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026585473215873394		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 0.026585473215873394 | validation: 0.03020096283145881]
	TIME [epoch: 68.8 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02645748612256285		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 0.02645748612256285 | validation: 0.03699755232303446]
	TIME [epoch: 68.9 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02679688383464321		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 0.02679688383464321 | validation: 0.02942234047998818]
	TIME [epoch: 68.7 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026021905548812518		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 0.026021905548812518 | validation: 0.031070039672709653]
	TIME [epoch: 68.8 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02777000802361264		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 0.02777000802361264 | validation: 0.031233431985062194]
	TIME [epoch: 68.7 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029777307157653074		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.029777307157653074 | validation: 0.02766038115328106]
	TIME [epoch: 68.6 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_1113.pth
	Model improved!!!
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02593252202815356		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 0.02593252202815356 | validation: 0.03220170910982516]
	TIME [epoch: 68.8 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026323044438043813		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 0.026323044438043813 | validation: 0.030362594487619694]
	TIME [epoch: 69 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025958923538344864		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 0.025958923538344864 | validation: 0.0290423511983861]
	TIME [epoch: 69.1 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025740035749715815		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 0.025740035749715815 | validation: 0.03284507176203662]
	TIME [epoch: 69.2 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029135083111759854		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 0.029135083111759854 | validation: 0.031199147366361688]
	TIME [epoch: 69.2 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02857354080758298		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 0.02857354080758298 | validation: 0.04061443129003109]
	TIME [epoch: 69.2 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032370711740763294		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 0.032370711740763294 | validation: 0.030709638664194176]
	TIME [epoch: 69.2 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027426816009376687		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 0.027426816009376687 | validation: 0.03023944921303351]
	TIME [epoch: 69 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02553018257078233		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 0.02553018257078233 | validation: 0.02789124337200132]
	TIME [epoch: 69.1 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02573712259009852		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 0.02573712259009852 | validation: 0.043317780186575276]
	TIME [epoch: 69 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030741123373850782		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 0.030741123373850782 | validation: 0.02846573780410419]
	TIME [epoch: 69 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02635708903320664		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 0.02635708903320664 | validation: 0.030112095164330527]
	TIME [epoch: 69.1 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026380334209492887		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 0.026380334209492887 | validation: 0.028541449021764143]
	TIME [epoch: 69.1 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026089606401748414		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 0.026089606401748414 | validation: 0.03073685071408036]
	TIME [epoch: 68.8 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024988493996995827		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 0.024988493996995827 | validation: 0.03120429355524993]
	TIME [epoch: 69.2 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026518222994475704		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 0.026518222994475704 | validation: 0.03184610971866302]
	TIME [epoch: 69 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026902319940529502		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 0.026902319940529502 | validation: 0.029340953644122886]
	TIME [epoch: 69.1 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026041334007206943		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 0.026041334007206943 | validation: 0.028633935626033252]
	TIME [epoch: 69.1 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02856830550159547		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 0.02856830550159547 | validation: 0.030088472058684458]
	TIME [epoch: 69.1 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027228103551183602		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 0.027228103551183602 | validation: 0.031297872714586936]
	TIME [epoch: 69 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02668384060481438		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 0.02668384060481438 | validation: 0.033036746119240595]
	TIME [epoch: 69.1 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02694052383579275		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 0.02694052383579275 | validation: 0.02940936004718961]
	TIME [epoch: 69.1 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026285807132221017		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 0.026285807132221017 | validation: 0.0292352647600087]
	TIME [epoch: 69 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02541457607938631		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 0.02541457607938631 | validation: 0.030581564006160546]
	TIME [epoch: 69.1 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026926703191451386		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 0.026926703191451386 | validation: 0.03398640730933176]
	TIME [epoch: 68.9 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0256997055340921		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 0.0256997055340921 | validation: 0.0316504598699128]
	TIME [epoch: 69 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026791291425251773		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 0.026791291425251773 | validation: 0.03047948560130834]
	TIME [epoch: 69.1 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02687336718237293		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 0.02687336718237293 | validation: 0.029566146596525754]
	TIME [epoch: 68.9 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026542112696035413		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 0.026542112696035413 | validation: 0.028953089671013073]
	TIME [epoch: 69 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025530782071467088		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 0.025530782071467088 | validation: 0.029206191695961916]
	TIME [epoch: 69 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027627331055376427		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 0.027627331055376427 | validation: 0.0316967546572863]
	TIME [epoch: 68.9 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026323108507771337		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 0.026323108507771337 | validation: 0.03486081930778611]
	TIME [epoch: 68.9 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026520612010172962		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 0.026520612010172962 | validation: 0.03096082728244939]
	TIME [epoch: 68.7 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026773708385117636		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 0.026773708385117636 | validation: 0.027724723509565362]
	TIME [epoch: 69.1 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025088571175672123		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 0.025088571175672123 | validation: 0.03632670886642807]
	TIME [epoch: 68.9 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028202913780242308		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 0.028202913780242308 | validation: 0.029821801284716253]
	TIME [epoch: 68.8 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02605772521438545		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 0.02605772521438545 | validation: 0.028258582846837035]
	TIME [epoch: 68.9 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025795902978798982		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 0.025795902978798982 | validation: 0.028705051282702645]
	TIME [epoch: 69.1 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025690525634539167		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 0.025690525634539167 | validation: 0.031677015189463205]
	TIME [epoch: 69.1 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02626096602841239		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 0.02626096602841239 | validation: 0.029037157672181985]
	TIME [epoch: 69.2 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026106711941198298		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 0.026106711941198298 | validation: 0.031492442575522826]
	TIME [epoch: 69.1 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02613181910098157		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 0.02613181910098157 | validation: 0.02739151660048956]
	TIME [epoch: 69 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_1155.pth
	Model improved!!!
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02670410748402412		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 0.02670410748402412 | validation: 0.028441452887712222]
	TIME [epoch: 69.2 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025843671806990723		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 0.025843671806990723 | validation: 0.029096721484304658]
	TIME [epoch: 68.9 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025585358984876214		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 0.025585358984876214 | validation: 0.029230092008737833]
	TIME [epoch: 69.1 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024655917672875793		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 0.024655917672875793 | validation: 0.03025480815261681]
	TIME [epoch: 69 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0278888551492525		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 0.0278888551492525 | validation: 0.03364085300804242]
	TIME [epoch: 69 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025104279904163612		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 0.025104279904163612 | validation: 0.027918138861319802]
	TIME [epoch: 69 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02556291137580646		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 0.02556291137580646 | validation: 0.029331259162986283]
	TIME [epoch: 69.1 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029944284821650353		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 0.029944284821650353 | validation: 0.045373011218726375]
	TIME [epoch: 69.1 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029252080025821074		[learning rate: 0.00019344]
	Learning Rate: 0.000193437
	LOSS [training: 0.029252080025821074 | validation: 0.030067883328008183]
	TIME [epoch: 68.8 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0252364507176603		[learning rate: 0.00019275]
	Learning Rate: 0.000192753
	LOSS [training: 0.0252364507176603 | validation: 0.030371928484618463]
	TIME [epoch: 68.7 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023972059406867495		[learning rate: 0.00019207]
	Learning Rate: 0.000192071
	LOSS [training: 0.023972059406867495 | validation: 0.02850387575115991]
	TIME [epoch: 68.9 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025322493204269395		[learning rate: 0.00019139]
	Learning Rate: 0.000191392
	LOSS [training: 0.025322493204269395 | validation: 0.02890457096366103]
	TIME [epoch: 68.9 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02442422424348016		[learning rate: 0.00019071]
	Learning Rate: 0.000190715
	LOSS [training: 0.02442422424348016 | validation: 0.02924200198008644]
	TIME [epoch: 69.1 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02419602203453638		[learning rate: 0.00019004]
	Learning Rate: 0.00019004
	LOSS [training: 0.02419602203453638 | validation: 0.028575031393065428]
	TIME [epoch: 68.9 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026919524141371824		[learning rate: 0.00018937]
	Learning Rate: 0.000189369
	LOSS [training: 0.026919524141371824 | validation: 0.029669923926887012]
	TIME [epoch: 69 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02613108619072782		[learning rate: 0.0001887]
	Learning Rate: 0.000188699
	LOSS [training: 0.02613108619072782 | validation: 0.03060707095891694]
	TIME [epoch: 69.3 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02459691975715598		[learning rate: 0.00018803]
	Learning Rate: 0.000188032
	LOSS [training: 0.02459691975715598 | validation: 0.030682406563389532]
	TIME [epoch: 68.9 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02490755915180401		[learning rate: 0.00018737]
	Learning Rate: 0.000187367
	LOSS [training: 0.02490755915180401 | validation: 0.028054228204911366]
	TIME [epoch: 69.1 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025271212702936058		[learning rate: 0.0001867]
	Learning Rate: 0.000186704
	LOSS [training: 0.025271212702936058 | validation: 0.02916321798282915]
	TIME [epoch: 68.9 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026213085675753745		[learning rate: 0.00018604]
	Learning Rate: 0.000186044
	LOSS [training: 0.026213085675753745 | validation: 0.027183319214475116]
	TIME [epoch: 69.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_1175.pth
	Model improved!!!
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028779259514342466		[learning rate: 0.00018539]
	Learning Rate: 0.000185386
	LOSS [training: 0.028779259514342466 | validation: 0.03770928133816087]
	TIME [epoch: 68.8 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027216029696591756		[learning rate: 0.00018473]
	Learning Rate: 0.00018473
	LOSS [training: 0.027216029696591756 | validation: 0.02905557984228323]
	TIME [epoch: 69 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025763628248989822		[learning rate: 0.00018408]
	Learning Rate: 0.000184077
	LOSS [training: 0.025763628248989822 | validation: 0.028437175248099064]
	TIME [epoch: 68.9 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024939427345280818		[learning rate: 0.00018343]
	Learning Rate: 0.000183426
	LOSS [training: 0.024939427345280818 | validation: 0.0289069184749177]
	TIME [epoch: 69.1 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025939582396026856		[learning rate: 0.00018278]
	Learning Rate: 0.000182778
	LOSS [training: 0.025939582396026856 | validation: 0.030635550700987967]
	TIME [epoch: 68.9 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026273690532973657		[learning rate: 0.00018213]
	Learning Rate: 0.000182131
	LOSS [training: 0.026273690532973657 | validation: 0.026270210973925262]
	TIME [epoch: 68.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_1181.pth
	Model improved!!!
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02509668229858321		[learning rate: 0.00018149]
	Learning Rate: 0.000181487
	LOSS [training: 0.02509668229858321 | validation: 0.030441914276133075]
	TIME [epoch: 68.9 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025924267069066503		[learning rate: 0.00018085]
	Learning Rate: 0.000180846
	LOSS [training: 0.025924267069066503 | validation: 0.02825509175714434]
	TIME [epoch: 68.8 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024427126157816774		[learning rate: 0.00018021]
	Learning Rate: 0.000180206
	LOSS [training: 0.024427126157816774 | validation: 0.02712595856656501]
	TIME [epoch: 69.1 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027843658177845648		[learning rate: 0.00017957]
	Learning Rate: 0.000179569
	LOSS [training: 0.027843658177845648 | validation: 0.03205160603257703]
	TIME [epoch: 69 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026681640809202184		[learning rate: 0.00017893]
	Learning Rate: 0.000178934
	LOSS [training: 0.026681640809202184 | validation: 0.029332471217427485]
	TIME [epoch: 68.9 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025774512998663707		[learning rate: 0.0001783]
	Learning Rate: 0.000178301
	LOSS [training: 0.025774512998663707 | validation: 0.02771298294064932]
	TIME [epoch: 68.9 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02507392591027703		[learning rate: 0.00017767]
	Learning Rate: 0.000177671
	LOSS [training: 0.02507392591027703 | validation: 0.029284899728997386]
	TIME [epoch: 69 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02486228740860564		[learning rate: 0.00017704]
	Learning Rate: 0.000177042
	LOSS [training: 0.02486228740860564 | validation: 0.028114080773307033]
	TIME [epoch: 69 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024457302705755163		[learning rate: 0.00017642]
	Learning Rate: 0.000176416
	LOSS [training: 0.024457302705755163 | validation: 0.026655365009105286]
	TIME [epoch: 68.8 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025436909339397712		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 0.025436909339397712 | validation: 0.027392180444322112]
	TIME [epoch: 68.9 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024801663620609465		[learning rate: 0.00017517]
	Learning Rate: 0.000175171
	LOSS [training: 0.024801663620609465 | validation: 0.02845391346125815]
	TIME [epoch: 69.1 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02578667609077543		[learning rate: 0.00017455]
	Learning Rate: 0.000174551
	LOSS [training: 0.02578667609077543 | validation: 0.03758677642924359]
	TIME [epoch: 68.9 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02933344025811911		[learning rate: 0.00017393]
	Learning Rate: 0.000173934
	LOSS [training: 0.02933344025811911 | validation: 0.02715508504991162]
	TIME [epoch: 68.9 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02419777557245128		[learning rate: 0.00017332]
	Learning Rate: 0.000173319
	LOSS [training: 0.02419777557245128 | validation: 0.02847998984570014]
	TIME [epoch: 69 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025255200596456875		[learning rate: 0.00017271]
	Learning Rate: 0.000172706
	LOSS [training: 0.025255200596456875 | validation: 0.026799607530704223]
	TIME [epoch: 68.9 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02407579450704473		[learning rate: 0.0001721]
	Learning Rate: 0.000172095
	LOSS [training: 0.02407579450704473 | validation: 0.02692753428535461]
	TIME [epoch: 68.8 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026262846917918886		[learning rate: 0.00017149]
	Learning Rate: 0.000171487
	LOSS [training: 0.026262846917918886 | validation: 0.032077783760529964]
	TIME [epoch: 68.8 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025497994310688894		[learning rate: 0.00017088]
	Learning Rate: 0.00017088
	LOSS [training: 0.025497994310688894 | validation: 0.02843106884658065]
	TIME [epoch: 68.7 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033126848012362264		[learning rate: 0.00017028]
	Learning Rate: 0.000170276
	LOSS [training: 0.033126848012362264 | validation: 0.0385170530949991]
	TIME [epoch: 68.9 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02904804147765464		[learning rate: 0.00016967]
	Learning Rate: 0.000169674
	LOSS [training: 0.02904804147765464 | validation: 0.028969107434554713]
	TIME [epoch: 68.9 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024935841928782062		[learning rate: 0.00016907]
	Learning Rate: 0.000169074
	LOSS [training: 0.024935841928782062 | validation: 0.027187718013342698]
	TIME [epoch: 68.9 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026075601482352714		[learning rate: 0.00016848]
	Learning Rate: 0.000168476
	LOSS [training: 0.026075601482352714 | validation: 0.028202291525589375]
	TIME [epoch: 68.9 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023993733388577118		[learning rate: 0.00016788]
	Learning Rate: 0.00016788
	LOSS [training: 0.023993733388577118 | validation: 0.027825925298729987]
	TIME [epoch: 68.9 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024980374645975058		[learning rate: 0.00016729]
	Learning Rate: 0.000167287
	LOSS [training: 0.024980374645975058 | validation: 0.029023365640967358]
	TIME [epoch: 68.9 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024426979295820155		[learning rate: 0.0001667]
	Learning Rate: 0.000166695
	LOSS [training: 0.024426979295820155 | validation: 0.025493843696139232]
	TIME [epoch: 68.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_1206.pth
	Model improved!!!
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0240171096448589		[learning rate: 0.00016611]
	Learning Rate: 0.000166106
	LOSS [training: 0.0240171096448589 | validation: 0.027302793186388095]
	TIME [epoch: 68.9 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023929660345776555		[learning rate: 0.00016552]
	Learning Rate: 0.000165518
	LOSS [training: 0.023929660345776555 | validation: 0.028050603593724282]
	TIME [epoch: 68.9 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024409317304101445		[learning rate: 0.00016493]
	Learning Rate: 0.000164933
	LOSS [training: 0.024409317304101445 | validation: 0.028211430773004715]
	TIME [epoch: 68.9 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024717247834600383		[learning rate: 0.00016435]
	Learning Rate: 0.00016435
	LOSS [training: 0.024717247834600383 | validation: 0.028566389223095266]
	TIME [epoch: 68.9 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024700105288482585		[learning rate: 0.00016377]
	Learning Rate: 0.000163769
	LOSS [training: 0.024700105288482585 | validation: 0.02859057847041937]
	TIME [epoch: 68.9 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024817124269189812		[learning rate: 0.00016319]
	Learning Rate: 0.00016319
	LOSS [training: 0.024817124269189812 | validation: 0.02716401918771927]
	TIME [epoch: 69 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02455705463470276		[learning rate: 0.00016261]
	Learning Rate: 0.000162613
	LOSS [training: 0.02455705463470276 | validation: 0.02865764131979333]
	TIME [epoch: 69 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026200777770232955		[learning rate: 0.00016204]
	Learning Rate: 0.000162037
	LOSS [training: 0.026200777770232955 | validation: 0.027503352344808156]
	TIME [epoch: 68.8 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024461479849549495		[learning rate: 0.00016146]
	Learning Rate: 0.000161464
	LOSS [training: 0.024461479849549495 | validation: 0.02924780967469867]
	TIME [epoch: 69 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024844709272642478		[learning rate: 0.00016089]
	Learning Rate: 0.000160893
	LOSS [training: 0.024844709272642478 | validation: 0.028026991807912054]
	TIME [epoch: 69 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024221840616405983		[learning rate: 0.00016032]
	Learning Rate: 0.000160325
	LOSS [training: 0.024221840616405983 | validation: 0.028858899951073845]
	TIME [epoch: 68.9 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025341881065106566		[learning rate: 0.00015976]
	Learning Rate: 0.000159758
	LOSS [training: 0.025341881065106566 | validation: 0.027646770720065914]
	TIME [epoch: 69 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02793472571721131		[learning rate: 0.00015919]
	Learning Rate: 0.000159193
	LOSS [training: 0.02793472571721131 | validation: 0.035940279882735696]
	TIME [epoch: 68.9 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02731987065759491		[learning rate: 0.00015863]
	Learning Rate: 0.00015863
	LOSS [training: 0.02731987065759491 | validation: 0.028949593206572685]
	TIME [epoch: 69 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024556780986030105		[learning rate: 0.00015807]
	Learning Rate: 0.000158069
	LOSS [training: 0.024556780986030105 | validation: 0.028592061821484042]
	TIME [epoch: 69 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027937493108381874		[learning rate: 0.00015751]
	Learning Rate: 0.00015751
	LOSS [training: 0.027937493108381874 | validation: 0.03187343144887833]
	TIME [epoch: 68.9 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025786462493645763		[learning rate: 0.00015695]
	Learning Rate: 0.000156953
	LOSS [training: 0.025786462493645763 | validation: 0.027298198082562196]
	TIME [epoch: 69 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02468027687661159		[learning rate: 0.0001564]
	Learning Rate: 0.000156398
	LOSS [training: 0.02468027687661159 | validation: 0.027448840100830867]
	TIME [epoch: 68.9 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023760374582333058		[learning rate: 0.00015584]
	Learning Rate: 0.000155845
	LOSS [training: 0.023760374582333058 | validation: 0.028109110890391788]
	TIME [epoch: 69 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023872547761933864		[learning rate: 0.00015529]
	Learning Rate: 0.000155294
	LOSS [training: 0.023872547761933864 | validation: 0.02863759666738538]
	TIME [epoch: 68.9 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024387824485006825		[learning rate: 0.00015474]
	Learning Rate: 0.000154745
	LOSS [training: 0.024387824485006825 | validation: 0.029500584773677813]
	TIME [epoch: 69 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024759221191270997		[learning rate: 0.0001542]
	Learning Rate: 0.000154197
	LOSS [training: 0.024759221191270997 | validation: 0.026245498264945332]
	TIME [epoch: 68.9 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024132083188774897		[learning rate: 0.00015365]
	Learning Rate: 0.000153652
	LOSS [training: 0.024132083188774897 | validation: 0.02883984793530269]
	TIME [epoch: 68.9 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024854867590413797		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 0.024854867590413797 | validation: 0.026451070196051085]
	TIME [epoch: 69 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024393551878388233		[learning rate: 0.00015257]
	Learning Rate: 0.000152567
	LOSS [training: 0.024393551878388233 | validation: 0.02774316063731408]
	TIME [epoch: 68.9 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024722939408726458		[learning rate: 0.00015203]
	Learning Rate: 0.000152028
	LOSS [training: 0.024722939408726458 | validation: 0.027142693984555818]
	TIME [epoch: 68.9 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024656106081452727		[learning rate: 0.00015149]
	Learning Rate: 0.00015149
	LOSS [training: 0.024656106081452727 | validation: 0.0277976530479413]
	TIME [epoch: 68.9 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024152990361827803		[learning rate: 0.00015095]
	Learning Rate: 0.000150955
	LOSS [training: 0.024152990361827803 | validation: 0.026706071109438942]
	TIME [epoch: 69 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024458600653791976		[learning rate: 0.00015042]
	Learning Rate: 0.000150421
	LOSS [training: 0.024458600653791976 | validation: 0.02684501009372687]
	TIME [epoch: 68.6 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02439987998346739		[learning rate: 0.00014989]
	Learning Rate: 0.000149889
	LOSS [training: 0.02439987998346739 | validation: 0.03022628741519815]
	TIME [epoch: 68.6 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024526190239372997		[learning rate: 0.00014936]
	Learning Rate: 0.000149359
	LOSS [training: 0.024526190239372997 | validation: 0.027056292955733315]
	TIME [epoch: 68.6 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029347491470939187		[learning rate: 0.00014883]
	Learning Rate: 0.000148831
	LOSS [training: 0.029347491470939187 | validation: 0.03345022380173887]
	TIME [epoch: 68.4 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025513700217995722		[learning rate: 0.0001483]
	Learning Rate: 0.000148304
	LOSS [training: 0.025513700217995722 | validation: 0.028271635016884324]
	TIME [epoch: 68.4 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024326851401987447		[learning rate: 0.00014778]
	Learning Rate: 0.00014778
	LOSS [training: 0.024326851401987447 | validation: 0.028194202063860474]
	TIME [epoch: 68.5 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0243728142439272		[learning rate: 0.00014726]
	Learning Rate: 0.000147257
	LOSS [training: 0.0243728142439272 | validation: 0.028878727017834803]
	TIME [epoch: 68.5 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024406477110367298		[learning rate: 0.00014674]
	Learning Rate: 0.000146737
	LOSS [training: 0.024406477110367298 | validation: 0.026943031848150605]
	TIME [epoch: 68.4 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024049824046950035		[learning rate: 0.00014622]
	Learning Rate: 0.000146218
	LOSS [training: 0.024049824046950035 | validation: 0.026919996306524384]
	TIME [epoch: 68.5 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024824877303489406		[learning rate: 0.0001457]
	Learning Rate: 0.000145701
	LOSS [training: 0.024824877303489406 | validation: 0.02643333522483759]
	TIME [epoch: 68.5 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02500775979862668		[learning rate: 0.00014519]
	Learning Rate: 0.000145185
	LOSS [training: 0.02500775979862668 | validation: 0.028524602790029212]
	TIME [epoch: 68.5 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024037047010406182		[learning rate: 0.00014467]
	Learning Rate: 0.000144672
	LOSS [training: 0.024037047010406182 | validation: 0.027761254363449103]
	TIME [epoch: 68.8 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02375014389229429		[learning rate: 0.00014416]
	Learning Rate: 0.00014416
	LOSS [training: 0.02375014389229429 | validation: 0.029799316324215036]
	TIME [epoch: 68.7 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02891851973498705		[learning rate: 0.00014365]
	Learning Rate: 0.000143651
	LOSS [training: 0.02891851973498705 | validation: 0.028868290059784633]
	TIME [epoch: 68.9 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02546975340289015		[learning rate: 0.00014314]
	Learning Rate: 0.000143143
	LOSS [training: 0.02546975340289015 | validation: 0.026981754822318408]
	TIME [epoch: 68.8 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023745080990150227		[learning rate: 0.00014264]
	Learning Rate: 0.000142637
	LOSS [training: 0.023745080990150227 | validation: 0.028126793298850927]
	TIME [epoch: 68.9 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023878150953771667		[learning rate: 0.00014213]
	Learning Rate: 0.000142132
	LOSS [training: 0.023878150953771667 | validation: 0.02721622160318358]
	TIME [epoch: 68.9 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023671897611830906		[learning rate: 0.00014163]
	Learning Rate: 0.00014163
	LOSS [training: 0.023671897611830906 | validation: 0.027891226458863334]
	TIME [epoch: 68.9 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025394225282733964		[learning rate: 0.00014113]
	Learning Rate: 0.000141129
	LOSS [training: 0.025394225282733964 | validation: 0.0276008139676321]
	TIME [epoch: 68.9 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023203464271012542		[learning rate: 0.00014063]
	Learning Rate: 0.00014063
	LOSS [training: 0.023203464271012542 | validation: 0.029104188390161018]
	TIME [epoch: 68.8 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025107707777274092		[learning rate: 0.00014013]
	Learning Rate: 0.000140132
	LOSS [training: 0.025107707777274092 | validation: 0.02812666741201831]
	TIME [epoch: 68.7 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024503539108697705		[learning rate: 0.00013964]
	Learning Rate: 0.000139637
	LOSS [training: 0.024503539108697705 | validation: 0.02755403145777663]
	TIME [epoch: 68.7 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024107962272364933		[learning rate: 0.00013914]
	Learning Rate: 0.000139143
	LOSS [training: 0.024107962272364933 | validation: 0.026300817951847247]
	TIME [epoch: 68.8 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023998668014251278		[learning rate: 0.00013865]
	Learning Rate: 0.000138651
	LOSS [training: 0.023998668014251278 | validation: 0.03158836157570928]
	TIME [epoch: 68.9 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0248697049694176		[learning rate: 0.00013816]
	Learning Rate: 0.000138161
	LOSS [training: 0.0248697049694176 | validation: 0.028719387027717938]
	TIME [epoch: 68.9 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024003736240330562		[learning rate: 0.00013767]
	Learning Rate: 0.000137672
	LOSS [training: 0.024003736240330562 | validation: 0.029464190531621068]
	TIME [epoch: 68.9 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024150313075418893		[learning rate: 0.00013719]
	Learning Rate: 0.000137185
	LOSS [training: 0.024150313075418893 | validation: 0.027912111107911617]
	TIME [epoch: 68.9 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024226177058693472		[learning rate: 0.0001367]
	Learning Rate: 0.0001367
	LOSS [training: 0.024226177058693472 | validation: 0.028044004211975365]
	TIME [epoch: 69 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024467223957754622		[learning rate: 0.00013622]
	Learning Rate: 0.000136217
	LOSS [training: 0.024467223957754622 | validation: 0.027590165403804087]
	TIME [epoch: 68.8 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023991462202966673		[learning rate: 0.00013574]
	Learning Rate: 0.000135735
	LOSS [training: 0.023991462202966673 | validation: 0.027441932558097744]
	TIME [epoch: 68.8 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023497105237075374		[learning rate: 0.00013526]
	Learning Rate: 0.000135255
	LOSS [training: 0.023497105237075374 | validation: 0.0264320642114928]
	TIME [epoch: 68.5 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02465502633769937		[learning rate: 0.00013478]
	Learning Rate: 0.000134777
	LOSS [training: 0.02465502633769937 | validation: 0.029157885300715707]
	TIME [epoch: 68.3 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024901936229102882		[learning rate: 0.0001343]
	Learning Rate: 0.0001343
	LOSS [training: 0.024901936229102882 | validation: 0.0268601344724259]
	TIME [epoch: 68.3 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024746235967005415		[learning rate: 0.00013383]
	Learning Rate: 0.000133825
	LOSS [training: 0.024746235967005415 | validation: 0.027061295249376317]
	TIME [epoch: 68.5 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02422899159624744		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 0.02422899159624744 | validation: 0.028914916646830206]
	TIME [epoch: 68.9 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025003902562276997		[learning rate: 0.00013288]
	Learning Rate: 0.000132881
	LOSS [training: 0.025003902562276997 | validation: 0.03016132288019524]
	TIME [epoch: 69 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024906654490395926		[learning rate: 0.00013241]
	Learning Rate: 0.000132411
	LOSS [training: 0.024906654490395926 | validation: 0.029309106119436417]
	TIME [epoch: 68.8 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023466394781139162		[learning rate: 0.00013194]
	Learning Rate: 0.000131942
	LOSS [training: 0.023466394781139162 | validation: 0.026022067983484352]
	TIME [epoch: 69 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024036386283122185		[learning rate: 0.00013148]
	Learning Rate: 0.000131476
	LOSS [training: 0.024036386283122185 | validation: 0.027700944110246387]
	TIME [epoch: 68.7 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024253695020202373		[learning rate: 0.00013101]
	Learning Rate: 0.000131011
	LOSS [training: 0.024253695020202373 | validation: 0.02638233567339947]
	TIME [epoch: 68.7 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02450891370789242		[learning rate: 0.00013055]
	Learning Rate: 0.000130548
	LOSS [training: 0.02450891370789242 | validation: 0.02573431748797647]
	TIME [epoch: 68.7 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023893362552913664		[learning rate: 0.00013009]
	Learning Rate: 0.000130086
	LOSS [training: 0.023893362552913664 | validation: 0.02809460535858533]
	TIME [epoch: 68.8 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025146478572314553		[learning rate: 0.00012963]
	Learning Rate: 0.000129626
	LOSS [training: 0.025146478572314553 | validation: 0.02913165985446877]
	TIME [epoch: 68.5 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02395110213911153		[learning rate: 0.00012917]
	Learning Rate: 0.000129168
	LOSS [training: 0.02395110213911153 | validation: 0.028571047646137437]
	TIME [epoch: 68.2 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023847851125506164		[learning rate: 0.00012871]
	Learning Rate: 0.000128711
	LOSS [training: 0.023847851125506164 | validation: 0.02764027122325969]
	TIME [epoch: 68.6 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025185432595283483		[learning rate: 0.00012826]
	Learning Rate: 0.000128256
	LOSS [training: 0.025185432595283483 | validation: 0.027064635087599737]
	TIME [epoch: 68.7 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024872972339855026		[learning rate: 0.0001278]
	Learning Rate: 0.000127802
	LOSS [training: 0.024872972339855026 | validation: 0.027815874769577975]
	TIME [epoch: 68.6 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023740204904670135		[learning rate: 0.00012735]
	Learning Rate: 0.00012735
	LOSS [training: 0.023740204904670135 | validation: 0.028202513870801003]
	TIME [epoch: 68.8 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024710773428532687		[learning rate: 0.0001269]
	Learning Rate: 0.0001269
	LOSS [training: 0.024710773428532687 | validation: 0.027336633788739986]
	TIME [epoch: 68.7 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02346504473975266		[learning rate: 0.00012645]
	Learning Rate: 0.000126451
	LOSS [training: 0.02346504473975266 | validation: 0.02793058829620408]
	TIME [epoch: 68.5 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02348933700444468		[learning rate: 0.000126]
	Learning Rate: 0.000126004
	LOSS [training: 0.02348933700444468 | validation: 0.027177895072501408]
	TIME [epoch: 68.5 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02438861173700911		[learning rate: 0.00012556]
	Learning Rate: 0.000125559
	LOSS [training: 0.02438861173700911 | validation: 0.027104753941971836]
	TIME [epoch: 68.5 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023433934952720264		[learning rate: 0.00012511]
	Learning Rate: 0.000125115
	LOSS [training: 0.023433934952720264 | validation: 0.02952780575713258]
	TIME [epoch: 68.6 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024765798661609738		[learning rate: 0.00012467]
	Learning Rate: 0.000124672
	LOSS [training: 0.024765798661609738 | validation: 0.025117802799878696]
	TIME [epoch: 68.5 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240704_134102/states/model_phi1_1a_v_mmd1_fix_noise_small_1288.pth
	Model improved!!!
EPOCH 1289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02362463294945401		[learning rate: 0.00012423]
	Learning Rate: 0.000124231
	LOSS [training: 0.02362463294945401 | validation: 0.0272573683039462]
	TIME [epoch: 69 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023513505130471037		[learning rate: 0.00012379]
	Learning Rate: 0.000123792
	LOSS [training: 0.023513505130471037 | validation: 0.02689260549287706]
	TIME [epoch: 69 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023261575392958873		[learning rate: 0.00012335]
	Learning Rate: 0.000123354
	LOSS [training: 0.023261575392958873 | validation: 0.028453907826266285]
	TIME [epoch: 69.1 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02385121473693158		[learning rate: 0.00012292]
	Learning Rate: 0.000122918
	LOSS [training: 0.02385121473693158 | validation: 0.027653386061765144]
	TIME [epoch: 69.1 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02652333842075035		[learning rate: 0.00012248]
	Learning Rate: 0.000122483
	LOSS [training: 0.02652333842075035 | validation: 0.025813016253564157]
	TIME [epoch: 69.1 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024329203853356615		[learning rate: 0.00012205]
	Learning Rate: 0.00012205
	LOSS [training: 0.024329203853356615 | validation: 0.025742419452466733]
	TIME [epoch: 69 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023295448972543632		[learning rate: 0.00012162]
	Learning Rate: 0.000121619
	LOSS [training: 0.023295448972543632 | validation: 0.028382968491051062]
	TIME [epoch: 69.1 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02325125532761207		[learning rate: 0.00012119]
	Learning Rate: 0.000121189
	LOSS [training: 0.02325125532761207 | validation: 0.028156949947426735]
	TIME [epoch: 69 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024080943843911073		[learning rate: 0.00012076]
	Learning Rate: 0.00012076
	LOSS [training: 0.024080943843911073 | validation: 0.02807145174797539]
	TIME [epoch: 69.1 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02317585258666841		[learning rate: 0.00012033]
	Learning Rate: 0.000120333
	LOSS [training: 0.02317585258666841 | validation: 0.02611929608334153]
	TIME [epoch: 69 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02292030389607238		[learning rate: 0.00011991]
	Learning Rate: 0.000119907
	LOSS [training: 0.02292030389607238 | validation: 0.026455855887149625]
	TIME [epoch: 69.1 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024644583673164144		[learning rate: 0.00011948]
	Learning Rate: 0.000119483
	LOSS [training: 0.024644583673164144 | validation: 0.027614279893237482]
	TIME [epoch: 69 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023303421227673607		[learning rate: 0.00011906]
	Learning Rate: 0.000119061
	LOSS [training: 0.023303421227673607 | validation: 0.027218231872072222]
	TIME [epoch: 69 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023189585833056837		[learning rate: 0.00011864]
	Learning Rate: 0.00011864
	LOSS [training: 0.023189585833056837 | validation: 0.03155263280481359]
	TIME [epoch: 68.8 sec]
EPOCH 1303/2000:
	Training over batches...
