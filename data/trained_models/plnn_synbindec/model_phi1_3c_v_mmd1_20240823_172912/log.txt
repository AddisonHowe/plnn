Args:
Namespace(name='model_phi1_3c_v_mmd1', outdir='out/model_training/model_phi1_3c_v_mmd1', training_data='data/training_data/data_phi1_3c/training', validation_data='data/training_data/data_phi1_3c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1706592115

Training model...

Saving initial model state to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.027861847037273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.027861847037273 | validation: 4.831417422173416]
	TIME [epoch: 31.5 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.574939087678979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.574939087678979 | validation: 4.578277001008283]
	TIME [epoch: 3.57 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.196241113794111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.196241113794111 | validation: 5.013031165321985]
	TIME [epoch: 3.56 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.922265756506981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.922265756506981 | validation: 4.419228959587348]
	TIME [epoch: 3.56 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.072876933507041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.072876933507041 | validation: 4.407426960317974]
	TIME [epoch: 3.57 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.015532032902204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.015532032902204 | validation: 4.384825930617176]
	TIME [epoch: 3.56 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.009033585265985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.009033585265985 | validation: 4.260437031878801]
	TIME [epoch: 3.56 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9003056725588263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9003056725588263 | validation: 4.215180222543063]
	TIME [epoch: 3.58 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.925693699809457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.925693699809457 | validation: 4.1451499681381945]
	TIME [epoch: 3.56 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7905046548652677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7905046548652677 | validation: 4.1409612742060675]
	TIME [epoch: 3.58 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8212464468520637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8212464468520637 | validation: 4.004225784707894]
	TIME [epoch: 3.57 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.710116183256273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.710116183256273 | validation: 3.8490175475789137]
	TIME [epoch: 3.56 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5340935611449074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5340935611449074 | validation: 3.414279112744729]
	TIME [epoch: 3.56 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.310699238772487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.310699238772487 | validation: 2.563608327420809]
	TIME [epoch: 3.56 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5715219065750836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5715219065750836 | validation: 1.9945081867331547]
	TIME [epoch: 3.56 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.137859615135333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.137859615135333 | validation: 1.7189245427875712]
	TIME [epoch: 3.56 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.923495771219043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.923495771219043 | validation: 1.6799283716401874]
	TIME [epoch: 3.58 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7538398588348845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7538398588348845 | validation: 1.083886947885111]
	TIME [epoch: 3.57 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.144973048840791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.144973048840791 | validation: 1.2429932548863922]
	TIME [epoch: 3.56 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2060811226779362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2060811226779362 | validation: 2.135320443373956]
	TIME [epoch: 3.56 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.453329235871542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.453329235871542 | validation: 1.343133591240236]
	TIME [epoch: 3.56 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3912600649343803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3912600649343803 | validation: 0.9561276817521022]
	TIME [epoch: 3.57 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0549157854659557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0549157854659557 | validation: 1.0333526853253214]
	TIME [epoch: 3.56 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.028516291052973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.028516291052973 | validation: 0.9263638979245874]
	TIME [epoch: 3.57 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9878450156457769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9878450156457769 | validation: 0.9022000391387988]
	TIME [epoch: 3.56 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9199530058125223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9199530058125223 | validation: 0.8618666324297225]
	TIME [epoch: 3.58 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8583058074129286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8583058074129286 | validation: 0.8568631352936162]
	TIME [epoch: 3.58 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8635730268534693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8635730268534693 | validation: 0.8986771553996216]
	TIME [epoch: 3.57 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8562554906604938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8562554906604938 | validation: 0.8737889566834132]
	TIME [epoch: 3.57 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8517696229424897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8517696229424897 | validation: 0.8841071006959794]
	TIME [epoch: 3.56 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8508385334451338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8508385334451338 | validation: 0.8611974551647932]
	TIME [epoch: 3.56 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8731294643361167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8731294643361167 | validation: 0.8442055293208647]
	TIME [epoch: 3.56 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8443596797022562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8443596797022562 | validation: 0.8677379664026468]
	TIME [epoch: 3.56 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8513043203091805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8513043203091805 | validation: 0.8256308585330672]
	TIME [epoch: 3.56 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.812334113892091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.812334113892091 | validation: 0.8220580315549114]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7861264493261713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7861264493261713 | validation: 0.8154406666733917]
	TIME [epoch: 3.57 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7843362072421826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7843362072421826 | validation: 0.9764660586262949]
	TIME [epoch: 3.57 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8584901244292584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8584901244292584 | validation: 0.7762349291810069]
	TIME [epoch: 3.57 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.770742929183069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.770742929183069 | validation: 0.8290754664751676]
	TIME [epoch: 3.57 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9130749573031791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9130749573031791 | validation: 0.8700824940333644]
	TIME [epoch: 3.58 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8127604243061236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8127604243061236 | validation: 0.8750013082641444]
	TIME [epoch: 3.57 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8320285011917886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8320285011917886 | validation: 0.9626171028868815]
	TIME [epoch: 3.58 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9705725625877542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9705725625877542 | validation: 0.7857585462655289]
	TIME [epoch: 3.57 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7421983800246857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7421983800246857 | validation: 0.7911237872691909]
	TIME [epoch: 3.59 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7288516664277641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7288516664277641 | validation: 0.8283324251079045]
	TIME [epoch: 3.58 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.751644458192686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.751644458192686 | validation: 0.844241291308145]
	TIME [epoch: 3.57 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.837608163734305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.837608163734305 | validation: 1.1241254451456215]
	TIME [epoch: 3.57 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1193402060744924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1193402060744924 | validation: 0.7907938377467489]
	TIME [epoch: 3.57 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8284265343289697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8284265343289697 | validation: 0.8613119133052204]
	TIME [epoch: 3.58 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8588217950803696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8588217950803696 | validation: 0.7137820486941197]
	TIME [epoch: 3.57 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7526790225973747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7526790225973747 | validation: 0.7685687053551533]
	TIME [epoch: 3.56 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7630097429460222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7630097429460222 | validation: 0.7737201607583611]
	TIME [epoch: 3.57 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6908784497377861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6908784497377861 | validation: 0.7613379759960512]
	TIME [epoch: 3.57 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6825785077205148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6825785077205148 | validation: 0.7235288100790997]
	TIME [epoch: 3.58 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.719232976812028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.719232976812028 | validation: 0.7265705736468224]
	TIME [epoch: 3.57 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6785166765815417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6785166765815417 | validation: 0.7230358882117871]
	TIME [epoch: 3.56 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6766693184789448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6766693184789448 | validation: 0.7178436445880458]
	TIME [epoch: 3.57 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6898645250588176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6898645250588176 | validation: 0.7192955131707506]
	TIME [epoch: 3.57 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6839933283574038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6839933283574038 | validation: 0.6916587240574383]
	TIME [epoch: 3.57 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6811626833216069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6811626833216069 | validation: 0.7506414196698445]
	TIME [epoch: 3.56 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6999409773464187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6999409773464187 | validation: 0.7500307678508463]
	TIME [epoch: 3.56 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7767024939999727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7767024939999727 | validation: 0.746779555466003]
	TIME [epoch: 3.56 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.749979269930338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.749979269930338 | validation: 0.7600575202394269]
	TIME [epoch: 3.57 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8021230077546335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8021230077546335 | validation: 0.9083986046171003]
	TIME [epoch: 3.56 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9028172926203792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9028172926203792 | validation: 0.7314693431049504]
	TIME [epoch: 3.56 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6951366775974568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6951366775974568 | validation: 0.8223885427088409]
	TIME [epoch: 3.56 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7928743402049457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7928743402049457 | validation: 0.7596945477940447]
	TIME [epoch: 3.56 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7263377241262113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7263377241262113 | validation: 0.7287759417780486]
	TIME [epoch: 3.56 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6755871221959515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6755871221959515 | validation: 0.7446920563873101]
	TIME [epoch: 3.56 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7006347820237286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7006347820237286 | validation: 0.708549567354846]
	TIME [epoch: 3.56 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.662609633388887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.662609633388887 | validation: 0.7308836171350988]
	TIME [epoch: 3.55 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7051760934057464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7051760934057464 | validation: 0.8225511891816718]
	TIME [epoch: 3.56 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7632753416149347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7632753416149347 | validation: 0.6881630300728236]
	TIME [epoch: 3.57 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.680703504862569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.680703504862569 | validation: 0.7226746923387345]
	TIME [epoch: 3.57 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7033272849726708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7033272849726708 | validation: 0.7704428190577413]
	TIME [epoch: 3.56 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7248590529559877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7248590529559877 | validation: 0.7384188792425604]
	TIME [epoch: 3.56 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7022094876264646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7022094876264646 | validation: 0.7271130459758809]
	TIME [epoch: 3.56 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7461078583330368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7461078583330368 | validation: 0.7062630497485366]
	TIME [epoch: 3.56 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7105427174346286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7105427174346286 | validation: 0.7922103062727486]
	TIME [epoch: 3.55 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7420060478940718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7420060478940718 | validation: 0.7525068586734878]
	TIME [epoch: 3.56 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7294549713981762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7294549713981762 | validation: 0.6963066521285584]
	TIME [epoch: 3.56 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6571924835470477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6571924835470477 | validation: 0.732911376395661]
	TIME [epoch: 3.56 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6794263948592563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6794263948592563 | validation: 0.719222907735005]
	TIME [epoch: 3.58 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6708429852145221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6708429852145221 | validation: 0.7100161558630032]
	TIME [epoch: 3.57 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6687336426181196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6687336426181196 | validation: 0.7597361104450017]
	TIME [epoch: 3.56 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7455428425542268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7455428425542268 | validation: 0.6901751205581061]
	TIME [epoch: 3.56 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6692879571144005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6692879571144005 | validation: 0.7421790761267053]
	TIME [epoch: 3.56 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7677023567415043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7677023567415043 | validation: 0.8931442217503133]
	TIME [epoch: 3.56 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8405103170853575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8405103170853575 | validation: 0.6926448646515507]
	TIME [epoch: 3.56 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7184750349775372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7184750349775372 | validation: 0.756358729827253]
	TIME [epoch: 3.55 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7208020333296109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7208020333296109 | validation: 0.7331119883953834]
	TIME [epoch: 3.56 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6838350597168895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6838350597168895 | validation: 0.716591265955842]
	TIME [epoch: 3.57 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6533101217115106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6533101217115106 | validation: 0.7329310625708158]
	TIME [epoch: 3.58 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6562296728519096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6562296728519096 | validation: 0.7224058973111341]
	TIME [epoch: 3.57 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6527734469903191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6527734469903191 | validation: 0.73387580199162]
	TIME [epoch: 3.56 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7087725117276072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7087725117276072 | validation: 0.7586353979957654]
	TIME [epoch: 3.57 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7145312876063166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7145312876063166 | validation: 0.7084544701529985]
	TIME [epoch: 3.57 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.673278742516169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.673278742516169 | validation: 0.6703894867928084]
	TIME [epoch: 3.56 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6508948706578677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6508948706578677 | validation: 0.7065992293349743]
	TIME [epoch: 3.56 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6789490568643148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6789490568643148 | validation: 0.6795190048060594]
	TIME [epoch: 3.56 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6858145316516072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6858145316516072 | validation: 0.685224915622888]
	TIME [epoch: 3.56 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6570775056626635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6570775056626635 | validation: 0.7037290048591861]
	TIME [epoch: 3.58 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6827753543700288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6827753543700288 | validation: 0.6829394719554331]
	TIME [epoch: 3.56 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.658341576948273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.658341576948273 | validation: 0.7179602521098305]
	TIME [epoch: 3.56 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.739240390710487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.739240390710487 | validation: 0.857843158862889]
	TIME [epoch: 3.56 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8209803507414165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8209803507414165 | validation: 0.6989747787864933]
	TIME [epoch: 3.56 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7064807340696162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7064807340696162 | validation: 0.7266743430356893]
	TIME [epoch: 3.55 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6909195623790692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6909195623790692 | validation: 0.7199301281708037]
	TIME [epoch: 3.56 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6937151489634252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6937151489634252 | validation: 0.7527377260719712]
	TIME [epoch: 3.56 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6972927786001384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6972927786001384 | validation: 0.7188635166156916]
	TIME [epoch: 3.56 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.664319222985863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.664319222985863 | validation: 0.7305138414581389]
	TIME [epoch: 3.56 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.692145259741354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.692145259741354 | validation: 0.7422713190036418]
	TIME [epoch: 3.57 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.708221355675804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.708221355675804 | validation: 0.7149089260800466]
	TIME [epoch: 3.56 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6986309765995261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6986309765995261 | validation: 0.6930513833367375]
	TIME [epoch: 3.56 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6595173723180956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6595173723180956 | validation: 0.70423212775844]
	TIME [epoch: 3.56 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6638812964537176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6638812964537176 | validation: 0.6908731855526256]
	TIME [epoch: 3.56 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6603066588651507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6603066588651507 | validation: 0.7079927144865505]
	TIME [epoch: 3.56 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6429057497138961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6429057497138961 | validation: 0.7006680886472385]
	TIME [epoch: 3.56 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6582970862229343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6582970862229343 | validation: 0.7301460524166414]
	TIME [epoch: 3.56 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6710973793797332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6710973793797332 | validation: 0.7250999312522536]
	TIME [epoch: 3.56 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7267733821721611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7267733821721611 | validation: 0.697633497937319]
	TIME [epoch: 3.57 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6650299496426995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6650299496426995 | validation: 0.7185832426653675]
	TIME [epoch: 3.57 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6980902540591061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6980902540591061 | validation: 0.7066688536183436]
	TIME [epoch: 3.56 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7024387591309313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7024387591309313 | validation: 0.7261167841077992]
	TIME [epoch: 3.56 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6817382825408776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6817382825408776 | validation: 0.6859598372416921]
	TIME [epoch: 3.56 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6494023380797773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6494023380797773 | validation: 0.7090165659142651]
	TIME [epoch: 3.56 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6478172384973584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6478172384973584 | validation: 0.6886188137729137]
	TIME [epoch: 3.56 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6566580991224454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6566580991224454 | validation: 0.7539668765112184]
	TIME [epoch: 3.56 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7403502061651345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7403502061651345 | validation: 0.784168706670167]
	TIME [epoch: 3.56 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7945037719473621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7945037719473621 | validation: 0.7235220448669558]
	TIME [epoch: 3.56 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6945703521339303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6945703521339303 | validation: 0.72513976036343]
	TIME [epoch: 3.57 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.69758395159612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.69758395159612 | validation: 0.719834352606129]
	TIME [epoch: 3.56 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6752911012846405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6752911012846405 | validation: 0.7306368654113352]
	TIME [epoch: 3.55 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6848919730799753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6848919730799753 | validation: 0.6747146647844429]
	TIME [epoch: 3.55 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.656003472730217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.656003472730217 | validation: 0.6909454885393949]
	TIME [epoch: 3.56 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6427872521935112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6427872521935112 | validation: 0.686404062478561]
	TIME [epoch: 3.56 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6383877795943194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6383877795943194 | validation: 0.6946450412811054]
	TIME [epoch: 3.56 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6334770723805514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6334770723805514 | validation: 0.6886514357798831]
	TIME [epoch: 3.56 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6420615892871865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6420615892871865 | validation: 0.6767997175699042]
	TIME [epoch: 3.56 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6789344097574316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6789344097574316 | validation: 0.7183409233590807]
	TIME [epoch: 3.56 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7291596210216299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7291596210216299 | validation: 0.6997105601940992]
	TIME [epoch: 3.58 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6858666720833683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6858666720833683 | validation: 0.7447127566302906]
	TIME [epoch: 3.57 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7605571385551199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7605571385551199 | validation: 0.6872213085742115]
	TIME [epoch: 3.56 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7101425518763074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7101425518763074 | validation: 0.6889307198016255]
	TIME [epoch: 3.56 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6768661972127442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6768661972127442 | validation: 0.6950883225566975]
	TIME [epoch: 3.55 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6406419158327318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6406419158327318 | validation: 0.6849664351824964]
	TIME [epoch: 3.56 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6459004899209007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6459004899209007 | validation: 0.7005982992889148]
	TIME [epoch: 3.57 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6676062808406027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6676062808406027 | validation: 0.6975727675722228]
	TIME [epoch: 3.56 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6348281264169066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6348281264169066 | validation: 0.6832272228045783]
	TIME [epoch: 3.57 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6453393971585846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6453393971585846 | validation: 0.7362796060419716]
	TIME [epoch: 3.55 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7066230397294456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7066230397294456 | validation: 0.6949011224231476]
	TIME [epoch: 3.58 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6940186075321881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6940186075321881 | validation: 0.7200667909578313]
	TIME [epoch: 3.56 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7024447979774565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7024447979774565 | validation: 0.6762552748492782]
	TIME [epoch: 3.57 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.672025931596541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.672025931596541 | validation: 0.6895484494672408]
	TIME [epoch: 3.56 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6712837250448684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6712837250448684 | validation: 0.6895310513657574]
	TIME [epoch: 3.56 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6675812285188907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6675812285188907 | validation: 0.6881705493514743]
	TIME [epoch: 3.56 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6421206341469694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6421206341469694 | validation: 0.6754906725267301]
	TIME [epoch: 3.56 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6537671718343829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6537671718343829 | validation: 0.6690162563831072]
	TIME [epoch: 3.56 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_158.pth
	Model improved!!!
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.633293759113514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.633293759113514 | validation: 0.6876550364420579]
	TIME [epoch: 3.56 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6429889985799919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6429889985799919 | validation: 0.6611837264282394]
	TIME [epoch: 3.57 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.657420986149987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.657420986149987 | validation: 0.6858004397224525]
	TIME [epoch: 3.58 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6352669779004159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6352669779004159 | validation: 0.7056733853846323]
	TIME [epoch: 3.56 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6806484316376922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6806484316376922 | validation: 0.7022873007497129]
	TIME [epoch: 3.56 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6990007787860566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6990007787860566 | validation: 0.7210464544573334]
	TIME [epoch: 3.57 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7001485155230379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7001485155230379 | validation: 0.6756903815822906]
	TIME [epoch: 3.57 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6393763902288674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6393763902288674 | validation: 0.6888464237343185]
	TIME [epoch: 3.57 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6646927870483711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6646927870483711 | validation: 0.6852693909122112]
	TIME [epoch: 3.56 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6447903836431914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6447903836431914 | validation: 0.6971930053080936]
	TIME [epoch: 3.57 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6573275112211209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6573275112211209 | validation: 0.6552650199818011]
	TIME [epoch: 3.56 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6556696707423133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6556696707423133 | validation: 0.6728885912577918]
	TIME [epoch: 3.58 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6325899995433931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6325899995433931 | validation: 0.6739840249127969]
	TIME [epoch: 3.56 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6236039621980457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6236039621980457 | validation: 0.7636285945544672]
	TIME [epoch: 3.57 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9105567224293392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9105567224293392 | validation: 0.7131610484981297]
	TIME [epoch: 3.56 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7452098231935215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7452098231935215 | validation: 0.7059038561435318]
	TIME [epoch: 3.56 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7648753834093631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7648753834093631 | validation: 0.6707270608997872]
	TIME [epoch: 3.56 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.660390547313454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.660390547313454 | validation: 0.7177735052300253]
	TIME [epoch: 3.56 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6563210131769381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6563210131769381 | validation: 0.6910968668569331]
	TIME [epoch: 3.56 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6317814937173869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6317814937173869 | validation: 0.6847019127029802]
	TIME [epoch: 3.57 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6482096592763159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6482096592763159 | validation: 0.6955472076096414]
	TIME [epoch: 3.56 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6323874364285065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6323874364285065 | validation: 0.67846717763407]
	TIME [epoch: 3.58 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6388486707709687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6388486707709687 | validation: 0.6490631534894119]
	TIME [epoch: 3.57 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6370703898450638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6370703898450638 | validation: 0.6480456096049138]
	TIME [epoch: 3.56 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_182.pth
	Model improved!!!
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6232580251841813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6232580251841813 | validation: 0.6853983789902496]
	TIME [epoch: 3.56 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7040878287567168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7040878287567168 | validation: 0.6451998491307144]
	TIME [epoch: 3.58 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_184.pth
	Model improved!!!
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6552802813778359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6552802813778359 | validation: 0.6703222744452445]
	TIME [epoch: 3.56 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6691510283008301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6691510283008301 | validation: 0.680150422165813]
	TIME [epoch: 3.57 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6477510680922255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6477510680922255 | validation: 0.6806274523834501]
	TIME [epoch: 3.57 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6227801110739781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6227801110739781 | validation: 0.6567970797386538]
	TIME [epoch: 3.57 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6432793095956174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6432793095956174 | validation: 0.6604844605151838]
	TIME [epoch: 3.58 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6191886118966012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6191886118966012 | validation: 0.6704764877715386]
	TIME [epoch: 3.57 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.604551505673296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.604551505673296 | validation: 0.6562121449590033]
	TIME [epoch: 3.56 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6054784039535511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6054784039535511 | validation: 0.6546905196219126]
	TIME [epoch: 3.56 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5967003465451046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5967003465451046 | validation: 0.6517628353882966]
	TIME [epoch: 3.56 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5919181576676416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5919181576676416 | validation: 0.6594595814307109]
	TIME [epoch: 3.57 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.640384851348118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.640384851348118 | validation: 0.6947454566221546]
	TIME [epoch: 3.56 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7321147498250835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7321147498250835 | validation: 0.6400815208157744]
	TIME [epoch: 3.57 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6668520189109856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6668520189109856 | validation: 0.7201421292305561]
	TIME [epoch: 3.56 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7093748236261632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7093748236261632 | validation: 0.6424218706398522]
	TIME [epoch: 3.58 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6091134373986405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6091134373986405 | validation: 0.6612133077945588]
	TIME [epoch: 3.56 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6107832746801442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6107832746801442 | validation: 0.633400575008595]
	TIME [epoch: 3.56 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_200.pth
	Model improved!!!
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5910818011681124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5910818011681124 | validation: 0.6388651414890121]
	TIME [epoch: 33.4 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5836197087288947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5836197087288947 | validation: 0.6134209565511144]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5649550640789992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5649550640789992 | validation: 0.6021178346135483]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5600463267814919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5600463267814919 | validation: 0.6183923931063519]
	TIME [epoch: 7.72 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5455186821457044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5455186821457044 | validation: 0.5629268966487627]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_205.pth
	Model improved!!!
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5593485218901687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5593485218901687 | validation: 0.5454089099031798]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_206.pth
	Model improved!!!
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5329035057371165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5329035057371165 | validation: 0.599076271452498]
	TIME [epoch: 7.75 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.113668649584763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.113668649584763 | validation: 1.1686611003929084]
	TIME [epoch: 7.75 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3972270158427733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3972270158427733 | validation: 0.8298403539784609]
	TIME [epoch: 7.73 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0435965275723718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0435965275723718 | validation: 0.7431489334744423]
	TIME [epoch: 7.75 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7904185161530566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7904185161530566 | validation: 0.6590179696765888]
	TIME [epoch: 7.74 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6981583750896675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6981583750896675 | validation: 0.6366785376685947]
	TIME [epoch: 7.74 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6401197872753154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6401197872753154 | validation: 0.6494404599187167]
	TIME [epoch: 7.73 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5960244749306262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5960244749306262 | validation: 0.6538675491424748]
	TIME [epoch: 7.75 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5837510805695145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5837510805695145 | validation: 0.6333906826613701]
	TIME [epoch: 7.74 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5696063210984038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5696063210984038 | validation: 0.5918505760563756]
	TIME [epoch: 7.75 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5482444498060463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5482444498060463 | validation: 0.5380590570496547]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_217.pth
	Model improved!!!
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5046411333276863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5046411333276863 | validation: 2.840691940553306]
	TIME [epoch: 7.74 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.259868365207998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.259868365207998 | validation: 0.6160278176624527]
	TIME [epoch: 7.72 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6805707505869835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6805707505869835 | validation: 0.6721301908843214]
	TIME [epoch: 7.74 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7590389421548659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7590389421548659 | validation: 0.6196726391900621]
	TIME [epoch: 7.75 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6206701699772027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6206701699772027 | validation: 0.5622380291953618]
	TIME [epoch: 7.74 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5734113653726531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5734113653726531 | validation: 0.5541954087481641]
	TIME [epoch: 7.73 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5451387466931448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5451387466931448 | validation: 0.5384687810038095]
	TIME [epoch: 7.74 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5174604989851429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5174604989851429 | validation: 0.5275498415295344]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_225.pth
	Model improved!!!
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5039259791857253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5039259791857253 | validation: 0.5122909273543429]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47961279316177796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47961279316177796 | validation: 0.46042971717312986]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_227.pth
	Model improved!!!
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4417974924394076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4417974924394076 | validation: 0.4232355335512429]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_228.pth
	Model improved!!!
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4281115763902056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4281115763902056 | validation: 0.4464189669635915]
	TIME [epoch: 7.74 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4367671647336809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4367671647336809 | validation: 0.4047279482523954]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_230.pth
	Model improved!!!
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39836254939101506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39836254939101506 | validation: 0.3914090758141349]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_231.pth
	Model improved!!!
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.398305823159165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.398305823159165 | validation: 0.3792884331604328]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_232.pth
	Model improved!!!
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3625271480288822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3625271480288822 | validation: 0.3824904775752656]
	TIME [epoch: 7.72 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39309322904944777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39309322904944777 | validation: 0.3742615723820126]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_234.pth
	Model improved!!!
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3513238157489393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3513238157489393 | validation: 0.36964140576307525]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_235.pth
	Model improved!!!
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37603853379448027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37603853379448027 | validation: 0.4644445428740186]
	TIME [epoch: 7.73 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5454779257097414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5454779257097414 | validation: 0.5435337256823802]
	TIME [epoch: 7.74 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5692907834883435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5692907834883435 | validation: 0.5924236294180962]
	TIME [epoch: 7.74 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6100395678719766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6100395678719766 | validation: 0.48159343549630584]
	TIME [epoch: 7.75 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4496547013899411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4496547013899411 | validation: 0.4017015109055178]
	TIME [epoch: 7.73 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3545989210815567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3545989210815567 | validation: 0.3804849850783925]
	TIME [epoch: 7.74 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3600011388995146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3600011388995146 | validation: 0.3935177022582109]
	TIME [epoch: 7.74 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4165406263518645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4165406263518645 | validation: 1.0792034667155008]
	TIME [epoch: 7.76 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2070302702625082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2070302702625082 | validation: 1.2177056578132979]
	TIME [epoch: 7.72 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5047250967904091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5047250967904091 | validation: 0.8292733899531015]
	TIME [epoch: 7.74 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0660357037886916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0660357037886916 | validation: 0.5472544956547404]
	TIME [epoch: 7.72 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5030392067253996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5030392067253996 | validation: 0.41768413948616917]
	TIME [epoch: 7.75 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3882855321720372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3882855321720372 | validation: 0.4220852360461691]
	TIME [epoch: 7.74 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41573403830462774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41573403830462774 | validation: 0.434851613104868]
	TIME [epoch: 7.75 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5150608608361054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5150608608361054 | validation: 0.5714193112431069]
	TIME [epoch: 7.72 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7738454438715863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7738454438715863 | validation: 0.6906493981263577]
	TIME [epoch: 7.74 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0638646987115263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0638646987115263 | validation: 0.4775392879254696]
	TIME [epoch: 7.75 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9087397826935444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9087397826935444 | validation: 0.3928329879390611]
	TIME [epoch: 7.76 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6297228589226438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6297228589226438 | validation: 0.6619821941022273]
	TIME [epoch: 7.73 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7016045311792946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7016045311792946 | validation: 0.4217499002592009]
	TIME [epoch: 7.74 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38376130076893933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38376130076893933 | validation: 0.4025394815887993]
	TIME [epoch: 7.74 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3639230567170826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3639230567170826 | validation: 0.3361982111773705]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_257.pth
	Model improved!!!
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3055946565344778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3055946565344778 | validation: 0.31243539951370836]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2927011474656037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2927011474656037 | validation: 0.29120833500840576]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_259.pth
	Model improved!!!
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28222008579344865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28222008579344865 | validation: 0.2884163684649636]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_260.pth
	Model improved!!!
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2667375599173711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2667375599173711 | validation: 0.3961682216350039]
	TIME [epoch: 7.76 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3192611304752332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3192611304752332 | validation: 0.30386031353117693]
	TIME [epoch: 7.74 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26876179015322593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26876179015322593 | validation: 0.256898356833252]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_263.pth
	Model improved!!!
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2306491747984833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2306491747984833 | validation: 0.24629720765339191]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_264.pth
	Model improved!!!
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21710716515949238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21710716515949238 | validation: 0.27068291314063303]
	TIME [epoch: 7.75 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22923267866636082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22923267866636082 | validation: 0.3092659071643004]
	TIME [epoch: 7.74 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2859856264486294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2859856264486294 | validation: 0.6911305058647265]
	TIME [epoch: 7.75 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5810650035837306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5810650035837306 | validation: 0.3726723800196347]
	TIME [epoch: 7.75 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3106622744772628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3106622744772628 | validation: 0.9834515740050206]
	TIME [epoch: 7.71 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.151029397958261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.151029397958261 | validation: 0.5668196274404933]
	TIME [epoch: 7.74 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6022249255942067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6022249255942067 | validation: 0.3598825354175419]
	TIME [epoch: 7.73 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40883452888042976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40883452888042976 | validation: 0.2986683988695266]
	TIME [epoch: 7.74 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30196474032222603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30196474032222603 | validation: 0.3362162381793781]
	TIME [epoch: 7.75 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3329871698794364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3329871698794364 | validation: 0.2581233740785424]
	TIME [epoch: 7.74 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23573023181252112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23573023181252112 | validation: 0.25873383973711545]
	TIME [epoch: 7.75 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2229951546782668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2229951546782668 | validation: 0.2234508070530332]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_276.pth
	Model improved!!!
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20821907950979196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20821907950979196 | validation: 0.20366430729381452]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19597397094879523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19597397094879523 | validation: 0.20115059764970541]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_278.pth
	Model improved!!!
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1936601009448718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1936601009448718 | validation: 0.1991560251207886]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_279.pth
	Model improved!!!
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19070186720671628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19070186720671628 | validation: 0.2074968237880758]
	TIME [epoch: 7.76 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18410525642809572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18410525642809572 | validation: 0.20159027817936737]
	TIME [epoch: 7.74 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1803137849146249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1803137849146249 | validation: 0.19532250602064294]
	TIME [epoch: 7.71 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_282.pth
	Model improved!!!
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17193713727126217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17193713727126217 | validation: 0.19271932735210395]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_283.pth
	Model improved!!!
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16648185218586747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16648185218586747 | validation: 0.19825887401814934]
	TIME [epoch: 7.73 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1674807647886147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1674807647886147 | validation: 0.20183441179196954]
	TIME [epoch: 7.72 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19443769749028517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19443769749028517 | validation: 0.3814557339905836]
	TIME [epoch: 7.72 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31235651972014333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31235651972014333 | validation: 0.30116393960734655]
	TIME [epoch: 7.71 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26342196769410586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26342196769410586 | validation: 0.2131482885824625]
	TIME [epoch: 7.72 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17852317100008427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17852317100008427 | validation: 0.25122047953985094]
	TIME [epoch: 7.72 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19954668695308175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19954668695308175 | validation: 0.28111271857700754]
	TIME [epoch: 7.72 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24399054102606343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24399054102606343 | validation: 0.21453174016832088]
	TIME [epoch: 7.72 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2027752464651489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2027752464651489 | validation: 0.21310198405859185]
	TIME [epoch: 7.72 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1717762013145321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1717762013145321 | validation: 0.22681453926704256]
	TIME [epoch: 7.72 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18266518491137113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18266518491137113 | validation: 0.24049770169484336]
	TIME [epoch: 7.72 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21186732329829128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21186732329829128 | validation: 0.3049013633402724]
	TIME [epoch: 7.71 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2515068779125782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2515068779125782 | validation: 0.36500125866207167]
	TIME [epoch: 7.71 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3080381235207616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3080381235207616 | validation: 0.422617008051226]
	TIME [epoch: 7.71 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47672204092582304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47672204092582304 | validation: 0.3214905729023281]
	TIME [epoch: 7.73 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29596740443613795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29596740443613795 | validation: 0.3419948855170474]
	TIME [epoch: 7.71 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4620457528578487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4620457528578487 | validation: 0.5745094638123659]
	TIME [epoch: 7.71 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4492529288657242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4492529288657242 | validation: 0.4067685556369881]
	TIME [epoch: 7.71 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28760300250524445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28760300250524445 | validation: 0.31415126878979693]
	TIME [epoch: 7.74 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27126026482109344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27126026482109344 | validation: 0.2728907443992256]
	TIME [epoch: 7.74 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2619570309460938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2619570309460938 | validation: 0.24981562650789788]
	TIME [epoch: 7.71 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27248193924202735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27248193924202735 | validation: 0.22934501926720824]
	TIME [epoch: 7.72 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20039699217865256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20039699217865256 | validation: 0.19270906151662556]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_306.pth
	Model improved!!!
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15873680872825674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15873680872825674 | validation: 0.2051383276781591]
	TIME [epoch: 7.73 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16748768003030556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16748768003030556 | validation: 0.1822440232609469]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_308.pth
	Model improved!!!
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14841459795526107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14841459795526107 | validation: 0.1730763091796566]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_309.pth
	Model improved!!!
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14400296459223835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14400296459223835 | validation: 0.19510807822379894]
	TIME [epoch: 7.73 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14560411070147866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14560411070147866 | validation: 0.19532724583007072]
	TIME [epoch: 7.76 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16832463024374797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16832463024374797 | validation: 0.44408966406959155]
	TIME [epoch: 7.76 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32356379155000653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32356379155000653 | validation: 0.24845385620399574]
	TIME [epoch: 7.74 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21898790177001265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21898790177001265 | validation: 0.1974175994076359]
	TIME [epoch: 7.75 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16619400785213803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16619400785213803 | validation: 0.23712532698067026]
	TIME [epoch: 7.74 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1831023907997018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1831023907997018 | validation: 0.18765481423172936]
	TIME [epoch: 7.75 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1547834042026923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1547834042026923 | validation: 0.18681090889258486]
	TIME [epoch: 7.74 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14903235456441338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14903235456441338 | validation: 0.16647354834178862]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_318.pth
	Model improved!!!
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13119578526869755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13119578526869755 | validation: 0.16006455663527178]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_319.pth
	Model improved!!!
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12372242265231755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12372242265231755 | validation: 0.14963277921292512]
	TIME [epoch: 7.72 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_320.pth
	Model improved!!!
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11604539140456904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11604539140456904 | validation: 0.16355265507857203]
	TIME [epoch: 7.75 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12251660998561896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12251660998561896 | validation: 0.2641746369880562]
	TIME [epoch: 7.73 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1780707050764159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1780707050764159 | validation: 0.4606331120796573]
	TIME [epoch: 7.73 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33755776547209293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33755776547209293 | validation: 0.3020134110960417]
	TIME [epoch: 7.73 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20839463587308651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20839463587308651 | validation: 0.27479030689957723]
	TIME [epoch: 7.74 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27035613761199606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27035613761199606 | validation: 0.2715223020454611]
	TIME [epoch: 7.74 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1797852844569054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1797852844569054 | validation: 0.3262234658549179]
	TIME [epoch: 7.73 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2535882737650165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2535882737650165 | validation: 0.23156565942982943]
	TIME [epoch: 7.74 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31290072504719424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31290072504719424 | validation: 0.20666187350219603]
	TIME [epoch: 7.73 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2868167909268068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2868167909268068 | validation: 0.21525090658779833]
	TIME [epoch: 7.74 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20555103794939172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20555103794939172 | validation: 0.1954039626082701]
	TIME [epoch: 7.74 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13949411535103654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13949411535103654 | validation: 0.17308285105702115]
	TIME [epoch: 7.74 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13590284719939894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13590284719939894 | validation: 0.1847618007054645]
	TIME [epoch: 7.73 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1289074903742915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1289074903742915 | validation: 0.15438223453313815]
	TIME [epoch: 7.74 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12053189635853151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12053189635853151 | validation: 0.1595424227743478]
	TIME [epoch: 7.76 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1219712278746584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1219712278746584 | validation: 0.1596416849039665]
	TIME [epoch: 7.73 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11917712926063971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11917712926063971 | validation: 0.16410282871061072]
	TIME [epoch: 7.73 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12888263129694927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12888263129694927 | validation: 0.22159190405014445]
	TIME [epoch: 7.73 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16325575357151104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16325575357151104 | validation: 0.1941455037926322]
	TIME [epoch: 7.74 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16731332591754763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16731332591754763 | validation: 0.3338223665576376]
	TIME [epoch: 7.75 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2262265245812519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2262265245812519 | validation: 0.586941101360705]
	TIME [epoch: 7.74 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.431423532388898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.431423532388898 | validation: 0.390582000011716]
	TIME [epoch: 7.74 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3526338859826418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3526338859826418 | validation: 0.3124520929491644]
	TIME [epoch: 7.74 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2687352457587717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2687352457587717 | validation: 0.25821217079206976]
	TIME [epoch: 7.74 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21617672200668175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21617672200668175 | validation: 0.16957034398617005]
	TIME [epoch: 7.75 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16469626067503487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16469626067503487 | validation: 0.14877087889002474]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_346.pth
	Model improved!!!
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1450380544783107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1450380544783107 | validation: 0.13997720405140784]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_347.pth
	Model improved!!!
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13828448729973136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13828448729973136 | validation: 0.1535746617347456]
	TIME [epoch: 7.77 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13524046396154016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13524046396154016 | validation: 0.1395266267763445]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_349.pth
	Model improved!!!
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1222633246572503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1222633246572503 | validation: 0.16663115952375743]
	TIME [epoch: 7.75 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14994804494427308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14994804494427308 | validation: 0.24773848966371462]
	TIME [epoch: 7.73 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18068563374529314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18068563374529314 | validation: 0.15152182184111784]
	TIME [epoch: 7.74 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1296439398044331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1296439398044331 | validation: 0.15452856958846872]
	TIME [epoch: 7.73 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1396481224711367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1396481224711367 | validation: 0.1655539819025716]
	TIME [epoch: 7.75 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1410972624797049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1410972624797049 | validation: 0.14181401161138382]
	TIME [epoch: 7.74 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11157582572370886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11157582572370886 | validation: 0.15521681634286236]
	TIME [epoch: 7.73 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13376498204198367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13376498204198367 | validation: 0.30900038459477513]
	TIME [epoch: 7.74 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22775490786427643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22775490786427643 | validation: 0.26860904329263935]
	TIME [epoch: 7.75 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2460985788612924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2460985788612924 | validation: 0.1474082349529652]
	TIME [epoch: 7.74 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1354443241813182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1354443241813182 | validation: 0.1977371227507023]
	TIME [epoch: 7.74 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1491002346670805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1491002346670805 | validation: 0.16049992993504628]
	TIME [epoch: 7.73 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12796797914743088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12796797914743088 | validation: 0.13012626631634833]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_362.pth
	Model improved!!!
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1387492278192501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1387492278192501 | validation: 0.1748139215448434]
	TIME [epoch: 7.77 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11673533579382549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11673533579382549 | validation: 0.12100182693348334]
	TIME [epoch: 7.74 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_364.pth
	Model improved!!!
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10252586881833471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10252586881833471 | validation: 0.12385280256952594]
	TIME [epoch: 7.76 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10236691101613149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10236691101613149 | validation: 0.12175788605134946]
	TIME [epoch: 7.76 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09548141948434304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09548141948434304 | validation: 0.12750473641499213]
	TIME [epoch: 7.78 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10232687336919215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10232687336919215 | validation: 0.12718669994099852]
	TIME [epoch: 7.76 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10285200765823425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10285200765823425 | validation: 0.18401608685227377]
	TIME [epoch: 7.76 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17261924827605962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17261924827605962 | validation: 0.4591141688150908]
	TIME [epoch: 7.76 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38499098715680674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38499098715680674 | validation: 0.15295351316850192]
	TIME [epoch: 7.78 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14574308178427425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14574308178427425 | validation: 0.14946112275857323]
	TIME [epoch: 7.78 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1508137881285494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1508137881285494 | validation: 0.11962925844681245]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_373.pth
	Model improved!!!
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11611345317825635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11611345317825635 | validation: 0.14558407628655276]
	TIME [epoch: 7.75 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11227008769258923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11227008769258923 | validation: 0.15132700596588222]
	TIME [epoch: 7.72 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11298821399122949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11298821399122949 | validation: 0.15368457403746524]
	TIME [epoch: 7.76 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13889376133176265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13889376133176265 | validation: 0.3802972632663453]
	TIME [epoch: 7.73 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2957577977936675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2957577977936675 | validation: 0.19169718678545622]
	TIME [epoch: 7.76 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16904450510860072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16904450510860072 | validation: 0.24311150390945016]
	TIME [epoch: 7.72 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22367762804680927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22367762804680927 | validation: 0.29782907690433197]
	TIME [epoch: 7.76 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2768701343210687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2768701343210687 | validation: 0.562042260900875]
	TIME [epoch: 7.74 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5001584435029256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5001584435029256 | validation: 0.346627585536646]
	TIME [epoch: 7.72 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29527198703101587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29527198703101587 | validation: 0.3067137029964091]
	TIME [epoch: 7.73 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2600281179849715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2600281179849715 | validation: 0.255962419258963]
	TIME [epoch: 7.72 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25436876463472013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25436876463472013 | validation: 0.14116964145995867]
	TIME [epoch: 7.73 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15234453646328905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15234453646328905 | validation: 0.14122256863479332]
	TIME [epoch: 7.74 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13930728726658798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13930728726658798 | validation: 0.2244941227241929]
	TIME [epoch: 7.72 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1761767999261223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1761767999261223 | validation: 0.17460516413586574]
	TIME [epoch: 7.72 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19502911696618874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19502911696618874 | validation: 0.2418924885435895]
	TIME [epoch: 7.73 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21282527413349123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21282527413349123 | validation: 0.18669956522867742]
	TIME [epoch: 7.73 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1803001193497211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1803001193497211 | validation: 0.1265660178416228]
	TIME [epoch: 7.73 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14155450326262936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14155450326262936 | validation: 0.13336202617816248]
	TIME [epoch: 7.72 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12151925343659584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12151925343659584 | validation: 0.20869137583911065]
	TIME [epoch: 7.73 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1441467656347561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1441467656347561 | validation: 0.14615679314369787]
	TIME [epoch: 7.72 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10976077475233798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10976077475233798 | validation: 0.14569230063800329]
	TIME [epoch: 7.74 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1110473126564106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1110473126564106 | validation: 0.12902718421167467]
	TIME [epoch: 7.71 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10337957813522548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10337957813522548 | validation: 0.13593153066103947]
	TIME [epoch: 7.73 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0994425444201702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0994425444201702 | validation: 0.12851481393197525]
	TIME [epoch: 7.72 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10031473475708037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10031473475708037 | validation: 0.17243624937143934]
	TIME [epoch: 7.75 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1258417278131016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1258417278131016 | validation: 0.2292117668113236]
	TIME [epoch: 7.74 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1784748592875024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1784748592875024 | validation: 0.2405081811779092]
	TIME [epoch: 7.73 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15970676807864576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15970676807864576 | validation: 0.143166066603769]
	TIME [epoch: 7.73 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11440393562002572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11440393562002572 | validation: 0.13875942873124855]
	TIME [epoch: 7.72 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10561208802520441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10561208802520441 | validation: 0.13014316220604624]
	TIME [epoch: 7.74 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10034746206644148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10034746206644148 | validation: 0.14108724268706804]
	TIME [epoch: 7.73 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09718800275231304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09718800275231304 | validation: 0.1062119262935509]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_406.pth
	Model improved!!!
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1005886695127009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1005886695127009 | validation: 0.15252853050900758]
	TIME [epoch: 7.74 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11211531352926091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11211531352926091 | validation: 0.2553693681549097]
	TIME [epoch: 7.75 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19925348094153733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19925348094153733 | validation: 0.272336787913439]
	TIME [epoch: 7.76 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1909133276998812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1909133276998812 | validation: 0.1978722666902336]
	TIME [epoch: 7.74 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12862997888881483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12862997888881483 | validation: 0.12955137281551346]
	TIME [epoch: 7.73 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12675980074466053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12675980074466053 | validation: 0.13800918037285156]
	TIME [epoch: 7.74 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1078399273288504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1078399273288504 | validation: 0.18098854914734003]
	TIME [epoch: 7.75 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13317250990807136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13317250990807136 | validation: 0.15185127025157918]
	TIME [epoch: 7.75 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1421226695126762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1421226695126762 | validation: 0.3071519791004044]
	TIME [epoch: 7.74 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2854715683415429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2854715683415429 | validation: 0.3192553019778164]
	TIME [epoch: 7.74 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30457821007144376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30457821007144376 | validation: 0.23456247511783973]
	TIME [epoch: 7.76 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2375414632012268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2375414632012268 | validation: 0.13980109396685442]
	TIME [epoch: 7.75 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17115119450204017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17115119450204017 | validation: 0.5086044404102702]
	TIME [epoch: 7.74 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3878389100451278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3878389100451278 | validation: 0.3253663904641022]
	TIME [epoch: 7.73 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27205828059954057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27205828059954057 | validation: 0.13437587937502832]
	TIME [epoch: 7.74 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14358665195196174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14358665195196174 | validation: 0.1153436690748972]
	TIME [epoch: 7.74 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14349399520335626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14349399520335626 | validation: 0.11376167962748213]
	TIME [epoch: 7.76 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13723818840538132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13723818840538132 | validation: 0.11906506735605871]
	TIME [epoch: 7.74 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10272900892315584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10272900892315584 | validation: 0.12324023007172298]
	TIME [epoch: 7.74 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10553390133536049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10553390133536049 | validation: 0.14680683609544595]
	TIME [epoch: 7.75 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10841734548255134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10841734548255134 | validation: 0.11212239992163205]
	TIME [epoch: 7.77 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09456102363449667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09456102363449667 | validation: 0.11019264469508216]
	TIME [epoch: 7.76 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08819732747631781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08819732747631781 | validation: 0.10817214920159822]
	TIME [epoch: 7.76 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09283832009976567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09283832009976567 | validation: 0.12073918787698115]
	TIME [epoch: 7.76 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10081677850395543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10081677850395543 | validation: 0.1595910366931828]
	TIME [epoch: 7.76 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12734913606926937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12734913606926937 | validation: 0.22774208409863334]
	TIME [epoch: 7.76 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17889661283674577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17889661283674577 | validation: 0.19889306560606682]
	TIME [epoch: 7.75 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1331377519982699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1331377519982699 | validation: 0.128332397012997]
	TIME [epoch: 7.74 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12058708312379317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12058708312379317 | validation: 0.16529340090792732]
	TIME [epoch: 7.74 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1535792579342811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1535792579342811 | validation: 0.15454394709892988]
	TIME [epoch: 7.77 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15261820308646779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15261820308646779 | validation: 0.17617121273916367]
	TIME [epoch: 7.77 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13466698814848768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13466698814848768 | validation: 0.11878345153078007]
	TIME [epoch: 7.74 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0919963879757669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0919963879757669 | validation: 0.11744798145481972]
	TIME [epoch: 7.74 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10351889694863581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10351889694863581 | validation: 0.2253390434474496]
	TIME [epoch: 7.75 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14017035678583672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14017035678583672 | validation: 0.1267103239330821]
	TIME [epoch: 7.76 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11647469506545903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11647469506545903 | validation: 0.10046822676071493]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_442.pth
	Model improved!!!
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10794633160431588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10794633160431588 | validation: 0.10816180668383524]
	TIME [epoch: 7.74 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.094960496336092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.094960496336092 | validation: 0.1436024627951399]
	TIME [epoch: 7.75 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10587589302479566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10587589302479566 | validation: 0.17430733414240246]
	TIME [epoch: 7.75 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12988479884358115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12988479884358115 | validation: 0.1437486245876491]
	TIME [epoch: 7.76 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1133450729764278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1133450729764278 | validation: 0.12809525926214668]
	TIME [epoch: 7.74 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10361270385492492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10361270385492492 | validation: 0.12853045064982502]
	TIME [epoch: 7.74 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09335241290149025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09335241290149025 | validation: 0.15682655466968726]
	TIME [epoch: 7.75 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11712190386964322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11712190386964322 | validation: 0.20220643914712985]
	TIME [epoch: 7.75 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13013225007665813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13013225007665813 | validation: 0.11782516106493957]
	TIME [epoch: 7.76 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11055059988174931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11055059988174931 | validation: 0.13792534354724895]
	TIME [epoch: 7.73 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10619285675834032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10619285675834032 | validation: 0.10836596844099794]
	TIME [epoch: 7.75 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10330628143238019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10330628143238019 | validation: 0.14159476634948343]
	TIME [epoch: 7.74 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11948861536323539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11948861536323539 | validation: 0.15261189561545274]
	TIME [epoch: 7.75 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13124618867036475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13124618867036475 | validation: 0.18846567660452995]
	TIME [epoch: 7.75 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15082937689634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15082937689634 | validation: 0.33386886705747576]
	TIME [epoch: 7.75 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22644451438059837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22644451438059837 | validation: 0.14689945239805527]
	TIME [epoch: 7.74 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09659708649332281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09659708649332281 | validation: 0.17095342235273922]
	TIME [epoch: 7.75 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1231920069569755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1231920069569755 | validation: 0.11301166401198502]
	TIME [epoch: 7.76 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09198260074936862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09198260074936862 | validation: 0.10399425649668477]
	TIME [epoch: 7.75 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08519669465088853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08519669465088853 | validation: 0.1260046119907361]
	TIME [epoch: 7.74 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08710889325677261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08710889325677261 | validation: 0.13028959187564482]
	TIME [epoch: 7.74 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10901578004472974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10901578004472974 | validation: 0.17920295881837264]
	TIME [epoch: 7.74 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16858264086953212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16858264086953212 | validation: 0.18220435669828933]
	TIME [epoch: 7.76 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16681997555198544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16681997555198544 | validation: 0.09491056231673531]
	TIME [epoch: 7.73 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_466.pth
	Model improved!!!
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08132721458542211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08132721458542211 | validation: 0.10345235167329636]
	TIME [epoch: 7.74 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0826870585803373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0826870585803373 | validation: 0.11690995894583572]
	TIME [epoch: 7.74 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09692153425018195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09692153425018195 | validation: 0.12624156416631424]
	TIME [epoch: 7.76 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12398451142828122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12398451142828122 | validation: 0.4357865756692668]
	TIME [epoch: 7.75 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2926326768316396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2926326768316396 | validation: 1.2028615117631878]
	TIME [epoch: 7.75 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9982653521380619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9982653521380619 | validation: 0.19325467072447036]
	TIME [epoch: 7.74 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15823958914693492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15823958914693492 | validation: 0.1693077582787177]
	TIME [epoch: 7.76 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21273082948964647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21273082948964647 | validation: 0.2580929292366494]
	TIME [epoch: 7.75 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42479180446673814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42479180446673814 | validation: 0.27942785220335636]
	TIME [epoch: 7.76 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.564561623147693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.564561623147693 | validation: 0.2576626032812674]
	TIME [epoch: 7.74 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4287430747842752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4287430747842752 | validation: 0.19685184588938842]
	TIME [epoch: 7.75 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31261128314440684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31261128314440684 | validation: 0.12224525671941451]
	TIME [epoch: 7.74 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20372871606386667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20372871606386667 | validation: 0.3149113164553694]
	TIME [epoch: 7.76 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2981493104582037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2981493104582037 | validation: 0.18723792175606918]
	TIME [epoch: 7.74 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15580400026405059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15580400026405059 | validation: 0.1763792067144234]
	TIME [epoch: 7.74 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13300857290949428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13300857290949428 | validation: 0.1659749229667672]
	TIME [epoch: 7.74 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.164953444503572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.164953444503572 | validation: 0.1608403789391697]
	TIME [epoch: 7.77 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14049961295696275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14049961295696275 | validation: 0.11289046741224423]
	TIME [epoch: 7.75 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12334857645537443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12334857645537443 | validation: 0.10284002041406565]
	TIME [epoch: 7.75 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09510357216009661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09510357216009661 | validation: 0.10639655130910462]
	TIME [epoch: 7.74 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09444080722509096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09444080722509096 | validation: 0.1825548570883453]
	TIME [epoch: 7.76 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1115232477806649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1115232477806649 | validation: 0.11673910702136228]
	TIME [epoch: 7.76 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09164317612486458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09164317612486458 | validation: 0.09772172970732007]
	TIME [epoch: 7.75 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07876679028277793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07876679028277793 | validation: 0.09586726153568909]
	TIME [epoch: 7.74 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07374776672106036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07374776672106036 | validation: 0.08888293519977034]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_491.pth
	Model improved!!!
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07510622930365372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07510622930365372 | validation: 0.08781932437833834]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_492.pth
	Model improved!!!
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07094569325215269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07094569325215269 | validation: 0.08809655371650917]
	TIME [epoch: 7.76 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06802769155612645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06802769155612645 | validation: 0.08292791640257603]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_494.pth
	Model improved!!!
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06664927272271977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06664927272271977 | validation: 0.07183360513525615]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_495.pth
	Model improved!!!
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06565894318619378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06565894318619378 | validation: 0.11429297585751184]
	TIME [epoch: 7.75 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08582837972149779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08582837972149779 | validation: 0.22152332056452195]
	TIME [epoch: 7.76 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20096804043821676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20096804043821676 | validation: 0.38180407399978455]
	TIME [epoch: 7.74 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2670986786450662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2670986786450662 | validation: 0.24896865846236121]
	TIME [epoch: 7.74 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14950725039048762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14950725039048762 | validation: 0.4785734328827245]
	TIME [epoch: 7.74 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33027193956308126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33027193956308126 | validation: 0.23615167377563748]
	TIME [epoch: 41.7 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23782936965661766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23782936965661766 | validation: 0.22041042273673117]
	TIME [epoch: 16.6 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22328519021358417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22328519021358417 | validation: 0.19953448276678426]
	TIME [epoch: 16.6 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15372875809334144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15372875809334144 | validation: 0.18008368210040704]
	TIME [epoch: 16.6 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14057655635202293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14057655635202293 | validation: 0.12761734797810773]
	TIME [epoch: 16.6 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1575709187471935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1575709187471935 | validation: 0.21636719697405563]
	TIME [epoch: 16.6 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19382259306636737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19382259306636737 | validation: 0.1585292433941597]
	TIME [epoch: 16.6 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1377070195846678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1377070195846678 | validation: 0.13575723704094647]
	TIME [epoch: 16.6 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14373839599624963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14373839599624963 | validation: 0.15925857622958975]
	TIME [epoch: 16.6 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10412786024217922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10412786024217922 | validation: 0.11931910662830938]
	TIME [epoch: 16.6 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08721277782629112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08721277782629112 | validation: 0.10411199768996429]
	TIME [epoch: 16.6 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08477580526458292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08477580526458292 | validation: 0.11802416053538563]
	TIME [epoch: 16.6 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09290782529596825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09290782529596825 | validation: 0.10342495895885911]
	TIME [epoch: 16.6 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08828340710547192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08828340710547192 | validation: 0.11258782586093373]
	TIME [epoch: 16.6 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08653827265045835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08653827265045835 | validation: 0.11019217362141734]
	TIME [epoch: 16.6 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09063694755963111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09063694755963111 | validation: 0.11173363469568605]
	TIME [epoch: 16.6 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10298925531957615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10298925531957615 | validation: 0.15588022517458588]
	TIME [epoch: 16.6 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10593906082285226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10593906082285226 | validation: 0.08811543210463842]
	TIME [epoch: 16.6 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07483535644987703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07483535644987703 | validation: 0.09321092734186065]
	TIME [epoch: 16.6 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07350301037801299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07350301037801299 | validation: 0.08549555840960399]
	TIME [epoch: 16.6 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08454407871511121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08454407871511121 | validation: 0.15909265043788579]
	TIME [epoch: 16.6 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10495679813998578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10495679813998578 | validation: 0.09245936445873698]
	TIME [epoch: 16.6 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0796784260829139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0796784260829139 | validation: 0.0934768141205885]
	TIME [epoch: 16.6 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09237772760930053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09237772760930053 | validation: 0.12284810406065212]
	TIME [epoch: 16.6 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11328873886002222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11328873886002222 | validation: 0.09185983430436728]
	TIME [epoch: 16.6 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09270681074956447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09270681074956447 | validation: 0.0879187071061951]
	TIME [epoch: 16.6 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07059470112115639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07059470112115639 | validation: 0.10815678087358221]
	TIME [epoch: 16.6 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08319268529361416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08319268529361416 | validation: 0.11697627629900197]
	TIME [epoch: 16.6 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10738423510847972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10738423510847972 | validation: 0.16873975713979208]
	TIME [epoch: 16.6 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12095282418334961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12095282418334961 | validation: 0.20801957215993233]
	TIME [epoch: 16.6 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17151660474518118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17151660474518118 | validation: 0.14086482046419815]
	TIME [epoch: 16.6 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14596123428848787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14596123428848787 | validation: 0.1358252743457695]
	TIME [epoch: 16.6 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08524327843874213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08524327843874213 | validation: 0.09876242699003496]
	TIME [epoch: 16.6 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07002136380225836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07002136380225836 | validation: 0.09515492823308215]
	TIME [epoch: 16.6 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08513526999470618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08513526999470618 | validation: 0.09272416488120888]
	TIME [epoch: 16.6 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07252337818739908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07252337818739908 | validation: 0.13096332001976466]
	TIME [epoch: 16.6 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11006316109541814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11006316109541814 | validation: 0.28142731653520575]
	TIME [epoch: 16.6 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2674284338246505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2674284338246505 | validation: 0.13726053763318632]
	TIME [epoch: 16.6 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1617949079397842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1617949079397842 | validation: 0.12478370872291339]
	TIME [epoch: 16.6 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14763328899728734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14763328899728734 | validation: 0.2504223895687107]
	TIME [epoch: 16.6 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3247136616589245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3247136616589245 | validation: 0.21166035036601138]
	TIME [epoch: 16.6 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4514247847711078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4514247847711078 | validation: 0.21546657833241645]
	TIME [epoch: 16.6 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44702829753105733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44702829753105733 | validation: 0.1968375226792425]
	TIME [epoch: 16.6 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41015478287704715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41015478287704715 | validation: 0.40447004778873624]
	TIME [epoch: 16.6 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40960343918359593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40960343918359593 | validation: 0.45082791050581933]
	TIME [epoch: 16.6 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31075925151675543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31075925151675543 | validation: 0.22239617629056588]
	TIME [epoch: 16.6 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16917411396566376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16917411396566376 | validation: 0.20745801541268577]
	TIME [epoch: 16.6 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20928104823653088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20928104823653088 | validation: 0.13451285913864328]
	TIME [epoch: 16.6 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11955828863788305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11955828863788305 | validation: 0.13947303946953582]
	TIME [epoch: 16.6 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11051250805159755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11051250805159755 | validation: 0.12073748729999156]
	TIME [epoch: 16.6 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09775449890319354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09775449890319354 | validation: 0.1101317099099553]
	TIME [epoch: 16.6 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09170923711861892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09170923711861892 | validation: 0.11233453496370725]
	TIME [epoch: 16.6 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08367706517982029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08367706517982029 | validation: 0.1262135778880852]
	TIME [epoch: 16.6 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07715060375306568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07715060375306568 | validation: 0.0976474348381126]
	TIME [epoch: 16.6 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07835043923973517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07835043923973517 | validation: 0.10219928065962983]
	TIME [epoch: 16.6 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07279301191443834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07279301191443834 | validation: 0.09352076975921418]
	TIME [epoch: 16.6 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07409209297616769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07409209297616769 | validation: 0.09183042498725263]
	TIME [epoch: 16.6 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08592936286232206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08592936286232206 | validation: 0.13931558102655392]
	TIME [epoch: 16.6 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10531202694596344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10531202694596344 | validation: 0.11981714385998468]
	TIME [epoch: 16.6 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11854006043027726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11854006043027726 | validation: 0.1336030529140195]
	TIME [epoch: 16.6 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10154903284985638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10154903284985638 | validation: 0.0913582008963643]
	TIME [epoch: 16.6 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.074859526885746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.074859526885746 | validation: 0.12553878368049007]
	TIME [epoch: 16.6 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1145600449405614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1145600449405614 | validation: 0.2634624306217866]
	TIME [epoch: 16.6 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16954389553396973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16954389553396973 | validation: 0.12954825626032265]
	TIME [epoch: 16.6 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09056342148904463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09056342148904463 | validation: 0.1046839843429292]
	TIME [epoch: 16.6 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10285620720990575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10285620720990575 | validation: 0.10488679471163408]
	TIME [epoch: 16.6 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07670912444195266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07670912444195266 | validation: 0.09565721550938437]
	TIME [epoch: 16.6 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06935816465704653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06935816465704653 | validation: 0.09666204627107394]
	TIME [epoch: 16.6 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08344998983253422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08344998983253422 | validation: 0.13543373540962952]
	TIME [epoch: 16.6 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12142976914703713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12142976914703713 | validation: 0.13659986152598594]
	TIME [epoch: 16.6 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11391363804850528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11391363804850528 | validation: 0.13184376311555515]
	TIME [epoch: 16.6 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13046224052319041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13046224052319041 | validation: 0.12073438953892279]
	TIME [epoch: 16.6 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11572546816575148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11572546816575148 | validation: 0.13313002122331735]
	TIME [epoch: 16.6 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10585629354040105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10585629354040105 | validation: 0.07708415746621189]
	TIME [epoch: 16.6 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07052560132543007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07052560132543007 | validation: 0.08461708022706632]
	TIME [epoch: 16.6 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07177866674625216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07177866674625216 | validation: 0.0865617814484188]
	TIME [epoch: 16.6 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0647811101124691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0647811101124691 | validation: 0.06759124513378441]
	TIME [epoch: 16.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_577.pth
	Model improved!!!
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07000101878289051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07000101878289051 | validation: 0.08986445558487993]
	TIME [epoch: 16.6 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0812596148201581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0812596148201581 | validation: 0.07490456751192642]
	TIME [epoch: 16.6 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06976164962585255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06976164962585255 | validation: 0.0844452045703636]
	TIME [epoch: 16.6 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0877653076349481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0877653076349481 | validation: 0.1610616244985833]
	TIME [epoch: 16.6 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14810470459823877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14810470459823877 | validation: 0.11481924229516385]
	TIME [epoch: 16.6 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.117753181713086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.117753181713086 | validation: 0.09445162236633899]
	TIME [epoch: 16.6 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09259724220283098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09259724220283098 | validation: 0.09387341507424707]
	TIME [epoch: 16.6 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06902624384440835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06902624384440835 | validation: 0.06473632424170288]
	TIME [epoch: 16.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_585.pth
	Model improved!!!
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06660447827542881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06660447827542881 | validation: 0.1002786083117686]
	TIME [epoch: 16.6 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06832365430493574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06832365430493574 | validation: 0.0689811535927215]
	TIME [epoch: 16.6 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09285885872690612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09285885872690612 | validation: 0.1805833726750855]
	TIME [epoch: 16.6 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12396075759212778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12396075759212778 | validation: 0.10381019984380546]
	TIME [epoch: 16.6 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08267185131857625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08267185131857625 | validation: 0.09891845331692205]
	TIME [epoch: 16.6 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09589206378044535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09589206378044535 | validation: 0.1459030026397363]
	TIME [epoch: 16.6 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11158356647821435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11158356647821435 | validation: 0.18802096081220726]
	TIME [epoch: 16.6 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21188035590726187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21188035590726187 | validation: 0.16290431367152475]
	TIME [epoch: 16.6 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12890433269974938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12890433269974938 | validation: 0.17727212258198766]
	TIME [epoch: 16.6 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16066889198690312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16066889198690312 | validation: 0.14979528145243673]
	TIME [epoch: 16.6 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1287391111490668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1287391111490668 | validation: 0.12705828909959993]
	TIME [epoch: 16.6 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12650538434709585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12650538434709585 | validation: 0.10119979337001782]
	TIME [epoch: 16.6 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1061086758476352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1061086758476352 | validation: 0.11491638431218124]
	TIME [epoch: 16.6 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11342982314487937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11342982314487937 | validation: 0.07985389984945912]
	TIME [epoch: 16.6 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08321405780530652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08321405780530652 | validation: 0.08757468530359787]
	TIME [epoch: 16.6 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06509239077803812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06509239077803812 | validation: 0.08435093975089702]
	TIME [epoch: 16.6 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06291909148870778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06291909148870778 | validation: 0.06632204650664157]
	TIME [epoch: 16.6 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06313734606991313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06313734606991313 | validation: 0.10771265122698839]
	TIME [epoch: 16.6 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0756296692931643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0756296692931643 | validation: 0.08042677161186085]
	TIME [epoch: 16.6 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0788142532184845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0788142532184845 | validation: 0.07802143419520335]
	TIME [epoch: 16.6 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08144737709141303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08144737709141303 | validation: 0.09734175751475305]
	TIME [epoch: 16.6 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07256615640094337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07256615640094337 | validation: 0.0789918613951841]
	TIME [epoch: 16.6 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08276728654587094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08276728654587094 | validation: 0.14337464163878236]
	TIME [epoch: 16.6 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09469004824441803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09469004824441803 | validation: 0.06498477803372381]
	TIME [epoch: 16.6 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060639648985426224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060639648985426224 | validation: 0.10330449523258244]
	TIME [epoch: 16.6 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10475359259860169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10475359259860169 | validation: 0.15454871645391208]
	TIME [epoch: 16.6 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13421660891554774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13421660891554774 | validation: 0.15009434121347234]
	TIME [epoch: 16.6 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14552446414266054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14552446414266054 | validation: 0.11547836452342337]
	TIME [epoch: 16.6 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12994771374140476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12994771374140476 | validation: 0.10361612368271081]
	TIME [epoch: 16.6 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11539300761029456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11539300761029456 | validation: 0.07290776963590183]
	TIME [epoch: 16.6 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06498744315835268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06498744315835268 | validation: 0.05204848390602408]
	TIME [epoch: 16.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_616.pth
	Model improved!!!
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09949360468001897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09949360468001897 | validation: 0.17759156227408307]
	TIME [epoch: 16.6 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2272114270456329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2272114270456329 | validation: 0.13857952425953776]
	TIME [epoch: 16.6 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22628574882928099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22628574882928099 | validation: 0.18137234180732775]
	TIME [epoch: 16.6 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3133924642259349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3133924642259349 | validation: 0.2219414911821065]
	TIME [epoch: 16.6 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24766103056972405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24766103056972405 | validation: 0.23215756031780943]
	TIME [epoch: 16.6 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20957414327636564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20957414327636564 | validation: 0.1561124391905068]
	TIME [epoch: 16.6 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14178704622651536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14178704622651536 | validation: 0.18126044717989886]
	TIME [epoch: 16.6 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16314145759996962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16314145759996962 | validation: 0.10553157648654549]
	TIME [epoch: 16.6 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1256588721063937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1256588721063937 | validation: 0.2047563118357376]
	TIME [epoch: 16.6 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14169932008018696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14169932008018696 | validation: 0.11570684906976757]
	TIME [epoch: 16.6 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07567503092021179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07567503092021179 | validation: 0.07426004844309612]
	TIME [epoch: 16.6 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0883189688250658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0883189688250658 | validation: 0.07639153337569116]
	TIME [epoch: 16.6 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06599939855538814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06599939855538814 | validation: 0.07670189145059014]
	TIME [epoch: 16.6 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05906337497814573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05906337497814573 | validation: 0.06524354098049595]
	TIME [epoch: 16.6 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06056139006824322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06056139006824322 | validation: 0.06322201473666872]
	TIME [epoch: 16.6 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056107146054849524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056107146054849524 | validation: 0.0642683933238528]
	TIME [epoch: 16.6 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04705277854622894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04705277854622894 | validation: 0.05085812020710915]
	TIME [epoch: 16.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_633.pth
	Model improved!!!
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05215779775124799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05215779775124799 | validation: 0.12843483331263647]
	TIME [epoch: 16.6 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08893472131439097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08893472131439097 | validation: 0.08744568921509961]
	TIME [epoch: 16.6 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09184763825071329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09184763825071329 | validation: 0.09200140397143167]
	TIME [epoch: 16.6 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10694421936904722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10694421936904722 | validation: 0.1309317846013221]
	TIME [epoch: 16.6 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10273813110080095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10273813110080095 | validation: 0.07852995834729337]
	TIME [epoch: 16.6 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06704420889470245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06704420889470245 | validation: 0.051125061103221875]
	TIME [epoch: 16.5 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06650437223759227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06650437223759227 | validation: 0.08484702725090831]
	TIME [epoch: 16.6 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05892789040703614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05892789040703614 | validation: 0.07561555790272079]
	TIME [epoch: 16.5 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06460887365546586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06460887365546586 | validation: 0.07258413395373207]
	TIME [epoch: 16.6 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08500834143335105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08500834143335105 | validation: 0.08145014982522637]
	TIME [epoch: 16.6 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07344960102674175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07344960102674175 | validation: 0.08218847699206222]
	TIME [epoch: 16.6 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06930936640182099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06930936640182099 | validation: 0.08419924725613365]
	TIME [epoch: 16.6 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0986674132158956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0986674132158956 | validation: 0.12087734436334317]
	TIME [epoch: 16.6 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09540164230638382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09540164230638382 | validation: 0.11180331603128915]
	TIME [epoch: 16.6 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10025909472173351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10025909472173351 | validation: 0.10078114717450155]
	TIME [epoch: 16.6 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09263986684149626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09263986684149626 | validation: 0.07082574664560747]
	TIME [epoch: 16.6 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061629285391117235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061629285391117235 | validation: 0.05247775008212349]
	TIME [epoch: 16.7 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06783264930620618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06783264930620618 | validation: 0.10937817886435695]
	TIME [epoch: 16.6 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07126001331929806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07126001331929806 | validation: 0.045153299106478965]
	TIME [epoch: 16.7 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_652.pth
	Model improved!!!
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05904077493884808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05904077493884808 | validation: 0.07437598953849997]
	TIME [epoch: 16.6 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05290171007428739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05290171007428739 | validation: 0.07241727232564879]
	TIME [epoch: 16.5 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05838997428643243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05838997428643243 | validation: 0.06374826415231036]
	TIME [epoch: 16.6 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07140790860447374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07140790860447374 | validation: 0.09802051600493733]
	TIME [epoch: 16.5 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09267869936156557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09267869936156557 | validation: 0.047424264469299406]
	TIME [epoch: 16.6 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07212409747958809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07212409747958809 | validation: 0.12167362101947243]
	TIME [epoch: 16.5 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1259768728614229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1259768728614229 | validation: 0.19458615448835614]
	TIME [epoch: 16.5 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1759535030797322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1759535030797322 | validation: 0.09971530441573268]
	TIME [epoch: 16.6 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08343952831362424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08343952831362424 | validation: 0.13846755698212032]
	TIME [epoch: 16.6 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1426088194410365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1426088194410365 | validation: 0.05057400409023896]
	TIME [epoch: 16.6 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07630361175991518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07630361175991518 | validation: 0.07544044107139827]
	TIME [epoch: 16.6 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0889937331022374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0889937331022374 | validation: 0.12239235167293638]
	TIME [epoch: 16.5 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09632225920958472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09632225920958472 | validation: 0.19376608305147508]
	TIME [epoch: 16.6 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17474306471007317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17474306471007317 | validation: 0.2025045406213848]
	TIME [epoch: 16.5 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14638370767910386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14638370767910386 | validation: 0.050006983426625046]
	TIME [epoch: 16.6 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07883825112907226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07883825112907226 | validation: 0.23963632742818702]
	TIME [epoch: 16.6 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15382105079588948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15382105079588948 | validation: 0.11188585538855644]
	TIME [epoch: 16.5 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1309278851144317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1309278851144317 | validation: 0.1612763405769604]
	TIME [epoch: 16.6 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1386306263759827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1386306263759827 | validation: 0.043345095149285466]
	TIME [epoch: 16.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_671.pth
	Model improved!!!
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07888982005991911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07888982005991911 | validation: 0.048964542263040915]
	TIME [epoch: 16.6 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05794578690658215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05794578690658215 | validation: 0.10652123551033582]
	TIME [epoch: 16.6 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06529060675252675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06529060675252675 | validation: 0.08423142056229695]
	TIME [epoch: 16.6 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08199370620493675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08199370620493675 | validation: 0.10895705890502161]
	TIME [epoch: 16.6 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06783257721887488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06783257721887488 | validation: 0.08555611481279655]
	TIME [epoch: 16.6 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06450072384805726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06450072384805726 | validation: 0.057468693688304366]
	TIME [epoch: 16.6 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05362495654888566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05362495654888566 | validation: 0.050691213059549635]
	TIME [epoch: 16.6 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043432234744092364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043432234744092364 | validation: 0.044176642401849125]
	TIME [epoch: 16.6 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043822150672846075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043822150672846075 | validation: 0.05634840036674769]
	TIME [epoch: 16.6 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047484524020351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047484524020351 | validation: 0.09057141251635388]
	TIME [epoch: 16.6 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09178561085673562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09178561085673562 | validation: 0.1500674617867182]
	TIME [epoch: 16.6 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14710375257560895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14710375257560895 | validation: 0.08399518003310324]
	TIME [epoch: 16.6 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1127875717797108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1127875717797108 | validation: 0.11323408653461575]
	TIME [epoch: 16.6 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0795496601620182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0795496601620182 | validation: 0.11542451453144596]
	TIME [epoch: 16.6 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08915272464422959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08915272464422959 | validation: 0.06418988029448923]
	TIME [epoch: 16.6 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.091223248154213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.091223248154213 | validation: 0.16748327333573]
	TIME [epoch: 16.6 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10553787697082441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10553787697082441 | validation: 0.12073030049509492]
	TIME [epoch: 16.6 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07192912752495921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07192912752495921 | validation: 0.08757773748777048]
	TIME [epoch: 16.6 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07933488187881342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07933488187881342 | validation: 0.08231064442733636]
	TIME [epoch: 16.6 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0795804885660788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0795804885660788 | validation: 0.10037086274711596]
	TIME [epoch: 16.6 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09586801248968506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09586801248968506 | validation: 0.06732332690831277]
	TIME [epoch: 16.6 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07932860615493895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07932860615493895 | validation: 0.060697107035317915]
	TIME [epoch: 16.6 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06484391910616202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06484391910616202 | validation: 0.08493712539792356]
	TIME [epoch: 16.6 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05554292473851494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05554292473851494 | validation: 0.040928686893355416]
	TIME [epoch: 16.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_695.pth
	Model improved!!!
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05139942871623338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05139942871623338 | validation: 0.07670744693188326]
	TIME [epoch: 16.7 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0543483731237819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0543483731237819 | validation: 0.044270460889185265]
	TIME [epoch: 16.6 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050151804766511474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050151804766511474 | validation: 0.06984479630274874]
	TIME [epoch: 16.6 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052482400726487306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052482400726487306 | validation: 0.05933700496296388]
	TIME [epoch: 16.6 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05148171236858648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05148171236858648 | validation: 0.05196232145489891]
	TIME [epoch: 16.5 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06427182498514875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06427182498514875 | validation: 0.07502562522130969]
	TIME [epoch: 16.6 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07600372499821682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07600372499821682 | validation: 0.06318407090513536]
	TIME [epoch: 16.7 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.088608949142323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.088608949142323 | validation: 0.11574156185131441]
	TIME [epoch: 16.6 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13406374737176327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13406374737176327 | validation: 0.18811122718807882]
	TIME [epoch: 16.6 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13928556436939662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13928556436939662 | validation: 0.23341722197088854]
	TIME [epoch: 16.6 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19102472799976533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19102472799976533 | validation: 1.84297809089341]
	TIME [epoch: 16.6 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.366369533271974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.366369533271974 | validation: 1.3249400705360026]
	TIME [epoch: 16.7 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7872032221101768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7872032221101768 | validation: 0.5866600948730574]
	TIME [epoch: 16.6 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5969552098638383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5969552098638383 | validation: 0.2445339323902619]
	TIME [epoch: 16.6 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23550634090850572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23550634090850572 | validation: 0.4536295013611016]
	TIME [epoch: 16.6 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4050891950923932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4050891950923932 | validation: 0.1422575642254703]
	TIME [epoch: 16.7 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14727991046029545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14727991046029545 | validation: 0.17600738651548647]
	TIME [epoch: 16.7 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13086710447829603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13086710447829603 | validation: 0.15191911122763757]
	TIME [epoch: 16.6 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10128276034539173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10128276034539173 | validation: 0.12105267769578179]
	TIME [epoch: 16.7 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0745949374089574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0745949374089574 | validation: 0.10302213260847048]
	TIME [epoch: 16.6 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07752019183757305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07752019183757305 | validation: 0.09722397330432841]
	TIME [epoch: 16.7 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06584907233514463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06584907233514463 | validation: 0.08634257029186491]
	TIME [epoch: 16.6 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06242973092315261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06242973092315261 | validation: 0.080286694270452]
	TIME [epoch: 16.7 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05749276101532099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05749276101532099 | validation: 0.07247821178529389]
	TIME [epoch: 16.6 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05540620523047789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05540620523047789 | validation: 0.06573070715954753]
	TIME [epoch: 16.6 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053913513969047755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053913513969047755 | validation: 0.07654910302622493]
	TIME [epoch: 16.5 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05197369648148758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05197369648148758 | validation: 0.061741669378892396]
	TIME [epoch: 16.6 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04954210114558094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04954210114558094 | validation: 0.05639793310532398]
	TIME [epoch: 16.6 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04919925578104284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04919925578104284 | validation: 0.05462401797419513]
	TIME [epoch: 16.6 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054773979290158786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054773979290158786 | validation: 0.09156712601406566]
	TIME [epoch: 16.6 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0879395160159065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0879395160159065 | validation: 0.09717409856599694]
	TIME [epoch: 16.6 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10937413403614471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10937413403614471 | validation: 0.0978126415742765]
	TIME [epoch: 16.6 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0719663788526838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0719663788526838 | validation: 0.059009461497383675]
	TIME [epoch: 16.6 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04653929567277833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04653929567277833 | validation: 0.04718983025221405]
	TIME [epoch: 16.6 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04682690897201809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04682690897201809 | validation: 0.05688482078227262]
	TIME [epoch: 16.6 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04448703749986435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04448703749986435 | validation: 0.04825056017714659]
	TIME [epoch: 16.6 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04316989627657522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04316989627657522 | validation: 0.03631504562602017]
	TIME [epoch: 16.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_732.pth
	Model improved!!!
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046777795625579544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046777795625579544 | validation: 0.05052111610635133]
	TIME [epoch: 16.6 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04713934646229425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04713934646229425 | validation: 0.061156727623956436]
	TIME [epoch: 16.7 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057192823750860815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057192823750860815 | validation: 0.0676843498176578]
	TIME [epoch: 16.6 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0702666907333975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0702666907333975 | validation: 0.06798581326290662]
	TIME [epoch: 16.7 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.073322311505254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.073322311505254 | validation: 0.10190857549863278]
	TIME [epoch: 16.6 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13324491919399395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13324491919399395 | validation: 0.08498719096829542]
	TIME [epoch: 16.7 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10183145837943268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10183145837943268 | validation: 0.08958297997802711]
	TIME [epoch: 16.6 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.080745871508033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.080745871508033 | validation: 0.06730804537593753]
	TIME [epoch: 16.6 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06910575694810994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06910575694810994 | validation: 0.04173713075989269]
	TIME [epoch: 16.6 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04433269532358345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04433269532358345 | validation: 0.054288395822520666]
	TIME [epoch: 16.7 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05217271772923489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05217271772923489 | validation: 0.11114720905010787]
	TIME [epoch: 16.6 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11445285227007018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11445285227007018 | validation: 0.0773592924300822]
	TIME [epoch: 16.6 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0787516965320323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0787516965320323 | validation: 0.04917219931119417]
	TIME [epoch: 16.7 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052746770896288896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052746770896288896 | validation: 0.14504172566143772]
	TIME [epoch: 16.6 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15645513395949037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15645513395949037 | validation: 0.23708303698449448]
	TIME [epoch: 16.7 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19996062484384422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19996062484384422 | validation: 0.1725749381872259]
	TIME [epoch: 16.6 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09219498655944439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09219498655944439 | validation: 0.07612252835214833]
	TIME [epoch: 16.6 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08695214669663166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08695214669663166 | validation: 0.07722288567904467]
	TIME [epoch: 16.6 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06018148161745334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06018148161745334 | validation: 0.15578410089646566]
	TIME [epoch: 16.6 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11227359802451993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11227359802451993 | validation: 0.041399407884975925]
	TIME [epoch: 16.6 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053269930065645774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053269930065645774 | validation: 0.06455898962984273]
	TIME [epoch: 16.6 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07657259578116796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07657259578116796 | validation: 0.10609519390341222]
	TIME [epoch: 16.6 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09904914002249388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09904914002249388 | validation: 0.06844014607145221]
	TIME [epoch: 16.6 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09241490979611965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09241490979611965 | validation: 0.06404220654089865]
	TIME [epoch: 16.6 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05391926570485953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05391926570485953 | validation: 0.03479809328524034]
	TIME [epoch: 16.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_757.pth
	Model improved!!!
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0384052541352198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0384052541352198 | validation: 0.0332439257249317]
	TIME [epoch: 16.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_758.pth
	Model improved!!!
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03809048003660712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03809048003660712 | validation: 0.047074720496721525]
	TIME [epoch: 16.6 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06284863500007994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06284863500007994 | validation: 0.135536019169693]
	TIME [epoch: 16.6 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12511639040109546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12511639040109546 | validation: 0.06824314233244305]
	TIME [epoch: 16.6 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07718339781272131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07718339781272131 | validation: 0.1177932862853596]
	TIME [epoch: 16.6 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09770962168921304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09770962168921304 | validation: 0.1440854414562219]
	TIME [epoch: 16.6 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13116853197580527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13116853197580527 | validation: 0.1476644956639515]
	TIME [epoch: 16.6 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16272381835996774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16272381835996774 | validation: 0.1910095969816001]
	TIME [epoch: 16.6 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11760586004345257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11760586004345257 | validation: 0.13571503754902825]
	TIME [epoch: 16.6 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10755609606005251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10755609606005251 | validation: 0.11250162174759802]
	TIME [epoch: 16.6 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10401200175505165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10401200175505165 | validation: 0.13080801334961203]
	TIME [epoch: 16.6 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08184316417377863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08184316417377863 | validation: 0.09295622610257355]
	TIME [epoch: 16.6 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06392131898780504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06392131898780504 | validation: 0.053447927092022285]
	TIME [epoch: 16.6 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07537636265446858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07537636265446858 | validation: 0.09115715555512544]
	TIME [epoch: 16.6 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07600016288238921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07600016288238921 | validation: 0.059486071869685755]
	TIME [epoch: 16.6 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06532340576435726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06532340576435726 | validation: 0.05019549191222094]
	TIME [epoch: 16.6 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05463522775627304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05463522775627304 | validation: 0.051788896306215484]
	TIME [epoch: 16.6 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060744501014290846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060744501014290846 | validation: 0.05880023903118601]
	TIME [epoch: 16.6 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044953760548139654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044953760548139654 | validation: 0.06374988974182226]
	TIME [epoch: 16.6 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053365454799733626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053365454799733626 | validation: 0.0409344213948779]
	TIME [epoch: 16.6 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04206873025615819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04206873025615819 | validation: 0.047427232841596514]
	TIME [epoch: 16.6 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05687302572228409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05687302572228409 | validation: 0.06445402226897612]
	TIME [epoch: 16.6 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06932440688462693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06932440688462693 | validation: 0.11709291772151982]
	TIME [epoch: 16.6 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10968399536699475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10968399536699475 | validation: 0.06615042131408551]
	TIME [epoch: 16.6 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07566836956781865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07566836956781865 | validation: 0.04510003421159902]
	TIME [epoch: 16.6 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04964532549404175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04964532549404175 | validation: 0.06660400407643513]
	TIME [epoch: 16.6 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056664076253469524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056664076253469524 | validation: 0.03694003107769198]
	TIME [epoch: 16.6 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042270174702369274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042270174702369274 | validation: 0.055434452321699815]
	TIME [epoch: 16.6 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051477037391888344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051477037391888344 | validation: 0.07836059592509152]
	TIME [epoch: 16.6 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08092313448879523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08092313448879523 | validation: 0.07143612977203641]
	TIME [epoch: 16.6 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07609707076888425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07609707076888425 | validation: 0.05856341212042168]
	TIME [epoch: 16.7 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07040559334681415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07040559334681415 | validation: 0.07046235517412884]
	TIME [epoch: 16.6 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06146175106484129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06146175106484129 | validation: 0.034048710379779704]
	TIME [epoch: 16.6 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04707572281432834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04707572281432834 | validation: 0.051965644560458504]
	TIME [epoch: 16.6 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04615559591434762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04615559591434762 | validation: 0.0871080777468637]
	TIME [epoch: 16.7 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07034575002302369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07034575002302369 | validation: 0.09050838765714503]
	TIME [epoch: 16.6 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10173756977619619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10173756977619619 | validation: 0.06273957398356772]
	TIME [epoch: 16.6 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05806304646800788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05806304646800788 | validation: 0.03665094848236642]
	TIME [epoch: 16.6 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0725579200646012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0725579200646012 | validation: 0.0340904924908707]
	TIME [epoch: 16.6 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04403665844795451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04403665844795451 | validation: 0.10334011954456684]
	TIME [epoch: 16.6 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07845403613823862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07845403613823862 | validation: 0.07871494395819734]
	TIME [epoch: 16.6 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08332246757450622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08332246757450622 | validation: 0.13926065732109053]
	TIME [epoch: 16.6 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14177285764697653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14177285764697653 | validation: 0.040115190330472776]
	TIME [epoch: 16.6 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05336202485543045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05336202485543045 | validation: 0.047849852363709755]
	TIME [epoch: 16.6 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06889411093192611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06889411093192611 | validation: 0.04763490914719404]
	TIME [epoch: 16.6 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048154726697841384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048154726697841384 | validation: 0.08901738432347435]
	TIME [epoch: 16.6 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08146572748489753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08146572748489753 | validation: 0.04051129477074727]
	TIME [epoch: 16.6 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05851339846398013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05851339846398013 | validation: 0.11232093698799864]
	TIME [epoch: 16.6 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07307062251404114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07307062251404114 | validation: 0.03457145962957921]
	TIME [epoch: 16.7 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0401608162729335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0401608162729335 | validation: 0.027258768796949363]
	TIME [epoch: 16.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_807.pth
	Model improved!!!
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032966132153274826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032966132153274826 | validation: 0.030507187339070575]
	TIME [epoch: 16.7 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03511198640817515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03511198640817515 | validation: 0.0393815895325097]
	TIME [epoch: 16.6 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05217085946373782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05217085946373782 | validation: 0.10337333149223893]
	TIME [epoch: 16.6 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10595144087858231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10595144087858231 | validation: 0.14735119304844357]
	TIME [epoch: 16.6 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13309705069009675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13309705069009675 | validation: 0.06964622322135537]
	TIME [epoch: 16.6 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0796625946181232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0796625946181232 | validation: 0.03778571796388495]
	TIME [epoch: 16.7 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043672654243111196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043672654243111196 | validation: 0.036228568298981516]
	TIME [epoch: 16.7 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03690485413270781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03690485413270781 | validation: 0.030720785379571337]
	TIME [epoch: 16.6 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048097788138176084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048097788138176084 | validation: 0.046918436133242494]
	TIME [epoch: 16.6 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048120245348252925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048120245348252925 | validation: 0.05849437548860551]
	TIME [epoch: 16.6 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07843090674534092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07843090674534092 | validation: 0.059004758712335716]
	TIME [epoch: 16.6 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06800839999544823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06800839999544823 | validation: 0.13117702134950648]
	TIME [epoch: 16.6 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1185579454850674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1185579454850674 | validation: 0.08425907122938318]
	TIME [epoch: 16.6 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08805168153429825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08805168153429825 | validation: 0.0396847073960649]
	TIME [epoch: 16.7 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044315798024132554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044315798024132554 | validation: 0.04091677374707782]
	TIME [epoch: 16.6 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05174458963861928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05174458963861928 | validation: 0.0827434101876758]
	TIME [epoch: 16.7 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07543168328769209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07543168328769209 | validation: 0.038667389239428224]
	TIME [epoch: 16.6 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04404213066421432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04404213066421432 | validation: 0.028154345221868805]
	TIME [epoch: 16.6 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037393701401195724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037393701401195724 | validation: 0.041260923044581614]
	TIME [epoch: 16.6 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05699522440147591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05699522440147591 | validation: 0.04063390827586802]
	TIME [epoch: 16.6 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04515329363148412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04515329363148412 | validation: 0.028894285812750976]
	TIME [epoch: 16.6 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04728282385539718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04728282385539718 | validation: 0.13903546322688595]
	TIME [epoch: 16.6 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1022556223968484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1022556223968484 | validation: 0.1355810888755558]
	TIME [epoch: 16.7 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14567018782893323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14567018782893323 | validation: 0.09179688891105103]
	TIME [epoch: 16.6 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09781962051470115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09781962051470115 | validation: 0.06705653188447402]
	TIME [epoch: 16.7 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05015888220036624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05015888220036624 | validation: 0.07314476781344424]
	TIME [epoch: 16.6 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09871387378855648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09871387378855648 | validation: 0.4765301494581687]
	TIME [epoch: 16.6 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.296024000073242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.296024000073242 | validation: 0.25060041182484677]
	TIME [epoch: 16.7 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17671611459395523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17671611459395523 | validation: 0.051207648197927315]
	TIME [epoch: 16.7 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06661312451466211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06661312451466211 | validation: 0.060036711522371555]
	TIME [epoch: 16.6 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12279545238996802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12279545238996802 | validation: 0.07774978268642291]
	TIME [epoch: 16.6 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10987275289181994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10987275289181994 | validation: 0.0921822269010675]
	TIME [epoch: 16.6 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07252126221854187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07252126221854187 | validation: 0.048889327897893776]
	TIME [epoch: 16.6 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07615561705613713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07615561705613713 | validation: 0.09025111778165167]
	TIME [epoch: 16.6 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05851332122640312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05851332122640312 | validation: 0.04913274999509396]
	TIME [epoch: 16.6 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047108343218561666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047108343218561666 | validation: 0.044179502860337796]
	TIME [epoch: 16.6 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05141691774904656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05141691774904656 | validation: 0.07481027553964001]
	TIME [epoch: 16.6 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07907891133258012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07907891133258012 | validation: 0.0941435044371938]
	TIME [epoch: 16.6 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09572329441013246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09572329441013246 | validation: 0.06404357835754926]
	TIME [epoch: 16.6 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0749206331172867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0749206331172867 | validation: 0.03848591761318065]
	TIME [epoch: 16.6 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04352756185513225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04352756185513225 | validation: 0.036248329799591306]
	TIME [epoch: 16.6 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03602277108307541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03602277108307541 | validation: 0.030057305338139873]
	TIME [epoch: 16.6 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04888906837276715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04888906837276715 | validation: 0.07044648724826706]
	TIME [epoch: 16.6 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05207844840230851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05207844840230851 | validation: 0.041630950894607915]
	TIME [epoch: 16.6 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048039097920329045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048039097920329045 | validation: 0.05886561809575483]
	TIME [epoch: 16.6 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06849665217966458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06849665217966458 | validation: 0.16645329026730016]
	TIME [epoch: 16.6 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15166930867051712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15166930867051712 | validation: 0.05977795674108177]
	TIME [epoch: 16.6 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06236361789576167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06236361789576167 | validation: 0.07707619707935066]
	TIME [epoch: 16.6 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08436010475746368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08436010475746368 | validation: 0.05955492022527305]
	TIME [epoch: 16.6 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04847909595796426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04847909595796426 | validation: 0.03173737881626667]
	TIME [epoch: 16.6 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05113404750133314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05113404750133314 | validation: 0.10274481376086403]
	TIME [epoch: 16.6 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06103660216749864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06103660216749864 | validation: 0.060843399049693096]
	TIME [epoch: 16.6 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060806797537042066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060806797537042066 | validation: 0.08335835547918317]
	TIME [epoch: 16.6 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08236952430189622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08236952430189622 | validation: 0.06815613132560411]
	TIME [epoch: 16.6 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08471781806240401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08471781806240401 | validation: 0.06857013470813134]
	TIME [epoch: 16.6 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06041456596953834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06041456596953834 | validation: 0.04483833809754052]
	TIME [epoch: 16.6 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03652666349209079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03652666349209079 | validation: 0.03329256723026493]
	TIME [epoch: 16.6 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040504082511088245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040504082511088245 | validation: 0.03383017204255858]
	TIME [epoch: 16.6 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04033853341455419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04033853341455419 | validation: 0.042572648399712476]
	TIME [epoch: 16.6 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037429575722903785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037429575722903785 | validation: 0.04392662334879691]
	TIME [epoch: 16.6 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06026053999118272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06026053999118272 | validation: 0.10439927726362377]
	TIME [epoch: 16.6 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08277165131073282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08277165131073282 | validation: 0.06403787906845218]
	TIME [epoch: 16.6 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06861478066477665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06861478066477665 | validation: 0.08029171056764309]
	TIME [epoch: 16.6 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08800105049488263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08800105049488263 | validation: 0.09404765976888485]
	TIME [epoch: 16.6 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11117592175630496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11117592175630496 | validation: 0.07463217576387399]
	TIME [epoch: 16.6 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0550560025373605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0550560025373605 | validation: 0.03584281214050196]
	TIME [epoch: 16.6 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03274903365852734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03274903365852734 | validation: 0.025848533173405488]
	TIME [epoch: 16.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_874.pth
	Model improved!!!
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031207179738956172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031207179738956172 | validation: 0.025276604077468835]
	TIME [epoch: 16.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_875.pth
	Model improved!!!
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04148569324258229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04148569324258229 | validation: 0.03981430054429219]
	TIME [epoch: 16.6 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042791490496513374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042791490496513374 | validation: 0.07707514896981858]
	TIME [epoch: 16.6 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0783718519870714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0783718519870714 | validation: 0.09987023445802629]
	TIME [epoch: 16.6 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10969783484186593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10969783484186593 | validation: 0.11992346788273617]
	TIME [epoch: 16.6 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1398180868517553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1398180868517553 | validation: 0.15579809361200617]
	TIME [epoch: 16.6 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13205441990015654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13205441990015654 | validation: 0.062107381850314086]
	TIME [epoch: 16.6 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08888339874665292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08888339874665292 | validation: 0.20740236334981646]
	TIME [epoch: 16.6 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16717319170448902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16717319170448902 | validation: 0.1514155639632595]
	TIME [epoch: 16.6 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10410585660150957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10410585660150957 | validation: 0.030272769735836026]
	TIME [epoch: 16.8 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049811088450517425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049811088450517425 | validation: 0.04013224685298439]
	TIME [epoch: 16.6 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039416366265581544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039416366265581544 | validation: 0.048150164015997615]
	TIME [epoch: 16.6 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07113035490168107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07113035490168107 | validation: 0.07424837879128755]
	TIME [epoch: 16.6 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055181438736629555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055181438736629555 | validation: 0.04984467625314437]
	TIME [epoch: 16.6 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050050570466470996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050050570466470996 | validation: 0.04777465786959391]
	TIME [epoch: 16.6 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04566671490565389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04566671490565389 | validation: 0.04100523632280084]
	TIME [epoch: 16.6 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03460721941504773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03460721941504773 | validation: 0.1127061455290958]
	TIME [epoch: 16.6 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06782369230485773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06782369230485773 | validation: 0.03592571047211797]
	TIME [epoch: 16.6 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0686283541211795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0686283541211795 | validation: 0.033799224193773826]
	TIME [epoch: 16.6 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047706470444611834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047706470444611834 | validation: 0.11511896010071672]
	TIME [epoch: 16.6 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09578490094661303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09578490094661303 | validation: 0.13392558298524385]
	TIME [epoch: 16.6 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13745726029426725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13745726029426725 | validation: 0.09843689117190707]
	TIME [epoch: 16.6 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1077278190505404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1077278190505404 | validation: 0.05064755505639029]
	TIME [epoch: 16.7 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05079448927471082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05079448927471082 | validation: 0.021698558994584685]
	TIME [epoch: 16.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_898.pth
	Model improved!!!
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04348340613840723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04348340613840723 | validation: 0.020637537696944264]
	TIME [epoch: 16.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_899.pth
	Model improved!!!
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0407454734317912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0407454734317912 | validation: 0.03996703496804808]
	TIME [epoch: 16.6 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038912321184137115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038912321184137115 | validation: 0.04277556109853882]
	TIME [epoch: 16.6 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049220702322184416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049220702322184416 | validation: 0.07111524784206907]
	TIME [epoch: 16.6 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08568844819598047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08568844819598047 | validation: 0.0878271508294282]
	TIME [epoch: 16.6 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09126994250333544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09126994250333544 | validation: 0.07602829094281117]
	TIME [epoch: 16.6 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08127438046868216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08127438046868216 | validation: 0.0349389013496077]
	TIME [epoch: 16.6 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0666065971721068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0666065971721068 | validation: 0.04602966037850642]
	TIME [epoch: 16.6 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05487435278005837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05487435278005837 | validation: 0.027663219647498218]
	TIME [epoch: 16.6 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038293768930171196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038293768930171196 | validation: 0.030049579886916367]
	TIME [epoch: 16.6 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030788081243419266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030788081243419266 | validation: 0.028610824334639507]
	TIME [epoch: 16.6 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03478743944826873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03478743944826873 | validation: 0.053976897257001645]
	TIME [epoch: 16.6 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05025160196788822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05025160196788822 | validation: 0.05928563257700749]
	TIME [epoch: 16.6 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08866479852998194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08866479852998194 | validation: 0.07241822394293854]
	TIME [epoch: 16.6 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07500459043108719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07500459043108719 | validation: 0.09930775822945047]
	TIME [epoch: 16.6 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08331748433932823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08331748433932823 | validation: 0.03443028498478258]
	TIME [epoch: 16.6 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04934510736486569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04934510736486569 | validation: 0.029894015991342327]
	TIME [epoch: 16.6 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034617958115511244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034617958115511244 | validation: 0.023565638834123128]
	TIME [epoch: 16.6 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035583838394436774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035583838394436774 | validation: 0.06935217379760238]
	TIME [epoch: 16.6 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09557235207128031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09557235207128031 | validation: 0.10084980541401892]
	TIME [epoch: 16.6 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0474197694106167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0474197694106167 | validation: 0.058914579493900034]
	TIME [epoch: 16.6 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05369716596664896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05369716596664896 | validation: 0.12593668091371277]
	TIME [epoch: 16.6 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14279490740628756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14279490740628756 | validation: 0.40295121537362616]
	TIME [epoch: 16.6 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2808527689453157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2808527689453157 | validation: 0.21918175147449373]
	TIME [epoch: 16.6 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23331939323346715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23331939323346715 | validation: 0.2186027701917558]
	TIME [epoch: 16.6 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18019801583527784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18019801583527784 | validation: 0.16230008733715065]
	TIME [epoch: 16.6 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11300492587479305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11300492587479305 | validation: 0.10223014667002768]
	TIME [epoch: 16.6 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09035943493029425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09035943493029425 | validation: 0.10642228697575473]
	TIME [epoch: 16.6 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10058215507778587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10058215507778587 | validation: 0.2044857669653097]
	TIME [epoch: 16.6 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13334915390515295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13334915390515295 | validation: 0.09237654990465496]
	TIME [epoch: 16.6 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06998275200435301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06998275200435301 | validation: 0.09368287198750658]
	TIME [epoch: 16.6 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08571885583551497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08571885583551497 | validation: 0.18578751288817405]
	TIME [epoch: 16.6 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09702593949175853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09702593949175853 | validation: 0.12280877457056723]
	TIME [epoch: 16.6 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06563994202165965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06563994202165965 | validation: 0.061604100279871836]
	TIME [epoch: 16.6 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052624424678133844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052624424678133844 | validation: 0.08041676090394106]
	TIME [epoch: 16.6 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0574373493266188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0574373493266188 | validation: 0.05571931231968962]
	TIME [epoch: 16.6 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05781108185704337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05781108185704337 | validation: 0.1318454740054082]
	TIME [epoch: 16.6 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058685868006163135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058685868006163135 | validation: 0.0670825716774917]
	TIME [epoch: 16.6 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04988446245907374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04988446245907374 | validation: 0.07266509257841934]
	TIME [epoch: 16.6 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06502120752504162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06502120752504162 | validation: 0.08339805935504783]
	TIME [epoch: 16.6 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09542419536184853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09542419536184853 | validation: 0.1028476306013328]
	TIME [epoch: 16.6 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07185583694737473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07185583694737473 | validation: 0.05056688393554104]
	TIME [epoch: 16.6 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055246543095576806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055246543095576806 | validation: 0.03947342440247226]
	TIME [epoch: 16.6 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04188528621247142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04188528621247142 | validation: 0.040798351749784946]
	TIME [epoch: 16.6 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042915915924961724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042915915924961724 | validation: 0.058812869006580504]
	TIME [epoch: 16.6 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04999386104887857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04999386104887857 | validation: 0.05484642926202154]
	TIME [epoch: 16.6 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05540352747515322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05540352747515322 | validation: 0.06357815084863377]
	TIME [epoch: 16.6 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061727790009564694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061727790009564694 | validation: 0.06222164404120304]
	TIME [epoch: 16.6 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07159017728017901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07159017728017901 | validation: 0.13336681984081822]
	TIME [epoch: 16.6 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08437090898906359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08437090898906359 | validation: 0.07967460738569099]
	TIME [epoch: 16.6 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04403372172450443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04403372172450443 | validation: 0.058467253560155676]
	TIME [epoch: 16.6 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060393210053933964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060393210053933964 | validation: 0.08998470367826339]
	TIME [epoch: 16.6 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04522988238989111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04522988238989111 | validation: 0.06527358025637105]
	TIME [epoch: 16.6 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03907259688740871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03907259688740871 | validation: 0.033698413504295775]
	TIME [epoch: 16.6 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03483319003404754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03483319003404754 | validation: 0.034228559053421974]
	TIME [epoch: 16.6 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043382320002135516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043382320002135516 | validation: 0.05041540453532359]
	TIME [epoch: 16.6 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05106590052926054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05106590052926054 | validation: 0.06622563525582102]
	TIME [epoch: 16.6 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06878643075283228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06878643075283228 | validation: 0.08085184898993988]
	TIME [epoch: 16.6 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08926097773626118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08926097773626118 | validation: 0.10037012883065251]
	TIME [epoch: 16.6 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1020181807214666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1020181807214666 | validation: 0.06344459199049256]
	TIME [epoch: 16.6 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07064360120392955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07064360120392955 | validation: 0.07123191291171797]
	TIME [epoch: 16.6 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05235663002111909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05235663002111909 | validation: 0.027248099887598166]
	TIME [epoch: 16.6 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03653522202797357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03653522202797357 | validation: 0.025108054319702613]
	TIME [epoch: 16.6 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04844203784236473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04844203784236473 | validation: 0.035011737392304634]
	TIME [epoch: 16.6 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03485803603637521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03485803603637521 | validation: 0.02385317398805602]
	TIME [epoch: 16.6 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027190697796398218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027190697796398218 | validation: 0.033657754715436854]
	TIME [epoch: 16.6 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0455882083581662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0455882083581662 | validation: 0.11334870863945996]
	TIME [epoch: 16.6 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08445203683918213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08445203683918213 | validation: 0.09016153450946827]
	TIME [epoch: 16.6 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07899638123208134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07899638123208134 | validation: 0.06611764835622155]
	TIME [epoch: 16.6 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07726670271985922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07726670271985922 | validation: 0.04062632545422873]
	TIME [epoch: 16.6 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05848931203074316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05848931203074316 | validation: 0.09132502511839297]
	TIME [epoch: 16.6 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05789057585090363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05789057585090363 | validation: 0.029336694725385706]
	TIME [epoch: 16.6 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03419136860660925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03419136860660925 | validation: 0.04176992234345659]
	TIME [epoch: 16.6 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05393575384714909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05393575384714909 | validation: 0.11396542886511654]
	TIME [epoch: 16.6 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1375401032448782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1375401032448782 | validation: 0.1396909134997823]
	TIME [epoch: 16.6 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09108412682961799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09108412682961799 | validation: 0.7147835102969501]
	TIME [epoch: 16.6 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1666996415639215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1666996415639215 | validation: 0.5751558085470254]
	TIME [epoch: 16.6 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8504196407177358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8504196407177358 | validation: 0.4680635497072779]
	TIME [epoch: 16.6 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3044488240229577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3044488240229577 | validation: 0.27018290911211873]
	TIME [epoch: 16.6 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2631879773917563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2631879773917563 | validation: 0.10727568615825378]
	TIME [epoch: 16.6 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09176612186485604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09176612186485604 | validation: 0.0980603387313847]
	TIME [epoch: 16.6 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08190147179994046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08190147179994046 | validation: 0.0710253140932986]
	TIME [epoch: 16.6 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09133359858662464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09133359858662464 | validation: 0.11602699854206291]
	TIME [epoch: 16.6 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08200984005169405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08200984005169405 | validation: 0.05851602541227812]
	TIME [epoch: 16.6 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06997188444977367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06997188444977367 | validation: 0.05995987917519279]
	TIME [epoch: 16.6 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05581024418804288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05581024418804288 | validation: 0.08361034925034544]
	TIME [epoch: 16.6 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04581025242878439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04581025242878439 | validation: 0.06754686161002076]
	TIME [epoch: 16.6 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040129084072730865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040129084072730865 | validation: 0.04508163670004339]
	TIME [epoch: 16.6 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03795379306961003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03795379306961003 | validation: 0.03712211697421743]
	TIME [epoch: 16.6 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03683923386001811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03683923386001811 | validation: 0.03634292438154995]
	TIME [epoch: 16.6 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03241873902507057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03241873902507057 | validation: 0.051143755423388004]
	TIME [epoch: 16.6 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04184106532506347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04184106532506347 | validation: 0.02985914002307638]
	TIME [epoch: 16.6 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04447134307543276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04447134307543276 | validation: 0.1535637757422144]
	TIME [epoch: 16.6 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10284784894446031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10284784894446031 | validation: 0.1592485154103573]
	TIME [epoch: 16.6 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1099389620063358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1099389620063358 | validation: 0.15365969688341857]
	TIME [epoch: 16.6 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1708835668016039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1708835668016039 | validation: 0.06663154334421244]
	TIME [epoch: 16.6 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08781760111273844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08781760111273844 | validation: 0.1268272810688909]
	TIME [epoch: 16.6 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06594787806064456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06594787806064456 | validation: 0.09617496098265263]
	TIME [epoch: 16.6 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048433974932840795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048433974932840795 | validation: 0.056570504462538154]
	TIME [epoch: 16.6 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04281535048424608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04281535048424608 | validation: 0.059398904729169935]
	TIME [epoch: 16.6 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055632736273236304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055632736273236304 | validation: 0.05670764328070799]
	TIME [epoch: 16.6 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05050906060366512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05050906060366512 | validation: 0.05110730430264793]
	TIME [epoch: 16.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240823_172912/states/model_phi1_3c_v_mmd1_1000.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 11529.110 seconds.
