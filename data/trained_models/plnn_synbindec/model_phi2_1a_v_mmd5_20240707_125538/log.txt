Args:
Namespace(name='model_phi2_1a_v_mmd5', outdir='out/model_training/model_phi2_1a_v_mmd5', training_data='data/training_data/data_phi2_1a/training', validation_data='data/training_data/data_phi2_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=250, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.1, weight_decay=0.9, clip=1.0, lr_schedule='warmup_cosine_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2186376985

Training model...

Saving initial model state to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5711382658620923		[learning rate: 0.01015]
	Learning Rate: 0.01015
	LOSS [training: 3.5711382658620923 | validation: 3.960780787105654]
	TIME [epoch: 104 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2347080849997534		[learning rate: 0.01035]
	Learning Rate: 0.01035
	LOSS [training: 3.2347080849997534 | validation: 2.828604294534185]
	TIME [epoch: 9.82 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.036694337455905		[learning rate: 0.01055]
	Learning Rate: 0.01055
	LOSS [training: 3.036694337455905 | validation: 3.3281845244280843]
	TIME [epoch: 9.68 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5762064755173277		[learning rate: 0.01075]
	Learning Rate: 0.01075
	LOSS [training: 2.5762064755173277 | validation: 2.103053676373377]
	TIME [epoch: 9.69 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1087783750372493		[learning rate: 0.01095]
	Learning Rate: 0.01095
	LOSS [training: 2.1087783750372493 | validation: 2.6548057135949277]
	TIME [epoch: 9.69 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.954272089895232		[learning rate: 0.01115]
	Learning Rate: 0.01115
	LOSS [training: 1.954272089895232 | validation: 2.967356531359134]
	TIME [epoch: 9.71 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9400706898468516		[learning rate: 0.01135]
	Learning Rate: 0.01135
	LOSS [training: 1.9400706898468516 | validation: 3.2150341986312903]
	TIME [epoch: 9.71 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1190565115310793		[learning rate: 0.01155]
	Learning Rate: 0.01155
	LOSS [training: 2.1190565115310793 | validation: 2.0960945715142607]
	TIME [epoch: 9.68 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4359686577331108		[learning rate: 0.01175]
	Learning Rate: 0.01175
	LOSS [training: 1.4359686577331108 | validation: 1.02783790219997]
	TIME [epoch: 9.67 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.539556770547001		[learning rate: 0.01195]
	Learning Rate: 0.01195
	LOSS [training: 1.539556770547001 | validation: 2.035943059964541]
	TIME [epoch: 9.66 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5438151418793995		[learning rate: 0.01215]
	Learning Rate: 0.01215
	LOSS [training: 1.5438151418793995 | validation: 1.6805443414739343]
	TIME [epoch: 9.71 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5651995568304526		[learning rate: 0.01235]
	Learning Rate: 0.01235
	LOSS [training: 1.5651995568304526 | validation: 1.3183523222889681]
	TIME [epoch: 9.68 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1369152404203264		[learning rate: 0.01255]
	Learning Rate: 0.01255
	LOSS [training: 1.1369152404203264 | validation: 1.2991081369894597]
	TIME [epoch: 9.66 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1780175346714343		[learning rate: 0.01275]
	Learning Rate: 0.01275
	LOSS [training: 1.1780175346714343 | validation: 1.0979524646067529]
	TIME [epoch: 9.66 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0367506994538582		[learning rate: 0.01295]
	Learning Rate: 0.01295
	LOSS [training: 1.0367506994538582 | validation: 0.7067392866807208]
	TIME [epoch: 9.66 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0235804203351213		[learning rate: 0.01315]
	Learning Rate: 0.01315
	LOSS [training: 1.0235804203351213 | validation: 1.1794699456666073]
	TIME [epoch: 9.77 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0175780174072049		[learning rate: 0.01335]
	Learning Rate: 0.01335
	LOSS [training: 1.0175780174072049 | validation: 0.972086410363575]
	TIME [epoch: 9.68 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0059674902596032		[learning rate: 0.01355]
	Learning Rate: 0.01355
	LOSS [training: 1.0059674902596032 | validation: 0.9951296564147912]
	TIME [epoch: 9.68 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8263767660016479		[learning rate: 0.01375]
	Learning Rate: 0.01375
	LOSS [training: 0.8263767660016479 | validation: 0.6733352514555759]
	TIME [epoch: 9.67 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7301005378026237		[learning rate: 0.01395]
	Learning Rate: 0.01395
	LOSS [training: 0.7301005378026237 | validation: 0.596634453043997]
	TIME [epoch: 9.71 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7058948972620365		[learning rate: 0.01415]
	Learning Rate: 0.01415
	LOSS [training: 0.7058948972620365 | validation: 1.0663088470649034]
	TIME [epoch: 9.71 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8732739762476041		[learning rate: 0.01435]
	Learning Rate: 0.01435
	LOSS [training: 0.8732739762476041 | validation: 0.42733547076883793]
	TIME [epoch: 9.69 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6629667059671444		[learning rate: 0.01455]
	Learning Rate: 0.01455
	LOSS [training: 0.6629667059671444 | validation: 0.5973771607869647]
	TIME [epoch: 9.69 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.633561514304873		[learning rate: 0.01475]
	Learning Rate: 0.01475
	LOSS [training: 0.633561514304873 | validation: 0.6278730008721365]
	TIME [epoch: 9.68 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6216841672738384		[learning rate: 0.01495]
	Learning Rate: 0.01495
	LOSS [training: 0.6216841672738384 | validation: 0.4493910685303206]
	TIME [epoch: 9.72 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5749122816555674		[learning rate: 0.01515]
	Learning Rate: 0.01515
	LOSS [training: 0.5749122816555674 | validation: 0.47070342957753447]
	TIME [epoch: 9.69 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6264395988534031		[learning rate: 0.01535]
	Learning Rate: 0.01535
	LOSS [training: 0.6264395988534031 | validation: 0.745628668738062]
	TIME [epoch: 9.67 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6531108726905549		[learning rate: 0.01555]
	Learning Rate: 0.01555
	LOSS [training: 0.6531108726905549 | validation: 0.34208124545606133]
	TIME [epoch: 9.68 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5933129958242519		[learning rate: 0.01575]
	Learning Rate: 0.01575
	LOSS [training: 0.5933129958242519 | validation: 0.37277727113989667]
	TIME [epoch: 9.7 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5987304170892709		[learning rate: 0.01595]
	Learning Rate: 0.01595
	LOSS [training: 0.5987304170892709 | validation: 0.310331105934849]
	TIME [epoch: 9.74 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5156722779512306		[learning rate: 0.01615]
	Learning Rate: 0.01615
	LOSS [training: 0.5156722779512306 | validation: 0.6256311951993042]
	TIME [epoch: 9.7 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6106000007469263		[learning rate: 0.01635]
	Learning Rate: 0.01635
	LOSS [training: 0.6106000007469263 | validation: 0.3254915950572078]
	TIME [epoch: 9.69 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5025822927161884		[learning rate: 0.01655]
	Learning Rate: 0.01655
	LOSS [training: 0.5025822927161884 | validation: 0.5152923557910711]
	TIME [epoch: 9.68 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6086929686978191		[learning rate: 0.01675]
	Learning Rate: 0.01675
	LOSS [training: 0.6086929686978191 | validation: 0.42694754591483997]
	TIME [epoch: 9.69 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7200818116746018		[learning rate: 0.01695]
	Learning Rate: 0.01695
	LOSS [training: 0.7200818116746018 | validation: 0.3817766420324581]
	TIME [epoch: 9.74 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5606481519693804		[learning rate: 0.01715]
	Learning Rate: 0.01715
	LOSS [training: 0.5606481519693804 | validation: 0.359779426007655]
	TIME [epoch: 9.66 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5861920723866236		[learning rate: 0.01735]
	Learning Rate: 0.01735
	LOSS [training: 0.5861920723866236 | validation: 0.5019093725461515]
	TIME [epoch: 9.68 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5156758468727751		[learning rate: 0.01755]
	Learning Rate: 0.01755
	LOSS [training: 0.5156758468727751 | validation: 0.3611272189916337]
	TIME [epoch: 9.66 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5461599553914248		[learning rate: 0.01775]
	Learning Rate: 0.01775
	LOSS [training: 0.5461599553914248 | validation: 0.37890273070890573]
	TIME [epoch: 9.71 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4892830126482743		[learning rate: 0.01795]
	Learning Rate: 0.01795
	LOSS [training: 0.4892830126482743 | validation: 0.43973545900653144]
	TIME [epoch: 9.7 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5570070213325211		[learning rate: 0.01815]
	Learning Rate: 0.01815
	LOSS [training: 0.5570070213325211 | validation: 0.5739679158562864]
	TIME [epoch: 9.68 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4713909108994668		[learning rate: 0.01835]
	Learning Rate: 0.01835
	LOSS [training: 0.4713909108994668 | validation: 0.5283908114407052]
	TIME [epoch: 9.67 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4392239112178644		[learning rate: 0.01855]
	Learning Rate: 0.01855
	LOSS [training: 0.4392239112178644 | validation: 0.4511651754599384]
	TIME [epoch: 9.68 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46384755233393854		[learning rate: 0.01875]
	Learning Rate: 0.01875
	LOSS [training: 0.46384755233393854 | validation: 0.540793059434752]
	TIME [epoch: 9.72 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5957987580759319		[learning rate: 0.01895]
	Learning Rate: 0.01895
	LOSS [training: 0.5957987580759319 | validation: 0.3995147542942026]
	TIME [epoch: 9.69 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.559096421935501		[learning rate: 0.01915]
	Learning Rate: 0.01915
	LOSS [training: 0.559096421935501 | validation: 0.3746019306246987]
	TIME [epoch: 9.68 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4681344922068785		[learning rate: 0.01935]
	Learning Rate: 0.01935
	LOSS [training: 0.4681344922068785 | validation: 0.39213193799829427]
	TIME [epoch: 9.68 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45755699889000606		[learning rate: 0.01955]
	Learning Rate: 0.01955
	LOSS [training: 0.45755699889000606 | validation: 0.5186142387735642]
	TIME [epoch: 9.68 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4664105715123954		[learning rate: 0.01975]
	Learning Rate: 0.01975
	LOSS [training: 0.4664105715123954 | validation: 0.7020193828447109]
	TIME [epoch: 9.73 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.532743627862308		[learning rate: 0.01995]
	Learning Rate: 0.01995
	LOSS [training: 0.532743627862308 | validation: 0.6239874636243552]
	TIME [epoch: 9.69 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.53866360824015		[learning rate: 0.02]
	Learning Rate: 0.02
	LOSS [training: 0.53866360824015 | validation: 0.465174366273938]
	TIME [epoch: 9.68 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44235739413019554		[learning rate: 0.02]
	Learning Rate: 0.02
	LOSS [training: 0.44235739413019554 | validation: 0.4728762069047079]
	TIME [epoch: 9.67 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4565081414478114		[learning rate: 0.02]
	Learning Rate: 0.0199999
	LOSS [training: 0.4565081414478114 | validation: 0.47458104198017564]
	TIME [epoch: 9.68 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4478895932711015		[learning rate: 0.02]
	Learning Rate: 0.0199998
	LOSS [training: 0.4478895932711015 | validation: 0.4223265430457114]
	TIME [epoch: 9.72 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45221947708057275		[learning rate: 0.02]
	Learning Rate: 0.0199997
	LOSS [training: 0.45221947708057275 | validation: 0.3741036059650287]
	TIME [epoch: 9.69 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.437149054182924		[learning rate: 0.02]
	Learning Rate: 0.0199995
	LOSS [training: 0.437149054182924 | validation: 0.2804232800099862]
	TIME [epoch: 9.67 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3653165511543423		[learning rate: 0.019999]
	Learning Rate: 0.0199994
	LOSS [training: 0.3653165511543423 | validation: 0.30871508222134675]
	TIME [epoch: 9.68 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3823367903307293		[learning rate: 0.019999]
	Learning Rate: 0.0199992
	LOSS [training: 0.3823367903307293 | validation: 0.28360269368389524]
	TIME [epoch: 9.73 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49351000665958467		[learning rate: 0.019999]
	Learning Rate: 0.019999
	LOSS [training: 0.49351000665958467 | validation: 0.44312499559136265]
	TIME [epoch: 9.71 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5426678655098205		[learning rate: 0.019999]
	Learning Rate: 0.0199987
	LOSS [training: 0.5426678655098205 | validation: 0.31922725773036803]
	TIME [epoch: 9.7 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43736207138927774		[learning rate: 0.019998]
	Learning Rate: 0.0199984
	LOSS [training: 0.43736207138927774 | validation: 0.2935226259696152]
	TIME [epoch: 9.69 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40838155007035715		[learning rate: 0.019998]
	Learning Rate: 0.0199981
	LOSS [training: 0.40838155007035715 | validation: 0.2582010299749883]
	TIME [epoch: 9.7 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3871419459908665		[learning rate: 0.019998]
	Learning Rate: 0.0199978
	LOSS [training: 0.3871419459908665 | validation: 0.3179254461097544]
	TIME [epoch: 9.74 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4205971828328898		[learning rate: 0.019997]
	Learning Rate: 0.0199974
	LOSS [training: 0.4205971828328898 | validation: 0.344952598420824]
	TIME [epoch: 9.7 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44637728648120806		[learning rate: 0.019997]
	Learning Rate: 0.019997
	LOSS [training: 0.44637728648120806 | validation: 0.28236008258150624]
	TIME [epoch: 9.69 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3876156032405344		[learning rate: 0.019997]
	Learning Rate: 0.0199966
	LOSS [training: 0.3876156032405344 | validation: 0.2670718818361311]
	TIME [epoch: 9.68 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3690028635083925		[learning rate: 0.019996]
	Learning Rate: 0.0199962
	LOSS [training: 0.3690028635083925 | validation: 0.4747076945961286]
	TIME [epoch: 9.68 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5484458702651597		[learning rate: 0.019996]
	Learning Rate: 0.0199957
	LOSS [training: 0.5484458702651597 | validation: 0.23545715888957452]
	TIME [epoch: 9.74 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3774834338846661		[learning rate: 0.019995]
	Learning Rate: 0.0199952
	LOSS [training: 0.3774834338846661 | validation: 0.23574915284207126]
	TIME [epoch: 9.7 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3470587296298641		[learning rate: 0.019995]
	Learning Rate: 0.0199947
	LOSS [training: 0.3470587296298641 | validation: 0.23837030262780995]
	TIME [epoch: 9.68 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3683260093534597		[learning rate: 0.019994]
	Learning Rate: 0.0199941
	LOSS [training: 0.3683260093534597 | validation: 0.3248270761276232]
	TIME [epoch: 9.69 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3651470859908221		[learning rate: 0.019994]
	Learning Rate: 0.0199935
	LOSS [training: 0.3651470859908221 | validation: 0.25154171668694064]
	TIME [epoch: 9.69 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41411659961635894		[learning rate: 0.019993]
	Learning Rate: 0.0199929
	LOSS [training: 0.41411659961635894 | validation: 0.38923358615022385]
	TIME [epoch: 9.74 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42923586469495695		[learning rate: 0.019992]
	Learning Rate: 0.0199923
	LOSS [training: 0.42923586469495695 | validation: 0.3172526038195733]
	TIME [epoch: 9.69 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39130693512844383		[learning rate: 0.019992]
	Learning Rate: 0.0199916
	LOSS [training: 0.39130693512844383 | validation: 0.2795920410296111]
	TIME [epoch: 9.68 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3890315307438431		[learning rate: 0.019991]
	Learning Rate: 0.0199909
	LOSS [training: 0.3890315307438431 | validation: 0.3543584861330765]
	TIME [epoch: 9.68 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.418868549496068		[learning rate: 0.01999]
	Learning Rate: 0.0199902
	LOSS [training: 0.418868549496068 | validation: 0.24004585050151744]
	TIME [epoch: 9.71 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3638730147701503		[learning rate: 0.019989]
	Learning Rate: 0.0199895
	LOSS [training: 0.3638730147701503 | validation: 0.2602164350222267]
	TIME [epoch: 9.71 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3932813235823479		[learning rate: 0.019989]
	Learning Rate: 0.0199887
	LOSS [training: 0.3932813235823479 | validation: 0.31006006459608854]
	TIME [epoch: 9.69 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3964995873493785		[learning rate: 0.019988]
	Learning Rate: 0.0199879
	LOSS [training: 0.3964995873493785 | validation: 0.2849513562657672]
	TIME [epoch: 9.69 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.390814117528569		[learning rate: 0.019987]
	Learning Rate: 0.0199871
	LOSS [training: 0.390814117528569 | validation: 0.2608475379281486]
	TIME [epoch: 9.69 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38811218265372927		[learning rate: 0.019986]
	Learning Rate: 0.0199862
	LOSS [training: 0.38811218265372927 | validation: 0.2211862044011953]
	TIME [epoch: 9.73 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33817080394563803		[learning rate: 0.019985]
	Learning Rate: 0.0199853
	LOSS [training: 0.33817080394563803 | validation: 0.27006978476268656]
	TIME [epoch: 9.71 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35119920011991207		[learning rate: 0.019984]
	Learning Rate: 0.0199844
	LOSS [training: 0.35119920011991207 | validation: 0.25172830168034865]
	TIME [epoch: 9.68 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3699089665223199		[learning rate: 0.019984]
	Learning Rate: 0.0199835
	LOSS [training: 0.3699089665223199 | validation: 0.3429787351802266]
	TIME [epoch: 9.68 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4440275016211376		[learning rate: 0.019983]
	Learning Rate: 0.0199825
	LOSS [training: 0.4440275016211376 | validation: 0.31054618140601264]
	TIME [epoch: 9.68 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3961347313174889		[learning rate: 0.019982]
	Learning Rate: 0.0199816
	LOSS [training: 0.3961347313174889 | validation: 0.20933159987168146]
	TIME [epoch: 9.73 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34980860491850285		[learning rate: 0.019981]
	Learning Rate: 0.0199805
	LOSS [training: 0.34980860491850285 | validation: 0.25264490268280215]
	TIME [epoch: 9.67 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3647192413369105		[learning rate: 0.019979]
	Learning Rate: 0.0199795
	LOSS [training: 0.3647192413369105 | validation: 0.30011574919769]
	TIME [epoch: 9.66 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3740261607797104		[learning rate: 0.019978]
	Learning Rate: 0.0199784
	LOSS [training: 0.3740261607797104 | validation: 0.24656376095098576]
	TIME [epoch: 9.66 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3417396589174818		[learning rate: 0.019977]
	Learning Rate: 0.0199773
	LOSS [training: 0.3417396589174818 | validation: 0.2251315200152837]
	TIME [epoch: 9.66 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3790775578984778		[learning rate: 0.019976]
	Learning Rate: 0.0199762
	LOSS [training: 0.3790775578984778 | validation: 0.34865429869147135]
	TIME [epoch: 9.7 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42019824709110737		[learning rate: 0.019975]
	Learning Rate: 0.019975
	LOSS [training: 0.42019824709110737 | validation: 0.3086196720503337]
	TIME [epoch: 9.67 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3599329752881961		[learning rate: 0.019974]
	Learning Rate: 0.0199739
	LOSS [training: 0.3599329752881961 | validation: 0.2168783048162175]
	TIME [epoch: 9.67 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3344217733247778		[learning rate: 0.019973]
	Learning Rate: 0.0199727
	LOSS [training: 0.3344217733247778 | validation: 0.3140372898197624]
	TIME [epoch: 9.68 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3618784116111526		[learning rate: 0.019971]
	Learning Rate: 0.0199714
	LOSS [training: 0.3618784116111526 | validation: 0.28356864422549893]
	TIME [epoch: 9.71 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37239892271574704		[learning rate: 0.01997]
	Learning Rate: 0.0199702
	LOSS [training: 0.37239892271574704 | validation: 0.2346674924910169]
	TIME [epoch: 9.7 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3376541570906771		[learning rate: 0.019969]
	Learning Rate: 0.0199689
	LOSS [training: 0.3376541570906771 | validation: 0.23648993981825234]
	TIME [epoch: 9.68 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.338269038753682		[learning rate: 0.019968]
	Learning Rate: 0.0199675
	LOSS [training: 0.338269038753682 | validation: 0.313233807090855]
	TIME [epoch: 9.68 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3943318862057143		[learning rate: 0.019966]
	Learning Rate: 0.0199662
	LOSS [training: 0.3943318862057143 | validation: 0.3099918477990258]
	TIME [epoch: 9.68 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38408967376816616		[learning rate: 0.019965]
	Learning Rate: 0.0199648
	LOSS [training: 0.38408967376816616 | validation: 0.25309621609223487]
	TIME [epoch: 9.71 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34965103567189915		[learning rate: 0.019963]
	Learning Rate: 0.0199634
	LOSS [training: 0.34965103567189915 | validation: 0.22027327939779517]
	TIME [epoch: 9.67 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34554428416213256		[learning rate: 0.019962]
	Learning Rate: 0.019962
	LOSS [training: 0.34554428416213256 | validation: 0.3305775988818726]
	TIME [epoch: 9.66 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3579908918853101		[learning rate: 0.019961]
	Learning Rate: 0.0199606
	LOSS [training: 0.3579908918853101 | validation: 0.2440211840035321]
	TIME [epoch: 9.66 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35009750929577454		[learning rate: 0.019959]
	Learning Rate: 0.0199591
	LOSS [training: 0.35009750929577454 | validation: 0.22004738491484244]
	TIME [epoch: 9.66 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3359836803794777		[learning rate: 0.019958]
	Learning Rate: 0.0199576
	LOSS [training: 0.3359836803794777 | validation: 0.25761819321440804]
	TIME [epoch: 9.7 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3730884413713659		[learning rate: 0.019956]
	Learning Rate: 0.019956
	LOSS [training: 0.3730884413713659 | validation: 0.27523630018109646]
	TIME [epoch: 9.67 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3371785648801259		[learning rate: 0.019954]
	Learning Rate: 0.0199545
	LOSS [training: 0.3371785648801259 | validation: 0.3194403760229765]
	TIME [epoch: 9.65 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3537116491994286		[learning rate: 0.019953]
	Learning Rate: 0.0199529
	LOSS [training: 0.3537116491994286 | validation: 0.22759121493307544]
	TIME [epoch: 9.65 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34110400743462405		[learning rate: 0.019951]
	Learning Rate: 0.0199513
	LOSS [training: 0.34110400743462405 | validation: 0.3577701038525192]
	TIME [epoch: 9.66 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3689792659794987		[learning rate: 0.01995]
	Learning Rate: 0.0199496
	LOSS [training: 0.3689792659794987 | validation: 0.2830013505530477]
	TIME [epoch: 9.7 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3630526442608996		[learning rate: 0.019948]
	Learning Rate: 0.0199479
	LOSS [training: 0.3630526442608996 | validation: 0.21626805699090346]
	TIME [epoch: 9.65 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33576158638738934		[learning rate: 0.019946]
	Learning Rate: 0.0199462
	LOSS [training: 0.33576158638738934 | validation: 0.2579944768527369]
	TIME [epoch: 9.65 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.338929659555056		[learning rate: 0.019945]
	Learning Rate: 0.0199445
	LOSS [training: 0.338929659555056 | validation: 0.25006840018895815]
	TIME [epoch: 9.65 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34127755770173485		[learning rate: 0.019943]
	Learning Rate: 0.0199428
	LOSS [training: 0.34127755770173485 | validation: 0.22325555313195244]
	TIME [epoch: 9.66 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34418335739444		[learning rate: 0.019941]
	Learning Rate: 0.019941
	LOSS [training: 0.34418335739444 | validation: 0.298947191700123]
	TIME [epoch: 9.69 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3948908477928826		[learning rate: 0.019939]
	Learning Rate: 0.0199392
	LOSS [training: 0.3948908477928826 | validation: 0.27104171611321726]
	TIME [epoch: 9.66 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3299241814381393		[learning rate: 0.019937]
	Learning Rate: 0.0199374
	LOSS [training: 0.3299241814381393 | validation: 0.2454980705312273]
	TIME [epoch: 9.66 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3175351290571636		[learning rate: 0.019935]
	Learning Rate: 0.0199355
	LOSS [training: 0.3175351290571636 | validation: 0.22887669037825895]
	TIME [epoch: 9.66 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3492590437574383		[learning rate: 0.019934]
	Learning Rate: 0.0199336
	LOSS [training: 0.3492590437574383 | validation: 0.24906656215862244]
	TIME [epoch: 9.69 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3444271854892301		[learning rate: 0.019932]
	Learning Rate: 0.0199317
	LOSS [training: 0.3444271854892301 | validation: 0.23900322067954882]
	TIME [epoch: 9.68 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3326747054929197		[learning rate: 0.01993]
	Learning Rate: 0.0199297
	LOSS [training: 0.3326747054929197 | validation: 0.23473668712679813]
	TIME [epoch: 9.66 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32712375961385975		[learning rate: 0.019928]
	Learning Rate: 0.0199278
	LOSS [training: 0.32712375961385975 | validation: 0.22674458679511403]
	TIME [epoch: 9.65 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3459440252301431		[learning rate: 0.019926]
	Learning Rate: 0.0199258
	LOSS [training: 0.3459440252301431 | validation: 0.31244805319795566]
	TIME [epoch: 9.65 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34578869132216006		[learning rate: 0.019924]
	Learning Rate: 0.0199238
	LOSS [training: 0.34578869132216006 | validation: 0.17280860694282896]
	TIME [epoch: 9.7 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4212564869220293		[learning rate: 0.019922]
	Learning Rate: 0.0199217
	LOSS [training: 0.4212564869220293 | validation: 0.20140631894631314]
	TIME [epoch: 9.69 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31846946129955916		[learning rate: 0.01992]
	Learning Rate: 0.0199196
	LOSS [training: 0.31846946129955916 | validation: 0.2073242663748307]
	TIME [epoch: 9.68 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3111465502647477		[learning rate: 0.019918]
	Learning Rate: 0.0199175
	LOSS [training: 0.3111465502647477 | validation: 0.21329063047400088]
	TIME [epoch: 9.68 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32054541668566355		[learning rate: 0.019915]
	Learning Rate: 0.0199154
	LOSS [training: 0.32054541668566355 | validation: 0.2093075239078276]
	TIME [epoch: 9.67 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31876170071664156		[learning rate: 0.019913]
	Learning Rate: 0.0199132
	LOSS [training: 0.31876170071664156 | validation: 0.2501979972855425]
	TIME [epoch: 9.72 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31957607215024164		[learning rate: 0.019911]
	Learning Rate: 0.019911
	LOSS [training: 0.31957607215024164 | validation: 0.2511151822993016]
	TIME [epoch: 9.68 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3619545293070049		[learning rate: 0.019909]
	Learning Rate: 0.0199088
	LOSS [training: 0.3619545293070049 | validation: 0.2480008184096648]
	TIME [epoch: 9.67 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34921652668479325		[learning rate: 0.019907]
	Learning Rate: 0.0199066
	LOSS [training: 0.34921652668479325 | validation: 0.2346624672303994]
	TIME [epoch: 9.67 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.325600416845278		[learning rate: 0.019904]
	Learning Rate: 0.0199043
	LOSS [training: 0.325600416845278 | validation: 0.20276873897208375]
	TIME [epoch: 9.67 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3092296192860833		[learning rate: 0.019902]
	Learning Rate: 0.019902
	LOSS [training: 0.3092296192860833 | validation: 0.2517655050111464]
	TIME [epoch: 9.73 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3151951885404961		[learning rate: 0.0199]
	Learning Rate: 0.0198997
	LOSS [training: 0.3151951885404961 | validation: 0.22961748578008878]
	TIME [epoch: 9.67 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3475535514544837		[learning rate: 0.019897]
	Learning Rate: 0.0198974
	LOSS [training: 0.3475535514544837 | validation: 0.26700719686374175]
	TIME [epoch: 9.67 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.37990345922583413		[learning rate: 0.019895]
	Learning Rate: 0.019895
	LOSS [training: 0.37990345922583413 | validation: 0.21984319074678482]
	TIME [epoch: 9.68 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3242697542996592		[learning rate: 0.019893]
	Learning Rate: 0.0198926
	LOSS [training: 0.3242697542996592 | validation: 0.21134442175975]
	TIME [epoch: 9.71 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31454023013195714		[learning rate: 0.01989]
	Learning Rate: 0.0198901
	LOSS [training: 0.31454023013195714 | validation: 0.19016543743009026]
	TIME [epoch: 9.7 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3057138632762284		[learning rate: 0.019888]
	Learning Rate: 0.0198877
	LOSS [training: 0.3057138632762284 | validation: 0.20924118045756837]
	TIME [epoch: 9.68 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33260446374225416		[learning rate: 0.019885]
	Learning Rate: 0.0198852
	LOSS [training: 0.33260446374225416 | validation: 0.22206212237583806]
	TIME [epoch: 9.66 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32718799447221575		[learning rate: 0.019883]
	Learning Rate: 0.0198827
	LOSS [training: 0.32718799447221575 | validation: 0.2439768685991613]
	TIME [epoch: 9.66 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35012165115280275		[learning rate: 0.01988]
	Learning Rate: 0.0198802
	LOSS [training: 0.35012165115280275 | validation: 0.2158038804182445]
	TIME [epoch: 9.7 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30353450653144276		[learning rate: 0.019878]
	Learning Rate: 0.0198776
	LOSS [training: 0.30353450653144276 | validation: 0.24424353800267562]
	TIME [epoch: 9.69 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3304542523971001		[learning rate: 0.019875]
	Learning Rate: 0.019875
	LOSS [training: 0.3304542523971001 | validation: 0.1880724637281504]
	TIME [epoch: 9.69 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30397655087454184		[learning rate: 0.019872]
	Learning Rate: 0.0198724
	LOSS [training: 0.30397655087454184 | validation: 0.2173455355268816]
	TIME [epoch: 9.68 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3428723847606306		[learning rate: 0.01987]
	Learning Rate: 0.0198697
	LOSS [training: 0.3428723847606306 | validation: 0.23521763495340275]
	TIME [epoch: 9.68 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36031590399921937		[learning rate: 0.019867]
	Learning Rate: 0.0198671
	LOSS [training: 0.36031590399921937 | validation: 0.2378645449525233]
	TIME [epoch: 9.73 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31534845508285403		[learning rate: 0.019864]
	Learning Rate: 0.0198644
	LOSS [training: 0.31534845508285403 | validation: 0.20309395353473447]
	TIME [epoch: 9.69 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33662460799478333		[learning rate: 0.019862]
	Learning Rate: 0.0198616
	LOSS [training: 0.33662460799478333 | validation: 0.18982167802408587]
	TIME [epoch: 9.68 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3082066017041873		[learning rate: 0.019859]
	Learning Rate: 0.0198589
	LOSS [training: 0.3082066017041873 | validation: 0.20288760280365076]
	TIME [epoch: 9.69 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3047169356090831		[learning rate: 0.019856]
	Learning Rate: 0.0198561
	LOSS [training: 0.3047169356090831 | validation: 0.2426176113316401]
	TIME [epoch: 9.69 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.315092620746383		[learning rate: 0.019853]
	Learning Rate: 0.0198533
	LOSS [training: 0.315092620746383 | validation: 0.23505533019572694]
	TIME [epoch: 9.72 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34799032381380984		[learning rate: 0.01985]
	Learning Rate: 0.0198505
	LOSS [training: 0.34799032381380984 | validation: 0.23250187564912556]
	TIME [epoch: 9.69 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35300520236081395		[learning rate: 0.019848]
	Learning Rate: 0.0198476
	LOSS [training: 0.35300520236081395 | validation: 0.24064431327404867]
	TIME [epoch: 9.7 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31466937241174237		[learning rate: 0.019845]
	Learning Rate: 0.0198447
	LOSS [training: 0.31466937241174237 | validation: 0.21696726252114087]
	TIME [epoch: 9.68 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30791902680233485		[learning rate: 0.019842]
	Learning Rate: 0.0198418
	LOSS [training: 0.30791902680233485 | validation: 0.2135188988213344]
	TIME [epoch: 9.69 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3255644503044604		[learning rate: 0.019839]
	Learning Rate: 0.0198388
	LOSS [training: 0.3255644503044604 | validation: 0.21003444392611811]
	TIME [epoch: 9.73 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3126961390269316		[learning rate: 0.019836]
	Learning Rate: 0.0198359
	LOSS [training: 0.3126961390269316 | validation: 0.22400598354904228]
	TIME [epoch: 9.68 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3232099079938111		[learning rate: 0.019833]
	Learning Rate: 0.0198329
	LOSS [training: 0.3232099079938111 | validation: 0.21957719606804865]
	TIME [epoch: 9.68 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3185194595836541		[learning rate: 0.01983]
	Learning Rate: 0.0198299
	LOSS [training: 0.3185194595836541 | validation: 0.212120234467209]
	TIME [epoch: 9.68 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31143457909294103		[learning rate: 0.019827]
	Learning Rate: 0.0198268
	LOSS [training: 0.31143457909294103 | validation: 0.21952058873923153]
	TIME [epoch: 9.71 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30302384283015654		[learning rate: 0.019824]
	Learning Rate: 0.0198237
	LOSS [training: 0.30302384283015654 | validation: 0.2045845719643224]
	TIME [epoch: 9.69 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29779737740859297		[learning rate: 0.019821]
	Learning Rate: 0.0198206
	LOSS [training: 0.29779737740859297 | validation: 0.2057085281420678]
	TIME [epoch: 9.68 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4068747392217166		[learning rate: 0.019818]
	Learning Rate: 0.0198175
	LOSS [training: 0.4068747392217166 | validation: 0.20578021378863992]
	TIME [epoch: 9.68 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33424194444208644		[learning rate: 0.019814]
	Learning Rate: 0.0198143
	LOSS [training: 0.33424194444208644 | validation: 0.21013755841500553]
	TIME [epoch: 9.68 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30161120110543843		[learning rate: 0.019811]
	Learning Rate: 0.0198112
	LOSS [training: 0.30161120110543843 | validation: 0.20857190312000445]
	TIME [epoch: 9.73 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3059507301628316		[learning rate: 0.019808]
	Learning Rate: 0.0198079
	LOSS [training: 0.3059507301628316 | validation: 0.18424016271955124]
	TIME [epoch: 9.69 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28938780921709917		[learning rate: 0.019805]
	Learning Rate: 0.0198047
	LOSS [training: 0.28938780921709917 | validation: 0.19337533632444898]
	TIME [epoch: 9.67 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3375539709834982		[learning rate: 0.019801]
	Learning Rate: 0.0198014
	LOSS [training: 0.3375539709834982 | validation: 0.19152045567332981]
	TIME [epoch: 9.68 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3203823658803673		[learning rate: 0.019798]
	Learning Rate: 0.0197982
	LOSS [training: 0.3203823658803673 | validation: 0.2539773826056734]
	TIME [epoch: 9.69 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30480867888790125		[learning rate: 0.019795]
	Learning Rate: 0.0197948
	LOSS [training: 0.30480867888790125 | validation: 0.21219516738033495]
	TIME [epoch: 9.73 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3004639565462968		[learning rate: 0.019791]
	Learning Rate: 0.0197915
	LOSS [training: 0.3004639565462968 | validation: 0.20070549368542503]
	TIME [epoch: 9.67 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3056638410020348		[learning rate: 0.019788]
	Learning Rate: 0.0197881
	LOSS [training: 0.3056638410020348 | validation: 0.19673591746011698]
	TIME [epoch: 9.67 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3388276360993283		[learning rate: 0.019785]
	Learning Rate: 0.0197847
	LOSS [training: 0.3388276360993283 | validation: 0.20560392736286734]
	TIME [epoch: 9.66 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3101386223956707		[learning rate: 0.019781]
	Learning Rate: 0.0197813
	LOSS [training: 0.3101386223956707 | validation: 0.18903447473263518]
	TIME [epoch: 9.68 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.339811991679925		[learning rate: 0.019778]
	Learning Rate: 0.0197778
	LOSS [training: 0.339811991679925 | validation: 0.1857234039778739]
	TIME [epoch: 9.72 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2821627646652944		[learning rate: 0.019774]
	Learning Rate: 0.0197744
	LOSS [training: 0.2821627646652944 | validation: 0.19368720472480094]
	TIME [epoch: 9.68 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29862501605678243		[learning rate: 0.019771]
	Learning Rate: 0.0197709
	LOSS [training: 0.29862501605678243 | validation: 0.21575443440538825]
	TIME [epoch: 9.68 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3022651256009029		[learning rate: 0.019767]
	Learning Rate: 0.0197673
	LOSS [training: 0.3022651256009029 | validation: 0.19333234135540792]
	TIME [epoch: 9.66 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3644409911716129		[learning rate: 0.019764]
	Learning Rate: 0.0197638
	LOSS [training: 0.3644409911716129 | validation: 0.20962425621400493]
	TIME [epoch: 9.71 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30669089999179144		[learning rate: 0.01976]
	Learning Rate: 0.0197602
	LOSS [training: 0.30669089999179144 | validation: 0.2074331304712667]
	TIME [epoch: 9.7 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31271462561760777		[learning rate: 0.019757]
	Learning Rate: 0.0197566
	LOSS [training: 0.31271462561760777 | validation: 0.20958158231161872]
	TIME [epoch: 9.68 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30015821639379986		[learning rate: 0.019753]
	Learning Rate: 0.0197529
	LOSS [training: 0.30015821639379986 | validation: 0.19302355207631325]
	TIME [epoch: 9.67 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2928876760620982		[learning rate: 0.019749]
	Learning Rate: 0.0197493
	LOSS [training: 0.2928876760620982 | validation: 0.2063212400679188]
	TIME [epoch: 9.68 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30473295696963343		[learning rate: 0.019746]
	Learning Rate: 0.0197456
	LOSS [training: 0.30473295696963343 | validation: 0.19150886502611383]
	TIME [epoch: 9.71 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31970905630519564		[learning rate: 0.019742]
	Learning Rate: 0.0197419
	LOSS [training: 0.31970905630519564 | validation: 0.22065994622792987]
	TIME [epoch: 9.69 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3492319833938686		[learning rate: 0.019738]
	Learning Rate: 0.0197381
	LOSS [training: 0.3492319833938686 | validation: 0.19652864783119495]
	TIME [epoch: 9.68 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.307667406637096		[learning rate: 0.019734]
	Learning Rate: 0.0197343
	LOSS [training: 0.307667406637096 | validation: 0.2040044195459158]
	TIME [epoch: 9.67 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3007368402202669		[learning rate: 0.019731]
	Learning Rate: 0.0197305
	LOSS [training: 0.3007368402202669 | validation: 0.19237514375836584]
	TIME [epoch: 9.68 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.301889815810948		[learning rate: 0.019727]
	Learning Rate: 0.0197267
	LOSS [training: 0.301889815810948 | validation: 0.21103782423071923]
	TIME [epoch: 9.73 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29819891203472954		[learning rate: 0.019723]
	Learning Rate: 0.0197229
	LOSS [training: 0.29819891203472954 | validation: 0.19106544681313078]
	TIME [epoch: 9.68 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3134906452169174		[learning rate: 0.019719]
	Learning Rate: 0.019719
	LOSS [training: 0.3134906452169174 | validation: 0.18772672929904466]
	TIME [epoch: 9.68 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30837882559555574		[learning rate: 0.019715]
	Learning Rate: 0.0197151
	LOSS [training: 0.30837882559555574 | validation: 0.2165660306942064]
	TIME [epoch: 9.67 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31328581693370033		[learning rate: 0.019711]
	Learning Rate: 0.0197112
	LOSS [training: 0.31328581693370033 | validation: 0.34886046241322965]
	TIME [epoch: 9.67 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3437929170700225		[learning rate: 0.019707]
	Learning Rate: 0.0197072
	LOSS [training: 0.3437929170700225 | validation: 0.18891636042271592]
	TIME [epoch: 9.72 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.294318359510378		[learning rate: 0.019703]
	Learning Rate: 0.0197032
	LOSS [training: 0.294318359510378 | validation: 0.18150884166973083]
	TIME [epoch: 9.68 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.292107630688929		[learning rate: 0.019699]
	Learning Rate: 0.0196992
	LOSS [training: 0.292107630688929 | validation: 0.2003357734395878]
	TIME [epoch: 9.69 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30211615714569584		[learning rate: 0.019695]
	Learning Rate: 0.0196952
	LOSS [training: 0.30211615714569584 | validation: 0.1903116445061126]
	TIME [epoch: 9.69 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29930489707604324		[learning rate: 0.019691]
	Learning Rate: 0.0196911
	LOSS [training: 0.29930489707604324 | validation: 0.2514102104107784]
	TIME [epoch: 113 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31791996354588914		[learning rate: 0.019687]
	Learning Rate: 0.019687
	LOSS [training: 0.31791996354588914 | validation: 0.2138564367958634]
	TIME [epoch: 19.2 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3018988696633282		[learning rate: 0.019683]
	Learning Rate: 0.0196829
	LOSS [training: 0.3018988696633282 | validation: 0.20579940613560155]
	TIME [epoch: 19.2 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3243023519339704		[learning rate: 0.019679]
	Learning Rate: 0.0196788
	LOSS [training: 0.3243023519339704 | validation: 0.19426720256238925]
	TIME [epoch: 19.1 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2962029251087323		[learning rate: 0.019675]
	Learning Rate: 0.0196746
	LOSS [training: 0.2962029251087323 | validation: 0.1934528070987793]
	TIME [epoch: 19.2 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28681738300325726		[learning rate: 0.01967]
	Learning Rate: 0.0196704
	LOSS [training: 0.28681738300325726 | validation: 0.19758772338247801]
	TIME [epoch: 19.1 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3024050934590182		[learning rate: 0.019666]
	Learning Rate: 0.0196662
	LOSS [training: 0.3024050934590182 | validation: 0.2297636285036509]
	TIME [epoch: 19.1 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.359293756268538		[learning rate: 0.019662]
	Learning Rate: 0.0196619
	LOSS [training: 0.359293756268538 | validation: 0.19911041242206046]
	TIME [epoch: 19.2 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29786522296898216		[learning rate: 0.019658]
	Learning Rate: 0.0196576
	LOSS [training: 0.29786522296898216 | validation: 0.19469250238818814]
	TIME [epoch: 19.1 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.297609765752708		[learning rate: 0.019653]
	Learning Rate: 0.0196533
	LOSS [training: 0.297609765752708 | validation: 0.18761364391913418]
	TIME [epoch: 19.2 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.296915965114274		[learning rate: 0.019649]
	Learning Rate: 0.019649
	LOSS [training: 0.296915965114274 | validation: 0.1867161309512512]
	TIME [epoch: 19.1 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2890646037582262		[learning rate: 0.019645]
	Learning Rate: 0.0196447
	LOSS [training: 0.2890646037582262 | validation: 0.1886208117109086]
	TIME [epoch: 19.1 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3073498513423011		[learning rate: 0.01964]
	Learning Rate: 0.0196403
	LOSS [training: 0.3073498513423011 | validation: 0.24984054825344015]
	TIME [epoch: 19.2 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32369710272308916		[learning rate: 0.019636]
	Learning Rate: 0.0196359
	LOSS [training: 0.32369710272308916 | validation: 0.20484827896551372]
	TIME [epoch: 19.2 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29175607423729544		[learning rate: 0.019631]
	Learning Rate: 0.0196314
	LOSS [training: 0.29175607423729544 | validation: 0.2091017281341446]
	TIME [epoch: 19.2 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3075466813143771		[learning rate: 0.019627]
	Learning Rate: 0.019627
	LOSS [training: 0.3075466813143771 | validation: 0.20396163949305046]
	TIME [epoch: 19.2 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3099393276956815		[learning rate: 0.019622]
	Learning Rate: 0.0196225
	LOSS [training: 0.3099393276956815 | validation: 0.17493886135479575]
	TIME [epoch: 19.2 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30115581130249336		[learning rate: 0.019618]
	Learning Rate: 0.019618
	LOSS [training: 0.30115581130249336 | validation: 0.2113065318661378]
	TIME [epoch: 19.2 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31472872147771924		[learning rate: 0.019613]
	Learning Rate: 0.0196134
	LOSS [training: 0.31472872147771924 | validation: 0.3019991693695568]
	TIME [epoch: 19.2 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3411736254053399		[learning rate: 0.019609]
	Learning Rate: 0.0196089
	LOSS [training: 0.3411736254053399 | validation: 0.18414134535965018]
	TIME [epoch: 19.2 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30119666398867473		[learning rate: 0.019604]
	Learning Rate: 0.0196043
	LOSS [training: 0.30119666398867473 | validation: 0.20134772729094397]
	TIME [epoch: 19.2 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29045204197095487		[learning rate: 0.0196]
	Learning Rate: 0.0195997
	LOSS [training: 0.29045204197095487 | validation: 0.17927794604070574]
	TIME [epoch: 19.2 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28643894596092045		[learning rate: 0.019595]
	Learning Rate: 0.019595
	LOSS [training: 0.28643894596092045 | validation: 0.17863293738363212]
	TIME [epoch: 19.2 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2853395649664332		[learning rate: 0.01959]
	Learning Rate: 0.0195904
	LOSS [training: 0.2853395649664332 | validation: 0.18545159504459047]
	TIME [epoch: 19.2 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30165541103514515		[learning rate: 0.019586]
	Learning Rate: 0.0195857
	LOSS [training: 0.30165541103514515 | validation: 0.2263875929392437]
	TIME [epoch: 19.2 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29732396723940946		[learning rate: 0.019581]
	Learning Rate: 0.0195809
	LOSS [training: 0.29732396723940946 | validation: 0.2175980056310704]
	TIME [epoch: 19.2 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30176220645757723		[learning rate: 0.019576]
	Learning Rate: 0.0195762
	LOSS [training: 0.30176220645757723 | validation: 0.21569342986824433]
	TIME [epoch: 19.2 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34376346040949873		[learning rate: 0.019571]
	Learning Rate: 0.0195714
	LOSS [training: 0.34376346040949873 | validation: 0.2854687336973325]
	TIME [epoch: 19.2 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3133522005458469		[learning rate: 0.019567]
	Learning Rate: 0.0195666
	LOSS [training: 0.3133522005458469 | validation: 0.1759906369370519]
	TIME [epoch: 19.2 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2829235841652103		[learning rate: 0.019562]
	Learning Rate: 0.0195618
	LOSS [training: 0.2829235841652103 | validation: 0.1793016527821461]
	TIME [epoch: 19.2 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28834041320272397		[learning rate: 0.019557]
	Learning Rate: 0.0195569
	LOSS [training: 0.28834041320272397 | validation: 0.1839162492142667]
	TIME [epoch: 19.2 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29785742991436687		[learning rate: 0.019552]
	Learning Rate: 0.0195521
	LOSS [training: 0.29785742991436687 | validation: 0.19281075428496766]
	TIME [epoch: 19.2 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2958513431615444		[learning rate: 0.019547]
	Learning Rate: 0.0195472
	LOSS [training: 0.2958513431615444 | validation: 0.18168526458774997]
	TIME [epoch: 19.2 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2951763577031891		[learning rate: 0.019542]
	Learning Rate: 0.0195422
	LOSS [training: 0.2951763577031891 | validation: 0.1967900278922134]
	TIME [epoch: 19.1 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3188656842808924		[learning rate: 0.019537]
	Learning Rate: 0.0195373
	LOSS [training: 0.3188656842808924 | validation: 0.26492217294404985]
	TIME [epoch: 19.2 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3342720128175708		[learning rate: 0.019532]
	Learning Rate: 0.0195323
	LOSS [training: 0.3342720128175708 | validation: 0.21403668495858716]
	TIME [epoch: 19.2 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29390025569960965		[learning rate: 0.019527]
	Learning Rate: 0.0195273
	LOSS [training: 0.29390025569960965 | validation: 0.1832350560373861]
	TIME [epoch: 19.2 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28420294204499935		[learning rate: 0.019522]
	Learning Rate: 0.0195222
	LOSS [training: 0.28420294204499935 | validation: 0.19313774696230418]
	TIME [epoch: 19.2 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29604970362380173		[learning rate: 0.019517]
	Learning Rate: 0.0195172
	LOSS [training: 0.29604970362380173 | validation: 0.1826279590017253]
	TIME [epoch: 19.2 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2914235419028114		[learning rate: 0.019512]
	Learning Rate: 0.0195121
	LOSS [training: 0.2914235419028114 | validation: 0.17576236490783304]
	TIME [epoch: 19.2 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2968768136717185		[learning rate: 0.019507]
	Learning Rate: 0.019507
	LOSS [training: 0.2968768136717185 | validation: 0.2077585037734395]
	TIME [epoch: 19.2 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35231186839440964		[learning rate: 0.019502]
	Learning Rate: 0.0195018
	LOSS [training: 0.35231186839440964 | validation: 0.18198162056462933]
	TIME [epoch: 19.2 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29252414513145786		[learning rate: 0.019497]
	Learning Rate: 0.0194967
	LOSS [training: 0.29252414513145786 | validation: 0.19023089796143605]
	TIME [epoch: 19.2 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2945364118320805		[learning rate: 0.019491]
	Learning Rate: 0.0194915
	LOSS [training: 0.2945364118320805 | validation: 0.17856191089858636]
	TIME [epoch: 19.2 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28850340037525457		[learning rate: 0.019486]
	Learning Rate: 0.0194863
	LOSS [training: 0.28850340037525457 | validation: 0.17777846989201404]
	TIME [epoch: 19.2 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2907918542777067		[learning rate: 0.019481]
	Learning Rate: 0.019481
	LOSS [training: 0.2907918542777067 | validation: 0.2050825304567986]
	TIME [epoch: 19.2 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31580609029129103		[learning rate: 0.019476]
	Learning Rate: 0.0194757
	LOSS [training: 0.31580609029129103 | validation: 0.20234502911775165]
	TIME [epoch: 19.2 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33723941763103416		[learning rate: 0.01947]
	Learning Rate: 0.0194705
	LOSS [training: 0.33723941763103416 | validation: 0.17303527913454042]
	TIME [epoch: 19.2 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28788376053934084		[learning rate: 0.019465]
	Learning Rate: 0.0194651
	LOSS [training: 0.28788376053934084 | validation: 0.191235845402236]
	TIME [epoch: 19.1 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.292201215064394		[learning rate: 0.01946]
	Learning Rate: 0.0194598
	LOSS [training: 0.292201215064394 | validation: 0.17553174396967772]
	TIME [epoch: 19.2 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2892191715752571		[learning rate: 0.019454]
	Learning Rate: 0.0194544
	LOSS [training: 0.2892191715752571 | validation: 0.18453892938984567]
	TIME [epoch: 19.1 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3015034169111075		[learning rate: 0.019449]
	Learning Rate: 0.019449
	LOSS [training: 0.3015034169111075 | validation: 0.18119421792457208]
	TIME [epoch: 19.2 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2958046359596552		[learning rate: 0.019444]
	Learning Rate: 0.0194436
	LOSS [training: 0.2958046359596552 | validation: 0.17879284942513493]
	TIME [epoch: 19.2 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2971629440016452		[learning rate: 0.019438]
	Learning Rate: 0.0194381
	LOSS [training: 0.2971629440016452 | validation: 0.1773094853315885]
	TIME [epoch: 19.2 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2930485827454046		[learning rate: 0.019433]
	Learning Rate: 0.0194327
	LOSS [training: 0.2930485827454046 | validation: 0.17798369852619722]
	TIME [epoch: 19.2 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3106870738323538		[learning rate: 0.019427]
	Learning Rate: 0.0194272
	LOSS [training: 0.3106870738323538 | validation: 0.18464946407957908]
	TIME [epoch: 19.2 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3509573470637654		[learning rate: 0.019422]
	Learning Rate: 0.0194216
	LOSS [training: 0.3509573470637654 | validation: 0.25547468264098017]
	TIME [epoch: 19.2 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3037399382354763		[learning rate: 0.019416]
	Learning Rate: 0.0194161
	LOSS [training: 0.3037399382354763 | validation: 0.17887163573077688]
	TIME [epoch: 19.2 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2820009298332423		[learning rate: 0.019411]
	Learning Rate: 0.0194105
	LOSS [training: 0.2820009298332423 | validation: 0.17373170251808207]
	TIME [epoch: 19.2 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29215921745623075		[learning rate: 0.019405]
	Learning Rate: 0.0194049
	LOSS [training: 0.29215921745623075 | validation: 0.18643961178835292]
	TIME [epoch: 19.2 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2856662295488093		[learning rate: 0.019399]
	Learning Rate: 0.0193993
	LOSS [training: 0.2856662295488093 | validation: 0.16975765987237312]
	TIME [epoch: 19.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_261.pth
	Model improved!!!
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2880370572089639		[learning rate: 0.019394]
	Learning Rate: 0.0193936
	LOSS [training: 0.2880370572089639 | validation: 0.1910012779079314]
	TIME [epoch: 19.2 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2975923279463659		[learning rate: 0.019388]
	Learning Rate: 0.0193879
	LOSS [training: 0.2975923279463659 | validation: 0.18770682418714388]
	TIME [epoch: 19.2 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2909679252841625		[learning rate: 0.019382]
	Learning Rate: 0.0193822
	LOSS [training: 0.2909679252841625 | validation: 0.21896768343480988]
	TIME [epoch: 19.1 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3220377972271524		[learning rate: 0.019376]
	Learning Rate: 0.0193765
	LOSS [training: 0.3220377972271524 | validation: 0.3755808661417031]
	TIME [epoch: 19.2 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33645636719721966		[learning rate: 0.019371]
	Learning Rate: 0.0193707
	LOSS [training: 0.33645636719721966 | validation: 0.17318805425060674]
	TIME [epoch: 19.1 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2928067186103284		[learning rate: 0.019365]
	Learning Rate: 0.0193649
	LOSS [training: 0.2928067186103284 | validation: 0.1744099418730951]
	TIME [epoch: 19.1 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28807349377053126		[learning rate: 0.019359]
	Learning Rate: 0.0193591
	LOSS [training: 0.28807349377053126 | validation: 0.16589376188380828]
	TIME [epoch: 19.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_268.pth
	Model improved!!!
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28349434067900464		[learning rate: 0.019353]
	Learning Rate: 0.0193533
	LOSS [training: 0.28349434067900464 | validation: 0.17330752907878108]
	TIME [epoch: 19.1 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2948778280256384		[learning rate: 0.019347]
	Learning Rate: 0.0193474
	LOSS [training: 0.2948778280256384 | validation: 0.17717988125067566]
	TIME [epoch: 19.2 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2937895806065459		[learning rate: 0.019342]
	Learning Rate: 0.0193416
	LOSS [training: 0.2937895806065459 | validation: 0.16572377743928626]
	TIME [epoch: 19.1 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_271.pth
	Model improved!!!
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2858879543541329		[learning rate: 0.019336]
	Learning Rate: 0.0193356
	LOSS [training: 0.2858879543541329 | validation: 0.16792122008750704]
	TIME [epoch: 19.2 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3017784345018518		[learning rate: 0.01933]
	Learning Rate: 0.0193297
	LOSS [training: 0.3017784345018518 | validation: 0.18605231786694004]
	TIME [epoch: 19.2 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3281187615591907		[learning rate: 0.019324]
	Learning Rate: 0.0193237
	LOSS [training: 0.3281187615591907 | validation: 0.25888809628377285]
	TIME [epoch: 19.1 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3068999668721151		[learning rate: 0.019318]
	Learning Rate: 0.0193178
	LOSS [training: 0.3068999668721151 | validation: 0.18606759362920902]
	TIME [epoch: 19.2 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2860310456080051		[learning rate: 0.019312]
	Learning Rate: 0.0193117
	LOSS [training: 0.2860310456080051 | validation: 0.17340711558076932]
	TIME [epoch: 19.1 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28606518991285584		[learning rate: 0.019306]
	Learning Rate: 0.0193057
	LOSS [training: 0.28606518991285584 | validation: 0.17000147934402748]
	TIME [epoch: 19.1 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2844307256166716		[learning rate: 0.0193]
	Learning Rate: 0.0192996
	LOSS [training: 0.2844307256166716 | validation: 0.17447684680554254]
	TIME [epoch: 19.2 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28998120006647		[learning rate: 0.019294]
	Learning Rate: 0.0192935
	LOSS [training: 0.28998120006647 | validation: 0.17137934177587805]
	TIME [epoch: 19.1 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29559888039379834		[learning rate: 0.019287]
	Learning Rate: 0.0192874
	LOSS [training: 0.29559888039379834 | validation: 0.18886063562614103]
	TIME [epoch: 19.2 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2961725961483017		[learning rate: 0.019281]
	Learning Rate: 0.0192813
	LOSS [training: 0.2961725961483017 | validation: 0.1763143214667161]
	TIME [epoch: 19.1 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32769912669424783		[learning rate: 0.019275]
	Learning Rate: 0.0192751
	LOSS [training: 0.32769912669424783 | validation: 0.18902323174368174]
	TIME [epoch: 19.1 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.290603858284203		[learning rate: 0.019269]
	Learning Rate: 0.0192689
	LOSS [training: 0.290603858284203 | validation: 0.1880574796556987]
	TIME [epoch: 19.2 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28521682631252654		[learning rate: 0.019263]
	Learning Rate: 0.0192627
	LOSS [training: 0.28521682631252654 | validation: 0.17591594975222336]
	TIME [epoch: 19.1 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2829428264353184		[learning rate: 0.019256]
	Learning Rate: 0.0192565
	LOSS [training: 0.2829428264353184 | validation: 0.17537745190161463]
	TIME [epoch: 19.2 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29411572951578285		[learning rate: 0.01925]
	Learning Rate: 0.0192502
	LOSS [training: 0.29411572951578285 | validation: 0.1798364958984577]
	TIME [epoch: 19.1 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.291274283331664		[learning rate: 0.019244]
	Learning Rate: 0.0192439
	LOSS [training: 0.291274283331664 | validation: 0.17321774705275556]
	TIME [epoch: 19.2 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2873850269453294		[learning rate: 0.019238]
	Learning Rate: 0.0192376
	LOSS [training: 0.2873850269453294 | validation: 0.20096617124315178]
	TIME [epoch: 19.2 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30578814087536377		[learning rate: 0.019231]
	Learning Rate: 0.0192313
	LOSS [training: 0.30578814087536377 | validation: 0.23584341658617525]
	TIME [epoch: 19.1 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33669673578143455		[learning rate: 0.019225]
	Learning Rate: 0.0192249
	LOSS [training: 0.33669673578143455 | validation: 0.16988930313876707]
	TIME [epoch: 19.2 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2854307787761375		[learning rate: 0.019218]
	Learning Rate: 0.0192185
	LOSS [training: 0.2854307787761375 | validation: 0.18717339470202518]
	TIME [epoch: 19.1 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2899457235227587		[learning rate: 0.019212]
	Learning Rate: 0.0192121
	LOSS [training: 0.2899457235227587 | validation: 0.16487978960361815]
	TIME [epoch: 19.1 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_292.pth
	Model improved!!!
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2814026879943338		[learning rate: 0.019206]
	Learning Rate: 0.0192056
	LOSS [training: 0.2814026879943338 | validation: 0.1773603607098166]
	TIME [epoch: 19.2 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28445494413775263		[learning rate: 0.019199]
	Learning Rate: 0.0191992
	LOSS [training: 0.28445494413775263 | validation: 0.16972009266315286]
	TIME [epoch: 19.1 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2907359491663777		[learning rate: 0.019193]
	Learning Rate: 0.0191927
	LOSS [training: 0.2907359491663777 | validation: 0.18044926677831505]
	TIME [epoch: 19.2 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2915144432690093		[learning rate: 0.019186]
	Learning Rate: 0.0191861
	LOSS [training: 0.2915144432690093 | validation: 0.16841382849049075]
	TIME [epoch: 19.1 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2984199571400598		[learning rate: 0.01918]
	Learning Rate: 0.0191796
	LOSS [training: 0.2984199571400598 | validation: 0.2422050862354233]
	TIME [epoch: 19.2 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31872984216593275		[learning rate: 0.019173]
	Learning Rate: 0.019173
	LOSS [training: 0.31872984216593275 | validation: 0.2143337236838828]
	TIME [epoch: 19.2 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2997254271612933		[learning rate: 0.019166]
	Learning Rate: 0.0191664
	LOSS [training: 0.2997254271612933 | validation: 0.17577132881251498]
	TIME [epoch: 19.1 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28543974052171156		[learning rate: 0.01916]
	Learning Rate: 0.0191598
	LOSS [training: 0.28543974052171156 | validation: 0.16988754086736135]
	TIME [epoch: 19.2 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2862993237537771		[learning rate: 0.019153]
	Learning Rate: 0.0191532
	LOSS [training: 0.2862993237537771 | validation: 0.1666856097929822]
	TIME [epoch: 19.2 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2810526611087222		[learning rate: 0.019146]
	Learning Rate: 0.0191465
	LOSS [training: 0.2810526611087222 | validation: 0.1805391636826138]
	TIME [epoch: 19.2 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30115648893614205		[learning rate: 0.01914]
	Learning Rate: 0.0191398
	LOSS [training: 0.30115648893614205 | validation: 0.17903083423704538]
	TIME [epoch: 19.2 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2984336963864469		[learning rate: 0.019133]
	Learning Rate: 0.0191331
	LOSS [training: 0.2984336963864469 | validation: 0.29326169925381007]
	TIME [epoch: 19.2 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3278505148739306		[learning rate: 0.019126]
	Learning Rate: 0.0191263
	LOSS [training: 0.3278505148739306 | validation: 0.1787619139827878]
	TIME [epoch: 19.2 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28599265843729466		[learning rate: 0.01912]
	Learning Rate: 0.0191196
	LOSS [training: 0.28599265843729466 | validation: 0.19384663763045945]
	TIME [epoch: 19.2 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2826798384377169		[learning rate: 0.019113]
	Learning Rate: 0.0191128
	LOSS [training: 0.2826798384377169 | validation: 0.17246457952665495]
	TIME [epoch: 19.2 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28206630491627027		[learning rate: 0.019106]
	Learning Rate: 0.019106
	LOSS [training: 0.28206630491627027 | validation: 0.18539674578984305]
	TIME [epoch: 19.2 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28277271966993256		[learning rate: 0.019099]
	Learning Rate: 0.0190991
	LOSS [training: 0.28277271966993256 | validation: 0.1715482655857659]
	TIME [epoch: 19.1 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3004479744109243		[learning rate: 0.019092]
	Learning Rate: 0.0190922
	LOSS [training: 0.3004479744109243 | validation: 0.1726633094039144]
	TIME [epoch: 19.2 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28338606998114063		[learning rate: 0.019085]
	Learning Rate: 0.0190853
	LOSS [training: 0.28338606998114063 | validation: 0.1874440440658036]
	TIME [epoch: 19.1 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29754329916432826		[learning rate: 0.019078]
	Learning Rate: 0.0190784
	LOSS [training: 0.29754329916432826 | validation: 0.18885063493825302]
	TIME [epoch: 19.2 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2929912063979246		[learning rate: 0.019071]
	Learning Rate: 0.0190715
	LOSS [training: 0.2929912063979246 | validation: 0.2240974275037395]
	TIME [epoch: 19.2 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31049841581328397		[learning rate: 0.019065]
	Learning Rate: 0.0190645
	LOSS [training: 0.31049841581328397 | validation: 0.19507555039249458]
	TIME [epoch: 19.1 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2852791983181391		[learning rate: 0.019058]
	Learning Rate: 0.0190575
	LOSS [training: 0.2852791983181391 | validation: 0.16834725568644401]
	TIME [epoch: 19.2 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28351943876530344		[learning rate: 0.019051]
	Learning Rate: 0.0190505
	LOSS [training: 0.28351943876530344 | validation: 0.16667066866634228]
	TIME [epoch: 19.1 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28543071662349256		[learning rate: 0.019043]
	Learning Rate: 0.0190435
	LOSS [training: 0.28543071662349256 | validation: 0.16855167090220372]
	TIME [epoch: 19.2 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.285794377926903		[learning rate: 0.019036]
	Learning Rate: 0.0190364
	LOSS [training: 0.285794377926903 | validation: 0.171143549208776]
	TIME [epoch: 19.2 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.291061400606894		[learning rate: 0.019029]
	Learning Rate: 0.0190293
	LOSS [training: 0.291061400606894 | validation: 0.17708048832418505]
	TIME [epoch: 19.1 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28675713251814716		[learning rate: 0.019022]
	Learning Rate: 0.0190222
	LOSS [training: 0.28675713251814716 | validation: 0.16620146728101437]
	TIME [epoch: 19.2 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2866595055207317		[learning rate: 0.019015]
	Learning Rate: 0.019015
	LOSS [training: 0.2866595055207317 | validation: 0.22731005653679132]
	TIME [epoch: 19.1 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3657926591749518		[learning rate: 0.019008]
	Learning Rate: 0.0190079
	LOSS [training: 0.3657926591749518 | validation: 0.17020285975702804]
	TIME [epoch: 19.2 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2758654869882527		[learning rate: 0.019001]
	Learning Rate: 0.0190007
	LOSS [training: 0.2758654869882527 | validation: 0.18135740043728932]
	TIME [epoch: 19.2 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2906393566355715		[learning rate: 0.018993]
	Learning Rate: 0.0189935
	LOSS [training: 0.2906393566355715 | validation: 0.17835871519013458]
	TIME [epoch: 19.2 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27892992063082006		[learning rate: 0.018986]
	Learning Rate: 0.0189862
	LOSS [training: 0.27892992063082006 | validation: 0.16405839733413166]
	TIME [epoch: 19.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_325.pth
	Model improved!!!
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28278260294990526		[learning rate: 0.018979]
	Learning Rate: 0.018979
	LOSS [training: 0.28278260294990526 | validation: 0.1741678726745646]
	TIME [epoch: 19.2 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.282703751419053		[learning rate: 0.018972]
	Learning Rate: 0.0189717
	LOSS [training: 0.282703751419053 | validation: 0.19599906523994343]
	TIME [epoch: 19.2 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2935441094524185		[learning rate: 0.018964]
	Learning Rate: 0.0189644
	LOSS [training: 0.2935441094524185 | validation: 0.18264076421319908]
	TIME [epoch: 19.1 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2947110274024477		[learning rate: 0.018957]
	Learning Rate: 0.018957
	LOSS [training: 0.2947110274024477 | validation: 0.1733375605426634]
	TIME [epoch: 19.1 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2847601103540093		[learning rate: 0.01895]
	Learning Rate: 0.0189497
	LOSS [training: 0.2847601103540093 | validation: 0.17268010446653875]
	TIME [epoch: 19.2 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2869626451742556		[learning rate: 0.018942]
	Learning Rate: 0.0189423
	LOSS [training: 0.2869626451742556 | validation: 0.1810318696214239]
	TIME [epoch: 19.2 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28912528071785304		[learning rate: 0.018935]
	Learning Rate: 0.0189349
	LOSS [training: 0.28912528071785304 | validation: 0.17839221225283566]
	TIME [epoch: 19.2 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2921044370926495		[learning rate: 0.018927]
	Learning Rate: 0.0189274
	LOSS [training: 0.2921044370926495 | validation: 0.19648972194427203]
	TIME [epoch: 19.2 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3111097773810599		[learning rate: 0.01892]
	Learning Rate: 0.01892
	LOSS [training: 0.3111097773810599 | validation: 0.18374459593947923]
	TIME [epoch: 19.2 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29147508972914216		[learning rate: 0.018912]
	Learning Rate: 0.0189125
	LOSS [training: 0.29147508972914216 | validation: 0.16461971596039066]
	TIME [epoch: 19.2 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2801090308476974		[learning rate: 0.018905]
	Learning Rate: 0.018905
	LOSS [training: 0.2801090308476974 | validation: 0.16658439301193137]
	TIME [epoch: 19.1 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2826734238625668		[learning rate: 0.018897]
	Learning Rate: 0.0188974
	LOSS [training: 0.2826734238625668 | validation: 0.1707559890225674]
	TIME [epoch: 19.2 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27947866795036697		[learning rate: 0.01889]
	Learning Rate: 0.0188899
	LOSS [training: 0.27947866795036697 | validation: 0.16852010190839475]
	TIME [epoch: 19.2 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2908214973823816		[learning rate: 0.018882]
	Learning Rate: 0.0188823
	LOSS [training: 0.2908214973823816 | validation: 0.16701507252767317]
	TIME [epoch: 19.2 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28181055764127483		[learning rate: 0.018875]
	Learning Rate: 0.0188747
	LOSS [training: 0.28181055764127483 | validation: 0.19044849818427914]
	TIME [epoch: 19.2 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3323051150663347		[learning rate: 0.018867]
	Learning Rate: 0.0188671
	LOSS [training: 0.3323051150663347 | validation: 0.1646371071127806]
	TIME [epoch: 19.1 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2835328493282938		[learning rate: 0.018859]
	Learning Rate: 0.0188594
	LOSS [training: 0.2835328493282938 | validation: 0.1842735307421695]
	TIME [epoch: 19.2 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2835009418905605		[learning rate: 0.018852]
	Learning Rate: 0.0188517
	LOSS [training: 0.2835009418905605 | validation: 0.16473604098946476]
	TIME [epoch: 19.2 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28747873515574085		[learning rate: 0.018844]
	Learning Rate: 0.018844
	LOSS [training: 0.28747873515574085 | validation: 0.17996363403511367]
	TIME [epoch: 19.2 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2845503315409691		[learning rate: 0.018836]
	Learning Rate: 0.0188363
	LOSS [training: 0.2845503315409691 | validation: 0.16894440189454818]
	TIME [epoch: 19.2 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27773309528564943		[learning rate: 0.018829]
	Learning Rate: 0.0188286
	LOSS [training: 0.27773309528564943 | validation: 0.1723013398311808]
	TIME [epoch: 19.1 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28710300926945226		[learning rate: 0.018821]
	Learning Rate: 0.0188208
	LOSS [training: 0.28710300926945226 | validation: 0.16689627591843312]
	TIME [epoch: 19.2 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.290578936326806		[learning rate: 0.018813]
	Learning Rate: 0.018813
	LOSS [training: 0.290578936326806 | validation: 0.20142910492672775]
	TIME [epoch: 19.1 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2978244193055868		[learning rate: 0.018805]
	Learning Rate: 0.0188052
	LOSS [training: 0.2978244193055868 | validation: 0.16176651041932755]
	TIME [epoch: 19.1 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_349.pth
	Model improved!!!
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27812763758812487		[learning rate: 0.018797]
	Learning Rate: 0.0187973
	LOSS [training: 0.27812763758812487 | validation: 0.16871849808027473]
	TIME [epoch: 19.2 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2915673764147903		[learning rate: 0.018789]
	Learning Rate: 0.0187894
	LOSS [training: 0.2915673764147903 | validation: 0.18544551780512358]
	TIME [epoch: 19.1 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32609116615975436		[learning rate: 0.018782]
	Learning Rate: 0.0187815
	LOSS [training: 0.32609116615975436 | validation: 0.23946080929593883]
	TIME [epoch: 19.2 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2967492573277098		[learning rate: 0.018774]
	Learning Rate: 0.0187736
	LOSS [training: 0.2967492573277098 | validation: 0.1708373066601615]
	TIME [epoch: 19.2 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2850725999094614		[learning rate: 0.018766]
	Learning Rate: 0.0187657
	LOSS [training: 0.2850725999094614 | validation: 0.17150879638821825]
	TIME [epoch: 19.1 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28313917087661633		[learning rate: 0.018758]
	Learning Rate: 0.0187577
	LOSS [training: 0.28313917087661633 | validation: 0.16556989483838086]
	TIME [epoch: 19.2 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27857949938359455		[learning rate: 0.01875]
	Learning Rate: 0.0187497
	LOSS [training: 0.27857949938359455 | validation: 0.16816168653208097]
	TIME [epoch: 19.1 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2842978908042067		[learning rate: 0.018742]
	Learning Rate: 0.0187417
	LOSS [training: 0.2842978908042067 | validation: 0.1675796372776967]
	TIME [epoch: 19.2 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28405376864897897		[learning rate: 0.018734]
	Learning Rate: 0.0187337
	LOSS [training: 0.28405376864897897 | validation: 0.17126523827997128]
	TIME [epoch: 19.1 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29184727911255137		[learning rate: 0.018726]
	Learning Rate: 0.0187256
	LOSS [training: 0.29184727911255137 | validation: 0.16649148216650578]
	TIME [epoch: 19.2 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.280468456778636		[learning rate: 0.018718]
	Learning Rate: 0.0187175
	LOSS [training: 0.280468456778636 | validation: 0.169968564136406]
	TIME [epoch: 19.2 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28698880420599066		[learning rate: 0.018709]
	Learning Rate: 0.0187094
	LOSS [training: 0.28698880420599066 | validation: 0.16979135300459358]
	TIME [epoch: 19.1 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29266278826491143		[learning rate: 0.018701]
	Learning Rate: 0.0187013
	LOSS [training: 0.29266278826491143 | validation: 0.20851608821459647]
	TIME [epoch: 19.2 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3001619043283671		[learning rate: 0.018693]
	Learning Rate: 0.0186931
	LOSS [training: 0.3001619043283671 | validation: 0.16188052512609574]
	TIME [epoch: 19.1 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2875459099926715		[learning rate: 0.018685]
	Learning Rate: 0.0186849
	LOSS [training: 0.2875459099926715 | validation: 0.17124238630912758]
	TIME [epoch: 19.1 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28104176721482843		[learning rate: 0.018677]
	Learning Rate: 0.0186767
	LOSS [training: 0.28104176721482843 | validation: 0.16516876926935387]
	TIME [epoch: 19.2 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2876684447089246		[learning rate: 0.018668]
	Learning Rate: 0.0186685
	LOSS [training: 0.2876684447089246 | validation: 0.17327616597577417]
	TIME [epoch: 19.2 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28831551349376583		[learning rate: 0.01866]
	Learning Rate: 0.0186602
	LOSS [training: 0.28831551349376583 | validation: 0.1685286857692944]
	TIME [epoch: 19.2 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2800873173657952		[learning rate: 0.018652]
	Learning Rate: 0.018652
	LOSS [training: 0.2800873173657952 | validation: 0.1639315645027276]
	TIME [epoch: 19.1 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27641601786901454		[learning rate: 0.018644]
	Learning Rate: 0.0186437
	LOSS [training: 0.27641601786901454 | validation: 0.1692244853473905]
	TIME [epoch: 19.1 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.299309492539378		[learning rate: 0.018635]
	Learning Rate: 0.0186353
	LOSS [training: 0.299309492539378 | validation: 0.20733499951345621]
	TIME [epoch: 19.2 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3053662805584471		[learning rate: 0.018627]
	Learning Rate: 0.018627
	LOSS [training: 0.3053662805584471 | validation: 0.19016153979830672]
	TIME [epoch: 19.1 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28735530053643804		[learning rate: 0.018619]
	Learning Rate: 0.0186186
	LOSS [training: 0.28735530053643804 | validation: 0.17169478229902613]
	TIME [epoch: 19.2 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2803245507916345		[learning rate: 0.01861]
	Learning Rate: 0.0186102
	LOSS [training: 0.2803245507916345 | validation: 0.1717884164704607]
	TIME [epoch: 19.1 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2820151803820382		[learning rate: 0.018602]
	Learning Rate: 0.0186018
	LOSS [training: 0.2820151803820382 | validation: 0.16920026739889424]
	TIME [epoch: 19.1 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2850818026997904		[learning rate: 0.018593]
	Learning Rate: 0.0185934
	LOSS [training: 0.2850818026997904 | validation: 0.16126202592331818]
	TIME [epoch: 19.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_375.pth
	Model improved!!!
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2764083401396749		[learning rate: 0.018585]
	Learning Rate: 0.0185849
	LOSS [training: 0.2764083401396749 | validation: 0.16573888365440426]
	TIME [epoch: 19.2 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2929003386738094		[learning rate: 0.018576]
	Learning Rate: 0.0185764
	LOSS [training: 0.2929003386738094 | validation: 0.19875875611294613]
	TIME [epoch: 19.2 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3313981793887201		[learning rate: 0.018568]
	Learning Rate: 0.0185679
	LOSS [training: 0.3313981793887201 | validation: 0.1648193570181003]
	TIME [epoch: 19.2 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28231116184244026		[learning rate: 0.018559]
	Learning Rate: 0.0185594
	LOSS [training: 0.28231116184244026 | validation: 0.17215667107208438]
	TIME [epoch: 19.2 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2822420065648242		[learning rate: 0.018551]
	Learning Rate: 0.0185508
	LOSS [training: 0.2822420065648242 | validation: 0.16496242603475686]
	TIME [epoch: 19.2 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28191460318381784		[learning rate: 0.018542]
	Learning Rate: 0.0185422
	LOSS [training: 0.28191460318381784 | validation: 0.16923581244903257]
	TIME [epoch: 19.2 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2859436891751155		[learning rate: 0.018534]
	Learning Rate: 0.0185336
	LOSS [training: 0.2859436891751155 | validation: 0.17105832143545074]
	TIME [epoch: 19.2 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2841735126371192		[learning rate: 0.018525]
	Learning Rate: 0.018525
	LOSS [training: 0.2841735126371192 | validation: 0.1678910450825096]
	TIME [epoch: 19.2 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2749471245183221		[learning rate: 0.018516]
	Learning Rate: 0.0185163
	LOSS [training: 0.2749471245183221 | validation: 0.16925863739957459]
	TIME [epoch: 19.2 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29050487705938566		[learning rate: 0.018508]
	Learning Rate: 0.0185077
	LOSS [training: 0.29050487705938566 | validation: 0.1745752420600339]
	TIME [epoch: 19.2 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.290719400441878		[learning rate: 0.018499]
	Learning Rate: 0.018499
	LOSS [training: 0.290719400441878 | validation: 0.182335064403297]
	TIME [epoch: 19.2 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.284338485548855		[learning rate: 0.01849]
	Learning Rate: 0.0184902
	LOSS [training: 0.284338485548855 | validation: 0.17353932211803655]
	TIME [epoch: 19.2 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28021675935319446		[learning rate: 0.018481]
	Learning Rate: 0.0184815
	LOSS [training: 0.28021675935319446 | validation: 0.17478052975470215]
	TIME [epoch: 19.2 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3020094284414871		[learning rate: 0.018473]
	Learning Rate: 0.0184727
	LOSS [training: 0.3020094284414871 | validation: 0.16123134704898087]
	TIME [epoch: 19.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_389.pth
	Model improved!!!
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29586683658444074		[learning rate: 0.018464]
	Learning Rate: 0.0184639
	LOSS [training: 0.29586683658444074 | validation: 0.20959850878298372]
	TIME [epoch: 19.2 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3009107529579492		[learning rate: 0.018455]
	Learning Rate: 0.0184551
	LOSS [training: 0.3009107529579492 | validation: 0.1768125812337088]
	TIME [epoch: 19.1 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28150769148785937		[learning rate: 0.018446]
	Learning Rate: 0.0184463
	LOSS [training: 0.28150769148785937 | validation: 0.17375217562881345]
	TIME [epoch: 19.2 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28542205008017174		[learning rate: 0.018437]
	Learning Rate: 0.0184374
	LOSS [training: 0.28542205008017174 | validation: 0.17838131261840298]
	TIME [epoch: 19.1 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2829057155567032		[learning rate: 0.018429]
	Learning Rate: 0.0184285
	LOSS [training: 0.2829057155567032 | validation: 0.16433049280170806]
	TIME [epoch: 19.1 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2777247792991298		[learning rate: 0.01842]
	Learning Rate: 0.0184196
	LOSS [training: 0.2777247792991298 | validation: 0.16382376338464452]
	TIME [epoch: 19.2 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28602851272048896		[learning rate: 0.018411]
	Learning Rate: 0.0184107
	LOSS [training: 0.28602851272048896 | validation: 0.17291336368494575]
	TIME [epoch: 19.1 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2823173674312232		[learning rate: 0.018402]
	Learning Rate: 0.0184017
	LOSS [training: 0.2823173674312232 | validation: 0.16075512082593613]
	TIME [epoch: 19.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_397.pth
	Model improved!!!
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2830918381388342		[learning rate: 0.018393]
	Learning Rate: 0.0183928
	LOSS [training: 0.2830918381388342 | validation: 0.20735203581295716]
	TIME [epoch: 19.1 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3028155035436612		[learning rate: 0.018384]
	Learning Rate: 0.0183838
	LOSS [training: 0.3028155035436612 | validation: 0.19682442238740028]
	TIME [epoch: 19.1 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28701671873471296		[learning rate: 0.018375]
	Learning Rate: 0.0183747
	LOSS [training: 0.28701671873471296 | validation: 0.16633521389060596]
	TIME [epoch: 19.2 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28154421795369783		[learning rate: 0.018366]
	Learning Rate: 0.0183657
	LOSS [training: 0.28154421795369783 | validation: 0.18242709909671856]
	TIME [epoch: 19.1 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28292630403968255		[learning rate: 0.018357]
	Learning Rate: 0.0183566
	LOSS [training: 0.28292630403968255 | validation: 0.1651025017059184]
	TIME [epoch: 19.2 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27881107151680246		[learning rate: 0.018348]
	Learning Rate: 0.0183475
	LOSS [training: 0.27881107151680246 | validation: 0.16671037587636517]
	TIME [epoch: 19.1 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28660859208634143		[learning rate: 0.018338]
	Learning Rate: 0.0183384
	LOSS [training: 0.28660859208634143 | validation: 0.1800601371350975]
	TIME [epoch: 19.2 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29504476200643187		[learning rate: 0.018329]
	Learning Rate: 0.0183293
	LOSS [training: 0.29504476200643187 | validation: 0.1997261425984419]
	TIME [epoch: 19.2 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29118004019843047		[learning rate: 0.01832]
	Learning Rate: 0.0183201
	LOSS [training: 0.29118004019843047 | validation: 0.16620877758824604]
	TIME [epoch: 19.1 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2920570616248488		[learning rate: 0.018311]
	Learning Rate: 0.0183109
	LOSS [training: 0.2920570616248488 | validation: 0.17072123291125457]
	TIME [epoch: 19.2 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.285094161185446		[learning rate: 0.018302]
	Learning Rate: 0.0183017
	LOSS [training: 0.285094161185446 | validation: 0.16346254665345453]
	TIME [epoch: 19.1 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28047900481842475		[learning rate: 0.018293]
	Learning Rate: 0.0182925
	LOSS [training: 0.28047900481842475 | validation: 0.16071323073936494]
	TIME [epoch: 19.1 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_409.pth
	Model improved!!!
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2837973154124298		[learning rate: 0.018283]
	Learning Rate: 0.0182833
	LOSS [training: 0.2837973154124298 | validation: 0.17808763679805237]
	TIME [epoch: 19.2 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30684177650418426		[learning rate: 0.018274]
	Learning Rate: 0.018274
	LOSS [training: 0.30684177650418426 | validation: 0.16663333389900106]
	TIME [epoch: 19.1 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30633848872962094		[learning rate: 0.018265]
	Learning Rate: 0.0182647
	LOSS [training: 0.30633848872962094 | validation: 0.162421010580948]
	TIME [epoch: 19.2 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2734836419927981		[learning rate: 0.018255]
	Learning Rate: 0.0182554
	LOSS [training: 0.2734836419927981 | validation: 0.1704466131825344]
	TIME [epoch: 19.1 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27868196003052614		[learning rate: 0.018246]
	Learning Rate: 0.018246
	LOSS [training: 0.27868196003052614 | validation: 0.1727906705333366]
	TIME [epoch: 19.2 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2781501536049175		[learning rate: 0.018237]
	Learning Rate: 0.0182367
	LOSS [training: 0.2781501536049175 | validation: 0.16747237034713228]
	TIME [epoch: 19.2 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2870889317062268		[learning rate: 0.018227]
	Learning Rate: 0.0182273
	LOSS [training: 0.2870889317062268 | validation: 0.1866415690738698]
	TIME [epoch: 19.1 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28981919001726336		[learning rate: 0.018218]
	Learning Rate: 0.0182179
	LOSS [training: 0.28981919001726336 | validation: 0.1844716499905132]
	TIME [epoch: 19.2 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28051428590955974		[learning rate: 0.018208]
	Learning Rate: 0.0182085
	LOSS [training: 0.28051428590955974 | validation: 0.16768316507654663]
	TIME [epoch: 19.1 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2844137754512152		[learning rate: 0.018199]
	Learning Rate: 0.018199
	LOSS [training: 0.2844137754512152 | validation: 0.16639624751455348]
	TIME [epoch: 19.2 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2836255064340238		[learning rate: 0.01819]
	Learning Rate: 0.0181895
	LOSS [training: 0.2836255064340238 | validation: 0.16913781774418413]
	TIME [epoch: 19.2 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28786679047828634		[learning rate: 0.01818]
	Learning Rate: 0.01818
	LOSS [training: 0.28786679047828634 | validation: 0.16670082623527993]
	TIME [epoch: 19.1 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2862079408519218		[learning rate: 0.018171]
	Learning Rate: 0.0181705
	LOSS [training: 0.2862079408519218 | validation: 0.16588671955645343]
	TIME [epoch: 19.2 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29103250106069267		[learning rate: 0.018161]
	Learning Rate: 0.018161
	LOSS [training: 0.29103250106069267 | validation: 0.24875442521027533]
	TIME [epoch: 19.1 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3142091815901592		[learning rate: 0.018151]
	Learning Rate: 0.0181514
	LOSS [training: 0.3142091815901592 | validation: 0.17413976757267985]
	TIME [epoch: 19.2 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28250919521146767		[learning rate: 0.018142]
	Learning Rate: 0.0181418
	LOSS [training: 0.28250919521146767 | validation: 0.17205900789350645]
	TIME [epoch: 19.2 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2753042172654588		[learning rate: 0.018132]
	Learning Rate: 0.0181322
	LOSS [training: 0.2753042172654588 | validation: 0.16654335166435075]
	TIME [epoch: 19.1 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27812293559042084		[learning rate: 0.018123]
	Learning Rate: 0.0181226
	LOSS [training: 0.27812293559042084 | validation: 0.17038695052078198]
	TIME [epoch: 19.2 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2774164741219922		[learning rate: 0.018113]
	Learning Rate: 0.0181129
	LOSS [training: 0.2774164741219922 | validation: 0.1810459403559615]
	TIME [epoch: 19.1 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28968826830654637		[learning rate: 0.018103]
	Learning Rate: 0.0181032
	LOSS [training: 0.28968826830654637 | validation: 0.18844700506701328]
	TIME [epoch: 19.2 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30290091645967304		[learning rate: 0.018094]
	Learning Rate: 0.0180936
	LOSS [training: 0.30290091645967304 | validation: 0.17906705886339655]
	TIME [epoch: 19.2 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28898805627128843		[learning rate: 0.018084]
	Learning Rate: 0.0180838
	LOSS [training: 0.28898805627128843 | validation: 0.17209227522053894]
	TIME [epoch: 19.1 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28202682431289133		[learning rate: 0.018074]
	Learning Rate: 0.0180741
	LOSS [training: 0.28202682431289133 | validation: 0.15650851563720167]
	TIME [epoch: 19.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_432.pth
	Model improved!!!
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.276057156797407		[learning rate: 0.018064]
	Learning Rate: 0.0180643
	LOSS [training: 0.276057156797407 | validation: 0.17284135062947148]
	TIME [epoch: 19.1 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2854900269286902		[learning rate: 0.018055]
	Learning Rate: 0.0180545
	LOSS [training: 0.2854900269286902 | validation: 0.16523036887127018]
	TIME [epoch: 19.2 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27630263249683606		[learning rate: 0.018045]
	Learning Rate: 0.0180447
	LOSS [training: 0.27630263249683606 | validation: 0.19705889673134497]
	TIME [epoch: 19.1 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3059971712831516		[learning rate: 0.018035]
	Learning Rate: 0.0180349
	LOSS [training: 0.3059971712831516 | validation: 0.1754107631682154]
	TIME [epoch: 19.1 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27666739135142177		[learning rate: 0.018025]
	Learning Rate: 0.0180251
	LOSS [training: 0.27666739135142177 | validation: 0.17411670572610077]
	TIME [epoch: 19.2 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2818574803950843		[learning rate: 0.018015]
	Learning Rate: 0.0180152
	LOSS [training: 0.2818574803950843 | validation: 0.16728210942268512]
	TIME [epoch: 19.1 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28098577431026495		[learning rate: 0.018005]
	Learning Rate: 0.0180053
	LOSS [training: 0.28098577431026495 | validation: 0.16262135292445423]
	TIME [epoch: 19.2 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.278842604453996		[learning rate: 0.017995]
	Learning Rate: 0.0179954
	LOSS [training: 0.278842604453996 | validation: 0.1685760097494271]
	TIME [epoch: 19.2 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2877691638977252		[learning rate: 0.017985]
	Learning Rate: 0.0179854
	LOSS [training: 0.2877691638977252 | validation: 0.19644026995057545]
	TIME [epoch: 19.1 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2985575501254529		[learning rate: 0.017975]
	Learning Rate: 0.0179755
	LOSS [training: 0.2985575501254529 | validation: 0.16587003821930127]
	TIME [epoch: 19.2 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27608007767752507		[learning rate: 0.017965]
	Learning Rate: 0.0179655
	LOSS [training: 0.27608007767752507 | validation: 0.17956359302749697]
	TIME [epoch: 19.1 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28502688080073313		[learning rate: 0.017955]
	Learning Rate: 0.0179555
	LOSS [training: 0.28502688080073313 | validation: 0.16691527271200196]
	TIME [epoch: 19.2 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2819298939135409		[learning rate: 0.017945]
	Learning Rate: 0.0179455
	LOSS [training: 0.2819298939135409 | validation: 0.17531286125436185]
	TIME [epoch: 19.2 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31097992143546255		[learning rate: 0.017935]
	Learning Rate: 0.0179354
	LOSS [training: 0.31097992143546255 | validation: 0.18173718888472185]
	TIME [epoch: 19.1 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28705938718692053		[learning rate: 0.017925]
	Learning Rate: 0.0179253
	LOSS [training: 0.28705938718692053 | validation: 0.15879043442013435]
	TIME [epoch: 19.2 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2750271774772272		[learning rate: 0.017915]
	Learning Rate: 0.0179152
	LOSS [training: 0.2750271774772272 | validation: 0.17132420749804633]
	TIME [epoch: 19.1 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28463902279258974		[learning rate: 0.017905]
	Learning Rate: 0.0179051
	LOSS [training: 0.28463902279258974 | validation: 0.16830065242740122]
	TIME [epoch: 19.2 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2804693026158085		[learning rate: 0.017895]
	Learning Rate: 0.017895
	LOSS [training: 0.2804693026158085 | validation: 0.15822259572057717]
	TIME [epoch: 19.1 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27956064372943956		[learning rate: 0.017885]
	Learning Rate: 0.0178848
	LOSS [training: 0.27956064372943956 | validation: 0.16644375443316797]
	TIME [epoch: 19.1 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28229797908330834		[learning rate: 0.017875]
	Learning Rate: 0.0178747
	LOSS [training: 0.28229797908330834 | validation: 0.17207578271582233]
	TIME [epoch: 19.2 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28322200510899587		[learning rate: 0.017864]
	Learning Rate: 0.0178645
	LOSS [training: 0.28322200510899587 | validation: 0.16484482414104457]
	TIME [epoch: 19.1 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27966526273856207		[learning rate: 0.017854]
	Learning Rate: 0.0178542
	LOSS [training: 0.27966526273856207 | validation: 0.16471376385185027]
	TIME [epoch: 19.2 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.281845928368044		[learning rate: 0.017844]
	Learning Rate: 0.017844
	LOSS [training: 0.281845928368044 | validation: 0.18666151334626324]
	TIME [epoch: 19.1 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3050168803708537		[learning rate: 0.017834]
	Learning Rate: 0.0178337
	LOSS [training: 0.3050168803708537 | validation: 0.16954582502476506]
	TIME [epoch: 19.1 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27941380224506746		[learning rate: 0.017823]
	Learning Rate: 0.0178235
	LOSS [training: 0.27941380224506746 | validation: 0.16526069411314137]
	TIME [epoch: 19.2 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2773915328123443		[learning rate: 0.017813]
	Learning Rate: 0.0178131
	LOSS [training: 0.2773915328123443 | validation: 0.1598390648531769]
	TIME [epoch: 19.1 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2794784206519699		[learning rate: 0.017803]
	Learning Rate: 0.0178028
	LOSS [training: 0.2794784206519699 | validation: 0.19228830090847798]
	TIME [epoch: 19.2 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29690157622630153		[learning rate: 0.017792]
	Learning Rate: 0.0177925
	LOSS [training: 0.29690157622630153 | validation: 0.19078854734706363]
	TIME [epoch: 19.1 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28654246798027		[learning rate: 0.017782]
	Learning Rate: 0.0177821
	LOSS [training: 0.28654246798027 | validation: 0.16334515224295054]
	TIME [epoch: 19.1 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2829912373763256		[learning rate: 0.017772]
	Learning Rate: 0.0177717
	LOSS [training: 0.2829912373763256 | validation: 0.17077987683832024]
	TIME [epoch: 19.2 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2774478359851017		[learning rate: 0.017761]
	Learning Rate: 0.0177613
	LOSS [training: 0.2774478359851017 | validation: 0.17539123410706162]
	TIME [epoch: 19.1 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2904931787149332		[learning rate: 0.017751]
	Learning Rate: 0.0177509
	LOSS [training: 0.2904931787149332 | validation: 0.18341689627616575]
	TIME [epoch: 19.2 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28198313512350154		[learning rate: 0.01774]
	Learning Rate: 0.0177404
	LOSS [training: 0.28198313512350154 | validation: 0.17205143018402433]
	TIME [epoch: 19.1 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2769383893102248		[learning rate: 0.01773]
	Learning Rate: 0.0177299
	LOSS [training: 0.2769383893102248 | validation: 0.16804417753074463]
	TIME [epoch: 19.1 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28462144986139987		[learning rate: 0.017719]
	Learning Rate: 0.0177194
	LOSS [training: 0.28462144986139987 | validation: 0.1868158852123954]
	TIME [epoch: 19.2 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31446904239558837		[learning rate: 0.017709]
	Learning Rate: 0.0177089
	LOSS [training: 0.31446904239558837 | validation: 0.17682722216598762]
	TIME [epoch: 19.1 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27871105809058766		[learning rate: 0.017698]
	Learning Rate: 0.0176984
	LOSS [training: 0.27871105809058766 | validation: 0.1670275502457526]
	TIME [epoch: 19.2 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2786671722505295		[learning rate: 0.017688]
	Learning Rate: 0.0176878
	LOSS [training: 0.2786671722505295 | validation: 0.17191289887926106]
	TIME [epoch: 19.1 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28255099098227593		[learning rate: 0.017677]
	Learning Rate: 0.0176772
	LOSS [training: 0.28255099098227593 | validation: 0.16567725462315966]
	TIME [epoch: 19.1 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2823211888830057		[learning rate: 0.017667]
	Learning Rate: 0.0176666
	LOSS [training: 0.2823211888830057 | validation: 0.18134950967868108]
	TIME [epoch: 19.2 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2812235079458997		[learning rate: 0.017656]
	Learning Rate: 0.017656
	LOSS [training: 0.2812235079458997 | validation: 0.15868078933359167]
	TIME [epoch: 19.1 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.268776131059835		[learning rate: 0.017645]
	Learning Rate: 0.0176454
	LOSS [training: 0.268776131059835 | validation: 0.16340421097671126]
	TIME [epoch: 19.2 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2803093630412539		[learning rate: 0.017635]
	Learning Rate: 0.0176347
	LOSS [training: 0.2803093630412539 | validation: 0.17696903800251418]
	TIME [epoch: 19.1 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2987085466641941		[learning rate: 0.017624]
	Learning Rate: 0.017624
	LOSS [training: 0.2987085466641941 | validation: 0.20090659763255905]
	TIME [epoch: 19.1 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2958703993452851		[learning rate: 0.017613]
	Learning Rate: 0.0176133
	LOSS [training: 0.2958703993452851 | validation: 0.16315827651657486]
	TIME [epoch: 19.2 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28197503805792773		[learning rate: 0.017603]
	Learning Rate: 0.0176026
	LOSS [training: 0.28197503805792773 | validation: 0.16459179691621206]
	TIME [epoch: 19.1 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27876400689674385		[learning rate: 0.017592]
	Learning Rate: 0.0175918
	LOSS [training: 0.27876400689674385 | validation: 0.16353217493960098]
	TIME [epoch: 19.2 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2855631999785787		[learning rate: 0.017581]
	Learning Rate: 0.0175811
	LOSS [training: 0.2855631999785787 | validation: 0.19961490845540683]
	TIME [epoch: 19.1 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.296424776851559		[learning rate: 0.01757]
	Learning Rate: 0.0175703
	LOSS [training: 0.296424776851559 | validation: 0.17310092567765534]
	TIME [epoch: 19.1 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2814347925130519		[learning rate: 0.017559]
	Learning Rate: 0.0175595
	LOSS [training: 0.2814347925130519 | validation: 0.15901811448626468]
	TIME [epoch: 19.2 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27629985472827523		[learning rate: 0.017549]
	Learning Rate: 0.0175486
	LOSS [training: 0.27629985472827523 | validation: 0.15764981967136443]
	TIME [epoch: 19.1 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2875370056076677		[learning rate: 0.017538]
	Learning Rate: 0.0175378
	LOSS [training: 0.2875370056076677 | validation: 0.17633996222934353]
	TIME [epoch: 19.2 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2896322133627759		[learning rate: 0.017527]
	Learning Rate: 0.0175269
	LOSS [training: 0.2896322133627759 | validation: 0.17019628452129348]
	TIME [epoch: 19.1 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2784761865468113		[learning rate: 0.017516]
	Learning Rate: 0.017516
	LOSS [training: 0.2784761865468113 | validation: 0.1672879046474426]
	TIME [epoch: 19.1 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2761855061659037		[learning rate: 0.017505]
	Learning Rate: 0.0175051
	LOSS [training: 0.2761855061659037 | validation: 0.18957862913369236]
	TIME [epoch: 19.2 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.309462640632643		[learning rate: 0.017494]
	Learning Rate: 0.0174942
	LOSS [training: 0.309462640632643 | validation: 0.1719668898123922]
	TIME [epoch: 19.1 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2736555760935854		[learning rate: 0.017483]
	Learning Rate: 0.0174832
	LOSS [training: 0.2736555760935854 | validation: 0.1698807454526186]
	TIME [epoch: 19.2 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28550481593241817		[learning rate: 0.017472]
	Learning Rate: 0.0174722
	LOSS [training: 0.28550481593241817 | validation: 0.1603479212703467]
	TIME [epoch: 19.1 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2927274530470898		[learning rate: 0.017461]
	Learning Rate: 0.0174612
	LOSS [training: 0.2927274530470898 | validation: 0.20227641874348473]
	TIME [epoch: 19.1 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2814091030298147		[learning rate: 0.01745]
	Learning Rate: 0.0174502
	LOSS [training: 0.2814091030298147 | validation: 0.1637343671894826]
	TIME [epoch: 19.2 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27456096672879376		[learning rate: 0.017439]
	Learning Rate: 0.0174392
	LOSS [training: 0.27456096672879376 | validation: 0.15789429818634265]
	TIME [epoch: 19.1 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26906879378252024		[learning rate: 0.017428]
	Learning Rate: 0.0174281
	LOSS [training: 0.26906879378252024 | validation: 0.17224202379618725]
	TIME [epoch: 19.2 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28092902875947123		[learning rate: 0.017417]
	Learning Rate: 0.017417
	LOSS [training: 0.28092902875947123 | validation: 0.1726165498654954]
	TIME [epoch: 19.1 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2628491828743476		[learning rate: 0.017406]
	Learning Rate: 0.0174059
	LOSS [training: 0.2628491828743476 | validation: 0.1742436292648368]
	TIME [epoch: 19.1 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3133924720104975		[learning rate: 0.017395]
	Learning Rate: 0.0173948
	LOSS [training: 0.3133924720104975 | validation: 0.1925811931614113]
	TIME [epoch: 19.2 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1947844903217496		[learning rate: 0.017384]
	Learning Rate: 0.0173837
	LOSS [training: 0.1947844903217496 | validation: 0.44209900100398225]
	TIME [epoch: 19.1 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3668305639293508		[learning rate: 0.017373]
	Learning Rate: 0.0173725
	LOSS [training: 0.3668305639293508 | validation: 0.2133647699349926]
	TIME [epoch: 19.2 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24965084348894606		[learning rate: 0.017361]
	Learning Rate: 0.0173614
	LOSS [training: 0.24965084348894606 | validation: 0.4250799855799475]
	TIME [epoch: 19.2 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29978106936668725		[learning rate: 0.01735]
	Learning Rate: 0.0173502
	LOSS [training: 0.29978106936668725 | validation: 0.3848341665838153]
	TIME [epoch: 136 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3116382855007177		[learning rate: 0.017339]
	Learning Rate: 0.0173389
	LOSS [training: 0.3116382855007177 | validation: 0.16600314157472237]
	TIME [epoch: 41.3 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2778576293632722		[learning rate: 0.017328]
	Learning Rate: 0.0173277
	LOSS [training: 0.2778576293632722 | validation: 0.1595099382179139]
	TIME [epoch: 41.2 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.286467606851318		[learning rate: 0.017316]
	Learning Rate: 0.0173164
	LOSS [training: 0.286467606851318 | validation: 0.13953453789945178]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_504.pth
	Model improved!!!
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2474951697123497		[learning rate: 0.017305]
	Learning Rate: 0.0173052
	LOSS [training: 0.2474951697123497 | validation: 0.387501258797116]
	TIME [epoch: 41.2 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2684899414640651		[learning rate: 0.017294]
	Learning Rate: 0.0172939
	LOSS [training: 0.2684899414640651 | validation: 0.180027531357769]
	TIME [epoch: 41.2 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26360144262494173		[learning rate: 0.017283]
	Learning Rate: 0.0172826
	LOSS [training: 0.26360144262494173 | validation: 0.15685084944355904]
	TIME [epoch: 41.2 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24185289561903234		[learning rate: 0.017271]
	Learning Rate: 0.0172712
	LOSS [training: 0.24185289561903234 | validation: 0.2646427187172734]
	TIME [epoch: 41.2 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2498129636524879		[learning rate: 0.01726]
	Learning Rate: 0.0172599
	LOSS [training: 0.2498129636524879 | validation: 0.35782165921944026]
	TIME [epoch: 41.2 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23757947944840488		[learning rate: 0.017248]
	Learning Rate: 0.0172485
	LOSS [training: 0.23757947944840488 | validation: 0.19830382250094558]
	TIME [epoch: 41.2 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27017526356882654		[learning rate: 0.017237]
	Learning Rate: 0.0172371
	LOSS [training: 0.27017526356882654 | validation: 0.2565965767005275]
	TIME [epoch: 41.2 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2580049264998908		[learning rate: 0.017226]
	Learning Rate: 0.0172257
	LOSS [training: 0.2580049264998908 | validation: 0.12972580428357636]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_512.pth
	Model improved!!!
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23885220956321807		[learning rate: 0.017214]
	Learning Rate: 0.0172142
	LOSS [training: 0.23885220956321807 | validation: 0.1381351681815818]
	TIME [epoch: 41.2 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24908703368485763		[learning rate: 0.017203]
	Learning Rate: 0.0172028
	LOSS [training: 0.24908703368485763 | validation: 0.2540294972097899]
	TIME [epoch: 41.2 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3038072018838532		[learning rate: 0.017191]
	Learning Rate: 0.0171913
	LOSS [training: 0.3038072018838532 | validation: 0.15716887156453155]
	TIME [epoch: 41.2 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26928424017557034		[learning rate: 0.01718]
	Learning Rate: 0.0171798
	LOSS [training: 0.26928424017557034 | validation: 0.15771271537627712]
	TIME [epoch: 41.2 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2577377740853012		[learning rate: 0.017168]
	Learning Rate: 0.0171683
	LOSS [training: 0.2577377740853012 | validation: 0.16526794185952845]
	TIME [epoch: 41.2 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25861149873223244		[learning rate: 0.017157]
	Learning Rate: 0.0171567
	LOSS [training: 0.25861149873223244 | validation: 0.154727148336821]
	TIME [epoch: 41.2 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24507725445666162		[learning rate: 0.017145]
	Learning Rate: 0.0171452
	LOSS [training: 0.24507725445666162 | validation: 0.15778812705320616]
	TIME [epoch: 41.2 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23202227196427713		[learning rate: 0.017134]
	Learning Rate: 0.0171336
	LOSS [training: 0.23202227196427713 | validation: 0.2120687656502925]
	TIME [epoch: 41.2 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27991539936542154		[learning rate: 0.017122]
	Learning Rate: 0.017122
	LOSS [training: 0.27991539936542154 | validation: 0.16300048256593974]
	TIME [epoch: 41.2 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2695044965471464		[learning rate: 0.01711]
	Learning Rate: 0.0171104
	LOSS [training: 0.2695044965471464 | validation: 0.19604835985732166]
	TIME [epoch: 41.2 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2442464436240872		[learning rate: 0.017099]
	Learning Rate: 0.0170988
	LOSS [training: 0.2442464436240872 | validation: 0.14354466962098106]
	TIME [epoch: 41.2 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2313960981688531		[learning rate: 0.017087]
	Learning Rate: 0.0170871
	LOSS [training: 0.2313960981688531 | validation: 0.18539773500353418]
	TIME [epoch: 41.2 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27567124736457693		[learning rate: 0.017075]
	Learning Rate: 0.0170755
	LOSS [training: 0.27567124736457693 | validation: 0.16424483519699445]
	TIME [epoch: 41.2 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24470762710762117		[learning rate: 0.017064]
	Learning Rate: 0.0170638
	LOSS [training: 0.24470762710762117 | validation: 0.1539836050240185]
	TIME [epoch: 41.2 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20505044917569143		[learning rate: 0.017052]
	Learning Rate: 0.0170521
	LOSS [training: 0.20505044917569143 | validation: 0.15593788962663055]
	TIME [epoch: 41.3 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2311050545179664		[learning rate: 0.01704]
	Learning Rate: 0.0170403
	LOSS [training: 0.2311050545179664 | validation: 0.1619571212666492]
	TIME [epoch: 41.2 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29659261331899756		[learning rate: 0.017029]
	Learning Rate: 0.0170286
	LOSS [training: 0.29659261331899756 | validation: 0.23524528545476214]
	TIME [epoch: 41.2 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27987469428910483		[learning rate: 0.017017]
	Learning Rate: 0.0170168
	LOSS [training: 0.27987469428910483 | validation: 0.15177229139679282]
	TIME [epoch: 41.1 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21072422247004627		[learning rate: 0.017005]
	Learning Rate: 0.017005
	LOSS [training: 0.21072422247004627 | validation: 0.2612851950196557]
	TIME [epoch: 41.2 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25174813845173766		[learning rate: 0.016993]
	Learning Rate: 0.0169932
	LOSS [training: 0.25174813845173766 | validation: 0.15288465955894054]
	TIME [epoch: 41.2 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25797747967881557		[learning rate: 0.016981]
	Learning Rate: 0.0169814
	LOSS [training: 0.25797747967881557 | validation: 0.16917627516875944]
	TIME [epoch: 41.2 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25451393859971133		[learning rate: 0.01697]
	Learning Rate: 0.0169695
	LOSS [training: 0.25451393859971133 | validation: 0.20794460227985606]
	TIME [epoch: 41.2 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22452456238919846		[learning rate: 0.016958]
	Learning Rate: 0.0169577
	LOSS [training: 0.22452456238919846 | validation: 0.14745915610543608]
	TIME [epoch: 41.2 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2584977805546822		[learning rate: 0.016946]
	Learning Rate: 0.0169458
	LOSS [training: 0.2584977805546822 | validation: 0.25790395068065114]
	TIME [epoch: 41.2 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22054696179459088		[learning rate: 0.016934]
	Learning Rate: 0.0169339
	LOSS [training: 0.22054696179459088 | validation: 0.217429199822967]
	TIME [epoch: 41.2 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24577617236797666		[learning rate: 0.016922]
	Learning Rate: 0.016922
	LOSS [training: 0.24577617236797666 | validation: 0.33031531387532903]
	TIME [epoch: 41.2 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2154166379621963		[learning rate: 0.01691]
	Learning Rate: 0.01691
	LOSS [training: 0.2154166379621963 | validation: 0.18163808238301044]
	TIME [epoch: 41.2 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25310728414392025		[learning rate: 0.016898]
	Learning Rate: 0.0168981
	LOSS [training: 0.25310728414392025 | validation: 0.3967997934065832]
	TIME [epoch: 41.1 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2487375442418083		[learning rate: 0.016886]
	Learning Rate: 0.0168861
	LOSS [training: 0.2487375442418083 | validation: 0.16765819536988663]
	TIME [epoch: 41.2 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25389487666473665		[learning rate: 0.016874]
	Learning Rate: 0.0168741
	LOSS [training: 0.25389487666473665 | validation: 0.13258340398367194]
	TIME [epoch: 41.2 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24321290457298975		[learning rate: 0.016862]
	Learning Rate: 0.0168621
	LOSS [training: 0.24321290457298975 | validation: 0.13890955958391804]
	TIME [epoch: 41.2 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23590003639258		[learning rate: 0.01685]
	Learning Rate: 0.0168501
	LOSS [training: 0.23590003639258 | validation: 0.1432369851068379]
	TIME [epoch: 41.2 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21522690249260157		[learning rate: 0.016838]
	Learning Rate: 0.016838
	LOSS [training: 0.21522690249260157 | validation: 0.14585818163512804]
	TIME [epoch: 41.2 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18362858065956417		[learning rate: 0.016826]
	Learning Rate: 0.0168259
	LOSS [training: 0.18362858065956417 | validation: 0.16210326885272974]
	TIME [epoch: 41.2 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2576048017618958		[learning rate: 0.016814]
	Learning Rate: 0.0168138
	LOSS [training: 0.2576048017618958 | validation: 0.39361534543536913]
	TIME [epoch: 41.2 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19195452911079067		[learning rate: 0.016802]
	Learning Rate: 0.0168017
	LOSS [training: 0.19195452911079067 | validation: 0.4165023063631075]
	TIME [epoch: 41.2 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3118342240088309		[learning rate: 0.01679]
	Learning Rate: 0.0167896
	LOSS [training: 0.3118342240088309 | validation: 0.17499164757581578]
	TIME [epoch: 41.2 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24781695490815972		[learning rate: 0.016777]
	Learning Rate: 0.0167775
	LOSS [training: 0.24781695490815972 | validation: 0.2536138607980465]
	TIME [epoch: 41.2 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2926334201667696		[learning rate: 0.016765]
	Learning Rate: 0.0167653
	LOSS [training: 0.2926334201667696 | validation: 0.1508532603206182]
	TIME [epoch: 41.2 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2253680920049459		[learning rate: 0.016753]
	Learning Rate: 0.0167531
	LOSS [training: 0.2253680920049459 | validation: 0.3666043380633329]
	TIME [epoch: 41.2 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2783313973294036		[learning rate: 0.016741]
	Learning Rate: 0.0167409
	LOSS [training: 0.2783313973294036 | validation: 0.1354806957199649]
	TIME [epoch: 41.2 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20734805999131747		[learning rate: 0.016729]
	Learning Rate: 0.0167287
	LOSS [training: 0.20734805999131747 | validation: 0.3616273700531787]
	TIME [epoch: 41.2 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2591397621630504		[learning rate: 0.016716]
	Learning Rate: 0.0167165
	LOSS [training: 0.2591397621630504 | validation: 0.16594434648506468]
	TIME [epoch: 41.2 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2040687840776007		[learning rate: 0.016704]
	Learning Rate: 0.0167042
	LOSS [training: 0.2040687840776007 | validation: 0.3915043871080315]
	TIME [epoch: 41.2 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2818459884462906		[learning rate: 0.016692]
	Learning Rate: 0.0166919
	LOSS [training: 0.2818459884462906 | validation: 0.18349630777635104]
	TIME [epoch: 41.2 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2308432893349732		[learning rate: 0.01668]
	Learning Rate: 0.0166796
	LOSS [training: 0.2308432893349732 | validation: 0.1417532754360279]
	TIME [epoch: 41.2 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21345314455369		[learning rate: 0.016667]
	Learning Rate: 0.0166673
	LOSS [training: 0.21345314455369 | validation: 0.19264065418303108]
	TIME [epoch: 41.2 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22403149683395224		[learning rate: 0.016655]
	Learning Rate: 0.016655
	LOSS [training: 0.22403149683395224 | validation: 0.20629313746491518]
	TIME [epoch: 41.2 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21648855952232293		[learning rate: 0.016643]
	Learning Rate: 0.0166427
	LOSS [training: 0.21648855952232293 | validation: 0.1185024863283168]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_561.pth
	Model improved!!!
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18958374346392423		[learning rate: 0.01663]
	Learning Rate: 0.0166303
	LOSS [training: 0.18958374346392423 | validation: 0.18399067106721306]
	TIME [epoch: 41.2 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2376284371692604		[learning rate: 0.016618]
	Learning Rate: 0.0166179
	LOSS [training: 0.2376284371692604 | validation: 0.1263257445089202]
	TIME [epoch: 41.2 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18904827478022582		[learning rate: 0.016606]
	Learning Rate: 0.0166055
	LOSS [training: 0.18904827478022582 | validation: 0.21808520362563868]
	TIME [epoch: 41.2 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22515734553305308		[learning rate: 0.016593]
	Learning Rate: 0.0165931
	LOSS [training: 0.22515734553305308 | validation: 0.138645133960548]
	TIME [epoch: 41.2 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22466854144487497		[learning rate: 0.016581]
	Learning Rate: 0.0165807
	LOSS [training: 0.22466854144487497 | validation: 0.11476824609146427]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_566.pth
	Model improved!!!
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21508467891817318		[learning rate: 0.016568]
	Learning Rate: 0.0165682
	LOSS [training: 0.21508467891817318 | validation: 0.14796023545730463]
	TIME [epoch: 41.2 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22512041791999618		[learning rate: 0.016556]
	Learning Rate: 0.0165557
	LOSS [training: 0.22512041791999618 | validation: 0.35021759957201787]
	TIME [epoch: 41.2 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18257735822565235		[learning rate: 0.016543]
	Learning Rate: 0.0165432
	LOSS [training: 0.18257735822565235 | validation: 0.1708540702138281]
	TIME [epoch: 41.2 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22800520221316137		[learning rate: 0.016531]
	Learning Rate: 0.0165307
	LOSS [training: 0.22800520221316137 | validation: 0.13488392369510388]
	TIME [epoch: 41.2 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24220474873421838		[learning rate: 0.016518]
	Learning Rate: 0.0165182
	LOSS [training: 0.24220474873421838 | validation: 0.21103578107275944]
	TIME [epoch: 41.2 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2648541046338482		[learning rate: 0.016506]
	Learning Rate: 0.0165057
	LOSS [training: 0.2648541046338482 | validation: 0.12467193866777238]
	TIME [epoch: 41.2 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2416181252652579		[learning rate: 0.016493]
	Learning Rate: 0.0164931
	LOSS [training: 0.2416181252652579 | validation: 0.2021353338972321]
	TIME [epoch: 41.2 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2264714071479123		[learning rate: 0.016481]
	Learning Rate: 0.0164805
	LOSS [training: 0.2264714071479123 | validation: 0.15934546367218216]
	TIME [epoch: 41.2 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23873105654705434		[learning rate: 0.016468]
	Learning Rate: 0.0164679
	LOSS [training: 0.23873105654705434 | validation: 0.12302050650070065]
	TIME [epoch: 41.2 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21645016043109994		[learning rate: 0.016455]
	Learning Rate: 0.0164553
	LOSS [training: 0.21645016043109994 | validation: 0.14638148296558245]
	TIME [epoch: 41.2 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19200415707242635		[learning rate: 0.016443]
	Learning Rate: 0.0164427
	LOSS [training: 0.19200415707242635 | validation: 0.1387039835854498]
	TIME [epoch: 41.2 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21886500530624925		[learning rate: 0.01643]
	Learning Rate: 0.01643
	LOSS [training: 0.21886500530624925 | validation: 0.2488668569299417]
	TIME [epoch: 41.2 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2133963082067771		[learning rate: 0.016417]
	Learning Rate: 0.0164173
	LOSS [training: 0.2133963082067771 | validation: 0.1320418223109001]
	TIME [epoch: 41.2 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18615263096430298		[learning rate: 0.016405]
	Learning Rate: 0.0164047
	LOSS [training: 0.18615263096430298 | validation: 0.13423024357467161]
	TIME [epoch: 41.2 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19161609164916218		[learning rate: 0.016392]
	Learning Rate: 0.016392
	LOSS [training: 0.19161609164916218 | validation: 0.12310347693896224]
	TIME [epoch: 41.2 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1858364522881421		[learning rate: 0.016379]
	Learning Rate: 0.0163792
	LOSS [training: 0.1858364522881421 | validation: 0.1266721798144006]
	TIME [epoch: 41.2 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24096832484933958		[learning rate: 0.016366]
	Learning Rate: 0.0163665
	LOSS [training: 0.24096832484933958 | validation: 0.11669656795803346]
	TIME [epoch: 41.2 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21359128266430905		[learning rate: 0.016354]
	Learning Rate: 0.0163537
	LOSS [training: 0.21359128266430905 | validation: 0.24721444584771712]
	TIME [epoch: 41.2 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1944407810644337		[learning rate: 0.016341]
	Learning Rate: 0.016341
	LOSS [training: 0.1944407810644337 | validation: 0.11909032033253034]
	TIME [epoch: 41.2 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21142909587606384		[learning rate: 0.016328]
	Learning Rate: 0.0163282
	LOSS [training: 0.21142909587606384 | validation: 0.12236579722286389]
	TIME [epoch: 41.2 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1666460747327543		[learning rate: 0.016315]
	Learning Rate: 0.0163154
	LOSS [training: 0.1666460747327543 | validation: 0.34163863596438176]
	TIME [epoch: 41.2 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2302577278791511		[learning rate: 0.016303]
	Learning Rate: 0.0163025
	LOSS [training: 0.2302577278791511 | validation: 0.17072854868779805]
	TIME [epoch: 41.2 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18903248323366142		[learning rate: 0.01629]
	Learning Rate: 0.0162897
	LOSS [training: 0.18903248323366142 | validation: 0.18146769381699795]
	TIME [epoch: 41.2 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2383490882347015		[learning rate: 0.016277]
	Learning Rate: 0.0162768
	LOSS [training: 0.2383490882347015 | validation: 0.12917593802943464]
	TIME [epoch: 41.2 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2141199872125717		[learning rate: 0.016264]
	Learning Rate: 0.016264
	LOSS [training: 0.2141199872125717 | validation: 0.13634481219137165]
	TIME [epoch: 41.2 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19662110737002356		[learning rate: 0.016251]
	Learning Rate: 0.0162511
	LOSS [training: 0.19662110737002356 | validation: 0.16147442888787497]
	TIME [epoch: 41.2 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2039687779073347		[learning rate: 0.016238]
	Learning Rate: 0.0162382
	LOSS [training: 0.2039687779073347 | validation: 0.2111279304563516]
	TIME [epoch: 41.2 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17195084683213763		[learning rate: 0.016225]
	Learning Rate: 0.0162252
	LOSS [training: 0.17195084683213763 | validation: 0.31044050417441527]
	TIME [epoch: 41.2 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22442639149659618		[learning rate: 0.016212]
	Learning Rate: 0.0162123
	LOSS [training: 0.22442639149659618 | validation: 0.12651666276527382]
	TIME [epoch: 41.1 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1898751780514811		[learning rate: 0.016199]
	Learning Rate: 0.0161993
	LOSS [training: 0.1898751780514811 | validation: 0.24493981446108182]
	TIME [epoch: 41.2 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17920796805401162		[learning rate: 0.016186]
	Learning Rate: 0.0161864
	LOSS [training: 0.17920796805401162 | validation: 0.2549736842836071]
	TIME [epoch: 41.2 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19081204780233602		[learning rate: 0.016173]
	Learning Rate: 0.0161734
	LOSS [training: 0.19081204780233602 | validation: 0.13221146056084732]
	TIME [epoch: 41.2 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16479092138621937		[learning rate: 0.01616]
	Learning Rate: 0.0161603
	LOSS [training: 0.16479092138621937 | validation: 0.1821104815174153]
	TIME [epoch: 41.2 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20600934608208035		[learning rate: 0.016147]
	Learning Rate: 0.0161473
	LOSS [training: 0.20600934608208035 | validation: 0.28967279998280515]
	TIME [epoch: 41.1 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17525875107134817		[learning rate: 0.016134]
	Learning Rate: 0.0161343
	LOSS [training: 0.17525875107134817 | validation: 0.15445761577493683]
	TIME [epoch: 41.2 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2019076492450117		[learning rate: 0.016121]
	Learning Rate: 0.0161212
	LOSS [training: 0.2019076492450117 | validation: 0.11289507433858001]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_602.pth
	Model improved!!!
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18039121264110325		[learning rate: 0.016108]
	Learning Rate: 0.0161081
	LOSS [training: 0.18039121264110325 | validation: 0.11372601780909655]
	TIME [epoch: 41.2 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17000380722576494		[learning rate: 0.016095]
	Learning Rate: 0.016095
	LOSS [training: 0.17000380722576494 | validation: 0.1422049881224938]
	TIME [epoch: 41.2 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18271524598631778		[learning rate: 0.016082]
	Learning Rate: 0.0160819
	LOSS [training: 0.18271524598631778 | validation: 0.14749106974355491]
	TIME [epoch: 41.1 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1785110811253181		[learning rate: 0.016069]
	Learning Rate: 0.0160688
	LOSS [training: 0.1785110811253181 | validation: 0.1010076394289685]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_606.pth
	Model improved!!!
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16800094246921915		[learning rate: 0.016056]
	Learning Rate: 0.0160556
	LOSS [training: 0.16800094246921915 | validation: 0.1522686179437232]
	TIME [epoch: 41.2 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18568275445677523		[learning rate: 0.016042]
	Learning Rate: 0.0160425
	LOSS [training: 0.18568275445677523 | validation: 0.21426160992867443]
	TIME [epoch: 41.2 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16248912330368498		[learning rate: 0.016029]
	Learning Rate: 0.0160293
	LOSS [training: 0.16248912330368498 | validation: 0.1132245997353599]
	TIME [epoch: 41.2 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18822352934793396		[learning rate: 0.016016]
	Learning Rate: 0.0160161
	LOSS [training: 0.18822352934793396 | validation: 0.2946425405361921]
	TIME [epoch: 41.2 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21434628560052676		[learning rate: 0.016003]
	Learning Rate: 0.0160029
	LOSS [training: 0.21434628560052676 | validation: 0.12056793644642123]
	TIME [epoch: 41.2 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15870533082035632		[learning rate: 0.01599]
	Learning Rate: 0.0159897
	LOSS [training: 0.15870533082035632 | validation: 0.09626168845846617]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_612.pth
	Model improved!!!
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1911364809168056		[learning rate: 0.015976]
	Learning Rate: 0.0159764
	LOSS [training: 0.1911364809168056 | validation: 0.23670944166588465]
	TIME [epoch: 41.2 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1811068841669211		[learning rate: 0.015963]
	Learning Rate: 0.0159632
	LOSS [training: 0.1811068841669211 | validation: 0.1740482795244525]
	TIME [epoch: 41.2 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16964089768954124		[learning rate: 0.01595]
	Learning Rate: 0.0159499
	LOSS [training: 0.16964089768954124 | validation: 0.20819719261348274]
	TIME [epoch: 41.2 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18812935054182756		[learning rate: 0.015937]
	Learning Rate: 0.0159366
	LOSS [training: 0.18812935054182756 | validation: 0.20012970597392588]
	TIME [epoch: 41.1 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1841482022967134		[learning rate: 0.015923]
	Learning Rate: 0.0159233
	LOSS [training: 0.1841482022967134 | validation: 0.1634032197438931]
	TIME [epoch: 41.2 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17226742738823977		[learning rate: 0.01591]
	Learning Rate: 0.01591
	LOSS [training: 0.17226742738823977 | validation: 0.186115932075474]
	TIME [epoch: 41.2 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20150538767320064		[learning rate: 0.015897]
	Learning Rate: 0.0158966
	LOSS [training: 0.20150538767320064 | validation: 0.11827779604765272]
	TIME [epoch: 41.2 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17617249643067623		[learning rate: 0.015883]
	Learning Rate: 0.0158833
	LOSS [training: 0.17617249643067623 | validation: 0.15865090772901452]
	TIME [epoch: 41.2 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11498995409969047		[learning rate: 0.01587]
	Learning Rate: 0.0158699
	LOSS [training: 0.11498995409969047 | validation: 0.08946503904265832]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_621.pth
	Model improved!!!
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15167516471945766		[learning rate: 0.015856]
	Learning Rate: 0.0158565
	LOSS [training: 0.15167516471945766 | validation: 0.1564008814756814]
	TIME [epoch: 41.2 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1850950371780954		[learning rate: 0.015843]
	Learning Rate: 0.0158431
	LOSS [training: 0.1850950371780954 | validation: 0.1811017078600316]
	TIME [epoch: 41.1 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19958128550488635		[learning rate: 0.01583]
	Learning Rate: 0.0158297
	LOSS [training: 0.19958128550488635 | validation: 0.11675627007453952]
	TIME [epoch: 41.2 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12913668646161724		[learning rate: 0.015816]
	Learning Rate: 0.0158162
	LOSS [training: 0.12913668646161724 | validation: 0.23384783078892876]
	TIME [epoch: 41.2 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15154180862218924		[learning rate: 0.015803]
	Learning Rate: 0.0158028
	LOSS [training: 0.15154180862218924 | validation: 0.19135477770967468]
	TIME [epoch: 41.2 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20592453009077327		[learning rate: 0.015789]
	Learning Rate: 0.0157893
	LOSS [training: 0.20592453009077327 | validation: 0.11684293244106632]
	TIME [epoch: 41.2 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1056599792478668		[learning rate: 0.015776]
	Learning Rate: 0.0157758
	LOSS [training: 0.1056599792478668 | validation: 0.09860259419574767]
	TIME [epoch: 41.2 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17081623862500872		[learning rate: 0.015762]
	Learning Rate: 0.0157623
	LOSS [training: 0.17081623862500872 | validation: 0.1934719160228723]
	TIME [epoch: 41.2 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18460871342731425		[learning rate: 0.015749]
	Learning Rate: 0.0157488
	LOSS [training: 0.18460871342731425 | validation: 0.14572824157833844]
	TIME [epoch: 41.1 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14392220751589657		[learning rate: 0.015735]
	Learning Rate: 0.0157353
	LOSS [training: 0.14392220751589657 | validation: 0.0852333830401254]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_631.pth
	Model improved!!!
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1551342888418456		[learning rate: 0.015722]
	Learning Rate: 0.0157217
	LOSS [training: 0.1551342888418456 | validation: 0.11267337702477526]
	TIME [epoch: 41.2 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16713398859834383		[learning rate: 0.015708]
	Learning Rate: 0.0157082
	LOSS [training: 0.16713398859834383 | validation: 0.17096881175076672]
	TIME [epoch: 41.2 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22567916271906382		[learning rate: 0.015695]
	Learning Rate: 0.0156946
	LOSS [training: 0.22567916271906382 | validation: 0.13981983589066405]
	TIME [epoch: 41.2 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15125731854773666		[learning rate: 0.015681]
	Learning Rate: 0.015681
	LOSS [training: 0.15125731854773666 | validation: 0.18474172528490285]
	TIME [epoch: 41.1 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14467592359469164		[learning rate: 0.015667]
	Learning Rate: 0.0156674
	LOSS [training: 0.14467592359469164 | validation: 0.11975031078847256]
	TIME [epoch: 41.1 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18233809480886898		[learning rate: 0.015654]
	Learning Rate: 0.0156537
	LOSS [training: 0.18233809480886898 | validation: 0.1335108698660036]
	TIME [epoch: 41.2 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15593363295973792		[learning rate: 0.01564]
	Learning Rate: 0.0156401
	LOSS [training: 0.15593363295973792 | validation: 0.09717358830707884]
	TIME [epoch: 41.2 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1650324377714355		[learning rate: 0.015626]
	Learning Rate: 0.0156264
	LOSS [training: 0.1650324377714355 | validation: 0.09785344353574468]
	TIME [epoch: 41.2 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15114810500665465		[learning rate: 0.015613]
	Learning Rate: 0.0156128
	LOSS [training: 0.15114810500665465 | validation: 0.09234250339161545]
	TIME [epoch: 41.2 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14911225805921058		[learning rate: 0.015599]
	Learning Rate: 0.0155991
	LOSS [training: 0.14911225805921058 | validation: 0.14473410813010076]
	TIME [epoch: 41.2 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17586528542825078		[learning rate: 0.015585]
	Learning Rate: 0.0155854
	LOSS [training: 0.17586528542825078 | validation: 0.14974324853635568]
	TIME [epoch: 41.2 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15342832715956817		[learning rate: 0.015572]
	Learning Rate: 0.0155717
	LOSS [training: 0.15342832715956817 | validation: 0.08414982735176306]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_643.pth
	Model improved!!!
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1641961834737367		[learning rate: 0.015558]
	Learning Rate: 0.0155579
	LOSS [training: 0.1641961834737367 | validation: 0.145335328881011]
	TIME [epoch: 41.2 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1528313532465248		[learning rate: 0.015544]
	Learning Rate: 0.0155442
	LOSS [training: 0.1528313532465248 | validation: 0.11704269675362305]
	TIME [epoch: 41.2 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17665258551463203		[learning rate: 0.01553]
	Learning Rate: 0.0155304
	LOSS [training: 0.17665258551463203 | validation: 0.1030648210555451]
	TIME [epoch: 41.2 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13886640169541337		[learning rate: 0.015517]
	Learning Rate: 0.0155166
	LOSS [training: 0.13886640169541337 | validation: 0.13371919860416165]
	TIME [epoch: 41.2 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1818693908006712		[learning rate: 0.015503]
	Learning Rate: 0.0155028
	LOSS [training: 0.1818693908006712 | validation: 0.10947464696114376]
	TIME [epoch: 41.2 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1673317517943531		[learning rate: 0.015489]
	Learning Rate: 0.015489
	LOSS [training: 0.1673317517943531 | validation: 0.08664446563268335]
	TIME [epoch: 41.2 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13862536073086792		[learning rate: 0.015475]
	Learning Rate: 0.0154752
	LOSS [training: 0.13862536073086792 | validation: 0.07856111855792158]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_650.pth
	Model improved!!!
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10262618992106201		[learning rate: 0.015461]
	Learning Rate: 0.0154614
	LOSS [training: 0.10262618992106201 | validation: 0.23836850301458629]
	TIME [epoch: 41.3 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15195788072718314		[learning rate: 0.015448]
	Learning Rate: 0.0154475
	LOSS [training: 0.15195788072718314 | validation: 0.2932854182974411]
	TIME [epoch: 41.2 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17552223120542237		[learning rate: 0.015434]
	Learning Rate: 0.0154336
	LOSS [training: 0.17552223120542237 | validation: 0.11756299725777558]
	TIME [epoch: 41.2 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15286943272383585		[learning rate: 0.01542]
	Learning Rate: 0.0154198
	LOSS [training: 0.15286943272383585 | validation: 0.09505372125381509]
	TIME [epoch: 41.2 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15585279173326205		[learning rate: 0.015406]
	Learning Rate: 0.0154059
	LOSS [training: 0.15585279173326205 | validation: 0.1283210530054983]
	TIME [epoch: 41.2 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13973318027396314		[learning rate: 0.015392]
	Learning Rate: 0.0153919
	LOSS [training: 0.13973318027396314 | validation: 0.1231019685894387]
	TIME [epoch: 41.2 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13823706698699548		[learning rate: 0.015378]
	Learning Rate: 0.015378
	LOSS [training: 0.13823706698699548 | validation: 0.10324799950663383]
	TIME [epoch: 41.2 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17068378214681615		[learning rate: 0.015364]
	Learning Rate: 0.0153641
	LOSS [training: 0.17068378214681615 | validation: 0.10274768857866066]
	TIME [epoch: 41.2 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.156901792003765		[learning rate: 0.01535]
	Learning Rate: 0.0153501
	LOSS [training: 0.156901792003765 | validation: 0.09793119277687323]
	TIME [epoch: 41.3 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14508323292670935		[learning rate: 0.015336]
	Learning Rate: 0.0153361
	LOSS [training: 0.14508323292670935 | validation: 0.0861043687275851]
	TIME [epoch: 41.2 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14695153720217097		[learning rate: 0.015322]
	Learning Rate: 0.0153222
	LOSS [training: 0.14695153720217097 | validation: 0.0869761396164028]
	TIME [epoch: 41.3 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12854915953049362		[learning rate: 0.015308]
	Learning Rate: 0.0153082
	LOSS [training: 0.12854915953049362 | validation: 0.09519783272253995]
	TIME [epoch: 41.3 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14072203682051646		[learning rate: 0.015294]
	Learning Rate: 0.0152941
	LOSS [training: 0.14072203682051646 | validation: 0.09379415507846244]
	TIME [epoch: 41.3 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15569283247910995		[learning rate: 0.01528]
	Learning Rate: 0.0152801
	LOSS [training: 0.15569283247910995 | validation: 0.07389673237456258]
	TIME [epoch: 41.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_664.pth
	Model improved!!!
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1507744805951733		[learning rate: 0.015266]
	Learning Rate: 0.0152661
	LOSS [training: 0.1507744805951733 | validation: 0.1192878093124973]
	TIME [epoch: 41.2 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13154608306504428		[learning rate: 0.015252]
	Learning Rate: 0.015252
	LOSS [training: 0.13154608306504428 | validation: 0.0845829667818449]
	TIME [epoch: 41.2 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13730825705149438		[learning rate: 0.015238]
	Learning Rate: 0.0152379
	LOSS [training: 0.13730825705149438 | validation: 0.11636249462608689]
	TIME [epoch: 41.2 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1465725265346343		[learning rate: 0.015224]
	Learning Rate: 0.0152238
	LOSS [training: 0.1465725265346343 | validation: 0.11680898192968828]
	TIME [epoch: 41.2 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15016337281359843		[learning rate: 0.01521]
	Learning Rate: 0.0152097
	LOSS [training: 0.15016337281359843 | validation: 0.09178698679362426]
	TIME [epoch: 41.2 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14530325521670237		[learning rate: 0.015196]
	Learning Rate: 0.0151956
	LOSS [training: 0.14530325521670237 | validation: 0.09863053915828862]
	TIME [epoch: 41.3 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1420657397834831		[learning rate: 0.015182]
	Learning Rate: 0.0151815
	LOSS [training: 0.1420657397834831 | validation: 0.11414128013407393]
	TIME [epoch: 41.3 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14883616103626648		[learning rate: 0.015167]
	Learning Rate: 0.0151674
	LOSS [training: 0.14883616103626648 | validation: 0.21341059438862303]
	TIME [epoch: 41.3 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.226790344412019		[learning rate: 0.015153]
	Learning Rate: 0.0151532
	LOSS [training: 0.226790344412019 | validation: 0.12040875909597407]
	TIME [epoch: 41.2 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1253903603099874		[learning rate: 0.015139]
	Learning Rate: 0.015139
	LOSS [training: 0.1253903603099874 | validation: 0.11400082413353979]
	TIME [epoch: 41.2 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14206510761393215		[learning rate: 0.015125]
	Learning Rate: 0.0151248
	LOSS [training: 0.14206510761393215 | validation: 0.0940464385082848]
	TIME [epoch: 41.2 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1140653297978159		[learning rate: 0.015111]
	Learning Rate: 0.0151106
	LOSS [training: 0.1140653297978159 | validation: 0.09443009249622737]
	TIME [epoch: 41.3 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12420333851875467		[learning rate: 0.015096]
	Learning Rate: 0.0150964
	LOSS [training: 0.12420333851875467 | validation: 0.13048094864860948]
	TIME [epoch: 41.2 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.127110743563673		[learning rate: 0.015082]
	Learning Rate: 0.0150822
	LOSS [training: 0.127110743563673 | validation: 0.10809676223446078]
	TIME [epoch: 41.2 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15829550684348848		[learning rate: 0.015068]
	Learning Rate: 0.015068
	LOSS [training: 0.15829550684348848 | validation: 0.15027700785129006]
	TIME [epoch: 41.2 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14106177204437953		[learning rate: 0.015054]
	Learning Rate: 0.0150537
	LOSS [training: 0.14106177204437953 | validation: 0.08009070334418321]
	TIME [epoch: 41.2 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10707193024442396		[learning rate: 0.015039]
	Learning Rate: 0.0150394
	LOSS [training: 0.10707193024442396 | validation: 0.16746999907561821]
	TIME [epoch: 41.3 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16202915454245054		[learning rate: 0.015025]
	Learning Rate: 0.0150251
	LOSS [training: 0.16202915454245054 | validation: 0.13521259466528912]
	TIME [epoch: 41.3 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15231174665979724		[learning rate: 0.015011]
	Learning Rate: 0.0150108
	LOSS [training: 0.15231174665979724 | validation: 0.16477935549829809]
	TIME [epoch: 41.3 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11134826320334187		[learning rate: 0.014997]
	Learning Rate: 0.0149965
	LOSS [training: 0.11134826320334187 | validation: 0.17061329833361694]
	TIME [epoch: 41.2 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15730237826089855		[learning rate: 0.014982]
	Learning Rate: 0.0149822
	LOSS [training: 0.15730237826089855 | validation: 0.08564686622933682]
	TIME [epoch: 41.2 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12180230853616486		[learning rate: 0.014968]
	Learning Rate: 0.0149679
	LOSS [training: 0.12180230853616486 | validation: 0.24045078068294568]
	TIME [epoch: 41.2 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1614978728128606		[learning rate: 0.014954]
	Learning Rate: 0.0149535
	LOSS [training: 0.1614978728128606 | validation: 0.14759428946545133]
	TIME [epoch: 41.2 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16170451713101833		[learning rate: 0.014939]
	Learning Rate: 0.0149392
	LOSS [training: 0.16170451713101833 | validation: 0.09939975915471093]
	TIME [epoch: 41.2 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15709056273298339		[learning rate: 0.014925]
	Learning Rate: 0.0149248
	LOSS [training: 0.15709056273298339 | validation: 0.0766707882923254]
	TIME [epoch: 41.2 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11162978392108375		[learning rate: 0.01491]
	Learning Rate: 0.0149104
	LOSS [training: 0.11162978392108375 | validation: 0.08225103058330971]
	TIME [epoch: 41.3 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12025014438471304		[learning rate: 0.014896]
	Learning Rate: 0.014896
	LOSS [training: 0.12025014438471304 | validation: 0.07215384603077513]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_691.pth
	Model improved!!!
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10373755866649614		[learning rate: 0.014882]
	Learning Rate: 0.0148816
	LOSS [training: 0.10373755866649614 | validation: 0.12480243204535957]
	TIME [epoch: 41.2 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.162439311456281		[learning rate: 0.014867]
	Learning Rate: 0.0148671
	LOSS [training: 0.162439311456281 | validation: 0.09212174261810123]
	TIME [epoch: 41.2 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13169329378342473		[learning rate: 0.014853]
	Learning Rate: 0.0148527
	LOSS [training: 0.13169329378342473 | validation: 0.1220076082268103]
	TIME [epoch: 41.2 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10713855761596412		[learning rate: 0.014838]
	Learning Rate: 0.0148382
	LOSS [training: 0.10713855761596412 | validation: 0.15552585794020019]
	TIME [epoch: 41.2 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13478653886549768		[learning rate: 0.014824]
	Learning Rate: 0.0148237
	LOSS [training: 0.13478653886549768 | validation: 0.14872955568099677]
	TIME [epoch: 41.2 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14080395212562052		[learning rate: 0.014809]
	Learning Rate: 0.0148093
	LOSS [training: 0.14080395212562052 | validation: 0.18907008461365035]
	TIME [epoch: 41.2 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1364108023514866		[learning rate: 0.014795]
	Learning Rate: 0.0147948
	LOSS [training: 0.1364108023514866 | validation: 0.11697820668368439]
	TIME [epoch: 41.2 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1238157293796454		[learning rate: 0.01478]
	Learning Rate: 0.0147803
	LOSS [training: 0.1238157293796454 | validation: 0.21295786360043284]
	TIME [epoch: 41.2 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11331493256494861		[learning rate: 0.014766]
	Learning Rate: 0.0147657
	LOSS [training: 0.11331493256494861 | validation: 0.21134584463500228]
	TIME [epoch: 41.2 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13358621793685319		[learning rate: 0.014751]
	Learning Rate: 0.0147512
	LOSS [training: 0.13358621793685319 | validation: 0.15764970298466063]
	TIME [epoch: 41.2 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11497353339233915		[learning rate: 0.014737]
	Learning Rate: 0.0147366
	LOSS [training: 0.11497353339233915 | validation: 0.16245770434810022]
	TIME [epoch: 41.2 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11284177859856409		[learning rate: 0.014722]
	Learning Rate: 0.0147221
	LOSS [training: 0.11284177859856409 | validation: 0.10360069109497586]
	TIME [epoch: 41.2 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14559683479575336		[learning rate: 0.014707]
	Learning Rate: 0.0147075
	LOSS [training: 0.14559683479575336 | validation: 0.061599942898684194]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_704.pth
	Model improved!!!
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10901198138227967		[learning rate: 0.014693]
	Learning Rate: 0.0146929
	LOSS [training: 0.10901198138227967 | validation: 0.12604268764846996]
	TIME [epoch: 41.2 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1463118697429456		[learning rate: 0.014678]
	Learning Rate: 0.0146783
	LOSS [training: 0.1463118697429456 | validation: 0.1617965257786026]
	TIME [epoch: 41.2 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.125300637734175		[learning rate: 0.014664]
	Learning Rate: 0.0146637
	LOSS [training: 0.125300637734175 | validation: 0.11901639099053232]
	TIME [epoch: 41.2 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11758389682217876		[learning rate: 0.014649]
	Learning Rate: 0.0146491
	LOSS [training: 0.11758389682217876 | validation: 0.22014387920683887]
	TIME [epoch: 41.2 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10631017276198579		[learning rate: 0.014634]
	Learning Rate: 0.0146344
	LOSS [training: 0.10631017276198579 | validation: 0.15060189895712403]
	TIME [epoch: 41.2 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14730013679990842		[learning rate: 0.01462]
	Learning Rate: 0.0146198
	LOSS [training: 0.14730013679990842 | validation: 0.17289105262745885]
	TIME [epoch: 41.2 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14373920098377704		[learning rate: 0.014605]
	Learning Rate: 0.0146051
	LOSS [training: 0.14373920098377704 | validation: 0.0857847542591827]
	TIME [epoch: 41.2 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11944325546400603		[learning rate: 0.01459]
	Learning Rate: 0.0145904
	LOSS [training: 0.11944325546400603 | validation: 0.06940579479985212]
	TIME [epoch: 41.2 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12264102074732278		[learning rate: 0.014576]
	Learning Rate: 0.0145757
	LOSS [training: 0.12264102074732278 | validation: 0.08286440825315579]
	TIME [epoch: 41.2 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10312828846073153		[learning rate: 0.014561]
	Learning Rate: 0.014561
	LOSS [training: 0.10312828846073153 | validation: 0.05734069347988227]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_714.pth
	Model improved!!!
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10970623392834386		[learning rate: 0.014546]
	Learning Rate: 0.0145463
	LOSS [training: 0.10970623392834386 | validation: 0.06175644844370311]
	TIME [epoch: 41.2 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08983734110791684		[learning rate: 0.014532]
	Learning Rate: 0.0145316
	LOSS [training: 0.08983734110791684 | validation: 0.08845388890976502]
	TIME [epoch: 41.2 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1583454275162462		[learning rate: 0.014517]
	Learning Rate: 0.0145168
	LOSS [training: 0.1583454275162462 | validation: 0.059797820888888055]
	TIME [epoch: 41.2 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1254147245438867		[learning rate: 0.014502]
	Learning Rate: 0.0145021
	LOSS [training: 0.1254147245438867 | validation: 0.09234676426951832]
	TIME [epoch: 41.2 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11696435515786663		[learning rate: 0.014487]
	Learning Rate: 0.0144873
	LOSS [training: 0.11696435515786663 | validation: 0.17833623677156496]
	TIME [epoch: 41.2 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1002076241153537		[learning rate: 0.014473]
	Learning Rate: 0.0144726
	LOSS [training: 0.1002076241153537 | validation: 0.13069043440890107]
	TIME [epoch: 41.2 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11991942571574107		[learning rate: 0.014458]
	Learning Rate: 0.0144578
	LOSS [training: 0.11991942571574107 | validation: 0.1610364855371503]
	TIME [epoch: 41.2 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1289593362397236		[learning rate: 0.014443]
	Learning Rate: 0.014443
	LOSS [training: 0.1289593362397236 | validation: 0.0571993018401869]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_722.pth
	Model improved!!!
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1191272693812134		[learning rate: 0.014428]
	Learning Rate: 0.0144281
	LOSS [training: 0.1191272693812134 | validation: 0.05626637119727146]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_723.pth
	Model improved!!!
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11516629398528135		[learning rate: 0.014413]
	Learning Rate: 0.0144133
	LOSS [training: 0.11516629398528135 | validation: 0.09057848404482924]
	TIME [epoch: 41.2 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1688090939868534		[learning rate: 0.014398]
	Learning Rate: 0.0143985
	LOSS [training: 0.1688090939868534 | validation: 0.08381594222109459]
	TIME [epoch: 41.2 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11634214559328863		[learning rate: 0.014384]
	Learning Rate: 0.0143836
	LOSS [training: 0.11634214559328863 | validation: 0.11724306984665922]
	TIME [epoch: 41.2 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09410348160117903		[learning rate: 0.014369]
	Learning Rate: 0.0143688
	LOSS [training: 0.09410348160117903 | validation: 0.0780428827591241]
	TIME [epoch: 41.2 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11444481287181128		[learning rate: 0.014354]
	Learning Rate: 0.0143539
	LOSS [training: 0.11444481287181128 | validation: 0.15573033595938462]
	TIME [epoch: 41.2 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12207499819130986		[learning rate: 0.014339]
	Learning Rate: 0.014339
	LOSS [training: 0.12207499819130986 | validation: 0.08919366911111132]
	TIME [epoch: 41.2 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09259361396514945		[learning rate: 0.014324]
	Learning Rate: 0.0143241
	LOSS [training: 0.09259361396514945 | validation: 0.12046944191955065]
	TIME [epoch: 41.2 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13336653976746898		[learning rate: 0.014309]
	Learning Rate: 0.0143092
	LOSS [training: 0.13336653976746898 | validation: 0.13697792228290318]
	TIME [epoch: 41.2 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11191230716591427		[learning rate: 0.014294]
	Learning Rate: 0.0142943
	LOSS [training: 0.11191230716591427 | validation: 0.09520407205059163]
	TIME [epoch: 41.2 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07954724465725853		[learning rate: 0.014279]
	Learning Rate: 0.0142793
	LOSS [training: 0.07954724465725853 | validation: 0.08314139947422199]
	TIME [epoch: 41.2 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11360254992481865		[learning rate: 0.014264]
	Learning Rate: 0.0142644
	LOSS [training: 0.11360254992481865 | validation: 0.04966060794159392]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_734.pth
	Model improved!!!
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11517255480172078		[learning rate: 0.014249]
	Learning Rate: 0.0142494
	LOSS [training: 0.11517255480172078 | validation: 0.15767839035175069]
	TIME [epoch: 41.2 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2548028886303754		[learning rate: 0.014234]
	Learning Rate: 0.0142345
	LOSS [training: 0.2548028886303754 | validation: 0.05464909207721286]
	TIME [epoch: 41.2 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09849550561856601		[learning rate: 0.014219]
	Learning Rate: 0.0142195
	LOSS [training: 0.09849550561856601 | validation: 0.10435042554680864]
	TIME [epoch: 41.2 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11411363041078634		[learning rate: 0.014205]
	Learning Rate: 0.0142045
	LOSS [training: 0.11411363041078634 | validation: 0.08897205914712814]
	TIME [epoch: 41.2 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08690781443208923		[learning rate: 0.01419]
	Learning Rate: 0.0141895
	LOSS [training: 0.08690781443208923 | validation: 0.3691067402264673]
	TIME [epoch: 41.2 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21531812892560062		[learning rate: 0.014174]
	Learning Rate: 0.0141745
	LOSS [training: 0.21531812892560062 | validation: 0.048010767330974986]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_740.pth
	Model improved!!!
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11651630957612476		[learning rate: 0.014159]
	Learning Rate: 0.0141595
	LOSS [training: 0.11651630957612476 | validation: 0.06989959188601205]
	TIME [epoch: 41.2 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09458733339649392		[learning rate: 0.014144]
	Learning Rate: 0.0141444
	LOSS [training: 0.09458733339649392 | validation: 0.06800765857435248]
	TIME [epoch: 41.2 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09793197091714671		[learning rate: 0.014129]
	Learning Rate: 0.0141294
	LOSS [training: 0.09793197091714671 | validation: 0.12423973266650003]
	TIME [epoch: 41.2 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09197829210392423		[learning rate: 0.014114]
	Learning Rate: 0.0141143
	LOSS [training: 0.09197829210392423 | validation: 0.14967579974542483]
	TIME [epoch: 41.2 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09723209243076189		[learning rate: 0.014099]
	Learning Rate: 0.0140992
	LOSS [training: 0.09723209243076189 | validation: 0.1851260479264521]
	TIME [epoch: 41.2 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12956306404909113		[learning rate: 0.014084]
	Learning Rate: 0.0140842
	LOSS [training: 0.12956306404909113 | validation: 0.09981645848575266]
	TIME [epoch: 41.2 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08842808304901795		[learning rate: 0.014069]
	Learning Rate: 0.0140691
	LOSS [training: 0.08842808304901795 | validation: 0.1721147582231249]
	TIME [epoch: 41.2 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20609691384634812		[learning rate: 0.014054]
	Learning Rate: 0.014054
	LOSS [training: 0.20609691384634812 | validation: 0.11448383648314485]
	TIME [epoch: 41.2 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12497672153323594		[learning rate: 0.014039]
	Learning Rate: 0.0140389
	LOSS [training: 0.12497672153323594 | validation: 0.06788999235323448]
	TIME [epoch: 41.2 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07140511647378837		[learning rate: 0.014024]
	Learning Rate: 0.0140237
	LOSS [training: 0.07140511647378837 | validation: 0.13338596900477317]
	TIME [epoch: 41.2 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11239070541326193		[learning rate: 0.014009]
	Learning Rate: 0.0140086
	LOSS [training: 0.11239070541326193 | validation: 0.06694949485438148]
	TIME [epoch: 41.2 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11295146283807606		[learning rate: 0.013993]
	Learning Rate: 0.0139934
	LOSS [training: 0.11295146283807606 | validation: 0.12095226977731383]
	TIME [epoch: 41.2 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07701938535673494		[learning rate: 0.013978]
	Learning Rate: 0.0139783
	LOSS [training: 0.07701938535673494 | validation: 0.07230516181161403]
	TIME [epoch: 41.2 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07743586921072575		[learning rate: 0.013963]
	Learning Rate: 0.0139631
	LOSS [training: 0.07743586921072575 | validation: 0.13424125922976626]
	TIME [epoch: 41.2 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09692693950170038		[learning rate: 0.013948]
	Learning Rate: 0.0139479
	LOSS [training: 0.09692693950170038 | validation: 0.13727420395823248]
	TIME [epoch: 41.2 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13690732825773577		[learning rate: 0.013933]
	Learning Rate: 0.0139327
	LOSS [training: 0.13690732825773577 | validation: 0.044547226325901135]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_756.pth
	Model improved!!!
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09477834983376679		[learning rate: 0.013918]
	Learning Rate: 0.0139175
	LOSS [training: 0.09477834983376679 | validation: 0.0497273409943272]
	TIME [epoch: 41.2 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12314686090848047		[learning rate: 0.013902]
	Learning Rate: 0.0139023
	LOSS [training: 0.12314686090848047 | validation: 0.05814317098753116]
	TIME [epoch: 41.2 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09946080861727886		[learning rate: 0.013887]
	Learning Rate: 0.0138871
	LOSS [training: 0.09946080861727886 | validation: 0.1271888889501067]
	TIME [epoch: 41.2 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1350621944987626		[learning rate: 0.013872]
	Learning Rate: 0.0138719
	LOSS [training: 0.1350621944987626 | validation: 0.050452483864595866]
	TIME [epoch: 41.1 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08052414350735254		[learning rate: 0.013857]
	Learning Rate: 0.0138566
	LOSS [training: 0.08052414350735254 | validation: 0.11601270014278858]
	TIME [epoch: 41.1 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11769478367761432		[learning rate: 0.013841]
	Learning Rate: 0.0138414
	LOSS [training: 0.11769478367761432 | validation: 0.04599118483215334]
	TIME [epoch: 41.2 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07872802024442081		[learning rate: 0.013826]
	Learning Rate: 0.0138261
	LOSS [training: 0.07872802024442081 | validation: 0.17100806300776672]
	TIME [epoch: 41.2 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11046290022922187		[learning rate: 0.013811]
	Learning Rate: 0.0138108
	LOSS [training: 0.11046290022922187 | validation: 0.12752177754466376]
	TIME [epoch: 41.2 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13288400195586922		[learning rate: 0.013796]
	Learning Rate: 0.0137955
	LOSS [training: 0.13288400195586922 | validation: 0.07061782255470794]
	TIME [epoch: 41.2 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06317094437562264		[learning rate: 0.01378]
	Learning Rate: 0.0137802
	LOSS [training: 0.06317094437562264 | validation: 0.1231857354240292]
	TIME [epoch: 41.2 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08703866193713897		[learning rate: 0.013765]
	Learning Rate: 0.0137649
	LOSS [training: 0.08703866193713897 | validation: 0.13539869088156115]
	TIME [epoch: 41.2 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0961144758050031		[learning rate: 0.01375]
	Learning Rate: 0.0137496
	LOSS [training: 0.0961144758050031 | validation: 0.1626281012018868]
	TIME [epoch: 41.2 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13631658551329887		[learning rate: 0.013734]
	Learning Rate: 0.0137343
	LOSS [training: 0.13631658551329887 | validation: 0.11623489883421304]
	TIME [epoch: 41.2 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0927257245683703		[learning rate: 0.013719]
	Learning Rate: 0.013719
	LOSS [training: 0.0927257245683703 | validation: 0.07334747104306556]
	TIME [epoch: 41.2 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10286218260143691		[learning rate: 0.013704]
	Learning Rate: 0.0137036
	LOSS [training: 0.10286218260143691 | validation: 0.04522534109084375]
	TIME [epoch: 41.2 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0492051610336303		[learning rate: 0.013688]
	Learning Rate: 0.0136882
	LOSS [training: 0.0492051610336303 | validation: 0.09995404493732454]
	TIME [epoch: 41.2 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12227587815264952		[learning rate: 0.013673]
	Learning Rate: 0.0136729
	LOSS [training: 0.12227587815264952 | validation: 0.20808569388621778]
	TIME [epoch: 41.1 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08802802877630916		[learning rate: 0.013657]
	Learning Rate: 0.0136575
	LOSS [training: 0.08802802877630916 | validation: 0.038942445084142796]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_774.pth
	Model improved!!!
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08450203630914153		[learning rate: 0.013642]
	Learning Rate: 0.0136421
	LOSS [training: 0.08450203630914153 | validation: 0.062333356356911104]
	TIME [epoch: 41.2 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09524615356654723		[learning rate: 0.013627]
	Learning Rate: 0.0136267
	LOSS [training: 0.09524615356654723 | validation: 0.1440777013994785]
	TIME [epoch: 41.2 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20353617987517816		[learning rate: 0.013611]
	Learning Rate: 0.0136113
	LOSS [training: 0.20353617987517816 | validation: 0.15367151331175252]
	TIME [epoch: 41.2 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17858171217156277		[learning rate: 0.013596]
	Learning Rate: 0.0135959
	LOSS [training: 0.17858171217156277 | validation: 0.10960665186853204]
	TIME [epoch: 41.2 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0739034391464094		[learning rate: 0.01358]
	Learning Rate: 0.0135805
	LOSS [training: 0.0739034391464094 | validation: 0.0971587536010377]
	TIME [epoch: 41.2 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11893943417738502		[learning rate: 0.013565]
	Learning Rate: 0.013565
	LOSS [training: 0.11893943417738502 | validation: 0.06895087654542743]
	TIME [epoch: 41.1 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1159910170823607		[learning rate: 0.01355]
	Learning Rate: 0.0135496
	LOSS [training: 0.1159910170823607 | validation: 0.1174931335122971]
	TIME [epoch: 41.2 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08609973879132471		[learning rate: 0.013534]
	Learning Rate: 0.0135341
	LOSS [training: 0.08609973879132471 | validation: 0.038970408558159475]
	TIME [epoch: 41.2 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07509857180991804		[learning rate: 0.013519]
	Learning Rate: 0.0135186
	LOSS [training: 0.07509857180991804 | validation: 0.06670246681833239]
	TIME [epoch: 41.2 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07909547557959073		[learning rate: 0.013503]
	Learning Rate: 0.0135032
	LOSS [training: 0.07909547557959073 | validation: 0.05877490745774337]
	TIME [epoch: 41.2 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13572977371693512		[learning rate: 0.013488]
	Learning Rate: 0.0134877
	LOSS [training: 0.13572977371693512 | validation: 0.15557054463249273]
	TIME [epoch: 41.2 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0737905792111925		[learning rate: 0.013472]
	Learning Rate: 0.0134722
	LOSS [training: 0.0737905792111925 | validation: 0.05097219620946203]
	TIME [epoch: 41.2 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12368907328245975		[learning rate: 0.013457]
	Learning Rate: 0.0134567
	LOSS [training: 0.12368907328245975 | validation: 0.0838778698787355]
	TIME [epoch: 41.2 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09746702431483589		[learning rate: 0.013441]
	Learning Rate: 0.0134412
	LOSS [training: 0.09746702431483589 | validation: 0.040315747670048474]
	TIME [epoch: 41.2 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036898768568181396		[learning rate: 0.013426]
	Learning Rate: 0.0134256
	LOSS [training: 0.036898768568181396 | validation: 0.04412617427800099]
	TIME [epoch: 41.2 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08333617564716854		[learning rate: 0.01341]
	Learning Rate: 0.0134101
	LOSS [training: 0.08333617564716854 | validation: 0.12816843661985802]
	TIME [epoch: 41.2 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13710590938779624		[learning rate: 0.013395]
	Learning Rate: 0.0133946
	LOSS [training: 0.13710590938779624 | validation: 0.09093617363822006]
	TIME [epoch: 41.1 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13840671085988723		[learning rate: 0.013379]
	Learning Rate: 0.013379
	LOSS [training: 0.13840671085988723 | validation: 0.031074619645460558]
	TIME [epoch: 41.1 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_792.pth
	Model improved!!!
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0792443977699647		[learning rate: 0.013363]
	Learning Rate: 0.0133635
	LOSS [training: 0.0792443977699647 | validation: 0.09042233181662544]
	TIME [epoch: 41.2 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06752517641982958		[learning rate: 0.013348]
	Learning Rate: 0.0133479
	LOSS [training: 0.06752517641982958 | validation: 0.026111030801191596]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_794.pth
	Model improved!!!
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11935663668436383		[learning rate: 0.013332]
	Learning Rate: 0.0133323
	LOSS [training: 0.11935663668436383 | validation: 0.1762855427227994]
	TIME [epoch: 41.2 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07144056792647273		[learning rate: 0.013317]
	Learning Rate: 0.0133167
	LOSS [training: 0.07144056792647273 | validation: 0.05840521483950492]
	TIME [epoch: 41.2 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12609903092593977		[learning rate: 0.013301]
	Learning Rate: 0.0133011
	LOSS [training: 0.12609903092593977 | validation: 0.1576927920301484]
	TIME [epoch: 41.2 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09864750124832505		[learning rate: 0.013286]
	Learning Rate: 0.0132855
	LOSS [training: 0.09864750124832505 | validation: 0.3111801873720349]
	TIME [epoch: 41.2 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1399338241862223		[learning rate: 0.01327]
	Learning Rate: 0.0132699
	LOSS [training: 0.1399338241862223 | validation: 0.03288352746232122]
	TIME [epoch: 41.2 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1538401998585878		[learning rate: 0.013254]
	Learning Rate: 0.0132543
	LOSS [training: 0.1538401998585878 | validation: 0.28877813493914445]
	TIME [epoch: 41.2 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1561819887031525		[learning rate: 0.013239]
	Learning Rate: 0.0132386
	LOSS [training: 0.1561819887031525 | validation: 0.17127964656765118]
	TIME [epoch: 41.2 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07271981821487564		[learning rate: 0.013223]
	Learning Rate: 0.013223
	LOSS [training: 0.07271981821487564 | validation: 0.02827542418574914]
	TIME [epoch: 41.3 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03319098485549407		[learning rate: 0.013207]
	Learning Rate: 0.0132074
	LOSS [training: 0.03319098485549407 | validation: 0.046754632146087764]
	TIME [epoch: 41.2 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08198827364472958		[learning rate: 0.013192]
	Learning Rate: 0.0131917
	LOSS [training: 0.08198827364472958 | validation: 0.0757611022043755]
	TIME [epoch: 41.2 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18668491760367972		[learning rate: 0.013176]
	Learning Rate: 0.013176
	LOSS [training: 0.18668491760367972 | validation: 0.04158257058921989]
	TIME [epoch: 41.3 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060325331580490194		[learning rate: 0.01316]
	Learning Rate: 0.0131603
	LOSS [training: 0.060325331580490194 | validation: 0.07114218411540624]
	TIME [epoch: 41.3 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04430370079595672		[learning rate: 0.013145]
	Learning Rate: 0.0131447
	LOSS [training: 0.04430370079595672 | validation: 0.03527313019460949]
	TIME [epoch: 41.3 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15912133291266278		[learning rate: 0.013129]
	Learning Rate: 0.013129
	LOSS [training: 0.15912133291266278 | validation: 0.16271431443539874]
	TIME [epoch: 41.3 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08737920898582871		[learning rate: 0.013113]
	Learning Rate: 0.0131133
	LOSS [training: 0.08737920898582871 | validation: 0.06547467402735155]
	TIME [epoch: 41.3 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07370631148750749		[learning rate: 0.013098]
	Learning Rate: 0.0130976
	LOSS [training: 0.07370631148750749 | validation: 0.04222602626925685]
	TIME [epoch: 41.3 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08139768564288152		[learning rate: 0.013082]
	Learning Rate: 0.0130818
	LOSS [training: 0.08139768564288152 | validation: 0.033253574110465416]
	TIME [epoch: 41.3 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026580240272979835		[learning rate: 0.013066]
	Learning Rate: 0.0130661
	LOSS [training: 0.026580240272979835 | validation: 0.172723935923884]
	TIME [epoch: 41.3 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08432807208265644		[learning rate: 0.01305]
	Learning Rate: 0.0130504
	LOSS [training: 0.08432807208265644 | validation: 0.0849601519692362]
	TIME [epoch: 41.3 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10323749230788763		[learning rate: 0.013035]
	Learning Rate: 0.0130346
	LOSS [training: 0.10323749230788763 | validation: 0.02180856488963802]
	TIME [epoch: 41.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_814.pth
	Model improved!!!
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026499082598314933		[learning rate: 0.013019]
	Learning Rate: 0.0130189
	LOSS [training: 0.026499082598314933 | validation: 0.06163513030560537]
	TIME [epoch: 41.2 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10102383506803463		[learning rate: 0.013003]
	Learning Rate: 0.0130031
	LOSS [training: 0.10102383506803463 | validation: 0.06232611907206413]
	TIME [epoch: 41.2 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07711475758371854		[learning rate: 0.012987]
	Learning Rate: 0.0129873
	LOSS [training: 0.07711475758371854 | validation: 0.04573051614512973]
	TIME [epoch: 41.2 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09157722841927145		[learning rate: 0.012972]
	Learning Rate: 0.0129716
	LOSS [training: 0.09157722841927145 | validation: 0.10989655832698994]
	TIME [epoch: 41.2 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0801230153892934		[learning rate: 0.012956]
	Learning Rate: 0.0129558
	LOSS [training: 0.0801230153892934 | validation: 0.04045987664570962]
	TIME [epoch: 41.2 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029406405852685752		[learning rate: 0.01294]
	Learning Rate: 0.01294
	LOSS [training: 0.029406405852685752 | validation: 0.039860155130525196]
	TIME [epoch: 41.2 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1776955488595538		[learning rate: 0.012924]
	Learning Rate: 0.0129242
	LOSS [training: 0.1776955488595538 | validation: 0.1060081331004107]
	TIME [epoch: 41.2 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11516051746352221		[learning rate: 0.012908]
	Learning Rate: 0.0129084
	LOSS [training: 0.11516051746352221 | validation: 0.06608319427484292]
	TIME [epoch: 41.2 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0914707561966978		[learning rate: 0.012893]
	Learning Rate: 0.0128926
	LOSS [training: 0.0914707561966978 | validation: 0.030451877127055025]
	TIME [epoch: 41.2 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05709711386725563		[learning rate: 0.012877]
	Learning Rate: 0.0128767
	LOSS [training: 0.05709711386725563 | validation: 0.04749099014388141]
	TIME [epoch: 41.2 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07464196253743038		[learning rate: 0.012861]
	Learning Rate: 0.0128609
	LOSS [training: 0.07464196253743038 | validation: 0.08438864475090085]
	TIME [epoch: 41.2 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06766784708633687		[learning rate: 0.012845]
	Learning Rate: 0.0128451
	LOSS [training: 0.06766784708633687 | validation: 0.09749581762736582]
	TIME [epoch: 41.2 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09461767678271321		[learning rate: 0.012829]
	Learning Rate: 0.0128292
	LOSS [training: 0.09461767678271321 | validation: 0.08688637984681803]
	TIME [epoch: 41.2 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05862818962760866		[learning rate: 0.012813]
	Learning Rate: 0.0128133
	LOSS [training: 0.05862818962760866 | validation: 0.03811564601302855]
	TIME [epoch: 41.2 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054373807885967955		[learning rate: 0.012797]
	Learning Rate: 0.0127975
	LOSS [training: 0.054373807885967955 | validation: 0.08505750264272013]
	TIME [epoch: 41.2 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044802134593702095		[learning rate: 0.012782]
	Learning Rate: 0.0127816
	LOSS [training: 0.044802134593702095 | validation: 0.03179136424166574]
	TIME [epoch: 41.2 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08572607097116249		[learning rate: 0.012766]
	Learning Rate: 0.0127657
	LOSS [training: 0.08572607097116249 | validation: 0.10281472140211109]
	TIME [epoch: 41.2 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16241738810417297		[learning rate: 0.01275]
	Learning Rate: 0.0127498
	LOSS [training: 0.16241738810417297 | validation: 0.3971591025416278]
	TIME [epoch: 41.2 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21293167138143868		[learning rate: 0.012734]
	Learning Rate: 0.0127339
	LOSS [training: 0.21293167138143868 | validation: 0.09273371691962969]
	TIME [epoch: 41.2 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15235437025154314		[learning rate: 0.012718]
	Learning Rate: 0.012718
	LOSS [training: 0.15235437025154314 | validation: 0.02778023768261864]
	TIME [epoch: 41.2 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061055050422305496		[learning rate: 0.012702]
	Learning Rate: 0.0127021
	LOSS [training: 0.061055050422305496 | validation: 0.0637527875784209]
	TIME [epoch: 41.2 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05094493417822098		[learning rate: 0.012686]
	Learning Rate: 0.0126862
	LOSS [training: 0.05094493417822098 | validation: 0.0369491694572579]
	TIME [epoch: 41.2 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13324670974665903		[learning rate: 0.01267]
	Learning Rate: 0.0126703
	LOSS [training: 0.13324670974665903 | validation: 0.09300785486371232]
	TIME [epoch: 41.2 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04315513802997231		[learning rate: 0.012654]
	Learning Rate: 0.0126544
	LOSS [training: 0.04315513802997231 | validation: 0.01840867849055694]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_838.pth
	Model improved!!!
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02315356379820349		[learning rate: 0.012638]
	Learning Rate: 0.0126384
	LOSS [training: 0.02315356379820349 | validation: 0.04655431288245611]
	TIME [epoch: 41.2 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14392475184563988		[learning rate: 0.012622]
	Learning Rate: 0.0126225
	LOSS [training: 0.14392475184563988 | validation: 0.0517892563270112]
	TIME [epoch: 41.2 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12869873253893296		[learning rate: 0.012607]
	Learning Rate: 0.0126065
	LOSS [training: 0.12869873253893296 | validation: 0.05534411323905383]
	TIME [epoch: 41.2 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041833721519400555		[learning rate: 0.012591]
	Learning Rate: 0.0125906
	LOSS [training: 0.041833721519400555 | validation: 0.025442855435096782]
	TIME [epoch: 41.2 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0925455771689994		[learning rate: 0.012575]
	Learning Rate: 0.0125746
	LOSS [training: 0.0925455771689994 | validation: 0.08876863921765747]
	TIME [epoch: 41.2 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0542544057027425		[learning rate: 0.012559]
	Learning Rate: 0.0125586
	LOSS [training: 0.0542544057027425 | validation: 0.05045944914696253]
	TIME [epoch: 41.2 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08372723630730788		[learning rate: 0.012543]
	Learning Rate: 0.0125426
	LOSS [training: 0.08372723630730788 | validation: 0.07143973693671556]
	TIME [epoch: 41.2 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051784918437663016		[learning rate: 0.012527]
	Learning Rate: 0.0125267
	LOSS [training: 0.051784918437663016 | validation: 0.18233087141469095]
	TIME [epoch: 41.2 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1388262423031968		[learning rate: 0.012511]
	Learning Rate: 0.0125107
	LOSS [training: 0.1388262423031968 | validation: 0.03301668353540238]
	TIME [epoch: 41.2 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06072670342081541		[learning rate: 0.012495]
	Learning Rate: 0.0124947
	LOSS [training: 0.06072670342081541 | validation: 0.22860897604992092]
	TIME [epoch: 41.2 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1381301315969579		[learning rate: 0.012479]
	Learning Rate: 0.0124786
	LOSS [training: 0.1381301315969579 | validation: 0.3365597758276301]
	TIME [epoch: 41.2 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14669495264463653		[learning rate: 0.012463]
	Learning Rate: 0.0124626
	LOSS [training: 0.14669495264463653 | validation: 0.03445874974818879]
	TIME [epoch: 41.2 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032490444121322154		[learning rate: 0.012447]
	Learning Rate: 0.0124466
	LOSS [training: 0.032490444121322154 | validation: 0.02773524604749493]
	TIME [epoch: 41.2 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037221146959878784		[learning rate: 0.012431]
	Learning Rate: 0.0124306
	LOSS [training: 0.037221146959878784 | validation: 0.03359387646279094]
	TIME [epoch: 41.2 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04062163560476181		[learning rate: 0.012415]
	Learning Rate: 0.0124145
	LOSS [training: 0.04062163560476181 | validation: 0.058195873429318215]
	TIME [epoch: 41.2 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04645876266362088		[learning rate: 0.012399]
	Learning Rate: 0.0123985
	LOSS [training: 0.04645876266362088 | validation: 0.1803547476019036]
	TIME [epoch: 41.2 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07722274610869861		[learning rate: 0.012382]
	Learning Rate: 0.0123825
	LOSS [training: 0.07722274610869861 | validation: 0.03277558164198429]
	TIME [epoch: 41.2 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08979998744362597		[learning rate: 0.012366]
	Learning Rate: 0.0123664
	LOSS [training: 0.08979998744362597 | validation: 0.0549925955296418]
	TIME [epoch: 41.2 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07465701959605632		[learning rate: 0.01235]
	Learning Rate: 0.0123503
	LOSS [training: 0.07465701959605632 | validation: 0.046157702234796094]
	TIME [epoch: 41.2 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059135647107367295		[learning rate: 0.012334]
	Learning Rate: 0.0123343
	LOSS [training: 0.059135647107367295 | validation: 0.020898209320452922]
	TIME [epoch: 41.2 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03321057883915836		[learning rate: 0.012318]
	Learning Rate: 0.0123182
	LOSS [training: 0.03321057883915836 | validation: 0.036340821906354674]
	TIME [epoch: 41.2 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1103826496704207		[learning rate: 0.012302]
	Learning Rate: 0.0123021
	LOSS [training: 0.1103826496704207 | validation: 0.07266584242858456]
	TIME [epoch: 41.2 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04881340303965526		[learning rate: 0.012286]
	Learning Rate: 0.012286
	LOSS [training: 0.04881340303965526 | validation: 0.025163810737677633]
	TIME [epoch: 41.2 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021564706510590094		[learning rate: 0.01227]
	Learning Rate: 0.0122699
	LOSS [training: 0.021564706510590094 | validation: 0.026696154928157467]
	TIME [epoch: 41.2 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0898742705677745		[learning rate: 0.012254]
	Learning Rate: 0.0122538
	LOSS [training: 0.0898742705677745 | validation: 0.04341818394362509]
	TIME [epoch: 41.2 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09534422583712449		[learning rate: 0.012238]
	Learning Rate: 0.0122377
	LOSS [training: 0.09534422583712449 | validation: 0.03473334751734817]
	TIME [epoch: 41.2 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035075026981096824		[learning rate: 0.012222]
	Learning Rate: 0.0122216
	LOSS [training: 0.035075026981096824 | validation: 0.03993981154718262]
	TIME [epoch: 41.2 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10134033586852284		[learning rate: 0.012205]
	Learning Rate: 0.0122055
	LOSS [training: 0.10134033586852284 | validation: 0.03617777887441219]
	TIME [epoch: 41.2 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028692006226877585		[learning rate: 0.012189]
	Learning Rate: 0.0121894
	LOSS [training: 0.028692006226877585 | validation: 0.021763978108613687]
	TIME [epoch: 41.2 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03101764767396946		[learning rate: 0.012173]
	Learning Rate: 0.0121732
	LOSS [training: 0.03101764767396946 | validation: 0.0658561065393621]
	TIME [epoch: 41.2 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027186677061435422		[learning rate: 0.012157]
	Learning Rate: 0.0121571
	LOSS [training: 0.027186677061435422 | validation: 0.025515923749001378]
	TIME [epoch: 41.2 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11264493512931942		[learning rate: 0.012141]
	Learning Rate: 0.012141
	LOSS [training: 0.11264493512931942 | validation: 0.028901739155348743]
	TIME [epoch: 41.2 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05232243481802807		[learning rate: 0.012125]
	Learning Rate: 0.0121248
	LOSS [training: 0.05232243481802807 | validation: 0.02745344099496245]
	TIME [epoch: 41.2 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09395052508637002		[learning rate: 0.012109]
	Learning Rate: 0.0121087
	LOSS [training: 0.09395052508637002 | validation: 0.04211499617574109]
	TIME [epoch: 41.2 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051673988237107185		[learning rate: 0.012093]
	Learning Rate: 0.0120925
	LOSS [training: 0.051673988237107185 | validation: 0.030880405186989444]
	TIME [epoch: 41.2 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04455770606378473		[learning rate: 0.012076]
	Learning Rate: 0.0120763
	LOSS [training: 0.04455770606378473 | validation: 0.1029129922269015]
	TIME [epoch: 41.2 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06924727379454206		[learning rate: 0.01206]
	Learning Rate: 0.0120602
	LOSS [training: 0.06924727379454206 | validation: 0.018053005829270077]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_875.pth
	Model improved!!!
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033058176941694245		[learning rate: 0.012044]
	Learning Rate: 0.012044
	LOSS [training: 0.033058176941694245 | validation: 0.06702225731608107]
	TIME [epoch: 41.2 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09504099107108345		[learning rate: 0.012028]
	Learning Rate: 0.0120278
	LOSS [training: 0.09504099107108345 | validation: 0.08775022563584954]
	TIME [epoch: 41.2 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06564497059566653		[learning rate: 0.012012]
	Learning Rate: 0.0120116
	LOSS [training: 0.06564497059566653 | validation: 0.07967310876614765]
	TIME [epoch: 41.2 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05860060249893788		[learning rate: 0.011995]
	Learning Rate: 0.0119954
	LOSS [training: 0.05860060249893788 | validation: 0.056684197664117086]
	TIME [epoch: 41.2 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04464290997446008		[learning rate: 0.011979]
	Learning Rate: 0.0119792
	LOSS [training: 0.04464290997446008 | validation: 0.214908747916942]
	TIME [epoch: 41.2 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08799949792457493		[learning rate: 0.011963]
	Learning Rate: 0.011963
	LOSS [training: 0.08799949792457493 | validation: 0.02323935348403659]
	TIME [epoch: 41.2 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024531885060274646		[learning rate: 0.011947]
	Learning Rate: 0.0119468
	LOSS [training: 0.024531885060274646 | validation: 0.09051748961068562]
	TIME [epoch: 41.2 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0917445113283938		[learning rate: 0.011931]
	Learning Rate: 0.0119306
	LOSS [training: 0.0917445113283938 | validation: 0.09489717336836703]
	TIME [epoch: 41.2 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08535460002186838		[learning rate: 0.011914]
	Learning Rate: 0.0119144
	LOSS [training: 0.08535460002186838 | validation: 0.14719581895914446]
	TIME [epoch: 41.2 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054041015542698634		[learning rate: 0.011898]
	Learning Rate: 0.0118982
	LOSS [training: 0.054041015542698634 | validation: 0.07703070730011075]
	TIME [epoch: 41.2 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1062481624791134		[learning rate: 0.011882]
	Learning Rate: 0.0118819
	LOSS [training: 0.1062481624791134 | validation: 0.12572298247444785]
	TIME [epoch: 41.1 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08309283932597814		[learning rate: 0.011866]
	Learning Rate: 0.0118657
	LOSS [training: 0.08309283932597814 | validation: 0.019764461961411246]
	TIME [epoch: 41.2 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0759987937183767		[learning rate: 0.011849]
	Learning Rate: 0.0118495
	LOSS [training: 0.0759987937183767 | validation: 0.08612159468133064]
	TIME [epoch: 41.2 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08337769313162496		[learning rate: 0.011833]
	Learning Rate: 0.0118332
	LOSS [training: 0.08337769313162496 | validation: 0.1833086516573238]
	TIME [epoch: 41.2 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058032087232130186		[learning rate: 0.011817]
	Learning Rate: 0.011817
	LOSS [training: 0.058032087232130186 | validation: 0.024403503050833684]
	TIME [epoch: 41.2 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03604253751525744		[learning rate: 0.011801]
	Learning Rate: 0.0118007
	LOSS [training: 0.03604253751525744 | validation: 0.02235792052629553]
	TIME [epoch: 41.2 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030259433169844273		[learning rate: 0.011784]
	Learning Rate: 0.0117844
	LOSS [training: 0.030259433169844273 | validation: 0.03180635246495143]
	TIME [epoch: 41.2 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056438809502642434		[learning rate: 0.011768]
	Learning Rate: 0.0117682
	LOSS [training: 0.056438809502642434 | validation: 0.07515956238294731]
	TIME [epoch: 41.2 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08895411867551523		[learning rate: 0.011752]
	Learning Rate: 0.0117519
	LOSS [training: 0.08895411867551523 | validation: 0.03922360196491962]
	TIME [epoch: 41.2 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026883615732457036		[learning rate: 0.011736]
	Learning Rate: 0.0117356
	LOSS [training: 0.026883615732457036 | validation: 0.012079947506741313]
	TIME [epoch: 41.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_895.pth
	Model improved!!!
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07601992796117271		[learning rate: 0.011719]
	Learning Rate: 0.0117194
	LOSS [training: 0.07601992796117271 | validation: 0.04262945642871811]
	TIME [epoch: 41.2 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10175148136209425		[learning rate: 0.011703]
	Learning Rate: 0.0117031
	LOSS [training: 0.10175148136209425 | validation: 0.08611057303368175]
	TIME [epoch: 41.2 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18175983436987944		[learning rate: 0.011687]
	Learning Rate: 0.0116868
	LOSS [training: 0.18175983436987944 | validation: 0.03463286436397193]
	TIME [epoch: 41.2 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029715616049699627		[learning rate: 0.01167]
	Learning Rate: 0.0116705
	LOSS [training: 0.029715616049699627 | validation: 0.035520369884260654]
	TIME [epoch: 41.2 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0409408770202503		[learning rate: 0.011654]
	Learning Rate: 0.0116542
	LOSS [training: 0.0409408770202503 | validation: 0.05499496887098308]
	TIME [epoch: 41.2 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09218822650146474		[learning rate: 0.011638]
	Learning Rate: 0.0116379
	LOSS [training: 0.09218822650146474 | validation: 0.02908415323105565]
	TIME [epoch: 41.2 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02586567395663734		[learning rate: 0.011622]
	Learning Rate: 0.0116216
	LOSS [training: 0.02586567395663734 | validation: 0.021627602010935575]
	TIME [epoch: 41.2 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030676609672565552		[learning rate: 0.011605]
	Learning Rate: 0.0116053
	LOSS [training: 0.030676609672565552 | validation: 0.05349122694144279]
	TIME [epoch: 41.2 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09269533661892723		[learning rate: 0.011589]
	Learning Rate: 0.011589
	LOSS [training: 0.09269533661892723 | validation: 0.06638558572863837]
	TIME [epoch: 41.1 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06279046194402964		[learning rate: 0.011573]
	Learning Rate: 0.0115726
	LOSS [training: 0.06279046194402964 | validation: 0.019967135485241913]
	TIME [epoch: 41.2 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04074678855785255		[learning rate: 0.011556]
	Learning Rate: 0.0115563
	LOSS [training: 0.04074678855785255 | validation: 0.12816676085662004]
	TIME [epoch: 41.2 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06621344642863065		[learning rate: 0.01154]
	Learning Rate: 0.01154
	LOSS [training: 0.06621344642863065 | validation: 0.046475844795426526]
	TIME [epoch: 41.2 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17589186091780426		[learning rate: 0.011524]
	Learning Rate: 0.0115237
	LOSS [training: 0.17589186091780426 | validation: 0.022683632809443854]
	TIME [epoch: 41.2 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018354347531244625		[learning rate: 0.011507]
	Learning Rate: 0.0115073
	LOSS [training: 0.018354347531244625 | validation: 0.012933185175479462]
	TIME [epoch: 41.2 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03643608921903721		[learning rate: 0.011491]
	Learning Rate: 0.011491
	LOSS [training: 0.03643608921903721 | validation: 0.024037619359701376]
	TIME [epoch: 41.1 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017190840151495607		[learning rate: 0.011475]
	Learning Rate: 0.0114746
	LOSS [training: 0.017190840151495607 | validation: 0.014137527850322384]
	TIME [epoch: 41.2 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023706820332460292		[learning rate: 0.011458]
	Learning Rate: 0.0114583
	LOSS [training: 0.023706820332460292 | validation: 0.10647288043328644]
	TIME [epoch: 41.2 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053199867578882384		[learning rate: 0.011442]
	Learning Rate: 0.0114419
	LOSS [training: 0.053199867578882384 | validation: 0.01356329685515905]
	TIME [epoch: 41.2 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07113471758336383		[learning rate: 0.011426]
	Learning Rate: 0.0114256
	LOSS [training: 0.07113471758336383 | validation: 0.0962770425480918]
	TIME [epoch: 41.2 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04278405018944237		[learning rate: 0.011409]
	Learning Rate: 0.0114092
	LOSS [training: 0.04278405018944237 | validation: 0.01825695099393452]
	TIME [epoch: 41.2 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04241075186286608		[learning rate: 0.011393]
	Learning Rate: 0.0113929
	LOSS [training: 0.04241075186286608 | validation: 0.04737564902032407]
	TIME [epoch: 41.2 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.049458079931718954		[learning rate: 0.011376]
	Learning Rate: 0.0113765
	LOSS [training: 0.049458079931718954 | validation: 0.01965789166879835]
	TIME [epoch: 41.2 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01931730072829379		[learning rate: 0.01136]
	Learning Rate: 0.0113601
	LOSS [training: 0.01931730072829379 | validation: 0.017919033801619487]
	TIME [epoch: 41.2 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07050808569072001		[learning rate: 0.011344]
	Learning Rate: 0.0113437
	LOSS [training: 0.07050808569072001 | validation: 0.1592857579159182]
	TIME [epoch: 41.2 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18561685309396297		[learning rate: 0.011327]
	Learning Rate: 0.0113274
	LOSS [training: 0.18561685309396297 | validation: 0.02547703724928991]
	TIME [epoch: 41.2 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02021079120699223		[learning rate: 0.011311]
	Learning Rate: 0.011311
	LOSS [training: 0.02021079120699223 | validation: 0.013780866557083334]
	TIME [epoch: 41.2 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0480001848645609		[learning rate: 0.011295]
	Learning Rate: 0.0112946
	LOSS [training: 0.0480001848645609 | validation: 0.1551868485299529]
	TIME [epoch: 41.2 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09682250207282843		[learning rate: 0.011278]
	Learning Rate: 0.0112782
	LOSS [training: 0.09682250207282843 | validation: 0.013480006240096829]
	TIME [epoch: 41.1 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022023344331576024		[learning rate: 0.011262]
	Learning Rate: 0.0112618
	LOSS [training: 0.022023344331576024 | validation: 0.027519261572857338]
	TIME [epoch: 41.2 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029091564590955354		[learning rate: 0.011245]
	Learning Rate: 0.0112454
	LOSS [training: 0.029091564590955354 | validation: 0.05919841739466088]
	TIME [epoch: 41.2 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10336563048692952		[learning rate: 0.011229]
	Learning Rate: 0.011229
	LOSS [training: 0.10336563048692952 | validation: 0.04310418989172336]
	TIME [epoch: 41.2 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029786079526042505		[learning rate: 0.011213]
	Learning Rate: 0.0112126
	LOSS [training: 0.029786079526042505 | validation: 0.01972755706145523]
	TIME [epoch: 41.2 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05784848104472006		[learning rate: 0.011196]
	Learning Rate: 0.0111962
	LOSS [training: 0.05784848104472006 | validation: 0.03476032958390812]
	TIME [epoch: 41.2 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061455465751191964		[learning rate: 0.01118]
	Learning Rate: 0.0111798
	LOSS [training: 0.061455465751191964 | validation: 0.03619853146482846]
	TIME [epoch: 41.2 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057500973704071354		[learning rate: 0.011163]
	Learning Rate: 0.0111634
	LOSS [training: 0.057500973704071354 | validation: 0.026573482899648275]
	TIME [epoch: 41.2 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021668144831103242		[learning rate: 0.011147]
	Learning Rate: 0.011147
	LOSS [training: 0.021668144831103242 | validation: 0.030639602512660236]
	TIME [epoch: 41.2 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058373231127305775		[learning rate: 0.011131]
	Learning Rate: 0.0111305
	LOSS [training: 0.058373231127305775 | validation: 0.03517678043130227]
	TIME [epoch: 41.2 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0397836160630296		[learning rate: 0.011114]
	Learning Rate: 0.0111141
	LOSS [training: 0.0397836160630296 | validation: 0.055037095035395014]
	TIME [epoch: 41.2 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03645060241075669		[learning rate: 0.011098]
	Learning Rate: 0.0110977
	LOSS [training: 0.03645060241075669 | validation: 0.03681921308854344]
	TIME [epoch: 41.2 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044877478481385354		[learning rate: 0.011081]
	Learning Rate: 0.0110813
	LOSS [training: 0.044877478481385354 | validation: 0.024022067271282574]
	TIME [epoch: 41.2 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07012704200952777		[learning rate: 0.011065]
	Learning Rate: 0.0110648
	LOSS [training: 0.07012704200952777 | validation: 0.03572929582148491]
	TIME [epoch: 41.2 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026198700392416365		[learning rate: 0.011048]
	Learning Rate: 0.0110484
	LOSS [training: 0.026198700392416365 | validation: 0.012095660304064861]
	TIME [epoch: 41.2 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09298955287150126		[learning rate: 0.011032]
	Learning Rate: 0.011032
	LOSS [training: 0.09298955287150126 | validation: 0.022748776068642153]
	TIME [epoch: 41.2 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033875254755112966		[learning rate: 0.011016]
	Learning Rate: 0.0110155
	LOSS [training: 0.033875254755112966 | validation: 0.1115267396574544]
	TIME [epoch: 41.2 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13632188311817436		[learning rate: 0.010999]
	Learning Rate: 0.0109991
	LOSS [training: 0.13632188311817436 | validation: 0.05544358750017413]
	TIME [epoch: 41.2 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025430851059768896		[learning rate: 0.010983]
	Learning Rate: 0.0109826
	LOSS [training: 0.025430851059768896 | validation: 0.014530563701296585]
	TIME [epoch: 41.1 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021196234648007007		[learning rate: 0.010966]
	Learning Rate: 0.0109662
	LOSS [training: 0.021196234648007007 | validation: 0.022979088109543376]
	TIME [epoch: 41.2 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021268008033540713		[learning rate: 0.01095]
	Learning Rate: 0.0109497
	LOSS [training: 0.021268008033540713 | validation: 0.023995621918601648]
	TIME [epoch: 41.2 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07703923815730934		[learning rate: 0.010933]
	Learning Rate: 0.0109333
	LOSS [training: 0.07703923815730934 | validation: 0.0593645723826596]
	TIME [epoch: 41.2 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029738953162424156		[learning rate: 0.010917]
	Learning Rate: 0.0109168
	LOSS [training: 0.029738953162424156 | validation: 0.02683080849409568]
	TIME [epoch: 41.2 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07265526515263519		[learning rate: 0.0109]
	Learning Rate: 0.0109004
	LOSS [training: 0.07265526515263519 | validation: 0.02931544930748266]
	TIME [epoch: 41.2 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02226615268739513		[learning rate: 0.010884]
	Learning Rate: 0.0108839
	LOSS [training: 0.02226615268739513 | validation: 0.017238247522377004]
	TIME [epoch: 41.2 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08291341765662721		[learning rate: 0.010867]
	Learning Rate: 0.0108674
	LOSS [training: 0.08291341765662721 | validation: 0.05204161801028018]
	TIME [epoch: 41.1 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11216379802524531		[learning rate: 0.010851]
	Learning Rate: 0.010851
	LOSS [training: 0.11216379802524531 | validation: 0.09650374753256982]
	TIME [epoch: 41.2 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.049862392309404875		[learning rate: 0.010834]
	Learning Rate: 0.0108345
	LOSS [training: 0.049862392309404875 | validation: 0.17357607692267762]
	TIME [epoch: 41.2 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06421360591883524		[learning rate: 0.010818]
	Learning Rate: 0.010818
	LOSS [training: 0.06421360591883524 | validation: 0.023450703590953036]
	TIME [epoch: 41.2 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04234841579425206		[learning rate: 0.010802]
	Learning Rate: 0.0108016
	LOSS [training: 0.04234841579425206 | validation: 0.017510621208526297]
	TIME [epoch: 41.2 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019639610028467633		[learning rate: 0.010785]
	Learning Rate: 0.0107851
	LOSS [training: 0.019639610028467633 | validation: 0.03144784722702099]
	TIME [epoch: 41.2 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03473455529098634		[learning rate: 0.010769]
	Learning Rate: 0.0107686
	LOSS [training: 0.03473455529098634 | validation: 0.025659274124317115]
	TIME [epoch: 41.2 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05759876385265285		[learning rate: 0.010752]
	Learning Rate: 0.0107521
	LOSS [training: 0.05759876385265285 | validation: 0.029011402711631724]
	TIME [epoch: 41.2 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024290587705021415		[learning rate: 0.010736]
	Learning Rate: 0.0107356
	LOSS [training: 0.024290587705021415 | validation: 0.015431773321526]
	TIME [epoch: 41.2 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02704391906088835		[learning rate: 0.010719]
	Learning Rate: 0.0107192
	LOSS [training: 0.02704391906088835 | validation: 0.016802464368134554]
	TIME [epoch: 41.2 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027699501322361333		[learning rate: 0.010703]
	Learning Rate: 0.0107027
	LOSS [training: 0.027699501322361333 | validation: 0.04682878120900845]
	TIME [epoch: 41.2 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07419034466004301		[learning rate: 0.010686]
	Learning Rate: 0.0106862
	LOSS [training: 0.07419034466004301 | validation: 0.12360332231897983]
	TIME [epoch: 41.2 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12551528345144417		[learning rate: 0.01067]
	Learning Rate: 0.0106697
	LOSS [training: 0.12551528345144417 | validation: 0.08375633562099491]
	TIME [epoch: 41.1 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09981191109096625		[learning rate: 0.010653]
	Learning Rate: 0.0106532
	LOSS [training: 0.09981191109096625 | validation: 0.053841967644030625]
	TIME [epoch: 41.2 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029460914043959216		[learning rate: 0.010637]
	Learning Rate: 0.0106367
	LOSS [training: 0.029460914043959216 | validation: 0.01639226157058033]
	TIME [epoch: 41.2 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020333497489982887		[learning rate: 0.01062]
	Learning Rate: 0.0106202
	LOSS [training: 0.020333497489982887 | validation: 0.23161325519613335]
	TIME [epoch: 41.2 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09213691827227707		[learning rate: 0.010604]
	Learning Rate: 0.0106037
	LOSS [training: 0.09213691827227707 | validation: 0.09710501218587858]
	TIME [epoch: 41.2 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051633841344130946		[learning rate: 0.010587]
	Learning Rate: 0.0105872
	LOSS [training: 0.051633841344130946 | validation: 0.022850137672460698]
	TIME [epoch: 41.2 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01774819347079393		[learning rate: 0.010571]
	Learning Rate: 0.0105707
	LOSS [training: 0.01774819347079393 | validation: 0.02637389778878147]
	TIME [epoch: 41.2 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08724546901043267		[learning rate: 0.010554]
	Learning Rate: 0.0105542
	LOSS [training: 0.08724546901043267 | validation: 0.02550047137404766]
	TIME [epoch: 41.2 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027323350707296412		[learning rate: 0.010538]
	Learning Rate: 0.0105377
	LOSS [training: 0.027323350707296412 | validation: 0.0611640133251679]
	TIME [epoch: 41.2 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05318477684635455		[learning rate: 0.010521]
	Learning Rate: 0.0105212
	LOSS [training: 0.05318477684635455 | validation: 0.02077364122613777]
	TIME [epoch: 41.2 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019712655422365145		[learning rate: 0.010505]
	Learning Rate: 0.0105047
	LOSS [training: 0.019712655422365145 | validation: 0.021616846714084644]
	TIME [epoch: 41.2 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07662300169166204		[learning rate: 0.010488]
	Learning Rate: 0.0104882
	LOSS [training: 0.07662300169166204 | validation: 0.029957297511781247]
	TIME [epoch: 41.2 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025569051243457826		[learning rate: 0.010472]
	Learning Rate: 0.0104717
	LOSS [training: 0.025569051243457826 | validation: 0.033201519938254265]
	TIME [epoch: 41.2 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05337916657258637		[learning rate: 0.010455]
	Learning Rate: 0.0104552
	LOSS [training: 0.05337916657258637 | validation: 0.023229823935860913]
	TIME [epoch: 41.1 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03872007898220596		[learning rate: 0.010439]
	Learning Rate: 0.0104387
	LOSS [training: 0.03872007898220596 | validation: 0.032046325376404276]
	TIME [epoch: 41.2 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06931664467011209		[learning rate: 0.010422]
	Learning Rate: 0.0104222
	LOSS [training: 0.06931664467011209 | validation: 0.012252496653118469]
	TIME [epoch: 41.2 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020553204152014514		[learning rate: 0.010406]
	Learning Rate: 0.0104057
	LOSS [training: 0.020553204152014514 | validation: 0.04206558232144447]
	TIME [epoch: 41.2 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03577426736906077		[learning rate: 0.010389]
	Learning Rate: 0.0103891
	LOSS [training: 0.03577426736906077 | validation: 0.0434001059589687]
	TIME [epoch: 41.1 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061485037326588714		[learning rate: 0.010373]
	Learning Rate: 0.0103726
	LOSS [training: 0.061485037326588714 | validation: 0.022198541132832622]
	TIME [epoch: 41.2 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01565079582969464		[learning rate: 0.010356]
	Learning Rate: 0.0103561
	LOSS [training: 0.01565079582969464 | validation: 0.01836756950760568]
	TIME [epoch: 41.1 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07371113474369197		[learning rate: 0.01034]
	Learning Rate: 0.0103396
	LOSS [training: 0.07371113474369197 | validation: 0.04017699250992064]
	TIME [epoch: 41.2 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026887145481937373		[learning rate: 0.010323]
	Learning Rate: 0.0103231
	LOSS [training: 0.026887145481937373 | validation: 0.03234055953967341]
	TIME [epoch: 41.2 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03043035385487649		[learning rate: 0.010307]
	Learning Rate: 0.0103066
	LOSS [training: 0.03043035385487649 | validation: 0.01917039139398615]
	TIME [epoch: 41.2 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02881489105830346		[learning rate: 0.01029]
	Learning Rate: 0.01029
	LOSS [training: 0.02881489105830346 | validation: 0.030361328417710207]
	TIME [epoch: 41.2 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07370425189727482		[learning rate: 0.010274]
	Learning Rate: 0.0102735
	LOSS [training: 0.07370425189727482 | validation: 0.03948827179326887]
	TIME [epoch: 41.1 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027294682335212407		[learning rate: 0.010257]
	Learning Rate: 0.010257
	LOSS [training: 0.027294682335212407 | validation: 0.01700314205665346]
	TIME [epoch: 41.1 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021575674268798355		[learning rate: 0.01024]
	Learning Rate: 0.0102405
	LOSS [training: 0.021575674268798355 | validation: 0.05249558536270438]
	TIME [epoch: 41.2 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0761520129355226		[learning rate: 0.010224]
	Learning Rate: 0.010224
	LOSS [training: 0.0761520129355226 | validation: 0.026660725889124243]
	TIME [epoch: 41.2 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029374301017529664		[learning rate: 0.010207]
	Learning Rate: 0.0102074
	LOSS [training: 0.029374301017529664 | validation: 0.09042788016129522]
	TIME [epoch: 41.2 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052799783624100545		[learning rate: 0.010191]
	Learning Rate: 0.0101909
	LOSS [training: 0.052799783624100545 | validation: 0.027439912001549948]
	TIME [epoch: 41.2 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0184334222989346		[learning rate: 0.010174]
	Learning Rate: 0.0101744
	LOSS [training: 0.0184334222989346 | validation: 0.015922387057204224]
	TIME [epoch: 41.2 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06978665437807247		[learning rate: 0.010158]
	Learning Rate: 0.0101579
	LOSS [training: 0.06978665437807247 | validation: 0.014666418130666582]
	TIME [epoch: 41.2 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02702456405022863		[learning rate: 0.010141]
	Learning Rate: 0.0101413
	LOSS [training: 0.02702456405022863 | validation: 0.033654407846200295]
	TIME [epoch: 41.2 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03302478337566154		[learning rate: 0.010125]
	Learning Rate: 0.0101248
	LOSS [training: 0.03302478337566154 | validation: 0.047264750249959464]
	TIME [epoch: 41.2 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07950833403514575		[learning rate: 0.010108]
	Learning Rate: 0.0101083
	LOSS [training: 0.07950833403514575 | validation: 0.04388845906395502]
	TIME [epoch: 41.2 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03347241434232275		[learning rate: 0.010092]
	Learning Rate: 0.0100918
	LOSS [training: 0.03347241434232275 | validation: 0.015855428540517826]
	TIME [epoch: 41.2 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10577559428836729		[learning rate: 0.010075]
	Learning Rate: 0.0100752
	LOSS [training: 0.10577559428836729 | validation: 0.015245639641407006]
	TIME [epoch: 41.2 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026040548274204663		[learning rate: 0.010059]
	Learning Rate: 0.0100587
	LOSS [training: 0.026040548274204663 | validation: 0.02327610153526107]
	TIME [epoch: 41.2 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06346846892441778		[learning rate: 0.010042]
	Learning Rate: 0.0100422
	LOSS [training: 0.06346846892441778 | validation: 0.015526168894865404]
	TIME [epoch: 41.2 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023016510217169068		[learning rate: 0.010026]
	Learning Rate: 0.0100257
	LOSS [training: 0.023016510217169068 | validation: 0.2081969473027131]
	TIME [epoch: 41.2 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08954219459580025		[learning rate: 0.010009]
	Learning Rate: 0.0100091
	LOSS [training: 0.08954219459580025 | validation: 0.0413027718247019]
	TIME [epoch: 41.2 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038413068172953396		[learning rate: 0.0099926]
	Learning Rate: 0.00999261
	LOSS [training: 0.038413068172953396 | validation: 0.0994001183679214]
	TIME [epoch: 181 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0417128963030599		[learning rate: 0.0099761]
	Learning Rate: 0.00997608
	LOSS [training: 0.0417128963030599 | validation: 0.009992868707192107]
	TIME [epoch: 87.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_1002.pth
	Model improved!!!
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025528799635263576		[learning rate: 0.0099596]
	Learning Rate: 0.00995955
	LOSS [training: 0.025528799635263576 | validation: 0.017186347634945046]
	TIME [epoch: 87.4 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03606556555246945		[learning rate: 0.009943]
	Learning Rate: 0.00994303
	LOSS [training: 0.03606556555246945 | validation: 0.02801549060086907]
	TIME [epoch: 87.4 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03898976619535918		[learning rate: 0.0099265]
	Learning Rate: 0.0099265
	LOSS [training: 0.03898976619535918 | validation: 0.10355420134152782]
	TIME [epoch: 87.4 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.105844936044197		[learning rate: 0.00991]
	Learning Rate: 0.00990997
	LOSS [training: 0.105844936044197 | validation: 0.07866128741016551]
	TIME [epoch: 87.5 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04529993900701122		[learning rate: 0.0098934]
	Learning Rate: 0.00989345
	LOSS [training: 0.04529993900701122 | validation: 0.016929830810078098]
	TIME [epoch: 87.5 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052965430678168265		[learning rate: 0.0098769]
	Learning Rate: 0.00987692
	LOSS [training: 0.052965430678168265 | validation: 0.05300650685050398]
	TIME [epoch: 87.5 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03166929777843315		[learning rate: 0.0098604]
	Learning Rate: 0.0098604
	LOSS [training: 0.03166929777843315 | validation: 0.016790487384058382]
	TIME [epoch: 87.5 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0202452098091968		[learning rate: 0.0098439]
	Learning Rate: 0.00984387
	LOSS [training: 0.0202452098091968 | validation: 0.020873809432787778]
	TIME [epoch: 87.4 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019272537299180006		[learning rate: 0.0098274]
	Learning Rate: 0.00982735
	LOSS [training: 0.019272537299180006 | validation: 0.018950771827102156]
	TIME [epoch: 87.4 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03250976351008113		[learning rate: 0.0098108]
	Learning Rate: 0.00981083
	LOSS [training: 0.03250976351008113 | validation: 0.04440306070870453]
	TIME [epoch: 87.4 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0739540656991139		[learning rate: 0.0097943]
	Learning Rate: 0.0097943
	LOSS [training: 0.0739540656991139 | validation: 0.011894574221778073]
	TIME [epoch: 87.5 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01786546411882052		[learning rate: 0.0097778]
	Learning Rate: 0.00977778
	LOSS [training: 0.01786546411882052 | validation: 0.023496148592227106]
	TIME [epoch: 87.5 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045056735864287024		[learning rate: 0.0097613]
	Learning Rate: 0.00976126
	LOSS [training: 0.045056735864287024 | validation: 0.013834106043324981]
	TIME [epoch: 87.4 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023577591069248626		[learning rate: 0.0097447]
	Learning Rate: 0.00974474
	LOSS [training: 0.023577591069248626 | validation: 0.048444532853231914]
	TIME [epoch: 87.5 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06954714277276589		[learning rate: 0.0097282]
	Learning Rate: 0.00972822
	LOSS [training: 0.06954714277276589 | validation: 0.02618221776471228]
	TIME [epoch: 87.5 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015319886764429089		[learning rate: 0.0097117]
	Learning Rate: 0.0097117
	LOSS [training: 0.015319886764429089 | validation: 0.016469447459450774]
	TIME [epoch: 87.5 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024519090645428312		[learning rate: 0.0096952]
	Learning Rate: 0.00969518
	LOSS [training: 0.024519090645428312 | validation: 0.02648612226427511]
	TIME [epoch: 87.5 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035995303016158324		[learning rate: 0.0096787]
	Learning Rate: 0.00967866
	LOSS [training: 0.035995303016158324 | validation: 0.03964035932114873]
	TIME [epoch: 87.5 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037275993009278766		[learning rate: 0.0096621]
	Learning Rate: 0.00966214
	LOSS [training: 0.037275993009278766 | validation: 0.11329875204622797]
	TIME [epoch: 87.4 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048360965763383446		[learning rate: 0.0096456]
	Learning Rate: 0.00964563
	LOSS [training: 0.048360965763383446 | validation: 0.03811569527825611]
	TIME [epoch: 87.4 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047159581739496755		[learning rate: 0.0096291]
	Learning Rate: 0.00962911
	LOSS [training: 0.047159581739496755 | validation: 0.016366989573003515]
	TIME [epoch: 87.4 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020139014876942717		[learning rate: 0.0096126]
	Learning Rate: 0.0096126
	LOSS [training: 0.020139014876942717 | validation: 0.026311577424695747]
	TIME [epoch: 87.4 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043344910125424366		[learning rate: 0.0095961]
	Learning Rate: 0.00959609
	LOSS [training: 0.043344910125424366 | validation: 0.037805979178793386]
	TIME [epoch: 87.5 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039109899970082726		[learning rate: 0.0095796]
	Learning Rate: 0.00957957
	LOSS [training: 0.039109899970082726 | validation: 0.01489105316601849]
	TIME [epoch: 87.5 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05687599313690221		[learning rate: 0.0095631]
	Learning Rate: 0.00956306
	LOSS [training: 0.05687599313690221 | validation: 0.01568754511055523]
	TIME [epoch: 87.4 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019662547496331433		[learning rate: 0.0095466]
	Learning Rate: 0.00954655
	LOSS [training: 0.019662547496331433 | validation: 0.03709722126521771]
	TIME [epoch: 87.5 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033805154546740476		[learning rate: 0.00953]
	Learning Rate: 0.00953004
	LOSS [training: 0.033805154546740476 | validation: 0.01301973953343372]
	TIME [epoch: 87.5 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06577060648298128		[learning rate: 0.0095135]
	Learning Rate: 0.00951354
	LOSS [training: 0.06577060648298128 | validation: 0.010852193643220001]
	TIME [epoch: 87.4 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01727460697981789		[learning rate: 0.009497]
	Learning Rate: 0.00949703
	LOSS [training: 0.01727460697981789 | validation: 0.07021454716481847]
	TIME [epoch: 87.4 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043281716433358895		[learning rate: 0.0094805]
	Learning Rate: 0.00948053
	LOSS [training: 0.043281716433358895 | validation: 0.025955125954479536]
	TIME [epoch: 87.5 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026762040741399165		[learning rate: 0.009464]
	Learning Rate: 0.00946402
	LOSS [training: 0.026762040741399165 | validation: 0.02741911220380698]
	TIME [epoch: 87.4 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0315002417821008		[learning rate: 0.0094475]
	Learning Rate: 0.00944752
	LOSS [training: 0.0315002417821008 | validation: 0.21141425218180213]
	TIME [epoch: 87.5 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07828729495051855		[learning rate: 0.009431]
	Learning Rate: 0.00943102
	LOSS [training: 0.07828729495051855 | validation: 0.011887638011477575]
	TIME [epoch: 87.5 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02354742211039494		[learning rate: 0.0094145]
	Learning Rate: 0.00941452
	LOSS [training: 0.02354742211039494 | validation: 0.025313057839785647]
	TIME [epoch: 87.5 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06775283654560284		[learning rate: 0.009398]
	Learning Rate: 0.00939803
	LOSS [training: 0.06775283654560284 | validation: 0.02053985122339171]
	TIME [epoch: 87.5 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04363639976930109		[learning rate: 0.0093815]
	Learning Rate: 0.00938153
	LOSS [training: 0.04363639976930109 | validation: 0.013157041571951345]
	TIME [epoch: 87.5 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07358716564776444		[learning rate: 0.009365]
	Learning Rate: 0.00936504
	LOSS [training: 0.07358716564776444 | validation: 0.018090316355916774]
	TIME [epoch: 87.5 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02043142154135814		[learning rate: 0.0093485]
	Learning Rate: 0.00934855
	LOSS [training: 0.02043142154135814 | validation: 0.015712023068382965]
	TIME [epoch: 87.5 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02259168830773401		[learning rate: 0.0093321]
	Learning Rate: 0.00933206
	LOSS [training: 0.02259168830773401 | validation: 0.03361219135613715]
	TIME [epoch: 87.4 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028402276425279836		[learning rate: 0.0093156]
	Learning Rate: 0.00931557
	LOSS [training: 0.028402276425279836 | validation: 0.02183736838231191]
	TIME [epoch: 87.4 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02999683141525411		[learning rate: 0.0092991]
	Learning Rate: 0.00929908
	LOSS [training: 0.02999683141525411 | validation: 0.028528927540460228]
	TIME [epoch: 87.5 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02549592248818356		[learning rate: 0.0092826]
	Learning Rate: 0.0092826
	LOSS [training: 0.02549592248818356 | validation: 0.024050726370709846]
	TIME [epoch: 87.5 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05687576861203607		[learning rate: 0.0092661]
	Learning Rate: 0.00926612
	LOSS [training: 0.05687576861203607 | validation: 0.049557282174229715]
	TIME [epoch: 87.4 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030831485294035316		[learning rate: 0.0092496]
	Learning Rate: 0.00924964
	LOSS [training: 0.030831485294035316 | validation: 0.013450687335682223]
	TIME [epoch: 87.5 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016627471324462455		[learning rate: 0.0092332]
	Learning Rate: 0.00923316
	LOSS [training: 0.016627471324462455 | validation: 0.06070575632913345]
	TIME [epoch: 87.4 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0517971952745741		[learning rate: 0.0092167]
	Learning Rate: 0.00921668
	LOSS [training: 0.0517971952745741 | validation: 0.018060475888156024]
	TIME [epoch: 87.4 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02623434040412551		[learning rate: 0.0092002]
	Learning Rate: 0.00920021
	LOSS [training: 0.02623434040412551 | validation: 0.023623612684983794]
	TIME [epoch: 87.5 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021384868032535028		[learning rate: 0.0091837]
	Learning Rate: 0.00918374
	LOSS [training: 0.021384868032535028 | validation: 0.018833701977932847]
	TIME [epoch: 87.5 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02277963863572248		[learning rate: 0.0091673]
	Learning Rate: 0.00916727
	LOSS [training: 0.02277963863572248 | validation: 0.017197909990653622]
	TIME [epoch: 87.5 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032670609148517056		[learning rate: 0.0091508]
	Learning Rate: 0.0091508
	LOSS [training: 0.032670609148517056 | validation: 0.0981005089940129]
	TIME [epoch: 87.6 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052363418802772795		[learning rate: 0.0091343]
	Learning Rate: 0.00913434
	LOSS [training: 0.052363418802772795 | validation: 0.06246499744544258]
	TIME [epoch: 87.5 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03831768414126309		[learning rate: 0.0091179]
	Learning Rate: 0.00911787
	LOSS [training: 0.03831768414126309 | validation: 0.024974584220169155]
	TIME [epoch: 87.6 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01516607975169819		[learning rate: 0.0091014]
	Learning Rate: 0.00910141
	LOSS [training: 0.01516607975169819 | validation: 0.036955391932098874]
	TIME [epoch: 87.5 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032618418806752786		[learning rate: 0.009085]
	Learning Rate: 0.00908496
	LOSS [training: 0.032618418806752786 | validation: 0.02254561731759783]
	TIME [epoch: 87.6 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04012503737928557		[learning rate: 0.0090685]
	Learning Rate: 0.0090685
	LOSS [training: 0.04012503737928557 | validation: 0.06303336570022743]
	TIME [epoch: 87.5 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04456277203393796		[learning rate: 0.009052]
	Learning Rate: 0.00905205
	LOSS [training: 0.04456277203393796 | validation: 0.014324796127556352]
	TIME [epoch: 87.6 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03780322900814548		[learning rate: 0.0090356]
	Learning Rate: 0.0090356
	LOSS [training: 0.03780322900814548 | validation: 0.024388850111208067]
	TIME [epoch: 87.5 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01777600088282484		[learning rate: 0.0090192]
	Learning Rate: 0.00901915
	LOSS [training: 0.01777600088282484 | validation: 0.014390312626330545]
	TIME [epoch: 87.5 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03709826027955762		[learning rate: 0.0090027]
	Learning Rate: 0.00900271
	LOSS [training: 0.03709826027955762 | validation: 0.02728737968514775]
	TIME [epoch: 87.6 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04506529299390985		[learning rate: 0.0089863]
	Learning Rate: 0.00898627
	LOSS [training: 0.04506529299390985 | validation: 0.018840909037404818]
	TIME [epoch: 87.5 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016084029484823213		[learning rate: 0.0089698]
	Learning Rate: 0.00896983
	LOSS [training: 0.016084029484823213 | validation: 0.009610680755423602]
	TIME [epoch: 87.6 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_1063.pth
	Model improved!!!
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0746128484786959		[learning rate: 0.0089534]
	Learning Rate: 0.00895339
	LOSS [training: 0.0746128484786959 | validation: 0.031227695992957105]
	TIME [epoch: 87.5 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0218440195967296		[learning rate: 0.008937]
	Learning Rate: 0.00893696
	LOSS [training: 0.0218440195967296 | validation: 0.0166066170510951]
	TIME [epoch: 87.4 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05132871964754803		[learning rate: 0.0089205]
	Learning Rate: 0.00892053
	LOSS [training: 0.05132871964754803 | validation: 0.022554812186323346]
	TIME [epoch: 87.4 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018035667476710728		[learning rate: 0.0089041]
	Learning Rate: 0.0089041
	LOSS [training: 0.018035667476710728 | validation: 0.01354201629155533]
	TIME [epoch: 87.4 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01561684670690954		[learning rate: 0.0088877]
	Learning Rate: 0.00888767
	LOSS [training: 0.01561684670690954 | validation: 0.11595477376309048]
	TIME [epoch: 87.5 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04726936963041308		[learning rate: 0.0088713]
	Learning Rate: 0.00887125
	LOSS [training: 0.04726936963041308 | validation: 0.009341253062809236]
	TIME [epoch: 87.5 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_1069.pth
	Model improved!!!
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028782805390719424		[learning rate: 0.0088548]
	Learning Rate: 0.00885484
	LOSS [training: 0.028782805390719424 | validation: 0.03621673438496044]
	TIME [epoch: 87.4 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024418349679927592		[learning rate: 0.0088384]
	Learning Rate: 0.00883842
	LOSS [training: 0.024418349679927592 | validation: 0.03812948360052758]
	TIME [epoch: 87.4 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056108607344074315		[learning rate: 0.008822]
	Learning Rate: 0.00882201
	LOSS [training: 0.056108607344074315 | validation: 0.013929728647178058]
	TIME [epoch: 87.4 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015529514782812483		[learning rate: 0.0088056]
	Learning Rate: 0.0088056
	LOSS [training: 0.015529514782812483 | validation: 0.02861071582491209]
	TIME [epoch: 87.4 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02637537490885089		[learning rate: 0.0087892]
	Learning Rate: 0.00878919
	LOSS [training: 0.02637537490885089 | validation: 0.011340953090407446]
	TIME [epoch: 87.4 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013534678977097245		[learning rate: 0.0087728]
	Learning Rate: 0.00877279
	LOSS [training: 0.013534678977097245 | validation: 0.017612657338906594]
	TIME [epoch: 87.4 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017990021995909853		[learning rate: 0.0087564]
	Learning Rate: 0.00875639
	LOSS [training: 0.017990021995909853 | validation: 0.015769823432413514]
	TIME [epoch: 87.4 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052813456086762564		[learning rate: 0.00874]
	Learning Rate: 0.00874
	LOSS [training: 0.052813456086762564 | validation: 0.020557350203739888]
	TIME [epoch: 87.4 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06205566025130249		[learning rate: 0.0087236]
	Learning Rate: 0.00872361
	LOSS [training: 0.06205566025130249 | validation: 0.09698062127161305]
	TIME [epoch: 87.4 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0482237569733132		[learning rate: 0.0087072]
	Learning Rate: 0.00870722
	LOSS [training: 0.0482237569733132 | validation: 0.03656846921151635]
	TIME [epoch: 87.4 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019289704624905455		[learning rate: 0.0086908]
	Learning Rate: 0.00869083
	LOSS [training: 0.019289704624905455 | validation: 0.009559194886442555]
	TIME [epoch: 87.4 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015482718114518375		[learning rate: 0.0086745]
	Learning Rate: 0.00867445
	LOSS [training: 0.015482718114518375 | validation: 0.014833825141756941]
	TIME [epoch: 87.4 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020562472185474856		[learning rate: 0.0086581]
	Learning Rate: 0.00865807
	LOSS [training: 0.020562472185474856 | validation: 0.0915643473370826]
	TIME [epoch: 87.4 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05754944232964456		[learning rate: 0.0086417]
	Learning Rate: 0.0086417
	LOSS [training: 0.05754944232964456 | validation: 0.011906126906281378]
	TIME [epoch: 87.4 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020626678952423004		[learning rate: 0.0086253]
	Learning Rate: 0.00862533
	LOSS [training: 0.020626678952423004 | validation: 0.03248634818543286]
	TIME [epoch: 87.4 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05188196366599828		[learning rate: 0.008609]
	Learning Rate: 0.00860896
	LOSS [training: 0.05188196366599828 | validation: 0.03618120497549918]
	TIME [epoch: 87.4 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02331598985466912		[learning rate: 0.0085926]
	Learning Rate: 0.0085926
	LOSS [training: 0.02331598985466912 | validation: 0.05242636395543847]
	TIME [epoch: 87.4 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029226850511634317		[learning rate: 0.0085762]
	Learning Rate: 0.00857624
	LOSS [training: 0.029226850511634317 | validation: 0.01010310427498359]
	TIME [epoch: 87.5 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028539783487221988		[learning rate: 0.0085599]
	Learning Rate: 0.00855989
	LOSS [training: 0.028539783487221988 | validation: 0.018460573000806522]
	TIME [epoch: 87.4 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03319243093850726		[learning rate: 0.0085435]
	Learning Rate: 0.00854354
	LOSS [training: 0.03319243093850726 | validation: 0.02006283802568727]
	TIME [epoch: 87.4 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0458950677968162		[learning rate: 0.0085272]
	Learning Rate: 0.00852719
	LOSS [training: 0.0458950677968162 | validation: 0.021092693880167105]
	TIME [epoch: 87.4 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02118934187628199		[learning rate: 0.0085108]
	Learning Rate: 0.00851085
	LOSS [training: 0.02118934187628199 | validation: 0.014435172401736158]
	TIME [epoch: 87.4 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03202260867688305		[learning rate: 0.0084945]
	Learning Rate: 0.00849451
	LOSS [training: 0.03202260867688305 | validation: 0.020023544956300245]
	TIME [epoch: 87.4 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045027960025629046		[learning rate: 0.0084782]
	Learning Rate: 0.00847817
	LOSS [training: 0.045027960025629046 | validation: 0.03810790045852494]
	TIME [epoch: 87.4 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025067915750695113		[learning rate: 0.0084618]
	Learning Rate: 0.00846184
	LOSS [training: 0.025067915750695113 | validation: 0.013475248897064517]
	TIME [epoch: 87.4 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01611483720939688		[learning rate: 0.0084455]
	Learning Rate: 0.00844552
	LOSS [training: 0.01611483720939688 | validation: 0.022433705711071762]
	TIME [epoch: 87.5 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027775845530381513		[learning rate: 0.0084292]
	Learning Rate: 0.0084292
	LOSS [training: 0.027775845530381513 | validation: 0.08891436647233295]
	TIME [epoch: 87.4 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05072734378483207		[learning rate: 0.0084129]
	Learning Rate: 0.00841288
	LOSS [training: 0.05072734378483207 | validation: 0.04741070339063401]
	TIME [epoch: 87.4 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033501828725866885		[learning rate: 0.0083966]
	Learning Rate: 0.00839657
	LOSS [training: 0.033501828725866885 | validation: 0.018436311634137573]
	TIME [epoch: 87.5 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017971553507793496		[learning rate: 0.0083803]
	Learning Rate: 0.00838026
	LOSS [training: 0.017971553507793496 | validation: 0.03399765004920073]
	TIME [epoch: 87.4 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02376214518509196		[learning rate: 0.008364]
	Learning Rate: 0.00836395
	LOSS [training: 0.02376214518509196 | validation: 0.01899908062497952]
	TIME [epoch: 87.4 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03699259352268852		[learning rate: 0.0083477]
	Learning Rate: 0.00834765
	LOSS [training: 0.03699259352268852 | validation: 0.011215185440723831]
	TIME [epoch: 87.5 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010741689821206813		[learning rate: 0.0083314]
	Learning Rate: 0.00833136
	LOSS [training: 0.010741689821206813 | validation: 0.02645762976949992]
	TIME [epoch: 87.5 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051155759855811056		[learning rate: 0.0083151]
	Learning Rate: 0.00831507
	LOSS [training: 0.051155759855811056 | validation: 0.012593747781066153]
	TIME [epoch: 87.4 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017294932682540683		[learning rate: 0.0082988]
	Learning Rate: 0.00829878
	LOSS [training: 0.017294932682540683 | validation: 0.01530172118857029]
	TIME [epoch: 87.5 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015813003451487572		[learning rate: 0.0082825]
	Learning Rate: 0.0082825
	LOSS [training: 0.015813003451487572 | validation: 0.019117890019480603]
	TIME [epoch: 87.5 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06937122702140318		[learning rate: 0.0082662]
	Learning Rate: 0.00826622
	LOSS [training: 0.06937122702140318 | validation: 0.01005436498342938]
	TIME [epoch: 87.5 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021953144102217464		[learning rate: 0.0082499]
	Learning Rate: 0.00824995
	LOSS [training: 0.021953144102217464 | validation: 0.08439692897580288]
	TIME [epoch: 87.4 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028557299133200528		[learning rate: 0.0082337]
	Learning Rate: 0.00823368
	LOSS [training: 0.028557299133200528 | validation: 0.014957450678640745]
	TIME [epoch: 87.5 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026277189973279727		[learning rate: 0.0082174]
	Learning Rate: 0.00821742
	LOSS [training: 0.026277189973279727 | validation: 0.019535924634474868]
	TIME [epoch: 87.4 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08550342209090406		[learning rate: 0.0082012]
	Learning Rate: 0.00820116
	LOSS [training: 0.08550342209090406 | validation: 0.1533265208084299]
	TIME [epoch: 87.5 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05809534202633979		[learning rate: 0.0081849]
	Learning Rate: 0.00818491
	LOSS [training: 0.05809534202633979 | validation: 0.016366526739432195]
	TIME [epoch: 87.4 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01642701806616196		[learning rate: 0.0081687]
	Learning Rate: 0.00816866
	LOSS [training: 0.01642701806616196 | validation: 0.012236800813863986]
	TIME [epoch: 87.4 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09565595595933338		[learning rate: 0.0081524]
	Learning Rate: 0.00815242
	LOSS [training: 0.09565595595933338 | validation: 0.019135604912243066]
	TIME [epoch: 87.5 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01874572731168251		[learning rate: 0.0081362]
	Learning Rate: 0.00813618
	LOSS [training: 0.01874572731168251 | validation: 0.009690990361801889]
	TIME [epoch: 87.5 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0452984795406981		[learning rate: 0.00812]
	Learning Rate: 0.00811995
	LOSS [training: 0.0452984795406981 | validation: 0.03134238148752102]
	TIME [epoch: 87.5 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02096579890688893		[learning rate: 0.0081037]
	Learning Rate: 0.00810372
	LOSS [training: 0.02096579890688893 | validation: 0.01065422049712805]
	TIME [epoch: 87.5 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009378926429543953		[learning rate: 0.0080875]
	Learning Rate: 0.0080875
	LOSS [training: 0.009378926429543953 | validation: 0.019854324779495378]
	TIME [epoch: 87.4 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022835731725151524		[learning rate: 0.0080713]
	Learning Rate: 0.00807128
	LOSS [training: 0.022835731725151524 | validation: 0.01886781548059223]
	TIME [epoch: 87.5 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015477612583566662		[learning rate: 0.0080551]
	Learning Rate: 0.00805507
	LOSS [training: 0.015477612583566662 | validation: 0.021812927705785697]
	TIME [epoch: 87.4 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03760594304389892		[learning rate: 0.0080389]
	Learning Rate: 0.00803887
	LOSS [training: 0.03760594304389892 | validation: 0.02876595422943265]
	TIME [epoch: 87.4 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03377649938405617		[learning rate: 0.0080227]
	Learning Rate: 0.00802267
	LOSS [training: 0.03377649938405617 | validation: 0.037899067797516305]
	TIME [epoch: 87.5 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02202442216472717		[learning rate: 0.0080065]
	Learning Rate: 0.00800647
	LOSS [training: 0.02202442216472717 | validation: 0.009815327128931642]
	TIME [epoch: 87.5 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014529123523045128		[learning rate: 0.0079903]
	Learning Rate: 0.00799028
	LOSS [training: 0.014529123523045128 | validation: 0.14189900602801753]
	TIME [epoch: 87.5 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05465422099648037		[learning rate: 0.0079741]
	Learning Rate: 0.0079741
	LOSS [training: 0.05465422099648037 | validation: 0.06924246072300529]
	TIME [epoch: 87.5 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03169604430617673		[learning rate: 0.0079579]
	Learning Rate: 0.00795792
	LOSS [training: 0.03169604430617673 | validation: 0.006847401361144273]
	TIME [epoch: 87.4 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_1125.pth
	Model improved!!!
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01576203856130296		[learning rate: 0.0079417]
	Learning Rate: 0.00794174
	LOSS [training: 0.01576203856130296 | validation: 0.014766492403381818]
	TIME [epoch: 87.4 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050918711464040596		[learning rate: 0.0079256]
	Learning Rate: 0.00792558
	LOSS [training: 0.050918711464040596 | validation: 0.024352098324895925]
	TIME [epoch: 87.4 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01848438160199814		[learning rate: 0.0079094]
	Learning Rate: 0.00790941
	LOSS [training: 0.01848438160199814 | validation: 0.015072890349977507]
	TIME [epoch: 87.4 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01625706976090706		[learning rate: 0.0078933]
	Learning Rate: 0.00789326
	LOSS [training: 0.01625706976090706 | validation: 0.026970639102055563]
	TIME [epoch: 87.4 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08786930510644439		[learning rate: 0.0078771]
	Learning Rate: 0.00787711
	LOSS [training: 0.08786930510644439 | validation: 0.011870201984681809]
	TIME [epoch: 87.4 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02153634104583834		[learning rate: 0.007861]
	Learning Rate: 0.00786096
	LOSS [training: 0.02153634104583834 | validation: 0.03215405543506788]
	TIME [epoch: 87.4 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018627006979390067		[learning rate: 0.0078448]
	Learning Rate: 0.00784482
	LOSS [training: 0.018627006979390067 | validation: 0.016050234112398355]
	TIME [epoch: 87.4 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02335786094247695		[learning rate: 0.0078287]
	Learning Rate: 0.00782869
	LOSS [training: 0.02335786094247695 | validation: 0.02438029540617917]
	TIME [epoch: 87.5 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02075716236856967		[learning rate: 0.0078126]
	Learning Rate: 0.00781256
	LOSS [training: 0.02075716236856967 | validation: 0.014959575152128715]
	TIME [epoch: 87.4 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044749554042247106		[learning rate: 0.0077964]
	Learning Rate: 0.00779644
	LOSS [training: 0.044749554042247106 | validation: 0.015659483448069028]
	TIME [epoch: 87.4 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016261002294001747		[learning rate: 0.0077803]
	Learning Rate: 0.00778033
	LOSS [training: 0.016261002294001747 | validation: 0.019642552959399748]
	TIME [epoch: 87.4 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052867970685161164		[learning rate: 0.0077642]
	Learning Rate: 0.00776422
	LOSS [training: 0.052867970685161164 | validation: 0.011870740582788433]
	TIME [epoch: 87.4 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027937053433686		[learning rate: 0.0077481]
	Learning Rate: 0.00774812
	LOSS [training: 0.027937053433686 | validation: 0.021830319328819806]
	TIME [epoch: 87.4 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022743459366546016		[learning rate: 0.007732]
	Learning Rate: 0.00773202
	LOSS [training: 0.022743459366546016 | validation: 0.035249729362899426]
	TIME [epoch: 87.4 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023106720085534292		[learning rate: 0.0077159]
	Learning Rate: 0.00771593
	LOSS [training: 0.023106720085534292 | validation: 0.015989874993175213]
	TIME [epoch: 87.4 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016425558917769926		[learning rate: 0.0076998]
	Learning Rate: 0.00769985
	LOSS [training: 0.016425558917769926 | validation: 0.020537967171937038]
	TIME [epoch: 87.5 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06631920223616346		[learning rate: 0.0076838]
	Learning Rate: 0.00768377
	LOSS [training: 0.06631920223616346 | validation: 0.018596083681878615]
	TIME [epoch: 87.4 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018215097009969535		[learning rate: 0.0076677]
	Learning Rate: 0.0076677
	LOSS [training: 0.018215097009969535 | validation: 0.017419608669006832]
	TIME [epoch: 87.4 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014486984014295672		[learning rate: 0.0076516]
	Learning Rate: 0.00765163
	LOSS [training: 0.014486984014295672 | validation: 0.027820279917604385]
	TIME [epoch: 87.4 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03786972606203429		[learning rate: 0.0076356]
	Learning Rate: 0.00763557
	LOSS [training: 0.03786972606203429 | validation: 0.015601697810590998]
	TIME [epoch: 87.4 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01492759511757144		[learning rate: 0.0076195]
	Learning Rate: 0.00761952
	LOSS [training: 0.01492759511757144 | validation: 0.01720944795667588]
	TIME [epoch: 87.4 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0267042186612002		[learning rate: 0.0076035]
	Learning Rate: 0.00760347
	LOSS [training: 0.0267042186612002 | validation: 0.012526143049909828]
	TIME [epoch: 87.4 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015744385899733787		[learning rate: 0.0075874]
	Learning Rate: 0.00758744
	LOSS [training: 0.015744385899733787 | validation: 0.046497753980809305]
	TIME [epoch: 87.4 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0436710352861873		[learning rate: 0.0075714]
	Learning Rate: 0.0075714
	LOSS [training: 0.0436710352861873 | validation: 0.020896105218100258]
	TIME [epoch: 87.4 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020503686226945934		[learning rate: 0.0075554]
	Learning Rate: 0.00755538
	LOSS [training: 0.020503686226945934 | validation: 0.015355050782704322]
	TIME [epoch: 87.4 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0411081098271778		[learning rate: 0.0075394]
	Learning Rate: 0.00753936
	LOSS [training: 0.0411081098271778 | validation: 0.01379960759618129]
	TIME [epoch: 87.5 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03315289838746853		[learning rate: 0.0075233]
	Learning Rate: 0.00752335
	LOSS [training: 0.03315289838746853 | validation: 0.009714627003551534]
	TIME [epoch: 87.5 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03331272753844261		[learning rate: 0.0075073]
	Learning Rate: 0.00750734
	LOSS [training: 0.03331272753844261 | validation: 0.04948294669126192]
	TIME [epoch: 87.4 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020821542876619997		[learning rate: 0.0074913]
	Learning Rate: 0.00749134
	LOSS [training: 0.020821542876619997 | validation: 0.008915585898633203]
	TIME [epoch: 87.4 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04156242208675619		[learning rate: 0.0074754]
	Learning Rate: 0.00747535
	LOSS [training: 0.04156242208675619 | validation: 0.015946465534689307]
	TIME [epoch: 87.4 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011852007151278609		[learning rate: 0.0074594]
	Learning Rate: 0.00745937
	LOSS [training: 0.011852007151278609 | validation: 0.03137554349051516]
	TIME [epoch: 87.4 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030430317496724434		[learning rate: 0.0074434]
	Learning Rate: 0.00744339
	LOSS [training: 0.030430317496724434 | validation: 0.0255310181589399]
	TIME [epoch: 87.5 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024807500063272692		[learning rate: 0.0074274]
	Learning Rate: 0.00742742
	LOSS [training: 0.024807500063272692 | validation: 0.008817161803479936]
	TIME [epoch: 87.5 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009835022510064179		[learning rate: 0.0074115]
	Learning Rate: 0.00741145
	LOSS [training: 0.009835022510064179 | validation: 0.006702717047449388]
	TIME [epoch: 87.4 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd5_20240707_125538/states/model_phi2_1a_v_mmd5_1159.pth
	Model improved!!!
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015148951456291117		[learning rate: 0.0073955]
	Learning Rate: 0.0073955
	LOSS [training: 0.015148951456291117 | validation: 0.056744184242003906]
	TIME [epoch: 87.5 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05324625069071784		[learning rate: 0.0073795]
	Learning Rate: 0.00737955
	LOSS [training: 0.05324625069071784 | validation: 0.024936398036812867]
	TIME [epoch: 87.3 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015272703427710851		[learning rate: 0.0073636]
	Learning Rate: 0.0073636
	LOSS [training: 0.015272703427710851 | validation: 0.009720533993705673]
	TIME [epoch: 87.4 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01442563134231652		[learning rate: 0.0073477]
	Learning Rate: 0.00734767
	LOSS [training: 0.01442563134231652 | validation: 0.0897384712284664]
	TIME [epoch: 87.4 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03335483250526444		[learning rate: 0.0073317]
	Learning Rate: 0.00733174
	LOSS [training: 0.03335483250526444 | validation: 0.016129589696142723]
	TIME [epoch: 87.4 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015535228572252254		[learning rate: 0.0073158]
	Learning Rate: 0.00731582
	LOSS [training: 0.015535228572252254 | validation: 0.02089727903091624]
	TIME [epoch: 87.5 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02946957770611521		[learning rate: 0.0072999]
	Learning Rate: 0.00729991
	LOSS [training: 0.02946957770611521 | validation: 0.028586980649468352]
	TIME [epoch: 87.4 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01846047334960038		[learning rate: 0.007284]
	Learning Rate: 0.007284
	LOSS [training: 0.01846047334960038 | validation: 0.0416747679896456]
	TIME [epoch: 87.4 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029165863478686133		[learning rate: 0.0072681]
	Learning Rate: 0.0072681
	LOSS [training: 0.029165863478686133 | validation: 0.037010717178327934]
	TIME [epoch: 87.4 sec]
EPOCH 1169/2000:
	Training over batches...
