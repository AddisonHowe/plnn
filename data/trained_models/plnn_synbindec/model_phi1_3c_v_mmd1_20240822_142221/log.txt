Args:
Namespace(name='model_phi1_3c_v_mmd1', outdir='out/model_training/model_phi1_3c_v_mmd1', training_data='data/training_data/data_phi1_3c/training', validation_data='data/training_data/data_phi1_3c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1314147127

Training model...

Saving initial model state to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.800287000876361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.800287000876361 | validation: 6.817490645982087]
	TIME [epoch: 29.1 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.570266746359375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.570266746359375 | validation: 5.340429618038101]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.173178513825987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.173178513825987 | validation: 4.932722014893699]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.1087156991561224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.1087156991561224 | validation: 4.728113318089357]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.064920353291872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.064920353291872 | validation: 4.421341858115947]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.511981784672971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.511981784672971 | validation: 4.531103779902428]
	TIME [epoch: 3.76 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.467600758352118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.467600758352118 | validation: 4.05210640985781]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.104203356508803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.104203356508803 | validation: 3.659739633430276]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9892369480325636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9892369480325636 | validation: 3.586314984425254]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8909801774403165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8909801774403165 | validation: 3.70344505730002]
	TIME [epoch: 3.78 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.810868120011358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.810868120011358 | validation: 3.632846485985047]
	TIME [epoch: 3.77 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7577955136280368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7577955136280368 | validation: 3.4909009762899945]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.707280988093769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.707280988093769 | validation: 3.4759310044807807]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.660485592341728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.660485592341728 | validation: 3.4780592929670506]
	TIME [epoch: 3.77 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6279426464296867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6279426464296867 | validation: 3.4114768528054884]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5967174295485576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5967174295485576 | validation: 3.439076943120689]
	TIME [epoch: 3.76 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6233624593815152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6233624593815152 | validation: 3.6328685128186553]
	TIME [epoch: 3.77 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.813544417403785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.813544417403785 | validation: 3.5962237716856467]
	TIME [epoch: 3.76 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6919771380581587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6919771380581587 | validation: 3.3100612943688135]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.495854544592808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.495854544592808 | validation: 3.275653180475314]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4769412410976113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4769412410976113 | validation: 3.3262036330114713]
	TIME [epoch: 3.78 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.485326158553486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.485326158553486 | validation: 3.2999055791408822]
	TIME [epoch: 3.77 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4915913445837328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4915913445837328 | validation: 3.3599923801747877]
	TIME [epoch: 3.76 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5447911208539358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5447911208539358 | validation: 3.2991094685493]
	TIME [epoch: 3.76 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.481682090594783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.481682090594783 | validation: 3.2563809703862265]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.425087130547053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.425087130547053 | validation: 3.1729182280836685]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3810136126872226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3810136126872226 | validation: 3.190958508993725]
	TIME [epoch: 3.76 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.361130775984208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.361130775984208 | validation: 3.149212170797665]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.353431242259743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.353431242259743 | validation: 3.179988739727336]
	TIME [epoch: 3.76 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3584179632914575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3584179632914575 | validation: 3.1768093860940727]
	TIME [epoch: 3.76 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.374381479375419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.374381479375419 | validation: 3.2207927716753972]
	TIME [epoch: 3.76 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.382251878528974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.382251878528974 | validation: 3.1156698911305027]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.326844284487935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.326844284487935 | validation: 3.108268928059465]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.295159905538002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.295159905538002 | validation: 3.0590748601955497]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2695477227186234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2695477227186234 | validation: 3.0609185605865967]
	TIME [epoch: 3.77 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2538990020232825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2538990020232825 | validation: 3.02552541153308]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2408265059302703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2408265059302703 | validation: 3.0537912654676025]
	TIME [epoch: 3.74 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2398443576684306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2398443576684306 | validation: 3.0496476936457135]
	TIME [epoch: 3.76 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2510654220825366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2510654220825366 | validation: 3.082044923799265]
	TIME [epoch: 3.77 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.266017001696269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.266017001696269 | validation: 3.0102195627388397]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2242005428012277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2242005428012277 | validation: 3.01596816343772]
	TIME [epoch: 3.78 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1929100349495303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1929100349495303 | validation: 2.953932047411167]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.162362455604025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.162362455604025 | validation: 2.9625280487060355]
	TIME [epoch: 3.78 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.144342217710479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.144342217710479 | validation: 2.93435956119437]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.141198224822883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.141198224822883 | validation: 2.953415424293155]
	TIME [epoch: 3.76 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.138646592757715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.138646592757715 | validation: 2.9284524239723866]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1393966643057594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1393966643057594 | validation: 2.9733380445887456]
	TIME [epoch: 3.77 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1559662004898845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1559662004898845 | validation: 2.922241107417742]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.118998064844457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.118998064844457 | validation: 2.9045504176522208]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0929277454108384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0929277454108384 | validation: 2.8577355976682703]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0584550451148527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0584550451148527 | validation: 2.862699258009878]
	TIME [epoch: 3.77 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0443537836222196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0443537836222196 | validation: 2.8349605608549395]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0293343858179718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0293343858179718 | validation: 2.834023263234218]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0184963222213153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0184963222213153 | validation: 2.8278579830629216]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.017813027613937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.017813027613937 | validation: 2.865518319722722]
	TIME [epoch: 3.77 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0565295787545477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0565295787545477 | validation: 2.857423994275832]
	TIME [epoch: 3.77 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0672086537600807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0672086537600807 | validation: 2.867109976330381]
	TIME [epoch: 3.78 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0608537119756023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0608537119756023 | validation: 2.789022616835954]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9724918940645844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9724918940645844 | validation: 2.757200325246639]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.955632206659309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.955632206659309 | validation: 2.7617647932104563]
	TIME [epoch: 3.76 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9432411388792814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9432411388792814 | validation: 2.7438523805696176]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.934861063448071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.934861063448071 | validation: 2.7583276060468966]
	TIME [epoch: 3.76 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9292348375192483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9292348375192483 | validation: 2.761368906453252]
	TIME [epoch: 3.76 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9542646700983175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9542646700983175 | validation: 2.8325245908087355]
	TIME [epoch: 3.76 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0112210300207107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0112210300207107 | validation: 2.7452691498656954]
	TIME [epoch: 3.76 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.929508987510043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.929508987510043 | validation: 2.7276299703999936]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8973427925494213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8973427925494213 | validation: 2.6990045034668078]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8776853156519233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8776853156519233 | validation: 2.683688823878209]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8655913368248127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8655913368248127 | validation: 2.6694956503087766]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8492836391225422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8492836391225422 | validation: 2.6725322529673186]
	TIME [epoch: 3.77 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8485655180131175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8485655180131175 | validation: 2.680703576196813]
	TIME [epoch: 3.76 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8466683091186393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8466683091186393 | validation: 2.7207165868241017]
	TIME [epoch: 3.75 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9027758283014453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9027758283014453 | validation: 2.88240897925448]
	TIME [epoch: 3.77 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0684318361999208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0684318361999208 | validation: 2.6830010265593796]
	TIME [epoch: 3.77 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8467322436082476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8467322436082476 | validation: 2.6515116322710472]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8397282049613604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8397282049613604 | validation: 2.6706324033896287]
	TIME [epoch: 3.77 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.83875872072696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.83875872072696 | validation: 2.6315314311024878]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.791995544752605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.791995544752605 | validation: 2.6132334865085087]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7800597886962977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7800597886962977 | validation: 2.596711359636235]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.773244075592777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.773244075592777 | validation: 2.5882357302512045]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.757159718225978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.757159718225978 | validation: 2.555386029229938]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7332577759348986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7332577759348986 | validation: 2.498794832066927]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6829249254034515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6829249254034515 | validation: 2.4319153739351473]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5957872961429693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5957872961429693 | validation: 2.3030049237286625]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4100258755232296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4100258755232296 | validation: 1.8013576368045598]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9189609788887196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9189609788887196 | validation: 1.7313310264210366]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.771227600303631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.771227600303631 | validation: 4.854625025221349]
	TIME [epoch: 3.77 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.6862317573227825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6862317573227825 | validation: 4.217215910595274]
	TIME [epoch: 3.77 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.028907876983074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.028907876983074 | validation: 1.54303818397457]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8207017548530953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8207017548530953 | validation: 1.730842815475988]
	TIME [epoch: 3.76 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8700416137072602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8700416137072602 | validation: 1.6574293247679586]
	TIME [epoch: 3.77 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.798846673615894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.798846673615894 | validation: 1.5280339505872211]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6159209964969614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6159209964969614 | validation: 1.2636380015476931]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3558245272352674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3558245272352674 | validation: 1.0663403342502218]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1332310357958684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1332310357958684 | validation: 0.9828054336962526]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9928828777233488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9928828777233488 | validation: 0.8806137919397212]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8633567182359662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8633567182359662 | validation: 0.9421597403092105]
	TIME [epoch: 3.77 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8994413481638835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8994413481638835 | validation: 1.0903672269299918]
	TIME [epoch: 3.77 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1918443940459007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1918443940459007 | validation: 1.073585831052179]
	TIME [epoch: 3.78 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1151039230917856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1151039230917856 | validation: 0.8352296536840229]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8295034395114184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8295034395114184 | validation: 0.8633349694881134]
	TIME [epoch: 3.76 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.85763504393779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.85763504393779 | validation: 0.8818860137750137]
	TIME [epoch: 3.77 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8687227123253587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8687227123253587 | validation: 0.8587850334357882]
	TIME [epoch: 3.76 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8621686061768068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8621686061768068 | validation: 0.8490563455684046]
	TIME [epoch: 3.78 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8062114096398996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8062114096398996 | validation: 0.7834583076299929]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7473315112268674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7473315112268674 | validation: 0.7624269149134095]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7527039216939392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7527039216939392 | validation: 0.8608115214937891]
	TIME [epoch: 3.76 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8695196259137782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8695196259137782 | validation: 0.8691735082204778]
	TIME [epoch: 3.76 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8840363332620537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8840363332620537 | validation: 0.8112713455734483]
	TIME [epoch: 3.76 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7688884036141426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7688884036141426 | validation: 0.7620925735796409]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.719761175964601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.719761175964601 | validation: 0.7492700452065137]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7281264527279215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7281264527279215 | validation: 0.7900076729464447]
	TIME [epoch: 3.77 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7466332425343021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7466332425343021 | validation: 0.7426201898726241]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7222972546465437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7222972546465437 | validation: 0.7359654453802454]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.69600122975531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.69600122975531 | validation: 0.7478065488526376]
	TIME [epoch: 3.77 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7235554238127536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7235554238127536 | validation: 0.8058473742255416]
	TIME [epoch: 3.77 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8146345970892448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8146345970892448 | validation: 0.8782158592957701]
	TIME [epoch: 3.76 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8834076279810743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8834076279810743 | validation: 0.7828611532267782]
	TIME [epoch: 3.75 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7676964687012924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7676964687012924 | validation: 0.752144212134613]
	TIME [epoch: 3.76 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.725307477473208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.725307477473208 | validation: 0.8360039470143761]
	TIME [epoch: 3.76 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7695931188973884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7695931188973884 | validation: 0.751029983771601]
	TIME [epoch: 3.75 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7265161101360761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7265161101360761 | validation: 0.7854453946398566]
	TIME [epoch: 3.72 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7552993818441224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7552993818441224 | validation: 0.8650451986600924]
	TIME [epoch: 3.72 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8275799501713442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8275799501713442 | validation: 0.8259925066103646]
	TIME [epoch: 3.76 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8167075351296444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8167075351296444 | validation: 0.7547395625915316]
	TIME [epoch: 3.77 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7297887523812557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7297887523812557 | validation: 0.8746201506372291]
	TIME [epoch: 4.01 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.805808465829925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.805808465829925 | validation: 0.7926581544607201]
	TIME [epoch: 3.74 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7954180744884087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7954180744884087 | validation: 0.7603053759137646]
	TIME [epoch: 3.77 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7280634237192442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7280634237192442 | validation: 0.7936719876011556]
	TIME [epoch: 3.78 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.729821180097573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.729821180097573 | validation: 0.7182406733504034]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7037043496851695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7037043496851695 | validation: 0.74866691653183]
	TIME [epoch: 3.77 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7092588501989988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7092588501989988 | validation: 0.8278406984916471]
	TIME [epoch: 3.77 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7528655910127604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7528655910127604 | validation: 0.7075107474832043]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7004645606956024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7004645606956024 | validation: 0.7824742706806559]
	TIME [epoch: 3.76 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7454897426724221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7454897426724221 | validation: 0.7598657280995536]
	TIME [epoch: 3.77 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7215414838083376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7215414838083376 | validation: 0.7234000552088949]
	TIME [epoch: 3.76 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.700405050779653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.700405050779653 | validation: 0.7416958310563928]
	TIME [epoch: 3.77 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6847591971921437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6847591971921437 | validation: 0.7029569573691817]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6693124371858425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6693124371858425 | validation: 0.7057977408216873]
	TIME [epoch: 3.76 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6683590169499806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6683590169499806 | validation: 0.7221815265131722]
	TIME [epoch: 3.77 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6790330002077535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6790330002077535 | validation: 0.8058202415028604]
	TIME [epoch: 3.79 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.756238779606025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.756238779606025 | validation: 0.9818962534263547]
	TIME [epoch: 3.77 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0291938013277127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0291938013277127 | validation: 0.706457023524333]
	TIME [epoch: 3.77 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7538587501825409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7538587501825409 | validation: 0.9389084054947286]
	TIME [epoch: 3.77 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8828806571405338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8828806571405338 | validation: 0.7311731777557201]
	TIME [epoch: 3.78 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.719201541731056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.719201541731056 | validation: 0.6993380121627332]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7099320774719919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7099320774719919 | validation: 0.7625160215936337]
	TIME [epoch: 3.77 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6879194556966749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6879194556966749 | validation: 0.714140044416039]
	TIME [epoch: 3.76 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6734243263685704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6734243263685704 | validation: 0.7272119537277062]
	TIME [epoch: 3.77 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6814832101910903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6814832101910903 | validation: 0.7056129622165703]
	TIME [epoch: 3.76 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6760876077282757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6760876077282757 | validation: 0.7233192463631313]
	TIME [epoch: 3.76 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6627673484387413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6627673484387413 | validation: 0.7145242305429953]
	TIME [epoch: 3.76 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6771379041107332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6771379041107332 | validation: 0.7069598251407775]
	TIME [epoch: 3.77 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6989594726427834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6989594726427834 | validation: 0.7186761842796058]
	TIME [epoch: 3.78 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6914704115914273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6914704115914273 | validation: 0.7516638915484374]
	TIME [epoch: 3.77 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7258242452779703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7258242452779703 | validation: 0.6981986488032703]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.675475673807006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.675475673807006 | validation: 0.7461592723164924]
	TIME [epoch: 3.77 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.718785640134817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.718785640134817 | validation: 0.8188589790155678]
	TIME [epoch: 3.71 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8407307463889526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8407307463889526 | validation: 0.7051217534232137]
	TIME [epoch: 3.77 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7022349790381304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7022349790381304 | validation: 0.7354623154549453]
	TIME [epoch: 3.76 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6824173393278171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6824173393278171 | validation: 0.7471831466749829]
	TIME [epoch: 3.76 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7218780799770613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7218780799770613 | validation: 0.7514034277139849]
	TIME [epoch: 3.75 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6741149362748301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6741149362748301 | validation: 0.7063497955477249]
	TIME [epoch: 3.77 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6726338373840534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6726338373840534 | validation: 0.7101733202160182]
	TIME [epoch: 3.76 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6676609908000434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6676609908000434 | validation: 0.8492387892140102]
	TIME [epoch: 3.74 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0171233858356075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0171233858356075 | validation: 0.7798898629595075]
	TIME [epoch: 3.76 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8360847307891325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8360847307891325 | validation: 0.8911687995662334]
	TIME [epoch: 3.75 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8967532145410212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8967532145410212 | validation: 0.7892853341331856]
	TIME [epoch: 3.74 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7890441699608031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7890441699608031 | validation: 0.7729572289614371]
	TIME [epoch: 3.73 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.75984186419774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.75984186419774 | validation: 0.7188150410766957]
	TIME [epoch: 3.76 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7066435185074821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7066435185074821 | validation: 0.698918180670654]
	TIME [epoch: 3.74 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7058503442106383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7058503442106383 | validation: 0.7152625046116152]
	TIME [epoch: 3.72 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6581999318646835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6581999318646835 | validation: 0.7186958455209056]
	TIME [epoch: 3.74 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.657100515419286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.657100515419286 | validation: 0.7157539758389796]
	TIME [epoch: 3.71 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6541036464375884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6541036464375884 | validation: 0.7287826245560143]
	TIME [epoch: 3.75 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6592229991867223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6592229991867223 | validation: 0.6921534584643123]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6634603507038878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6634603507038878 | validation: 0.7146372486475114]
	TIME [epoch: 3.76 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6809614653439511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6809614653439511 | validation: 0.7143678740616304]
	TIME [epoch: 3.77 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7218127093581762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7218127093581762 | validation: 0.7681718063059559]
	TIME [epoch: 3.78 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7629511319149023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7629511319149023 | validation: 0.8303276423763617]
	TIME [epoch: 3.75 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7578603455630989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7578603455630989 | validation: 0.6905619634729927]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.721595111845995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.721595111845995 | validation: 0.6919892345341859]
	TIME [epoch: 3.77 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6574214743980514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6574214743980514 | validation: 0.7134056747468792]
	TIME [epoch: 3.77 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6926897070894285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6926897070894285 | validation: 0.7200960734153747]
	TIME [epoch: 3.76 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7322058161760018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7322058161760018 | validation: 0.7038564660792264]
	TIME [epoch: 3.76 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6884029695477009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6884029695477009 | validation: 0.8000005047370538]
	TIME [epoch: 3.77 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7411034683901525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7411034683901525 | validation: 0.7132515007879867]
	TIME [epoch: 3.74 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7098124483997156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7098124483997156 | validation: 0.6861969656787207]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6421197175480411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6421197175480411 | validation: 0.7198432999320026]
	TIME [epoch: 3.76 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6536076010937893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6536076010937893 | validation: 0.6859765322392207]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_190.pth
	Model improved!!!
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6599581270807575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6599581270807575 | validation: 0.6997528838967182]
	TIME [epoch: 3.77 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6532020747046663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6532020747046663 | validation: 0.6937512531452262]
	TIME [epoch: 3.75 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6807503826011628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6807503826011628 | validation: 0.8312625267221878]
	TIME [epoch: 3.75 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.828485871584775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.828485871584775 | validation: 0.8001321746834302]
	TIME [epoch: 3.76 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8269772477623805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8269772477623805 | validation: 0.8067469252170215]
	TIME [epoch: 3.76 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8003175962776612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8003175962776612 | validation: 0.694410758142182]
	TIME [epoch: 3.75 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6753206546888214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6753206546888214 | validation: 0.7999833532473332]
	TIME [epoch: 3.74 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7410820773058859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7410820773058859 | validation: 0.7192055595618764]
	TIME [epoch: 3.75 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6828901109684296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6828901109684296 | validation: 0.7231175802224202]
	TIME [epoch: 3.76 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.671452536618242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.671452536618242 | validation: 0.723127240330717]
	TIME [epoch: 3.76 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7085165485110446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7085165485110446 | validation: 0.6844049981737799]
	TIME [epoch: 31 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6450341702022132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6450341702022132 | validation: 0.7139409414759621]
	TIME [epoch: 8.18 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6657446023653497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6657446023653497 | validation: 0.7114605922287478]
	TIME [epoch: 8.17 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7232523557522079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7232523557522079 | validation: 0.7049995869090231]
	TIME [epoch: 8.14 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6547963250070439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6547963250070439 | validation: 0.7008492849639951]
	TIME [epoch: 8.12 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6697328398767413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6697328398767413 | validation: 0.700464397285596]
	TIME [epoch: 8.12 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7099960518775451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7099960518775451 | validation: 0.6793168575286063]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_207.pth
	Model improved!!!
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6405905764752349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6405905764752349 | validation: 0.7328402679023038]
	TIME [epoch: 8.18 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7763619365691895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7763619365691895 | validation: 0.8304021202518816]
	TIME [epoch: 8.16 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8358859198087788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8358859198087788 | validation: 0.718371026577224]
	TIME [epoch: 8.18 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7736855991123998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7736855991123998 | validation: 0.7726251573650057]
	TIME [epoch: 8.18 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7338874781922045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7338874781922045 | validation: 0.7442377402844799]
	TIME [epoch: 8.19 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6999800097992339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6999800097992339 | validation: 0.6906096177631319]
	TIME [epoch: 8.16 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6500910429535103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6500910429535103 | validation: 0.7095659105970263]
	TIME [epoch: 8.18 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6459737663263022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6459737663263022 | validation: 0.7076510791325643]
	TIME [epoch: 8.17 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.657347614051532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.657347614051532 | validation: 0.7324058059037204]
	TIME [epoch: 8.17 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.69074457900986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.69074457900986 | validation: 0.7873173666549262]
	TIME [epoch: 8.19 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.764651158782105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.764651158782105 | validation: 0.6992178048411677]
	TIME [epoch: 8.14 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6662303861378089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6662303861378089 | validation: 0.676433962627335]
	TIME [epoch: 8.14 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6452591631137407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6452591631137407 | validation: 0.7051045847427799]
	TIME [epoch: 8.17 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6382170774820402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6382170774820402 | validation: 0.6731872793800524]
	TIME [epoch: 8.18 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6505375800851348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6505375800851348 | validation: 0.7565688130060712]
	TIME [epoch: 8.21 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7003974006571013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7003974006571013 | validation: 0.8280979270642913]
	TIME [epoch: 8.2 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8256736893830575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8256736893830575 | validation: 0.7077947585186314]
	TIME [epoch: 8.2 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6442545398044746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6442545398044746 | validation: 0.6796603377617523]
	TIME [epoch: 8.18 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6432245671513386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6432245671513386 | validation: 0.6928246526596848]
	TIME [epoch: 8.2 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6636712112726895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6636712112726895 | validation: 0.7018656214770702]
	TIME [epoch: 8.19 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6542805161751882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6542805161751882 | validation: 0.7054826568383343]
	TIME [epoch: 8.21 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7236111421274665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7236111421274665 | validation: 0.7786888747797489]
	TIME [epoch: 8.2 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7911579564473702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7911579564473702 | validation: 0.6815597130343337]
	TIME [epoch: 8.19 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6918701065295534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6918701065295534 | validation: 0.8365676627870027]
	TIME [epoch: 8.17 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8478198780503567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8478198780503567 | validation: 0.700931906465797]
	TIME [epoch: 8.19 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7209218958128182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7209218958128182 | validation: 0.7142833198674148]
	TIME [epoch: 8.13 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7192846995035381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7192846995035381 | validation: 0.6983569562062033]
	TIME [epoch: 8.15 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6589924482514676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6589924482514676 | validation: 0.7400360816834546]
	TIME [epoch: 8.15 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6889519148661762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6889519148661762 | validation: 0.6832928129603317]
	TIME [epoch: 8.16 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6711359136845383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6711359136845383 | validation: 0.6995274150320303]
	TIME [epoch: 8.17 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6449406698034469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6449406698034469 | validation: 0.714501237440925]
	TIME [epoch: 8.2 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6827547085771102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6827547085771102 | validation: 0.7262106159639156]
	TIME [epoch: 8.19 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7015017997200448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7015017997200448 | validation: 0.7841633476859372]
	TIME [epoch: 8.16 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7700738362685365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7700738362685365 | validation: 0.759370087442102]
	TIME [epoch: 8.18 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.704252037818101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.704252037818101 | validation: 0.6722289029374506]
	TIME [epoch: 8.19 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6560750068000561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6560750068000561 | validation: 0.6607631141581145]
	TIME [epoch: 8.18 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_243.pth
	Model improved!!!
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6393018372416074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6393018372416074 | validation: 0.6997359377206488]
	TIME [epoch: 8.18 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6651655743453612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6651655743453612 | validation: 0.68300061821594]
	TIME [epoch: 8.15 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6467363684147063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6467363684147063 | validation: 0.6823915466869477]
	TIME [epoch: 8.18 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6551275313264442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6551275313264442 | validation: 0.8078354055714763]
	TIME [epoch: 8.16 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7426698519724343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7426698519724343 | validation: 0.8969179589648715]
	TIME [epoch: 8.14 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8873836190744561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8873836190744561 | validation: 0.6750043090388205]
	TIME [epoch: 8.17 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6418153705256396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6418153705256396 | validation: 0.874805250290307]
	TIME [epoch: 8.16 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8279645124660897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8279645124660897 | validation: 0.7563632259861983]
	TIME [epoch: 8.14 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7600525753146187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7600525753146187 | validation: 0.6938831074964238]
	TIME [epoch: 8.14 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6724471442196464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6724471442196464 | validation: 0.8073693023026597]
	TIME [epoch: 8.21 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7200894113164731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7200894113164731 | validation: 0.6874071363914848]
	TIME [epoch: 8.17 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.687643463969467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.687643463969467 | validation: 0.6625445257996886]
	TIME [epoch: 8.15 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6286142074516587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6286142074516587 | validation: 0.7155267846998785]
	TIME [epoch: 8.13 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6710128849799327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6710128849799327 | validation: 0.713302433088542]
	TIME [epoch: 8.2 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7251192800094696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7251192800094696 | validation: 0.6755991113906329]
	TIME [epoch: 8.15 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.638706635934392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.638706635934392 | validation: 0.7245995913902926]
	TIME [epoch: 8.17 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7239570139344812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7239570139344812 | validation: 0.7185317207830615]
	TIME [epoch: 8.19 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7333068469605408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7333068469605408 | validation: 0.7182429619288513]
	TIME [epoch: 8.15 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.725938447200084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.725938447200084 | validation: 0.6827511786996386]
	TIME [epoch: 8.17 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6471246504765482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6471246504765482 | validation: 0.7104653756215206]
	TIME [epoch: 8.17 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6669090156606317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6669090156606317 | validation: 0.6948543065409147]
	TIME [epoch: 8.19 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6871039185406203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6871039185406203 | validation: 0.7331282828567386]
	TIME [epoch: 8.15 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6643115739571553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6643115739571553 | validation: 0.75373537548676]
	TIME [epoch: 8.17 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7398499196578524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7398499196578524 | validation: 0.7356337167315822]
	TIME [epoch: 8.16 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7235760098787126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7235760098787126 | validation: 0.7250378626103555]
	TIME [epoch: 8.2 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.72286119976835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.72286119976835 | validation: 0.7656165891427711]
	TIME [epoch: 8.19 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6839932494884695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6839932494884695 | validation: 0.6619273705867509]
	TIME [epoch: 8.12 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6451894073768049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6451894073768049 | validation: 0.6791691904118141]
	TIME [epoch: 8.18 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.634557895110097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.634557895110097 | validation: 0.6978942622171277]
	TIME [epoch: 8.15 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6390296022175493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6390296022175493 | validation: 0.673753001584424]
	TIME [epoch: 8.09 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6833882266969858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6833882266969858 | validation: 0.6816546850675705]
	TIME [epoch: 8.2 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6408807214998314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6408807214998314 | validation: 0.7586105944577599]
	TIME [epoch: 8.19 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6967745903581515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6967745903581515 | validation: 0.7557995523264494]
	TIME [epoch: 8.16 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7632238777454172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7632238777454172 | validation: 0.725730049252171]
	TIME [epoch: 8.18 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.694010678400988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.694010678400988 | validation: 0.7327649297768097]
	TIME [epoch: 8.19 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7382937296451012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7382937296451012 | validation: 0.6663492540610128]
	TIME [epoch: 8.19 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6611103912719709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6611103912719709 | validation: 0.6756630146517648]
	TIME [epoch: 8.17 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6337126887339573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6337126887339573 | validation: 0.6838937535102076]
	TIME [epoch: 8.18 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6545106688285734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6545106688285734 | validation: 0.6808616339468974]
	TIME [epoch: 8.16 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6356646176882158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6356646176882158 | validation: 0.6840783629529357]
	TIME [epoch: 8.18 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6268204008065565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6268204008065565 | validation: 0.679400673674766]
	TIME [epoch: 8.2 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6310793551857751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6310793551857751 | validation: 0.6516638874837364]
	TIME [epoch: 8.23 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_285.pth
	Model improved!!!
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6565900522369411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6565900522369411 | validation: 0.760232471146254]
	TIME [epoch: 8.19 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7071277707422258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7071277707422258 | validation: 0.7585774583609751]
	TIME [epoch: 8.13 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.799767901552631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.799767901552631 | validation: 0.7119557018543866]
	TIME [epoch: 8.13 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.72935210104047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.72935210104047 | validation: 0.7879353625013205]
	TIME [epoch: 8.19 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8365819515481019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8365819515481019 | validation: 0.6545755302201806]
	TIME [epoch: 8.17 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6712375465229029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6712375465229029 | validation: 0.8089317040201668]
	TIME [epoch: 8.21 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7931566595914007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7931566595914007 | validation: 0.757888799851205]
	TIME [epoch: 8.16 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7503910272497908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7503910272497908 | validation: 0.7068955616224067]
	TIME [epoch: 8.21 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6677268091340446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6677268091340446 | validation: 0.7270726423270377]
	TIME [epoch: 8.16 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6695936955526637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6695936955526637 | validation: 0.6663704965024486]
	TIME [epoch: 8.19 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6309335613329607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6309335613329607 | validation: 0.7064799809221726]
	TIME [epoch: 8.15 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6578717346963276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6578717346963276 | validation: 0.6741609848168215]
	TIME [epoch: 8.12 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6350783666331036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6350783666331036 | validation: 0.7091512518599089]
	TIME [epoch: 8.12 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6360352838814203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6360352838814203 | validation: 0.6878868821265278]
	TIME [epoch: 8.22 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6438799113712841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6438799113712841 | validation: 0.6782464950408947]
	TIME [epoch: 8.2 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6233502435135374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6233502435135374 | validation: 0.6748036059738285]
	TIME [epoch: 8.19 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6344029945451157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6344029945451157 | validation: 0.662695072604727]
	TIME [epoch: 8.15 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6733209027412312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6733209027412312 | validation: 0.6781206314742765]
	TIME [epoch: 8.16 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6329239851740142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6329239851740142 | validation: 0.6538732057203422]
	TIME [epoch: 8.17 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6484365129335415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6484365129335415 | validation: 0.7832560672721706]
	TIME [epoch: 8.18 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6913719920579322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6913719920579322 | validation: 0.7942300489999274]
	TIME [epoch: 8.12 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8102152332406525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8102152332406525 | validation: 0.6867120121310357]
	TIME [epoch: 8.09 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6342255960022941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6342255960022941 | validation: 0.6628131166565696]
	TIME [epoch: 8.19 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6389070737863144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6389070737863144 | validation: 0.6686566242807768]
	TIME [epoch: 8.17 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6359976233504608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6359976233504608 | validation: 0.6840865674113483]
	TIME [epoch: 8.22 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6270195948605488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6270195948605488 | validation: 0.6718955512786939]
	TIME [epoch: 8.16 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6615234498758761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6615234498758761 | validation: 0.7374409839051984]
	TIME [epoch: 8.17 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6813334484699246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6813334484699246 | validation: 0.6891781272773362]
	TIME [epoch: 8.14 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.719570218619898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.719570218619898 | validation: 0.6976686802081434]
	TIME [epoch: 8.16 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6444150270656606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6444150270656606 | validation: 0.7453420822120022]
	TIME [epoch: 8.19 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7583859343896117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7583859343896117 | validation: 0.6922791791944444]
	TIME [epoch: 8.19 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7283418291026891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7283418291026891 | validation: 0.645191949643499]
	TIME [epoch: 8.13 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_317.pth
	Model improved!!!
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6573077943242842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6573077943242842 | validation: 0.7065767077545438]
	TIME [epoch: 8.17 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6787488177837697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6787488177837697 | validation: 0.6597420327134178]
	TIME [epoch: 8.17 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6272880483632212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6272880483632212 | validation: 0.6491225754398613]
	TIME [epoch: 8.17 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6121580140519632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6121580140519632 | validation: 0.6800930988769567]
	TIME [epoch: 8.18 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6195494110618813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6195494110618813 | validation: 0.655382175746646]
	TIME [epoch: 8.18 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6144334923581544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6144334923581544 | validation: 0.7563294118256615]
	TIME [epoch: 8.1 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6627751233457119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6627751233457119 | validation: 0.900731174033718]
	TIME [epoch: 8.14 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9089692091059824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9089692091059824 | validation: 0.7573474638852609]
	TIME [epoch: 8.15 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7116827416906579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7116827416906579 | validation: 0.650840589822142]
	TIME [epoch: 8.17 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6942679867441638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6942679867441638 | validation: 0.6428665257330747]
	TIME [epoch: 8.18 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_327.pth
	Model improved!!!
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6202427603629584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6202427603629584 | validation: 0.7225724212236179]
	TIME [epoch: 8.18 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7069971799157628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7069971799157628 | validation: 0.6362996759676491]
	TIME [epoch: 8.13 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_329.pth
	Model improved!!!
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6669128130501479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6669128130501479 | validation: 0.6471689501317197]
	TIME [epoch: 8.19 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6194983547586386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6194983547586386 | validation: 0.6847587908427946]
	TIME [epoch: 8.18 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6480380153004512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6480380153004512 | validation: 0.6466915471915032]
	TIME [epoch: 8.16 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.607342838490438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.607342838490438 | validation: 0.6325173809508174]
	TIME [epoch: 8.16 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_333.pth
	Model improved!!!
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5989387001523382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5989387001523382 | validation: 0.7185208101788839]
	TIME [epoch: 8.19 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6325038552413641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6325038552413641 | validation: 0.7524932372573425]
	TIME [epoch: 8.11 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7889175245351127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7889175245351127 | validation: 1.0375075422513031]
	TIME [epoch: 8.16 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1306703584294968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1306703584294968 | validation: 0.6591780388059234]
	TIME [epoch: 8.15 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6303939746548748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6303939746548748 | validation: 0.6654550630933336]
	TIME [epoch: 8.18 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6525678549751089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6525678549751089 | validation: 0.6976385461639153]
	TIME [epoch: 8.17 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6177802437146709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6177802437146709 | validation: 0.6092543104256989]
	TIME [epoch: 8.19 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_340.pth
	Model improved!!!
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.581406877675918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.581406877675918 | validation: 0.61910087336012]
	TIME [epoch: 8.2 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5656302884771202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5656302884771202 | validation: 0.6373699599352343]
	TIME [epoch: 8.19 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.558812782040621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.558812782040621 | validation: 0.5841602833620904]
	TIME [epoch: 8.16 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_343.pth
	Model improved!!!
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5618207555034546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5618207555034546 | validation: 0.646937300529033]
	TIME [epoch: 8.17 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5391468695412042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5391468695412042 | validation: 0.7150719511736748]
	TIME [epoch: 8.14 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7445790470976988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7445790470976988 | validation: 1.5275981587788656]
	TIME [epoch: 8.16 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9231812923204223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9231812923204223 | validation: 0.9483354153779265]
	TIME [epoch: 8.18 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1071619176336973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1071619176336973 | validation: 0.9519128608129137]
	TIME [epoch: 8.17 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9810814220230988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9810814220230988 | validation: 0.8606856137793493]
	TIME [epoch: 8.16 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8521447999567775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8521447999567775 | validation: 0.6424694465919784]
	TIME [epoch: 8.15 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6743346568498285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6743346568498285 | validation: 0.7011822713644774]
	TIME [epoch: 8.09 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.726254717393948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.726254717393948 | validation: 0.6251425987543764]
	TIME [epoch: 8.08 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6229665092884892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6229665092884892 | validation: 0.6539753111986525]
	TIME [epoch: 8.13 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6502162128010524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6502162128010524 | validation: 0.6329105758615745]
	TIME [epoch: 8.06 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5988896435635453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5988896435635453 | validation: 0.6384369213708959]
	TIME [epoch: 8.15 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6060139662283777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6060139662283777 | validation: 0.6285068280631745]
	TIME [epoch: 8.15 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5969414351353138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5969414351353138 | validation: 0.6083847792833232]
	TIME [epoch: 8.17 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.590815724330161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.590815724330161 | validation: 0.6334326811265394]
	TIME [epoch: 8.17 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5900815428418267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5900815428418267 | validation: 0.6260007043689138]
	TIME [epoch: 8.13 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5728532022539791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5728532022539791 | validation: 0.6186563913240557]
	TIME [epoch: 8.13 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5699873570394464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5699873570394464 | validation: 0.611750648598502]
	TIME [epoch: 8.17 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5678815020564929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5678815020564929 | validation: 0.6142277159970221]
	TIME [epoch: 8.19 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5537242793725331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5537242793725331 | validation: 0.5772728944802871]
	TIME [epoch: 8.18 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_363.pth
	Model improved!!!
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5479806371578656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5479806371578656 | validation: 0.6260881836008192]
	TIME [epoch: 8.16 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5526619412945432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5526619412945432 | validation: 0.6615861667048333]
	TIME [epoch: 8.14 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6956284441830419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6956284441830419 | validation: 1.0714055971638456]
	TIME [epoch: 8.15 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2476331721451046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2476331721451046 | validation: 0.6797441616090604]
	TIME [epoch: 8.15 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6679309569499339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6679309569499339 | validation: 0.6030563146379659]
	TIME [epoch: 8.17 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5708380841140165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5708380841140165 | validation: 0.7530181275534104]
	TIME [epoch: 8.16 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6614765663747172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6614765663747172 | validation: 0.5968693536923331]
	TIME [epoch: 8.17 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5639007339033729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5639007339033729 | validation: 0.5980345505242072]
	TIME [epoch: 8.16 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5249627710374882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5249627710374882 | validation: 0.5601116840347143]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_372.pth
	Model improved!!!
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5084108649344623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5084108649344623 | validation: 0.566753523319928]
	TIME [epoch: 8.23 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4864955042155307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4864955042155307 | validation: 0.5565281216921151]
	TIME [epoch: 8.17 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_374.pth
	Model improved!!!
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4814914381303425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4814914381303425 | validation: 0.6261668512332634]
	TIME [epoch: 8.21 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5767227948344783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5767227948344783 | validation: 0.8669101936049922]
	TIME [epoch: 8.18 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9474038107350674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9474038107350674 | validation: 0.8075248533397524]
	TIME [epoch: 8.17 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.02932496686321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.02932496686321 | validation: 0.5655168918456152]
	TIME [epoch: 8.19 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6953193691334457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6953193691334457 | validation: 1.0501213387812203]
	TIME [epoch: 8.2 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1956615817688006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1956615817688006 | validation: 0.837731494387056]
	TIME [epoch: 8.18 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8761872557110436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8761872557110436 | validation: 0.6645566448876566]
	TIME [epoch: 8.2 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6709569998008924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6709569998008924 | validation: 0.6239486264228403]
	TIME [epoch: 8.17 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6208907595526852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6208907595526852 | validation: 0.6321741751234605]
	TIME [epoch: 8.18 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6235688085865231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6235688085865231 | validation: 0.5914996943293819]
	TIME [epoch: 8.21 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.561471289384877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.561471289384877 | validation: 0.6220488076334614]
	TIME [epoch: 8.16 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5585621504325817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5585621504325817 | validation: 0.5671854289353471]
	TIME [epoch: 8.19 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5489455471182292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5489455471182292 | validation: 0.5754472938836587]
	TIME [epoch: 8.18 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5203625749262712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5203625749262712 | validation: 0.5771366262805635]
	TIME [epoch: 8.17 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5015413175242924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5015413175242924 | validation: 0.5485126302927779]
	TIME [epoch: 8.18 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_389.pth
	Model improved!!!
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4953929987535191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4953929987535191 | validation: 0.5499188425123729]
	TIME [epoch: 8.2 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4785763946325977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4785763946325977 | validation: 0.526753633195965]
	TIME [epoch: 8.17 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_391.pth
	Model improved!!!
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4843415668919826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4843415668919826 | validation: 0.7102564124506663]
	TIME [epoch: 8.18 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6815173335527912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6815173335527912 | validation: 0.705727753828557]
	TIME [epoch: 8.19 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7652076169314063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7652076169314063 | validation: 0.6326339010504459]
	TIME [epoch: 8.2 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7058329748965467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7058329748965467 | validation: 0.5788819738918821]
	TIME [epoch: 8.19 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5641508648089916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5641508648089916 | validation: 0.5293841991203806]
	TIME [epoch: 8.16 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47593845241992344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47593845241992344 | validation: 0.544929567170698]
	TIME [epoch: 8.11 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46573108358130455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46573108358130455 | validation: 0.6433671398660616]
	TIME [epoch: 8.18 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6022898368700667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6022898368700667 | validation: 1.0153764676991344]
	TIME [epoch: 8.21 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3537704202898238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3537704202898238 | validation: 1.0055761715653069]
	TIME [epoch: 8.15 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2677512397025386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2677512397025386 | validation: 1.044960520218766]
	TIME [epoch: 8.19 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1873382588216326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1873382588216326 | validation: 0.8596682161433062]
	TIME [epoch: 8.16 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8377793364005084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8377793364005084 | validation: 0.7385470761022814]
	TIME [epoch: 8.16 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7499459500457962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7499459500457962 | validation: 0.5900980339340354]
	TIME [epoch: 8.2 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5879865203975754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5879865203975754 | validation: 0.5795808804012804]
	TIME [epoch: 8.19 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6032925085245554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6032925085245554 | validation: 0.5332240598224824]
	TIME [epoch: 8.2 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5202250917131568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5202250917131568 | validation: 0.5315703777985981]
	TIME [epoch: 8.17 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4967074654805203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4967074654805203 | validation: 0.5271896582495915]
	TIME [epoch: 8.18 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49699499476597087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49699499476597087 | validation: 0.5640965047931215]
	TIME [epoch: 8.18 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5199492731736226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5199492731736226 | validation: 0.5491582149397228]
	TIME [epoch: 8.22 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5253416775386202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5253416775386202 | validation: 0.6073181042252632]
	TIME [epoch: 8.18 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5748404114351113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5748404114351113 | validation: 0.5149184764429519]
	TIME [epoch: 8.19 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_412.pth
	Model improved!!!
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49580378271660625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49580378271660625 | validation: 0.47736612863042716]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_413.pth
	Model improved!!!
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4466852454130667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4466852454130667 | validation: 0.49082957637948504]
	TIME [epoch: 8.19 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4291188546897874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4291188546897874 | validation: 0.4985900087299598]
	TIME [epoch: 8.19 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42684471900186227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42684471900186227 | validation: 0.5069940239052192]
	TIME [epoch: 8.18 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48052984382274466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48052984382274466 | validation: 0.6292808732389918]
	TIME [epoch: 8.18 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6781639981298163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6781639981298163 | validation: 0.7976165199551599]
	TIME [epoch: 8.16 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9434117652628655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9434117652628655 | validation: 0.5653211338691693]
	TIME [epoch: 8.18 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.575424809056594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.575424809056594 | validation: 0.49755395793273377]
	TIME [epoch: 8.19 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.477766204138995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.477766204138995 | validation: 0.6619856120838503]
	TIME [epoch: 8.22 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6413076398834288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6413076398834288 | validation: 0.4719231931319152]
	TIME [epoch: 8.18 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_422.pth
	Model improved!!!
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4478225145226727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4478225145226727 | validation: 0.4706454438793204]
	TIME [epoch: 8.18 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_423.pth
	Model improved!!!
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41853612312098976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41853612312098976 | validation: 0.4641280269315896]
	TIME [epoch: 8.18 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_424.pth
	Model improved!!!
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.409643830164639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.409643830164639 | validation: 0.45545204847030596]
	TIME [epoch: 8.16 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_425.pth
	Model improved!!!
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39955986520817294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39955986520817294 | validation: 0.4522599952594523]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_426.pth
	Model improved!!!
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4001188118601943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4001188118601943 | validation: 0.47017476900113675]
	TIME [epoch: 8.21 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42731554198248556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42731554198248556 | validation: 0.5967852758220072]
	TIME [epoch: 8.2 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6036057058973424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6036057058973424 | validation: 0.5668157214046069]
	TIME [epoch: 8.21 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5692391576074344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5692391576074344 | validation: 0.4687899929282425]
	TIME [epoch: 8.21 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4447048930921717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4447048930921717 | validation: 0.43350105652552084]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_431.pth
	Model improved!!!
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3710432226568026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3710432226568026 | validation: 0.4367707849075874]
	TIME [epoch: 8.23 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37183396502371496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37183396502371496 | validation: 0.49792347754697597]
	TIME [epoch: 8.11 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.477797750946936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.477797750946936 | validation: 0.6063682196084361]
	TIME [epoch: 8.14 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.626889899877128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.626889899877128 | validation: 0.5985754389917638]
	TIME [epoch: 8.22 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6621024864868644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6621024864868644 | validation: 0.4656823706548052]
	TIME [epoch: 8.2 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43957584932208393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43957584932208393 | validation: 0.44185148506407046]
	TIME [epoch: 8.16 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40488761755065455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40488761755065455 | validation: 0.6650463065624921]
	TIME [epoch: 8.21 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6574365106470599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6574365106470599 | validation: 0.4278786074861305]
	TIME [epoch: 8.19 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_439.pth
	Model improved!!!
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39292849016492787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39292849016492787 | validation: 0.41410770019795057]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_440.pth
	Model improved!!!
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36996642125440843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36996642125440843 | validation: 0.5055229793743021]
	TIME [epoch: 8.19 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4846379426766801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4846379426766801 | validation: 0.5634932871337626]
	TIME [epoch: 8.23 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6077040557196213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6077040557196213 | validation: 0.7105138175368577]
	TIME [epoch: 8.21 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8956809912162376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8956809912162376 | validation: 0.5177465904822645]
	TIME [epoch: 8.15 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6423633277694947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6423633277694947 | validation: 0.559753868088904]
	TIME [epoch: 8.19 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6082017258806066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6082017258806066 | validation: 0.39620280258544327]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_446.pth
	Model improved!!!
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4097575521120181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4097575521120181 | validation: 0.4550824040433584]
	TIME [epoch: 8.19 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4295824397932906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4295824397932906 | validation: 0.5211002676576305]
	TIME [epoch: 8.18 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.501063412942392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.501063412942392 | validation: 0.431586795961686]
	TIME [epoch: 8.16 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41823783055593383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41823783055593383 | validation: 0.3944369880546683]
	TIME [epoch: 8.16 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_450.pth
	Model improved!!!
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35308162382140423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35308162382140423 | validation: 0.3740596162946026]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_451.pth
	Model improved!!!
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3296273613884906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3296273613884906 | validation: 0.41107463503416003]
	TIME [epoch: 8.17 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37440589057896545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37440589057896545 | validation: 0.4571863339676327]
	TIME [epoch: 8.14 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45676597601005037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45676597601005037 | validation: 0.6514705175232476]
	TIME [epoch: 8.13 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7271454621276398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7271454621276398 | validation: 0.4125639568328451]
	TIME [epoch: 8.15 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4383947771433101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4383947771433101 | validation: 0.4662733345967087]
	TIME [epoch: 8.15 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45432715401135043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45432715401135043 | validation: 0.41428172148657105]
	TIME [epoch: 8.16 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3660073282009813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3660073282009813 | validation: 0.5647912447635862]
	TIME [epoch: 8.18 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5250589369473356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5250589369473356 | validation: 0.8466851972899324]
	TIME [epoch: 8.16 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0594950541647157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0594950541647157 | validation: 0.570161759702917]
	TIME [epoch: 8.16 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6417381329209139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6417381329209139 | validation: 0.7330647471838241]
	TIME [epoch: 8.16 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7554841755157097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7554841755157097 | validation: 0.43672468905714734]
	TIME [epoch: 8.18 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4092931677931984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4092931677931984 | validation: 0.4891732930969845]
	TIME [epoch: 8.16 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4750351016713493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4750351016713493 | validation: 0.42335415430056966]
	TIME [epoch: 8.14 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38462742926980625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38462742926980625 | validation: 0.43849574501193395]
	TIME [epoch: 8.19 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3771441997683054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3771441997683054 | validation: 0.3641696577845097]
	TIME [epoch: 8.12 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_466.pth
	Model improved!!!
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3138604649011145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3138604649011145 | validation: 0.3607623828327573]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_467.pth
	Model improved!!!
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3047027352389008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3047027352389008 | validation: 0.3538398398937373]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_468.pth
	Model improved!!!
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29811838258940865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29811838258940865 | validation: 0.3363266222069758]
	TIME [epoch: 8.18 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_469.pth
	Model improved!!!
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2751732612213724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2751732612213724 | validation: 0.3311297713403363]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_470.pth
	Model improved!!!
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27697366987675526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27697366987675526 | validation: 0.3471163314312482]
	TIME [epoch: 8.16 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29342956872314424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29342956872314424 | validation: 0.488161646291648]
	TIME [epoch: 8.17 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5445691755551388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5445691755551388 | validation: 0.46439657958728464]
	TIME [epoch: 8.15 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4271653759810826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4271653759810826 | validation: 0.4031161036843203]
	TIME [epoch: 8.19 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4806507060599281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4806507060599281 | validation: 0.4279822890722681]
	TIME [epoch: 8.16 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36795084136105966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36795084136105966 | validation: 0.36825136470655595]
	TIME [epoch: 8.18 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3460223121777223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3460223121777223 | validation: 0.3543413638580331]
	TIME [epoch: 8.16 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2899002800230505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2899002800230505 | validation: 0.40599472953772103]
	TIME [epoch: 8.17 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4080704357378933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4080704357378933 | validation: 0.3354680223660895]
	TIME [epoch: 8.18 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3023502952994105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3023502952994105 | validation: 0.31527238658954543]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_480.pth
	Model improved!!!
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23734747518037247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23734747518037247 | validation: 0.3690362003467609]
	TIME [epoch: 8.17 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31871025862032193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31871025862032193 | validation: 0.4809062078896993]
	TIME [epoch: 8.17 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46403409591811035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46403409591811035 | validation: 0.7080712414254191]
	TIME [epoch: 8.14 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9123871198180226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9123871198180226 | validation: 1.0046280192624877]
	TIME [epoch: 8.16 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1402170710264343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1402170710264343 | validation: 0.6791883172772848]
	TIME [epoch: 8.16 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7806021360877378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7806021360877378 | validation: 0.41884516586968595]
	TIME [epoch: 8.18 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.515990031354405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.515990031354405 | validation: 0.3422362538560916]
	TIME [epoch: 8.16 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3194794622472908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3194794622472908 | validation: 0.46138730195331545]
	TIME [epoch: 8.16 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42798483079104416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42798483079104416 | validation: 0.6817297165808727]
	TIME [epoch: 8.18 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.720376717592377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.720376717592377 | validation: 0.4098033327170514]
	TIME [epoch: 8.16 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4076333036696273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4076333036696273 | validation: 0.5127443014694425]
	TIME [epoch: 8.16 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49937361110898004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49937361110898004 | validation: 0.3677354321966113]
	TIME [epoch: 8.16 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3407074200546162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3407074200546162 | validation: 0.31583053057953897]
	TIME [epoch: 8.14 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2532271211272636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2532271211272636 | validation: 0.35075472537415514]
	TIME [epoch: 8.17 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26471333209304143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26471333209304143 | validation: 0.43561627153230503]
	TIME [epoch: 8.16 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41676029730565367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41676029730565367 | validation: 0.340931960788461]
	TIME [epoch: 8.14 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27955001230718296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27955001230718296 | validation: 0.27275406981751427]
	TIME [epoch: 8.18 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_497.pth
	Model improved!!!
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22654521972470176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22654521972470176 | validation: 0.3070649858467671]
	TIME [epoch: 8.13 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24093942624509213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24093942624509213 | validation: 0.36380144416266336]
	TIME [epoch: 8.17 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3798187974151741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3798187974151741 | validation: 0.38451487667800244]
	TIME [epoch: 8.17 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3189076590361993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3189076590361993 | validation: 0.30193948912471114]
	TIME [epoch: 40.2 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2666875452993278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2666875452993278 | validation: 0.3416672800776932]
	TIME [epoch: 17.6 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2641507409662166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2641507409662166 | validation: 0.4263518723392757]
	TIME [epoch: 17.6 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42923468311452767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42923468311452767 | validation: 0.748544069519593]
	TIME [epoch: 17.6 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7280147709703689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7280147709703689 | validation: 0.376178709259757]
	TIME [epoch: 17.6 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43470627341057905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43470627341057905 | validation: 0.3777436341118857]
	TIME [epoch: 17.6 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.318666311501035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.318666311501035 | validation: 0.34296206358268977]
	TIME [epoch: 17.6 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3007978818304591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3007978818304591 | validation: 0.2757769954722454]
	TIME [epoch: 17.6 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.272175327631425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.272175327631425 | validation: 0.2962069409889056]
	TIME [epoch: 17.5 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23094081240424696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23094081240424696 | validation: 0.2890094112195914]
	TIME [epoch: 17.6 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19770157086354212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19770157086354212 | validation: 0.273948384997529]
	TIME [epoch: 17.5 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19183055643470462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19183055643470462 | validation: 0.21102452814828931]
	TIME [epoch: 17.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_512.pth
	Model improved!!!
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1809751798739209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1809751798739209 | validation: 0.24591034081832272]
	TIME [epoch: 17.6 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17167496272395674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17167496272395674 | validation: 0.19404587139744814]
	TIME [epoch: 17.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_514.pth
	Model improved!!!
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18411587578034558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18411587578034558 | validation: 0.34243009841285793]
	TIME [epoch: 17.6 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3182661638228103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3182661638228103 | validation: 0.5093627816966457]
	TIME [epoch: 17.6 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6723231565207257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6723231565207257 | validation: 0.3331415288241548]
	TIME [epoch: 17.6 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2490073453323135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2490073453323135 | validation: 0.3278892842201808]
	TIME [epoch: 17.6 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2308283370887082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2308283370887082 | validation: 0.2909587701478686]
	TIME [epoch: 17.6 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28087596339274945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28087596339274945 | validation: 0.19517831940033328]
	TIME [epoch: 17.6 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16470802095114637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16470802095114637 | validation: 0.28462019105269903]
	TIME [epoch: 17.6 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20017720150698964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20017720150698964 | validation: 0.332041101349143]
	TIME [epoch: 17.6 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32260714295236087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32260714295236087 | validation: 0.2689061151428111]
	TIME [epoch: 17.6 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2409156981434233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2409156981434233 | validation: 0.24181636467228784]
	TIME [epoch: 17.6 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1832403834493922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1832403834493922 | validation: 0.2753228174780613]
	TIME [epoch: 17.6 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22401795474641098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22401795474641098 | validation: 0.5899857685028157]
	TIME [epoch: 17.5 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6214005051968448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6214005051968448 | validation: 0.7339062011747335]
	TIME [epoch: 17.6 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6521089296428426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6521089296428426 | validation: 0.29879374907732986]
	TIME [epoch: 17.5 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2668559923043256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2668559923043256 | validation: 0.3483335906636553]
	TIME [epoch: 17.5 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.302130049302093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.302130049302093 | validation: 0.2498128914123063]
	TIME [epoch: 17.6 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1876139329478739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1876139329478739 | validation: 0.23686495077216918]
	TIME [epoch: 17.6 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20843642781375868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20843642781375868 | validation: 0.1926967073208133]
	TIME [epoch: 17.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_532.pth
	Model improved!!!
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14427262476994065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14427262476994065 | validation: 0.17811562663989866]
	TIME [epoch: 17.5 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_533.pth
	Model improved!!!
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1619545275056908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1619545275056908 | validation: 0.3174377582298882]
	TIME [epoch: 17.5 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2684644803466728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2684644803466728 | validation: 0.5432869965578259]
	TIME [epoch: 17.5 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7710882492868611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7710882492868611 | validation: 0.7146566392298587]
	TIME [epoch: 17.5 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8564227678516377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8564227678516377 | validation: 0.8780056456987003]
	TIME [epoch: 17.5 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9521478704218749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9521478704218749 | validation: 0.8830968090579575]
	TIME [epoch: 17.5 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9758331880529935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9758331880529935 | validation: 0.2896863943573872]
	TIME [epoch: 17.5 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24573794982260155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24573794982260155 | validation: 0.3903977925911704]
	TIME [epoch: 17.5 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33554846410242933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33554846410242933 | validation: 0.2356346915743031]
	TIME [epoch: 17.5 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2073329722891885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2073329722891885 | validation: 0.28134977411107526]
	TIME [epoch: 17.5 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27612824124650975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27612824124650975 | validation: 0.23220085880730626]
	TIME [epoch: 17.5 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22029417504122506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22029417504122506 | validation: 0.21350397850496067]
	TIME [epoch: 17.5 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15359411349871357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15359411349871357 | validation: 0.20327654953432833]
	TIME [epoch: 17.5 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16333048593513283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16333048593513283 | validation: 0.21572582860621223]
	TIME [epoch: 17.5 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15054029758031595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15054029758031595 | validation: 0.1764122189960271]
	TIME [epoch: 17.5 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_547.pth
	Model improved!!!
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18347625835593812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18347625835593812 | validation: 0.27879041367240787]
	TIME [epoch: 17.6 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20355444695065966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20355444695065966 | validation: 0.46109221366167347]
	TIME [epoch: 17.6 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4420129880641586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4420129880641586 | validation: 0.2255189038596353]
	TIME [epoch: 17.6 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2417142024650336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2417142024650336 | validation: 0.6011164013532735]
	TIME [epoch: 17.5 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5137829895765195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5137829895765195 | validation: 0.42667107065679605]
	TIME [epoch: 17.6 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4923039751120307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4923039751120307 | validation: 0.4395999839793093]
	TIME [epoch: 17.5 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5702972006828099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5702972006828099 | validation: 0.7493742514974262]
	TIME [epoch: 17.6 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9964370841955602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9964370841955602 | validation: 0.4479914295884685]
	TIME [epoch: 17.5 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5351694999558154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5351694999558154 | validation: 0.4057251837842209]
	TIME [epoch: 17.5 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3657218512852392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3657218512852392 | validation: 0.4057479459829977]
	TIME [epoch: 17.5 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2811400389594495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2811400389594495 | validation: 0.32056431371323973]
	TIME [epoch: 17.5 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2567877348763665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2567877348763665 | validation: 0.27730525198846595]
	TIME [epoch: 17.5 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2351879992428126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2351879992428126 | validation: 0.24721632634191001]
	TIME [epoch: 17.6 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21708961510089883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21708961510089883 | validation: 0.20319437645973834]
	TIME [epoch: 17.5 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17420437466399286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17420437466399286 | validation: 0.24099813754940339]
	TIME [epoch: 17.6 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15875016046162346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15875016046162346 | validation: 0.2737747265426651]
	TIME [epoch: 17.6 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1821795898702296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1821795898702296 | validation: 0.26395136815152764]
	TIME [epoch: 17.5 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2365633628335154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2365633628335154 | validation: 0.23291307908133585]
	TIME [epoch: 17.5 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18161168069992958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18161168069992958 | validation: 0.20470847363930283]
	TIME [epoch: 17.6 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14778857132794632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14778857132794632 | validation: 0.23583910351064882]
	TIME [epoch: 17.6 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20055588092438928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20055588092438928 | validation: 0.2749802586717194]
	TIME [epoch: 17.5 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2071192586784836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2071192586784836 | validation: 0.18169412476882715]
	TIME [epoch: 17.6 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1994432696638313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1994432696638313 | validation: 0.1385633230029987]
	TIME [epoch: 17.5 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_570.pth
	Model improved!!!
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13986722864007473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13986722864007473 | validation: 0.21469652048790488]
	TIME [epoch: 17.5 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1538098452631375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1538098452631375 | validation: 0.20471118197005103]
	TIME [epoch: 17.5 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16622549388638128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16622549388638128 | validation: 0.2992227766318404]
	TIME [epoch: 17.5 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3411659696311623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3411659696311623 | validation: 0.2830423979696791]
	TIME [epoch: 17.5 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27571967617889837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27571967617889837 | validation: 0.2196104018432041]
	TIME [epoch: 17.5 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15628391420988313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15628391420988313 | validation: 0.31039436036799173]
	TIME [epoch: 17.5 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30010475434860806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30010475434860806 | validation: 0.5798089558478436]
	TIME [epoch: 17.5 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8168156445931608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8168156445931608 | validation: 0.7478234377330063]
	TIME [epoch: 17.5 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0143128312701866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0143128312701866 | validation: 0.8046895666579774]
	TIME [epoch: 17.5 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.053803088422626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.053803088422626 | validation: 0.6242213581888116]
	TIME [epoch: 17.5 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5456256691027482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5456256691027482 | validation: 0.44204245619471483]
	TIME [epoch: 17.5 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3643004218175113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3643004218175113 | validation: 0.35081471339923587]
	TIME [epoch: 17.5 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34096268753558095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34096268753558095 | validation: 0.21677249557985578]
	TIME [epoch: 17.5 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1998088726813891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1998088726813891 | validation: 0.23018266018082173]
	TIME [epoch: 17.6 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19341451075539037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19341451075539037 | validation: 0.17664660399304744]
	TIME [epoch: 17.6 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1632850734196509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1632850734196509 | validation: 0.1681645967169673]
	TIME [epoch: 17.6 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14683456670404393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14683456670404393 | validation: 0.19605174207258502]
	TIME [epoch: 17.6 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15545426400917844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15545426400917844 | validation: 0.15664770558813365]
	TIME [epoch: 17.6 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1570173707381444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1570173707381444 | validation: 0.2938728232363147]
	TIME [epoch: 17.6 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22429073944862377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22429073944862377 | validation: 0.18116261242912624]
	TIME [epoch: 17.5 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14300923877449345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14300923877449345 | validation: 0.16411483080634712]
	TIME [epoch: 17.5 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14312554775174588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14312554775174588 | validation: 0.1693514031211316]
	TIME [epoch: 17.5 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13340127555326073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13340127555326073 | validation: 0.1802639995457288]
	TIME [epoch: 17.5 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15730556279719424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15730556279719424 | validation: 0.17905481923516026]
	TIME [epoch: 17.5 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14269529466716552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14269529466716552 | validation: 0.26363311032068143]
	TIME [epoch: 17.6 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2704057377781884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2704057377781884 | validation: 0.18544638713890121]
	TIME [epoch: 17.5 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21636966654894493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21636966654894493 | validation: 0.23554491735754024]
	TIME [epoch: 17.6 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20543134883464578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20543134883464578 | validation: 0.3326607498202166]
	TIME [epoch: 17.5 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40981359623275937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40981359623275937 | validation: 0.2810795734089126]
	TIME [epoch: 17.5 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3721885533270088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3721885533270088 | validation: 0.25295715484120057]
	TIME [epoch: 17.5 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23120398291545588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23120398291545588 | validation: 0.15614982545040468]
	TIME [epoch: 17.6 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15935308412038268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15935308412038268 | validation: 0.32706763799637334]
	TIME [epoch: 17.6 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32734603564260156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32734603564260156 | validation: 0.464597151034788]
	TIME [epoch: 17.6 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49616702997467815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49616702997467815 | validation: 1.640052678017285]
	TIME [epoch: 17.6 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0903435098845753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0903435098845753 | validation: 1.3401307050071394]
	TIME [epoch: 17.5 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8021048449834975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8021048449834975 | validation: 0.6262609967962361]
	TIME [epoch: 17.6 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8740203458955043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8740203458955043 | validation: 0.470858971393342]
	TIME [epoch: 17.6 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3377569936750481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3377569936750481 | validation: 0.2996526263622362]
	TIME [epoch: 17.5 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2594984864463065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2594984864463065 | validation: 0.22122726495424472]
	TIME [epoch: 17.6 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2054613040258959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2054613040258959 | validation: 0.18200650816025618]
	TIME [epoch: 17.5 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17625314397089084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17625314397089084 | validation: 0.19525857832619167]
	TIME [epoch: 17.5 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1654211154010816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1654211154010816 | validation: 0.18868172126931224]
	TIME [epoch: 17.5 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16121710043603407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16121710043603407 | validation: 0.1876117472735248]
	TIME [epoch: 17.5 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15032370068349524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15032370068349524 | validation: 0.1951640726711247]
	TIME [epoch: 17.5 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18253929750719572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18253929750719572 | validation: 0.2258605976428877]
	TIME [epoch: 17.5 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18517223102099437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18517223102099437 | validation: 0.20156432356790496]
	TIME [epoch: 17.5 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15003236357380617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15003236357380617 | validation: 0.18456168665450315]
	TIME [epoch: 17.5 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14510321755342645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14510321755342645 | validation: 0.18064527491134746]
	TIME [epoch: 17.5 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14955081242572585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14955081242572585 | validation: 0.1686885186437629]
	TIME [epoch: 17.5 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1442570919430019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1442570919430019 | validation: 0.1567067635681998]
	TIME [epoch: 17.5 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1452608909776781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1452608909776781 | validation: 0.15332908278628654]
	TIME [epoch: 17.5 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13552926411025726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13552926411025726 | validation: 0.14805513851327837]
	TIME [epoch: 17.5 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1363498942943211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1363498942943211 | validation: 0.17244908070691922]
	TIME [epoch: 17.5 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15120396382424495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15120396382424495 | validation: 0.2059565533611238]
	TIME [epoch: 17.5 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16942014513128917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16942014513128917 | validation: 0.17189426703624575]
	TIME [epoch: 17.4 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13668382558161077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13668382558161077 | validation: 0.13311756245198988]
	TIME [epoch: 17.5 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_626.pth
	Model improved!!!
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11903205419061186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11903205419061186 | validation: 0.14177480384936644]
	TIME [epoch: 17.5 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12653499146902253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12653499146902253 | validation: 0.16807691558895096]
	TIME [epoch: 17.5 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12566449755379253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12566449755379253 | validation: 0.18257968378309486]
	TIME [epoch: 17.5 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16803338848217358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16803338848217358 | validation: 0.2596823187900107]
	TIME [epoch: 17.5 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19382080165741378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19382080165741378 | validation: 0.1601163951100034]
	TIME [epoch: 17.5 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12276092035689351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12276092035689351 | validation: 0.14420595211556902]
	TIME [epoch: 17.4 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15347321229751415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15347321229751415 | validation: 0.2293815202592355]
	TIME [epoch: 17.5 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20278771825439762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20278771825439762 | validation: 0.3165787330886389]
	TIME [epoch: 17.5 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3353425670293467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3353425670293467 | validation: 0.5426409627527828]
	TIME [epoch: 17.5 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4515518839532915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4515518839532915 | validation: 0.32092539095861355]
	TIME [epoch: 17.6 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22651810259116942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22651810259116942 | validation: 0.21223834850126477]
	TIME [epoch: 17.5 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21361435781954355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21361435781954355 | validation: 0.1445004276486253]
	TIME [epoch: 17.5 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1293621787845935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1293621787845935 | validation: 0.2148886802859273]
	TIME [epoch: 17.5 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15401730767043703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15401730767043703 | validation: 0.17597256043471982]
	TIME [epoch: 17.5 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13787934397601795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13787934397601795 | validation: 0.32647203867057883]
	TIME [epoch: 17.5 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26763159157957694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26763159157957694 | validation: 0.3679835409868646]
	TIME [epoch: 17.5 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3337950966390504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3337950966390504 | validation: 0.34775158467451117]
	TIME [epoch: 17.6 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.307216041690892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.307216041690892 | validation: 0.2802987289971451]
	TIME [epoch: 17.5 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2500007512746719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2500007512746719 | validation: 0.20953394102054076]
	TIME [epoch: 17.6 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17825224389070876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17825224389070876 | validation: 0.14022336656699824]
	TIME [epoch: 17.5 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1632566774331838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1632566774331838 | validation: 0.1624016304211607]
	TIME [epoch: 17.6 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2036722055834262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2036722055834262 | validation: 0.1463272167165556]
	TIME [epoch: 17.5 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12931195413110932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12931195413110932 | validation: 0.11376709682483928]
	TIME [epoch: 17.5 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_649.pth
	Model improved!!!
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11449121590021197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11449121590021197 | validation: 0.10784404702856398]
	TIME [epoch: 17.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_650.pth
	Model improved!!!
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11774829790129118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11774829790129118 | validation: 0.1412620937638813]
	TIME [epoch: 17.5 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15024302781708246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15024302781708246 | validation: 0.28687144621042326]
	TIME [epoch: 17.5 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26204997690860593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26204997690860593 | validation: 0.19613412885512405]
	TIME [epoch: 17.6 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17032929444598444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17032929444598444 | validation: 0.10531384401005371]
	TIME [epoch: 17.5 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_654.pth
	Model improved!!!
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11301033858391499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11301033858391499 | validation: 0.12307856672402369]
	TIME [epoch: 17.5 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13778856105680537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13778856105680537 | validation: 1.5115250787587233]
	TIME [epoch: 17.5 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0097596237643094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0097596237643094 | validation: 0.6400919241757237]
	TIME [epoch: 17.5 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7923533824634277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7923533824634277 | validation: 0.4114917586787845]
	TIME [epoch: 17.5 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3746315805673862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3746315805673862 | validation: 0.23680850875481246]
	TIME [epoch: 17.5 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28957170790777453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28957170790777453 | validation: 0.41595130314846984]
	TIME [epoch: 17.5 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4857482494808663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4857482494808663 | validation: 0.3333816480255233]
	TIME [epoch: 17.5 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3265320489116988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3265320489116988 | validation: 0.40508181612811517]
	TIME [epoch: 17.5 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3463520996753647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3463520996753647 | validation: 0.4002172557461856]
	TIME [epoch: 17.5 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33054108920858627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33054108920858627 | validation: 0.38543088953227733]
	TIME [epoch: 17.5 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3116702011216816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3116702011216816 | validation: 0.2887770715194154]
	TIME [epoch: 17.5 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24800358759333077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24800358759333077 | validation: 0.23321286514746087]
	TIME [epoch: 17.5 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19938837695666486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19938837695666486 | validation: 0.15954855620231562]
	TIME [epoch: 17.5 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15456639847094436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15456639847094436 | validation: 0.11834197802170615]
	TIME [epoch: 17.5 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14202493877176692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14202493877176692 | validation: 0.15072557898079963]
	TIME [epoch: 17.5 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16544996304448126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16544996304448126 | validation: 0.7427235017980114]
	TIME [epoch: 17.5 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7547758356468481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7547758356468481 | validation: 0.39616548149950576]
	TIME [epoch: 17.5 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.313241060349579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.313241060349579 | validation: 0.33109497964436185]
	TIME [epoch: 17.5 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2815994039486222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2815994039486222 | validation: 0.22166382192430079]
	TIME [epoch: 17.5 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16934232780718136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16934232780718136 | validation: 0.19496886980637626]
	TIME [epoch: 17.5 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15896096709595145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15896096709595145 | validation: 0.17242670837109897]
	TIME [epoch: 17.5 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13902923872910633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13902923872910633 | validation: 0.17467843183865245]
	TIME [epoch: 17.5 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13542767377306084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13542767377306084 | validation: 0.1580324670960569]
	TIME [epoch: 17.5 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13332737102735565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13332737102735565 | validation: 0.1640571690100796]
	TIME [epoch: 17.5 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14742925914799332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14742925914799332 | validation: 0.21665109901725801]
	TIME [epoch: 17.5 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16199599022426733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16199599022426733 | validation: 0.20844414295045724]
	TIME [epoch: 17.5 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15102392446075297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15102392446075297 | validation: 0.14456811976503572]
	TIME [epoch: 17.5 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13704541724663585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13704541724663585 | validation: 0.1248286255068277]
	TIME [epoch: 17.3 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1193514770992852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1193514770992852 | validation: 0.137552497135585]
	TIME [epoch: 17.3 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12230491107938128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12230491107938128 | validation: 0.1768420387795342]
	TIME [epoch: 17.2 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17114885185333253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17114885185333253 | validation: 0.21125811558075416]
	TIME [epoch: 17.2 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18260304266784377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18260304266784377 | validation: 0.1977131047836319]
	TIME [epoch: 17.2 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14910379956478736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14910379956478736 | validation: 0.13717925474333328]
	TIME [epoch: 17.3 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12050558541097121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12050558541097121 | validation: 0.1109281535052133]
	TIME [epoch: 17.2 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10500763251795033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10500763251795033 | validation: 0.09383592515835394]
	TIME [epoch: 17.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_689.pth
	Model improved!!!
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11303731104186387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11303731104186387 | validation: 0.13363874460082678]
	TIME [epoch: 17.3 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10798262924219647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10798262924219647 | validation: 0.1320886364760471]
	TIME [epoch: 17.3 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11894355933129781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11894355933129781 | validation: 0.2456491872219079]
	TIME [epoch: 17.3 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24377185852552655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24377185852552655 | validation: 0.3450320358528376]
	TIME [epoch: 17.2 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42696775061515707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42696775061515707 | validation: 0.4000065606364137]
	TIME [epoch: 17.2 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4306224353601549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4306224353601549 | validation: 0.3651365823773887]
	TIME [epoch: 17.3 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4006144614998074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4006144614998074 | validation: 0.3844149327454283]
	TIME [epoch: 17.2 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3853817080170572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3853817080170572 | validation: 0.37469608707843194]
	TIME [epoch: 17.2 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38173093811041925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38173093811041925 | validation: 0.30849264985331504]
	TIME [epoch: 17.3 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30645366042253996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30645366042253996 | validation: 0.18217309667229348]
	TIME [epoch: 17.2 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18905269186993978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18905269186993978 | validation: 0.15522480290071428]
	TIME [epoch: 17.4 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20222949082405212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20222949082405212 | validation: 0.29702854028677855]
	TIME [epoch: 17.2 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4562876248207475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4562876248207475 | validation: 0.24968748061333926]
	TIME [epoch: 17.2 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2528265498802924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2528265498802924 | validation: 0.5994509602403658]
	TIME [epoch: 17.3 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7457667748813784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7457667748813784 | validation: 0.3263368916805771]
	TIME [epoch: 17.2 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2621271768979661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2621271768979661 | validation: 0.15355109104168524]
	TIME [epoch: 17.3 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19801248602155602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19801248602155602 | validation: 0.10641472474411331]
	TIME [epoch: 17.3 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13747077536499952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13747077536499952 | validation: 0.17619438710404312]
	TIME [epoch: 17.2 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14749392076790435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14749392076790435 | validation: 0.1454835519172383]
	TIME [epoch: 17.2 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1302351301086452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1302351301086452 | validation: 0.15374047438481797]
	TIME [epoch: 17.2 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14041214161092735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14041214161092735 | validation: 0.2940529147803878]
	TIME [epoch: 17.3 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2826830982830896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2826830982830896 | validation: 0.225378617652646]
	TIME [epoch: 17.2 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20172793186740343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20172793186740343 | validation: 0.21768802417199973]
	TIME [epoch: 17.3 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1977867024099376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1977867024099376 | validation: 0.16431327092684112]
	TIME [epoch: 17.3 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14599086022048544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14599086022048544 | validation: 0.11880235290589876]
	TIME [epoch: 17.3 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15402728665328735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15402728665328735 | validation: 0.12206973914869433]
	TIME [epoch: 17.4 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13525776829071703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13525776829071703 | validation: 0.1236222433068444]
	TIME [epoch: 17.4 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12597980165865008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12597980165865008 | validation: 0.10419543640932952]
	TIME [epoch: 17.3 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11665047086759937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11665047086759937 | validation: 0.3738153714914614]
	TIME [epoch: 17.3 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48111975904700255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48111975904700255 | validation: 0.29929194586500113]
	TIME [epoch: 17.4 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25027528259630233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25027528259630233 | validation: 0.690309917463806]
	TIME [epoch: 17.4 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.770032230930623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.770032230930623 | validation: 0.5457681288566546]
	TIME [epoch: 17.3 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5464394014174251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5464394014174251 | validation: 0.27324063808088755]
	TIME [epoch: 17.4 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2362782403921824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2362782403921824 | validation: 0.20178511582706032]
	TIME [epoch: 17.4 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18624518191726216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18624518191726216 | validation: 0.16831959774557853]
	TIME [epoch: 17.4 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1681530999999992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1681530999999992 | validation: 0.16675599273726746]
	TIME [epoch: 17.4 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14669568650632991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14669568650632991 | validation: 0.16745357502745273]
	TIME [epoch: 17.3 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14812112517397785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14812112517397785 | validation: 0.1658911908539653]
	TIME [epoch: 17.3 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1459304671935107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1459304671935107 | validation: 0.15330138315851682]
	TIME [epoch: 17.3 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1367064050723719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1367064050723719 | validation: 0.15228369401557673]
	TIME [epoch: 17.3 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12974530004529708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12974530004529708 | validation: 0.15789201156569718]
	TIME [epoch: 17.3 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12885413556973466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12885413556973466 | validation: 0.1590091654893877]
	TIME [epoch: 17.3 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14234241668328929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14234241668328929 | validation: 0.2076076385229434]
	TIME [epoch: 17.3 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18520759490234143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18520759490234143 | validation: 0.2011730501461586]
	TIME [epoch: 17.2 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16305727853461527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16305727853461527 | validation: 0.16977978117569484]
	TIME [epoch: 17.3 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12933423179614248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12933423179614248 | validation: 0.13083831362736606]
	TIME [epoch: 17.3 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12260436765931548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12260436765931548 | validation: 0.135041623746229]
	TIME [epoch: 17.3 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13394505527587533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13394505527587533 | validation: 0.16321558991201515]
	TIME [epoch: 17.3 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1365513535711553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1365513535711553 | validation: 0.12313728552846231]
	TIME [epoch: 17.2 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10829685248687676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10829685248687676 | validation: 0.11551811537311477]
	TIME [epoch: 17.2 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11150482833771336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11150482833771336 | validation: 0.1468022710722958]
	TIME [epoch: 17.4 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1277889960812781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1277889960812781 | validation: 0.1502792770437419]
	TIME [epoch: 17.2 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13613407864714144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13613407864714144 | validation: 0.13158069017177848]
	TIME [epoch: 17.4 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11672260575991197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11672260575991197 | validation: 0.163425268869696]
	TIME [epoch: 17.2 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13385545605759422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13385545605759422 | validation: 0.14855729747386973]
	TIME [epoch: 17.3 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11399053972038811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11399053972038811 | validation: 0.09385648975705485]
	TIME [epoch: 17.3 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10570758465567216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10570758465567216 | validation: 0.10952849321725805]
	TIME [epoch: 17.4 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11035410895947129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11035410895947129 | validation: 0.12375474043867381]
	TIME [epoch: 17.5 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11592992526307853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11592992526307853 | validation: 0.24388628978833268]
	TIME [epoch: 17.5 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30303401332539137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30303401332539137 | validation: 0.41146010958881557]
	TIME [epoch: 17.4 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33206160936786183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33206160936786183 | validation: 0.5097754552571389]
	TIME [epoch: 17.4 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4137458058623836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4137458058623836 | validation: 0.2822089948344666]
	TIME [epoch: 17.5 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3344856434765228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3344856434765228 | validation: 0.18248875629223102]
	TIME [epoch: 17.5 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1825723928084097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1825723928084097 | validation: 0.14315023882714437]
	TIME [epoch: 17.3 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19986151896808516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19986151896808516 | validation: 0.20265966288686643]
	TIME [epoch: 17.4 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22335813226860765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22335813226860765 | validation: 0.15630203173882384]
	TIME [epoch: 17.5 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16298942271374486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16298942271374486 | validation: 0.13321969205387157]
	TIME [epoch: 17.5 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.124843663002926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.124843663002926 | validation: 0.11083075627426485]
	TIME [epoch: 17.4 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11787455297363546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11787455297363546 | validation: 0.13286945621858615]
	TIME [epoch: 17.4 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12641033016672432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12641033016672432 | validation: 0.12077004979567146]
	TIME [epoch: 17.4 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12048002272004524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12048002272004524 | validation: 0.13964963173958256]
	TIME [epoch: 17.3 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11393186979797972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11393186979797972 | validation: 0.10522101932012445]
	TIME [epoch: 17.5 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09436729456895128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09436729456895128 | validation: 0.087231012099568]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_762.pth
	Model improved!!!
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09673838904559821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09673838904559821 | validation: 0.49508569889720333]
	TIME [epoch: 17.4 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7819735288248084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7819735288248084 | validation: 0.6163808476919438]
	TIME [epoch: 17.4 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4468448803084941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4468448803084941 | validation: 0.4425094303053264]
	TIME [epoch: 17.4 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3213624655298517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3213624655298517 | validation: 0.296603529956073]
	TIME [epoch: 17.4 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23560885970698242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23560885970698242 | validation: 0.19431902484789884]
	TIME [epoch: 17.4 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18440199930694134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18440199930694134 | validation: 0.226096813439714]
	TIME [epoch: 17.3 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3280990751881791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3280990751881791 | validation: 0.24273598183300546]
	TIME [epoch: 17.3 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2023689855456938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2023689855456938 | validation: 0.23412233441571903]
	TIME [epoch: 17.3 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18098655440822903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18098655440822903 | validation: 0.6152074057559318]
	TIME [epoch: 17.3 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7185494153443104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7185494153443104 | validation: 0.46138662629947946]
	TIME [epoch: 17.4 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39873763780559757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39873763780559757 | validation: 0.30118660387488516]
	TIME [epoch: 17.3 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2844879043894501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2844879043894501 | validation: 0.15632734838478346]
	TIME [epoch: 17.3 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17718054642864034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17718054642864034 | validation: 0.17032091505151742]
	TIME [epoch: 17.4 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1709225649959454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1709225649959454 | validation: 0.14424850371753756]
	TIME [epoch: 17.4 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14538943863907325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14538943863907325 | validation: 0.11501992639857528]
	TIME [epoch: 17.3 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12999217215268646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12999217215268646 | validation: 0.13862039727566017]
	TIME [epoch: 17.4 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13994796176367083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13994796176367083 | validation: 0.15942715765004822]
	TIME [epoch: 17.3 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1403457077481238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1403457077481238 | validation: 0.11345210561730101]
	TIME [epoch: 17.4 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10944758043602032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10944758043602032 | validation: 0.0987038540528062]
	TIME [epoch: 17.3 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10272800745988132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10272800745988132 | validation: 0.10691784593604817]
	TIME [epoch: 17.3 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10366270056687003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10366270056687003 | validation: 0.11277519565557022]
	TIME [epoch: 17.5 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10651393197521428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10651393197521428 | validation: 0.10384724106067532]
	TIME [epoch: 17.6 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10338263624534473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10338263624534473 | validation: 0.11565023735563965]
	TIME [epoch: 17.6 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10616272216176675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10616272216176675 | validation: 0.09309928816652334]
	TIME [epoch: 17.3 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10583474917982129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10583474917982129 | validation: 0.21869836399139442]
	TIME [epoch: 17.2 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1644882789381978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1644882789381978 | validation: 0.14512708410454206]
	TIME [epoch: 17.3 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13340698233076734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13340698233076734 | validation: 0.09956527547686173]
	TIME [epoch: 17.2 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1846216223240556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1846216223240556 | validation: 0.25086088528872497]
	TIME [epoch: 17.3 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23737675666184493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23737675666184493 | validation: 0.15140913917337842]
	TIME [epoch: 17.2 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1430330090840589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1430330090840589 | validation: 0.22712469902656865]
	TIME [epoch: 17.3 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20311899137036155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20311899137036155 | validation: 0.16700985380304811]
	TIME [epoch: 17.3 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18542062374920712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18542062374920712 | validation: 0.14201113775285795]
	TIME [epoch: 17.3 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15317273804396536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15317273804396536 | validation: 0.0816586975787937]
	TIME [epoch: 17.3 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_795.pth
	Model improved!!!
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12869504083358332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12869504083358332 | validation: 0.9172490517808729]
	TIME [epoch: 17.4 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0873375277554533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0873375277554533 | validation: 0.2632225292160742]
	TIME [epoch: 17.3 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42381964214885215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42381964214885215 | validation: 0.16846308899321186]
	TIME [epoch: 17.3 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1833449358328118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1833449358328118 | validation: 0.2051301366014756]
	TIME [epoch: 17.4 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18324799735092456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18324799735092456 | validation: 0.17383130135227304]
	TIME [epoch: 17.3 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17412096972342006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17412096972342006 | validation: 0.201813405681473]
	TIME [epoch: 17.2 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18638151356535712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18638151356535712 | validation: 0.1608251172438916]
	TIME [epoch: 17.3 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17818739100472464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17818739100472464 | validation: 0.16798628348317501]
	TIME [epoch: 17.3 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1724903578177767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1724903578177767 | validation: 0.14010752871650184]
	TIME [epoch: 17.2 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14986052279535345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14986052279535345 | validation: 0.15815435889980364]
	TIME [epoch: 17.3 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15891413203553748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15891413203553748 | validation: 0.1254577676914771]
	TIME [epoch: 17.3 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.149333176735687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.149333176735687 | validation: 0.12622482202063942]
	TIME [epoch: 17.2 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.135875854866198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.135875854866198 | validation: 0.12484488122163784]
	TIME [epoch: 17.2 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.135022620906987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.135022620906987 | validation: 0.12033765488691742]
	TIME [epoch: 17.2 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1350604903801678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1350604903801678 | validation: 0.12668373657428106]
	TIME [epoch: 17.2 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1280112622119176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1280112622119176 | validation: 0.12871321113668038]
	TIME [epoch: 17.2 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14122928413341915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14122928413341915 | validation: 0.13261020564241927]
	TIME [epoch: 17.2 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1551328643009256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1551328643009256 | validation: 0.16823450636653928]
	TIME [epoch: 17.3 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18541929326164983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18541929326164983 | validation: 0.1231775783006362]
	TIME [epoch: 17.1 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1648631202802742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1648631202802742 | validation: 0.09594668315008206]
	TIME [epoch: 17.2 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12206218002059284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12206218002059284 | validation: 0.1343933267379101]
	TIME [epoch: 17.2 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14560927790108555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14560927790108555 | validation: 0.1270510867258787]
	TIME [epoch: 17.2 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14978318727640066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14978318727640066 | validation: 0.1281387842571522]
	TIME [epoch: 17.2 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15985590440217787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15985590440217787 | validation: 0.18603209715527008]
	TIME [epoch: 17.2 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16371366781661947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16371366781661947 | validation: 0.0904585378679525]
	TIME [epoch: 17.2 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10228593927677768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10228593927677768 | validation: 0.07960737549213237]
	TIME [epoch: 17.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_821.pth
	Model improved!!!
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09502604884659814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09502604884659814 | validation: 0.08605423584940168]
	TIME [epoch: 17.2 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09587181567407317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09587181567407317 | validation: 0.0804697413348709]
	TIME [epoch: 17.3 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10479726878256415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10479726878256415 | validation: 0.11890456910097079]
	TIME [epoch: 17.3 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15195729641486375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15195729641486375 | validation: 0.25425884749975414]
	TIME [epoch: 17.3 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21557746481033888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21557746481033888 | validation: 0.16594976534785325]
	TIME [epoch: 17.2 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14284513934713367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14284513934713367 | validation: 0.07990699586792227]
	TIME [epoch: 17.3 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1057089336434304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1057089336434304 | validation: 0.07286324003903581]
	TIME [epoch: 17.3 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_828.pth
	Model improved!!!
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09463496613492475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09463496613492475 | validation: 0.07235390293308812]
	TIME [epoch: 17.3 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_829.pth
	Model improved!!!
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09419170687114103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09419170687114103 | validation: 0.10552354369543711]
	TIME [epoch: 17.3 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11296900794451376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11296900794451376 | validation: 0.10913395271596858]
	TIME [epoch: 17.3 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12442092471302912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12442092471302912 | validation: 0.13482064278571002]
	TIME [epoch: 17.2 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1407573063115796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1407573063115796 | validation: 0.09156262070140818]
	TIME [epoch: 17.2 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08909463302383694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08909463302383694 | validation: 0.10075553523804469]
	TIME [epoch: 17.3 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11581163796224464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11581163796224464 | validation: 0.17680871909838433]
	TIME [epoch: 17.2 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17069444581951826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17069444581951826 | validation: 0.1900723576423454]
	TIME [epoch: 17.4 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16524170328721888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16524170328721888 | validation: 0.09037454624186145]
	TIME [epoch: 17.3 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10702314365977972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10702314365977972 | validation: 0.08999334071937509]
	TIME [epoch: 17.4 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11036502501165892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11036502501165892 | validation: 0.16515091620952832]
	TIME [epoch: 17.3 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12646291742019172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12646291742019172 | validation: 0.09568568648366214]
	TIME [epoch: 17.3 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09598157062235789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09598157062235789 | validation: 0.07486186486013992]
	TIME [epoch: 17.3 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09969135495269718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09969135495269718 | validation: 0.1339178009975195]
	TIME [epoch: 17.3 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11527778561395487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11527778561395487 | validation: 0.09523744047377965]
	TIME [epoch: 17.2 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1151473488463346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1151473488463346 | validation: 0.10526566169251268]
	TIME [epoch: 17.2 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1214420924340006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1214420924340006 | validation: 0.21028589104986334]
	TIME [epoch: 17.2 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16126848889693704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16126848889693704 | validation: 0.09403373568436141]
	TIME [epoch: 17.4 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0871012819465688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0871012819465688 | validation: 0.07035257894499818]
	TIME [epoch: 17.3 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_847.pth
	Model improved!!!
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1025322629337353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1025322629337353 | validation: 0.17095061633590014]
	TIME [epoch: 17.5 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1164409979728604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1164409979728604 | validation: 0.09537047342956961]
	TIME [epoch: 17.5 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11742944137630079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11742944137630079 | validation: 0.13169697485608567]
	TIME [epoch: 17.5 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16081193406568225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16081193406568225 | validation: 0.17366324822902374]
	TIME [epoch: 17.5 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13533667374264627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13533667374264627 | validation: 0.15672933094033162]
	TIME [epoch: 17.3 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10295278203549738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10295278203549738 | validation: 0.08117233503966322]
	TIME [epoch: 17.3 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09029269799484489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09029269799484489 | validation: 0.10326092088076405]
	TIME [epoch: 17.4 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10522673836628188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10522673836628188 | validation: 0.10612667059880386]
	TIME [epoch: 17.3 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14192613953283253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14192613953283253 | validation: 0.08062738560775402]
	TIME [epoch: 17.4 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09655285665145844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09655285665145844 | validation: 0.08123314038588232]
	TIME [epoch: 17.3 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0951670952930716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0951670952930716 | validation: 0.2868041093651791]
	TIME [epoch: 17.3 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21100873162747844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21100873162747844 | validation: 0.1730209193617681]
	TIME [epoch: 17.3 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11885284357213508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11885284357213508 | validation: 0.08639277231393641]
	TIME [epoch: 17.3 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09547181347205613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09547181347205613 | validation: 0.0746368802228284]
	TIME [epoch: 17.3 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07710097612441921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07710097612441921 | validation: 0.08010069864631762]
	TIME [epoch: 17.3 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09047417173272457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09047417173272457 | validation: 0.12105584934144933]
	TIME [epoch: 17.3 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11326125196817545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11326125196817545 | validation: 0.1428081721579423]
	TIME [epoch: 17.4 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18194246321292348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18194246321292348 | validation: 0.15366771215119213]
	TIME [epoch: 17.5 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1576719738955093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1576719738955093 | validation: 0.1305970738598938]
	TIME [epoch: 17.5 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12280012076377621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12280012076377621 | validation: 0.10003418538455439]
	TIME [epoch: 17.5 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10322939718098112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10322939718098112 | validation: 0.09324649830944697]
	TIME [epoch: 17.6 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10277160242632785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10277160242632785 | validation: 0.09401058842171199]
	TIME [epoch: 17.6 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08689508357961928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08689508357961928 | validation: 0.07309490176971549]
	TIME [epoch: 17.5 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08733857683039441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08733857683039441 | validation: 0.13896925105116167]
	TIME [epoch: 17.6 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12768478424029212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12768478424029212 | validation: 0.09484769871735262]
	TIME [epoch: 17.6 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12620540202667724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12620540202667724 | validation: 0.0942132700934256]
	TIME [epoch: 17.5 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1184234706590731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1184234706590731 | validation: 0.19145743834189635]
	TIME [epoch: 17.6 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14286798060664804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14286798060664804 | validation: 0.08650049301000778]
	TIME [epoch: 17.6 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09451270482699267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09451270482699267 | validation: 0.08270547370010606]
	TIME [epoch: 17.6 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10910759276136173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10910759276136173 | validation: 0.126578747048903]
	TIME [epoch: 17.5 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.114906279128568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.114906279128568 | validation: 0.09071083413900992]
	TIME [epoch: 17.6 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08907940187919353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08907940187919353 | validation: 0.08361345060900294]
	TIME [epoch: 17.6 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07539650306206146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07539650306206146 | validation: 0.11010913049597604]
	TIME [epoch: 17.6 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12009048118128374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12009048118128374 | validation: 0.15699138816538674]
	TIME [epoch: 17.5 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1034925679916996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1034925679916996 | validation: 0.1172105965990442]
	TIME [epoch: 17.5 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09243548592796191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09243548592796191 | validation: 0.1254205025020266]
	TIME [epoch: 17.6 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1377417713294036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1377417713294036 | validation: 0.2612484326568348]
	TIME [epoch: 17.6 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16372639376183934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16372639376183934 | validation: 0.14511825811828588]
	TIME [epoch: 17.5 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09764067553466536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09764067553466536 | validation: 1.3292495447966588]
	TIME [epoch: 17.6 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2120586109418394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2120586109418394 | validation: 0.9353109714671926]
	TIME [epoch: 17.5 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.959088907940733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.959088907940733 | validation: 0.31023860171100903]
	TIME [epoch: 17.6 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2628316331588099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2628316331588099 | validation: 0.2065258022619553]
	TIME [epoch: 17.5 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1409011158435884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1409011158435884 | validation: 0.22318479122116808]
	TIME [epoch: 17.5 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12459056189466722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12459056189466722 | validation: 0.19365604320296104]
	TIME [epoch: 17.6 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10533261020648489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10533261020648489 | validation: 0.17276361461280304]
	TIME [epoch: 17.6 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09894845956983528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09894845956983528 | validation: 0.10785428504870137]
	TIME [epoch: 17.6 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08501379994108126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08501379994108126 | validation: 0.07563287913475929]
	TIME [epoch: 17.6 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07925440193181642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07925440193181642 | validation: 0.11421886328759633]
	TIME [epoch: 17.6 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09165407159449451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09165407159449451 | validation: 0.10553385241303165]
	TIME [epoch: 17.6 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11186474729080499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11186474729080499 | validation: 0.14669288608815265]
	TIME [epoch: 17.5 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11284684686275291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11284684686275291 | validation: 0.09718078129654079]
	TIME [epoch: 17.5 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09379909735218096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09379909735218096 | validation: 0.07330304714806132]
	TIME [epoch: 17.6 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08002579825753951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08002579825753951 | validation: 0.08138074955515306]
	TIME [epoch: 17.5 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07500478187878706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07500478187878706 | validation: 0.06675829246487354]
	TIME [epoch: 17.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_901.pth
	Model improved!!!
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08086638812075016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08086638812075016 | validation: 0.0748984124748035]
	TIME [epoch: 17.6 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07069610888270013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07069610888270013 | validation: 0.06537179647830113]
	TIME [epoch: 17.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_903.pth
	Model improved!!!
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06559953573854924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06559953573854924 | validation: 0.07162755113042793]
	TIME [epoch: 17.6 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06208114874064114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06208114874064114 | validation: 0.06989518109938296]
	TIME [epoch: 17.6 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06910319772428476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06910319772428476 | validation: 0.0815635520361499]
	TIME [epoch: 17.6 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0865001682456608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0865001682456608 | validation: 0.13446847440169793]
	TIME [epoch: 17.6 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12331510847155434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12331510847155434 | validation: 0.11789973758578705]
	TIME [epoch: 17.5 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13559431501541278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13559431501541278 | validation: 0.1921647200494094]
	TIME [epoch: 17.6 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17833935138210535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17833935138210535 | validation: 0.1446239477276209]
	TIME [epoch: 17.6 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12533839472312777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12533839472312777 | validation: 0.07564002569664999]
	TIME [epoch: 17.6 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08027724184911439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08027724184911439 | validation: 0.08012700858834321]
	TIME [epoch: 17.6 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1026001731207864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1026001731207864 | validation: 0.13459556537420456]
	TIME [epoch: 17.6 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0971377467500769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0971377467500769 | validation: 0.07780840495115404]
	TIME [epoch: 17.6 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0867803755820623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0867803755820623 | validation: 0.07502717350914295]
	TIME [epoch: 17.6 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07619783015806668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07619783015806668 | validation: 0.06615975062806541]
	TIME [epoch: 17.5 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07173434872687949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07173434872687949 | validation: 0.1321717728370608]
	TIME [epoch: 17.5 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10781487855166982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10781487855166982 | validation: 0.10348821191546907]
	TIME [epoch: 17.5 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09641655391669794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09641655391669794 | validation: 0.06759466333087683]
	TIME [epoch: 17.6 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07735560504878075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07735560504878075 | validation: 0.0709095442143937]
	TIME [epoch: 17.6 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08444201579596357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08444201579596357 | validation: 0.1756806619889375]
	TIME [epoch: 17.6 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1181146183102183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1181146183102183 | validation: 0.10417056006239177]
	TIME [epoch: 17.5 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10112743012347764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10112743012347764 | validation: 0.08570024307550654]
	TIME [epoch: 17.5 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08322691473486035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08322691473486035 | validation: 0.1108651152901113]
	TIME [epoch: 17.6 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09683990164685356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09683990164685356 | validation: 0.13215108445579524]
	TIME [epoch: 17.6 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11501323232886895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11501323232886895 | validation: 0.10015592065211375]
	TIME [epoch: 17.5 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09831266064631444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09831266064631444 | validation: 0.10609272017411149]
	TIME [epoch: 17.6 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10288765865400543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10288765865400543 | validation: 0.1450681697334676]
	TIME [epoch: 17.5 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.124539772501178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.124539772501178 | validation: 0.07153405908318153]
	TIME [epoch: 17.5 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10259537322989778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10259537322989778 | validation: 0.06840345065025523]
	TIME [epoch: 17.5 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07462793772630662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07462793772630662 | validation: 0.05562333875628004]
	TIME [epoch: 17.5 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_931.pth
	Model improved!!!
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0896637173713645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0896637173713645 | validation: 0.22059180577479676]
	TIME [epoch: 17.6 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17847971710632912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17847971710632912 | validation: 0.30177795392560713]
	TIME [epoch: 17.5 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20182963455913136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20182963455913136 | validation: 0.11163286903646151]
	TIME [epoch: 17.6 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08638556879327865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08638556879327865 | validation: 0.07575449409722457]
	TIME [epoch: 17.6 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08418095534550052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08418095534550052 | validation: 0.06570461989104749]
	TIME [epoch: 17.5 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07801515324334977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07801515324334977 | validation: 0.08057032805484277]
	TIME [epoch: 17.6 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07992976829558372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07992976829558372 | validation: 0.06725751900980387]
	TIME [epoch: 17.6 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0729306341379023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0729306341379023 | validation: 0.06985285374741577]
	TIME [epoch: 17.6 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07215935533704845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07215935533704845 | validation: 0.12303193396895168]
	TIME [epoch: 17.5 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11878325763730445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11878325763730445 | validation: 0.3260479867224385]
	TIME [epoch: 17.6 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33924737511324593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33924737511324593 | validation: 0.29752775893864314]
	TIME [epoch: 17.6 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2783089209428165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2783089209428165 | validation: 0.2880575129980602]
	TIME [epoch: 17.6 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21888312338017885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21888312338017885 | validation: 0.24585196148330002]
	TIME [epoch: 17.6 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2246951153753188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2246951153753188 | validation: 0.33713698535761316]
	TIME [epoch: 17.6 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29259671772445744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29259671772445744 | validation: 0.23198496307792366]
	TIME [epoch: 17.5 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19991933458590858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19991933458590858 | validation: 0.14519320072299036]
	TIME [epoch: 17.6 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17102538258196207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17102538258196207 | validation: 0.08733405628581431]
	TIME [epoch: 17.6 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12262421952106851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12262421952106851 | validation: 0.08663141432058447]
	TIME [epoch: 17.6 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11977265871146209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11977265871146209 | validation: 0.08965428238375389]
	TIME [epoch: 17.6 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11353003105780397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11353003105780397 | validation: 0.10560768914177124]
	TIME [epoch: 17.6 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10496095488347344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10496095488347344 | validation: 0.13501277861390532]
	TIME [epoch: 17.6 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09868053778064116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09868053778064116 | validation: 0.10137532026529344]
	TIME [epoch: 17.5 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0887075369566337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0887075369566337 | validation: 0.08196336539354356]
	TIME [epoch: 17.5 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08018355300929322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08018355300929322 | validation: 0.08796735834970887]
	TIME [epoch: 17.6 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08378649115507898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08378649115507898 | validation: 0.10736443530601875]
	TIME [epoch: 17.5 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08804622872716164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08804622872716164 | validation: 0.10698266353385391]
	TIME [epoch: 17.6 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0831114976682288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0831114976682288 | validation: 0.08284683248241242]
	TIME [epoch: 17.5 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0855340753874526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0855340753874526 | validation: 0.09609291432210731]
	TIME [epoch: 17.6 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08525467894890142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08525467894890142 | validation: 0.08752816118207542]
	TIME [epoch: 17.5 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08657859150129342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08657859150129342 | validation: 0.09634752754357816]
	TIME [epoch: 17.6 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08878665483843876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08878665483843876 | validation: 0.09544832184229303]
	TIME [epoch: 17.5 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07968990557723077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07968990557723077 | validation: 0.08096632146603328]
	TIME [epoch: 17.5 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07149210969966918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07149210969966918 | validation: 0.08225672229558037]
	TIME [epoch: 17.5 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07694799760161418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07694799760161418 | validation: 0.10717202330277327]
	TIME [epoch: 17.6 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10914085466650626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10914085466650626 | validation: 0.2458689261508519]
	TIME [epoch: 17.6 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19250975592553726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19250975592553726 | validation: 0.14925061883862653]
	TIME [epoch: 17.6 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10887833322930558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10887833322930558 | validation: 0.07915455339327576]
	TIME [epoch: 17.6 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0728529561809411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0728529561809411 | validation: 0.08764628400756418]
	TIME [epoch: 17.6 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09712251242149762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09712251242149762 | validation: 0.10248309937176221]
	TIME [epoch: 17.6 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08434116193564531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08434116193564531 | validation: 0.09613341484635284]
	TIME [epoch: 17.6 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0778810866195503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0778810866195503 | validation: 0.07739508592554523]
	TIME [epoch: 17.5 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.080961314495035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.080961314495035 | validation: 0.07016078751251054]
	TIME [epoch: 17.6 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08421709865876167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08421709865876167 | validation: 0.07386191370360522]
	TIME [epoch: 17.5 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09743762056219189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09743762056219189 | validation: 0.10600302411684354]
	TIME [epoch: 17.6 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08384681191089215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08384681191089215 | validation: 0.08954521260011761]
	TIME [epoch: 17.6 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07216462161545267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07216462161545267 | validation: 0.07338277622921303]
	TIME [epoch: 17.6 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07327033693062894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07327033693062894 | validation: 0.10814927348667913]
	TIME [epoch: 17.5 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08850272240457806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08850272240457806 | validation: 0.06274966845498146]
	TIME [epoch: 17.6 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07023624414561669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07023624414561669 | validation: 0.07184697630511017]
	TIME [epoch: 17.6 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08046134221585657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08046134221585657 | validation: 0.15457353150966183]
	TIME [epoch: 17.6 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1158264124011956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1158264124011956 | validation: 0.09662697491750764]
	TIME [epoch: 17.6 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0849865114278243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0849865114278243 | validation: 0.07182585373852828]
	TIME [epoch: 17.6 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08606618441447651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08606618441447651 | validation: 0.1492008041062344]
	TIME [epoch: 17.6 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11540682665250306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11540682665250306 | validation: 0.12367152205094714]
	TIME [epoch: 17.3 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1087870305494622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1087870305494622 | validation: 0.08364122119709322]
	TIME [epoch: 17.3 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07534844494229018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07534844494229018 | validation: 0.05798231689398012]
	TIME [epoch: 17.3 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07542051045874162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07542051045874162 | validation: 0.06768118272789965]
	TIME [epoch: 17.2 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06031037568884884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06031037568884884 | validation: 0.04959545836131834]
	TIME [epoch: 17.3 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_989.pth
	Model improved!!!
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07090257684195421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07090257684195421 | validation: 0.14458962138900278]
	TIME [epoch: 17.4 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10378527695924661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10378527695924661 | validation: 0.10181544961292413]
	TIME [epoch: 17.2 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07519004936149733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07519004936149733 | validation: 0.06701050984703298]
	TIME [epoch: 17.2 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07588237387816901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07588237387816901 | validation: 0.09398541046732752]
	TIME [epoch: 17.2 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0693360040093853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0693360040093853 | validation: 0.09398216557885929]
	TIME [epoch: 17.2 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.078551750945251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.078551750945251 | validation: 0.0857281249031962]
	TIME [epoch: 17.2 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0822011676903978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0822011676903978 | validation: 0.0810197367134979]
	TIME [epoch: 17.2 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07937951963186604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07937951963186604 | validation: 0.15471271955086646]
	TIME [epoch: 17.3 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17226628812057965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17226628812057965 | validation: 0.20007539799373772]
	TIME [epoch: 17.3 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14236846688882035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14236846688882035 | validation: 0.1546197805223028]
	TIME [epoch: 17.3 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08954355669818981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08954355669818981 | validation: 0.06945929450506058]
	TIME [epoch: 17.3 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05525410577096013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05525410577096013 | validation: 0.06898579892894992]
	TIME [epoch: 59.4 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09373005491733513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09373005491733513 | validation: 0.12498507818274258]
	TIME [epoch: 35.6 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07289551473186492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07289551473186492 | validation: 0.10331648512548651]
	TIME [epoch: 35.5 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08955996673434821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08955996673434821 | validation: 0.06046967301630595]
	TIME [epoch: 35.7 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06906726068636966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06906726068636966 | validation: 0.07377857379364801]
	TIME [epoch: 35.7 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0886600890198546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0886600890198546 | validation: 0.1327486718369992]
	TIME [epoch: 35.6 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10475733644673967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10475733644673967 | validation: 0.160548668029854]
	TIME [epoch: 36.1 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12107752136338724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12107752136338724 | validation: 0.08604365613040488]
	TIME [epoch: 36.3 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09959934860655409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09959934860655409 | validation: 0.09228637864063455]
	TIME [epoch: 36.3 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10388849600109783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10388849600109783 | validation: 0.09955993807578963]
	TIME [epoch: 36.3 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09065288246801152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09065288246801152 | validation: 0.07268167816798236]
	TIME [epoch: 36.2 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06412159479306838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06412159479306838 | validation: 0.043186768090089624]
	TIME [epoch: 36.3 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_1012.pth
	Model improved!!!
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05152448895892124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05152448895892124 | validation: 0.052300340637677856]
	TIME [epoch: 36.3 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048515423315774166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048515423315774166 | validation: 0.07229042133238656]
	TIME [epoch: 36.1 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05833188027574458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05833188027574458 | validation: 0.06939421311152319]
	TIME [epoch: 36.3 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07480069292316971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07480069292316971 | validation: 0.10763144194116046]
	TIME [epoch: 36.2 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09049115437749242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09049115437749242 | validation: 0.07816620251198646]
	TIME [epoch: 36.3 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09703220042397576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09703220042397576 | validation: 0.18396137558686385]
	TIME [epoch: 36.1 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12582066597218877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12582066597218877 | validation: 0.11206602571478928]
	TIME [epoch: 36.3 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08481658152869191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08481658152869191 | validation: 0.05324023102197348]
	TIME [epoch: 36.1 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06248400865104216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06248400865104216 | validation: 0.05597652875352382]
	TIME [epoch: 36.2 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0567465519972737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0567465519972737 | validation: 0.07585993810796292]
	TIME [epoch: 36.2 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07179808461144008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07179808461144008 | validation: 0.07714822054223719]
	TIME [epoch: 36.3 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07748822095028576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07748822095028576 | validation: 0.0898431905753565]
	TIME [epoch: 36.2 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07823199331633662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07823199331633662 | validation: 0.06403850439984658]
	TIME [epoch: 36.3 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08389712064994266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08389712064994266 | validation: 0.13406603162367092]
	TIME [epoch: 36.3 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11158873808796967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11158873808796967 | validation: 0.09153992459428491]
	TIME [epoch: 36.3 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08098001138240413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08098001138240413 | validation: 0.0813283388898788]
	TIME [epoch: 36.3 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10263542948647945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10263542948647945 | validation: 0.1292054188646324]
	TIME [epoch: 36.3 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09405809025498289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09405809025498289 | validation: 0.0798544289625785]
	TIME [epoch: 36.3 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0628237439638156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0628237439638156 | validation: 0.04579354836483865]
	TIME [epoch: 36.2 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05675784019809936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05675784019809936 | validation: 0.1311480244885698]
	TIME [epoch: 36.3 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09515514300582802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09515514300582802 | validation: 0.10913164550345784]
	TIME [epoch: 36.3 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08166529228745095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08166529228745095 | validation: 0.06523746258546491]
	TIME [epoch: 36.3 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0562039790811899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0562039790811899 | validation: 0.054006896581149114]
	TIME [epoch: 36.3 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06636750142873227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06636750142873227 | validation: 0.10145380813488405]
	TIME [epoch: 36.3 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07935799671828467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07935799671828467 | validation: 0.05858946886889593]
	TIME [epoch: 36.3 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07190584185147512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07190584185147512 | validation: 0.10556698947426886]
	TIME [epoch: 36.3 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06960610776099593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06960610776099593 | validation: 0.0794537251698828]
	TIME [epoch: 36.3 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07736828647485941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07736828647485941 | validation: 0.10002332805257463]
	TIME [epoch: 36.3 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09142540122044707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09142540122044707 | validation: 0.06494035650113815]
	TIME [epoch: 36.3 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06956391459436734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06956391459436734 | validation: 0.06441300954724537]
	TIME [epoch: 36.3 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05792335762531236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05792335762531236 | validation: 0.04466006620178503]
	TIME [epoch: 36.3 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04887012748035504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04887012748035504 | validation: 0.12108037668008756]
	TIME [epoch: 36.3 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13017525503995178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13017525503995178 | validation: 0.20378727439051245]
	TIME [epoch: 36.3 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14857096580913776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14857096580913776 | validation: 0.11613900295090676]
	TIME [epoch: 36.3 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07773000077214871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07773000077214871 | validation: 0.06102790522116051]
	TIME [epoch: 36.3 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06403741926088713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06403741926088713 | validation: 0.07541219749976757]
	TIME [epoch: 36.3 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08251920349528395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08251920349528395 | validation: 0.07332157055987014]
	TIME [epoch: 36.3 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0725293841972917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0725293841972917 | validation: 0.08075400507625359]
	TIME [epoch: 36.3 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0727272964954317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0727272964954317 | validation: 0.12260361792723318]
	TIME [epoch: 36.3 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10376536840013642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10376536840013642 | validation: 0.06337820916881366]
	TIME [epoch: 36.3 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06955919778137482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06955919778137482 | validation: 0.08338391274092027]
	TIME [epoch: 36.2 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08067986829868055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08067986829868055 | validation: 0.09119395909649133]
	TIME [epoch: 36.3 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08820939622864661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08820939622864661 | validation: 0.19298212443534185]
	TIME [epoch: 36.3 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12906395432767828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12906395432767828 | validation: 0.06725595323346713]
	TIME [epoch: 36.3 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09373827495716111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09373827495716111 | validation: 0.17148861165121812]
	TIME [epoch: 36.3 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14232044084633352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14232044084633352 | validation: 0.0669876441224234]
	TIME [epoch: 36.3 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054399971258377214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054399971258377214 | validation: 0.050825312748477985]
	TIME [epoch: 36.2 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07068167000915927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07068167000915927 | validation: 0.10800572305079972]
	TIME [epoch: 36.3 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07877038448442307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07877038448442307 | validation: 0.07225878565382883]
	TIME [epoch: 36.2 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052526358347872865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052526358347872865 | validation: 0.0376460419408741]
	TIME [epoch: 36.3 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_1062.pth
	Model improved!!!
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03977204649132881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03977204649132881 | validation: 0.043907289753733925]
	TIME [epoch: 36.2 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051356789861695465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051356789861695465 | validation: 0.08195089759104314]
	TIME [epoch: 36.3 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060776564959035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060776564959035 | validation: 0.24376703720275056]
	TIME [epoch: 36.2 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19979527854545268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19979527854545268 | validation: 0.1601341678192469]
	TIME [epoch: 36.3 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12515966309034604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12515966309034604 | validation: 0.07409817267014536]
	TIME [epoch: 36.2 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09601009080741855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09601009080741855 | validation: 0.08099983864182503]
	TIME [epoch: 35.6 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11153253207661268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11153253207661268 | validation: 0.07080087831026241]
	TIME [epoch: 35.6 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09283643165431142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09283643165431142 | validation: 0.16109562319697437]
	TIME [epoch: 36.3 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1272335063734019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1272335063734019 | validation: 0.12773108862431504]
	TIME [epoch: 36.3 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11378726710747802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11378726710747802 | validation: 0.06746417835102707]
	TIME [epoch: 36.3 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06497791843660823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06497791843660823 | validation: 0.04381536301217956]
	TIME [epoch: 36.3 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052887695298208665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052887695298208665 | validation: 0.050723236745521365]
	TIME [epoch: 36.3 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0490617571432562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0490617571432562 | validation: 0.04979944024695487]
	TIME [epoch: 36.3 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05221278005716752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05221278005716752 | validation: 0.08735968960649576]
	TIME [epoch: 36.2 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07189232748666562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07189232748666562 | validation: 0.05773853039172758]
	TIME [epoch: 36.3 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06483461898061249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06483461898061249 | validation: 0.07580434563318479]
	TIME [epoch: 36.3 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06645839127406604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06645839127406604 | validation: 0.050709015202955565]
	TIME [epoch: 36.3 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051400506655668254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051400506655668254 | validation: 0.09320954636364276]
	TIME [epoch: 36.3 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06307307950384651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06307307950384651 | validation: 0.08244108863764478]
	TIME [epoch: 36.3 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07712978038955497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07712978038955497 | validation: 0.10653600631538636]
	TIME [epoch: 36.3 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11192711631443984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11192711631443984 | validation: 0.12768136659491694]
	TIME [epoch: 36.3 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1048269032228895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1048269032228895 | validation: 0.06293678636994507]
	TIME [epoch: 36.3 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07267049087877286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07267049087877286 | validation: 0.09781115569250257]
	TIME [epoch: 36.3 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06196294500548943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06196294500548943 | validation: 0.0916924487890976]
	TIME [epoch: 36.3 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0687983007646602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0687983007646602 | validation: 0.05407669536274728]
	TIME [epoch: 36.3 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062081750851654766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062081750851654766 | validation: 0.0460045786163681]
	TIME [epoch: 36.3 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054126088048060524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054126088048060524 | validation: 0.04772953351850856]
	TIME [epoch: 36.3 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04879890244172522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04879890244172522 | validation: 0.043486042019155714]
	TIME [epoch: 36.3 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04997810656531037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04997810656531037 | validation: 0.14066566375007847]
	TIME [epoch: 36.3 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0977532571468816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0977532571468816 | validation: 0.06746198770211706]
	TIME [epoch: 36.3 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0612742793141072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0612742793141072 | validation: 0.04768903945641452]
	TIME [epoch: 36.3 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05789401419719766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05789401419719766 | validation: 0.05419000061894722]
	TIME [epoch: 36.3 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04311640552875473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04311640552875473 | validation: 0.0669714126123986]
	TIME [epoch: 36.3 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06375754015169363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06375754015169363 | validation: 0.0771612343200559]
	TIME [epoch: 36.3 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08186609301687614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08186609301687614 | validation: 0.08040241175292706]
	TIME [epoch: 36.3 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0714708769158107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0714708769158107 | validation: 0.06897470376376451]
	TIME [epoch: 36.3 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07495665904162664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07495665904162664 | validation: 0.10859097086961773]
	TIME [epoch: 36.3 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08038781948656634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08038781948656634 | validation: 0.04581670515636506]
	TIME [epoch: 36.2 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06848503184728266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06848503184728266 | validation: 0.10994793716774219]
	TIME [epoch: 36.2 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07961437184718323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07961437184718323 | validation: 0.082399391169105]
	TIME [epoch: 36.3 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06318289055187709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06318289055187709 | validation: 0.045611105854739424]
	TIME [epoch: 36.2 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05983255158052268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05983255158052268 | validation: 0.23283686523297994]
	TIME [epoch: 36.3 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17511585514561923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17511585514561923 | validation: 0.21998440121388008]
	TIME [epoch: 36.2 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1866917252330271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1866917252330271 | validation: 0.17519663421957976]
	TIME [epoch: 36.2 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.178525907235067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.178525907235067 | validation: 0.11152065721213784]
	TIME [epoch: 36.2 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16984081494312922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16984081494312922 | validation: 0.2799296664759893]
	TIME [epoch: 36.3 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25418184334380195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25418184334380195 | validation: 0.20627735805090222]
	TIME [epoch: 36.2 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20387170712747002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20387170712747002 | validation: 0.13963674294329598]
	TIME [epoch: 36.3 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14012114204167975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14012114204167975 | validation: 0.06436580623910677]
	TIME [epoch: 36.2 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11069400982086953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11069400982086953 | validation: 0.25282106343345206]
	TIME [epoch: 36.3 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44026430576617254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44026430576617254 | validation: 0.14585359906382725]
	TIME [epoch: 36.3 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21535852799219965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21535852799219965 | validation: 0.08566325823179208]
	TIME [epoch: 36.3 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11365230767113557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11365230767113557 | validation: 0.0718751679310391]
	TIME [epoch: 36.2 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10326038776288689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10326038776288689 | validation: 0.12436107339341765]
	TIME [epoch: 36.2 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1148490331127228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1148490331127228 | validation: 0.07741697779008792]
	TIME [epoch: 36.3 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07954573161274894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07954573161274894 | validation: 0.06312589635628175]
	TIME [epoch: 36.3 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06894660680363306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06894660680363306 | validation: 0.06350308290127135]
	TIME [epoch: 36.4 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06779609270021626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06779609270021626 | validation: 0.05648797021944085]
	TIME [epoch: 36.3 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0690363745118062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0690363745118062 | validation: 0.13101338226407852]
	TIME [epoch: 36.3 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10444307128010198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10444307128010198 | validation: 0.10926754669506199]
	TIME [epoch: 36.3 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11395487067145595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11395487067145595 | validation: 0.06086142416795442]
	TIME [epoch: 36.3 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09606812087493179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09606812087493179 | validation: 0.0823853249952255]
	TIME [epoch: 36.3 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10627274747424403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10627274747424403 | validation: 0.18675178654998947]
	TIME [epoch: 36.3 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1575298827572864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1575298827572864 | validation: 0.1363601245342692]
	TIME [epoch: 36.2 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15309520698557735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15309520698557735 | validation: 0.06264042671431562]
	TIME [epoch: 36.3 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10399548248336835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10399548248336835 | validation: 0.05410974536919079]
	TIME [epoch: 36.1 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07537533997417149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07537533997417149 | validation: 0.03854159607003963]
	TIME [epoch: 36.3 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07368663919923321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07368663919923321 | validation: 0.03943954648840186]
	TIME [epoch: 36.3 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06040836656462483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06040836656462483 | validation: 0.03481580175931344]
	TIME [epoch: 36.3 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_1131.pth
	Model improved!!!
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05695240786261645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05695240786261645 | validation: 0.05657515579216213]
	TIME [epoch: 36.3 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07467118598042273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07467118598042273 | validation: 0.06683246393687133]
	TIME [epoch: 36.3 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07210369967392079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07210369967392079 | validation: 0.048396307262225206]
	TIME [epoch: 36.3 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06748234719161637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06748234719161637 | validation: 0.09647653017840044]
	TIME [epoch: 36.3 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0786073240958003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0786073240958003 | validation: 0.1650923982515233]
	TIME [epoch: 36.2 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16682987154820297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16682987154820297 | validation: 0.14308360435781114]
	TIME [epoch: 36.3 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1573131052125937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1573131052125937 | validation: 0.08532907509885118]
	TIME [epoch: 36.2 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06495413013065647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06495413013065647 | validation: 0.0664200295215223]
	TIME [epoch: 36.3 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07109961080999448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07109961080999448 | validation: 0.0664197820639261]
	TIME [epoch: 36.3 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055250131911532094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055250131911532094 | validation: 0.043428307948832894]
	TIME [epoch: 36.3 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033243683376807914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033243683376807914 | validation: 0.043266883000271976]
	TIME [epoch: 36.3 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041125119913895815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041125119913895815 | validation: 0.10365231916548626]
	TIME [epoch: 36.3 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10856049636734161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10856049636734161 | validation: 0.20380364768964623]
	TIME [epoch: 36.2 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13371467781702603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13371467781702603 | validation: 0.12420308247972595]
	TIME [epoch: 36.3 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07762144969615914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07762144969615914 | validation: 0.07597292147277901]
	TIME [epoch: 36.2 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06363661391667544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06363661391667544 | validation: 0.08270285857057402]
	TIME [epoch: 36.3 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07154301002927964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07154301002927964 | validation: 0.05804050517410461]
	TIME [epoch: 36.3 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06982011166624434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06982011166624434 | validation: 0.07096974119255722]
	TIME [epoch: 36.3 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05146248876708496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05146248876708496 | validation: 0.04313966154090879]
	TIME [epoch: 36.4 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047901330756589644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047901330756589644 | validation: 0.07389337353139115]
	TIME [epoch: 36.4 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062301589949401256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062301589949401256 | validation: 0.07235589687872393]
	TIME [epoch: 36.3 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07420794009255578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07420794009255578 | validation: 0.09549972251971744]
	TIME [epoch: 36.3 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08116482481075454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08116482481075454 | validation: 0.042184620498183015]
	TIME [epoch: 36.3 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053781949623206864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053781949623206864 | validation: 0.10482390793920318]
	TIME [epoch: 36.3 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06871498593253382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06871498593253382 | validation: 0.07093472549621434]
	TIME [epoch: 36.3 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06923735160275932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06923735160275932 | validation: 0.0442432487406014]
	TIME [epoch: 36.3 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042257464880495164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042257464880495164 | validation: 0.06281704863923036]
	TIME [epoch: 36.5 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06898159915250945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06898159915250945 | validation: 0.12806526998949805]
	TIME [epoch: 36.2 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08985280341861397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08985280341861397 | validation: 0.06779966203108771]
	TIME [epoch: 36.3 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06695767408944328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06695767408944328 | validation: 0.048975846948452265]
	TIME [epoch: 36.2 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05117642222481848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05117642222481848 | validation: 0.06281740965357557]
	TIME [epoch: 36.3 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05185641404336364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05185641404336364 | validation: 0.03899090143372587]
	TIME [epoch: 36.3 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04813959037354778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04813959037354778 | validation: 0.11289960780364545]
	TIME [epoch: 36.3 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06951573123769465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06951573123769465 | validation: 0.06614964264381344]
	TIME [epoch: 36.3 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05442275632835406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05442275632835406 | validation: 0.03298596187188503]
	TIME [epoch: 36.3 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_1166.pth
	Model improved!!!
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047853279164353585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047853279164353585 | validation: 0.06181288729778303]
	TIME [epoch: 36.1 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05736151674626875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05736151674626875 | validation: 0.06862310152956597]
	TIME [epoch: 36.1 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06220776379023855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06220776379023855 | validation: 0.06712174142953979]
	TIME [epoch: 36 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07147593760540467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07147593760540467 | validation: 0.14957640364574712]
	TIME [epoch: 36.2 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10861252747405903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10861252747405903 | validation: 0.10833288818007701]
	TIME [epoch: 36.2 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09256701238832708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09256701238832708 | validation: 0.06548344507701699]
	TIME [epoch: 36.2 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09077546569879016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09077546569879016 | validation: 0.15346329403918313]
	TIME [epoch: 36.1 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08520713719301569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08520713719301569 | validation: 0.11799137818188861]
	TIME [epoch: 36.2 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06362711604646668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06362711604646668 | validation: 0.05804849677676706]
	TIME [epoch: 36.2 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03791703315637954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03791703315637954 | validation: 0.0422825040489072]
	TIME [epoch: 36.2 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06701208252117893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06701208252117893 | validation: 0.1339104237096767]
	TIME [epoch: 36.1 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06371716164787912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06371716164787912 | validation: 0.0769921385338187]
	TIME [epoch: 36.3 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05333499428703794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05333499428703794 | validation: 0.044116946667637574]
	TIME [epoch: 36.1 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03865271766351963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03865271766351963 | validation: 0.03497396359325947]
	TIME [epoch: 36.2 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043316878110895116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043316878110895116 | validation: 0.069801370963034]
	TIME [epoch: 36.1 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06157074312342434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06157074312342434 | validation: 0.05839692526225923]
	TIME [epoch: 36.2 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06354771963495033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06354771963495033 | validation: 0.10669946837961009]
	TIME [epoch: 36.1 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0906567851364714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0906567851364714 | validation: 0.08128472422162693]
	TIME [epoch: 36.1 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0856960394934427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0856960394934427 | validation: 0.061401229001371176]
	TIME [epoch: 36.2 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06147058884302462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06147058884302462 | validation: 0.028683156665849932]
	TIME [epoch: 36.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_1186.pth
	Model improved!!!
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04147399206085039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04147399206085039 | validation: 0.04506069603721179]
	TIME [epoch: 36.1 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045347848439531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045347848439531 | validation: 0.050625562242727996]
	TIME [epoch: 36.2 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05843162700719221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05843162700719221 | validation: 0.07219151587659303]
	TIME [epoch: 36.2 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0638204307901073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0638204307901073 | validation: 0.05753389701282594]
	TIME [epoch: 36.2 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06224551286273315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06224551286273315 | validation: 0.04734763489588956]
	TIME [epoch: 36.2 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05161260469197756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05161260469197756 | validation: 0.07654941158528461]
	TIME [epoch: 36.1 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07775718593970972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07775718593970972 | validation: 0.05121671707081064]
	TIME [epoch: 36 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06741108531164665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06741108531164665 | validation: 0.03253368286191791]
	TIME [epoch: 36.2 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044343289678889786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044343289678889786 | validation: 0.03127887211779734]
	TIME [epoch: 36.1 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05658828575805087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05658828575805087 | validation: 0.06681245954587596]
	TIME [epoch: 36.2 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10406765695903769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10406765695903769 | validation: 0.1605131032613775]
	TIME [epoch: 36 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11406705756026213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11406705756026213 | validation: 0.09899334155272695]
	TIME [epoch: 36.2 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07182660163183449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07182660163183449 | validation: 0.021227362904831838]
	TIME [epoch: 36.1 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_1199.pth
	Model improved!!!
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032585876366987836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032585876366987836 | validation: 0.03398383896205927]
	TIME [epoch: 36.2 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055720843858792446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055720843858792446 | validation: 0.07093781620844263]
	TIME [epoch: 36.1 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07172603390571265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07172603390571265 | validation: 0.10802189540196977]
	TIME [epoch: 36.2 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10982852578944291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10982852578944291 | validation: 0.062144640919798]
	TIME [epoch: 36.2 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06641776348303703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06641776348303703 | validation: 0.029312335958614533]
	TIME [epoch: 36.2 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036104227440897044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036104227440897044 | validation: 0.020478939472232685]
	TIME [epoch: 36.4 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_1205.pth
	Model improved!!!
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02284940427110907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02284940427110907 | validation: 0.3241270867314106]
	TIME [epoch: 36.1 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2727101795035198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2727101795035198 | validation: 0.1339224760196722]
	TIME [epoch: 36 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11286594463210495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11286594463210495 | validation: 0.09169983597252307]
	TIME [epoch: 36.1 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09393098275750406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09393098275750406 | validation: 0.13563276634895577]
	TIME [epoch: 36.1 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07844305509138325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07844305509138325 | validation: 0.08914147071206183]
	TIME [epoch: 36.1 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04358676658461438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04358676658461438 | validation: 0.041921914569470024]
	TIME [epoch: 36.1 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030366135535446714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030366135535446714 | validation: 0.027833681629841345]
	TIME [epoch: 36.1 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030708820637598333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030708820637598333 | validation: 0.04194816340868324]
	TIME [epoch: 36.2 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03164173936652082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03164173936652082 | validation: 0.03083947783024058]
	TIME [epoch: 36.1 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031409333295606835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031409333295606835 | validation: 0.05228865177921959]
	TIME [epoch: 36.2 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04342077276932264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04342077276932264 | validation: 0.09755381099337046]
	TIME [epoch: 36.2 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10803818323863301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10803818323863301 | validation: 0.13251554198245574]
	TIME [epoch: 36.1 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09753180868060123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09753180868060123 | validation: 0.09145381601581858]
	TIME [epoch: 36.1 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05765780673905984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05765780673905984 | validation: 0.027869435010515243]
	TIME [epoch: 36.1 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04590946427885502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04590946427885502 | validation: 0.054067955697252194]
	TIME [epoch: 36.3 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04439571510579512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04439571510579512 | validation: 0.025634207026126322]
	TIME [epoch: 36.3 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028239401829763616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028239401829763616 | validation: 0.028351917748743162]
	TIME [epoch: 36.3 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026653511692220508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026653511692220508 | validation: 0.03808235611517814]
	TIME [epoch: 36.3 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03850611964842397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03850611964842397 | validation: 0.06299375968707716]
	TIME [epoch: 36.3 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0668716330272806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0668716330272806 | validation: 0.06233494080049932]
	TIME [epoch: 36.3 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05961278065489962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05961278065489962 | validation: 0.06397787689908956]
	TIME [epoch: 36.3 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056999433118185845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056999433118185845 | validation: 0.06776138286568084]
	TIME [epoch: 36.3 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0650010011463199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0650010011463199 | validation: 0.0848530660496316]
	TIME [epoch: 36.2 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09521845583511002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09521845583511002 | validation: 0.11202363755763677]
	TIME [epoch: 36.3 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11797037115549149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11797037115549149 | validation: 0.11956511719162313]
	TIME [epoch: 36.3 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06616140790018822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06616140790018822 | validation: 0.06332545870112684]
	TIME [epoch: 36.3 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04933687634495954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04933687634495954 | validation: 0.025434465950972242]
	TIME [epoch: 36.3 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027554638510531065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027554638510531065 | validation: 0.02517963807362469]
	TIME [epoch: 36.3 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02484506972876175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02484506972876175 | validation: 0.029503515585018836]
	TIME [epoch: 36.2 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02178812762801671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02178812762801671 | validation: 0.837675046789126]
	TIME [epoch: 36.3 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7016107827929046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7016107827929046 | validation: 0.5441162237296595]
	TIME [epoch: 36.2 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4461025741799833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4461025741799833 | validation: 0.46989093969888834]
	TIME [epoch: 36.3 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2966821267068488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2966821267068488 | validation: 0.3650355535128422]
	TIME [epoch: 36.1 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23776768173113394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23776768173113394 | validation: 0.29990740178563574]
	TIME [epoch: 36.2 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16616144506676586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16616144506676586 | validation: 0.25586408204322925]
	TIME [epoch: 36.2 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13198408743406637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13198408743406637 | validation: 0.2594192324154683]
	TIME [epoch: 36.3 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1197076080389563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1197076080389563 | validation: 0.2229400878700913]
	TIME [epoch: 36.3 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10385675167064864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10385675167064864 | validation: 0.205295250696781]
	TIME [epoch: 36 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09226718570486718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09226718570486718 | validation: 0.1977257325664562]
	TIME [epoch: 36.3 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08550086747773221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08550086747773221 | validation: 0.17301468609493426]
	TIME [epoch: 36.2 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07425461627891804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07425461627891804 | validation: 0.1539986530856633]
	TIME [epoch: 36.2 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07171964507101272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07171964507101272 | validation: 0.1505225618931666]
	TIME [epoch: 36.2 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06624019304402597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06624019304402597 | validation: 0.1351571772020784]
	TIME [epoch: 36.3 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058761292825703415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058761292825703415 | validation: 0.10913175673914083]
	TIME [epoch: 36.2 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05684214360861813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05684214360861813 | validation: 0.07984926133041019]
	TIME [epoch: 36.3 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05355872376751254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05355872376751254 | validation: 0.0734858777508942]
	TIME [epoch: 36.3 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06659543023088536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06659543023088536 | validation: 0.06623311334221253]
	TIME [epoch: 36.3 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053016332932620695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053016332932620695 | validation: 0.07481359265440028]
	TIME [epoch: 36.2 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0578976169821402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0578976169821402 | validation: 0.06124936603600736]
	TIME [epoch: 36.2 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05697696729698033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05697696729698033 | validation: 0.05875718748782029]
	TIME [epoch: 36.2 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05429037389373211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05429037389373211 | validation: 0.07358217608860725]
	TIME [epoch: 36.2 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06851439258331077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06851439258331077 | validation: 0.11970125314686886]
	TIME [epoch: 36.2 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09329238277255157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09329238277255157 | validation: 0.05669931530656293]
	TIME [epoch: 36.2 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04532927639534516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04532927639534516 | validation: 0.029259482456161945]
	TIME [epoch: 36.2 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030858266444896223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030858266444896223 | validation: 0.025605213185507594]
	TIME [epoch: 36.2 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031039267757507392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031039267757507392 | validation: 0.03440755904983674]
	TIME [epoch: 36.2 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031004557947063607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031004557947063607 | validation: 0.038571148515780224]
	TIME [epoch: 36.2 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039161806682028226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039161806682028226 | validation: 0.08165783983107225]
	TIME [epoch: 36.2 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06328108931183307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06328108931183307 | validation: 0.06787099815724089]
	TIME [epoch: 36.2 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07034682295546782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07034682295546782 | validation: 0.10233308671985292]
	TIME [epoch: 36.2 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07242105536015185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07242105536015185 | validation: 0.05574098785762119]
	TIME [epoch: 36.2 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04800759297645529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04800759297645529 | validation: 0.0345894635014526]
	TIME [epoch: 36.1 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038308019272576564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038308019272576564 | validation: 0.025523133000221344]
	TIME [epoch: 36.2 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03237392675930354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03237392675930354 | validation: 0.07363914493857106]
	TIME [epoch: 36.1 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06139368392526663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06139368392526663 | validation: 0.06354844023674189]
	TIME [epoch: 36.2 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07028321629770519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07028321629770519 | validation: 0.05970304642105249]
	TIME [epoch: 36.2 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06215530361312891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06215530361312891 | validation: 0.20903374141501407]
	TIME [epoch: 36.1 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41942624496789366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41942624496789366 | validation: 0.16994028787162319]
	TIME [epoch: 36.2 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25450751628229856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25450751628229856 | validation: 0.08210940779184706]
	TIME [epoch: 36.1 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07050087730281984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07050087730281984 | validation: 0.059399637553036545]
	TIME [epoch: 36.1 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038584168349073134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038584168349073134 | validation: 0.04095015245554232]
	TIME [epoch: 36.2 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043840835456447874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043840835456447874 | validation: 0.051037036203692016]
	TIME [epoch: 36.2 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03780269943353952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03780269943353952 | validation: 0.051682321487585864]
	TIME [epoch: 36.2 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0426493053269229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0426493053269229 | validation: 0.04976341292353251]
	TIME [epoch: 36.1 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05222334329770275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05222334329770275 | validation: 0.05800914689629513]
	TIME [epoch: 36.2 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04991595844981531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04991595844981531 | validation: 0.05173781755881829]
	TIME [epoch: 36.2 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04402030409911738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04402030409911738 | validation: 0.04261979675205993]
	TIME [epoch: 36.1 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04053662089818964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04053662089818964 | validation: 0.04534961410884461]
	TIME [epoch: 36 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04224886949985513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04224886949985513 | validation: 0.053710537900007106]
	TIME [epoch: 36.2 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04450948875011823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04450948875011823 | validation: 0.051269381696602556]
	TIME [epoch: 36.1 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047265131702904827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047265131702904827 | validation: 0.05352815226684857]
	TIME [epoch: 36.2 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048423550241536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048423550241536 | validation: 0.04587541111992593]
	TIME [epoch: 36.1 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048118950536475885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048118950536475885 | validation: 0.04470533508661889]
	TIME [epoch: 36.2 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038833054804396756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038833054804396756 | validation: 0.03099165148198877]
	TIME [epoch: 36.1 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03915374464818162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03915374464818162 | validation: 0.04226116314774767]
	TIME [epoch: 36.2 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03592475558409402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03592475558409402 | validation: 0.03618414223678053]
	TIME [epoch: 36.2 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033784694983584364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033784694983584364 | validation: 0.04492092264977109]
	TIME [epoch: 36.3 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04916404791653219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04916404791653219 | validation: 0.07838830423984776]
	TIME [epoch: 36.2 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08067293606261368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08067293606261368 | validation: 0.07937396582816698]
	TIME [epoch: 36.1 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07843073688023633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07843073688023633 | validation: 0.07845795807082183]
	TIME [epoch: 36.1 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06678491844655314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06678491844655314 | validation: 0.040960252375856515]
	TIME [epoch: 36.2 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051126863885961694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051126863885961694 | validation: 0.07656702140887736]
	TIME [epoch: 36.2 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05294332811830097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05294332811830097 | validation: 0.03953721334331774]
	TIME [epoch: 36.2 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034757170199332874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034757170199332874 | validation: 0.028210130938118375]
	TIME [epoch: 36.3 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03228072581426023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03228072581426023 | validation: 0.03597947072677946]
	TIME [epoch: 36.2 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03405476748412048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03405476748412048 | validation: 0.04407458047262158]
	TIME [epoch: 36.2 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04773180439976011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04773180439976011 | validation: 0.08968196684585705]
	TIME [epoch: 36.2 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08402517323141748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08402517323141748 | validation: 0.07240175607142842]
	TIME [epoch: 36.3 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07136023899484911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07136023899484911 | validation: 0.05791093329636981]
	TIME [epoch: 36.2 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06091487753533224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06091487753533224 | validation: 0.11698033721374541]
	TIME [epoch: 36.2 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07588660930235262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07588660930235262 | validation: 0.08802110503664977]
	TIME [epoch: 36.2 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142221/states/model_phi1_3c_v_mmd1_1306.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 23284.266 seconds.
