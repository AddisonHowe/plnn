Args:
Namespace(name='model_phi1_4a_v_mmd1', outdir='out/model_training/model_phi1_4a_v_mmd1', training_data='data/training_data/data_phi1_4a/training', validation_data='data/training_data/data_phi1_4a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1421674598

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.400611896414415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.400611896414415 | validation: 6.852772822545802]
	TIME [epoch: 43.6 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.302389431868171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.302389431868171 | validation: 6.512286377725255]
	TIME [epoch: 0.954 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.005218009143427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.005218009143427 | validation: 6.574179745942039]
	TIME [epoch: 0.94 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.503250089760017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.503250089760017 | validation: 6.766660072892917]
	TIME [epoch: 0.941 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.2848248855564135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.2848248855564135 | validation: 6.199326173959307]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.784218102701125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.784218102701125 | validation: 6.3209799978737715]
	TIME [epoch: 0.939 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.804986158295981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.804986158295981 | validation: 6.252026797320359]
	TIME [epoch: 0.94 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.497152700337647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.497152700337647 | validation: 6.207394373628846]
	TIME [epoch: 0.938 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.42296438187159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.42296438187159 | validation: 6.037156428935049]
	TIME [epoch: 0.94 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.308642388834355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.308642388834355 | validation: 6.004990887170273]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.243279519334294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.243279519334294 | validation: 5.997719989569158]
	TIME [epoch: 0.94 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.1962319513299375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1962319513299375 | validation: 5.968815462769337]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.16280192984187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.16280192984187 | validation: 6.044599087400357]
	TIME [epoch: 0.936 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.177259265510848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.177259265510848 | validation: 6.025582676246599]
	TIME [epoch: 0.934 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.279507533016576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.279507533016576 | validation: 6.1344342107344545]
	TIME [epoch: 0.937 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.413708868535219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.413708868535219 | validation: 5.933993132956306]
	TIME [epoch: 0.94 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.040650275405901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.040650275405901 | validation: 5.950839623989914]
	TIME [epoch: 0.938 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.163845650066885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.163845650066885 | validation: 5.993824887786317]
	TIME [epoch: 0.935 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.133960233713351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.133960233713351 | validation: 5.8896002208140965]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9781287349401904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9781287349401904 | validation: 5.864082047368374]
	TIME [epoch: 0.94 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.00778930027075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.00778930027075 | validation: 5.903185751214576]
	TIME [epoch: 0.947 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9935216090010477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9935216090010477 | validation: 5.825843705579921]
	TIME [epoch: 0.955 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9182244243616275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9182244243616275 | validation: 5.817286545934355]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8845680324201375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8845680324201375 | validation: 5.800366650239502]
	TIME [epoch: 0.94 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.860406776591241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.860406776591241 | validation: 5.780261022218843]
	TIME [epoch: 0.941 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.836635823354849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.836635823354849 | validation: 5.779765006515084]
	TIME [epoch: 0.943 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.821390874743537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.821390874743537 | validation: 5.769389574678548]
	TIME [epoch: 0.941 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.859460922787817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.859460922787817 | validation: 5.941680740644763]
	TIME [epoch: 0.939 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.212951405812964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.212951405812964 | validation: 5.743369723890249]
	TIME [epoch: 0.939 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8003163685791606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8003163685791606 | validation: 5.7157790957960515]
	TIME [epoch: 0.945 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.748165638272884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.748165638272884 | validation: 5.70830400272742]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.732479783085966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.732479783085966 | validation: 5.690674345197531]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.718850445561608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.718850445561608 | validation: 5.706007324891019]
	TIME [epoch: 0.938 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7369919960162985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7369919960162985 | validation: 5.756705558605185]
	TIME [epoch: 0.935 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8958854039516724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8958854039516724 | validation: 5.827066037075939]
	TIME [epoch: 0.938 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.080361141943073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.080361141943073 | validation: 5.681185539787375]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.704719140008513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.704719140008513 | validation: 5.768961212813311]
	TIME [epoch: 0.938 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.952007522616284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.952007522616284 | validation: 5.648548982906188]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.69828411181736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.69828411181736 | validation: 5.626563770614885]
	TIME [epoch: 0.942 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6468095697993768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6468095697993768 | validation: 5.610084349259253]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6576435789075137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6576435789075137 | validation: 5.589677263945572]
	TIME [epoch: 0.939 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.607025260501315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.607025260501315 | validation: 5.566744914945726]
	TIME [epoch: 0.939 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.582021524682418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.582021524682418 | validation: 5.554852733899013]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5708213632036463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5708213632036463 | validation: 5.542158638482748]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.554992586607878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.554992586607878 | validation: 5.522823278620881]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.537016443231785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.537016443231785 | validation: 5.51280814668545]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5207714875525085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5207714875525085 | validation: 5.48458061722434]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.504583609179485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.504583609179485 | validation: 5.474673146787666]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.489175909415224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.489175909415224 | validation: 5.463167365446319]
	TIME [epoch: 0.941 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4732384433615993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4732384433615993 | validation: 5.444356422322783]
	TIME [epoch: 0.939 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4602112751565133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4602112751565133 | validation: 5.4172166917361055]
	TIME [epoch: 0.941 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4499669565320574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4499669565320574 | validation: 5.398906443703139]
	TIME [epoch: 0.943 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.459720229580498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.459720229580498 | validation: 5.4079324685222225]
	TIME [epoch: 0.94 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.478738728534309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.478738728534309 | validation: 5.347539383293813]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5172989979497946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5172989979497946 | validation: 5.3149193957761]
	TIME [epoch: 0.941 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3550273896391376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3550273896391376 | validation: 5.280547131938265]
	TIME [epoch: 0.942 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.348154938518127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.348154938518127 | validation: 5.2190371205290065]
	TIME [epoch: 0.952 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3602179593771377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3602179593771377 | validation: 5.216833549307972]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2999831963162225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2999831963162225 | validation: 5.14880531206639]
	TIME [epoch: 0.946 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2582182307048546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2582182307048546 | validation: 5.119348279175526]
	TIME [epoch: 0.944 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2350611074356834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2350611074356834 | validation: 5.095987988656098]
	TIME [epoch: 0.943 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.21875984299583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.21875984299583 | validation: 5.060379556858251]
	TIME [epoch: 0.943 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1998539165822018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1998539165822018 | validation: 5.028108542572326]
	TIME [epoch: 0.94 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1794934675889874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1794934675889874 | validation: 4.97947767521951]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.18018395361322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.18018395361322 | validation: 4.9840967253904385]
	TIME [epoch: 0.938 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1625411124820073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1625411124820073 | validation: 4.918041839236858]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1567978097084284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1567978097084284 | validation: 4.920571710522903]
	TIME [epoch: 0.939 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1086350510006926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1086350510006926 | validation: 4.8524626379415885]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0826023586423776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0826023586423776 | validation: 4.839714103785841]
	TIME [epoch: 0.942 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0655426338460665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0655426338460665 | validation: 4.811677596268431]
	TIME [epoch: 0.94 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0519358378654364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0519358378654364 | validation: 4.779426020257742]
	TIME [epoch: 0.939 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.03443515501704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.03443515501704 | validation: 4.745057628875924]
	TIME [epoch: 0.941 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.029998181948769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.029998181948769 | validation: 4.76153262695841]
	TIME [epoch: 0.938 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0341518805579795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0341518805579795 | validation: 4.699490894368739]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.112209630826443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.112209630826443 | validation: 4.689052360479306]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9771209469465476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9771209469465476 | validation: 4.682051468919204]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9855123646588875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9855123646588875 | validation: 4.609273483937037]
	TIME [epoch: 0.942 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.007895804717346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.007895804717346 | validation: 4.594336964186955]
	TIME [epoch: 0.939 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9380045241386212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9380045241386212 | validation: 4.593986627807405]
	TIME [epoch: 0.94 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9294639232044664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9294639232044664 | validation: 4.517122261504022]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9314480078837266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9314480078837266 | validation: 4.513668977916324]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.889945743554211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.889945743554211 | validation: 4.449861357089378]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8684707661555287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8684707661555287 | validation: 4.345034110699614]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.79026584010869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.79026584010869 | validation: 3.815903402446088]
	TIME [epoch: 0.939 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4439933788320105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4439933788320105 | validation: 3.2739329919655944]
	TIME [epoch: 0.94 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.155862460248087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.155862460248087 | validation: 2.7794351460374522]
	TIME [epoch: 0.94 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8142320815699726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8142320815699726 | validation: 1.5729134292990492]
	TIME [epoch: 0.945 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5149264333382977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5149264333382977 | validation: 1.3004643632773822]
	TIME [epoch: 0.951 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6736766236410154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6736766236410154 | validation: 2.1126056080662567]
	TIME [epoch: 0.937 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.529888180633759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.529888180633759 | validation: 1.529692562901146]
	TIME [epoch: 0.94 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4209987101000865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4209987101000865 | validation: 0.9282884890063253]
	TIME [epoch: 0.939 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2055592850241323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2055592850241323 | validation: 1.3860789788804253]
	TIME [epoch: 0.939 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2601089871550017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2601089871550017 | validation: 0.8692288264280702]
	TIME [epoch: 0.94 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.17083767002386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.17083767002386 | validation: 1.125630470449132]
	TIME [epoch: 0.941 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.093494932425641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.093494932425641 | validation: 0.9655630618426907]
	TIME [epoch: 0.94 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0350754674010691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0350754674010691 | validation: 0.9059998658571082]
	TIME [epoch: 0.94 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0071956660451515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0071956660451515 | validation: 1.0183141690260091]
	TIME [epoch: 0.94 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0079395517044092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0079395517044092 | validation: 0.8530267399985858]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0019415479652904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0019415479652904 | validation: 1.2523872275933128]
	TIME [epoch: 0.941 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.066601516642016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.066601516642016 | validation: 0.8007715469246096]
	TIME [epoch: 0.939 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0508783833624462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0508783833624462 | validation: 1.2216369576780073]
	TIME [epoch: 0.94 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0300927395508044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0300927395508044 | validation: 0.8179913439565081]
	TIME [epoch: 0.939 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9558569751121863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9558569751121863 | validation: 0.8683738115233969]
	TIME [epoch: 0.939 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9332490307997705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9332490307997705 | validation: 0.885161317542507]
	TIME [epoch: 0.939 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9278495862724963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9278495862724963 | validation: 0.8267430624271404]
	TIME [epoch: 0.938 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9220069967871439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9220069967871439 | validation: 0.8887754411748503]
	TIME [epoch: 0.939 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.911925957531427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.911925957531427 | validation: 0.7681982097995776]
	TIME [epoch: 0.94 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9140669860144689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9140669860144689 | validation: 0.9549993138872441]
	TIME [epoch: 0.94 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9327094389304783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9327094389304783 | validation: 0.7130211752074658]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0109694039606516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0109694039606516 | validation: 1.4270439415380658]
	TIME [epoch: 0.939 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1455971356680033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1455971356680033 | validation: 0.7404229168730018]
	TIME [epoch: 0.935 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0031446927610577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0031446927610577 | validation: 1.0342678841017308]
	TIME [epoch: 0.937 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9569594317903143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9569594317903143 | validation: 0.8291848468987649]
	TIME [epoch: 0.936 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9273165011626501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9273165011626501 | validation: 0.8273431392678653]
	TIME [epoch: 0.937 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9538033990697639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9538033990697639 | validation: 0.8420390489622793]
	TIME [epoch: 0.938 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9010362282807276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9010362282807276 | validation: 0.8288585467412753]
	TIME [epoch: 0.935 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8904521569070167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8904521569070167 | validation: 0.7519652343521828]
	TIME [epoch: 0.937 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8789926091989058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8789926091989058 | validation: 0.9144155263410472]
	TIME [epoch: 0.937 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8990513310111794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8990513310111794 | validation: 0.697300154733621]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9239791851815321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9239791851815321 | validation: 1.130169144603951]
	TIME [epoch: 0.936 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9789702668337114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9789702668337114 | validation: 0.637849762163722]
	TIME [epoch: 0.933 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9584561584439001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9584561584439001 | validation: 0.9596709320407513]
	TIME [epoch: 0.942 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9235622575948859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9235622575948859 | validation: 0.6286288061685698]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9639600618499966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9639600618499966 | validation: 0.9157990135305623]
	TIME [epoch: 0.941 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9179808188620632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9179808188620632 | validation: 0.7079915524368856]
	TIME [epoch: 0.939 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9153959800181519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9153959800181519 | validation: 0.7539146817063597]
	TIME [epoch: 0.939 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8684410701587748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8684410701587748 | validation: 0.9241652742901562]
	TIME [epoch: 0.939 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8991192766005512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8991192766005512 | validation: 0.7290255951030439]
	TIME [epoch: 0.941 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9427189752473021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9427189752473021 | validation: 1.1578943337600964]
	TIME [epoch: 0.95 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0030836096592273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0030836096592273 | validation: 0.6914458259885221]
	TIME [epoch: 0.937 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.856072500859695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.856072500859695 | validation: 0.6885623002867189]
	TIME [epoch: 0.937 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8425877453963364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8425877453963364 | validation: 0.847539510948105]
	TIME [epoch: 0.941 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8610929809921817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8610929809921817 | validation: 0.6805641956658016]
	TIME [epoch: 0.942 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8774208982420282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8774208982420282 | validation: 0.9740730959946369]
	TIME [epoch: 0.939 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9227876519007603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9227876519007603 | validation: 0.6568227647660609]
	TIME [epoch: 0.94 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9078837647289467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9078837647289467 | validation: 0.9589428745751771]
	TIME [epoch: 0.941 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9064697481570656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9064697481570656 | validation: 0.6070447120153312]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_137.pth
	Model improved!!!
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8453497910809961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8453497910809961 | validation: 0.7781161837078069]
	TIME [epoch: 0.94 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8686377309956899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8686377309956899 | validation: 0.6985723798770249]
	TIME [epoch: 0.938 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.053906198408453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.053906198408453 | validation: 0.9500313851031363]
	TIME [epoch: 0.939 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9138620258358806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9138620258358806 | validation: 0.634629843133224]
	TIME [epoch: 0.941 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8906616930183242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8906616930183242 | validation: 0.90973108878615]
	TIME [epoch: 0.938 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.87645921135676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.87645921135676 | validation: 0.7245405178996895]
	TIME [epoch: 0.938 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9121368074685825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9121368074685825 | validation: 1.0754184849986306]
	TIME [epoch: 0.938 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9896441517980332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9896441517980332 | validation: 0.6770794909984246]
	TIME [epoch: 0.939 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.832223395084949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.832223395084949 | validation: 0.6502317429765254]
	TIME [epoch: 0.939 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.830070380416376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.830070380416376 | validation: 0.8080189998248536]
	TIME [epoch: 0.941 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8328944350893527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8328944350893527 | validation: 0.5935138447997732]
	TIME [epoch: 0.939 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_148.pth
	Model improved!!!
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8746582800614522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8746582800614522 | validation: 1.0032120237515338]
	TIME [epoch: 0.936 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9318684399715298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9318684399715298 | validation: 0.5758841302743449]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_150.pth
	Model improved!!!
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9794633739605331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9794633739605331 | validation: 0.9246954493783002]
	TIME [epoch: 0.936 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8826653083014346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8826653083014346 | validation: 0.6206412250807267]
	TIME [epoch: 0.934 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8408804015992459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8408804015992459 | validation: 0.7338970854344418]
	TIME [epoch: 0.941 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8204569486218195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8204569486218195 | validation: 0.7157259301308233]
	TIME [epoch: 0.938 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8164463705051378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8164463705051378 | validation: 0.7342742234694091]
	TIME [epoch: 0.94 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8172911756137476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8172911756137476 | validation: 0.7783321679434205]
	TIME [epoch: 0.94 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9133837388641942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9133837388641942 | validation: 1.298610902517147]
	TIME [epoch: 0.936 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1474664949015745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1474664949015745 | validation: 0.6886137852680142]
	TIME [epoch: 0.936 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8144715205824693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8144715205824693 | validation: 0.6477215696372727]
	TIME [epoch: 0.936 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8721330414991263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8721330414991263 | validation: 1.0501770359549845]
	TIME [epoch: 0.94 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9577653309468732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9577653309468732 | validation: 0.6921150233458067]
	TIME [epoch: 0.938 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8281437992499244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8281437992499244 | validation: 0.5563359110822121]
	TIME [epoch: 0.941 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8790466565154915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8790466565154915 | validation: 0.9709763742982163]
	TIME [epoch: 0.935 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9257043201610862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9257043201610862 | validation: 0.5633178727352695]
	TIME [epoch: 0.933 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9201897693889011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9201897693889011 | validation: 0.7620486808900593]
	TIME [epoch: 0.934 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8122251508729132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8122251508729132 | validation: 0.7130062996834788]
	TIME [epoch: 0.933 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8010908637392186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8010908637392186 | validation: 0.6299435628642529]
	TIME [epoch: 0.932 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8008331828276348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8008331828276348 | validation: 0.6987928811988282]
	TIME [epoch: 0.931 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7977149724523563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7977149724523563 | validation: 0.6646127368352565]
	TIME [epoch: 0.935 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7965253145485619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7965253145485619 | validation: 0.7210789330005671]
	TIME [epoch: 0.934 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8251350429300149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8251350429300149 | validation: 0.9299291565051855]
	TIME [epoch: 0.932 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9841855536849516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9841855536849516 | validation: 0.7858513549223884]
	TIME [epoch: 0.934 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.960211629388314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.960211629388314 | validation: 1.1110902798967808]
	TIME [epoch: 0.936 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.958602680665733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.958602680665733 | validation: 0.5826477238783148]
	TIME [epoch: 0.95 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7918584619936607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7918584619936607 | validation: 0.6476682374062653]
	TIME [epoch: 0.938 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.826814772332827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.826814772332827 | validation: 0.81974911811879]
	TIME [epoch: 0.937 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8916438294826458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8916438294826458 | validation: 0.6930792073582438]
	TIME [epoch: 0.937 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8231491255397717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8231491255397717 | validation: 0.6619987781714753]
	TIME [epoch: 0.949 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8048790726088134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8048790726088134 | validation: 0.687750103748977]
	TIME [epoch: 0.94 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7882149193915317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7882149193915317 | validation: 0.6346382985884877]
	TIME [epoch: 0.94 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7966615065670933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7966615065670933 | validation: 0.7882745321075347]
	TIME [epoch: 0.941 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.823460546214581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.823460546214581 | validation: 0.5684261397842508]
	TIME [epoch: 0.943 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8895030236023468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8895030236023468 | validation: 0.8939199901183461]
	TIME [epoch: 0.939 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8420831441057477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8420831441057477 | validation: 0.7078983310611087]
	TIME [epoch: 0.942 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9321485906649674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9321485906649674 | validation: 1.1903947620639241]
	TIME [epoch: 0.937 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9792653557611672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9792653557611672 | validation: 0.660650928700476]
	TIME [epoch: 0.936 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7835439996655328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7835439996655328 | validation: 0.5404766698307674]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_187.pth
	Model improved!!!
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8211572538421777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8211572538421777 | validation: 0.7978035562562914]
	TIME [epoch: 0.94 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8078279502642505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8078279502642505 | validation: 0.585155860160091]
	TIME [epoch: 0.938 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7721727579604272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7721727579604272 | validation: 0.6520905130341101]
	TIME [epoch: 0.936 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7521730198043062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7521730198043062 | validation: 0.6533669680233775]
	TIME [epoch: 0.936 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7546571135974834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7546571135974834 | validation: 0.6639756994735072]
	TIME [epoch: 0.935 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8039860717060949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8039860717060949 | validation: 0.8542484939043663]
	TIME [epoch: 0.937 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9669108164532149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9669108164532149 | validation: 0.9734152544306043]
	TIME [epoch: 0.935 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9568899685457024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9568899685457024 | validation: 0.5778035998604057]
	TIME [epoch: 0.942 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.77571732915757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.77571732915757 | validation: 0.7244482853499002]
	TIME [epoch: 0.936 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7822421792397779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7822421792397779 | validation: 0.569688643277453]
	TIME [epoch: 0.94 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7893052609371168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7893052609371168 | validation: 0.7474937950940107]
	TIME [epoch: 0.936 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.780316506043514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.780316506043514 | validation: 0.5725169868559836]
	TIME [epoch: 0.935 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.777124638803775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.777124638803775 | validation: 0.701446680386402]
	TIME [epoch: 0.937 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7617088315071981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7617088315071981 | validation: 0.5798481276058147]
	TIME [epoch: 41.5 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7444083498279472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7444083498279472 | validation: 0.690122841434997]
	TIME [epoch: 1.85 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7371454457663416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7371454457663416 | validation: 0.5464685025929187]
	TIME [epoch: 1.84 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7634396658170373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7634396658170373 | validation: 0.951596778029146]
	TIME [epoch: 1.84 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8775868454446678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8775868454446678 | validation: 0.7608634978963034]
	TIME [epoch: 1.84 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0598210537643653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0598210537643653 | validation: 0.8828155611055669]
	TIME [epoch: 1.83 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8221636032274667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8221636032274667 | validation: 0.7161706165344386]
	TIME [epoch: 1.84 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7972901270873277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7972901270873277 | validation: 0.5708200034573426]
	TIME [epoch: 1.84 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7802259179575828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7802259179575828 | validation: 0.6391082522383793]
	TIME [epoch: 1.84 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.722750796561478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.722750796561478 | validation: 0.6458892587105818]
	TIME [epoch: 1.85 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7157972426314071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7157972426314071 | validation: 0.5539754308035104]
	TIME [epoch: 1.84 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7127072358885518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7127072358885518 | validation: 0.7529353692237116]
	TIME [epoch: 1.85 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7478530441232049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7478530441232049 | validation: 0.6889130045324668]
	TIME [epoch: 1.84 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8374470692312322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8374470692312322 | validation: 0.769783451611235]
	TIME [epoch: 1.84 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8377392775366851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8377392775366851 | validation: 0.6037149123977399]
	TIME [epoch: 1.85 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7013640718644666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7013640718644666 | validation: 0.5107250141600143]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_216.pth
	Model improved!!!
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6936829201355116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6936829201355116 | validation: 0.8007445452047701]
	TIME [epoch: 1.85 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.746290426438122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.746290426438122 | validation: 0.5146901238086597]
	TIME [epoch: 1.84 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7951130556758301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7951130556758301 | validation: 0.7091744155230408]
	TIME [epoch: 1.84 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7049444646021772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7049444646021772 | validation: 0.6546154778337169]
	TIME [epoch: 1.83 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.69592977366117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.69592977366117 | validation: 0.6444827538936745]
	TIME [epoch: 1.85 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7291046208676117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7291046208676117 | validation: 0.5975483810751337]
	TIME [epoch: 1.84 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7274729951091148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7274729951091148 | validation: 0.7356106066967953]
	TIME [epoch: 1.85 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7170077229651654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7170077229651654 | validation: 0.5765675124988283]
	TIME [epoch: 1.84 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6843454536138517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6843454536138517 | validation: 0.69441671669827]
	TIME [epoch: 1.84 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6631097861024827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6631097861024827 | validation: 0.4980002084445957]
	TIME [epoch: 1.85 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6665838724182425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6665838724182425 | validation: 0.7265248038943922]
	TIME [epoch: 1.85 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6679234080948853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6679234080948853 | validation: 0.4997461720838538]
	TIME [epoch: 1.84 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6363786414163697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6363786414163697 | validation: 0.6563923248682875]
	TIME [epoch: 1.84 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6204651377804712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6204651377804712 | validation: 0.5591563779211052]
	TIME [epoch: 1.84 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6078994552349041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6078994552349041 | validation: 0.6870017884001749]
	TIME [epoch: 1.85 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6968564857896766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6968564857896766 | validation: 0.8394826369301183]
	TIME [epoch: 1.85 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8174430724292577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8174430724292577 | validation: 0.5709480215683989]
	TIME [epoch: 1.84 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.803584601294287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.803584601294287 | validation: 0.6382695284037223]
	TIME [epoch: 1.83 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6814158068240049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6814158068240049 | validation: 0.48418477815967037]
	TIME [epoch: 1.85 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_235.pth
	Model improved!!!
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6014102130118636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6014102130118636 | validation: 0.6767471452503551]
	TIME [epoch: 1.84 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6232350152448872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6232350152448872 | validation: 0.5734403452840094]
	TIME [epoch: 1.84 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6301447547141791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6301447547141791 | validation: 0.6122835620011542]
	TIME [epoch: 1.84 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6490124473567807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6490124473567807 | validation: 0.6526337444539929]
	TIME [epoch: 1.84 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6071683765602833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6071683765602833 | validation: 0.539285530906356]
	TIME [epoch: 1.84 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6076668618806719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6076668618806719 | validation: 0.7071825741456563]
	TIME [epoch: 1.84 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6255433692427685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6255433692427685 | validation: 0.5215190107422359]
	TIME [epoch: 1.85 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.625352925748112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.625352925748112 | validation: 0.5924914849033804]
	TIME [epoch: 1.84 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5553135687964454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5553135687964454 | validation: 0.5609238227059453]
	TIME [epoch: 1.84 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5355821020966995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5355821020966995 | validation: 0.48678811738779254]
	TIME [epoch: 1.84 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5315041725340313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5315041725340313 | validation: 0.6906151046848547]
	TIME [epoch: 1.84 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5760862691533928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5760862691533928 | validation: 0.5514570985482734]
	TIME [epoch: 1.86 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6542025164810579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6542025164810579 | validation: 0.6096929080671186]
	TIME [epoch: 1.84 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5871974109125602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5871974109125602 | validation: 0.6189862933003235]
	TIME [epoch: 1.85 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5439230121930803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5439230121930803 | validation: 0.5515743362695694]
	TIME [epoch: 1.84 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5619538691721866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5619538691721866 | validation: 0.7580869185162453]
	TIME [epoch: 1.85 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5643576044770845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5643576044770845 | validation: 0.4511870666474517]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_252.pth
	Model improved!!!
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5123104721395495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5123104721395495 | validation: 0.5732807770262872]
	TIME [epoch: 1.84 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48578910940879705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48578910940879705 | validation: 0.5990748029720696]
	TIME [epoch: 1.84 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5226174865174602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5226174865174602 | validation: 0.636237432684521]
	TIME [epoch: 1.84 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7039443377754799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7039443377754799 | validation: 0.49645478686118116]
	TIME [epoch: 1.84 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6159527363472667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6159527363472667 | validation: 0.6730433686855268]
	TIME [epoch: 1.84 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5648387411041528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5648387411041528 | validation: 0.5405435579852752]
	TIME [epoch: 1.84 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5027275550748611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5027275550748611 | validation: 0.6193129456944768]
	TIME [epoch: 1.84 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48488040022152873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48488040022152873 | validation: 0.47774399118438665]
	TIME [epoch: 1.84 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5226234128098668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5226234128098668 | validation: 0.7274804076780088]
	TIME [epoch: 1.84 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5174981404199983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5174981404199983 | validation: 0.5029270229860092]
	TIME [epoch: 1.84 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4692222913373416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4692222913373416 | validation: 0.5670044017292909]
	TIME [epoch: 1.84 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4542981433863517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4542981433863517 | validation: 0.6271033253055305]
	TIME [epoch: 1.84 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5572662269347778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5572662269347778 | validation: 0.5340819324135192]
	TIME [epoch: 1.85 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6450150251948417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6450150251948417 | validation: 0.49624144489833955]
	TIME [epoch: 1.84 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45711840359499917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45711840359499917 | validation: 0.6278970747046024]
	TIME [epoch: 1.84 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5135107008462143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5135107008462143 | validation: 0.4991691242761861]
	TIME [epoch: 1.84 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5070389392995958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5070389392995958 | validation: 0.525160430350016]
	TIME [epoch: 1.84 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4115407830611089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4115407830611089 | validation: 0.4623204475075916]
	TIME [epoch: 1.84 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3962406525144286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3962406525144286 | validation: 0.8284961690293612]
	TIME [epoch: 1.85 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5498674019424514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5498674019424514 | validation: 0.9051419469213725]
	TIME [epoch: 1.85 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8965499843569837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8965499843569837 | validation: 0.7535823120895139]
	TIME [epoch: 1.84 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5736552370239212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5736552370239212 | validation: 0.5934474017796978]
	TIME [epoch: 1.85 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49685818900159046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49685818900159046 | validation: 0.6124645495012235]
	TIME [epoch: 1.85 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5252316565668811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5252316565668811 | validation: 0.5049093055785673]
	TIME [epoch: 1.85 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4253527767926954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4253527767926954 | validation: 0.585387325943741]
	TIME [epoch: 1.84 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4194806241597982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4194806241597982 | validation: 0.48584376882449015]
	TIME [epoch: 1.85 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42797887109172666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42797887109172666 | validation: 0.5983905908623911]
	TIME [epoch: 1.84 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49392800796424413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49392800796424413 | validation: 0.4875591400421594]
	TIME [epoch: 1.84 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49433032068420807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49433032068420807 | validation: 0.5193490743837923]
	TIME [epoch: 1.84 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4139265374638137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4139265374638137 | validation: 0.48218239552418396]
	TIME [epoch: 1.86 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37680937060286257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37680937060286257 | validation: 0.4863550265684775]
	TIME [epoch: 1.84 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36898627885122764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36898627885122764 | validation: 0.5862205568649624]
	TIME [epoch: 1.84 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4116892761788219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4116892761788219 | validation: 0.3882800953702983]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_285.pth
	Model improved!!!
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6802656653821082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6802656653821082 | validation: 0.6219910621448735]
	TIME [epoch: 1.84 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4753921105042638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4753921105042638 | validation: 0.5928210304316403]
	TIME [epoch: 1.84 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4990621217588047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4990621217588047 | validation: 0.534939841696182]
	TIME [epoch: 1.84 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4488426983306417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4488426983306417 | validation: 0.49308510716046466]
	TIME [epoch: 1.84 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3589452553022875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3589452553022875 | validation: 0.4479865815239779]
	TIME [epoch: 1.84 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3557382885739236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3557382885739236 | validation: 0.4723775855556134]
	TIME [epoch: 1.84 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34224351614093984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34224351614093984 | validation: 0.5158746893594751]
	TIME [epoch: 1.85 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3480633704819851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3480633704819851 | validation: 0.2859988030196222]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_293.pth
	Model improved!!!
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5828163221903706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5828163221903706 | validation: 0.9202258270625612]
	TIME [epoch: 1.85 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6350486208756254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6350486208756254 | validation: 0.5651902570559016]
	TIME [epoch: 1.84 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6143939625677252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6143939625677252 | validation: 0.47235949264728205]
	TIME [epoch: 1.86 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4370397451039738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4370397451039738 | validation: 0.5055948529730401]
	TIME [epoch: 1.84 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4004882964421648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4004882964421648 | validation: 0.5275143487929617]
	TIME [epoch: 1.84 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4034324854033537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4034324854033537 | validation: 0.4931556546027839]
	TIME [epoch: 1.84 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36235790320648165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36235790320648165 | validation: 0.43768943721901654]
	TIME [epoch: 1.84 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33189262039404627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33189262039404627 | validation: 0.4766914462593955]
	TIME [epoch: 1.85 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3090774562337922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3090774562337922 | validation: 0.4158702462944436]
	TIME [epoch: 1.84 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30157756255192647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30157756255192647 | validation: 0.5621652662629463]
	TIME [epoch: 1.84 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4846738246570286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4846738246570286 | validation: 0.38434017352520455]
	TIME [epoch: 1.84 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6891727698759916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6891727698759916 | validation: 0.5069437187085354]
	TIME [epoch: 1.84 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5350125458500946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5350125458500946 | validation: 0.6471316170011958]
	TIME [epoch: 1.84 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5011064622801554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5011064622801554 | validation: 0.6131229225421305]
	TIME [epoch: 1.84 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4419722430566705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4419722430566705 | validation: 0.482366626704719]
	TIME [epoch: 1.84 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32253492253313654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32253492253313654 | validation: 0.3794211997353044]
	TIME [epoch: 1.84 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.333942197244727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.333942197244727 | validation: 0.4434663383146411]
	TIME [epoch: 1.84 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2813617910664537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2813617910664537 | validation: 0.42697091659069586]
	TIME [epoch: 1.84 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.277396858925696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.277396858925696 | validation: 0.4232729419664185]
	TIME [epoch: 1.84 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.294199282433706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.294199282433706 | validation: 0.5864610610863193]
	TIME [epoch: 1.84 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5714554600510737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5714554600510737 | validation: 0.413933934040521]
	TIME [epoch: 1.84 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5373531558507283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5373531558507283 | validation: 0.4069519712261516]
	TIME [epoch: 1.84 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2849575370878544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2849575370878544 | validation: 0.5270877106751178]
	TIME [epoch: 1.84 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3617284957823304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3617284957823304 | validation: 0.41483691778738807]
	TIME [epoch: 1.84 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3685719405294657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3685719405294657 | validation: 0.6658710927466487]
	TIME [epoch: 1.84 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4134213014988296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4134213014988296 | validation: 0.5657993588277558]
	TIME [epoch: 1.84 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42044163891229674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42044163891229674 | validation: 0.4762284186405885]
	TIME [epoch: 1.84 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2969932198958481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2969932198958481 | validation: 0.430231018184824]
	TIME [epoch: 1.85 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29169805440286617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29169805440286617 | validation: 0.4353573023094368]
	TIME [epoch: 1.83 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2913710007438652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2913710007438652 | validation: 0.422857560350625]
	TIME [epoch: 1.83 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3026477180988941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3026477180988941 | validation: 0.580651078747458]
	TIME [epoch: 1.83 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41338345693338907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41338345693338907 | validation: 0.4100191130968771]
	TIME [epoch: 1.83 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4966115412604895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4966115412604895 | validation: 0.5303305429388245]
	TIME [epoch: 1.83 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3716567140234939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3716567140234939 | validation: 0.4254452956178234]
	TIME [epoch: 1.84 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25299721664960484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25299721664960484 | validation: 0.395984690976334]
	TIME [epoch: 1.83 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2709419919922646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2709419919922646 | validation: 0.5001494668145317]
	TIME [epoch: 1.83 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2973801620233033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2973801620233033 | validation: 0.3833173439700689]
	TIME [epoch: 1.83 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42774455186063376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42774455186063376 | validation: 0.5028114864053591]
	TIME [epoch: 1.85 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29850765788357414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29850765788357414 | validation: 0.36827118935487635]
	TIME [epoch: 1.84 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2466873549029456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2466873549029456 | validation: 0.3554484519309075]
	TIME [epoch: 1.84 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2927119229555295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2927119229555295 | validation: 0.48952518051772853]
	TIME [epoch: 1.84 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27463888956157223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27463888956157223 | validation: 0.4197409526017071]
	TIME [epoch: 1.84 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3193366865132193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3193366865132193 | validation: 0.5234801844161694]
	TIME [epoch: 1.85 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3312776273464338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3312776273464338 | validation: 0.4454984431353056]
	TIME [epoch: 1.84 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3120083443918552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3120083443918552 | validation: 0.34104060159064736]
	TIME [epoch: 1.84 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2729745670254071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2729745670254071 | validation: 0.5404136716862001]
	TIME [epoch: 1.84 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3469810064573824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3469810064573824 | validation: 0.39142088754298554]
	TIME [epoch: 1.84 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33745847973704984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33745847973704984 | validation: 0.4505101523199466]
	TIME [epoch: 1.85 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24507464339402193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24507464339402193 | validation: 0.4035124753047885]
	TIME [epoch: 1.84 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20857970474068788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20857970474068788 | validation: 0.3098184044581729]
	TIME [epoch: 1.85 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22654749117811143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22654749117811143 | validation: 0.5049271283713671]
	TIME [epoch: 1.85 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2893052798338315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2893052798338315 | validation: 0.3871126804000765]
	TIME [epoch: 1.85 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3898714562061262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3898714562061262 | validation: 0.4721821306041739]
	TIME [epoch: 1.85 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3519275581597758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3519275581597758 | validation: 0.41313742868049275]
	TIME [epoch: 1.85 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24381814020535952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24381814020535952 | validation: 0.33175874894518986]
	TIME [epoch: 1.84 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.254477920376178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.254477920376178 | validation: 0.47861976317769667]
	TIME [epoch: 1.85 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24696188916594303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24696188916594303 | validation: 0.3103836387976153]
	TIME [epoch: 1.84 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28971200399403285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28971200399403285 | validation: 0.56092893020358]
	TIME [epoch: 1.84 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3384507093725735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3384507093725735 | validation: 0.3111230593165759]
	TIME [epoch: 1.84 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30142742187258126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30142742187258126 | validation: 0.3212976144477002]
	TIME [epoch: 1.84 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1812776053812709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1812776053812709 | validation: 0.36829549713101306]
	TIME [epoch: 1.84 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2241185623949311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2241185623949311 | validation: 0.29098478446376824]
	TIME [epoch: 1.83 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2293615625533771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2293615625533771 | validation: 0.3553022003130619]
	TIME [epoch: 1.83 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22270540889828525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22270540889828525 | validation: 0.3792737306965419]
	TIME [epoch: 1.83 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31411267079390864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31411267079390864 | validation: 0.33956758169136353]
	TIME [epoch: 1.83 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27987050644262085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27987050644262085 | validation: 0.5405577589404035]
	TIME [epoch: 1.83 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.307440749850958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.307440749850958 | validation: 0.56186323781922]
	TIME [epoch: 1.83 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36298121195029015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36298121195029015 | validation: 0.5121104643355141]
	TIME [epoch: 1.84 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2781840011204001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2781840011204001 | validation: 0.34784788619243723]
	TIME [epoch: 1.83 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.322926761385611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.322926761385611 | validation: 0.3645069834679048]
	TIME [epoch: 1.83 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18939900153109612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18939900153109612 | validation: 0.4028850736408502]
	TIME [epoch: 1.84 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2440029677444606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2440029677444606 | validation: 0.40106384373060555]
	TIME [epoch: 1.83 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3224339318009009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3224339318009009 | validation: 0.32961288259657157]
	TIME [epoch: 1.84 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3817159191966112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3817159191966112 | validation: 0.44500570842755977]
	TIME [epoch: 1.83 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25625596486388785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25625596486388785 | validation: 0.3957003255124327]
	TIME [epoch: 1.84 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22036780143760498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22036780143760498 | validation: 0.40077972468605183]
	TIME [epoch: 1.84 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23022150403331015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23022150403331015 | validation: 0.3536406580008644]
	TIME [epoch: 1.85 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25438915618514557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25438915618514557 | validation: 0.3193664846560804]
	TIME [epoch: 1.85 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21980301968724378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21980301968724378 | validation: 0.3585531660421342]
	TIME [epoch: 1.84 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2417176067799997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2417176067799997 | validation: 0.2867589012853102]
	TIME [epoch: 1.83 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26554340995127973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26554340995127973 | validation: 0.5912595816425746]
	TIME [epoch: 1.84 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35427609686554645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35427609686554645 | validation: 0.3925713663029619]
	TIME [epoch: 1.84 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2665575452322028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2665575452322028 | validation: 0.47498676861791145]
	TIME [epoch: 1.84 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25654284741119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25654284741119 | validation: 0.33865808409378423]
	TIME [epoch: 1.83 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23899314040803643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23899314040803643 | validation: 0.3082117503967328]
	TIME [epoch: 1.83 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15393457474249647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15393457474249647 | validation: 0.31461102054795126]
	TIME [epoch: 1.84 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1535474098416527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1535474098416527 | validation: 0.23886943262741808]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_380.pth
	Model improved!!!
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16919931278341974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16919931278341974 | validation: 0.33375115935313854]
	TIME [epoch: 1.84 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22510728152699486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22510728152699486 | validation: 0.4846588397084355]
	TIME [epoch: 1.84 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41132942355319246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41132942355319246 | validation: 0.4713024493157266]
	TIME [epoch: 1.85 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3167432592869205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3167432592869205 | validation: 0.38625118252423507]
	TIME [epoch: 1.84 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21692919277208667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21692919277208667 | validation: 0.3127838322610521]
	TIME [epoch: 1.85 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24034269374191516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24034269374191516 | validation: 0.3823538238263722]
	TIME [epoch: 1.84 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20301686069414707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20301686069414707 | validation: 0.25513213855030586]
	TIME [epoch: 1.84 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22266364365644958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22266364365644958 | validation: 0.5949835262033746]
	TIME [epoch: 1.84 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33850839817224015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33850839817224015 | validation: 0.31155834576858615]
	TIME [epoch: 1.84 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5026385156421145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5026385156421145 | validation: 0.33348149087703055]
	TIME [epoch: 1.84 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28496502039245225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28496502039245225 | validation: 0.28244823200743663]
	TIME [epoch: 1.84 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22997343829738945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22997343829738945 | validation: 0.2899441971657159]
	TIME [epoch: 1.84 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24806143577601567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24806143577601567 | validation: 0.3857010557062474]
	TIME [epoch: 1.84 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2906956608327261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2906956608327261 | validation: 0.37687950778661006]
	TIME [epoch: 1.84 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23604250427780138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23604250427780138 | validation: 0.295402888378369]
	TIME [epoch: 1.84 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2047424652296013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2047424652296013 | validation: 0.3034865648697624]
	TIME [epoch: 1.85 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1428605796941166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1428605796941166 | validation: 0.2597225255507402]
	TIME [epoch: 1.84 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16335319045339572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16335319045339572 | validation: 0.3159655326760823]
	TIME [epoch: 1.84 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22379996286712914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22379996286712914 | validation: 0.24336267875332285]
	TIME [epoch: 1.85 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24428718746156933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24428718746156933 | validation: 0.3431971652276258]
	TIME [epoch: 1.84 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17905184236303912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17905184236303912 | validation: 0.2504544940198685]
	TIME [epoch: 1.83 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18486802675212027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18486802675212027 | validation: 0.5445108641954434]
	TIME [epoch: 1.83 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30152997183908437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30152997183908437 | validation: 0.31059270210412926]
	TIME [epoch: 1.83 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3127653482909229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3127653482909229 | validation: 0.3713610158466097]
	TIME [epoch: 1.83 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.219011449517395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.219011449517395 | validation: 0.3075497208879441]
	TIME [epoch: 1.82 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16635984090799127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16635984090799127 | validation: 0.3246601985607089]
	TIME [epoch: 1.83 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2107717645511655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2107717645511655 | validation: 0.5126496853595167]
	TIME [epoch: 1.82 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2768236111886453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2768236111886453 | validation: 0.28394686599791624]
	TIME [epoch: 1.83 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19261513379828954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19261513379828954 | validation: 0.3475461443767624]
	TIME [epoch: 1.82 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17331221818826373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17331221818826373 | validation: 0.25405865118646265]
	TIME [epoch: 1.83 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19207064213521052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19207064213521052 | validation: 0.296430953923727]
	TIME [epoch: 1.83 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17631996947249917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17631996947249917 | validation: 0.27899781034059545]
	TIME [epoch: 1.83 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2047220582581562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2047220582581562 | validation: 0.2898127191853609]
	TIME [epoch: 1.82 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14586429785142976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14586429785142976 | validation: 0.2774811415872076]
	TIME [epoch: 1.83 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13294727548321675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13294727548321675 | validation: 0.2798837598327845]
	TIME [epoch: 1.83 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17870932890571717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17870932890571717 | validation: 0.6381185604871612]
	TIME [epoch: 1.83 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4467485560073734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4467485560073734 | validation: 0.3965153046717116]
	TIME [epoch: 1.83 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3819437202400559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3819437202400559 | validation: 0.30511600042275266]
	TIME [epoch: 1.84 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17932741564678445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17932741564678445 | validation: 0.3544487148427504]
	TIME [epoch: 1.84 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.176129745197932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.176129745197932 | validation: 0.2728398138875486]
	TIME [epoch: 1.84 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18480050285173008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18480050285173008 | validation: 0.2730669724114522]
	TIME [epoch: 1.84 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12634396747729634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12634396747729634 | validation: 0.22116037230140304]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_422.pth
	Model improved!!!
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12197851530779573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12197851530779573 | validation: 0.30284857345836574]
	TIME [epoch: 1.85 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14695558528311323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14695558528311323 | validation: 0.29736304641087774]
	TIME [epoch: 1.84 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23719060888011434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23719060888011434 | validation: 0.6117858496455977]
	TIME [epoch: 1.84 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3505414299193596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3505414299193596 | validation: 0.293730037201709]
	TIME [epoch: 1.84 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24031513712892497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24031513712892497 | validation: 0.30101641797343415]
	TIME [epoch: 1.84 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15448893687644058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15448893687644058 | validation: 0.26133491788615154]
	TIME [epoch: 1.84 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21908080402049734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21908080402049734 | validation: 0.4056861732023045]
	TIME [epoch: 1.84 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3158166363265281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3158166363265281 | validation: 0.2342340296250045]
	TIME [epoch: 1.84 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18700649799392863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18700649799392863 | validation: 0.34288761048375505]
	TIME [epoch: 1.84 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15882429492541092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15882429492541092 | validation: 0.23448372971456388]
	TIME [epoch: 1.84 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13797382885981974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13797382885981974 | validation: 0.5826602469009664]
	TIME [epoch: 1.84 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34999506534283215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34999506534283215 | validation: 0.3462716544365016]
	TIME [epoch: 1.83 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25497143160761904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25497143160761904 | validation: 0.358767437939101]
	TIME [epoch: 1.84 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16772436615229258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16772436615229258 | validation: 0.23167669269221475]
	TIME [epoch: 1.84 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12748578850246334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12748578850246334 | validation: 0.21062332656640806]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_437.pth
	Model improved!!!
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12913862859704767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12913862859704767 | validation: 0.27382857179520725]
	TIME [epoch: 1.83 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17362129390671194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17362129390671194 | validation: 0.24475595447447113]
	TIME [epoch: 1.83 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23009440206527312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23009440206527312 | validation: 0.32388468465270864]
	TIME [epoch: 1.83 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.231510176827085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.231510176827085 | validation: 0.3549573378899067]
	TIME [epoch: 1.83 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27160432415770147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27160432415770147 | validation: 0.5746444103636721]
	TIME [epoch: 1.84 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36823784268708076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36823784268708076 | validation: 0.29491370444917336]
	TIME [epoch: 1.83 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27612381256580476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27612381256580476 | validation: 0.49450843750645873]
	TIME [epoch: 1.83 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23822103683429469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23822103683429469 | validation: 0.28366389899129046]
	TIME [epoch: 1.85 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1780151657668589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1780151657668589 | validation: 0.2534553768180278]
	TIME [epoch: 1.83 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1410055551736222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1410055551736222 | validation: 0.298435128396695]
	TIME [epoch: 1.84 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16669082782510714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16669082782510714 | validation: 0.29453900810645073]
	TIME [epoch: 1.83 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17347320439735156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17347320439735156 | validation: 0.23663241460811754]
	TIME [epoch: 1.83 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13132720917194102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13132720917194102 | validation: 0.2649300348752108]
	TIME [epoch: 1.83 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15412058472572432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15412058472572432 | validation: 0.2599093682926418]
	TIME [epoch: 1.83 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16156547755246892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16156547755246892 | validation: 0.24050631597955682]
	TIME [epoch: 1.83 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17061737619121975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17061737619121975 | validation: 0.3280287243464334]
	TIME [epoch: 1.83 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1523996940921118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1523996940921118 | validation: 0.24136380549214398]
	TIME [epoch: 1.83 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24650021040563388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24650021040563388 | validation: 0.5505241091813303]
	TIME [epoch: 1.83 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33457746515573805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33457746515573805 | validation: 0.23075520709869368]
	TIME [epoch: 1.83 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1657497253839915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1657497253839915 | validation: 0.2857634344443521]
	TIME [epoch: 1.83 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12513038849347383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12513038849347383 | validation: 0.27014570719731074]
	TIME [epoch: 1.83 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11733798767544823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11733798767544823 | validation: 0.24022935220012243]
	TIME [epoch: 1.83 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14893563306417887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14893563306417887 | validation: 0.24050727118669593]
	TIME [epoch: 1.83 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16086531813718252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16086531813718252 | validation: 0.5487700417558738]
	TIME [epoch: 1.83 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35720078460300414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35720078460300414 | validation: 0.344710764651631]
	TIME [epoch: 1.83 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2867357498095363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2867357498095363 | validation: 0.30104383579214233]
	TIME [epoch: 1.84 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13691989601809468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13691989601809468 | validation: 0.2731355666161719]
	TIME [epoch: 1.83 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1780410322802829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1780410322802829 | validation: 0.2877704106903158]
	TIME [epoch: 1.83 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14289100920765566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14289100920765566 | validation: 0.24816035830686778]
	TIME [epoch: 1.83 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14043889789807104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14043889789807104 | validation: 0.2755823219155687]
	TIME [epoch: 1.83 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.224055668353547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.224055668353547 | validation: 0.32418734054617837]
	TIME [epoch: 1.83 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20082174549641102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20082174549641102 | validation: 0.22509225561809476]
	TIME [epoch: 1.84 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14027259785842794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14027259785842794 | validation: 0.3458313486373455]
	TIME [epoch: 1.84 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18401753443146884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18401753443146884 | validation: 0.34205237050183374]
	TIME [epoch: 1.83 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2649447003695392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2649447003695392 | validation: 0.4226468461863482]
	TIME [epoch: 1.83 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20165024918227237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20165024918227237 | validation: 0.23669514404516928]
	TIME [epoch: 1.84 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14866376964022146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14866376964022146 | validation: 0.3325036167563207]
	TIME [epoch: 1.83 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17420193078433194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17420193078433194 | validation: 0.22935561251124145]
	TIME [epoch: 1.83 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1400171587290813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1400171587290813 | validation: 0.276089529082774]
	TIME [epoch: 1.83 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13123800332430224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13123800332430224 | validation: 0.19282044232539294]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_477.pth
	Model improved!!!
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1461549553970317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1461549553970317 | validation: 0.3029230684702038]
	TIME [epoch: 1.84 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15830720501669138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15830720501669138 | validation: 0.19379976454753436]
	TIME [epoch: 1.83 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1859791572477402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1859791572477402 | validation: 0.33045593241076937]
	TIME [epoch: 1.84 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1672460696312622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1672460696312622 | validation: 0.2550017911641033]
	TIME [epoch: 1.84 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19757844011900283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19757844011900283 | validation: 0.3668101255167982]
	TIME [epoch: 1.84 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24444025082240964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24444025082240964 | validation: 0.35787903001690896]
	TIME [epoch: 1.84 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20460738878809218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20460738878809218 | validation: 0.27977233847369337]
	TIME [epoch: 1.84 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2342104260452811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2342104260452811 | validation: 0.3641300246682659]
	TIME [epoch: 1.84 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24722575578576428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24722575578576428 | validation: 0.3440344752068997]
	TIME [epoch: 1.83 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17180684455185458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17180684455185458 | validation: 0.22422707191050703]
	TIME [epoch: 1.84 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10160509858088163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10160509858088163 | validation: 0.21464507676986125]
	TIME [epoch: 1.84 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1147112793684114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1147112793684114 | validation: 0.27908315233019154]
	TIME [epoch: 1.83 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1490077225177399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1490077225177399 | validation: 0.22586790711578378]
	TIME [epoch: 1.83 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13948057525166657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13948057525166657 | validation: 0.29329246488616817]
	TIME [epoch: 1.84 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16613518757755905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16613518757755905 | validation: 0.20898429112488826]
	TIME [epoch: 1.84 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1490089402129078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1490089402129078 | validation: 0.2961133804382838]
	TIME [epoch: 1.84 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18501740782866763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18501740782866763 | validation: 0.18060660263895867]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_494.pth
	Model improved!!!
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22076912891194483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22076912891194483 | validation: 0.34918724818644364]
	TIME [epoch: 1.85 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.210181837393822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.210181837393822 | validation: 0.4449223136077892]
	TIME [epoch: 1.83 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2612568373787908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2612568373787908 | validation: 0.4218291128465748]
	TIME [epoch: 1.83 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1895273800920859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1895273800920859 | validation: 0.21010888337616143]
	TIME [epoch: 1.83 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13207100202451696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13207100202451696 | validation: 0.24015957839450264]
	TIME [epoch: 1.83 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13871120256117028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13871120256117028 | validation: 1.0276976481343651]
	TIME [epoch: 1.83 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7281804031633594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7281804031633594 | validation: 0.7319050761302102]
	TIME [epoch: 44.2 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5337468491104244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5337468491104244 | validation: 0.5894471213275528]
	TIME [epoch: 3.66 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38925696345798894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38925696345798894 | validation: 0.3043300104124426]
	TIME [epoch: 3.63 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20060727563321995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20060727563321995 | validation: 0.3110950492204678]
	TIME [epoch: 3.63 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1503301448065526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1503301448065526 | validation: 0.19259281267684863]
	TIME [epoch: 3.62 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14454022300746672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14454022300746672 | validation: 0.20727022496962128]
	TIME [epoch: 3.61 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11853492830739727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11853492830739727 | validation: 0.2148343890015656]
	TIME [epoch: 3.6 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11697222091394224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11697222091394224 | validation: 0.19958601250132485]
	TIME [epoch: 3.63 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16310046280618315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16310046280618315 | validation: 0.3141066775998491]
	TIME [epoch: 3.62 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22421750929306836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22421750929306836 | validation: 0.218899537551889]
	TIME [epoch: 3.62 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25015078855321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25015078855321 | validation: 0.25803275976231904]
	TIME [epoch: 3.62 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13636309276630396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13636309276630396 | validation: 0.2057757030812916]
	TIME [epoch: 3.63 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11599070632503893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11599070632503893 | validation: 0.2969773848150247]
	TIME [epoch: 3.63 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16086704734259843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16086704734259843 | validation: 0.3149926108365509]
	TIME [epoch: 3.62 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20144322349420826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20144322349420826 | validation: 0.3463134283917573]
	TIME [epoch: 3.63 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24438632922823675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24438632922823675 | validation: 0.23581922687918727]
	TIME [epoch: 3.63 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1991744744641918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1991744744641918 | validation: 0.26712422148474163]
	TIME [epoch: 3.62 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12349097190067049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12349097190067049 | validation: 0.18785201136081572]
	TIME [epoch: 3.62 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11062715391282218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11062715391282218 | validation: 0.2620544864968692]
	TIME [epoch: 3.62 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11412537301820187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11412537301820187 | validation: 0.20630389991243608]
	TIME [epoch: 3.62 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13422072252392445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13422072252392445 | validation: 0.3630958863423387]
	TIME [epoch: 3.62 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21571878572560024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21571878572560024 | validation: 0.2174591294236029]
	TIME [epoch: 3.61 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17467534981175195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17467534981175195 | validation: 0.30387942385250405]
	TIME [epoch: 3.62 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14549202634212477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14549202634212477 | validation: 0.19342490710899607]
	TIME [epoch: 3.62 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13414287972891834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13414287972891834 | validation: 0.25907766830559215]
	TIME [epoch: 3.62 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11171071146208783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11171071146208783 | validation: 0.24722245032581142]
	TIME [epoch: 3.62 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12170350175072443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12170350175072443 | validation: 0.27442072614348734]
	TIME [epoch: 3.62 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13202944814446735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13202944814446735 | validation: 0.26860405504320256]
	TIME [epoch: 3.62 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13672458030949927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13672458030949927 | validation: 0.23707602689122043]
	TIME [epoch: 3.62 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11376718795279828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11376718795279828 | validation: 0.2643751768364268]
	TIME [epoch: 3.64 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15753815497573304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15753815497573304 | validation: 0.32932582164551516]
	TIME [epoch: 3.63 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2461279383937493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2461279383937493 | validation: 0.43642175460394433]
	TIME [epoch: 3.63 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3268931236498099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3268931236498099 | validation: 0.2622472012712759]
	TIME [epoch: 3.62 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2216644739079294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2216644739079294 | validation: 0.30867912649918866]
	TIME [epoch: 3.62 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14238682817181764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14238682817181764 | validation: 0.20899979850835423]
	TIME [epoch: 3.63 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11236914126108298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11236914126108298 | validation: 0.23134023795863198]
	TIME [epoch: 3.62 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11437482158921747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11437482158921747 | validation: 0.2556831240812638]
	TIME [epoch: 3.63 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1544844283434471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1544844283434471 | validation: 0.2271438862357366]
	TIME [epoch: 3.63 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10094393100384635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10094393100384635 | validation: 0.27725007005732566]
	TIME [epoch: 3.62 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1025493927620926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1025493927620926 | validation: 0.18160275187134883]
	TIME [epoch: 3.63 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12417647913975832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12417647913975832 | validation: 0.3023985830051166]
	TIME [epoch: 3.62 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18097582633857315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18097582633857315 | validation: 0.24093483818069483]
	TIME [epoch: 3.64 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24662970803661288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24662970803661288 | validation: 0.435946734461157]
	TIME [epoch: 3.64 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2325944770684807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2325944770684807 | validation: 0.28187916340907776]
	TIME [epoch: 3.63 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17080558053116968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17080558053116968 | validation: 0.28052569626838775]
	TIME [epoch: 3.64 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16181117820712154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16181117820712154 | validation: 0.23140269773478492]
	TIME [epoch: 3.63 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11740411589230537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11740411589230537 | validation: 0.18624485126823198]
	TIME [epoch: 3.63 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09071871543381313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09071871543381313 | validation: 0.21451450002372435]
	TIME [epoch: 3.63 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09725214163175046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09725214163175046 | validation: 0.22271450394722603]
	TIME [epoch: 3.64 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15726374983909872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15726374983909872 | validation: 0.4172676074135912]
	TIME [epoch: 3.63 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2904177206907172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2904177206907172 | validation: 0.21471208220533958]
	TIME [epoch: 3.63 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16097048931457242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16097048931457242 | validation: 0.3240615499008952]
	TIME [epoch: 3.63 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1781207346383382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1781207346383382 | validation: 0.18251117717803164]
	TIME [epoch: 3.63 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19204006556183317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19204006556183317 | validation: 0.2938183398740661]
	TIME [epoch: 3.63 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1263587646176115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1263587646176115 | validation: 0.24660618325328737]
	TIME [epoch: 3.63 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13821389315563173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13821389315563173 | validation: 0.3363509512914006]
	TIME [epoch: 3.63 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15741482254431305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15741482254431305 | validation: 0.22648057539664262]
	TIME [epoch: 3.63 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15013927738064609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15013927738064609 | validation: 0.2634819200815471]
	TIME [epoch: 3.62 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1545670659834324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1545670659834324 | validation: 0.214040629836754]
	TIME [epoch: 3.63 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14125355549808755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14125355549808755 | validation: 0.2311306081987756]
	TIME [epoch: 3.64 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12046649140445247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12046649140445247 | validation: 0.20076955669495467]
	TIME [epoch: 3.64 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11410839525694876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11410839525694876 | validation: 0.24222131494984503]
	TIME [epoch: 3.63 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13009756762216682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13009756762216682 | validation: 0.2037484751885923]
	TIME [epoch: 3.61 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.156490823323839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.156490823323839 | validation: 0.33139903214580235]
	TIME [epoch: 3.64 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18296085933987008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18296085933987008 | validation: 0.35288652503931234]
	TIME [epoch: 3.63 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5935716719076345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5935716719076345 | validation: 0.4729801594541776]
	TIME [epoch: 3.63 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3459987155995071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3459987155995071 | validation: 0.399394388614349]
	TIME [epoch: 3.61 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.233620553470323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.233620553470323 | validation: 0.2441467553271541]
	TIME [epoch: 3.62 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13025945032574143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13025945032574143 | validation: 0.2766510406513675]
	TIME [epoch: 3.62 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13582411645512826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13582411645512826 | validation: 0.2518806907745188]
	TIME [epoch: 3.63 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22906232303578747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22906232303578747 | validation: 0.2608213641300018]
	TIME [epoch: 3.64 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15971380562428236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15971380562428236 | validation: 0.27657673744524425]
	TIME [epoch: 3.64 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16560129220968975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16560129220968975 | validation: 0.3382794362430697]
	TIME [epoch: 3.63 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18045685813470172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18045685813470172 | validation: 0.2430235571203345]
	TIME [epoch: 3.61 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1385527284494464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1385527284494464 | validation: 0.2477656101059318]
	TIME [epoch: 3.63 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11783043696021991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11783043696021991 | validation: 0.1720612149023537]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_576.pth
	Model improved!!!
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10860019054412262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10860019054412262 | validation: 0.24909228470229003]
	TIME [epoch: 3.61 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12032895224257927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12032895224257927 | validation: 0.19822535617101897]
	TIME [epoch: 3.62 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16627241017928995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16627241017928995 | validation: 0.39138542201814097]
	TIME [epoch: 3.62 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21161234339902152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21161234339902152 | validation: 0.19092601789374797]
	TIME [epoch: 3.62 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.170226126916704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.170226126916704 | validation: 0.2395241355440777]
	TIME [epoch: 3.62 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12792401097040101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12792401097040101 | validation: 0.2169616436194166]
	TIME [epoch: 3.62 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11427176564309718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11427176564309718 | validation: 0.210356023395604]
	TIME [epoch: 3.63 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10304896190483405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10304896190483405 | validation: 0.21169341238234055]
	TIME [epoch: 3.63 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1101697300685636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1101697300685636 | validation: 0.23656871422726483]
	TIME [epoch: 3.63 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1518370000872268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1518370000872268 | validation: 0.23531158561860585]
	TIME [epoch: 3.63 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15720111284368607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15720111284368607 | validation: 0.2670224331219277]
	TIME [epoch: 3.62 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14813038066879727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14813038066879727 | validation: 0.1805631302046783]
	TIME [epoch: 3.61 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15396176125739985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15396176125739985 | validation: 0.3180584526048008]
	TIME [epoch: 3.62 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19282713135010263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19282713135010263 | validation: 0.20373343341272251]
	TIME [epoch: 3.62 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16728958757023904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16728958757023904 | validation: 0.27676609611886316]
	TIME [epoch: 3.62 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11998929544868096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11998929544868096 | validation: 0.18558977550516398]
	TIME [epoch: 3.62 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09517987764840484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09517987764840484 | validation: 0.20095004081969747]
	TIME [epoch: 3.62 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08269628553400957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08269628553400957 | validation: 0.16957012724688694]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_594.pth
	Model improved!!!
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08291895914724821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08291895914724821 | validation: 0.20286714382590953]
	TIME [epoch: 3.62 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09428805859710936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09428805859710936 | validation: 0.19665360694305856]
	TIME [epoch: 3.63 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14805662014705773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14805662014705773 | validation: 0.4295373571140501]
	TIME [epoch: 3.64 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3041456920905191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3041456920905191 | validation: 0.21605150900458098]
	TIME [epoch: 3.64 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27082433751118046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27082433751118046 | validation: 0.3152963164494292]
	TIME [epoch: 3.63 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.157748257063332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.157748257063332 | validation: 0.27108089520870465]
	TIME [epoch: 3.62 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1364364629678866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1364364629678866 | validation: 0.2729800384175013]
	TIME [epoch: 3.64 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18355079629055354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18355079629055354 | validation: 0.22562446016077928]
	TIME [epoch: 3.63 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1622331289884269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1622331289884269 | validation: 0.19685768406621373]
	TIME [epoch: 3.63 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09480115420810384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09480115420810384 | validation: 0.17219432021108597]
	TIME [epoch: 3.63 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07797332394103339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07797332394103339 | validation: 0.17399968426308612]
	TIME [epoch: 3.63 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07932821152612991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07932821152612991 | validation: 0.15361839878417333]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_606.pth
	Model improved!!!
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08252628883632653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08252628883632653 | validation: 0.19349810095766415]
	TIME [epoch: 3.62 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10539719282908046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10539719282908046 | validation: 0.2113145202712431]
	TIME [epoch: 3.62 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15352375140947236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15352375140947236 | validation: 0.4687921389210821]
	TIME [epoch: 3.62 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2759752423142862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2759752423142862 | validation: 0.2205473258751959]
	TIME [epoch: 3.63 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28406364736676387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28406364736676387 | validation: 0.25725759158505124]
	TIME [epoch: 3.63 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11610381196182305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11610381196182305 | validation: 0.235880469678426]
	TIME [epoch: 3.62 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09281917753354148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09281917753354148 | validation: 0.15035511863206555]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_613.pth
	Model improved!!!
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07626243676605154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07626243676605154 | validation: 0.18479386027456712]
	TIME [epoch: 3.64 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08180752309481769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08180752309481769 | validation: 0.16193688226874361]
	TIME [epoch: 3.63 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11561727142595082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11561727142595082 | validation: 0.2679042327383479]
	TIME [epoch: 3.64 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21197811398561608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21197811398561608 | validation: 0.26340946871872584]
	TIME [epoch: 3.64 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2605646191801735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2605646191801735 | validation: 0.28565192434450676]
	TIME [epoch: 3.63 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1394480308100308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1394480308100308 | validation: 0.2244524540734979]
	TIME [epoch: 3.63 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15120640612542033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15120640612542033 | validation: 0.301516586307387]
	TIME [epoch: 3.63 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17340841842686167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17340841842686167 | validation: 0.15846243128218673]
	TIME [epoch: 3.62 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0874294773638327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0874294773638327 | validation: 0.20586752890826238]
	TIME [epoch: 3.64 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10228549129659331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10228549129659331 | validation: 0.15714417863556612]
	TIME [epoch: 3.64 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10647239460958208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10647239460958208 | validation: 0.21905582657809386]
	TIME [epoch: 3.64 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12130560903339152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12130560903339152 | validation: 0.16808450937965]
	TIME [epoch: 3.65 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1629760397090119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1629760397090119 | validation: 0.25111291200733876]
	TIME [epoch: 3.65 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16444147244085244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16444147244085244 | validation: 0.18833124408937532]
	TIME [epoch: 3.64 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12758185368326064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12758185368326064 | validation: 0.32581022754487665]
	TIME [epoch: 3.63 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13540897630526774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13540897630526774 | validation: 0.21088077414092576]
	TIME [epoch: 3.64 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13461225722589315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13461225722589315 | validation: 0.2739766105299872]
	TIME [epoch: 3.64 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1431972568084662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1431972568084662 | validation: 0.18761848009579019]
	TIME [epoch: 3.62 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10783808557902237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10783808557902237 | validation: 0.1907652983731879]
	TIME [epoch: 3.63 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0839344864629367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0839344864629367 | validation: 0.2236981949634127]
	TIME [epoch: 3.63 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11821456160948472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11821456160948472 | validation: 0.24012440818615569]
	TIME [epoch: 3.64 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1574572571157445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1574572571157445 | validation: 0.24621541659836038]
	TIME [epoch: 3.63 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19351398846136347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19351398846136347 | validation: 0.19429544288182757]
	TIME [epoch: 3.63 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21273594509363328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21273594509363328 | validation: 0.2221591741518516]
	TIME [epoch: 3.64 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17957056895266135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17957056895266135 | validation: 0.15416263697619362]
	TIME [epoch: 3.65 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1099803545368247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1099803545368247 | validation: 0.2160546935997505]
	TIME [epoch: 3.64 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08887786899318971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08887786899318971 | validation: 0.1608936102326346]
	TIME [epoch: 3.63 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08351456794857245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08351456794857245 | validation: 0.2015957528790523]
	TIME [epoch: 3.64 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09124733719280856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09124733719280856 | validation: 0.15555816683020873]
	TIME [epoch: 3.63 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10843994179636113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10843994179636113 | validation: 0.27231689436949624]
	TIME [epoch: 3.64 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13772299752987188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13772299752987188 | validation: 0.18549087720231228]
	TIME [epoch: 3.63 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16688275143272774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16688275143272774 | validation: 0.2960220291891601]
	TIME [epoch: 3.63 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18355691555539197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18355691555539197 | validation: 0.2221362058864585]
	TIME [epoch: 3.63 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1767214297788179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1767214297788179 | validation: 0.2145785625273264]
	TIME [epoch: 3.63 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11517117234903801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11517117234903801 | validation: 0.17633911823524479]
	TIME [epoch: 3.63 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08466787790431234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08466787790431234 | validation: 0.16661692434608777]
	TIME [epoch: 3.63 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09764560697733218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09764560697733218 | validation: 0.19746450546138422]
	TIME [epoch: 3.64 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12793883159327166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12793883159327166 | validation: 0.19727347598129136]
	TIME [epoch: 3.64 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11770561833874255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11770561833874255 | validation: 0.21139840573464608]
	TIME [epoch: 3.64 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09693351257949238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09693351257949238 | validation: 0.20250304474664677]
	TIME [epoch: 3.63 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09871333162838684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09871333162838684 | validation: 0.23397659352473996]
	TIME [epoch: 3.63 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12264575255860365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12264575255860365 | validation: 0.1600299788209743]
	TIME [epoch: 3.63 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18552175618830394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18552175618830394 | validation: 0.2425058688227112]
	TIME [epoch: 3.63 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17565045161760587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17565045161760587 | validation: 0.21528673860018338]
	TIME [epoch: 3.63 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1713511786254569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1713511786254569 | validation: 0.31464285180391816]
	TIME [epoch: 3.63 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15479004494278906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15479004494278906 | validation: 0.17545134918807592]
	TIME [epoch: 3.64 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09708224074442451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09708224074442451 | validation: 0.16605905549686625]
	TIME [epoch: 3.63 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07798540376803322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07798540376803322 | validation: 0.16418099647812406]
	TIME [epoch: 3.64 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08564659532781634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08564659532781634 | validation: 0.17363050276501785]
	TIME [epoch: 3.63 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10376323994548134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10376323994548134 | validation: 0.21729369537956536]
	TIME [epoch: 3.64 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1546594891555667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1546594891555667 | validation: 0.2480346765075209]
	TIME [epoch: 3.64 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18051569773077694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18051569773077694 | validation: 0.20512601964582622]
	TIME [epoch: 3.65 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.115618195883814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.115618195883814 | validation: 0.13860116942661455]
	TIME [epoch: 3.65 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_666.pth
	Model improved!!!
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08179384238322172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08179384238322172 | validation: 0.19087753373338545]
	TIME [epoch: 3.64 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12301298123459693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12301298123459693 | validation: 0.19374315573036]
	TIME [epoch: 3.63 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17010619234851931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17010619234851931 | validation: 0.2224496075631498]
	TIME [epoch: 3.63 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11665300165916946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11665300165916946 | validation: 0.17161937015721082]
	TIME [epoch: 3.63 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.180931579385975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.180931579385975 | validation: 0.3545962547209519]
	TIME [epoch: 3.63 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22270400624112724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22270400624112724 | validation: 0.2131197020148004]
	TIME [epoch: 3.64 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13704500945271236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13704500945271236 | validation: 0.21670222326641886]
	TIME [epoch: 3.63 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08598637812425267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08598637812425267 | validation: 0.1611625850441594]
	TIME [epoch: 3.64 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07157585974138733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07157585974138733 | validation: 0.1618547632813262]
	TIME [epoch: 3.64 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07256773860953833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07256773860953833 | validation: 0.1583935612227757]
	TIME [epoch: 3.64 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0830669584595803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0830669584595803 | validation: 0.1792349277486459]
	TIME [epoch: 3.64 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1043090359133854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1043090359133854 | validation: 0.17452062768642673]
	TIME [epoch: 3.65 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12385806093334126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12385806093334126 | validation: 0.24348012257371063]
	TIME [epoch: 3.64 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15223963158260365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15223963158260365 | validation: 0.2143753632462398]
	TIME [epoch: 3.64 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13158698481999095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13158698481999095 | validation: 0.23798048938889718]
	TIME [epoch: 3.64 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13026341330516858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13026341330516858 | validation: 0.17509381414594954]
	TIME [epoch: 3.64 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20381580084022766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20381580084022766 | validation: 0.2695688622633296]
	TIME [epoch: 3.64 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16786119333771052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16786119333771052 | validation: 0.15219149077974842]
	TIME [epoch: 3.64 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11957434277070247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11957434277070247 | validation: 0.1991784778626718]
	TIME [epoch: 3.64 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1179910397489869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1179910397489869 | validation: 0.16870736945008866]
	TIME [epoch: 3.64 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1208539878987386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1208539878987386 | validation: 0.23463247476404986]
	TIME [epoch: 3.64 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1352079755112582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1352079755112582 | validation: 0.21719653236568326]
	TIME [epoch: 3.68 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1401943551641061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1401943551641061 | validation: 0.261926369697256]
	TIME [epoch: 3.63 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14047152036195504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14047152036195504 | validation: 0.16240765113500452]
	TIME [epoch: 3.64 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13185644474910657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13185644474910657 | validation: 0.22504518052305272]
	TIME [epoch: 3.64 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13847307775041956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13847307775041956 | validation: 0.13512935951666413]
	TIME [epoch: 3.64 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_692.pth
	Model improved!!!
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09415648492884363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09415648492884363 | validation: 0.17831402279235808]
	TIME [epoch: 3.64 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0852509028922108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0852509028922108 | validation: 0.13132762825898542]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_694.pth
	Model improved!!!
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0751243011977998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0751243011977998 | validation: 0.1818311591789119]
	TIME [epoch: 3.63 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07607330707496238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07607330707496238 | validation: 0.1274498952458786]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_696.pth
	Model improved!!!
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09851078914228201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09851078914228201 | validation: 0.21909470337623138]
	TIME [epoch: 3.64 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14051456382462363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14051456382462363 | validation: 0.21735457517262402]
	TIME [epoch: 3.64 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19222520017321587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19222520017321587 | validation: 0.29229480069663094]
	TIME [epoch: 3.63 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.168883902605197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.168883902605197 | validation: 0.20710999971810484]
	TIME [epoch: 3.63 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1221633861112971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1221633861112971 | validation: 0.1987058328419934]
	TIME [epoch: 3.62 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0998406833199634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0998406833199634 | validation: 0.14066540964887056]
	TIME [epoch: 3.63 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07324331974549932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07324331974549932 | validation: 0.14523009961952243]
	TIME [epoch: 3.63 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06531202980277988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06531202980277988 | validation: 0.13572570878611423]
	TIME [epoch: 3.64 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05951479842366622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05951479842366622 | validation: 0.16240594225975566]
	TIME [epoch: 3.64 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08436303903900806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08436303903900806 | validation: 0.17534356109543903]
	TIME [epoch: 3.64 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15769044286171485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15769044286171485 | validation: 0.25609298007385756]
	TIME [epoch: 3.63 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1800255577041576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1800255577041576 | validation: 0.20223315398984942]
	TIME [epoch: 3.63 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14118133703832178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14118133703832178 | validation: 0.16701570351378164]
	TIME [epoch: 3.63 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09147523861721489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09147523861721489 | validation: 0.16708237086687264]
	TIME [epoch: 3.63 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08593640056239522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08593640056239522 | validation: 0.13954693130587645]
	TIME [epoch: 3.64 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12240580077336738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12240580077336738 | validation: 0.20722293082609655]
	TIME [epoch: 3.63 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12245278738293458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12245278738293458 | validation: 0.14886556604176535]
	TIME [epoch: 3.64 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09745345492335548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09745345492335548 | validation: 0.2012735231655161]
	TIME [epoch: 3.62 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09058432796566017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09058432796566017 | validation: 0.1643269642760731]
	TIME [epoch: 3.64 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1131426373479145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1131426373479145 | validation: 0.2567191889587228]
	TIME [epoch: 3.63 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17981823981065856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17981823981065856 | validation: 0.1990907688546161]
	TIME [epoch: 3.64 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16573628769899706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16573628769899706 | validation: 0.3679334870639984]
	TIME [epoch: 3.64 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18749600816194012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18749600816194012 | validation: 0.2304889908330537]
	TIME [epoch: 3.65 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1777516937709115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1777516937709115 | validation: 0.19243817792092024]
	TIME [epoch: 3.62 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10590050916090407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10590050916090407 | validation: 0.1537511111809774]
	TIME [epoch: 3.63 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06780216092644581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06780216092644581 | validation: 0.12897377990446554]
	TIME [epoch: 3.63 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06136789144421979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06136789144421979 | validation: 0.14789451328962616]
	TIME [epoch: 3.63 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062068207151134785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062068207151134785 | validation: 0.13582465801628865]
	TIME [epoch: 3.64 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07458083231739476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07458083231739476 | validation: 0.1564560460264076]
	TIME [epoch: 3.63 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08145525614909618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08145525614909618 | validation: 0.16964065064896683]
	TIME [epoch: 3.64 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08432434511060109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08432434511060109 | validation: 0.14747882611377652]
	TIME [epoch: 3.64 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11088130142451341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11088130142451341 | validation: 0.2415707083753762]
	TIME [epoch: 3.63 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17780238496733589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17780238496733589 | validation: 0.17022975472065124]
	TIME [epoch: 3.64 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1829508604304317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1829508604304317 | validation: 0.1894936478908906]
	TIME [epoch: 3.64 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09771120440669379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09771120440669379 | validation: 0.22942359251254785]
	TIME [epoch: 3.63 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10445508785774577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10445508785774577 | validation: 0.18577703620072566]
	TIME [epoch: 3.65 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10589118346987836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10589118346987836 | validation: 0.20596391790972463]
	TIME [epoch: 3.63 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12493242126247324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12493242126247324 | validation: 0.1981378602346439]
	TIME [epoch: 3.64 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17663451881198156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17663451881198156 | validation: 0.18088237367689072]
	TIME [epoch: 3.63 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1430340129163444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1430340129163444 | validation: 0.15347250459725942]
	TIME [epoch: 3.63 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09643267444232408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09643267444232408 | validation: 0.14379536478791072]
	TIME [epoch: 3.63 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0896690986597851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0896690986597851 | validation: 0.1821381232888871]
	TIME [epoch: 3.63 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11519223640458272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11519223640458272 | validation: 0.16824409189312994]
	TIME [epoch: 3.63 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13536819262922886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13536819262922886 | validation: 0.23437393429808542]
	TIME [epoch: 3.63 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1479260196214765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1479260196214765 | validation: 0.18662383905054056]
	TIME [epoch: 3.63 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12207824095173868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12207824095173868 | validation: 0.25162999761309435]
	TIME [epoch: 3.63 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11994261812265816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11994261812265816 | validation: 0.16051301628466652]
	TIME [epoch: 3.63 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09979118107504846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09979118107504846 | validation: 0.1697953386401276]
	TIME [epoch: 3.64 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07851392824461048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07851392824461048 | validation: 0.11612291364707489]
	TIME [epoch: 3.65 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_745.pth
	Model improved!!!
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08051595865242452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08051595865242452 | validation: 0.15568708939983808]
	TIME [epoch: 3.64 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07894719348689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07894719348689 | validation: 0.11940927844360903]
	TIME [epoch: 3.64 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.086521707659804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.086521707659804 | validation: 0.17129968971810772]
	TIME [epoch: 3.64 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09939070149219553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09939070149219553 | validation: 0.10793643989393123]
	TIME [epoch: 3.64 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_749.pth
	Model improved!!!
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12686967538055627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12686967538055627 | validation: 0.20072452486450337]
	TIME [epoch: 3.62 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11712160495850489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11712160495850489 | validation: 0.18412399847081368]
	TIME [epoch: 3.62 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1235183846812543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1235183846812543 | validation: 0.3262463987397164]
	TIME [epoch: 3.62 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14595859413657347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14595859413657347 | validation: 0.1864123498805096]
	TIME [epoch: 3.62 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10140705970910326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10140705970910326 | validation: 0.1818860429219631]
	TIME [epoch: 3.62 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12488736593908996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12488736593908996 | validation: 0.18963577584552607]
	TIME [epoch: 3.62 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17302114517330008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17302114517330008 | validation: 0.20107928955766952]
	TIME [epoch: 3.62 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14685410557802248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14685410557802248 | validation: 0.17734888471969176]
	TIME [epoch: 3.63 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10851035355913792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10851035355913792 | validation: 0.15014025525478922]
	TIME [epoch: 3.64 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0675940672801564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0675940672801564 | validation: 0.11667610919253578]
	TIME [epoch: 3.62 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05562350348392037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05562350348392037 | validation: 0.14078193706020614]
	TIME [epoch: 3.63 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062117174767248894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062117174767248894 | validation: 0.1481320383384929]
	TIME [epoch: 3.62 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0812322656457279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0812322656457279 | validation: 0.13537891665256696]
	TIME [epoch: 3.62 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13205001430191365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13205001430191365 | validation: 0.1935968094086188]
	TIME [epoch: 3.62 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14264478514573875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14264478514573875 | validation: 0.12891230058886288]
	TIME [epoch: 3.63 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10416566471007255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10416566471007255 | validation: 0.18670927995259023]
	TIME [epoch: 3.62 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09230279260552805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09230279260552805 | validation: 0.18923779671951094]
	TIME [epoch: 3.62 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12404952100833537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12404952100833537 | validation: 0.28564812951063034]
	TIME [epoch: 3.61 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16073650563398803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16073650563398803 | validation: 0.15174898131425577]
	TIME [epoch: 3.62 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1021520800270337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1021520800270337 | validation: 0.15289020621307425]
	TIME [epoch: 3.62 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07254526791532938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07254526791532938 | validation: 0.1813125523998802]
	TIME [epoch: 3.63 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11568132154172286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11568132154172286 | validation: 0.1914406538926881]
	TIME [epoch: 3.63 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1366727483613829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1366727483613829 | validation: 0.2188888424003162]
	TIME [epoch: 3.62 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1714696946937817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1714696946937817 | validation: 0.13966718119385782]
	TIME [epoch: 3.62 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16679093883309057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16679093883309057 | validation: 0.16436485133555082]
	TIME [epoch: 3.63 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07928293610237805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07928293610237805 | validation: 0.11750470136709619]
	TIME [epoch: 3.62 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05545070641638625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05545070641638625 | validation: 0.14641752393595836]
	TIME [epoch: 3.61 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06266403430653368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06266403430653368 | validation: 0.16375300701581202]
	TIME [epoch: 3.62 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07433766069191386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07433766069191386 | validation: 0.20691133303692577]
	TIME [epoch: 3.61 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1039234128709748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1039234128709748 | validation: 0.1701965385146081]
	TIME [epoch: 3.62 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09508781252148799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09508781252148799 | validation: 0.13140829952989588]
	TIME [epoch: 3.61 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09431661713170755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09431661713170755 | validation: 0.17118619418052297]
	TIME [epoch: 3.62 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09556940482016411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09556940482016411 | validation: 0.12644158407036513]
	TIME [epoch: 3.62 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1054628333055384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1054628333055384 | validation: 0.19236366278374079]
	TIME [epoch: 3.63 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09478424132093366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09478424132093366 | validation: 0.15977419605214116]
	TIME [epoch: 3.64 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1220381502390617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1220381502390617 | validation: 0.25882194229202743]
	TIME [epoch: 3.62 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22539162163369253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22539162163369253 | validation: 0.1721049244415395]
	TIME [epoch: 3.62 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16991883870439842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16991883870439842 | validation: 0.21835031585200546]
	TIME [epoch: 3.61 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12575687996473586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12575687996473586 | validation: 0.17361601962650153]
	TIME [epoch: 3.63 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10578728789127155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10578728789127155 | validation: 0.14559919129906757]
	TIME [epoch: 3.62 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0696725811677164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0696725811677164 | validation: 0.12649364769355328]
	TIME [epoch: 3.63 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04913382159685776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04913382159685776 | validation: 0.11584435636442694]
	TIME [epoch: 3.61 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05750799383753618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05750799383753618 | validation: 0.16636217753158664]
	TIME [epoch: 3.62 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07972990278155798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07972990278155798 | validation: 0.1903675589852184]
	TIME [epoch: 3.62 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10933831016369014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10933831016369014 | validation: 0.21239009105526707]
	TIME [epoch: 3.62 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09368464728242026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09368464728242026 | validation: 0.16159530799055558]
	TIME [epoch: 3.62 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0815323232330485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0815323232330485 | validation: 0.15944710634065673]
	TIME [epoch: 3.63 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13983846011023535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13983846011023535 | validation: 0.18184015103240997]
	TIME [epoch: 3.63 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2096354754213616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2096354754213616 | validation: 0.2038144577475367]
	TIME [epoch: 3.62 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13136273423136874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13136273423136874 | validation: 0.16807617435927444]
	TIME [epoch: 3.62 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10407941030853868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10407941030853868 | validation: 0.1846145272603738]
	TIME [epoch: 3.62 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08165512840145368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08165512840145368 | validation: 0.10373090826968961]
	TIME [epoch: 3.65 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_801.pth
	Model improved!!!
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0646933232200953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0646933232200953 | validation: 0.14429098898547252]
	TIME [epoch: 3.63 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06845953886253883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06845953886253883 | validation: 0.12427752025229617]
	TIME [epoch: 3.64 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10387469280878955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10387469280878955 | validation: 0.18509435275498165]
	TIME [epoch: 3.64 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1429263454459645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1429263454459645 | validation: 0.15886948677933554]
	TIME [epoch: 3.65 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13627986547890864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13627986547890864 | validation: 0.15969158419831356]
	TIME [epoch: 3.64 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09529395137729704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09529395137729704 | validation: 0.14247495194668222]
	TIME [epoch: 3.65 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07196553554706082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07196553554706082 | validation: 0.14447487489192704]
	TIME [epoch: 3.64 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08214269748103516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08214269748103516 | validation: 0.15546753668122834]
	TIME [epoch: 3.65 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10623943989001684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10623943989001684 | validation: 0.14992329060429088]
	TIME [epoch: 3.63 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12753224664236887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12753224664236887 | validation: 0.12816192482422215]
	TIME [epoch: 3.63 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09719965530476898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09719965530476898 | validation: 0.12771247310964726]
	TIME [epoch: 3.64 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07970979731452707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07970979731452707 | validation: 0.2029888357735807]
	TIME [epoch: 3.64 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1096390557888872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1096390557888872 | validation: 0.23822834564169898]
	TIME [epoch: 3.64 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.142900932829833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.142900932829833 | validation: 0.16412290939576543]
	TIME [epoch: 3.64 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09960100305961031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09960100305961031 | validation: 0.1427704852837491]
	TIME [epoch: 3.65 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09872132190535059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09872132190535059 | validation: 0.14222305850396597]
	TIME [epoch: 3.63 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09519251948965712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09519251948965712 | validation: 0.12954743551254244]
	TIME [epoch: 3.65 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06867500828483897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06867500828483897 | validation: 0.12387078669143332]
	TIME [epoch: 3.64 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06602364758750817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06602364758750817 | validation: 0.10669527454449675]
	TIME [epoch: 3.65 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08175948462527616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08175948462527616 | validation: 0.16983315457873505]
	TIME [epoch: 3.63 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12476804428870741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12476804428870741 | validation: 0.14604934914357692]
	TIME [epoch: 3.65 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15705426914095108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15705426914095108 | validation: 0.2785632704523432]
	TIME [epoch: 3.62 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14882132755885094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14882132755885094 | validation: 0.22050659114802573]
	TIME [epoch: 3.63 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1270780951734793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1270780951734793 | validation: 0.13643260225443674]
	TIME [epoch: 3.64 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06366835650528056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06366835650528056 | validation: 0.15008340659733732]
	TIME [epoch: 3.64 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08974061774667484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08974061774667484 | validation: 0.16469710104245966]
	TIME [epoch: 3.64 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1298995436043335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1298995436043335 | validation: 0.14321089307951956]
	TIME [epoch: 3.64 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09365451099834665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09365451099834665 | validation: 0.11477407423091608]
	TIME [epoch: 3.64 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05556022259489357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05556022259489357 | validation: 0.09317881594815668]
	TIME [epoch: 3.64 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_830.pth
	Model improved!!!
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051768829264427885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051768829264427885 | validation: 0.152129344091247]
	TIME [epoch: 3.63 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06261170561286855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06261170561286855 | validation: 0.13201052047368697]
	TIME [epoch: 3.62 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09166668406746797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09166668406746797 | validation: 0.1907662883851169]
	TIME [epoch: 3.62 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09738573412958564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09738573412958564 | validation: 0.13804464726429874]
	TIME [epoch: 3.63 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10001095804460669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10001095804460669 | validation: 0.11377388638861091]
	TIME [epoch: 3.64 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13807671589406936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13807671589406936 | validation: 0.18683610376001147]
	TIME [epoch: 3.63 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15640625137836042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15640625137836042 | validation: 0.18077998485997004]
	TIME [epoch: 3.63 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16360357760489372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16360357760489372 | validation: 0.263685180947245]
	TIME [epoch: 3.63 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15280425415376042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15280425415376042 | validation: 0.1596560127112825]
	TIME [epoch: 3.63 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08114342760667445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08114342760667445 | validation: 0.12678438289979282]
	TIME [epoch: 3.63 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05705129198621951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05705129198621951 | validation: 0.13533467523154102]
	TIME [epoch: 3.63 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049889280452474295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049889280452474295 | validation: 0.11008846374356228]
	TIME [epoch: 3.63 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04549343231553953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04549343231553953 | validation: 0.11071316740887367]
	TIME [epoch: 3.63 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05448002634214593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05448002634214593 | validation: 0.1727411791870772]
	TIME [epoch: 3.63 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09709142831356946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09709142831356946 | validation: 0.17108988089784619]
	TIME [epoch: 3.63 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1538422633134178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1538422633134178 | validation: 0.16858234245949022]
	TIME [epoch: 3.64 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10700919766546296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10700919766546296 | validation: 0.163632187626241]
	TIME [epoch: 3.64 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09563531687597006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09563531687597006 | validation: 0.14300011266897525]
	TIME [epoch: 3.64 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.111390585844929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.111390585844929 | validation: 0.15703293768854412]
	TIME [epoch: 3.64 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13001818039598423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13001818039598423 | validation: 0.14438986254330435]
	TIME [epoch: 3.64 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1396031359777484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1396031359777484 | validation: 0.16152027173200448]
	TIME [epoch: 3.64 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10588439149401892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10588439149401892 | validation: 0.1487531795972529]
	TIME [epoch: 3.64 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07567782730692092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07567782730692092 | validation: 0.23374949636427284]
	TIME [epoch: 3.64 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1259885213420548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1259885213420548 | validation: 0.18878227874388864]
	TIME [epoch: 3.64 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09894750265991718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09894750265991718 | validation: 0.12833356093070708]
	TIME [epoch: 3.64 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04599095018857277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04599095018857277 | validation: 0.10715790565949856]
	TIME [epoch: 3.64 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04980473508873245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04980473508873245 | validation: 0.11391478072610027]
	TIME [epoch: 3.62 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054872829713994306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054872829713994306 | validation: 0.10510771629957788]
	TIME [epoch: 3.64 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0736698165334096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0736698165334096 | validation: 0.1326913445286298]
	TIME [epoch: 3.65 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11235930609447593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11235930609447593 | validation: 0.15251098498494553]
	TIME [epoch: 3.65 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14402539358944993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14402539358944993 | validation: 0.23188580504189613]
	TIME [epoch: 3.64 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13892774902738683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13892774902738683 | validation: 0.2035982172870957]
	TIME [epoch: 3.64 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11739451746405029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11739451746405029 | validation: 0.12980223831388113]
	TIME [epoch: 3.65 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11232545421749848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11232545421749848 | validation: 0.18211119346151589]
	TIME [epoch: 3.64 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1286062068392446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1286062068392446 | validation: 0.16192509794974108]
	TIME [epoch: 3.61 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11711429452147734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11711429452147734 | validation: 0.1390689007117567]
	TIME [epoch: 3.62 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07591680997307541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07591680997307541 | validation: 0.11185366155178147]
	TIME [epoch: 3.61 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049135745711344055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049135745711344055 | validation: 0.09687275329329237]
	TIME [epoch: 3.62 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04313848530858708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04313848530858708 | validation: 0.1169654038907201]
	TIME [epoch: 3.61 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04524600968696335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04524600968696335 | validation: 0.12640814063199643]
	TIME [epoch: 3.62 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06495583130801971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06495583130801971 | validation: 0.15251399777909716]
	TIME [epoch: 3.62 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12240441286894747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12240441286894747 | validation: 0.17904906273763352]
	TIME [epoch: 3.64 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13802401620947197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13802401620947197 | validation: 0.1206260414284307]
	TIME [epoch: 3.63 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0969236054500681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0969236054500681 | validation: 0.11553597112187806]
	TIME [epoch: 3.62 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05585173927130411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05585173927130411 | validation: 0.11217663392989422]
	TIME [epoch: 3.63 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0518135273504365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0518135273504365 | validation: 0.1399854850055555]
	TIME [epoch: 3.63 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0897967832417509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0897967832417509 | validation: 0.2575549321730506]
	TIME [epoch: 3.64 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17894371421336616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17894371421336616 | validation: 0.24409415790561495]
	TIME [epoch: 3.64 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19103261484914705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19103261484914705 | validation: 0.1303273606176132]
	TIME [epoch: 3.63 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1071842302405639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1071842302405639 | validation: 0.13624522229090263]
	TIME [epoch: 3.63 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05924827083811898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05924827083811898 | validation: 0.09372507290974946]
	TIME [epoch: 3.63 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04736748837708668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04736748837708668 | validation: 0.1232917346242679]
	TIME [epoch: 3.63 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04197803523281772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04197803523281772 | validation: 0.09465202121736271]
	TIME [epoch: 3.64 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05145794308488291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05145794308488291 | validation: 0.13338763673534665]
	TIME [epoch: 3.64 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05886179756872156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05886179756872156 | validation: 0.12196954099343449]
	TIME [epoch: 3.65 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10780844315277203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10780844315277203 | validation: 0.32970976322348905]
	TIME [epoch: 3.65 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21985578326323335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21985578326323335 | validation: 0.25914215610195085]
	TIME [epoch: 3.64 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18818397646970275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18818397646970275 | validation: 0.1433322796451829]
	TIME [epoch: 3.63 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0961773899914876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0961773899914876 | validation: 0.1665143374956838]
	TIME [epoch: 3.65 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12833428563570828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12833428563570828 | validation: 0.13686844391383218]
	TIME [epoch: 3.63 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10241322095531138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10241322095531138 | validation: 0.09445541915074981]
	TIME [epoch: 3.64 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06117417076388252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06117417076388252 | validation: 0.09528986525497833]
	TIME [epoch: 3.64 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04723023337461183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04723023337461183 | validation: 0.12130952299376144]
	TIME [epoch: 3.63 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050741129769205584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050741129769205584 | validation: 0.1194049415899483]
	TIME [epoch: 3.63 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06257167355862799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06257167355862799 | validation: 0.15407452414186062]
	TIME [epoch: 3.63 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0784960548656148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0784960548656148 | validation: 0.18126379920593513]
	TIME [epoch: 3.65 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10984998024883258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10984998024883258 | validation: 0.14864950756359163]
	TIME [epoch: 3.64 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13753827649204856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13753827649204856 | validation: 0.1485621886358821]
	TIME [epoch: 3.65 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09205584075610006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09205584075610006 | validation: 0.09401743588834975]
	TIME [epoch: 3.64 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06983358704019098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06983358704019098 | validation: 0.12638606437955713]
	TIME [epoch: 3.64 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05572113862648237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05572113862648237 | validation: 0.07785454716456408]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_901.pth
	Model improved!!!
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06207975912755339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06207975912755339 | validation: 0.12960102590793962]
	TIME [epoch: 3.64 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05854930651942457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05854930651942457 | validation: 0.1013648409080473]
	TIME [epoch: 3.63 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07386126357135242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07386126357135242 | validation: 0.1515363283576642]
	TIME [epoch: 3.63 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1113245576495921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1113245576495921 | validation: 0.23929647104717833]
	TIME [epoch: 3.64 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1714612318235407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1714612318235407 | validation: 0.2249745314821685]
	TIME [epoch: 3.62 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12959556685821488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12959556685821488 | validation: 0.11933349582009814]
	TIME [epoch: 3.63 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07125091305950801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07125091305950801 | validation: 0.13493105568901057]
	TIME [epoch: 3.63 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08906258468925617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08906258468925617 | validation: 0.1572119480631936]
	TIME [epoch: 3.63 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11616312195540857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11616312195540857 | validation: 0.1602063448636919]
	TIME [epoch: 3.63 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13128191202805312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13128191202805312 | validation: 0.19597496016852495]
	TIME [epoch: 3.64 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11150562240383113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11150562240383113 | validation: 0.2741479633910827]
	TIME [epoch: 3.64 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14487398153916237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14487398153916237 | validation: 0.144030286752751]
	TIME [epoch: 3.63 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08407598552811774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08407598552811774 | validation: 0.1440383630212776]
	TIME [epoch: 3.63 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06988567069519347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06988567069519347 | validation: 0.12434343645421607]
	TIME [epoch: 3.64 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07769269805614483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07769269805614483 | validation: 0.10204830974384685]
	TIME [epoch: 3.62 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05971521199745412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05971521199745412 | validation: 0.12188089420113234]
	TIME [epoch: 3.63 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055208990467351504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055208990467351504 | validation: 0.09349450777001712]
	TIME [epoch: 3.62 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05578726230289568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05578726230289568 | validation: 0.14057641545871338]
	TIME [epoch: 3.63 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07078053646046539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07078053646046539 | validation: 0.18227117743113247]
	TIME [epoch: 3.64 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10170909999954063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10170909999954063 | validation: 0.1534619816673436]
	TIME [epoch: 3.64 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11664104681439572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11664104681439572 | validation: 0.16408314063593654]
	TIME [epoch: 3.64 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12202295066788843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12202295066788843 | validation: 0.1130539999586556]
	TIME [epoch: 3.64 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10803961425704742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10803961425704742 | validation: 0.1362555430568048]
	TIME [epoch: 3.65 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08019551998539455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08019551998539455 | validation: 0.12730930718641723]
	TIME [epoch: 3.62 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07234034794083469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07234034794083469 | validation: 0.10772749710355814]
	TIME [epoch: 3.62 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06816174621673864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06816174621673864 | validation: 0.1591475102937687]
	TIME [epoch: 3.64 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0949268670650576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0949268670650576 | validation: 0.11066058106274333]
	TIME [epoch: 3.64 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10187237417952422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10187237417952422 | validation: 0.11286479008508006]
	TIME [epoch: 3.63 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057826503041412514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057826503041412514 | validation: 0.1518666592575814]
	TIME [epoch: 3.62 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07297915612369599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07297915612369599 | validation: 0.16290684544380768]
	TIME [epoch: 3.64 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13267003716756887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13267003716756887 | validation: 0.25127866729266296]
	TIME [epoch: 3.63 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14774330851804302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14774330851804302 | validation: 0.13964211923575595]
	TIME [epoch: 3.62 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09753018217141847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09753018217141847 | validation: 0.14495456639063806]
	TIME [epoch: 3.63 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07906133851987264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07906133851987264 | validation: 0.1215817970039716]
	TIME [epoch: 3.63 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06243847910947113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06243847910947113 | validation: 0.11331506269188081]
	TIME [epoch: 3.63 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05980781440329794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05980781440329794 | validation: 0.11636081160159072]
	TIME [epoch: 3.65 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057386328027115935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057386328027115935 | validation: 0.11988321684032771]
	TIME [epoch: 3.61 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05250126906108072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05250126906108072 | validation: 0.10700507138147253]
	TIME [epoch: 3.63 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059030460020654235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059030460020654235 | validation: 0.12779859272811098]
	TIME [epoch: 3.62 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06262341896080245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06262341896080245 | validation: 0.09437016654802562]
	TIME [epoch: 3.62 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08184145392538023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08184145392538023 | validation: 0.1181371313379728]
	TIME [epoch: 3.63 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08143392394874412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08143392394874412 | validation: 0.0941363809988536]
	TIME [epoch: 3.63 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09525684766403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09525684766403 | validation: 0.15783885638473671]
	TIME [epoch: 3.63 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10317961428746594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10317961428746594 | validation: 0.20632619608864466]
	TIME [epoch: 3.63 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14075002696195182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14075002696195182 | validation: 0.24485966110286117]
	TIME [epoch: 3.63 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15064066190674455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15064066190674455 | validation: 0.18367883679469965]
	TIME [epoch: 3.64 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12137593948565943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12137593948565943 | validation: 0.1534911714304671]
	TIME [epoch: 3.65 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14330587789858773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14330587789858773 | validation: 0.13054788790304503]
	TIME [epoch: 3.65 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08337676482696497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08337676482696497 | validation: 0.11256943625461231]
	TIME [epoch: 3.65 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05345129704937462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05345129704937462 | validation: 0.0756037670025868]
	TIME [epoch: 3.64 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_951.pth
	Model improved!!!
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04986864748144647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04986864748144647 | validation: 0.15019554092522447]
	TIME [epoch: 3.64 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07424704263602287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07424704263602287 | validation: 0.12228866731438606]
	TIME [epoch: 3.64 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09743054376087514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09743054376087514 | validation: 0.14222728125738068]
	TIME [epoch: 3.65 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08995434145283021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08995434145283021 | validation: 0.13050190718540217]
	TIME [epoch: 3.65 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06749481364651058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06749481364651058 | validation: 0.0962664209525823]
	TIME [epoch: 3.64 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05797243588333547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05797243588333547 | validation: 0.10990226765721284]
	TIME [epoch: 3.64 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05019828721572018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05019828721572018 | validation: 0.09243976995606651]
	TIME [epoch: 3.64 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051923155459333514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051923155459333514 | validation: 0.1248127576039954]
	TIME [epoch: 3.64 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07278359705984334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07278359705984334 | validation: 0.1613853442784454]
	TIME [epoch: 3.63 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12021169138485027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12021169138485027 | validation: 0.1502691327361604]
	TIME [epoch: 3.64 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09870873900008494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09870873900008494 | validation: 0.08922617543682222]
	TIME [epoch: 3.65 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06097622601238449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06097622601238449 | validation: 0.12137956680113118]
	TIME [epoch: 3.65 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07058906453687668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07058906453687668 | validation: 0.12279924956696919]
	TIME [epoch: 3.64 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09773867633837649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09773867633837649 | validation: 0.1914492755458359]
	TIME [epoch: 3.64 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12785795420098625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12785795420098625 | validation: 0.14394104459040244]
	TIME [epoch: 3.63 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08733701515462199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08733701515462199 | validation: 0.08891809023065277]
	TIME [epoch: 3.64 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04662075628721353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04662075628721353 | validation: 0.09712811095395955]
	TIME [epoch: 3.64 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041526519103943074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041526519103943074 | validation: 0.06084633463644659]
	TIME [epoch: 3.64 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_969.pth
	Model improved!!!
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03793343377400476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03793343377400476 | validation: 0.1121428374972457]
	TIME [epoch: 3.63 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04890560935754604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04890560935754604 | validation: 0.1487962254654073]
	TIME [epoch: 3.64 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11562720340575748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11562720340575748 | validation: 0.41923144326508366]
	TIME [epoch: 3.63 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.277428073344599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.277428073344599 | validation: 0.16084236850882433]
	TIME [epoch: 3.63 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1479532810540663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1479532810540663 | validation: 0.13587483746048182]
	TIME [epoch: 3.64 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07606525749729648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07606525749729648 | validation: 0.15433873929501132]
	TIME [epoch: 3.64 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061044942450675155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061044942450675155 | validation: 0.08424527591000044]
	TIME [epoch: 3.65 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04942983239115344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04942983239115344 | validation: 0.11821576688809379]
	TIME [epoch: 3.63 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05275748953341432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05275748953341432 | validation: 0.12005324930309808]
	TIME [epoch: 3.63 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07033167270043214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07033167270043214 | validation: 0.13466738610868625]
	TIME [epoch: 3.63 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08958322798249677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08958322798249677 | validation: 0.18833190929019994]
	TIME [epoch: 3.63 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12362739789457536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12362739789457536 | validation: 0.09973631954467456]
	TIME [epoch: 3.63 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11980524977627764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11980524977627764 | validation: 0.10991256441470312]
	TIME [epoch: 3.64 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04115392451649357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04115392451649357 | validation: 0.09451811038993059]
	TIME [epoch: 3.63 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03156636053210998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03156636053210998 | validation: 0.07880466575717589]
	TIME [epoch: 3.64 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03815134656380905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03815134656380905 | validation: 0.11125709267746364]
	TIME [epoch: 3.63 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054504319825772446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054504319825772446 | validation: 0.257718040179089]
	TIME [epoch: 3.63 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14114089757602785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14114089757602785 | validation: 0.2983830436205036]
	TIME [epoch: 3.64 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20450025039545913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20450025039545913 | validation: 0.14264413349800315]
	TIME [epoch: 3.66 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15500609076792735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15500609076792735 | validation: 0.18203811184717142]
	TIME [epoch: 3.63 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11493914730130048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11493914730130048 | validation: 0.10784571538657542]
	TIME [epoch: 3.64 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052295015538685076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052295015538685076 | validation: 0.08346797974031048]
	TIME [epoch: 3.63 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034651197790318874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034651197790318874 | validation: 0.10354178017960414]
	TIME [epoch: 3.63 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04173871493312195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04173871493312195 | validation: 0.09491212952405215]
	TIME [epoch: 3.63 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050363832597082714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050363832597082714 | validation: 0.12399033841132999]
	TIME [epoch: 3.63 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06793149859413007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06793149859413007 | validation: 0.12433873585409316]
	TIME [epoch: 3.64 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08702282747244197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08702282747244197 | validation: 0.13602858521662334]
	TIME [epoch: 3.62 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07949746581799852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07949746581799852 | validation: 0.0926698106745013]
	TIME [epoch: 3.62 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0789619144101381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0789619144101381 | validation: 0.15931423365226782]
	TIME [epoch: 3.61 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10811316745856146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10811316745856146 | validation: 0.13617196838911144]
	TIME [epoch: 3.62 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13059161409083123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13059161409083123 | validation: 0.10751748698650487]
	TIME [epoch: 3.63 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08788071034191752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08788071034191752 | validation: 0.09350312117048708]
	TIME [epoch: 47.7 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04426753561267065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04426753561267065 | validation: 0.06572041015678516]
	TIME [epoch: 7.86 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03669515001508344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03669515001508344 | validation: 0.11729436400857751]
	TIME [epoch: 7.85 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05415767925478582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05415767925478582 | validation: 0.1957871345762732]
	TIME [epoch: 7.86 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11219248729982179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11219248729982179 | validation: 0.22216729281718728]
	TIME [epoch: 7.84 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12695124938772184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12695124938772184 | validation: 0.11507068922935289]
	TIME [epoch: 7.85 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07478669765349884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07478669765349884 | validation: 0.12296290834261206]
	TIME [epoch: 7.86 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07723434150662697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07723434150662697 | validation: 0.12427607488739519]
	TIME [epoch: 7.86 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07350826023569505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07350826023569505 | validation: 0.1013610599328213]
	TIME [epoch: 7.86 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05941670221580837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05941670221580837 | validation: 0.08766663453616953]
	TIME [epoch: 7.88 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058315078541538216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058315078541538216 | validation: 0.12840372876623227]
	TIME [epoch: 7.86 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07086823659819022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07086823659819022 | validation: 0.1559173192000348]
	TIME [epoch: 7.86 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11136912537016905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11136912537016905 | validation: 0.2325595744675436]
	TIME [epoch: 7.86 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12345645178975001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12345645178975001 | validation: 0.11042216960428841]
	TIME [epoch: 7.86 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059794332409542746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059794332409542746 | validation: 0.08303349616611286]
	TIME [epoch: 7.87 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03676977354514273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03676977354514273 | validation: 0.10619086820659907]
	TIME [epoch: 7.88 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04177391761514035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04177391761514035 | validation: 0.07582594960507966]
	TIME [epoch: 7.87 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06353352171899485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06353352171899485 | validation: 0.1716032451734743]
	TIME [epoch: 7.86 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10193092049695474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10193092049695474 | validation: 0.11615865223870657]
	TIME [epoch: 7.86 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11548728914633685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11548728914633685 | validation: 0.10652712051797493]
	TIME [epoch: 7.86 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07671179679384184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07671179679384184 | validation: 0.13276536459263363]
	TIME [epoch: 7.87 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.094450395738827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.094450395738827 | validation: 0.14821281147450405]
	TIME [epoch: 7.87 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10924530728320563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10924530728320563 | validation: 0.1959961875597035]
	TIME [epoch: 7.88 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12738943299540842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12738943299540842 | validation: 0.1392893826442558]
	TIME [epoch: 7.86 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08851822602119973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08851822602119973 | validation: 0.1308574220065127]
	TIME [epoch: 7.87 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07472879212099232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07472879212099232 | validation: 0.1328321991662902]
	TIME [epoch: 7.85 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08271994054437885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08271994054437885 | validation: 0.13949096744951986]
	TIME [epoch: 7.83 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08094046271464973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08094046271464973 | validation: 0.09394439297407355]
	TIME [epoch: 7.86 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08123875503818147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08123875503818147 | validation: 0.15332038836753561]
	TIME [epoch: 7.88 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07677463966279353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07677463966279353 | validation: 0.09681066852352584]
	TIME [epoch: 7.85 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06262421733389893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06262421733389893 | validation: 0.1055355459434795]
	TIME [epoch: 7.88 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05962942523297683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05962942523297683 | validation: 0.10121909438757025]
	TIME [epoch: 7.83 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06177967402048549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06177967402048549 | validation: 0.06597102814175769]
	TIME [epoch: 7.86 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05970172022095168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05970172022095168 | validation: 0.15209443482380888]
	TIME [epoch: 7.85 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07555589863195508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07555589863195508 | validation: 0.2419161432451405]
	TIME [epoch: 7.87 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13965793170382423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13965793170382423 | validation: 0.18358917701982574]
	TIME [epoch: 7.87 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10032890938978926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10032890938978926 | validation: 0.10164862921017398]
	TIME [epoch: 7.87 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06306106162104544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06306106162104544 | validation: 0.12017107841321494]
	TIME [epoch: 7.86 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06854966997276417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06854966997276417 | validation: 0.13688482488487752]
	TIME [epoch: 7.86 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06695433026455305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06695433026455305 | validation: 0.09101995678619013]
	TIME [epoch: 7.86 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0630104413530757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0630104413530757 | validation: 0.12780110407399348]
	TIME [epoch: 7.87 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05529150818502782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05529150818502782 | validation: 0.09607257237050305]
	TIME [epoch: 7.87 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04751777872833568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04751777872833568 | validation: 0.08396276897912751]
	TIME [epoch: 7.86 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0520382444728476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0520382444728476 | validation: 0.1396604890695119]
	TIME [epoch: 7.85 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0805110856062736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0805110856062736 | validation: 0.15217811743283838]
	TIME [epoch: 7.87 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13095952618263812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13095952618263812 | validation: 0.21251602547526174]
	TIME [epoch: 7.86 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10921783783454096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10921783783454096 | validation: 0.10207326915063804]
	TIME [epoch: 7.86 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04763366838127611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04763366838127611 | validation: 0.07220624856201352]
	TIME [epoch: 7.88 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040333882658960825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040333882658960825 | validation: 0.11941756166634665]
	TIME [epoch: 7.86 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07809450318309483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07809450318309483 | validation: 0.1463522247788446]
	TIME [epoch: 7.83 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14640093556777017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14640093556777017 | validation: 0.17804038689447485]
	TIME [epoch: 7.86 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1206001271766622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1206001271766622 | validation: 0.17296866836474512]
	TIME [epoch: 7.85 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10002703732457613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10002703732457613 | validation: 0.097085468686628]
	TIME [epoch: 7.86 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.094238103234282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.094238103234282 | validation: 0.1009082081203169]
	TIME [epoch: 7.86 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029943201484468905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029943201484468905 | validation: 0.105627743911099]
	TIME [epoch: 7.9 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0338007721716398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0338007721716398 | validation: 0.059705972455662795]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_1056.pth
	Model improved!!!
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03701346461174933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03701346461174933 | validation: 0.12611041672456946]
	TIME [epoch: 7.87 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04663624235660554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04663624235660554 | validation: 0.1568752558843837]
	TIME [epoch: 7.86 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09588571441523591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09588571441523591 | validation: 0.1738061385882126]
	TIME [epoch: 7.87 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12889891191270894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12889891191270894 | validation: 0.11026878073237972]
	TIME [epoch: 7.86 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10391645438699827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10391645438699827 | validation: 0.14772626820009904]
	TIME [epoch: 7.88 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08549117969657478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08549117969657478 | validation: 0.10532905571979333]
	TIME [epoch: 7.86 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06784020868057473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06784020868057473 | validation: 0.07499813180459408]
	TIME [epoch: 7.87 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05797858385561026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05797858385561026 | validation: 0.12401716017944918]
	TIME [epoch: 7.86 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06767697642383178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06767697642383178 | validation: 0.09331250422492147]
	TIME [epoch: 7.87 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08621193677325885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08621193677325885 | validation: 0.14605661948748558]
	TIME [epoch: 7.86 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08806383939330799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08806383939330799 | validation: 0.16311216932765438]
	TIME [epoch: 7.88 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09116196033211893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09116196033211893 | validation: 0.13256901405034613]
	TIME [epoch: 7.85 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07163329327998808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07163329327998808 | validation: 0.09733675948025577]
	TIME [epoch: 7.87 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038240042836052604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038240042836052604 | validation: 0.0937301684520568]
	TIME [epoch: 7.86 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05407671358944282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05407671358944282 | validation: 0.10662760839377816]
	TIME [epoch: 7.85 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08549538815670231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08549538815670231 | validation: 0.1245698960756386]
	TIME [epoch: 7.85 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10506625473261803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10506625473261803 | validation: 0.08865621636937243]
	TIME [epoch: 7.87 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07545368462684206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07545368462684206 | validation: 0.11442687868617324]
	TIME [epoch: 7.86 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04669210343425228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04669210343425228 | validation: 0.14631080539555705]
	TIME [epoch: 7.87 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06738726769419401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06738726769419401 | validation: 0.1363702906631182]
	TIME [epoch: 7.85 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06716943752565221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06716943752565221 | validation: 0.12322075259696237]
	TIME [epoch: 7.88 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044628892582298636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044628892582298636 | validation: 0.07008493921897258]
	TIME [epoch: 7.86 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046800324113547696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046800324113547696 | validation: 0.12272315233035291]
	TIME [epoch: 7.87 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06183593961994365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06183593961994365 | validation: 0.06786985048803834]
	TIME [epoch: 7.88 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07209931061967902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07209931061967902 | validation: 0.1296933169441011]
	TIME [epoch: 7.86 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07811972332463035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07811972332463035 | validation: 0.12243032151594016]
	TIME [epoch: 7.85 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0949996366102223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0949996366102223 | validation: 0.12100338020993413]
	TIME [epoch: 7.85 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10162281649250655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10162281649250655 | validation: 0.1345044182289222]
	TIME [epoch: 7.85 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06328238856504126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06328238856504126 | validation: 0.09927176305392325]
	TIME [epoch: 7.85 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047527346565751236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047527346565751236 | validation: 0.13072477038494287]
	TIME [epoch: 7.86 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05456606873269263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05456606873269263 | validation: 0.17362432054757032]
	TIME [epoch: 7.86 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09338446311213769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09338446311213769 | validation: 0.17564075981675]
	TIME [epoch: 7.85 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10713957434609789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10713957434609789 | validation: 0.09706763831208612]
	TIME [epoch: 7.85 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07889804335421925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07889804335421925 | validation: 0.13258890637381152]
	TIME [epoch: 7.85 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06640152434671405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06640152434671405 | validation: 0.10402362943034751]
	TIME [epoch: 7.86 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06469258194257932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06469258194257932 | validation: 0.11844806323322131]
	TIME [epoch: 7.86 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053548320264239904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053548320264239904 | validation: 0.09093046474351965]
	TIME [epoch: 7.88 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07653035644330909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07653035644330909 | validation: 0.13224153411799833]
	TIME [epoch: 7.86 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12932114499592895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12932114499592895 | validation: 0.14308716617886008]
	TIME [epoch: 7.86 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11587623584669444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11587623584669444 | validation: 0.09731078734115407]
	TIME [epoch: 7.85 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061282911861656165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061282911861656165 | validation: 0.09427089073334233]
	TIME [epoch: 7.85 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03413138982201701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03413138982201701 | validation: 0.05804869907377322]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_1098.pth
	Model improved!!!
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034939583753637087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034939583753637087 | validation: 0.1016108049916652]
	TIME [epoch: 7.88 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04187632075832919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04187632075832919 | validation: 0.1209083202482753]
	TIME [epoch: 7.83 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0543600976248293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0543600976248293 | validation: 0.11410406946765504]
	TIME [epoch: 7.83 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06143268169992334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06143268169992334 | validation: 0.11135638913753008]
	TIME [epoch: 7.84 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057405717621637146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057405717621637146 | validation: 0.09808433564521118]
	TIME [epoch: 7.84 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0705970627763487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0705970627763487 | validation: 0.17566521336739718]
	TIME [epoch: 7.85 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07840521966134191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07840521966134191 | validation: 0.0844758764526812]
	TIME [epoch: 7.85 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05618573634720517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05618573634720517 | validation: 0.09399860872695308]
	TIME [epoch: 7.86 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04887903650354425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04887903650354425 | validation: 0.09761026657675966]
	TIME [epoch: 7.85 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0949299753506852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0949299753506852 | validation: 0.12575237624269914]
	TIME [epoch: 7.87 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13869634169827508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13869634169827508 | validation: 0.16859955018351458]
	TIME [epoch: 7.86 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11628400470184945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11628400470184945 | validation: 0.20251490717008913]
	TIME [epoch: 7.86 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11817495700719313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11817495700719313 | validation: 0.14045580884029646]
	TIME [epoch: 7.87 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06288150474880623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06288150474880623 | validation: 0.10022213203975006]
	TIME [epoch: 7.89 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0398338182551631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0398338182551631 | validation: 0.08630840477677522]
	TIME [epoch: 7.87 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03671833362548378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03671833362548378 | validation: 0.10366980977868252]
	TIME [epoch: 7.87 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04040194507636912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04040194507636912 | validation: 0.12890470086599262]
	TIME [epoch: 7.85 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05391455716977071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05391455716977071 | validation: 0.10516446123338274]
	TIME [epoch: 7.85 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05780623544777079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05780623544777079 | validation: 0.10308191023452173]
	TIME [epoch: 7.85 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05189713685744986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05189713685744986 | validation: 0.07574913741919026]
	TIME [epoch: 7.89 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0611902817423396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0611902817423396 | validation: 0.13415117466214163]
	TIME [epoch: 7.85 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07825999860649742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07825999860649742 | validation: 0.14466685715752142]
	TIME [epoch: 7.86 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11109909746609013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11109909746609013 | validation: 0.1679914483033629]
	TIME [epoch: 7.88 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11852461243561521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11852461243561521 | validation: 0.2009858250408735]
	TIME [epoch: 7.87 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11987464257243112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11987464257243112 | validation: 0.12021971331185909]
	TIME [epoch: 7.87 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08734117844626338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08734117844626338 | validation: 0.11707161019792314]
	TIME [epoch: 7.9 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05486786754233715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05486786754233715 | validation: 0.10961260812906254]
	TIME [epoch: 7.88 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05289087796542007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05289087796542007 | validation: 0.07979081350367179]
	TIME [epoch: 7.88 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041730946375088854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041730946375088854 | validation: 0.09100532014374263]
	TIME [epoch: 7.86 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03521819260992388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03521819260992388 | validation: 0.05615343720715468]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_1128.pth
	Model improved!!!
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03284680204275621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03284680204275621 | validation: 0.11157620195864036]
	TIME [epoch: 7.85 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030547390554850784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030547390554850784 | validation: 0.07108945379000219]
	TIME [epoch: 7.86 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039951319529536955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039951319529536955 | validation: 0.11286267476637098]
	TIME [epoch: 7.87 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06667056676090503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06667056676090503 | validation: 0.13304751438197446]
	TIME [epoch: 7.85 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1356494759084915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1356494759084915 | validation: 0.19624350930503126]
	TIME [epoch: 7.86 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1301702943206635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1301702943206635 | validation: 0.0932470182881862]
	TIME [epoch: 7.86 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05330091584275486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05330091584275486 | validation: 0.08631484939062534]
	TIME [epoch: 7.85 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03286572318234234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03286572318234234 | validation: 0.08792455058309413]
	TIME [epoch: 7.85 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05245314805425983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05245314805425983 | validation: 0.09236170945933456]
	TIME [epoch: 7.85 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06443430585525571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06443430585525571 | validation: 0.10228728237229001]
	TIME [epoch: 7.86 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08112619907471952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08112619907471952 | validation: 0.16510981652261056]
	TIME [epoch: 7.87 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09803846596805162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09803846596805162 | validation: 0.11987471107633399]
	TIME [epoch: 7.85 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06907642447548325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06907642447548325 | validation: 0.08662538531861952]
	TIME [epoch: 7.86 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03722398424983533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03722398424983533 | validation: 0.0739058033244455]
	TIME [epoch: 7.86 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02867060749403725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02867060749403725 | validation: 0.072258654489628]
	TIME [epoch: 7.85 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037825631156732646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037825631156732646 | validation: 0.0993929097224563]
	TIME [epoch: 7.87 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0800683459643604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0800683459643604 | validation: 0.09679210238976382]
	TIME [epoch: 7.86 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12926097823955432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12926097823955432 | validation: 0.21978344316616394]
	TIME [epoch: 7.87 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1440281706659574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1440281706659574 | validation: 0.20001816087137875]
	TIME [epoch: 7.85 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13742513794311367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13742513794311367 | validation: 0.13576706890538398]
	TIME [epoch: 7.86 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08465718418944886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08465718418944886 | validation: 0.11623809158154819]
	TIME [epoch: 7.86 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06784291827452758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06784291827452758 | validation: 0.07596310293574478]
	TIME [epoch: 7.88 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04533968421031988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04533968421031988 | validation: 0.07716010444196732]
	TIME [epoch: 7.87 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02959503535721359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02959503535721359 | validation: 0.07763568280540677]
	TIME [epoch: 7.87 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022427016184096073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022427016184096073 | validation: 0.07351731701588375]
	TIME [epoch: 7.86 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022820527668558844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022820527668558844 | validation: 0.07865250823557728]
	TIME [epoch: 7.87 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0279588239503879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0279588239503879 | validation: 0.06829314666871567]
	TIME [epoch: 7.85 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04554046275218198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04554046275218198 | validation: 0.11770075850914814]
	TIME [epoch: 7.9 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0874482839229836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0874482839229836 | validation: 0.10185912572343647]
	TIME [epoch: 7.88 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10090644834199168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10090644834199168 | validation: 0.1478799162644305]
	TIME [epoch: 7.87 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0801861531371189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0801861531371189 | validation: 0.16344938492305944]
	TIME [epoch: 7.86 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10325295699261591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10325295699261591 | validation: 0.17757693654714604]
	TIME [epoch: 7.86 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12585900823773716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12585900823773716 | validation: 0.10354512599994843]
	TIME [epoch: 7.86 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05222468497100825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05222468497100825 | validation: 0.0721992168308429]
	TIME [epoch: 7.86 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04086320045507971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04086320045507971 | validation: 0.11754386591432948]
	TIME [epoch: 7.88 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04608505532534335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04608505532534335 | validation: 0.06854969313437881]
	TIME [epoch: 7.86 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03567192650489395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03567192650489395 | validation: 0.09745840644561499]
	TIME [epoch: 7.85 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03105012095478365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03105012095478365 | validation: 0.08508713652075957]
	TIME [epoch: 7.86 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04063507049079213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04063507049079213 | validation: 0.11869241970590966]
	TIME [epoch: 7.86 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06281947530674221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06281947530674221 | validation: 0.13062066126476857]
	TIME [epoch: 7.85 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10794249782101421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10794249782101421 | validation: 0.11196184130501848]
	TIME [epoch: 7.87 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09591038003131074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09591038003131074 | validation: 0.09769603778280597]
	TIME [epoch: 7.86 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051682808640289776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051682808640289776 | validation: 0.0933897089847146]
	TIME [epoch: 7.86 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05736594643355357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05736594643355357 | validation: 0.11539829553235573]
	TIME [epoch: 7.86 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08659559803516141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08659559803516141 | validation: 0.09432446648561595]
	TIME [epoch: 7.85 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08454773169691478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08454773169691478 | validation: 0.11017849549932808]
	TIME [epoch: 7.86 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042880138216255526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042880138216255526 | validation: 0.06198663104515021]
	TIME [epoch: 7.85 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03179884229760575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03179884229760575 | validation: 0.10390852715421578]
	TIME [epoch: 7.88 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04405332007369411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04405332007369411 | validation: 0.10671749409085261]
	TIME [epoch: 7.85 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08536562083444402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08536562083444402 | validation: 0.16109482772212325]
	TIME [epoch: 7.86 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10560116400286322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10560116400286322 | validation: 0.10680193059171864]
	TIME [epoch: 7.84 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08273958972537511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08273958972537511 | validation: 0.10237796146582584]
	TIME [epoch: 7.85 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06824703395949049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06824703395949049 | validation: 0.1278496533547698]
	TIME [epoch: 7.85 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08479120222504047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08479120222504047 | validation: 0.13180368069134377]
	TIME [epoch: 7.88 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07182135603036602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07182135603036602 | validation: 0.10414673230526934]
	TIME [epoch: 7.85 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0529787406527321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0529787406527321 | validation: 0.09776631658553554]
	TIME [epoch: 7.85 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033831415094420794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033831415094420794 | validation: 0.07756385153368925]
	TIME [epoch: 7.85 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022009384333018883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022009384333018883 | validation: 0.08801265394158571]
	TIME [epoch: 7.85 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020979741148238897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020979741148238897 | validation: 0.0671418165457316]
	TIME [epoch: 7.85 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025649260546164853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025649260546164853 | validation: 0.11857423709430902]
	TIME [epoch: 7.86 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04818796964150643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04818796964150643 | validation: 0.15455025048143978]
	TIME [epoch: 7.86 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11225730917096456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11225730917096456 | validation: 0.1520106013179626]
	TIME [epoch: 7.84 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11954973456152088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11954973456152088 | validation: 0.08320059457993938]
	TIME [epoch: 7.84 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06302434743784514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06302434743784514 | validation: 0.08294694615347416]
	TIME [epoch: 7.85 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04820108075772143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04820108075772143 | validation: 0.0799276783383489]
	TIME [epoch: 7.83 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053504440811809564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053504440811809564 | validation: 0.1373686516720593]
	TIME [epoch: 7.86 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07244208308724329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07244208308724329 | validation: 0.12331278410901488]
	TIME [epoch: 7.88 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06628041638915201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06628041638915201 | validation: 0.0748888591031321]
	TIME [epoch: 7.86 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05200036244419116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05200036244419116 | validation: 0.1403177632607016]
	TIME [epoch: 7.86 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07135731056381428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07135731056381428 | validation: 0.08956607623200537]
	TIME [epoch: 7.86 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07200405634649436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07200405634649436 | validation: 0.11352689670940613]
	TIME [epoch: 7.86 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03920550126451312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03920550126451312 | validation: 0.13213766421110795]
	TIME [epoch: 7.87 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07622934979636296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07622934979636296 | validation: 0.14499392784912074]
	TIME [epoch: 7.89 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13395150330062294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13395150330062294 | validation: 0.14113135858901968]
	TIME [epoch: 7.86 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10982418049074244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10982418049074244 | validation: 0.09405591205112596]
	TIME [epoch: 7.87 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05100372844835605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05100372844835605 | validation: 0.07867975130328393]
	TIME [epoch: 7.86 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033105498989878655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033105498989878655 | validation: 0.09141867121064295]
	TIME [epoch: 7.87 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030851648284522613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030851648284522613 | validation: 0.055317380080783164]
	TIME [epoch: 7.87 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_1206.pth
	Model improved!!!
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04113512776916126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04113512776916126 | validation: 0.1551021283331681]
	TIME [epoch: 7.87 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05814812000822388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05814812000822388 | validation: 0.10382996405083485]
	TIME [epoch: 7.86 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05461381389875461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05461381389875461 | validation: 0.08247464766587098]
	TIME [epoch: 7.85 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033637073032713335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033637073032713335 | validation: 0.08303609717572927]
	TIME [epoch: 7.85 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038147041285067566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038147041285067566 | validation: 0.06778045367373715]
	TIME [epoch: 7.85 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06948622890814062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06948622890814062 | validation: 0.13004171798559744]
	TIME [epoch: 7.85 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10463036390430137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10463036390430137 | validation: 0.09287053186508856]
	TIME [epoch: 7.86 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07770152215039529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07770152215039529 | validation: 0.09171125467409658]
	TIME [epoch: 7.87 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052495825941675865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052495825941675865 | validation: 0.1632663884508793]
	TIME [epoch: 7.85 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0969559412762114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0969559412762114 | validation: 0.1812640785885755]
	TIME [epoch: 7.85 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14811507617143907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14811507617143907 | validation: 0.11954592458103451]
	TIME [epoch: 7.85 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07298510494521636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07298510494521636 | validation: 0.09830098364098591]
	TIME [epoch: 7.86 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03188328095961942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03188328095961942 | validation: 0.07863537448700267]
	TIME [epoch: 7.85 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026761625213798636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026761625213798636 | validation: 0.08372698525633371]
	TIME [epoch: 7.87 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035636510720863046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035636510720863046 | validation: 0.1070400795934417]
	TIME [epoch: 7.86 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04380766912087825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04380766912087825 | validation: 0.10690847111244757]
	TIME [epoch: 7.85 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052860088941417145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052860088941417145 | validation: 0.09111245397420442]
	TIME [epoch: 7.86 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0507398330819344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0507398330819344 | validation: 0.0808512337607472]
	TIME [epoch: 7.86 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06672225288491151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06672225288491151 | validation: 0.09606027068787505]
	TIME [epoch: 7.86 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07364749642144004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07364749642144004 | validation: 0.10213626357057658]
	TIME [epoch: 7.87 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07008071665490019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07008071665490019 | validation: 0.07932923101040212]
	TIME [epoch: 7.87 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06899297005655365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06899297005655365 | validation: 0.13524033750060202]
	TIME [epoch: 7.85 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061691997880902474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061691997880902474 | validation: 0.10059423916412191]
	TIME [epoch: 7.85 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05291919252631928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05291919252631928 | validation: 0.10327784616009837]
	TIME [epoch: 7.85 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05046661929754071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05046661929754071 | validation: 0.0711402261940287]
	TIME [epoch: 7.86 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05121754447949334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05121754447949334 | validation: 0.06647004955099597]
	TIME [epoch: 7.86 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05010572786576295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05010572786576295 | validation: 0.1119687266064648]
	TIME [epoch: 7.86 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049490624016842844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049490624016842844 | validation: 0.0610742841520833]
	TIME [epoch: 7.85 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03359645976560215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03359645976560215 | validation: 0.05866177303020653]
	TIME [epoch: 7.85 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0188423478118771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0188423478118771 | validation: 0.08978213503895827]
	TIME [epoch: 7.86 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030014149786183584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030014149786183584 | validation: 0.16180392717594846]
	TIME [epoch: 7.85 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09861018530411443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09861018530411443 | validation: 0.26480238462672845]
	TIME [epoch: 7.86 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20462985261595376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20462985261595376 | validation: 0.13852194548795738]
	TIME [epoch: 7.86 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12399037058234047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12399037058234047 | validation: 0.1166153316521499]
	TIME [epoch: 7.86 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0817942462379533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0817942462379533 | validation: 0.12651954660407613]
	TIME [epoch: 7.85 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05824223075307551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05824223075307551 | validation: 0.08451134885235996]
	TIME [epoch: 7.86 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04202531173975289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04202531173975289 | validation: 0.09509040503219858]
	TIME [epoch: 7.85 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038422176079911594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038422176079911594 | validation: 0.1094392613626226]
	TIME [epoch: 7.86 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05047785668236944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05047785668236944 | validation: 0.08486207087683345]
	TIME [epoch: 7.86 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06936362456535604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06936362456535604 | validation: 0.1282187539306741]
	TIME [epoch: 7.87 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06181405329202555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06181405329202555 | validation: 0.09470850274037808]
	TIME [epoch: 7.85 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036945199964214866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036945199964214866 | validation: 0.062367839630319256]
	TIME [epoch: 7.85 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022645013325134657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022645013325134657 | validation: 0.06845702960566716]
	TIME [epoch: 7.84 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02033467288621038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02033467288621038 | validation: 0.048977459026000474]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_1250.pth
	Model improved!!!
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02172772132499162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02172772132499162 | validation: 0.09574320819543615]
	TIME [epoch: 7.87 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030327533198915024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030327533198915024 | validation: 0.055670516404209204]
	TIME [epoch: 7.88 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05368328498067375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05368328498067375 | validation: 0.10997534513290433]
	TIME [epoch: 7.84 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0912236321118566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0912236321118566 | validation: 0.14501829393681775]
	TIME [epoch: 7.86 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11945036794872724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11945036794872724 | validation: 0.10274015061573266]
	TIME [epoch: 7.84 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06871595738025026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06871595738025026 | validation: 0.14034109683447824]
	TIME [epoch: 7.85 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062345347825060175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062345347825060175 | validation: 0.09217646590024843]
	TIME [epoch: 7.84 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059594200802524123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059594200802524123 | validation: 0.08606234196673447]
	TIME [epoch: 7.86 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03768771998235086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03768771998235086 | validation: 0.11655275036483062]
	TIME [epoch: 7.87 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06902992714224115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06902992714224115 | validation: 0.17662754798698496]
	TIME [epoch: 7.85 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1195932902676322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1195932902676322 | validation: 0.16409108239848572]
	TIME [epoch: 7.84 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11220044028747406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11220044028747406 | validation: 0.10802160993639075]
	TIME [epoch: 7.84 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0703386676815453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0703386676815453 | validation: 0.07867455015378529]
	TIME [epoch: 7.86 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04827628281740849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04827628281740849 | validation: 0.0887760594335679]
	TIME [epoch: 7.86 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04638380375130712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04638380375130712 | validation: 0.058598993987475104]
	TIME [epoch: 7.86 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041739311339159146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041739311339159146 | validation: 0.09263964298234773]
	TIME [epoch: 7.85 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035287182651154166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035287182651154166 | validation: 0.0713825334693912]
	TIME [epoch: 7.84 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030645093116460336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030645093116460336 | validation: 0.09550730590395973]
	TIME [epoch: 7.85 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03588919514175174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03588919514175174 | validation: 0.13458873248116673]
	TIME [epoch: 7.86 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06549379932922078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06549379932922078 | validation: 0.12633686353007212]
	TIME [epoch: 7.85 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06972837560967775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06972837560967775 | validation: 0.09936000479363957]
	TIME [epoch: 7.86 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06006553406495137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06006553406495137 | validation: 0.07543290680668402]
	TIME [epoch: 7.85 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07938576851414617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07938576851414617 | validation: 0.09701038792546346]
	TIME [epoch: 7.85 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06532634308859062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06532634308859062 | validation: 0.0807332227815603]
	TIME [epoch: 7.85 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04307374020531844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04307374020531844 | validation: 0.06112766404106423]
	TIME [epoch: 7.84 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03035090899970296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03035090899970296 | validation: 0.11000895797410329]
	TIME [epoch: 7.85 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04057273686714151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04057273686714151 | validation: 0.09527830824695185]
	TIME [epoch: 7.88 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06082932055364044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06082932055364044 | validation: 0.12406153917111534]
	TIME [epoch: 7.85 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050084005947660215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050084005947660215 | validation: 0.06624141597758541]
	TIME [epoch: 7.85 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043136086799823906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043136086799823906 | validation: 0.09434471412211735]
	TIME [epoch: 7.85 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04516237856166491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04516237856166491 | validation: 0.0770189269138653]
	TIME [epoch: 7.84 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07662628304155249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07662628304155249 | validation: 0.16618830846682645]
	TIME [epoch: 7.91 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11714797310663165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11714797310663165 | validation: 0.2648685259647387]
	TIME [epoch: 7.85 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16074386307252256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16074386307252256 | validation: 0.10884130472384453]
	TIME [epoch: 7.87 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04988651208537876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04988651208537876 | validation: 0.10759902746407542]
	TIME [epoch: 7.85 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055696373916209206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055696373916209206 | validation: 0.11800860490225923]
	TIME [epoch: 7.85 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04544212841999182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04544212841999182 | validation: 0.07032278252516315]
	TIME [epoch: 7.85 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03581593629486045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03581593629486045 | validation: 0.06806526207677303]
	TIME [epoch: 7.85 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04280699584863294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04280699584863294 | validation: 0.13020883833909844]
	TIME [epoch: 7.85 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06499680738014077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06499680738014077 | validation: 0.0942613497131559]
	TIME [epoch: 7.86 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09249519022862863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09249519022862863 | validation: 0.13608609751878084]
	TIME [epoch: 7.86 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06053161642928039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06053161642928039 | validation: 0.11142146298418716]
	TIME [epoch: 7.85 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037354363819442724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037354363819442724 | validation: 0.05986097630231151]
	TIME [epoch: 7.85 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03445456278150794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03445456278150794 | validation: 0.10209267593999885]
	TIME [epoch: 7.86 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04905778186100529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04905778186100529 | validation: 0.07859119898957899]
	TIME [epoch: 7.84 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08080682410194147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08080682410194147 | validation: 0.10143823781148505]
	TIME [epoch: 7.87 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07724995280640819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07724995280640819 | validation: 0.10610460151470913]
	TIME [epoch: 7.86 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0568542492713433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0568542492713433 | validation: 0.11220595339183857]
	TIME [epoch: 7.86 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05699457364882632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05699457364882632 | validation: 0.10607315089127584]
	TIME [epoch: 7.85 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03907217370635939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03907217370635939 | validation: 0.05679593598006266]
	TIME [epoch: 7.87 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01976081389642869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01976081389642869 | validation: 0.06296709395139324]
	TIME [epoch: 7.86 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019568679665335213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019568679665335213 | validation: 0.059119306916235397]
	TIME [epoch: 7.87 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03967897204175013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03967897204175013 | validation: 0.1438481043996179]
	TIME [epoch: 7.87 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08292236404835995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08292236404835995 | validation: 0.12593131526336207]
	TIME [epoch: 7.88 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09037256949046224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09037256949046224 | validation: 0.13680247420461314]
	TIME [epoch: 7.86 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08547247696587647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08547247696587647 | validation: 0.1191404473361099]
	TIME [epoch: 7.87 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08161325164137978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08161325164137978 | validation: 0.08380663756159147]
	TIME [epoch: 7.87 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06654456032832409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06654456032832409 | validation: 0.08633622557459497]
	TIME [epoch: 7.86 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04822021655339975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04822021655339975 | validation: 0.10031165536641468]
	TIME [epoch: 7.87 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03961679621947321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03961679621947321 | validation: 0.05638955100938906]
	TIME [epoch: 7.87 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03738309552194169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03738309552194169 | validation: 0.08705976786120137]
	TIME [epoch: 7.87 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030555948455918393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030555948455918393 | validation: 0.07706729359620446]
	TIME [epoch: 7.86 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0340776159309267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0340776159309267 | validation: 0.07709881209388153]
	TIME [epoch: 7.85 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056919052514969176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056919052514969176 | validation: 0.1544440762741903]
	TIME [epoch: 7.86 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09790543165967468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09790543165967468 | validation: 0.11159635399728861]
	TIME [epoch: 7.89 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09582267876129681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09582267876129681 | validation: 0.09051589601727561]
	TIME [epoch: 7.86 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058223282573593986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058223282573593986 | validation: 0.06549168955327803]
	TIME [epoch: 7.86 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045208021466279745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045208021466279745 | validation: 0.06972195359685321]
	TIME [epoch: 7.87 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053364533135010016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053364533135010016 | validation: 0.08859639485769387]
	TIME [epoch: 7.86 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04594596005385159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04594596005385159 | validation: 0.10573415087062094]
	TIME [epoch: 7.86 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043879344419001404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043879344419001404 | validation: 0.09756808572450384]
	TIME [epoch: 7.85 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0636543492122831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0636543492122831 | validation: 0.1364565303712694]
	TIME [epoch: 7.87 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05460456710827588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05460456710827588 | validation: 0.10359334274228571]
	TIME [epoch: 7.85 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057658830890357574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057658830890357574 | validation: 0.1013524299928982]
	TIME [epoch: 7.85 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051896123514561555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051896123514561555 | validation: 0.04185409986987876]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_1325.pth
	Model improved!!!
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036684221731497775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036684221731497775 | validation: 0.08395942526107811]
	TIME [epoch: 7.84 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02832584375258608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02832584375258608 | validation: 0.048685035642400246]
	TIME [epoch: 7.84 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0337744916044234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0337744916044234 | validation: 0.05780133206386689]
	TIME [epoch: 7.86 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04231348994081638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04231348994081638 | validation: 0.09984608469219024]
	TIME [epoch: 7.87 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0831569780164476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0831569780164476 | validation: 0.18404800529299645]
	TIME [epoch: 7.85 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14968718269130346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14968718269130346 | validation: 0.21215119733981772]
	TIME [epoch: 7.85 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12180501188061456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12180501188061456 | validation: 0.092360100314406]
	TIME [epoch: 7.85 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05417449179967991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05417449179967991 | validation: 0.1029308544435885]
	TIME [epoch: 7.84 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037074381074613985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037074381074613985 | validation: 0.06420441523824576]
	TIME [epoch: 7.85 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03506791451755836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03506791451755836 | validation: 0.08727866971510143]
	TIME [epoch: 7.87 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025970547630453637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025970547630453637 | validation: 0.05942036356741523]
	TIME [epoch: 7.85 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026411206079926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026411206079926 | validation: 0.07137242047317094]
	TIME [epoch: 7.85 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028267238357284834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028267238357284834 | validation: 0.08542910055766327]
	TIME [epoch: 7.85 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042538321281690876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042538321281690876 | validation: 0.09263890748063956]
	TIME [epoch: 7.85 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05801083004816279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05801083004816279 | validation: 0.10497036060379111]
	TIME [epoch: 7.86 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06564715226329051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06564715226329051 | validation: 0.0699206007354121]
	TIME [epoch: 7.86 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06374131737747663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06374131737747663 | validation: 0.11411453297065198]
	TIME [epoch: 7.86 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03634917754469307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03634917754469307 | validation: 0.09762036045366151]
	TIME [epoch: 7.85 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04473482123916526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04473482123916526 | validation: 0.10815392074632446]
	TIME [epoch: 7.86 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09271749049210408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09271749049210408 | validation: 0.11768349288704201]
	TIME [epoch: 7.86 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12190679299439014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12190679299439014 | validation: 0.09998245477602781]
	TIME [epoch: 7.85 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07424363213370058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07424363213370058 | validation: 0.05235884796859866]
	TIME [epoch: 7.87 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03398541819578451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03398541819578451 | validation: 0.08349399766617653]
	TIME [epoch: 7.86 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03069682608094776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03069682608094776 | validation: 0.09564183607950605]
	TIME [epoch: 7.86 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04499249489557021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04499249489557021 | validation: 0.1157503039375702]
	TIME [epoch: 7.89 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04186420913272825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04186420913272825 | validation: 0.08076348337187761]
	TIME [epoch: 7.87 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045494594673739654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045494594673739654 | validation: 0.12511808447076583]
	TIME [epoch: 7.85 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06568349806489657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06568349806489657 | validation: 0.07646944516256189]
	TIME [epoch: 7.86 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06951710949526135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06951710949526135 | validation: 0.10544965002288342]
	TIME [epoch: 7.87 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04649113060304846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04649113060304846 | validation: 0.04968581593032728]
	TIME [epoch: 7.86 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022322363871113124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022322363871113124 | validation: 0.06379754649398829]
	TIME [epoch: 7.85 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016375703463289818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016375703463289818 | validation: 0.052549193284196716]
	TIME [epoch: 7.85 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024232645750868885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024232645750868885 | validation: 0.07700376639094081]
	TIME [epoch: 7.84 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055089890424326306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055089890424326306 | validation: 0.09699783928177488]
	TIME [epoch: 7.85 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10077325507115785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10077325507115785 | validation: 0.1102969224665996]
	TIME [epoch: 7.87 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08490653875381089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08490653875381089 | validation: 0.08465108564447499]
	TIME [epoch: 7.86 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05082095036828335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05082095036828335 | validation: 0.14362020853106455]
	TIME [epoch: 7.85 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05757904125021233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05757904125021233 | validation: 0.17220674849475973]
	TIME [epoch: 7.85 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10434931779278916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10434931779278916 | validation: 0.18777393513500484]
	TIME [epoch: 7.85 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10288589615464831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10288589615464831 | validation: 0.07159035422542372]
	TIME [epoch: 7.88 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043173497966099816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043173497966099816 | validation: 0.10799726878155086]
	TIME [epoch: 7.92 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052670289064694564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052670289064694564 | validation: 0.07703787885630359]
	TIME [epoch: 7.91 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03883089058476794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03883089058476794 | validation: 0.046290712361018144]
	TIME [epoch: 7.89 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022853647096487954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022853647096487954 | validation: 0.05376102790880517]
	TIME [epoch: 7.9 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027352445709126483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027352445709126483 | validation: 0.09014001053644265]
	TIME [epoch: 7.9 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055728417154671306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055728417154671306 | validation: 0.184168146132502]
	TIME [epoch: 7.87 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12117130615384834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12117130615384834 | validation: 0.10633790314916901]
	TIME [epoch: 7.86 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0838118128771676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0838118128771676 | validation: 0.06528961062142621]
	TIME [epoch: 7.87 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03285638563399314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03285638563399314 | validation: 0.07065931972160082]
	TIME [epoch: 7.88 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027860679098902254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027860679098902254 | validation: 0.048941678126333724]
	TIME [epoch: 7.88 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030768111255155352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030768111255155352 | validation: 0.1290716491539045]
	TIME [epoch: 7.88 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05335984029918972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05335984029918972 | validation: 0.08915082718022237]
	TIME [epoch: 7.98 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08392941349692558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08392941349692558 | validation: 0.11154255017191439]
	TIME [epoch: 7.88 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06533173836100142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06533173836100142 | validation: 0.06526439602968757]
	TIME [epoch: 7.89 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04795451702368043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04795451702368043 | validation: 0.10164925083272668]
	TIME [epoch: 7.9 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044384348203434495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044384348203434495 | validation: 0.15416235897116565]
	TIME [epoch: 7.88 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.069748725462726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.069748725462726 | validation: 0.09885193463183817]
	TIME [epoch: 7.89 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04678091133498613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04678091133498613 | validation: 0.07300147769708876]
	TIME [epoch: 7.88 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03842815927170427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03842815927170427 | validation: 0.08824954817958838]
	TIME [epoch: 7.91 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07381725386767471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07381725386767471 | validation: 0.07409330878121205]
	TIME [epoch: 7.9 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09128608688580749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09128608688580749 | validation: 0.1033367578669358]
	TIME [epoch: 7.91 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05107632101068786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05107632101068786 | validation: 0.0530081203556165]
	TIME [epoch: 7.88 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026058311710408608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026058311710408608 | validation: 0.07048298883530692]
	TIME [epoch: 7.9 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018185103698250576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018185103698250576 | validation: 0.048167635796958576]
	TIME [epoch: 7.88 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0199144229208003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0199144229208003 | validation: 0.09200940426346718]
	TIME [epoch: 7.88 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029582507857020185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029582507857020185 | validation: 0.12033317237006667]
	TIME [epoch: 7.87 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06441383857892301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06441383857892301 | validation: 0.1433706016717655]
	TIME [epoch: 7.95 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06522979204132297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06522979204132297 | validation: 0.07126559873567385]
	TIME [epoch: 7.89 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04800745957029665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04800745957029665 | validation: 0.12408482664576721]
	TIME [epoch: 7.88 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08102060088721783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08102060088721783 | validation: 0.10761916390766575]
	TIME [epoch: 7.9 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11390837916626734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11390837916626734 | validation: 0.09012737507956248]
	TIME [epoch: 7.88 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05042466732895113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05042466732895113 | validation: 0.04799620957599602]
	TIME [epoch: 7.89 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03801408989360213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03801408989360213 | validation: 0.06584817488902087]
	TIME [epoch: 7.89 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03526190798632253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03526190798632253 | validation: 0.14346188542267987]
	TIME [epoch: 7.9 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06086458991879651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06086458991879651 | validation: 0.1344186416807813]
	TIME [epoch: 7.88 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07149744807824351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07149744807824351 | validation: 0.08047713788145648]
	TIME [epoch: 7.88 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03147241973567381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03147241973567381 | validation: 0.06770749852941053]
	TIME [epoch: 7.87 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02923993193395405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02923993193395405 | validation: 0.05151725350218511]
	TIME [epoch: 7.87 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040811403111472536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040811403111472536 | validation: 0.07895275167401719]
	TIME [epoch: 7.87 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05517862601536189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05517862601536189 | validation: 0.08099466262029939]
	TIME [epoch: 7.88 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05735159960787053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05735159960787053 | validation: 0.07934379309789061]
	TIME [epoch: 7.87 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04688040932535611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04688040932535611 | validation: 0.055660027168478655]
	TIME [epoch: 7.87 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03843721616748085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03843721616748085 | validation: 0.09513652268623814]
	TIME [epoch: 7.87 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04954520728400319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04954520728400319 | validation: 0.08374061958882677]
	TIME [epoch: 7.87 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07721903573832088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07721903573832088 | validation: 0.1559163231401893]
	TIME [epoch: 7.89 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08174600528671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08174600528671 | validation: 0.14391451608594702]
	TIME [epoch: 7.95 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08601784759039964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08601784759039964 | validation: 0.11775939106504991]
	TIME [epoch: 7.88 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051492888464871996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051492888464871996 | validation: 0.0705658007613913]
	TIME [epoch: 7.87 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05692435933779359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05692435933779359 | validation: 0.09757320155130689]
	TIME [epoch: 7.89 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0495581706850816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0495581706850816 | validation: 0.05842893302056606]
	TIME [epoch: 7.87 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03116568685979317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03116568685979317 | validation: 0.06736797803273485]
	TIME [epoch: 7.87 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024705509334461122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024705509334461122 | validation: 0.05591553370426871]
	TIME [epoch: 7.89 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03657411607111375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03657411607111375 | validation: 0.10581036678232857]
	TIME [epoch: 7.89 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06848799505902717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06848799505902717 | validation: 0.1585289870928651]
	TIME [epoch: 7.89 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11049885291322921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11049885291322921 | validation: 0.09811055384517138]
	TIME [epoch: 7.88 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07207523267449177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07207523267449177 | validation: 0.09151659941139624]
	TIME [epoch: 7.88 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02990615126979617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02990615126979617 | validation: 0.055405691789568993]
	TIME [epoch: 7.88 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031562476575775884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031562476575775884 | validation: 0.08924844033607754]
	TIME [epoch: 7.88 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0382584163783747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0382584163783747 | validation: 0.06203997737580791]
	TIME [epoch: 7.9 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04420038897406415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04420038897406415 | validation: 0.11523524143703218]
	TIME [epoch: 7.88 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0375867828453524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0375867828453524 | validation: 0.11550455150451291]
	TIME [epoch: 7.88 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_144616/states/model_phi1_4a_v_mmd1_1426.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 6191.448 seconds.
