Args:
Namespace(name='model_phi1_4b_v_mmd1', outdir='out/model_training/model_phi1_4b_v_mmd1', training_data='data/training_data/data_phi1_4b/training', validation_data='data/training_data/data_phi1_4b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1111571246

Training model...

Saving initial model state to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.548810702913766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.548810702913766 | validation: 5.108030415777115]
	TIME [epoch: 43.1 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.938462326938174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.938462326938174 | validation: 6.1559449775065715]
	TIME [epoch: 1.84 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.228000285886481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.228000285886481 | validation: 5.646031697146665]
	TIME [epoch: 1.82 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.703461118562008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.703461118562008 | validation: 4.897217931247376]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.533349409988523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.533349409988523 | validation: 4.7611396296108035]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.928366118596075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.928366118596075 | validation: 5.176446970504371]
	TIME [epoch: 1.83 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.244287028387386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.244287028387386 | validation: 4.750511180393886]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.756662996727494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.756662996727494 | validation: 4.661046859233259]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.880240679776257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.880240679776257 | validation: 4.543218041290547]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.6274346010870495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6274346010870495 | validation: 4.6686658218543515]
	TIME [epoch: 1.83 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.659754060164134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.659754060164134 | validation: 4.491823083617716]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.582259947570345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.582259947570345 | validation: 4.413357716385434]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.478559188721663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.478559188721663 | validation: 4.470254632422804]
	TIME [epoch: 1.83 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.442591451063781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.442591451063781 | validation: 4.346564380169251]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.417335816375169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.417335816375169 | validation: 4.3387312301656555]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.372887859139906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.372887859139906 | validation: 4.266122483414332]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.331272284158439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.331272284158439 | validation: 4.310298918573316]
	TIME [epoch: 1.83 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.289416320111961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.289416320111961 | validation: 4.1333749493885135]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.281168506598544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.281168506598544 | validation: 4.3015185810147045]
	TIME [epoch: 1.83 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.2186080941668385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2186080941668385 | validation: 4.018200966840617]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.194918455006112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.194918455006112 | validation: 4.178923935447599]
	TIME [epoch: 1.83 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.104512078919921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.104512078919921 | validation: 3.9640298767138207]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.06098190256396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.06098190256396 | validation: 4.11699301362947]
	TIME [epoch: 1.82 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.013013419739462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.013013419739462 | validation: 3.9384519786988648]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.015369041872985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.015369041872985 | validation: 4.196925096227592]
	TIME [epoch: 1.82 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.049622825234663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.049622825234663 | validation: 4.192509479103985]
	TIME [epoch: 1.83 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.184043131789883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.184043131789883 | validation: 3.8774495025456575]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.919136965973239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.919136965973239 | validation: 4.163179730139016]
	TIME [epoch: 1.84 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.982609873342255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.982609873342255 | validation: 3.9681469201626487]
	TIME [epoch: 1.83 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9858691204632932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9858691204632932 | validation: 3.8490604693525103]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.797631964274491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.797631964274491 | validation: 4.069745958306906]
	TIME [epoch: 1.83 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8868720114176223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8868720114176223 | validation: 3.826111684837686]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8493013580413855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8493013580413855 | validation: 3.8302521775806597]
	TIME [epoch: 1.83 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7323699233288723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7323699233288723 | validation: 3.912469522066633]
	TIME [epoch: 1.83 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7575719160761616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7575719160761616 | validation: 3.7584776608327704]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7440107331900903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7440107331900903 | validation: 3.7997606502081824]
	TIME [epoch: 1.82 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6798585306274423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6798585306274423 | validation: 3.754318459358571]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.660544064489908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.660544064489908 | validation: 3.708118090323546]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6468558283958576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6468558283958576 | validation: 3.7534461356083613]
	TIME [epoch: 1.82 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.632786423398173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.632786423398173 | validation: 3.6602830269673023]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.624701390974117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.624701390974117 | validation: 3.7214942940752516]
	TIME [epoch: 1.82 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6209520991220296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6209520991220296 | validation: 3.7061451148258513]
	TIME [epoch: 1.82 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.651754919770277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.651754919770277 | validation: 3.75740599515637]
	TIME [epoch: 1.82 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.66412858421862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.66412858421862 | validation: 3.65282764782736]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.610516922717678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.610516922717678 | validation: 3.6486816347614766]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.538322972173416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.538322972173416 | validation: 3.57934359835391]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.514645967216328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.514645967216328 | validation: 3.5809701377972547]
	TIME [epoch: 1.82 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4942819237290417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4942819237290417 | validation: 3.5476944821375485]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.484336908676604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.484336908676604 | validation: 3.5441087587806908]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4627511314274613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4627511314274613 | validation: 3.5144381913451554]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.445614347267378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.445614347267378 | validation: 3.519986062941741]
	TIME [epoch: 1.83 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.449763799687545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.449763799687545 | validation: 3.66003360265592]
	TIME [epoch: 1.82 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5893408388647186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5893408388647186 | validation: 3.7778069699444297]
	TIME [epoch: 1.82 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.737937953369859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.737937953369859 | validation: 3.478564789162114]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.449974484164201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.449974484164201 | validation: 3.546904106397047]
	TIME [epoch: 1.83 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4645431486480636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4645431486480636 | validation: 3.4770498943154973]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.414112274908966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.414112274908966 | validation: 3.4299022712631473]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3922330463535553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3922330463535553 | validation: 3.420049839532944]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3456037232387725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3456037232387725 | validation: 3.420837351705716]
	TIME [epoch: 1.82 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3373160219117177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3373160219117177 | validation: 3.393597941742194]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3260251243555565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3260251243555565 | validation: 3.3639569197986066]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3127733329277396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3127733329277396 | validation: 3.3542170039284858]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3013269965328744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3013269965328744 | validation: 3.332255226518301]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.275365926085949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.275365926085949 | validation: 3.3408125917531737]
	TIME [epoch: 1.83 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.27180826444825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.27180826444825 | validation: 3.333073042011238]
	TIME [epoch: 1.84 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2690330272898898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2690330272898898 | validation: 3.3398063344349596]
	TIME [epoch: 1.83 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.230953020679235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.230953020679235 | validation: 2.544060249706608]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6624028674826956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6624028674826956 | validation: 1.9876291563551505]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9570374056606512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9570374056606512 | validation: 2.65101323877655]
	TIME [epoch: 1.83 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.702010711521359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.702010711521359 | validation: 1.5988403199011385]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.530367058978523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.530367058978523 | validation: 1.3996890735600873]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.288072370717143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.288072370717143 | validation: 1.1615445871868046]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.143579382966095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.143579382966095 | validation: 1.10182947137332]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9778592125506005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9778592125506005 | validation: 1.0020701567379076]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9288383812487162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9288383812487162 | validation: 0.9719385831209699]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8860454581990337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8860454581990337 | validation: 0.9512027600406392]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8731387638980115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8731387638980115 | validation: 0.9503986195751282]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8656618511901398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8656618511901398 | validation: 0.9602783077606998]
	TIME [epoch: 1.83 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8425347783055795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8425347783055795 | validation: 0.928768767501818]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8369960903670096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8369960903670096 | validation: 0.9097626370573514]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8235077439367311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8235077439367311 | validation: 0.9117570914374982]
	TIME [epoch: 1.82 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8170169320324466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8170169320324466 | validation: 0.9154886233180813]
	TIME [epoch: 1.82 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8210705501599719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8210705501599719 | validation: 0.9436462130976729]
	TIME [epoch: 1.83 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8138742793581741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8138742793581741 | validation: 0.8952071872863997]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8168209575627929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8168209575627929 | validation: 0.9245098387644785]
	TIME [epoch: 1.84 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8404530436453551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8404530436453551 | validation: 1.0838729756880852]
	TIME [epoch: 1.83 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.937164592262652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.937164592262652 | validation: 1.0741225725958548]
	TIME [epoch: 1.82 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.025945290542712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.025945290542712 | validation: 0.9436244473718536]
	TIME [epoch: 1.82 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.798387409356113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.798387409356113 | validation: 0.970362612246426]
	TIME [epoch: 1.87 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8057171214360664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8057171214360664 | validation: 0.9217130454961886]
	TIME [epoch: 1.83 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8520159815782387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8520159815782387 | validation: 0.9690257967635276]
	TIME [epoch: 1.82 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8080957626250316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8080957626250316 | validation: 0.888413194199126]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7897734839545563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7897734839545563 | validation: 0.8742325802122135]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7844392349390356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7844392349390356 | validation: 0.9110705611399471]
	TIME [epoch: 1.83 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7762711132957537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7762711132957537 | validation: 0.8616393301176803]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7751421771645066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7751421771645066 | validation: 0.9244930352181374]
	TIME [epoch: 1.83 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7817848942319426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7817848942319426 | validation: 0.9378164435562276]
	TIME [epoch: 1.83 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8308693409951508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8308693409951508 | validation: 1.009884740802123]
	TIME [epoch: 1.83 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9003366774894818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9003366774894818 | validation: 0.9570942949138669]
	TIME [epoch: 1.83 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8330137335067644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8330137335067644 | validation: 0.9019370360334545]
	TIME [epoch: 1.83 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7936260795031143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7936260795031143 | validation: 0.948289501081879]
	TIME [epoch: 1.83 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7791186008858838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7791186008858838 | validation: 0.8570815101802005]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7831271794693888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7831271794693888 | validation: 1.0594570166854071]
	TIME [epoch: 1.83 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.821290786909043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.821290786909043 | validation: 0.8707527005757211]
	TIME [epoch: 1.84 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8279794720868275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8279794720868275 | validation: 1.018483841540772]
	TIME [epoch: 1.83 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8073923963710743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8073923963710743 | validation: 0.866092238306554]
	TIME [epoch: 1.83 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7576144151271142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7576144151271142 | validation: 0.8817771853242041]
	TIME [epoch: 1.83 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7571396815864051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7571396815864051 | validation: 0.911862618668383]
	TIME [epoch: 1.82 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7617895218630082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7617895218630082 | validation: 0.8494268563815837]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7705054293717649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7705054293717649 | validation: 0.9404377147102502]
	TIME [epoch: 1.83 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.773049830804002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.773049830804002 | validation: 0.8870441044930332]
	TIME [epoch: 1.83 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8279649992804076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8279649992804076 | validation: 1.0944345785038194]
	TIME [epoch: 1.82 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9487275404838025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9487275404838025 | validation: 1.1973554020751118]
	TIME [epoch: 1.82 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9216392344882656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9216392344882656 | validation: 0.9861039038826314]
	TIME [epoch: 1.82 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8992400452589118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8992400452589118 | validation: 1.0007239541020765]
	TIME [epoch: 1.83 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7800380093888072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7800380093888072 | validation: 0.9178157856164312]
	TIME [epoch: 1.82 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7854390450348792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7854390450348792 | validation: 0.9140760420989535]
	TIME [epoch: 1.82 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8150781339863795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8150781339863795 | validation: 0.9210495382506011]
	TIME [epoch: 1.82 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7627081730800012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7627081730800012 | validation: 0.8784306177071557]
	TIME [epoch: 1.82 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7596389965887331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7596389965887331 | validation: 0.8459937320745203]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7586120285592765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7586120285592765 | validation: 0.954261929895451]
	TIME [epoch: 1.82 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7639186613791552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7639186613791552 | validation: 0.8451947799657135]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7566548995800426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7566548995800426 | validation: 0.9471690704842722]
	TIME [epoch: 1.82 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7581775568974581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7581775568974581 | validation: 0.857163675759315]
	TIME [epoch: 1.83 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7524956502130046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7524956502130046 | validation: 0.8957002631515878]
	TIME [epoch: 1.82 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.746455837570017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.746455837570017 | validation: 0.8611200560940564]
	TIME [epoch: 1.84 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7511816732929362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7511816732929362 | validation: 0.9659285646683952]
	TIME [epoch: 1.83 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7683438896632042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7683438896632042 | validation: 0.991867135630473]
	TIME [epoch: 1.83 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8751151186902689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8751151186902689 | validation: 1.134284173708093]
	TIME [epoch: 1.83 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0309872521993555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0309872521993555 | validation: 1.200224328452946]
	TIME [epoch: 1.82 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9854612244942531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9854612244942531 | validation: 0.9142470981143351]
	TIME [epoch: 1.82 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.765511970464286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.765511970464286 | validation: 0.9149613372061989]
	TIME [epoch: 1.82 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.830752312060954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.830752312060954 | validation: 0.9950444221205743]
	TIME [epoch: 1.82 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.828063060822127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.828063060822127 | validation: 0.9172768991101172]
	TIME [epoch: 1.82 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7545439161421393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7545439161421393 | validation: 0.8498364659705113]
	TIME [epoch: 1.82 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7525355777757886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7525355777757886 | validation: 0.9482322312600397]
	TIME [epoch: 1.82 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7614851803085031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7614851803085031 | validation: 0.8435858229831855]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_137.pth
	Model improved!!!
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7601716407495352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7601716407495352 | validation: 0.9904933726484523]
	TIME [epoch: 1.83 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7755385666435464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7755385666435464 | validation: 0.85574550336257]
	TIME [epoch: 1.82 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7707023863431698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7707023863431698 | validation: 1.0862717438641099]
	TIME [epoch: 1.82 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8165648062555395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8165648062555395 | validation: 0.9439114584217159]
	TIME [epoch: 1.83 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8674806418957118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8674806418957118 | validation: 0.9358090380767901]
	TIME [epoch: 1.83 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7681016788461884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7681016788461884 | validation: 0.9198930457907157]
	TIME [epoch: 1.82 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7531089462953994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7531089462953994 | validation: 0.8419753146417316]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_144.pth
	Model improved!!!
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7783689842603951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7783689842603951 | validation: 1.0216029012025014]
	TIME [epoch: 1.82 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8023842645597484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8023842645597484 | validation: 0.8869501818848392]
	TIME [epoch: 1.82 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7936423862689438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7936423862689438 | validation: 0.9942991958882165]
	TIME [epoch: 1.82 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8443854688065932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8443854688065932 | validation: 0.9466501196296964]
	TIME [epoch: 1.83 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8068322642214054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8068322642214054 | validation: 0.9265630985148139]
	TIME [epoch: 1.82 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7763244362644257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7763244362644257 | validation: 0.8722776700544372]
	TIME [epoch: 1.83 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7548753117579912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7548753117579912 | validation: 0.9284146003136252]
	TIME [epoch: 1.82 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7562215492721805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7562215492721805 | validation: 0.8584702430928464]
	TIME [epoch: 1.82 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7553169673187088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7553169673187088 | validation: 0.984075342663617]
	TIME [epoch: 1.83 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7608470159582046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7608470159582046 | validation: 0.8615297932345989]
	TIME [epoch: 1.82 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7907320041009898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7907320041009898 | validation: 1.072121294880778]
	TIME [epoch: 1.82 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8016146276406879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8016146276406879 | validation: 0.8700837240110382]
	TIME [epoch: 1.83 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7596414414093338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7596414414093338 | validation: 0.8628839863742362]
	TIME [epoch: 1.83 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7568938912677615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7568938912677615 | validation: 1.007253870290991]
	TIME [epoch: 1.82 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8167202934584602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8167202934584602 | validation: 0.9573292189735124]
	TIME [epoch: 1.82 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8637476990287087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8637476990287087 | validation: 1.032118887620371]
	TIME [epoch: 1.83 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8917955167954986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8917955167954986 | validation: 0.9254257456507343]
	TIME [epoch: 1.82 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.766183024936679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.766183024936679 | validation: 0.8438498951455369]
	TIME [epoch: 1.83 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.749493943356745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.749493943356745 | validation: 0.9480649477845152]
	TIME [epoch: 1.83 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7611346017446704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7611346017446704 | validation: 0.8604642176711789]
	TIME [epoch: 1.83 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7553841638619386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7553841638619386 | validation: 0.8999736362319912]
	TIME [epoch: 1.83 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7619209279805685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7619209279805685 | validation: 0.8980924399003514]
	TIME [epoch: 1.83 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7719530981143616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7719530981143616 | validation: 0.9364354376589885]
	TIME [epoch: 1.83 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7990429372954668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7990429372954668 | validation: 0.9883668079522278]
	TIME [epoch: 1.83 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8110757154579497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8110757154579497 | validation: 0.959269011222011]
	TIME [epoch: 1.82 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8241945866356118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8241945866356118 | validation: 0.9432028209426817]
	TIME [epoch: 1.84 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7710617316335249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7710617316335249 | validation: 0.9053382660427673]
	TIME [epoch: 1.82 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7557274743583944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7557274743583944 | validation: 0.9067244560247668]
	TIME [epoch: 1.84 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7512227616188694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7512227616188694 | validation: 0.8757567758610971]
	TIME [epoch: 1.82 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.751330504135869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.751330504135869 | validation: 0.8997996723856485]
	TIME [epoch: 1.83 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7519889927023158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7519889927023158 | validation: 0.9326930665245836]
	TIME [epoch: 1.82 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7625801228581307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7625801228581307 | validation: 0.8762629437533775]
	TIME [epoch: 1.83 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8127506770824867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8127506770824867 | validation: 1.1113175198986631]
	TIME [epoch: 1.82 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8931614640044214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8931614640044214 | validation: 0.9263317670179664]
	TIME [epoch: 1.83 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7785405523069371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7785405523069371 | validation: 0.8610652928450133]
	TIME [epoch: 1.83 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7705126874687851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7705126874687851 | validation: 1.0701373560880136]
	TIME [epoch: 1.83 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.80197201503048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.80197201503048 | validation: 0.8550061262723677]
	TIME [epoch: 1.83 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7862881164491714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7862881164491714 | validation: 0.8987868875336886]
	TIME [epoch: 1.83 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7428332086435887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7428332086435887 | validation: 0.8806838755922574]
	TIME [epoch: 1.82 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7351705389695067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7351705389695067 | validation: 0.8392427182634283]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_184.pth
	Model improved!!!
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7536098456310047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7536098456310047 | validation: 0.9806606962149331]
	TIME [epoch: 1.83 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7723360620965182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7723360620965182 | validation: 0.8641989240221115]
	TIME [epoch: 1.83 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.782454893363556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.782454893363556 | validation: 0.9577885808542004]
	TIME [epoch: 1.83 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.828491466865585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.828491466865585 | validation: 1.0105895360767028]
	TIME [epoch: 1.83 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8280138618440893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8280138618440893 | validation: 0.9209213118782812]
	TIME [epoch: 1.83 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8003568896218343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8003568896218343 | validation: 0.8809418995303354]
	TIME [epoch: 1.83 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7532295923840658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7532295923840658 | validation: 0.9055229293748482]
	TIME [epoch: 1.83 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7434745348202568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7434745348202568 | validation: 0.8415478825100497]
	TIME [epoch: 1.82 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7364496984429908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7364496984429908 | validation: 1.024868926977514]
	TIME [epoch: 1.83 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7659305497650785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7659305497650785 | validation: 0.8323189398105968]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7478815064640059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7478815064640059 | validation: 0.9349888586422577]
	TIME [epoch: 1.83 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7392215377623734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7392215377623734 | validation: 0.825940268280702]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7396461257489236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7396461257489236 | validation: 0.9290439417587759]
	TIME [epoch: 1.83 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7431447620050904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7431447620050904 | validation: 0.8517944851909655]
	TIME [epoch: 1.83 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7569509645037051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7569509645037051 | validation: 1.0574795981792044]
	TIME [epoch: 1.82 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8714026706164993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8714026706164993 | validation: 1.0899912230016966]
	TIME [epoch: 1.82 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8851194972640875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8851194972640875 | validation: 0.9622500598012316]
	TIME [epoch: 43.5 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8780060062340254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8780060062340254 | validation: 0.8986301688690649]
	TIME [epoch: 3.64 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7309526099242675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7309526099242675 | validation: 0.9303680504097873]
	TIME [epoch: 3.62 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7605200668863702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7605200668863702 | validation: 0.8706674254423755]
	TIME [epoch: 3.62 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7794280577314697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7794280577314697 | validation: 0.8776244631676033]
	TIME [epoch: 3.62 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7473374836553259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7473374836553259 | validation: 0.9074350711348139]
	TIME [epoch: 3.62 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.742647714185028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.742647714185028 | validation: 0.8393810531418708]
	TIME [epoch: 3.61 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7457197389549538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7457197389549538 | validation: 0.9547935573493405]
	TIME [epoch: 3.62 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7692138945269895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7692138945269895 | validation: 0.8297251970809025]
	TIME [epoch: 3.61 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7726361701546368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7726361701546368 | validation: 0.9799582263658355]
	TIME [epoch: 3.62 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7862634408378942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7862634408378942 | validation: 0.8831027382952649]
	TIME [epoch: 3.62 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7664901264912278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7664901264912278 | validation: 0.8880100580333927]
	TIME [epoch: 3.63 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7714599002640838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7714599002640838 | validation: 0.9739792949624699]
	TIME [epoch: 3.63 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7926923137891606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7926923137891606 | validation: 0.8984631977847809]
	TIME [epoch: 3.62 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8022467101208965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8022467101208965 | validation: 0.9006236915019854]
	TIME [epoch: 3.62 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7514326427475573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7514326427475573 | validation: 0.8734808581120044]
	TIME [epoch: 3.62 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7352721306714365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7352721306714365 | validation: 0.8561189575866877]
	TIME [epoch: 3.63 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7268645269053977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7268645269053977 | validation: 0.903524911741473]
	TIME [epoch: 3.62 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7406582743780271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7406582743780271 | validation: 0.827568288020996]
	TIME [epoch: 3.62 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7505483243269759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7505483243269759 | validation: 0.985986856340319]
	TIME [epoch: 3.62 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7859413796251903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7859413796251903 | validation: 0.8807542703210753]
	TIME [epoch: 3.62 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8021268475000136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8021268475000136 | validation: 0.9729246080870202]
	TIME [epoch: 3.62 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8478765768818269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8478765768818269 | validation: 0.9446739248410376]
	TIME [epoch: 3.62 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7808475284552256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7808475284552256 | validation: 0.8640519821619066]
	TIME [epoch: 3.62 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7549289127485002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7549289127485002 | validation: 0.913801467508955]
	TIME [epoch: 3.63 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7371291537543755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7371291537543755 | validation: 0.8094074641291075]
	TIME [epoch: 3.64 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7441043123340717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7441043123340717 | validation: 0.9710011277935569]
	TIME [epoch: 3.61 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7424929691776797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7424929691776797 | validation: 0.8051016756520888]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_228.pth
	Model improved!!!
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.750734494369425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.750734494369425 | validation: 0.950089029581243]
	TIME [epoch: 3.62 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7491017061594678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7491017061594678 | validation: 0.8517669827619149]
	TIME [epoch: 3.6 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7491359175800697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7491359175800697 | validation: 0.941447316709922]
	TIME [epoch: 3.6 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7963517848552377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7963517848552377 | validation: 1.0264791245253493]
	TIME [epoch: 3.6 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8503788717596967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8503788717596967 | validation: 0.9583023324691763]
	TIME [epoch: 3.6 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8358302240804032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8358302240804032 | validation: 0.8708643974073461]
	TIME [epoch: 3.6 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7333851475022576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7333851475022576 | validation: 0.8942961643660663]
	TIME [epoch: 3.59 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7197286246927385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7197286246927385 | validation: 0.823903793249918]
	TIME [epoch: 3.6 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7244577364240786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7244577364240786 | validation: 0.8677488438593726]
	TIME [epoch: 3.61 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7195483780137942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7195483780137942 | validation: 0.8671111801897994]
	TIME [epoch: 3.61 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7216730228285456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7216730228285456 | validation: 0.8505243960787245]
	TIME [epoch: 3.6 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7386348489969241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7386348489969241 | validation: 1.003600667357425]
	TIME [epoch: 3.6 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8158232967839437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8158232967839437 | validation: 0.9657163287847633]
	TIME [epoch: 3.6 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8557513172740779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8557513172740779 | validation: 0.9783237460534553]
	TIME [epoch: 3.6 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8257266129611379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8257266129611379 | validation: 0.9322919334837082]
	TIME [epoch: 3.59 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7347843573922833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7347843573922833 | validation: 0.8231161821378209]
	TIME [epoch: 3.59 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7226109508716198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7226109508716198 | validation: 0.8922998094201466]
	TIME [epoch: 3.6 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7340445026545498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7340445026545498 | validation: 0.8270419022056219]
	TIME [epoch: 3.59 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.73311773197535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.73311773197535 | validation: 0.9074274938172331]
	TIME [epoch: 3.6 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7448432529082523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7448432529082523 | validation: 0.8579169479220141]
	TIME [epoch: 3.59 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7528862336242037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7528862336242037 | validation: 0.926151181123332]
	TIME [epoch: 3.6 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7990328605804377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7990328605804377 | validation: 1.0947424699785129]
	TIME [epoch: 3.61 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8190528268090462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8190528268090462 | validation: 0.8510749157613813]
	TIME [epoch: 3.61 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7932840096649825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7932840096649825 | validation: 0.9071616742497457]
	TIME [epoch: 3.6 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7177152124028217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7177152124028217 | validation: 0.8806620072229212]
	TIME [epoch: 3.6 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7346419555197576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7346419555197576 | validation: 0.9060910792379627]
	TIME [epoch: 3.6 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7692664242373809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7692664242373809 | validation: 0.8345214798642158]
	TIME [epoch: 3.59 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7664662442393058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7664662442393058 | validation: 1.0715841766868872]
	TIME [epoch: 3.6 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8134129205649324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8134129205649324 | validation: 0.8138314124134723]
	TIME [epoch: 3.59 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.721889679499428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.721889679499428 | validation: 0.8191083184579911]
	TIME [epoch: 3.6 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7013662406914162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7013662406914162 | validation: 0.9009318497861472]
	TIME [epoch: 3.59 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7155132606511131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7155132606511131 | validation: 0.7823765627019915]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_260.pth
	Model improved!!!
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7187589263135868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7187589263135868 | validation: 0.8821048492640327]
	TIME [epoch: 3.61 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.715140667933849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.715140667933849 | validation: 0.8380827444060798]
	TIME [epoch: 3.62 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7336908484728578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7336908484728578 | validation: 0.9551552787353114]
	TIME [epoch: 3.63 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8395832001065148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8395832001065148 | validation: 1.0983876594508561]
	TIME [epoch: 3.63 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8940603724392492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8940603724392492 | validation: 0.9391396673202638]
	TIME [epoch: 3.62 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7817116004471086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7817116004471086 | validation: 0.8361585504442023]
	TIME [epoch: 3.62 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7115295997202188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7115295997202188 | validation: 0.9189714980411241]
	TIME [epoch: 3.62 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7176833468070437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7176833468070437 | validation: 0.8205471404760403]
	TIME [epoch: 3.6 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7250235987979126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7250235987979126 | validation: 0.8284743509353456]
	TIME [epoch: 3.61 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.713155468360003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.713155468360003 | validation: 0.9257817528846926]
	TIME [epoch: 3.6 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7250589608718429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7250589608718429 | validation: 0.7814710172037729]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_271.pth
	Model improved!!!
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7688685319604092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7688685319604092 | validation: 1.0131591801524404]
	TIME [epoch: 3.6 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7915428476357409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7915428476357409 | validation: 0.8698490582637322]
	TIME [epoch: 3.59 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7346056614555359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7346056614555359 | validation: 0.8662993081684027]
	TIME [epoch: 3.6 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7593304343700735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7593304343700735 | validation: 0.9457209116351962]
	TIME [epoch: 3.6 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7464562932513024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7464562932513024 | validation: 0.8768368312351325]
	TIME [epoch: 3.61 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7476495080109783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7476495080109783 | validation: 0.8379195540579407]
	TIME [epoch: 3.6 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7353609656349533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7353609656349533 | validation: 0.9081951072087101]
	TIME [epoch: 3.61 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7379075574624226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7379075574624226 | validation: 0.8156419082226232]
	TIME [epoch: 3.6 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7318191421086467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7318191421086467 | validation: 0.887033757664224]
	TIME [epoch: 3.59 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7338618375228967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7338618375228967 | validation: 0.8446185284166399]
	TIME [epoch: 3.59 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7369020681163948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7369020681163948 | validation: 0.900420745933222]
	TIME [epoch: 3.6 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7485642174827771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7485642174827771 | validation: 0.8711454785190949]
	TIME [epoch: 3.6 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7366230123127349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7366230123127349 | validation: 0.8983435265970254]
	TIME [epoch: 3.59 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7482204844062755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7482204844062755 | validation: 0.8608595387216003]
	TIME [epoch: 3.6 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7338633663410009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7338633663410009 | validation: 0.883986210515761]
	TIME [epoch: 3.59 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7331212029406043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7331212029406043 | validation: 0.8478247940301918]
	TIME [epoch: 3.6 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7238749776659691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7238749776659691 | validation: 0.8896262021771039]
	TIME [epoch: 3.61 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7329676216032011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7329676216032011 | validation: 0.8871476640183042]
	TIME [epoch: 3.62 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7385787231341044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7385787231341044 | validation: 0.9130904347677293]
	TIME [epoch: 3.6 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7616941230906658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7616941230906658 | validation: 0.9228192844268275]
	TIME [epoch: 3.6 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7378989665660831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7378989665660831 | validation: 0.8761520358458548]
	TIME [epoch: 3.6 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7340416425114173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7340416425114173 | validation: 0.8687001631282487]
	TIME [epoch: 3.6 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7171815076810393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7171815076810393 | validation: 0.8819305199013736]
	TIME [epoch: 3.59 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7098573566371728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7098573566371728 | validation: 0.814133708712212]
	TIME [epoch: 3.59 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7231706803694189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7231706803694189 | validation: 1.0542460151626203]
	TIME [epoch: 3.6 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8125193435851084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8125193435851084 | validation: 0.7789614720159928]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_297.pth
	Model improved!!!
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7602304485733987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7602304485733987 | validation: 0.8938374384569242]
	TIME [epoch: 3.62 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6904591838670986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6904591838670986 | validation: 0.8045873943336478]
	TIME [epoch: 3.62 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6788694545306657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6788694545306657 | validation: 0.7554804475526645]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_300.pth
	Model improved!!!
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6837231519204908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6837231519204908 | validation: 0.9866981816255278]
	TIME [epoch: 3.62 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7289382803209864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7289382803209864 | validation: 0.8479073973315345]
	TIME [epoch: 3.6 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7997006212902591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7997006212902591 | validation: 0.951561560601687]
	TIME [epoch: 3.6 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7629881874769601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7629881874769601 | validation: 0.9900203459386031]
	TIME [epoch: 3.6 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7881968976141905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7881968976141905 | validation: 0.8359749794338763]
	TIME [epoch: 3.63 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7226350814216678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7226350814216678 | validation: 0.8412082167001325]
	TIME [epoch: 3.62 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6885421021856464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6885421021856464 | validation: 0.8118346488311776]
	TIME [epoch: 3.62 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6831063258441057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6831063258441057 | validation: 0.8105458929042904]
	TIME [epoch: 3.63 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6818507989464558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6818507989464558 | validation: 0.8206302407351608]
	TIME [epoch: 3.62 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6850478554714693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6850478554714693 | validation: 0.8593262002736364]
	TIME [epoch: 3.62 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7052051255554673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7052051255554673 | validation: 0.9268973765959224]
	TIME [epoch: 3.61 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7796228736642846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7796228736642846 | validation: 1.030439802270518]
	TIME [epoch: 3.62 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.879695121139502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.879695121139502 | validation: 0.8065505861485047]
	TIME [epoch: 3.63 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7210843595071497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7210843595071497 | validation: 0.864816196649743]
	TIME [epoch: 3.63 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6750880535974961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6750880535974961 | validation: 0.7835281185969248]
	TIME [epoch: 3.62 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6691638541930253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6691638541930253 | validation: 0.8814098744687598]
	TIME [epoch: 3.62 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6795999272737533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6795999272737533 | validation: 0.8036399075164493]
	TIME [epoch: 3.62 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6812396214844552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6812396214844552 | validation: 0.8931904248074121]
	TIME [epoch: 3.6 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6948045822145325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6948045822145325 | validation: 0.864465871044698]
	TIME [epoch: 3.63 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.741163828228822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.741163828228822 | validation: 0.9332984591589887]
	TIME [epoch: 3.62 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7641353630121066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7641353630121066 | validation: 1.0011906864690219]
	TIME [epoch: 3.62 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8049052043614691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8049052043614691 | validation: 0.784516118312431]
	TIME [epoch: 3.62 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7471666376443602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7471666376443602 | validation: 0.928936640648959]
	TIME [epoch: 3.62 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7012979649450662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7012979649450662 | validation: 0.768816506990694]
	TIME [epoch: 3.62 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6595256286693467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6595256286693467 | validation: 0.8083276229516955]
	TIME [epoch: 3.62 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.666861755285475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.666861755285475 | validation: 0.8443678149322136]
	TIME [epoch: 3.62 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6810615196055931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6810615196055931 | validation: 0.837772994802696]
	TIME [epoch: 3.63 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.700368531470048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.700368531470048 | validation: 0.9118277563279499]
	TIME [epoch: 3.63 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7747206301062696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7747206301062696 | validation: 0.9977798798033071]
	TIME [epoch: 3.62 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7749522686159387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7749522686159387 | validation: 0.8433289514203033]
	TIME [epoch: 3.62 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7193117465833422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7193117465833422 | validation: 0.7843558648409769]
	TIME [epoch: 3.62 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6545346132031467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6545346132031467 | validation: 0.8533564847973605]
	TIME [epoch: 3.62 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6582339923891176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6582339923891176 | validation: 0.745300491942373]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_333.pth
	Model improved!!!
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6699170802714057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6699170802714057 | validation: 0.9181966848221882]
	TIME [epoch: 3.6 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.678567911416057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.678567911416057 | validation: 0.7256411924071204]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_335.pth
	Model improved!!!
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6953092696166928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6953092696166928 | validation: 0.9687079058424826]
	TIME [epoch: 3.59 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7458280425611704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7458280425611704 | validation: 0.8896859150021179]
	TIME [epoch: 3.6 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7853175987473966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7853175987473966 | validation: 0.9187075776175475]
	TIME [epoch: 3.6 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7954526183626557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7954526183626557 | validation: 0.8496735631042989]
	TIME [epoch: 3.61 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6673764215118294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6673764215118294 | validation: 0.8069406291407646]
	TIME [epoch: 3.6 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6416868067287351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6416868067287351 | validation: 0.7738066926055693]
	TIME [epoch: 3.59 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6339402632874281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6339402632874281 | validation: 0.8071053650106141]
	TIME [epoch: 3.6 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6291250256584026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6291250256584026 | validation: 0.7597303228376865]
	TIME [epoch: 3.6 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6382889804125486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6382889804125486 | validation: 0.8981564938084126]
	TIME [epoch: 3.62 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.668876341114496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.668876341114496 | validation: 0.9151315677319115]
	TIME [epoch: 3.62 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7991955244566576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7991955244566576 | validation: 1.003409218355836]
	TIME [epoch: 3.62 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9308795337397774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9308795337397774 | validation: 1.0428392986043462]
	TIME [epoch: 3.62 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7790240904140435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7790240904140435 | validation: 0.758542291430477]
	TIME [epoch: 3.62 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6545250532913823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6545250532913823 | validation: 0.7996073174461223]
	TIME [epoch: 3.62 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6761719476151001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6761719476151001 | validation: 0.8401576910740878]
	TIME [epoch: 3.62 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.687463984273342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.687463984273342 | validation: 0.7706078658818061]
	TIME [epoch: 3.63 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6405750193389241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6405750193389241 | validation: 0.7720830745269204]
	TIME [epoch: 3.64 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6335499920438025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6335499920438025 | validation: 0.7866246323362472]
	TIME [epoch: 3.62 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.618135919716002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.618135919716002 | validation: 0.7296576125042229]
	TIME [epoch: 3.62 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6282689979674448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6282689979674448 | validation: 0.9579837241819761]
	TIME [epoch: 3.62 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6955031255996784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6955031255996784 | validation: 0.8381795404811048]
	TIME [epoch: 3.62 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8139820225384002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8139820225384002 | validation: 0.8498102315619751]
	TIME [epoch: 3.61 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6843759271403284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6843759271403284 | validation: 1.0108417359597284]
	TIME [epoch: 3.61 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7549478159180231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7549478159180231 | validation: 0.7177322567656832]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_359.pth
	Model improved!!!
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7188319918567098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7188319918567098 | validation: 0.8014986066765432]
	TIME [epoch: 3.6 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6425425027598682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6425425027598682 | validation: 0.7851134310607952]
	TIME [epoch: 3.61 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6243171832788271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6243171832788271 | validation: 0.7272369130911993]
	TIME [epoch: 3.61 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6238899439918953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6238899439918953 | validation: 0.7867100137022588]
	TIME [epoch: 3.61 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6177991701179681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6177991701179681 | validation: 0.739686316626171]
	TIME [epoch: 3.62 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6259415906403704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6259415906403704 | validation: 0.8749471488506577]
	TIME [epoch: 3.63 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7022713085779745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7022713085779745 | validation: 1.0199374796866405]
	TIME [epoch: 3.62 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8864322176692394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8864322176692394 | validation: 0.8248865087482198]
	TIME [epoch: 3.61 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7766293998700851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7766293998700851 | validation: 0.8711587586529275]
	TIME [epoch: 3.61 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6201315748966554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6201315748966554 | validation: 0.753543592794485]
	TIME [epoch: 3.61 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.621585557780578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.621585557780578 | validation: 0.8299242246094325]
	TIME [epoch: 3.61 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6289202091142979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6289202091142979 | validation: 0.8027699696459991]
	TIME [epoch: 3.61 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.61640494380733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.61640494380733 | validation: 0.7387762949889711]
	TIME [epoch: 3.61 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6163062907727365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6163062907727365 | validation: 0.8133212972304911]
	TIME [epoch: 3.6 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6142414525367211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6142414525367211 | validation: 0.7212675069218362]
	TIME [epoch: 3.62 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6563598127880657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6563598127880657 | validation: 0.9147482644594787]
	TIME [epoch: 3.61 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7201781364144649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7201781364144649 | validation: 0.836104529775869]
	TIME [epoch: 3.62 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7118974588363924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7118974588363924 | validation: 0.7963293826545406]
	TIME [epoch: 3.61 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6481258054150217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6481258054150217 | validation: 0.896116983759762]
	TIME [epoch: 3.62 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6248379649023831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6248379649023831 | validation: 0.6769517715869035]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_379.pth
	Model improved!!!
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6128291116322789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6128291116322789 | validation: 0.7781881183021518]
	TIME [epoch: 3.61 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5793873878556595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5793873878556595 | validation: 0.6194058203048703]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_381.pth
	Model improved!!!
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5742742337475822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5742742337475822 | validation: 0.7828243954901037]
	TIME [epoch: 3.61 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5844225689223537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5844225689223537 | validation: 0.6403514061959323]
	TIME [epoch: 3.61 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6442274740717864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6442274740717864 | validation: 0.9139788391082999]
	TIME [epoch: 3.61 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.750795833188342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.750795833188342 | validation: 1.142999913852674]
	TIME [epoch: 3.61 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8507180375345386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8507180375345386 | validation: 0.7015864829733024]
	TIME [epoch: 3.61 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6159752262944794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6159752262944794 | validation: 0.7679790496682146]
	TIME [epoch: 3.61 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.564576086072602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.564576086072602 | validation: 0.760095242419655]
	TIME [epoch: 3.6 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5717438710035213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5717438710035213 | validation: 0.6982877975642384]
	TIME [epoch: 3.62 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5765060779140833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5765060779140833 | validation: 0.7577270272508532]
	TIME [epoch: 3.62 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5983722541253079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5983722541253079 | validation: 0.7955837704396596]
	TIME [epoch: 3.62 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6883573195969122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6883573195969122 | validation: 0.8950374314231191]
	TIME [epoch: 3.61 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7556649846536994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7556649846536994 | validation: 0.8517442880234223]
	TIME [epoch: 3.61 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.588045435765601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.588045435765601 | validation: 0.6769618732478935]
	TIME [epoch: 3.6 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5623562755080974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5623562755080974 | validation: 0.7224623609058627]
	TIME [epoch: 3.61 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5350177538203965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5350177538203965 | validation: 0.6702966167653168]
	TIME [epoch: 3.61 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.533185785756889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.533185785756889 | validation: 0.660147209004824]
	TIME [epoch: 3.61 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5931463869931013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5931463869931013 | validation: 0.8631826750617517]
	TIME [epoch: 3.61 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7017345112734361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7017345112734361 | validation: 0.7785440434831272]
	TIME [epoch: 3.61 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6652147186348605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6652147186348605 | validation: 0.6587077849847367]
	TIME [epoch: 3.61 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5232553745605826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5232553745605826 | validation: 0.7657489028099435]
	TIME [epoch: 3.62 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5189695804250896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5189695804250896 | validation: 0.5631227974516556]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_402.pth
	Model improved!!!
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4916655750402492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4916655750402492 | validation: 0.6530952606245647]
	TIME [epoch: 3.61 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.487269149104349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.487269149104349 | validation: 0.591354990563823]
	TIME [epoch: 3.61 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5057742796725508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5057742796725508 | validation: 0.7102361221106555]
	TIME [epoch: 3.62 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5582833576345604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5582833576345604 | validation: 1.046104442109575]
	TIME [epoch: 3.61 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6955036113736756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6955036113736756 | validation: 0.5958510603177809]
	TIME [epoch: 3.61 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5204146301299079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5204146301299079 | validation: 0.7095913088463072]
	TIME [epoch: 3.62 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48569047341425015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48569047341425015 | validation: 0.578437678991809]
	TIME [epoch: 3.61 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4831583914823696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4831583914823696 | validation: 0.6980913123624968]
	TIME [epoch: 3.62 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5850597060903111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5850597060903111 | validation: 0.9393542860518557]
	TIME [epoch: 3.61 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.650812215758359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.650812215758359 | validation: 0.7734473962110251]
	TIME [epoch: 3.61 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5077720640700444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5077720640700444 | validation: 0.6834818562257386]
	TIME [epoch: 3.61 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4766136821051579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4766136821051579 | validation: 0.5615464548438045]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_414.pth
	Model improved!!!
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3878779875019333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3878779875019333 | validation: 0.5827708058346722]
	TIME [epoch: 3.62 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45182223312753145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45182223312753145 | validation: 0.7977765467093789]
	TIME [epoch: 3.62 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5828828018434666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5828828018434666 | validation: 0.7280255567911831]
	TIME [epoch: 3.61 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49799480913071087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49799480913071087 | validation: 0.5157613479049401]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_418.pth
	Model improved!!!
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40245999105828817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40245999105828817 | validation: 0.6984354742148161]
	TIME [epoch: 3.59 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44638135553232616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44638135553232616 | validation: 0.505818751881029]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_420.pth
	Model improved!!!
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5817385238494656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5817385238494656 | validation: 0.7000297630012299]
	TIME [epoch: 3.61 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40911299950080227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40911299950080227 | validation: 0.5118613578391121]
	TIME [epoch: 3.61 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3375535603290863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3375535603290863 | validation: 0.4906196837972875]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_423.pth
	Model improved!!!
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.332696074652347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.332696074652347 | validation: 0.5751132436092167]
	TIME [epoch: 3.62 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33844115260386204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33844115260386204 | validation: 0.44520317275579235]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_425.pth
	Model improved!!!
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3647996509716562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3647996509716562 | validation: 0.6795070752996897]
	TIME [epoch: 3.63 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4677416868013998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4677416868013998 | validation: 0.7700758810195073]
	TIME [epoch: 3.63 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5090863139872073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5090863139872073 | validation: 0.5290178540610067]
	TIME [epoch: 3.63 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37800810521663053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37800810521663053 | validation: 0.670426548998507]
	TIME [epoch: 3.61 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33987852792146556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33987852792146556 | validation: 0.43445762175907865]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_430.pth
	Model improved!!!
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38385828846943015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38385828846943015 | validation: 0.7564727589311396]
	TIME [epoch: 3.62 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3687982825422765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3687982825422765 | validation: 0.48965604910246996]
	TIME [epoch: 3.61 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2964130751094173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2964130751094173 | validation: 0.5244319080281409]
	TIME [epoch: 3.61 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3714712474782813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3714712474782813 | validation: 0.7424478730386298]
	TIME [epoch: 3.61 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6453308125782019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6453308125782019 | validation: 1.0899657965642688]
	TIME [epoch: 3.62 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.615061427266818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.615061427266818 | validation: 0.6345381980772715]
	TIME [epoch: 3.61 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5391729759842371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5391729759842371 | validation: 0.5511477020767084]
	TIME [epoch: 3.62 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3443331596870011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3443331596870011 | validation: 0.5381307005158942]
	TIME [epoch: 3.62 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3213492941280698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3213492941280698 | validation: 0.4224994098719672]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_439.pth
	Model improved!!!
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3324033294438128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3324033294438128 | validation: 0.6275036202569088]
	TIME [epoch: 3.62 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3875300531939655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3875300531939655 | validation: 0.48120066760359004]
	TIME [epoch: 3.62 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3201224492596571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3201224492596571 | validation: 0.49147817213089334]
	TIME [epoch: 3.61 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2921302652292662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2921302652292662 | validation: 0.5692973373594605]
	TIME [epoch: 3.62 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3153057524256709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3153057524256709 | validation: 0.5095908140481926]
	TIME [epoch: 3.61 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3440266625955597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3440266625955597 | validation: 0.7064816717028526]
	TIME [epoch: 3.62 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.429741121785607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.429741121785607 | validation: 0.5535469872282927]
	TIME [epoch: 3.62 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2948139583087639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2948139583087639 | validation: 0.397357600395684]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_447.pth
	Model improved!!!
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.312560212850035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.312560212850035 | validation: 0.8145490635101416]
	TIME [epoch: 3.61 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41314813909053427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41314813909053427 | validation: 0.40192553986882873]
	TIME [epoch: 3.62 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3202640472768733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3202640472768733 | validation: 0.4841256922718197]
	TIME [epoch: 3.62 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23952143093793596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23952143093793596 | validation: 0.5687210712248802]
	TIME [epoch: 3.63 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3021601840035383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3021601840035383 | validation: 0.7072085143722742]
	TIME [epoch: 3.63 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6113898295097514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6113898295097514 | validation: 0.9146887695338105]
	TIME [epoch: 3.62 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5922394687209522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5922394687209522 | validation: 0.8194648799003016]
	TIME [epoch: 3.61 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4745204204263166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4745204204263166 | validation: 0.5693109459902387]
	TIME [epoch: 3.61 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28505199362517525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28505199362517525 | validation: 0.4317645274309458]
	TIME [epoch: 3.62 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33107157409066956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33107157409066956 | validation: 0.5723699111413623]
	TIME [epoch: 3.62 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27472271077751553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27472271077751553 | validation: 0.43771110617663433]
	TIME [epoch: 3.62 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2766571877357088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2766571877357088 | validation: 0.5754681704198108]
	TIME [epoch: 3.62 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27566213595289973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27566213595289973 | validation: 0.4150988257235124]
	TIME [epoch: 3.62 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32072661726338264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32072661726338264 | validation: 0.6044006968943914]
	TIME [epoch: 3.62 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38765938308510406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38765938308510406 | validation: 0.7163530584970227]
	TIME [epoch: 3.62 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37148061908004404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37148061908004404 | validation: 0.4334248383275279]
	TIME [epoch: 3.62 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2473709571316192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2473709571316192 | validation: 0.4605807395929478]
	TIME [epoch: 3.63 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22984840630933953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22984840630933953 | validation: 0.5288212628268757]
	TIME [epoch: 3.62 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27931733521918406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27931733521918406 | validation: 0.5930380008693866]
	TIME [epoch: 3.62 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42616444112325835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42616444112325835 | validation: 0.8246913866558345]
	TIME [epoch: 3.62 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5258521998978796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5258521998978796 | validation: 0.7673512246675968]
	TIME [epoch: 3.62 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35226598583888413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35226598583888413 | validation: 0.5400914252003295]
	TIME [epoch: 3.62 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4721955488686364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4721955488686364 | validation: 0.5478483256758687]
	TIME [epoch: 3.62 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35143839920166065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35143839920166065 | validation: 0.6600808968417113]
	TIME [epoch: 3.62 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34823017840999376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34823017840999376 | validation: 0.4224133945418734]
	TIME [epoch: 3.62 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2453256170655118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2453256170655118 | validation: 0.4951739126477005]
	TIME [epoch: 3.62 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25222645321689924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25222645321689924 | validation: 0.5315919152479417]
	TIME [epoch: 3.62 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28397110697512234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28397110697512234 | validation: 0.4933062101584422]
	TIME [epoch: 3.62 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26160570201056316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26160570201056316 | validation: 0.47839493101092656]
	TIME [epoch: 3.63 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2592457741643016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2592457741643016 | validation: 0.6503047519615317]
	TIME [epoch: 3.63 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3386076213039978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3386076213039978 | validation: 0.48993119010756286]
	TIME [epoch: 3.63 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3195281986002626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3195281986002626 | validation: 0.6062268281865754]
	TIME [epoch: 3.62 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29731541367597575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29731541367597575 | validation: 0.45673015497464964]
	TIME [epoch: 3.62 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2346900283544801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2346900283544801 | validation: 0.4152564004439766]
	TIME [epoch: 3.62 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2504954860287872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2504954860287872 | validation: 0.6118308211050105]
	TIME [epoch: 3.62 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32596175723549337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32596175723549337 | validation: 0.403532448322523]
	TIME [epoch: 3.62 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2968772097113696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2968772097113696 | validation: 0.5857259722083333]
	TIME [epoch: 3.62 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25087203748038006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25087203748038006 | validation: 0.3814865086940611]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_485.pth
	Model improved!!!
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21350628521971027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21350628521971027 | validation: 0.5651813101550537]
	TIME [epoch: 3.61 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21461147942378753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21461147942378753 | validation: 0.4182114546160849]
	TIME [epoch: 3.61 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26210099283439403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26210099283439403 | validation: 0.8543266970898691]
	TIME [epoch: 3.61 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5305714079004785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5305714079004785 | validation: 0.5676988040659293]
	TIME [epoch: 3.62 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27840367718725995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27840367718725995 | validation: 0.3881223778075614]
	TIME [epoch: 3.61 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25727196806957087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25727196806957087 | validation: 0.6679419430424701]
	TIME [epoch: 3.61 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2723836339365897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2723836339365897 | validation: 0.4249123785216582]
	TIME [epoch: 3.61 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25549123381097005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25549123381097005 | validation: 0.5663084913265036]
	TIME [epoch: 3.6 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2741186124909921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2741186124909921 | validation: 0.5355967201501907]
	TIME [epoch: 3.61 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31282769174398356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31282769174398356 | validation: 0.6130195529214362]
	TIME [epoch: 3.6 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38454329639300894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38454329639300894 | validation: 0.39055042625188024]
	TIME [epoch: 3.61 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1975236760916932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1975236760916932 | validation: 0.5010928742411801]
	TIME [epoch: 3.61 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20262928941503067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20262928941503067 | validation: 0.3804363898854649]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_498.pth
	Model improved!!!
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23831400731302707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23831400731302707 | validation: 0.6171222077455546]
	TIME [epoch: 3.59 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31064768158552936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31064768158552936 | validation: 0.36445674515723836]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_500.pth
	Model improved!!!
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23144501463245867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23144501463245867 | validation: 0.5073090392924205]
	TIME [epoch: 47.9 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21811531494457712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21811531494457712 | validation: 0.5160568739959491]
	TIME [epoch: 7.84 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3748361286401221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3748361286401221 | validation: 0.8311545512744104]
	TIME [epoch: 7.83 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7612484677340652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7612484677340652 | validation: 0.7308748252543988]
	TIME [epoch: 7.84 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46275354472579533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46275354472579533 | validation: 0.6772971740295683]
	TIME [epoch: 7.83 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3126386497906543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3126386497906543 | validation: 0.45369050498414]
	TIME [epoch: 7.83 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27656160425171517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27656160425171517 | validation: 0.4591662370457698]
	TIME [epoch: 7.84 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2654461934707527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2654461934707527 | validation: 0.5952960020998699]
	TIME [epoch: 7.83 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25723642002961367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25723642002961367 | validation: 0.38399464331503474]
	TIME [epoch: 7.84 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2440806320110234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2440806320110234 | validation: 0.4950304201536341]
	TIME [epoch: 7.85 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21859169250809815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21859169250809815 | validation: 0.4159069987139743]
	TIME [epoch: 7.83 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20807295395789716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20807295395789716 | validation: 0.5269325014761744]
	TIME [epoch: 7.84 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23340870268543168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23340870268543168 | validation: 0.4257184583842344]
	TIME [epoch: 7.83 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2049355030660152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2049355030660152 | validation: 0.4801509814494618]
	TIME [epoch: 7.83 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19458998749694353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19458998749694353 | validation: 0.46126125426827064]
	TIME [epoch: 7.84 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1884444589197468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1884444589197468 | validation: 0.6633863366929167]
	TIME [epoch: 7.84 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.574420815223526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.574420815223526 | validation: 0.7553008657299075]
	TIME [epoch: 7.85 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46496407494909375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46496407494909375 | validation: 0.7949360789999006]
	TIME [epoch: 7.83 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35810872958750245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35810872958750245 | validation: 0.39828552590872257]
	TIME [epoch: 7.84 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24520809846665143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24520809846665143 | validation: 0.4419871343362063]
	TIME [epoch: 7.83 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.204180363687724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.204180363687724 | validation: 0.48696731295395435]
	TIME [epoch: 7.84 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19318092984006568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19318092984006568 | validation: 0.43280584914081716]
	TIME [epoch: 7.85 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19590823215296624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19590823215296624 | validation: 0.47803544641381307]
	TIME [epoch: 7.85 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2294776311499342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2294776311499342 | validation: 0.5046041108581659]
	TIME [epoch: 7.83 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24356117642909886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24356117642909886 | validation: 0.43569407077598366]
	TIME [epoch: 7.83 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25683154682267134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25683154682267134 | validation: 0.6121057158054906]
	TIME [epoch: 7.83 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33099835024352375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33099835024352375 | validation: 0.4501375138123578]
	TIME [epoch: 7.84 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17337617537585373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17337617537585373 | validation: 0.4062811864852447]
	TIME [epoch: 7.85 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16509430513022239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16509430513022239 | validation: 0.49254474070289644]
	TIME [epoch: 7.86 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18183002539004936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18183002539004936 | validation: 0.3989140291951174]
	TIME [epoch: 7.83 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22785376230652823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22785376230652823 | validation: 0.6637637682977775]
	TIME [epoch: 7.84 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30883602612327055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30883602612327055 | validation: 0.35703089642652325]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_532.pth
	Model improved!!!
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21001132719870008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21001132719870008 | validation: 0.7225347343031715]
	TIME [epoch: 7.84 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3619359612768717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3619359612768717 | validation: 0.4823888468111081]
	TIME [epoch: 7.83 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1987413249269085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1987413249269085 | validation: 0.46198466918455705]
	TIME [epoch: 7.85 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18304460614916618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18304460614916618 | validation: 0.47503248881378685]
	TIME [epoch: 7.83 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2433519269542878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2433519269542878 | validation: 0.8256153507817509]
	TIME [epoch: 7.83 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46476627486483074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46476627486483074 | validation: 0.396254427505846]
	TIME [epoch: 7.83 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16260562951922236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16260562951922236 | validation: 0.4542136236678716]
	TIME [epoch: 7.84 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18058158613872516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18058158613872516 | validation: 0.4777428572277552]
	TIME [epoch: 7.82 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26942135340517454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26942135340517454 | validation: 0.5130512451055051]
	TIME [epoch: 7.84 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2659427140104592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2659427140104592 | validation: 0.6256871286366407]
	TIME [epoch: 7.85 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.264564573547427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.264564573547427 | validation: 0.44588499673834914]
	TIME [epoch: 7.83 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.327757943574427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.327757943574427 | validation: 0.6731587311348651]
	TIME [epoch: 7.84 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2637298722099284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2637298722099284 | validation: 0.36512252386962646]
	TIME [epoch: 7.83 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1717011638315698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1717011638315698 | validation: 0.43756174903862494]
	TIME [epoch: 7.83 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15183842472056688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15183842472056688 | validation: 0.4389010929233109]
	TIME [epoch: 7.84 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15205005059286456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15205005059286456 | validation: 0.39594881996598774]
	TIME [epoch: 7.85 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17077425746863717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17077425746863717 | validation: 0.5713622139596722]
	TIME [epoch: 7.84 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33162214876245827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33162214876245827 | validation: 0.5837026997043865]
	TIME [epoch: 7.83 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39665584925926295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39665584925926295 | validation: 0.697629180251688]
	TIME [epoch: 7.83 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30274901401759063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30274901401759063 | validation: 0.39812207041417697]
	TIME [epoch: 7.84 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1598512016535389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1598512016535389 | validation: 0.5824435634342571]
	TIME [epoch: 7.83 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21784050325004323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21784050325004323 | validation: 0.34063060048226124]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_554.pth
	Model improved!!!
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2878580674972616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2878580674972616 | validation: 0.5316928677321385]
	TIME [epoch: 7.83 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19633218593270996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19633218593270996 | validation: 0.43861739462265686]
	TIME [epoch: 7.83 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1589328986537181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1589328986537181 | validation: 0.41645002653322366]
	TIME [epoch: 7.84 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15619720905211965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15619720905211965 | validation: 0.6410562384039805]
	TIME [epoch: 7.83 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24437270872269978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24437270872269978 | validation: 0.4676100533829914]
	TIME [epoch: 7.84 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3149236675932807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3149236675932807 | validation: 0.759035657861356]
	TIME [epoch: 7.84 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43325741643750787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43325741643750787 | validation: 0.7993259266654724]
	TIME [epoch: 7.85 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3638039296073324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3638039296073324 | validation: 0.39465236861629593]
	TIME [epoch: 7.84 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23919772065818237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23919772065818237 | validation: 0.4458489462140345]
	TIME [epoch: 7.84 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2028934203916471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2028934203916471 | validation: 0.5414583535631918]
	TIME [epoch: 7.83 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2015120295180571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2015120295180571 | validation: 0.40564813684696194]
	TIME [epoch: 7.83 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24322468194957292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24322468194957292 | validation: 0.4256553340470882]
	TIME [epoch: 7.84 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18294965651216674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18294965651216674 | validation: 0.6153171618222133]
	TIME [epoch: 7.86 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21076210713224253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21076210713224253 | validation: 1.0602307656939625]
	TIME [epoch: 7.84 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1437535590212087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1437535590212087 | validation: 0.3657770144329162]
	TIME [epoch: 7.83 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5571047785261558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5571047785261558 | validation: 0.5103445519595644]
	TIME [epoch: 7.84 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.270520475479138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.270520475479138 | validation: 0.7546839595992456]
	TIME [epoch: 7.86 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3546680272237876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3546680272237876 | validation: 0.522043454467517]
	TIME [epoch: 7.83 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26345524207924775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26345524207924775 | validation: 0.42876529194760693]
	TIME [epoch: 7.85 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19633617015580107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19633617015580107 | validation: 0.41865573103427]
	TIME [epoch: 7.82 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19763190476519868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19763190476519868 | validation: 0.4508543292095129]
	TIME [epoch: 7.83 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16801199750178206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16801199750178206 | validation: 0.45416376743093567]
	TIME [epoch: 7.83 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16042179947552992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16042179947552992 | validation: 0.42823520721905173]
	TIME [epoch: 7.83 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16814451169528383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16814451169528383 | validation: 0.4307896660229001]
	TIME [epoch: 7.82 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15295717221023303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15295717221023303 | validation: 0.40268995827814397]
	TIME [epoch: 7.84 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14996028897026056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14996028897026056 | validation: 0.41265951798910105]
	TIME [epoch: 7.83 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14246140866352516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14246140866352516 | validation: 0.3975456984080077]
	TIME [epoch: 7.83 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13976961790080977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13976961790080977 | validation: 0.4506686698657057]
	TIME [epoch: 7.82 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14196406175819196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14196406175819196 | validation: 0.3662300748665477]
	TIME [epoch: 7.84 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16691828175550927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16691828175550927 | validation: 0.5844570001067261]
	TIME [epoch: 7.83 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20352468741293517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20352468741293517 | validation: 0.3974569213499253]
	TIME [epoch: 7.83 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23824447896309506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23824447896309506 | validation: 0.5255772183686712]
	TIME [epoch: 7.85 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21132503318487314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21132503318487314 | validation: 0.5940221391810664]
	TIME [epoch: 7.84 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26202563951404473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26202563951404473 | validation: 0.5433821088998433]
	TIME [epoch: 7.84 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3646907626788578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3646907626788578 | validation: 0.7464499705226926]
	TIME [epoch: 7.83 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42376994660085543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42376994660085543 | validation: 0.6411064023217838]
	TIME [epoch: 7.83 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2232958699777676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2232958699777676 | validation: 0.4319402236132316]
	TIME [epoch: 7.83 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2881296873108562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2881296873108562 | validation: 0.5325138274281501]
	TIME [epoch: 7.85 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2561215927271834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2561215927271834 | validation: 0.5798243154193264]
	TIME [epoch: 7.84 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21002824482664054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21002824482664054 | validation: 0.3733987817376111]
	TIME [epoch: 7.82 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20511215596462143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20511215596462143 | validation: 0.4616954214679522]
	TIME [epoch: 7.85 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1679857865755578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1679857865755578 | validation: 0.4798751073795011]
	TIME [epoch: 7.84 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17619708716149352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17619708716149352 | validation: 0.38385427081162443]
	TIME [epoch: 7.84 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20573341884062324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20573341884062324 | validation: 0.46174946784292087]
	TIME [epoch: 7.85 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22066310209073386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22066310209073386 | validation: 0.48354615321205324]
	TIME [epoch: 7.84 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16891339133625874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16891339133625874 | validation: 0.35975010350178543]
	TIME [epoch: 7.84 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1434836880675476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1434836880675476 | validation: 0.4725580664722795]
	TIME [epoch: 7.84 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13685032334465116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13685032334465116 | validation: 0.40445093673700383]
	TIME [epoch: 7.83 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13452300860925373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13452300860925373 | validation: 0.4292693137032605]
	TIME [epoch: 7.82 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1453604875669151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1453604875669151 | validation: 0.4514010993874344]
	TIME [epoch: 7.85 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2245996074710232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2245996074710232 | validation: 0.6881682715604858]
	TIME [epoch: 7.84 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34694120981975035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34694120981975035 | validation: 0.5586626157197921]
	TIME [epoch: 7.82 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44796369155669324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44796369155669324 | validation: 0.6284065281247386]
	TIME [epoch: 7.85 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23900144452915892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23900144452915892 | validation: 0.3739411431441142]
	TIME [epoch: 7.85 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15573388503823613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15573388503823613 | validation: 0.3909778234570075]
	TIME [epoch: 7.84 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14600946497390258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14600946497390258 | validation: 0.46696491833228926]
	TIME [epoch: 7.84 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1835688832071107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1835688832071107 | validation: 0.5595614193433176]
	TIME [epoch: 7.85 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3012737634289604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3012737634289604 | validation: 0.3934554552314229]
	TIME [epoch: 7.85 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1471509925317133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1471509925317133 | validation: 0.37986583125687895]
	TIME [epoch: 7.83 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1459856695147683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1459856695147683 | validation: 0.451326450308673]
	TIME [epoch: 7.84 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15062868615291142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15062868615291142 | validation: 0.3239702794299476]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_615.pth
	Model improved!!!
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14155653271820015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14155653271820015 | validation: 0.44636228401226924]
	TIME [epoch: 7.85 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1312840884900006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1312840884900006 | validation: 0.36983919695308665]
	TIME [epoch: 7.85 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12456077518344838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12456077518344838 | validation: 0.4164786973001638]
	TIME [epoch: 7.84 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13030931546480834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13030931546480834 | validation: 0.4264231486565045]
	TIME [epoch: 7.84 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16850975885393635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16850975885393635 | validation: 1.4775149728466186]
	TIME [epoch: 7.84 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6871853102182934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6871853102182934 | validation: 0.8001272744322763]
	TIME [epoch: 7.85 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9748707839365564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9748707839365564 | validation: 0.43697123727901444]
	TIME [epoch: 7.83 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3163784690269785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3163784690269785 | validation: 0.9064282256078084]
	TIME [epoch: 7.85 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.430878939337177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.430878939337177 | validation: 0.626686154870726]
	TIME [epoch: 7.85 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23264226916093236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23264226916093236 | validation: 0.3722627361269191]
	TIME [epoch: 7.83 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2426656744619327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2426656744619327 | validation: 0.3837736024125792]
	TIME [epoch: 7.84 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16393516226135782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16393516226135782 | validation: 0.4863977872590288]
	TIME [epoch: 7.88 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1670733249236165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1670733249236165 | validation: 0.46302823447839175]
	TIME [epoch: 7.83 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15280855701650262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15280855701650262 | validation: 0.36757602583651805]
	TIME [epoch: 7.84 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17784418589439277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17784418589439277 | validation: 0.4618396698981716]
	TIME [epoch: 7.84 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17178683679187517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17178683679187517 | validation: 0.48194479881244967]
	TIME [epoch: 7.83 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1875615123273574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1875615123273574 | validation: 0.4047993108630495]
	TIME [epoch: 7.82 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14247741129869856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14247741129869856 | validation: 0.36148705979103396]
	TIME [epoch: 7.85 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1323290044974801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1323290044974801 | validation: 0.44911469442719487]
	TIME [epoch: 7.83 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12915393515744394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12915393515744394 | validation: 0.373887237208589]
	TIME [epoch: 7.83 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12657602973730567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12657602973730567 | validation: 0.5030350294413671]
	TIME [epoch: 7.87 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21131690077895016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21131690077895016 | validation: 0.4897897678519876]
	TIME [epoch: 7.88 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.272257733754006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.272257733754006 | validation: 0.6314160220533918]
	TIME [epoch: 7.87 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2552996338488738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2552996338488738 | validation: 0.3976987726602311]
	TIME [epoch: 7.85 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1218994636538837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1218994636538837 | validation: 0.3962085584724081]
	TIME [epoch: 7.85 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1876468584399209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1876468584399209 | validation: 0.7273932793524781]
	TIME [epoch: 7.85 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31864120171548926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31864120171548926 | validation: 0.5052941969608655]
	TIME [epoch: 7.83 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14949370118764962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14949370118764962 | validation: 0.4482087254877398]
	TIME [epoch: 7.87 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28933832971404605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28933832971404605 | validation: 0.6915866731880899]
	TIME [epoch: 7.83 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31772676496591884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31772676496591884 | validation: 0.43031367654718217]
	TIME [epoch: 7.86 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14774305722666634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14774305722666634 | validation: 0.34001064121471325]
	TIME [epoch: 7.83 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14301092005729282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14301092005729282 | validation: 0.4754547568634541]
	TIME [epoch: 7.86 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13847136730256257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13847136730256257 | validation: 0.3474887756423717]
	TIME [epoch: 7.83 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1230130334650558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1230130334650558 | validation: 0.3727596611639168]
	TIME [epoch: 7.86 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1225437779934654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1225437779934654 | validation: 0.4270428621328205]
	TIME [epoch: 7.84 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16329688841596107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16329688841596107 | validation: 0.4760297747497216]
	TIME [epoch: 7.84 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2528084404230107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2528084404230107 | validation: 0.5570149419491329]
	TIME [epoch: 7.84 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24457915216900852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24457915216900852 | validation: 0.4413894362352999]
	TIME [epoch: 7.86 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1671012869257322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1671012869257322 | validation: 0.39083095951839264]
	TIME [epoch: 7.85 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12579913127687845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12579913127687845 | validation: 0.4118371499682786]
	TIME [epoch: 7.87 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14554290962360383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14554290962360383 | validation: 0.41081221146159086]
	TIME [epoch: 7.86 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16297852960020934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16297852960020934 | validation: 0.40613866283610545]
	TIME [epoch: 7.85 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14244737755116288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14244737755116288 | validation: 0.3636789552460271]
	TIME [epoch: 7.85 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12224598967736029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12224598967736029 | validation: 0.5123964990896489]
	TIME [epoch: 7.85 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19444852862465709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19444852862465709 | validation: 0.48938410056867676]
	TIME [epoch: 7.84 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.318078290234093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.318078290234093 | validation: 0.9789767583925482]
	TIME [epoch: 7.86 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5458872837766733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5458872837766733 | validation: 0.49920587426554]
	TIME [epoch: 7.85 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45689784799837646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45689784799837646 | validation: 0.4821590964422462]
	TIME [epoch: 7.86 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3541643637031093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3541643637031093 | validation: 0.5172492154569897]
	TIME [epoch: 7.84 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17725663294936048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17725663294936048 | validation: 0.5928687077932555]
	TIME [epoch: 7.84 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1903498147391235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1903498147391235 | validation: 0.4385487665768615]
	TIME [epoch: 7.83 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13945238789158373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13945238789158373 | validation: 0.3861463171031425]
	TIME [epoch: 7.82 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14820260967940632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14820260967940632 | validation: 0.3864205982830312]
	TIME [epoch: 7.82 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1342695492733712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1342695492733712 | validation: 0.4342338170725837]
	TIME [epoch: 7.8 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.128985832573051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.128985832573051 | validation: 0.4189513062859713]
	TIME [epoch: 7.8 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12474045983063171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12474045983063171 | validation: 0.402444953774153]
	TIME [epoch: 7.83 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12425307375363519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12425307375363519 | validation: 0.4008613678845456]
	TIME [epoch: 7.8 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1190363688950038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1190363688950038 | validation: 0.42886962253243555]
	TIME [epoch: 7.83 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12121895385670159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12121895385670159 | validation: 0.3841492410071255]
	TIME [epoch: 7.88 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12012461525750084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12012461525750084 | validation: 0.41930250467118263]
	TIME [epoch: 7.86 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1332200617970829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1332200617970829 | validation: 0.4504141875651696]
	TIME [epoch: 7.87 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1705287545514397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1705287545514397 | validation: 0.503303653311567]
	TIME [epoch: 7.82 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17632138137738157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17632138137738157 | validation: 0.42959700984226185]
	TIME [epoch: 7.85 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1697494079120732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1697494079120732 | validation: 0.4188247485793941]
	TIME [epoch: 7.85 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11290001944862744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11290001944862744 | validation: 0.3897771361698623]
	TIME [epoch: 7.88 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11373760552439667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11373760552439667 | validation: 0.42144190351114513]
	TIME [epoch: 7.86 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11135956032938336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11135956032938336 | validation: 0.38981895442413494]
	TIME [epoch: 7.87 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12410714686949202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12410714686949202 | validation: 0.4263576695300172]
	TIME [epoch: 7.81 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14963857075689282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14963857075689282 | validation: 0.4091216524646788]
	TIME [epoch: 7.86 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17212377526579217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17212377526579217 | validation: 0.45986044394492903]
	TIME [epoch: 7.85 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1950892577869403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1950892577869403 | validation: 0.5416748559126611]
	TIME [epoch: 7.87 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14529093824471792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14529093824471792 | validation: 0.3513983939186958]
	TIME [epoch: 7.86 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17427145154835466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17427145154835466 | validation: 0.6268689652624704]
	TIME [epoch: 7.86 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18564720090224426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18564720090224426 | validation: 0.3566070916075499]
	TIME [epoch: 7.78 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15810687113235897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15810687113235897 | validation: 0.5393260644921747]
	TIME [epoch: 7.88 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1697391228946364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1697391228946364 | validation: 0.4042598233402004]
	TIME [epoch: 7.86 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13565285507423955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13565285507423955 | validation: 0.3554791079038925]
	TIME [epoch: 7.88 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10556712960627912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10556712960627912 | validation: 0.3883986257510289]
	TIME [epoch: 7.86 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11086920082992378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11086920082992378 | validation: 0.36677590825277634]
	TIME [epoch: 7.85 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13967984353271032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13967984353271032 | validation: 0.5805614237103626]
	TIME [epoch: 7.83 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19347998934980656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19347998934980656 | validation: 0.4426381221789164]
	TIME [epoch: 7.86 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2259341577594922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2259341577594922 | validation: 0.6863086015455544]
	TIME [epoch: 7.82 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25055966014246034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25055966014246034 | validation: 0.4862627500961285]
	TIME [epoch: 7.88 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16449595938978326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16449595938978326 | validation: 0.36490772436367547]
	TIME [epoch: 7.86 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20951002229789328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20951002229789328 | validation: 0.5125622083814904]
	TIME [epoch: 7.9 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14380389380480407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14380389380480407 | validation: 0.41447425503753926]
	TIME [epoch: 7.86 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2039351015461814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2039351015461814 | validation: 0.4250146627709812]
	TIME [epoch: 7.87 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20664972916914764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20664972916914764 | validation: 0.6416922173844126]
	TIME [epoch: 7.86 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29647433598924355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29647433598924355 | validation: 0.46841219289744895]
	TIME [epoch: 7.85 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16535137872850314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16535137872850314 | validation: 0.35213453784862747]
	TIME [epoch: 7.85 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15310071701241867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15310071701241867 | validation: 0.541860876505695]
	TIME [epoch: 7.87 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18303863135338025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18303863135338025 | validation: 0.4318579785056247]
	TIME [epoch: 7.86 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15272377283686558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15272377283686558 | validation: 0.34258279963986055]
	TIME [epoch: 7.85 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11765193838029696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11765193838029696 | validation: 0.40943473174763007]
	TIME [epoch: 7.85 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10908165093878161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10908165093878161 | validation: 0.3586447191080693]
	TIME [epoch: 7.83 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11546812310827037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11546812310827037 | validation: 0.3519341904584466]
	TIME [epoch: 7.85 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11607199631409426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11607199631409426 | validation: 0.4065829577734068]
	TIME [epoch: 7.88 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11886677550406939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11886677550406939 | validation: 0.32794767557747867]
	TIME [epoch: 7.83 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10637010645625122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10637010645625122 | validation: 0.41709576516528774]
	TIME [epoch: 7.85 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10449575764496072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10449575764496072 | validation: 0.31387253378320085]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_715.pth
	Model improved!!!
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11817211598889774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11817211598889774 | validation: 0.6870699266295244]
	TIME [epoch: 7.83 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22099351878558346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22099351878558346 | validation: 0.5475509323034141]
	TIME [epoch: 7.86 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41705544180597254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41705544180597254 | validation: 0.8336451739031965]
	TIME [epoch: 7.86 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6568629882076118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6568629882076118 | validation: 1.070366331423619]
	TIME [epoch: 7.85 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7840013545901249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7840013545901249 | validation: 1.120933313134193]
	TIME [epoch: 7.85 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6191065554872321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6191065554872321 | validation: 0.9340377495991938]
	TIME [epoch: 7.85 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5490068255392554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5490068255392554 | validation: 0.5935924301948206]
	TIME [epoch: 7.85 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3811968490333329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3811968490333329 | validation: 0.4766748470288076]
	TIME [epoch: 7.84 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18049024186386553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18049024186386553 | validation: 1.1349028311548837]
	TIME [epoch: 7.87 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3456860225811085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3456860225811085 | validation: 1.0409469694840632]
	TIME [epoch: 7.87 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2893914560020472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2893914560020472 | validation: 0.6411261493830889]
	TIME [epoch: 7.85 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.805704731426598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.805704731426598 | validation: 0.3439966630782599]
	TIME [epoch: 7.86 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3603897521910738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3603897521910738 | validation: 0.5302912801558683]
	TIME [epoch: 7.86 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24219616741105376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24219616741105376 | validation: 0.5926249033347697]
	TIME [epoch: 7.86 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2312725269835853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2312725269835853 | validation: 0.47570945335092313]
	TIME [epoch: 7.87 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21559423698421631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21559423698421631 | validation: 0.41777660276203826]
	TIME [epoch: 7.87 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.184869267639396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.184869267639396 | validation: 0.39461192004792617]
	TIME [epoch: 7.83 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16872645671639838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16872645671639838 | validation: 0.40263627111457795]
	TIME [epoch: 7.84 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16000706941660575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16000706941660575 | validation: 0.3890737695916082]
	TIME [epoch: 7.84 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15204649533997766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15204649533997766 | validation: 0.3815906116309149]
	TIME [epoch: 7.83 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14802596452412126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14802596452412126 | validation: 0.3987568040299361]
	TIME [epoch: 7.86 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13858377621747206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13858377621747206 | validation: 0.38527056137866245]
	TIME [epoch: 7.85 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13613693591560566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13613693591560566 | validation: 0.3823140658212031]
	TIME [epoch: 7.83 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13426903561163447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13426903561163447 | validation: 0.38625681836356907]
	TIME [epoch: 7.84 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13272739030589914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13272739030589914 | validation: 0.3709805192164797]
	TIME [epoch: 7.83 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14990553544748755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14990553544748755 | validation: 0.42331008741383]
	TIME [epoch: 7.85 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1616724033912832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1616724033912832 | validation: 0.3893129967417774]
	TIME [epoch: 7.84 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15201505568957585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15201505568957585 | validation: 0.4530014691845827]
	TIME [epoch: 7.85 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14090322772215585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14090322772215585 | validation: 0.3494397623736859]
	TIME [epoch: 7.84 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11889244760671261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11889244760671261 | validation: 0.3925510429031045]
	TIME [epoch: 7.84 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11474071552902831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11474071552902831 | validation: 0.3585743212034158]
	TIME [epoch: 7.84 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10891760585660595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10891760585660595 | validation: 0.3785470715751369]
	TIME [epoch: 7.85 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10922077989712864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10922077989712864 | validation: 0.3459754631667571]
	TIME [epoch: 7.83 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11120258395941493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11120258395941493 | validation: 0.5093225728469531]
	TIME [epoch: 7.84 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14247994425643176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14247994425643176 | validation: 0.457436115950183]
	TIME [epoch: 7.85 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2846509837824407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2846509837824407 | validation: 0.7164054945710134]
	TIME [epoch: 7.83 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3512665427881582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3512665427881582 | validation: 0.7386478879856662]
	TIME [epoch: 7.85 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3060009436597029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3060009436597029 | validation: 0.3480695023405988]
	TIME [epoch: 7.83 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17100728376159438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17100728376159438 | validation: 0.3648886594330163]
	TIME [epoch: 7.83 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18647687000857738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18647687000857738 | validation: 0.4049061517431707]
	TIME [epoch: 7.83 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12727374274295689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12727374274295689 | validation: 0.3618642413285535]
	TIME [epoch: 7.85 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11709400734904493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11709400734904493 | validation: 0.3309872651865111]
	TIME [epoch: 7.85 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1135450378481385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1135450378481385 | validation: 0.3722781367463117]
	TIME [epoch: 7.84 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10860840673552985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10860840673552985 | validation: 0.31870734302368275]
	TIME [epoch: 7.86 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11022638398602745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11022638398602745 | validation: 0.4420475617757797]
	TIME [epoch: 7.84 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13805270580497603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13805270580497603 | validation: 0.5296089280921604]
	TIME [epoch: 7.85 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.339954840064663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.339954840064663 | validation: 0.9041260910680834]
	TIME [epoch: 7.87 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5181015066014458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5181015066014458 | validation: 1.2596951788949564]
	TIME [epoch: 7.86 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7269603127443303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7269603127443303 | validation: 0.4279061599703653]
	TIME [epoch: 7.85 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15573688380527095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15573688380527095 | validation: 0.4309945968318336]
	TIME [epoch: 7.84 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26498196241731836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26498196241731836 | validation: 0.38978752638807634]
	TIME [epoch: 7.84 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14023894081134763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14023894081134763 | validation: 0.4913651976938851]
	TIME [epoch: 7.84 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15579822107372437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15579822107372437 | validation: 0.4195506318612065]
	TIME [epoch: 7.86 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13434907766268964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13434907766268964 | validation: 0.33898886379610654]
	TIME [epoch: 7.86 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1334519114823689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1334519114823689 | validation: 0.3674260411100023]
	TIME [epoch: 7.84 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1289311556138872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1289311556138872 | validation: 0.3917374875165331]
	TIME [epoch: 7.84 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.130946203845839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.130946203845839 | validation: 0.412059567761598]
	TIME [epoch: 7.84 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13252639926398538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13252639926398538 | validation: 0.36819368810565917]
	TIME [epoch: 7.83 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14425868991325005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14425868991325005 | validation: 0.4306591922896674]
	TIME [epoch: 7.84 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13267980901418683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13267980901418683 | validation: 0.3658563062145779]
	TIME [epoch: 7.87 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12933849676156048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12933849676156048 | validation: 0.3899403375227795]
	TIME [epoch: 7.84 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11475962287688757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11475962287688757 | validation: 0.37663359744284136]
	TIME [epoch: 7.83 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1289812512182652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1289812512182652 | validation: 0.5119519315701541]
	TIME [epoch: 7.84 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4113906263824022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4113906263824022 | validation: 0.3584473973705531]
	TIME [epoch: 7.83 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20769970209407337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20769970209407337 | validation: 0.4873667222414689]
	TIME [epoch: 7.85 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2801838583325641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2801838583325641 | validation: 0.5233006439533926]
	TIME [epoch: 7.86 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15161136506717407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15161136506717407 | validation: 0.3213095139272019]
	TIME [epoch: 7.84 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17514898715261712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17514898715261712 | validation: 0.3516773643601844]
	TIME [epoch: 7.84 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1090002563858601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1090002563858601 | validation: 0.39908370180826536]
	TIME [epoch: 7.83 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11772285513908544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11772285513908544 | validation: 0.31147693138933663]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_785.pth
	Model improved!!!
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10722174631351379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10722174631351379 | validation: 0.39242187616244084]
	TIME [epoch: 7.84 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09990076846313359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09990076846313359 | validation: 0.3336731392807213]
	TIME [epoch: 7.86 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0991971481190906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0991971481190906 | validation: 0.6229736485395965]
	TIME [epoch: 7.86 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19633329387856394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19633329387856394 | validation: 0.5277765222263339]
	TIME [epoch: 7.84 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3798871406121604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3798871406121604 | validation: 0.7760842426716388]
	TIME [epoch: 7.84 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4372678547673035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4372678547673035 | validation: 0.6727447529840096]
	TIME [epoch: 7.84 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24345123348634254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24345123348634254 | validation: 0.7117962657776671]
	TIME [epoch: 7.83 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6066690024993714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6066690024993714 | validation: 0.4502422142863498]
	TIME [epoch: 7.85 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2165951024321783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2165951024321783 | validation: 0.5858930547450697]
	TIME [epoch: 7.85 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22075063073964285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22075063073964285 | validation: 0.3323031751732627]
	TIME [epoch: 7.84 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12978765414584648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12978765414584648 | validation: 0.33721549454453603]
	TIME [epoch: 7.85 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1467087194972808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1467087194972808 | validation: 0.37556844978969756]
	TIME [epoch: 7.85 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11948009639166256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11948009639166256 | validation: 0.4390679633259879]
	TIME [epoch: 7.85 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12225433994533784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12225433994533784 | validation: 0.3629228588630395]
	TIME [epoch: 7.84 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10817036770362096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10817036770362096 | validation: 0.345744131045019]
	TIME [epoch: 7.87 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10743491698067899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10743491698067899 | validation: 0.33200530406041684]
	TIME [epoch: 7.84 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10961517743979796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10961517743979796 | validation: 0.45606326303192257]
	TIME [epoch: 7.85 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13251589553730428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13251589553730428 | validation: 0.38977494179689015]
	TIME [epoch: 7.84 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16834978905844747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16834978905844747 | validation: 0.3506938159129452]
	TIME [epoch: 7.86 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12578502833941885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12578502833941885 | validation: 0.39765800094048304]
	TIME [epoch: 7.86 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1101157558947613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1101157558947613 | validation: 0.3339664713752808]
	TIME [epoch: 7.86 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10493733564513495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10493733564513495 | validation: 0.33670148343547196]
	TIME [epoch: 7.85 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10755842758103926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10755842758103926 | validation: 0.40567053535732517]
	TIME [epoch: 7.84 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10424626054117496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10424626054117496 | validation: 0.31773444427045255]
	TIME [epoch: 7.85 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14391593406755757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14391593406755757 | validation: 0.7276729029003984]
	TIME [epoch: 7.86 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3025907111110295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3025907111110295 | validation: 0.7981662816136379]
	TIME [epoch: 7.9 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8708198251441996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8708198251441996 | validation: 0.2777729644204301]
	TIME [epoch: 7.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_812.pth
	Model improved!!!
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2622137362742901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2622137362742901 | validation: 0.4890891748285096]
	TIME [epoch: 7.8 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15873452111339842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15873452111339842 | validation: 0.48462402757638756]
	TIME [epoch: 7.79 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16284900348835113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16284900348835113 | validation: 0.30457934690471444]
	TIME [epoch: 7.79 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11030048203456946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11030048203456946 | validation: 0.28295324391419097]
	TIME [epoch: 7.81 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12228492004394301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12228492004394301 | validation: 0.29620619881841564]
	TIME [epoch: 7.79 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10431323264044126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10431323264044126 | validation: 0.3543804028749093]
	TIME [epoch: 7.78 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10780597080066555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10780597080066555 | validation: 0.3186206820924574]
	TIME [epoch: 7.8 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09601655598592793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09601655598592793 | validation: 0.31409566246236464]
	TIME [epoch: 7.78 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09859903533443054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09859903533443054 | validation: 0.30672496541447714]
	TIME [epoch: 7.84 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1084990868761296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1084990868761296 | validation: 0.396558186642885]
	TIME [epoch: 7.84 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12971468461259594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12971468461259594 | validation: 0.37134449004744596]
	TIME [epoch: 7.87 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19218100490320922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19218100490320922 | validation: 0.38190174327760984]
	TIME [epoch: 7.86 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15467577616908285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15467577616908285 | validation: 0.3322530889840536]
	TIME [epoch: 7.82 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11820357337153682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11820357337153682 | validation: 0.30103182053646466]
	TIME [epoch: 7.84 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0913856671251646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0913856671251646 | validation: 0.3420741479313513]
	TIME [epoch: 7.85 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08934651320762686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08934651320762686 | validation: 0.3187934044345142]
	TIME [epoch: 7.84 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09577964724612702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09577964724612702 | validation: 0.39113202061752295]
	TIME [epoch: 7.85 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11101031146962223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11101031146962223 | validation: 0.3177865078490922]
	TIME [epoch: 7.85 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1564485612662125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1564485612662125 | validation: 0.6158491796964579]
	TIME [epoch: 7.85 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2475719979923001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2475719979923001 | validation: 0.3260493327191458]
	TIME [epoch: 7.87 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16740667566037665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16740667566037665 | validation: 0.47582944790934606]
	TIME [epoch: 7.84 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11839215741071274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11839215741071274 | validation: 0.3203577369831817]
	TIME [epoch: 7.85 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11616249314134043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11616249314134043 | validation: 0.5203414286163508]
	TIME [epoch: 7.84 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15120322449735646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15120322449735646 | validation: 0.28093654970880805]
	TIME [epoch: 7.85 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11979057709686793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11979057709686793 | validation: 0.4300273931565295]
	TIME [epoch: 7.85 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10253865470790605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10253865470790605 | validation: 0.3115564510939137]
	TIME [epoch: 7.81 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10905241168080636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10905241168080636 | validation: 0.36123174079322684]
	TIME [epoch: 7.82 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11724585356374963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11724585356374963 | validation: 0.3593931630608081]
	TIME [epoch: 7.85 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18109894209413555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18109894209413555 | validation: 0.7031970078989855]
	TIME [epoch: 7.84 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26623015195446836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26623015195446836 | validation: 0.5294248439438637]
	TIME [epoch: 7.85 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14151683405261722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14151683405261722 | validation: 0.6359270998128425]
	TIME [epoch: 7.84 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.543412028362218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.543412028362218 | validation: 0.5401633334575239]
	TIME [epoch: 7.87 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2739887192303922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2739887192303922 | validation: 0.6434082427948948]
	TIME [epoch: 7.85 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26403947016186097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26403947016186097 | validation: 0.36657705646652555]
	TIME [epoch: 7.86 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14539524998456496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14539524998456496 | validation: 0.3269610841957952]
	TIME [epoch: 7.81 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13999887786882986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13999887786882986 | validation: 0.4252631147536655]
	TIME [epoch: 7.84 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11387319660885561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11387319660885561 | validation: 0.39810784680978845]
	TIME [epoch: 7.83 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10465153746910649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10465153746910649 | validation: 0.3031661801634333]
	TIME [epoch: 7.85 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09738049016440287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09738049016440287 | validation: 0.32848534900958415]
	TIME [epoch: 7.84 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09286671486642988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09286671486642988 | validation: 0.3122315790025105]
	TIME [epoch: 7.85 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1008783450697985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1008783450697985 | validation: 0.3298095767357608]
	TIME [epoch: 7.83 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13292826306082556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13292826306082556 | validation: 0.39268173074246504]
	TIME [epoch: 7.86 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13997155610493844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13997155610493844 | validation: 0.3334714198837728]
	TIME [epoch: 7.83 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13950226986828193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13950226986828193 | validation: 0.30983965103287164]
	TIME [epoch: 7.86 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08853373491861298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08853373491861298 | validation: 0.39891272114197784]
	TIME [epoch: 7.85 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08854526280739172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08854526280739172 | validation: 0.3071595734480044]
	TIME [epoch: 7.85 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08912718569145031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08912718569145031 | validation: 0.3288467911205415]
	TIME [epoch: 7.83 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0995350623470404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0995350623470404 | validation: 0.33833624965615544]
	TIME [epoch: 7.83 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1492060242388853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1492060242388853 | validation: 0.4556414816843751]
	TIME [epoch: 7.86 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16054422812392147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16054422812392147 | validation: 0.30091335979011785]
	TIME [epoch: 7.82 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1307944238316698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1307944238316698 | validation: 0.5458907531431653]
	TIME [epoch: 7.88 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1233116482220353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1233116482220353 | validation: 0.3347559966540453]
	TIME [epoch: 7.85 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19749752676467405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19749752676467405 | validation: 0.678481473025045]
	TIME [epoch: 7.85 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21651068927905692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21651068927905692 | validation: 0.35688196723939086]
	TIME [epoch: 7.82 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11522239778068138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11522239778068138 | validation: 0.26839831992819424]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_867.pth
	Model improved!!!
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10431440675277594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10431440675277594 | validation: 0.2911008170312276]
	TIME [epoch: 7.85 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10643066338083639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10643066338083639 | validation: 0.620659935296123]
	TIME [epoch: 7.9 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17892446618823735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17892446618823735 | validation: 0.38513700391092576]
	TIME [epoch: 7.92 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21952107917099825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21952107917099825 | validation: 0.5574509479381321]
	TIME [epoch: 7.87 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18243997726477174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18243997726477174 | validation: 0.4385446065201144]
	TIME [epoch: 7.89 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12113518964059833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12113518964059833 | validation: 0.2811986271619426]
	TIME [epoch: 7.87 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10754372516326825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10754372516326825 | validation: 0.27619517357633755]
	TIME [epoch: 7.89 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09018365844195979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09018365844195979 | validation: 0.3058158092573239]
	TIME [epoch: 7.87 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09730177845036449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09730177845036449 | validation: 0.32156161553445545]
	TIME [epoch: 7.91 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09139203979357607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09139203979357607 | validation: 0.291107443048938]
	TIME [epoch: 7.85 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10068268490544846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10068268490544846 | validation: 0.32293468299978834]
	TIME [epoch: 7.86 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11884538480478755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11884538480478755 | validation: 0.31119799634787587]
	TIME [epoch: 7.88 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1428877228227519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1428877228227519 | validation: 0.4574781201805516]
	TIME [epoch: 7.86 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14203658648757223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14203658648757223 | validation: 0.37665908265126646]
	TIME [epoch: 7.87 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17270735725453099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17270735725453099 | validation: 0.7943818970424332]
	TIME [epoch: 7.9 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3421611090420226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3421611090420226 | validation: 0.4878489889233958]
	TIME [epoch: 7.87 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16987724676297397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16987724676297397 | validation: 0.3422729122659924]
	TIME [epoch: 7.85 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17052112060426006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17052112060426006 | validation: 0.42234781507169733]
	TIME [epoch: 7.87 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09443055300257318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09443055300257318 | validation: 0.42498411751929804]
	TIME [epoch: 7.89 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09707328406079617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09707328406079617 | validation: 0.2672694885736558]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_887.pth
	Model improved!!!
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09244602168474124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09244602168474124 | validation: 0.30601459620690585]
	TIME [epoch: 7.91 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08359282291294994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08359282291294994 | validation: 0.4288536245313247]
	TIME [epoch: 7.91 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09225188982761566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09225188982761566 | validation: 0.277207956408519]
	TIME [epoch: 7.88 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07632825244684047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07632825244684047 | validation: 0.2972134490828228]
	TIME [epoch: 7.89 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07302663489601989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07302663489601989 | validation: 0.43138510906012456]
	TIME [epoch: 7.9 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09667783296276145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09667783296276145 | validation: 0.3402904721186684]
	TIME [epoch: 7.89 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18403680338173664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18403680338173664 | validation: 0.5495432961020265]
	TIME [epoch: 7.86 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24456887039975123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24456887039975123 | validation: 0.32263910172662436]
	TIME [epoch: 7.91 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11298909799217705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11298909799217705 | validation: 0.24617129095330734]
	TIME [epoch: 7.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_896.pth
	Model improved!!!
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0924891110559613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0924891110559613 | validation: 0.5186693197906985]
	TIME [epoch: 7.86 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12120461011291674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12120461011291674 | validation: 0.45229996058061306]
	TIME [epoch: 7.88 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24888693834383957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24888693834383957 | validation: 0.7560557922113715]
	TIME [epoch: 10.7 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3249918506530004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3249918506530004 | validation: 0.8055149019539982]
	TIME [epoch: 7.88 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3510322489832446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3510322489832446 | validation: 0.30180771464934913]
	TIME [epoch: 7.87 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0878451764960049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0878451764960049 | validation: 0.2959921402998323]
	TIME [epoch: 7.87 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15009655049390655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15009655049390655 | validation: 0.30075490006560746]
	TIME [epoch: 7.87 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09072807989706952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09072807989706952 | validation: 0.37704973799528213]
	TIME [epoch: 7.86 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1008242008377779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1008242008377779 | validation: 0.36126901591971333]
	TIME [epoch: 7.85 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10309830912957388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10309830912957388 | validation: 0.2892249378811817]
	TIME [epoch: 7.86 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10204466132361645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10204466132361645 | validation: 0.30722810687167357]
	TIME [epoch: 7.88 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1046710212532928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1046710212532928 | validation: 0.3364062947555914]
	TIME [epoch: 7.86 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09444926890641307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09444926890641307 | validation: 0.2878161391670383]
	TIME [epoch: 7.85 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07905930450155545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07905930450155545 | validation: 0.3237884623528277]
	TIME [epoch: 7.85 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07336813193889802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07336813193889802 | validation: 0.29230494798988194]
	TIME [epoch: 7.86 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06608077433774238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06608077433774238 | validation: 0.43333363041795253]
	TIME [epoch: 7.86 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08172179109256199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08172179109256199 | validation: 0.39599602291385055]
	TIME [epoch: 7.87 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2029266833743025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2029266833743025 | validation: 0.8855555165385094]
	TIME [epoch: 7.86 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47377079861167615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47377079861167615 | validation: 0.7159309775840024]
	TIME [epoch: 7.86 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2677514835582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2677514835582 | validation: 0.35918078183321]
	TIME [epoch: 7.84 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19492008413949932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19492008413949932 | validation: 0.2865149277146018]
	TIME [epoch: 7.87 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07823330259912777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07823330259912777 | validation: 0.3688875498241653]
	TIME [epoch: 7.83 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10214498524972576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10214498524972576 | validation: 0.347375421088902]
	TIME [epoch: 7.87 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08948256765161249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08948256765161249 | validation: 0.25767189868189183]
	TIME [epoch: 7.85 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0747154685910393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0747154685910393 | validation: 0.2865714446347891]
	TIME [epoch: 7.88 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0755071361759882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0755071361759882 | validation: 0.29975218735301423]
	TIME [epoch: 7.85 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10163170252647985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10163170252647985 | validation: 0.26598187576921445]
	TIME [epoch: 7.85 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07995876094889787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07995876094889787 | validation: 0.34869001064044103]
	TIME [epoch: 7.88 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10274252895542346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10274252895542346 | validation: 0.2888300010558677]
	TIME [epoch: 7.91 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12029838478973004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12029838478973004 | validation: 0.295301289124495]
	TIME [epoch: 7.89 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12508312086275736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12508312086275736 | validation: 0.3002010759616719]
	TIME [epoch: 7.88 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07901915698905555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07901915698905555 | validation: 0.21917053696976893]
	TIME [epoch: 7.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_928.pth
	Model improved!!!
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08200654104896321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08200654104896321 | validation: 0.6176151092168229]
	TIME [epoch: 7.85 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19304471221266872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19304471221266872 | validation: 0.9066959766173025]
	TIME [epoch: 7.84 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1086507355923139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1086507355923139 | validation: 0.46429248544710744]
	TIME [epoch: 7.88 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4440141217451647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4440141217451647 | validation: 0.49142118607793717]
	TIME [epoch: 7.88 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23244270117596535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23244270117596535 | validation: 0.6663191777752625]
	TIME [epoch: 7.87 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2542929954042437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2542929954042437 | validation: 0.4549378014533172]
	TIME [epoch: 7.85 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1425607858527348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1425607858527348 | validation: 0.323282466943991]
	TIME [epoch: 7.86 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12789388657120757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12789388657120757 | validation: 0.3121289182442344]
	TIME [epoch: 7.85 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13217423670073997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13217423670073997 | validation: 0.3312961365545679]
	TIME [epoch: 7.87 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09336091325521785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09336091325521785 | validation: 0.301444211579197]
	TIME [epoch: 7.84 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08137079513438333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08137079513438333 | validation: 0.29562802973681607]
	TIME [epoch: 7.84 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08005855849425739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08005855849425739 | validation: 0.3065921573737463]
	TIME [epoch: 7.85 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07492121199951045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07492121199951045 | validation: 0.28714068631428463]
	TIME [epoch: 7.84 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07054042041644856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07054042041644856 | validation: 0.3123899813767568]
	TIME [epoch: 7.85 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07269903361928658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07269903361928658 | validation: 0.27246437452910777]
	TIME [epoch: 7.87 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07649628964985562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07649628964985562 | validation: 0.33799482876352976]
	TIME [epoch: 7.85 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0994763523252902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0994763523252902 | validation: 0.34030339407684546]
	TIME [epoch: 7.83 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1622442799671092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1622442799671092 | validation: 0.3377364882562354]
	TIME [epoch: 7.84 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10582480418533402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10582480418533402 | validation: 0.27031341572758877]
	TIME [epoch: 7.83 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06983595083377699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06983595083377699 | validation: 0.2738760719048531]
	TIME [epoch: 7.83 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06252387572651562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06252387572651562 | validation: 0.2754383998967919]
	TIME [epoch: 7.85 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05937078667715561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05937078667715561 | validation: 0.25330526317822094]
	TIME [epoch: 7.82 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0623961532859242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0623961532859242 | validation: 0.3587013429912087]
	TIME [epoch: 7.85 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0856247467866601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0856247467866601 | validation: 0.26385815691472086]
	TIME [epoch: 7.84 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13134782800928596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13134782800928596 | validation: 0.4625716323135907]
	TIME [epoch: 7.86 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29756753408194736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29756753408194736 | validation: 0.269057748741068]
	TIME [epoch: 7.83 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10260766303522685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10260766303522685 | validation: 0.23644189455498604]
	TIME [epoch: 7.84 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09690818120118205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09690818120118205 | validation: 0.6604497952948426]
	TIME [epoch: 7.84 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19080175779736233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19080175779736233 | validation: 0.3547844904216385]
	TIME [epoch: 7.84 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18671277535082373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18671277535082373 | validation: 0.4408020382375372]
	TIME [epoch: 7.82 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09882102454982829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09882102454982829 | validation: 0.4515869738543808]
	TIME [epoch: 7.83 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09189492363295461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09189492363295461 | validation: 0.2877909706119791]
	TIME [epoch: 7.83 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06802983422346279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06802983422346279 | validation: 0.24398517552567928]
	TIME [epoch: 7.86 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07867622530625414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07867622530625414 | validation: 0.2550705839087586]
	TIME [epoch: 7.85 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07623409108660986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07623409108660986 | validation: 0.35901686600460314]
	TIME [epoch: 7.86 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07781258508399479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07781258508399479 | validation: 0.35758586586065866]
	TIME [epoch: 7.84 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07952585748517417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07952585748517417 | validation: 0.23628114832395242]
	TIME [epoch: 7.85 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07994559569640443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07994559569640443 | validation: 0.2796120305886165]
	TIME [epoch: 7.86 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11420361677751972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11420361677751972 | validation: 0.3034779685945146]
	TIME [epoch: 7.88 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10347974352611349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10347974352611349 | validation: 0.28248169630517495]
	TIME [epoch: 7.85 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08560853139356726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08560853139356726 | validation: 0.2521593221740808]
	TIME [epoch: 7.85 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07498468764270735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07498468764270735 | validation: 0.27314489767112615]
	TIME [epoch: 7.85 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0593256921279807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0593256921279807 | validation: 0.2335598666136105]
	TIME [epoch: 7.88 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06390205651883982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06390205651883982 | validation: 0.16214076006672423]
	TIME [epoch: 7.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_972.pth
	Model improved!!!
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09857627950255983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09857627950255983 | validation: 0.36366453134479665]
	TIME [epoch: 7.89 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2061562366686457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2061562366686457 | validation: 0.4928630982694402]
	TIME [epoch: 7.89 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13670068995560364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13670068995560364 | validation: 0.6642433398239214]
	TIME [epoch: 7.84 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8405388438896918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8405388438896918 | validation: 0.21053402137836008]
	TIME [epoch: 7.89 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20361934096343778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20361934096343778 | validation: 0.44747823222502303]
	TIME [epoch: 7.84 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12824032388704906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12824032388704906 | validation: 0.375172695558358]
	TIME [epoch: 7.88 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11448915471009251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11448915471009251 | validation: 0.29261127961163363]
	TIME [epoch: 7.92 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06981112309705335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06981112309705335 | validation: 0.21631879978246396]
	TIME [epoch: 7.89 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07272602379593349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07272602379593349 | validation: 0.21847020667862732]
	TIME [epoch: 7.88 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06984504284023815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06984504284023815 | validation: 0.23498572272036325]
	TIME [epoch: 7.85 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06067728628258658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06067728628258658 | validation: 0.22548046592591042]
	TIME [epoch: 7.89 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0582768807129496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0582768807129496 | validation: 0.2322655278048109]
	TIME [epoch: 7.87 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0616608868031102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0616608868031102 | validation: 0.23350601289183526]
	TIME [epoch: 7.9 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0868471071632682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0868471071632682 | validation: 0.2570299020073194]
	TIME [epoch: 7.87 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08614153561455379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08614153561455379 | validation: 0.21599966990600789]
	TIME [epoch: 7.86 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08906663723893524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08906663723893524 | validation: 0.29401153846135236]
	TIME [epoch: 7.86 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.095029012393641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.095029012393641 | validation: 0.23511088371011135]
	TIME [epoch: 7.88 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10921942209435402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10921942209435402 | validation: 0.4695251384377209]
	TIME [epoch: 7.88 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12507302822115668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12507302822115668 | validation: 0.20795809284216263]
	TIME [epoch: 7.88 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07822566814051377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07822566814051377 | validation: 0.2194049586222552]
	TIME [epoch: 7.85 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047236401902936626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047236401902936626 | validation: 0.20678810346238313]
	TIME [epoch: 7.82 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054576189014748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054576189014748 | validation: 0.18854018319444674]
	TIME [epoch: 7.84 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061801564901407355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061801564901407355 | validation: 0.28159378853994127]
	TIME [epoch: 7.81 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09631655445743789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09631655445743789 | validation: 0.28965507499623405]
	TIME [epoch: 7.87 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15303144525510098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15303144525510098 | validation: 0.20278173300070274]
	TIME [epoch: 7.81 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07433517920567653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07433517920567653 | validation: 0.3952979052049515]
	TIME [epoch: 7.84 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0794873855707973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0794873855707973 | validation: 0.2295482441138054]
	TIME [epoch: 7.83 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08147810807494997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08147810807494997 | validation: 0.1840130069662843]
	TIME [epoch: 7.84 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07531228755746516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07531228755746516 | validation: 0.21213928001771715]
	TIME [epoch: 56.8 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09814114088014346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09814114088014346 | validation: 0.5750675307137117]
	TIME [epoch: 16.7 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1454384637503514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1454384637503514 | validation: 0.2774856403058942]
	TIME [epoch: 16.7 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13377304266873777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13377304266873777 | validation: 0.5466542131456689]
	TIME [epoch: 16.6 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.150731412490241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.150731412490241 | validation: 0.2061050930289805]
	TIME [epoch: 16.7 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04503802683724775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04503802683724775 | validation: 0.19688967712622818]
	TIME [epoch: 16.7 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0759889478338969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0759889478338969 | validation: 0.30573168442574433]
	TIME [epoch: 16.6 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06697984035998544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06697984035998544 | validation: 0.19997207368033296]
	TIME [epoch: 16.6 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0536958147340658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0536958147340658 | validation: 0.277776358820388]
	TIME [epoch: 16.7 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13737218476710739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13737218476710739 | validation: 0.644883649334683]
	TIME [epoch: 16.7 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23078970676070326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23078970676070326 | validation: 0.22379430369369793]
	TIME [epoch: 16.7 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07905630023475099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07905630023475099 | validation: 0.20099488050054207]
	TIME [epoch: 16.7 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06805990021103579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06805990021103579 | validation: 0.41703374298130075]
	TIME [epoch: 16.7 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08578951895272137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08578951895272137 | validation: 0.2004632159718045]
	TIME [epoch: 16.7 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06926310587366549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06926310587366549 | validation: 0.16726826084759305]
	TIME [epoch: 16.7 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05138076084550175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05138076084550175 | validation: 0.1882679950452539]
	TIME [epoch: 16.6 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043197243479859856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043197243479859856 | validation: 0.1795603319651852]
	TIME [epoch: 16.7 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043617784172578916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043617784172578916 | validation: 0.16990650938001092]
	TIME [epoch: 16.7 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04937235794805478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04937235794805478 | validation: 0.22778753358765558]
	TIME [epoch: 16.7 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11462452804774817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11462452804774817 | validation: 0.7383426942716705]
	TIME [epoch: 16.7 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3801829785529536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3801829785529536 | validation: 0.4912087624924111]
	TIME [epoch: 16.7 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22343721246795192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22343721246795192 | validation: 0.3681943710917224]
	TIME [epoch: 16.7 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19375397251738233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19375397251738233 | validation: 0.5913994030753668]
	TIME [epoch: 16.6 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18066975218106968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18066975218106968 | validation: 0.49000181721597474]
	TIME [epoch: 16.7 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10456730576620914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10456730576620914 | validation: 0.2329771967391285]
	TIME [epoch: 16.7 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.071850780987488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.071850780987488 | validation: 0.20442421024446783]
	TIME [epoch: 16.7 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06090701606830816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06090701606830816 | validation: 0.24886881581513307]
	TIME [epoch: 16.7 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054910056309218726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054910056309218726 | validation: 0.2887002924433262]
	TIME [epoch: 16.7 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05237803103760817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05237803103760817 | validation: 0.24923806347708527]
	TIME [epoch: 16.7 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04314307122260069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04314307122260069 | validation: 0.20878973826784022]
	TIME [epoch: 16.7 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0444895971204279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0444895971204279 | validation: 0.2913297666710628]
	TIME [epoch: 16.7 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04895605420900037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04895605420900037 | validation: 0.24186982564175336]
	TIME [epoch: 16.7 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06521294789753458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06521294789753458 | validation: 0.27677799342299014]
	TIME [epoch: 16.7 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1527200589926893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1527200589926893 | validation: 0.37423929009490225]
	TIME [epoch: 16.7 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14808070533798764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14808070533798764 | validation: 0.19946784186563532]
	TIME [epoch: 16.7 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08276452855095916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08276452855095916 | validation: 0.283698029023436]
	TIME [epoch: 16.7 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0675841535554259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0675841535554259 | validation: 0.18504375971238965]
	TIME [epoch: 16.7 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0625587353489696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0625587353489696 | validation: 0.25143010624588297]
	TIME [epoch: 16.6 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06952354883209033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06952354883209033 | validation: 0.17605282689624638]
	TIME [epoch: 16.7 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05754052939934967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05754052939934967 | validation: 0.18782464582835173]
	TIME [epoch: 16.7 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052106741922730404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052106741922730404 | validation: 0.19147208197219923]
	TIME [epoch: 16.7 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0776556229690565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0776556229690565 | validation: 0.5903578170653292]
	TIME [epoch: 16.7 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17635447357974982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17635447357974982 | validation: 0.33938058496607715]
	TIME [epoch: 16.7 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16078845227385757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16078845227385757 | validation: 0.5352292738812271]
	TIME [epoch: 16.7 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11406231524527735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11406231524527735 | validation: 0.18282199608765265]
	TIME [epoch: 16.7 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0849090088656699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0849090088656699 | validation: 0.232298060853576]
	TIME [epoch: 16.7 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08432906079470559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08432906079470559 | validation: 0.301076539930913]
	TIME [epoch: 16.7 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052699036667347626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052699036667347626 | validation: 0.2575967703912306]
	TIME [epoch: 16.7 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06527914540805849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06527914540805849 | validation: 0.23104887600847765]
	TIME [epoch: 16.7 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0783582664128647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0783582664128647 | validation: 0.2174575165393197]
	TIME [epoch: 16.7 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10227719516615294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10227719516615294 | validation: 0.24288663368916344]
	TIME [epoch: 16.7 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08188655424284186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08188655424284186 | validation: 0.23005240788336298]
	TIME [epoch: 16.7 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06536635098820227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06536635098820227 | validation: 0.1664613465080536]
	TIME [epoch: 16.7 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04732252492811249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04732252492811249 | validation: 0.2577577695910859]
	TIME [epoch: 16.7 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04197265229104268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04197265229104268 | validation: 0.1657628816557826]
	TIME [epoch: 16.6 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03836223493746583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03836223493746583 | validation: 0.17289304437364847]
	TIME [epoch: 16.7 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04221028851338062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04221028851338062 | validation: 0.23282433768403576]
	TIME [epoch: 16.7 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14543656947339298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14543656947339298 | validation: 0.6588255746405569]
	TIME [epoch: 16.7 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1768549118848514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1768549118848514 | validation: 0.2215192978164737]
	TIME [epoch: 16.6 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0868132300262315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0868132300262315 | validation: 0.19788420333935927]
	TIME [epoch: 16.7 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09160058787898705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09160058787898705 | validation: 0.2564029940000292]
	TIME [epoch: 16.7 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056407291733575525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056407291733575525 | validation: 0.2043011455239634]
	TIME [epoch: 16.7 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12780572484955907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12780572484955907 | validation: 0.26139626253735093]
	TIME [epoch: 16.7 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14036741283397927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14036741283397927 | validation: 0.5324400273744142]
	TIME [epoch: 16.7 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1293685165053365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1293685165053365 | validation: 0.1787624772851122]
	TIME [epoch: 16.7 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05774383129499284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05774383129499284 | validation: 0.17413533983648655]
	TIME [epoch: 16.7 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051051915319744894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051051915319744894 | validation: 0.22503019810563113]
	TIME [epoch: 16.7 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04581479761426607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04581479761426607 | validation: 0.16893125661394293]
	TIME [epoch: 16.7 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04973577531982607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04973577531982607 | validation: 0.21952616546529413]
	TIME [epoch: 16.7 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052505791199603795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052505791199603795 | validation: 0.1655309190574021]
	TIME [epoch: 16.7 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05807252519001272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05807252519001272 | validation: 0.4468080183461032]
	TIME [epoch: 16.7 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08956566508602908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08956566508602908 | validation: 0.376479599243974]
	TIME [epoch: 16.7 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.185817335553838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.185817335553838 | validation: 0.6735397887949794]
	TIME [epoch: 16.7 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_182813/states/model_phi1_4b_v_mmd1_1073.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 6861.454 seconds.
