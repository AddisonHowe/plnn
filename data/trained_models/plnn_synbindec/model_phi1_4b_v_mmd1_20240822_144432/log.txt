Args:
Namespace(name='model_phi1_4b_v_mmd1', outdir='out/model_training/model_phi1_4b_v_mmd1', training_data='data/training_data/data_phi1_4b/training', validation_data='data/training_data/data_phi1_4b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1168592231

Training model...

Saving initial model state to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.228339250778786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.228339250778786 | validation: 5.570586949220611]
	TIME [epoch: 44.7 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.013939543180426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.013939543180426 | validation: 4.674145949494199]
	TIME [epoch: 1.9 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.797040106206833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.797040106206833 | validation: 4.948545250324565]
	TIME [epoch: 1.88 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.307785477205914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.307785477205914 | validation: 4.867178900495201]
	TIME [epoch: 1.88 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.193243889259198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.193243889259198 | validation: 4.218386369801708]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.500315580024729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.500315580024729 | validation: 4.023829095503709]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.128546725480547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.128546725480547 | validation: 3.915069457111162]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.992976606125643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.992976606125643 | validation: 3.696593666703894]
	TIME [epoch: 1.9 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.844045181497727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.844045181497727 | validation: 3.6681662937096093]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8141661629773442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8141661629773442 | validation: 3.707298563646549]
	TIME [epoch: 1.89 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.781460931633726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.781460931633726 | validation: 3.6775446568381884]
	TIME [epoch: 1.88 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.760225582743442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.760225582743442 | validation: 3.637604707303259]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7390942162050136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7390942162050136 | validation: 3.6205590247235264]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7292463038018675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7292463038018675 | validation: 3.5975653829498437]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.710115573166532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.710115573166532 | validation: 3.5913529427342157]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.69841482219305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.69841482219305 | validation: 3.5614884092965102]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6823994202941845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6823994202941845 | validation: 3.5415564113542986]
	TIME [epoch: 1.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6720708788650547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6720708788650547 | validation: 3.521404118416415]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6581282221322358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6581282221322358 | validation: 3.509187629039568]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.645274461879018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.645274461879018 | validation: 3.4814660914602693]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6416149519745566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6416149519745566 | validation: 3.542788071713327]
	TIME [epoch: 1.88 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6816652337568216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6816652337568216 | validation: 3.592695818529467]
	TIME [epoch: 1.89 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8804220681866957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8804220681866957 | validation: 3.652842583889512]
	TIME [epoch: 1.89 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7877224932603655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7877224932603655 | validation: 3.4359165390605444]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.590524630600528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.590524630600528 | validation: 3.433138658076075]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6383466404638605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6383466404638605 | validation: 3.493713949893276]
	TIME [epoch: 1.89 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6101393744599264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6101393744599264 | validation: 3.3859680870912]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.53005343748981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.53005343748981 | validation: 3.362392615748067]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4891826634782297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4891826634782297 | validation: 3.3915860064838044]
	TIME [epoch: 1.88 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.477714053192654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.477714053192654 | validation: 3.369833601091509]
	TIME [epoch: 1.87 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5220405127222665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5220405127222665 | validation: 3.4639182866825005]
	TIME [epoch: 1.88 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6100824059242975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6100824059242975 | validation: 3.3449482461122635]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5098289876958417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5098289876958417 | validation: 3.2934959997071065]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4067998523826737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4067998523826737 | validation: 3.2248630697796408]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3721402191820102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3721402191820102 | validation: 3.230455877505406]
	TIME [epoch: 1.88 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3487605774295175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3487605774295175 | validation: 3.2027689824660337]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.336437959402458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.336437959402458 | validation: 3.18191164924988]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.327443139439619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.327443139439619 | validation: 3.177553050340068]
	TIME [epoch: 1.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3283450784826143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3283450784826143 | validation: 3.1732435245573662]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.343391660139827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.343391660139827 | validation: 3.206763144977616]
	TIME [epoch: 1.89 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.385659086168398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.385659086168398 | validation: 3.161429929981418]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.342761007960164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.342761007960164 | validation: 3.112000020658887]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.284820690676048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.284820690676048 | validation: 3.06323801758921]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2408831062460797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2408831062460797 | validation: 3.0543629631706857]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2090619755423626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2090619755423626 | validation: 3.027452675327421]
	TIME [epoch: 1.9 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2021471340594623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2021471340594623 | validation: 3.0112314029776677]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.184035524799891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.184035524799891 | validation: 2.991651042637321]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1676428813708912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1676428813708912 | validation: 2.975310671779545]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1526580608597157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1526580608597157 | validation: 2.9633666989209786]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1450411621996603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1450411621996603 | validation: 2.9299065850060932]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1470228375676808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1470228375676808 | validation: 3.070013111059286]
	TIME [epoch: 1.88 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2808505627151714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2808505627151714 | validation: 3.2143049069336307]
	TIME [epoch: 1.89 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.497798185944309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.497798185944309 | validation: 2.9634749920638095]
	TIME [epoch: 1.88 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1601484281800443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1601484281800443 | validation: 3.003721081069257]
	TIME [epoch: 1.88 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.211600507204394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.211600507204394 | validation: 2.9535930162522415]
	TIME [epoch: 1.88 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.219886003147958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.219886003147958 | validation: 2.867153364173736]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0717691611857494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0717691611857494 | validation: 2.9028569845951187]
	TIME [epoch: 1.89 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1062717666660666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1062717666660666 | validation: 2.857581429408799]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0825365285466955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0825365285466955 | validation: 2.820502065490668]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.039787172433007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.039787172433007 | validation: 2.814205135101523]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.032581503779472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.032581503779472 | validation: 2.791962509913748]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0319362271934565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0319362271934565 | validation: 2.7987983039363344]
	TIME [epoch: 1.9 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.007824909151151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.007824909151151 | validation: 2.7655632054275516]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9994168115975652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9994168115975652 | validation: 2.7751512057816776]
	TIME [epoch: 1.88 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9883112114953345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9883112114953345 | validation: 2.749048751238741]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.983161479970619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.983161479970619 | validation: 2.764354524695319]
	TIME [epoch: 1.88 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9903071172183706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9903071172183706 | validation: 2.7352124336873977]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0255888553971055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0255888553971055 | validation: 2.874110730863931]
	TIME [epoch: 1.88 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.108827748758407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.108827748758407 | validation: 2.7730000735118954]
	TIME [epoch: 1.88 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.024181977210546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.024181977210546 | validation: 2.699703914283676]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9392924796184663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9392924796184663 | validation: 2.7278669384203544]
	TIME [epoch: 1.89 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9581254709548013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9581254709548013 | validation: 2.7042190798911236]
	TIME [epoch: 1.88 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9569071664095037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9569071664095037 | validation: 2.696111965938101]
	TIME [epoch: 1.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.932032411685114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.932032411685114 | validation: 2.666435593050224]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9148416316720422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9148416316720422 | validation: 2.666821357938609]
	TIME [epoch: 1.88 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.905704891286199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.905704891286199 | validation: 2.6574221876257766]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9008748576291805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9008748576291805 | validation: 2.663179685173057]
	TIME [epoch: 1.89 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.892521441479928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.892521441479928 | validation: 2.633728631922707]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8955405790520716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8955405790520716 | validation: 2.732657354009867]
	TIME [epoch: 1.88 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9502304812904137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9502304812904137 | validation: 2.749617529747028]
	TIME [epoch: 1.89 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0423485037505813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0423485037505813 | validation: 2.673288321559695]
	TIME [epoch: 1.88 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.898746505850809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.898746505850809 | validation: 2.6384182725311103]
	TIME [epoch: 1.88 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.863186438932032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.863186438932032 | validation: 2.60453801504525]
	TIME [epoch: 1.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.861299495116599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.861299495116599 | validation: 2.6153991053179504]
	TIME [epoch: 1.88 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.83644007745457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.83644007745457 | validation: 2.6166913175594013]
	TIME [epoch: 1.89 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8179384667803347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8179384667803347 | validation: 2.5978227151901523]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.797843137459309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.797843137459309 | validation: 2.573813828543466]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7506363097368456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7506363097368456 | validation: 2.507141490105527]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5699617015130127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5699617015130127 | validation: 2.332426707631988]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3294775411750344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3294775411750344 | validation: 2.2396512850899684]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0847370148732396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0847370148732396 | validation: 2.213676771531913]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3535445204336285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3535445204336285 | validation: 2.0993694495798545]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.744367713512629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.744367713512629 | validation: 1.5818178280855753]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4186861594593654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4186861594593654 | validation: 1.2980743811095345]
	TIME [epoch: 1.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1815352709405726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1815352709405726 | validation: 1.1539040465164019]
	TIME [epoch: 1.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0024133107779323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0024133107779323 | validation: 1.1827789764395504]
	TIME [epoch: 1.88 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.286655369298045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.286655369298045 | validation: 1.6145911695046238]
	TIME [epoch: 1.88 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2296550591405866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2296550591405866 | validation: 1.038146518792414]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9113491753127851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9113491753127851 | validation: 0.9889554193409362]
	TIME [epoch: 1.91 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9741242498148543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9741242498148543 | validation: 0.999736724767298]
	TIME [epoch: 1.88 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8919509617334231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8919509617334231 | validation: 1.039328099546762]
	TIME [epoch: 1.89 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8827698200300622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8827698200300622 | validation: 0.8901225821619854]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8568856347132697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8568856347132697 | validation: 0.9240136208028439]
	TIME [epoch: 1.88 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8432257788651174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8432257788651174 | validation: 0.9291528983574289]
	TIME [epoch: 1.88 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8337026744973153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8337026744973153 | validation: 0.9051011131983291]
	TIME [epoch: 1.88 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8300689958893207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8300689958893207 | validation: 0.9185120543888374]
	TIME [epoch: 1.89 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8265451234382524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8265451234382524 | validation: 0.8947719885895196]
	TIME [epoch: 1.88 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8153078125254476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8153078125254476 | validation: 0.901466130500104]
	TIME [epoch: 1.88 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.819477673273929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.819477673273929 | validation: 0.9663977769786259]
	TIME [epoch: 1.88 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8408454770626248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8408454770626248 | validation: 0.9975122568374538]
	TIME [epoch: 1.88 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9251461356400731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9251461356400731 | validation: 0.9789115824810593]
	TIME [epoch: 1.88 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8721028650677797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8721028650677797 | validation: 0.909946153232442]
	TIME [epoch: 1.89 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8137596565336733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8137596565336733 | validation: 0.9153344890549733]
	TIME [epoch: 1.89 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7959286908966918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7959286908966918 | validation: 0.9433492610404578]
	TIME [epoch: 1.89 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7971332974447828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7971332974447828 | validation: 0.8636281191691317]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_115.pth
	Model improved!!!
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7975204897086545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7975204897086545 | validation: 0.9874593773807834]
	TIME [epoch: 1.87 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.80691814704188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.80691814704188 | validation: 0.873450548798259]
	TIME [epoch: 1.88 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8188628013813216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8188628013813216 | validation: 1.1283802896852453]
	TIME [epoch: 1.89 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8581704225415275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8581704225415275 | validation: 0.91057493903056]
	TIME [epoch: 1.88 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8758161283199806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8758161283199806 | validation: 1.0928917215247957]
	TIME [epoch: 1.88 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8336029956539843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8336029956539843 | validation: 0.8959632129316724]
	TIME [epoch: 1.88 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7803865230333729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7803865230333729 | validation: 0.845814056235449]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7832981056132935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7832981056132935 | validation: 0.9569293507693303]
	TIME [epoch: 1.88 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7962028762292181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7962028762292181 | validation: 0.8922827144278769]
	TIME [epoch: 1.88 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8008990818850634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8008990818850634 | validation: 0.9504535059186303]
	TIME [epoch: 1.88 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8465696834895023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8465696834895023 | validation: 1.0333857901783527]
	TIME [epoch: 1.88 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9251201225977395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9251201225977395 | validation: 0.8764260280936145]
	TIME [epoch: 1.88 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8060678821916998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8060678821916998 | validation: 0.9203366754268013]
	TIME [epoch: 1.88 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.770356243032968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.770356243032968 | validation: 0.9206359907179146]
	TIME [epoch: 1.88 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7693142746465773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7693142746465773 | validation: 0.8575265518069961]
	TIME [epoch: 1.88 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7701646411514014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7701646411514014 | validation: 0.9158169370141975]
	TIME [epoch: 1.88 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7748092656363735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7748092656363735 | validation: 0.8996454591505334]
	TIME [epoch: 1.89 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7793774864928636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7793774864928636 | validation: 0.9327488672856511]
	TIME [epoch: 1.89 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.806366881287116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.806366881287116 | validation: 0.9370811947774886]
	TIME [epoch: 1.87 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8154844998239844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8154844998239844 | validation: 1.0135175229876265]
	TIME [epoch: 1.87 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8349233500712637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8349233500712637 | validation: 0.8695801066487068]
	TIME [epoch: 1.87 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8164384478414417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8164384478414417 | validation: 1.0697749630817417]
	TIME [epoch: 1.87 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8155169402016861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8155169402016861 | validation: 0.8568558365842609]
	TIME [epoch: 1.87 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7745733114042908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7745733114042908 | validation: 0.9985163587684358]
	TIME [epoch: 1.87 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7907976864413236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7907976864413236 | validation: 0.9265589045147611]
	TIME [epoch: 1.87 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8166208965628377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8166208965628377 | validation: 0.9536181665224737]
	TIME [epoch: 1.89 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7855986083708633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7855986083708633 | validation: 0.9153388639022719]
	TIME [epoch: 1.89 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7768616112850754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7768616112850754 | validation: 0.843504389285227]
	TIME [epoch: 1.9 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_143.pth
	Model improved!!!
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7659009864081299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7659009864081299 | validation: 0.9427289480484682]
	TIME [epoch: 1.9 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7659846725131901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7659846725131901 | validation: 0.8446224108064211]
	TIME [epoch: 1.89 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.76978062573458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.76978062573458 | validation: 1.0784218928160025]
	TIME [epoch: 1.88 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8115155149558945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8115155149558945 | validation: 0.8448185663341783]
	TIME [epoch: 1.87 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7853324182048603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7853324182048603 | validation: 0.9905202020869656]
	TIME [epoch: 1.89 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7687526664888403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7687526664888403 | validation: 0.8406335249814725]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_149.pth
	Model improved!!!
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7628667763338587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7628667763338587 | validation: 0.9767913497567382]
	TIME [epoch: 1.88 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7999011252974029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7999011252974029 | validation: 0.9839487932563664]
	TIME [epoch: 1.88 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.867868966972851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.867868966972851 | validation: 1.0251958667323153]
	TIME [epoch: 1.89 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8860938814845354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8860938814845354 | validation: 1.035496540814707]
	TIME [epoch: 1.88 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8182430331729087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8182430331729087 | validation: 0.8545923922892659]
	TIME [epoch: 1.88 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7524974842665142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7524974842665142 | validation: 0.8799572709443073]
	TIME [epoch: 1.88 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7691221667076871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7691221667076871 | validation: 0.9188137155345534]
	TIME [epoch: 1.88 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7732229804739044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7732229804739044 | validation: 0.842164716914936]
	TIME [epoch: 1.88 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7672271327352091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7672271327352091 | validation: 0.9718941106040626]
	TIME [epoch: 1.89 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7677040205891368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7677040205891368 | validation: 0.8408646085795873]
	TIME [epoch: 1.9 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.760170998156014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.760170998156014 | validation: 0.9380693382938479]
	TIME [epoch: 1.89 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7591519302763914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7591519302763914 | validation: 0.8340850186871936]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_161.pth
	Model improved!!!
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7667473057221054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7667473057221054 | validation: 1.1067677985592714]
	TIME [epoch: 1.89 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.816789115405021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.816789115405021 | validation: 0.9144322067180973]
	TIME [epoch: 1.89 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8299734263033628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8299734263033628 | validation: 0.9345681063851332]
	TIME [epoch: 1.89 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8238523780931419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8238523780931419 | validation: 0.9772853701646828]
	TIME [epoch: 1.88 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8275442104930238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8275442104930238 | validation: 0.8665840695047766]
	TIME [epoch: 1.87 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7660771880114265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7660771880114265 | validation: 0.8848427733043369]
	TIME [epoch: 1.87 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7468458732715864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7468458732715864 | validation: 0.8575291665106324]
	TIME [epoch: 1.88 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7423034348882999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7423034348882999 | validation: 0.8557403725058678]
	TIME [epoch: 1.88 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7369114688409281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7369114688409281 | validation: 0.8724723147196287]
	TIME [epoch: 1.88 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7399019325527206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7399019325527206 | validation: 0.8391058715687043]
	TIME [epoch: 1.88 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7438665814482187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7438665814482187 | validation: 0.9342140826748632]
	TIME [epoch: 1.88 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7554345460230704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7554345460230704 | validation: 0.8615333065454194]
	TIME [epoch: 1.89 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8068040664452382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8068040664452382 | validation: 1.1017348674387641]
	TIME [epoch: 1.88 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9538030220751352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9538030220751352 | validation: 1.0195934518212664]
	TIME [epoch: 1.88 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8121608103208378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8121608103208378 | validation: 0.8593039713545072]
	TIME [epoch: 1.87 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7744537318488385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7744537318488385 | validation: 0.9601527176294784]
	TIME [epoch: 1.87 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7597850296284056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7597850296284056 | validation: 0.8806607408128051]
	TIME [epoch: 1.87 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7552678965372515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7552678965372515 | validation: 0.8477355947135475]
	TIME [epoch: 1.88 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.758541886154739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.758541886154739 | validation: 0.9533307969067766]
	TIME [epoch: 1.88 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7584870456334926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7584870456334926 | validation: 0.8407090259132721]
	TIME [epoch: 1.88 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7544517824129183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7544517824129183 | validation: 0.8979758934293858]
	TIME [epoch: 1.87 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.738850783313448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.738850783313448 | validation: 0.8357645787952199]
	TIME [epoch: 1.87 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7341167381604367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7341167381604367 | validation: 0.8546586616241941]
	TIME [epoch: 1.89 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7338463948732902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7338463948732902 | validation: 0.8645860356547672]
	TIME [epoch: 1.88 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7406494853309848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7406494853309848 | validation: 0.8858091358022107]
	TIME [epoch: 1.88 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7700812374134576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7700812374134576 | validation: 1.0705136547354621]
	TIME [epoch: 1.87 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.916269760759945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.916269760759945 | validation: 0.9049336580118145]
	TIME [epoch: 1.87 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.838520940705879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.838520940705879 | validation: 1.0163669806048328]
	TIME [epoch: 1.88 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7788288000189957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7788288000189957 | validation: 0.8694780415573766]
	TIME [epoch: 1.88 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7371517511164294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7371517511164294 | validation: 0.8495421262926106]
	TIME [epoch: 1.88 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7567934937756527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7567934937756527 | validation: 0.9150039329871004]
	TIME [epoch: 1.88 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7596566024457504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7596566024457504 | validation: 0.8250831073098983]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7362651719051404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7362651719051404 | validation: 0.8292915141639418]
	TIME [epoch: 1.88 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7278054196998267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7278054196998267 | validation: 0.9034027297935471]
	TIME [epoch: 1.89 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7277452767443624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7277452767443624 | validation: 0.8087164838050462]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7452396656583103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7452396656583103 | validation: 1.055887379277987]
	TIME [epoch: 1.88 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7848893634397827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7848893634397827 | validation: 0.8498637563976662]
	TIME [epoch: 1.88 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.816294595685714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.816294595685714 | validation: 0.9014430304449617]
	TIME [epoch: 1.88 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7509652675256796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7509652675256796 | validation: 0.896049987972412]
	TIME [epoch: 1.88 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7402024877341338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7402024877341338 | validation: 0.7943753061803479]
	TIME [epoch: 45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7752390275639699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7752390275639699 | validation: 1.0206949270857801]
	TIME [epoch: 3.72 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8227296540720744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8227296540720744 | validation: 0.9202751827401592]
	TIME [epoch: 3.7 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7725339017921347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7725339017921347 | validation: 0.8375420474694635]
	TIME [epoch: 3.71 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7580496792090271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7580496792090271 | validation: 0.8503049310638358]
	TIME [epoch: 3.7 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7145520840227979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7145520840227979 | validation: 0.8454302020280164]
	TIME [epoch: 3.72 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.71412410412734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.71412410412734 | validation: 0.7860588692020434]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_207.pth
	Model improved!!!
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7219649518393864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7219649518393864 | validation: 0.8450094140123809]
	TIME [epoch: 3.73 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.714351930097726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.714351930097726 | validation: 0.8365316085991328]
	TIME [epoch: 3.72 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7310291691795925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7310291691795925 | validation: 0.895528618084899]
	TIME [epoch: 3.73 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8195479737253956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8195479737253956 | validation: 1.1324961046978324]
	TIME [epoch: 3.72 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.979548461863847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.979548461863847 | validation: 0.8533447389843327]
	TIME [epoch: 3.72 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7278743669820621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7278743669820621 | validation: 0.845954706782769]
	TIME [epoch: 3.72 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7156988994661118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7156988994661118 | validation: 0.9086120447219195]
	TIME [epoch: 3.72 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.733803276375771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.733803276375771 | validation: 0.828639101616877]
	TIME [epoch: 3.73 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7194496169166044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7194496169166044 | validation: 0.8644811302868645]
	TIME [epoch: 3.72 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7075069888521334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7075069888521334 | validation: 0.78683906044232]
	TIME [epoch: 3.72 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7096295786008424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7096295786008424 | validation: 0.8661046696724051]
	TIME [epoch: 3.74 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7129073473030841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7129073473030841 | validation: 0.7626768856468971]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7081759858805678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7081759858805678 | validation: 0.8787472017345443]
	TIME [epoch: 3.73 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7138453189920867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7138453189920867 | validation: 0.7412893609900766]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.718783866983335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.718783866983335 | validation: 0.936012920424842]
	TIME [epoch: 3.73 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7324739161271488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7324739161271488 | validation: 0.74206902014211]
	TIME [epoch: 3.73 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7214746003621915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7214746003621915 | validation: 0.903200663818539]
	TIME [epoch: 3.73 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7325589470546897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7325589470546897 | validation: 0.9815260163331762]
	TIME [epoch: 3.73 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8527724942166722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8527724942166722 | validation: 1.0180811408817443]
	TIME [epoch: 3.73 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9491190087711203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9491190087711203 | validation: 0.7949315767883594]
	TIME [epoch: 3.73 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6946552528294003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6946552528294003 | validation: 0.9093954102230724]
	TIME [epoch: 3.72 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7229446349922728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7229446349922728 | validation: 0.87080209659521]
	TIME [epoch: 3.72 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7111392406592947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7111392406592947 | validation: 0.7685781786819584]
	TIME [epoch: 3.74 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7050896525430218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7050896525430218 | validation: 0.8439772088848212]
	TIME [epoch: 3.75 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6855341967775356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6855341967775356 | validation: 0.793699972712971]
	TIME [epoch: 3.71 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6819531349943418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6819531349943418 | validation: 0.7845973108373303]
	TIME [epoch: 3.71 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6746073402792265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6746073402792265 | validation: 0.7719370044018494]
	TIME [epoch: 3.72 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6758470196480749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6758470196480749 | validation: 0.8355066222739718]
	TIME [epoch: 3.71 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7115265293217055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7115265293217055 | validation: 0.9302720371446498]
	TIME [epoch: 3.7 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9166364435351372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9166364435351372 | validation: 1.0990841474978097]
	TIME [epoch: 3.71 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9085415113237759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9085415113237759 | validation: 0.865090171661056]
	TIME [epoch: 3.71 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6833475330372596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6833475330372596 | validation: 0.7919344106777418]
	TIME [epoch: 3.72 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.712673123279327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.712673123279327 | validation: 0.8348971546922969]
	TIME [epoch: 3.72 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6982544397855348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6982544397855348 | validation: 0.8491061945459735]
	TIME [epoch: 3.72 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.668413153648602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.668413153648602 | validation: 0.7257683167271192]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6669750499943016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6669750499943016 | validation: 0.8163252951583809]
	TIME [epoch: 3.72 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.667167391571912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.667167391571912 | validation: 0.7647930191599289]
	TIME [epoch: 3.72 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6481120441970804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6481120441970804 | validation: 0.7661757716188337]
	TIME [epoch: 3.72 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6529344543497484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6529344543497484 | validation: 0.8223623280313218]
	TIME [epoch: 3.72 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6894116955092711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6894116955092711 | validation: 0.9900129369833373]
	TIME [epoch: 3.73 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8912831765091574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8912831765091574 | validation: 0.9061203409391272]
	TIME [epoch: 3.72 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8632240550241309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8632240550241309 | validation: 0.9461308039888521]
	TIME [epoch: 3.73 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.709677882950173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.709677882950173 | validation: 0.8203889627531016]
	TIME [epoch: 3.73 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6678408498166178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6678408498166178 | validation: 0.7841218771937817]
	TIME [epoch: 3.72 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6779191548026643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6779191548026643 | validation: 0.8314290894345938]
	TIME [epoch: 3.72 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6660760367200544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6660760367200544 | validation: 0.7355952277668287]
	TIME [epoch: 3.73 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6446204404769952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6446204404769952 | validation: 0.760248057383675]
	TIME [epoch: 3.73 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6304278548085093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6304278548085093 | validation: 0.7386423065520216]
	TIME [epoch: 3.74 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6319522906736513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6319522906736513 | validation: 0.7332205124137218]
	TIME [epoch: 3.72 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6239085118747221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6239085118747221 | validation: 0.7802648191093634]
	TIME [epoch: 3.73 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6618257060428634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6618257060428634 | validation: 1.0027889357913429]
	TIME [epoch: 3.73 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8540918232348625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8540918232348625 | validation: 0.9391477677217863]
	TIME [epoch: 3.72 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.911231660671684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.911231660671684 | validation: 0.8953568398439263]
	TIME [epoch: 3.73 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.666045268065422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.666045268065422 | validation: 0.8228226221709177]
	TIME [epoch: 3.72 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6566939215594323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6566939215594323 | validation: 0.7616981301326625]
	TIME [epoch: 3.72 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6676361629918486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6676361629918486 | validation: 0.8083309319863429]
	TIME [epoch: 3.73 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6360114530044586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6360114530044586 | validation: 0.7467357373712367]
	TIME [epoch: 3.74 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6155834470454019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6155834470454019 | validation: 0.7265742202039518]
	TIME [epoch: 3.74 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6049121955589355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6049121955589355 | validation: 0.7369977521893537]
	TIME [epoch: 3.74 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6057955483355388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6057955483355388 | validation: 0.6826550106900436]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.61311950647407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.61311950647407 | validation: 0.8541907482107232]
	TIME [epoch: 3.71 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.64846128675264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.64846128675264 | validation: 0.6750390317955972]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_269.pth
	Model improved!!!
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7030996246115001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7030996246115001 | validation: 0.8355095247285185]
	TIME [epoch: 3.74 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6604981035739931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6604981035739931 | validation: 0.9074238835301615]
	TIME [epoch: 3.71 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7079294754703426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7079294754703426 | validation: 0.8541561563328302]
	TIME [epoch: 3.71 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.768734829370352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.768734829370352 | validation: 0.8011230205671871]
	TIME [epoch: 3.72 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6369267243690849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6369267243690849 | validation: 0.8139249495521462]
	TIME [epoch: 3.75 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5956599086189417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5956599086189417 | validation: 0.7007565517299509]
	TIME [epoch: 3.72 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5835566278519356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5835566278519356 | validation: 0.6811284778826742]
	TIME [epoch: 3.72 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5675240286600136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5675240286600136 | validation: 0.7569197308386412]
	TIME [epoch: 3.71 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5949533107080428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5949533107080428 | validation: 0.6849794925456899]
	TIME [epoch: 3.72 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6750913406949731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6750913406949731 | validation: 0.925454321747544]
	TIME [epoch: 3.71 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.790628418559313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.790628418559313 | validation: 0.8811033642457575]
	TIME [epoch: 3.71 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6835068061139896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6835068061139896 | validation: 0.7651031751732295]
	TIME [epoch: 3.72 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5793704909530514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5793704909530514 | validation: 0.7186206038620627]
	TIME [epoch: 3.71 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5752254880236091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5752254880236091 | validation: 0.7533432045989135]
	TIME [epoch: 3.71 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5730128656033672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5730128656033672 | validation: 0.7378406513614493]
	TIME [epoch: 3.71 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5986901954591888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5986901954591888 | validation: 0.6820061320031225]
	TIME [epoch: 3.71 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6620000225825445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6620000225825445 | validation: 0.7663467720259433]
	TIME [epoch: 3.72 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6143335597179391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6143335597179391 | validation: 0.7040688863189118]
	TIME [epoch: 3.72 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5496599336597054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5496599336597054 | validation: 0.6720696207164496]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_288.pth
	Model improved!!!
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5280246641097428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5280246641097428 | validation: 0.6627513019769307]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_289.pth
	Model improved!!!
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5097525331345998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5097525331345998 | validation: 0.622234598245818]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_290.pth
	Model improved!!!
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4974134147152185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4974134147152185 | validation: 0.6473930606638084]
	TIME [epoch: 3.71 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5049601664571387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5049601664571387 | validation: 0.780516550756785]
	TIME [epoch: 3.7 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5910437081261487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5910437081261487 | validation: 0.7427554548881152]
	TIME [epoch: 3.71 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7506655965428658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7506655965428658 | validation: 0.6813508867170205]
	TIME [epoch: 3.71 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5562781278656432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5562781278656432 | validation: 0.7782551603561243]
	TIME [epoch: 3.71 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5187563855972538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5187563855972538 | validation: 0.5404913924267652]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_296.pth
	Model improved!!!
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5307288031885659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5307288031885659 | validation: 0.6957755593305661]
	TIME [epoch: 3.73 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5552037955638989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5552037955638989 | validation: 0.7849986004881716]
	TIME [epoch: 3.73 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5967943648076013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5967943648076013 | validation: 0.7143429909367882]
	TIME [epoch: 3.72 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4965662275182893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4965662275182893 | validation: 0.6252019853007562]
	TIME [epoch: 3.71 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43201209023720305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43201209023720305 | validation: 0.54049128552724]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_301.pth
	Model improved!!!
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45192829894147735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45192829894147735 | validation: 0.6646935689754635]
	TIME [epoch: 3.73 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47843405092108426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47843405092108426 | validation: 0.5551628836185353]
	TIME [epoch: 3.72 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5687039899884836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5687039899884836 | validation: 0.7049994509921872]
	TIME [epoch: 3.71 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4563772861471575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4563772861471575 | validation: 0.6300115703117002]
	TIME [epoch: 3.73 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40669963832912787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40669963832912787 | validation: 0.45713487735106667]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_306.pth
	Model improved!!!
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5122630506170945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5122630506170945 | validation: 0.8705221713769702]
	TIME [epoch: 3.71 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5797903454227056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5797903454227056 | validation: 0.5528477474531918]
	TIME [epoch: 3.7 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44656949317800965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44656949317800965 | validation: 0.5711775944306696]
	TIME [epoch: 3.71 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41003243544938933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41003243544938933 | validation: 0.6533970228864702]
	TIME [epoch: 3.75 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46190795904216414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46190795904216414 | validation: 0.5861928717770847]
	TIME [epoch: 3.74 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45456349491901493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45456349491901493 | validation: 0.5609245773675503]
	TIME [epoch: 3.73 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38406088320319437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38406088320319437 | validation: 0.6318978813146661]
	TIME [epoch: 3.73 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36632019199648824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36632019199648824 | validation: 0.4385324358965867]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_314.pth
	Model improved!!!
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39855918365532944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39855918365532944 | validation: 0.7695783983679797]
	TIME [epoch: 3.7 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4645869229095183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4645869229095183 | validation: 0.45056600743771513]
	TIME [epoch: 3.71 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41482276484920055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41482276484920055 | validation: 0.5522312166919056]
	TIME [epoch: 3.71 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32394456566860724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32394456566860724 | validation: 0.5860224442221854]
	TIME [epoch: 3.72 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3277411122544669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3277411122544669 | validation: 0.4851945593977327]
	TIME [epoch: 3.71 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35800396494129755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35800396494129755 | validation: 0.7668404861192326]
	TIME [epoch: 3.71 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3969908841721958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3969908841721958 | validation: 0.44723164544723926]
	TIME [epoch: 3.72 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4392208965154181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4392208965154181 | validation: 0.6629300829328493]
	TIME [epoch: 3.7 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38006343647992125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38006343647992125 | validation: 0.542904626994991]
	TIME [epoch: 3.72 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4761766345093414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4761766345093414 | validation: 0.5870966340069511]
	TIME [epoch: 3.72 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3286026489383918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3286026489383918 | validation: 0.4664328322599577]
	TIME [epoch: 3.71 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2769345087849737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2769345087849737 | validation: 0.4997878120593828]
	TIME [epoch: 3.72 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2602686886527734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2602686886527734 | validation: 0.4533269670763728]
	TIME [epoch: 3.71 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2646249335392206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2646249335392206 | validation: 0.6133628994000712]
	TIME [epoch: 3.71 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2915891007760773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2915891007760773 | validation: 0.44004334028279324]
	TIME [epoch: 3.72 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5946792327319225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5946792327319225 | validation: 0.6957706228097633]
	TIME [epoch: 3.7 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35429707303055324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35429707303055324 | validation: 0.44556518954671687]
	TIME [epoch: 3.71 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27419207236670545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27419207236670545 | validation: 0.5595187666878108]
	TIME [epoch: 3.71 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3045113638517311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3045113638517311 | validation: 0.4837636483791177]
	TIME [epoch: 3.74 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43875649858876303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43875649858876303 | validation: 0.5020054252386306]
	TIME [epoch: 3.74 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25023613568964354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25023613568964354 | validation: 0.5809182885114927]
	TIME [epoch: 3.74 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37437044935517355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37437044935517355 | validation: 0.4756722544934549]
	TIME [epoch: 3.74 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4296808974620806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4296808974620806 | validation: 0.6563020596024347]
	TIME [epoch: 3.73 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32981168171451436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32981168171451436 | validation: 0.4693725397769701]
	TIME [epoch: 3.7 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27740898323735225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27740898323735225 | validation: 0.4563924671726016]
	TIME [epoch: 3.71 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24252974750412284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24252974750412284 | validation: 0.5868296793588677]
	TIME [epoch: 3.71 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29636365447946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29636365447946 | validation: 0.3786628528311614]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_341.pth
	Model improved!!!
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43171030523644166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43171030523644166 | validation: 0.8171961612593581]
	TIME [epoch: 3.73 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47801201313174574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47801201313174574 | validation: 0.4549819789580915]
	TIME [epoch: 3.72 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28501572177006684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28501572177006684 | validation: 0.5099408207902408]
	TIME [epoch: 3.74 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2862820244857362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2862820244857362 | validation: 0.5527042048735034]
	TIME [epoch: 3.72 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2850660877628121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2850660877628121 | validation: 0.3990015053284256]
	TIME [epoch: 3.73 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3142733784312047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3142733784312047 | validation: 0.6043648412145293]
	TIME [epoch: 3.73 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2832830367918617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2832830367918617 | validation: 0.40068055608008524]
	TIME [epoch: 3.74 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24487992354520324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24487992354520324 | validation: 0.4564933689224425]
	TIME [epoch: 3.73 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2130344591990153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2130344591990153 | validation: 0.47182044583991073]
	TIME [epoch: 3.73 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21343175408937776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21343175408937776 | validation: 0.46200206559061785]
	TIME [epoch: 3.72 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21141939946462043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21141939946462043 | validation: 0.44815276843181256]
	TIME [epoch: 3.74 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26234144465349973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26234144465349973 | validation: 0.7728424735050754]
	TIME [epoch: 3.71 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4962170470073316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4962170470073316 | validation: 0.39077914822491683]
	TIME [epoch: 3.73 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35547068263385406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35547068263385406 | validation: 0.7133481051456277]
	TIME [epoch: 3.72 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31620120051281636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31620120051281636 | validation: 0.4079471443470086]
	TIME [epoch: 3.71 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23343929183979442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23343929183979442 | validation: 0.4557079803415661]
	TIME [epoch: 3.71 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22322546427993778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22322546427993778 | validation: 0.4620023301031102]
	TIME [epoch: 3.72 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23075733689956263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23075733689956263 | validation: 0.4671093326080614]
	TIME [epoch: 3.71 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2240001516926648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2240001516926648 | validation: 0.42948675646739337]
	TIME [epoch: 3.73 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2341949373086294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2341949373086294 | validation: 0.5978503377117284]
	TIME [epoch: 3.72 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29957476424693397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29957476424693397 | validation: 0.3833481778651624]
	TIME [epoch: 3.71 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3682808397247721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3682808397247721 | validation: 0.7498139572022885]
	TIME [epoch: 3.72 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3707061778120533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3707061778120533 | validation: 0.400645127417231]
	TIME [epoch: 3.72 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2173165063715266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2173165063715266 | validation: 0.43152530291664903]
	TIME [epoch: 3.71 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21435706829292003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21435706829292003 | validation: 0.5153368019472412]
	TIME [epoch: 3.71 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23744076697675284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23744076697675284 | validation: 0.4174614661955155]
	TIME [epoch: 3.7 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22558129325946413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22558129325946413 | validation: 0.4700430265518639]
	TIME [epoch: 3.71 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22500930374455125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22500930374455125 | validation: 0.4609128142148185]
	TIME [epoch: 3.68 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24098060103674365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24098060103674365 | validation: 0.49944581556041556]
	TIME [epoch: 3.71 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22139684340207894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22139684340207894 | validation: 0.39881821214297686]
	TIME [epoch: 3.69 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28499123066144444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28499123066144444 | validation: 0.7120936450052439]
	TIME [epoch: 3.69 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3367984305480325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3367984305480325 | validation: 0.3904894035768943]
	TIME [epoch: 3.73 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3221479241969315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3221479241969315 | validation: 0.4430198265822114]
	TIME [epoch: 3.69 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1865595106063326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1865595106063326 | validation: 0.5152976374015612]
	TIME [epoch: 3.7 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21069656602040077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21069656602040077 | validation: 0.4019912876012323]
	TIME [epoch: 3.72 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19569507057273833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19569507057273833 | validation: 0.4401936494322806]
	TIME [epoch: 3.71 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20521597609391554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20521597609391554 | validation: 0.491865341122854]
	TIME [epoch: 3.7 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2320035258941202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2320035258941202 | validation: 0.45947416267065866]
	TIME [epoch: 3.71 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21247311063762403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21247311063762403 | validation: 0.40890810102974673]
	TIME [epoch: 3.71 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23454254939867952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23454254939867952 | validation: 0.7850307066192104]
	TIME [epoch: 3.71 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44576803285606675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44576803285606675 | validation: 0.3962557184278702]
	TIME [epoch: 3.71 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26070742327907176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26070742327907176 | validation: 0.46273858723618116]
	TIME [epoch: 3.7 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1984064474506824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1984064474506824 | validation: 0.43757072131116126]
	TIME [epoch: 3.72 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18295388042605373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18295388042605373 | validation: 0.3978533092423442]
	TIME [epoch: 3.72 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1690963110265232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1690963110265232 | validation: 0.473783740884099]
	TIME [epoch: 3.7 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17525912360092336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17525912360092336 | validation: 0.3937326136817865]
	TIME [epoch: 3.71 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20668679229542272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20668679229542272 | validation: 0.6198545100005086]
	TIME [epoch: 3.71 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3382277176790649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3382277176790649 | validation: 0.40370029552286457]
	TIME [epoch: 3.71 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5277740825670912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5277740825670912 | validation: 0.5889251937140713]
	TIME [epoch: 3.71 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27949833755486736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27949833755486736 | validation: 0.5153746352241707]
	TIME [epoch: 3.7 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28798839906251217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28798839906251217 | validation: 0.40297911772183304]
	TIME [epoch: 3.71 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19537245309996137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19537245309996137 | validation: 0.44530422016530596]
	TIME [epoch: 3.71 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2236825452313044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2236825452313044 | validation: 0.4342532872947543]
	TIME [epoch: 3.71 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18193611350840222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18193611350840222 | validation: 0.4135228488414031]
	TIME [epoch: 3.71 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18258880014498022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18258880014498022 | validation: 0.4052146318377721]
	TIME [epoch: 3.71 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16929042574524394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16929042574524394 | validation: 0.4390562340735626]
	TIME [epoch: 3.72 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16718740806915064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16718740806915064 | validation: 0.3629283520780089]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_398.pth
	Model improved!!!
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18246670476490112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18246670476490112 | validation: 0.6781729709438484]
	TIME [epoch: 3.72 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27904277836048663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27904277836048663 | validation: 0.34039712335416705]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_400.pth
	Model improved!!!
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36574165504462286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36574165504462286 | validation: 0.5085793273274039]
	TIME [epoch: 3.71 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25306768687535114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25306768687535114 | validation: 0.5475040556250298]
	TIME [epoch: 3.7 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24373765800322653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24373765800322653 | validation: 0.4289896031494932]
	TIME [epoch: 3.72 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19539497467961994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19539497467961994 | validation: 0.4094293909640425]
	TIME [epoch: 3.71 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1750356141487562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1750356141487562 | validation: 0.43971743139974073]
	TIME [epoch: 3.71 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17536773402994832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17536773402994832 | validation: 0.4139651511579362]
	TIME [epoch: 3.71 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17521130825677608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17521130825677608 | validation: 0.3658177413166108]
	TIME [epoch: 3.71 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23958550821798177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23958550821798177 | validation: 0.6649483371284053]
	TIME [epoch: 3.71 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34037040980968114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34037040980968114 | validation: 0.37742167708994834]
	TIME [epoch: 3.73 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19513926701372775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19513926701372775 | validation: 0.4310060051995859]
	TIME [epoch: 3.72 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1565253862643185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1565253862643185 | validation: 0.433191141672379]
	TIME [epoch: 3.72 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17202526824407235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17202526824407235 | validation: 0.39513414781045636]
	TIME [epoch: 3.72 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20220434203285934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20220434203285934 | validation: 0.4971971177063201]
	TIME [epoch: 3.71 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24068171916650463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24068171916650463 | validation: 0.39125107684208144]
	TIME [epoch: 3.71 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2542211424607414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2542211424607414 | validation: 0.4988542589234468]
	TIME [epoch: 3.7 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23678825493791944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23678825493791944 | validation: 0.3706749352458272]
	TIME [epoch: 3.71 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2535429596191736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2535429596191736 | validation: 0.4579066104217187]
	TIME [epoch: 3.71 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16878005564971724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16878005564971724 | validation: 0.39770786968437166]
	TIME [epoch: 3.7 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1758310761646109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1758310761646109 | validation: 0.41497278824978834]
	TIME [epoch: 3.71 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17216115243708488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17216115243708488 | validation: 0.4069741758364826]
	TIME [epoch: 3.71 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1583103405318461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1583103405318461 | validation: 0.4282001026601885]
	TIME [epoch: 3.71 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1601269546756482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1601269546756482 | validation: 0.3904141982815489]
	TIME [epoch: 3.74 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16898125978353246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16898125978353246 | validation: 0.5106910037021267]
	TIME [epoch: 3.71 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18637167988228281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18637167988228281 | validation: 0.37576183977022926]
	TIME [epoch: 3.71 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29459198401753073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29459198401753073 | validation: 0.7976343504179049]
	TIME [epoch: 3.72 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3748074746207108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3748074746207108 | validation: 0.3818978464081901]
	TIME [epoch: 3.71 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17579006605865397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17579006605865397 | validation: 0.3875235408969986]
	TIME [epoch: 3.72 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19633958956279438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19633958956279438 | validation: 0.43281622145624693]
	TIME [epoch: 3.71 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1789624353154477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1789624353154477 | validation: 0.4643120252078908]
	TIME [epoch: 3.71 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.157434732125851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.157434732125851 | validation: 0.3879772298635846]
	TIME [epoch: 3.72 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15331049242694647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15331049242694647 | validation: 0.4072159791835068]
	TIME [epoch: 3.7 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15987753563402934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15987753563402934 | validation: 0.40168084020808686]
	TIME [epoch: 3.71 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15939161949687147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15939161949687147 | validation: 0.45669496716063096]
	TIME [epoch: 3.73 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19484467647258355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19484467647258355 | validation: 0.37306178605131185]
	TIME [epoch: 3.72 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3278485403411226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3278485403411226 | validation: 0.6891389968175614]
	TIME [epoch: 3.73 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32818856341857616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32818856341857616 | validation: 0.39196031129955533]
	TIME [epoch: 3.72 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1822441223959388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1822441223959388 | validation: 0.36626494055140735]
	TIME [epoch: 3.7 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19985706087075814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19985706087075814 | validation: 0.42024527161630065]
	TIME [epoch: 3.71 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.166497943275145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.166497943275145 | validation: 0.38550321789958397]
	TIME [epoch: 3.7 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15056004860635547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15056004860635547 | validation: 0.39665978775607713]
	TIME [epoch: 3.71 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1561143856615653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1561143856615653 | validation: 0.3806322745106818]
	TIME [epoch: 3.72 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16046943403129635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16046943403129635 | validation: 0.4602369583343917]
	TIME [epoch: 3.7 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1652201913892545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1652201913892545 | validation: 0.38585328804635344]
	TIME [epoch: 3.71 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18376203399430982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18376203399430982 | validation: 0.628520534527655]
	TIME [epoch: 3.72 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21869853000232092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21869853000232092 | validation: 0.33513090165628534]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_445.pth
	Model improved!!!
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19852172833514986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19852172833514986 | validation: 0.5340259775821693]
	TIME [epoch: 3.73 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2531193783179471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2531193783179471 | validation: 0.38391748719645824]
	TIME [epoch: 3.72 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1964602026340201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1964602026340201 | validation: 0.41630128001735534]
	TIME [epoch: 3.72 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17555642090109858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17555642090109858 | validation: 0.39899637406828153]
	TIME [epoch: 3.71 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13336778486253387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13336778486253387 | validation: 0.3550613340709566]
	TIME [epoch: 3.7 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13877861888851029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13877861888851029 | validation: 0.4276123313067981]
	TIME [epoch: 3.72 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14063203616629974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14063203616629974 | validation: 0.3419216595858748]
	TIME [epoch: 3.71 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1835053843517523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1835053843517523 | validation: 0.6662304392405476]
	TIME [epoch: 3.71 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29999497201591246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29999497201591246 | validation: 0.36062539661044285]
	TIME [epoch: 3.72 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1950393703605005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1950393703605005 | validation: 0.40173601538798054]
	TIME [epoch: 3.72 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1394948034541404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1394948034541404 | validation: 0.41947526925332745]
	TIME [epoch: 3.71 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14085314282844943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14085314282844943 | validation: 0.35298952265892114]
	TIME [epoch: 3.72 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16349452840181536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16349452840181536 | validation: 0.4912739073247521]
	TIME [epoch: 3.73 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26322298971697633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26322298971697633 | validation: 0.4221620842326992]
	TIME [epoch: 3.73 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21307250035064276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21307250035064276 | validation: 0.38462494578338746]
	TIME [epoch: 3.73 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14647326018209586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14647326018209586 | validation: 0.3711263386091041]
	TIME [epoch: 3.71 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14573466336902585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14573466336902585 | validation: 0.44724441016541044]
	TIME [epoch: 3.71 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16878581047518007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16878581047518007 | validation: 0.4008688470066255]
	TIME [epoch: 3.7 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17050907423087477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17050907423087477 | validation: 0.44063780241087047]
	TIME [epoch: 3.71 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14253669555129544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14253669555129544 | validation: 0.35412290798726176]
	TIME [epoch: 3.72 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13140239959411992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13140239959411992 | validation: 0.4261235988933959]
	TIME [epoch: 3.71 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12852793795835932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12852793795835932 | validation: 1.0550998487683927]
	TIME [epoch: 3.71 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3452159405749466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3452159405749466 | validation: 1.0914108848864608]
	TIME [epoch: 3.71 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4169302560652814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4169302560652814 | validation: 0.870287710620929]
	TIME [epoch: 3.72 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2156338205886346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2156338205886346 | validation: 0.6754948846639284]
	TIME [epoch: 3.71 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7105532232323116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7105532232323116 | validation: 0.42720523862087045]
	TIME [epoch: 3.71 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3158295668449054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3158295668449054 | validation: 0.5275675065706821]
	TIME [epoch: 3.72 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23790613677960692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23790613677960692 | validation: 0.5557686437825619]
	TIME [epoch: 3.72 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4752480039345913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4752480039345913 | validation: 0.5508301554694989]
	TIME [epoch: 3.7 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5151956012567039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5151956012567039 | validation: 0.5313622852477697]
	TIME [epoch: 3.71 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48879273884096286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48879273884096286 | validation: 0.522354887182732]
	TIME [epoch: 3.72 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.386065750162581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.386065750162581 | validation: 0.49210609575653746]
	TIME [epoch: 3.71 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31121734705832127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31121734705832127 | validation: 0.4805496029403338]
	TIME [epoch: 3.71 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27520243200334904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27520243200334904 | validation: 0.497377536350631]
	TIME [epoch: 3.71 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2704724571185603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2704724571185603 | validation: 0.536101935336692]
	TIME [epoch: 3.71 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2662071354456692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2662071354456692 | validation: 0.5206923463398464]
	TIME [epoch: 3.71 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2519884280442218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2519884280442218 | validation: 0.4907391994648836]
	TIME [epoch: 3.72 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2448840185343145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2448840185343145 | validation: 0.4772414419893295]
	TIME [epoch: 3.72 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2325455191259755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2325455191259755 | validation: 0.4785528065606137]
	TIME [epoch: 3.72 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22806677988182875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22806677988182875 | validation: 0.48584648757303867]
	TIME [epoch: 3.72 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22590629722877167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22590629722877167 | validation: 0.49343261845412684]
	TIME [epoch: 3.72 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2320249439630558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2320249439630558 | validation: 0.5015734753153852]
	TIME [epoch: 3.71 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22985943645233073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22985943645233073 | validation: 0.46989613774900935]
	TIME [epoch: 3.73 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.210274815336358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.210274815336358 | validation: 0.4710263258171463]
	TIME [epoch: 3.71 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19255574860354968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19255574860354968 | validation: 0.457968033705096]
	TIME [epoch: 3.7 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18720110346939287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18720110346939287 | validation: 0.45557038279360057]
	TIME [epoch: 3.71 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18198092433743035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18198092433743035 | validation: 0.43626629545608875]
	TIME [epoch: 3.73 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17628388287730687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17628388287730687 | validation: 0.4390412811721346]
	TIME [epoch: 3.71 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17495635851516292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17495635851516292 | validation: 0.4408396779226422]
	TIME [epoch: 3.72 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1768114391007468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1768114391007468 | validation: 0.4521394169813462]
	TIME [epoch: 3.71 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17560516911493967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17560516911493967 | validation: 0.47339015075817414]
	TIME [epoch: 3.73 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2009709331139159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2009709331139159 | validation: 0.4346330029465746]
	TIME [epoch: 3.73 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19361254410829584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19361254410829584 | validation: 0.4213966903771148]
	TIME [epoch: 3.72 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16146199344533543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16146199344533543 | validation: 0.4052440433825517]
	TIME [epoch: 3.72 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17355158507585905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17355158507585905 | validation: 0.4902675151587184]
	TIME [epoch: 3.72 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19418546985660798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19418546985660798 | validation: 0.3702741366506862]
	TIME [epoch: 49.4 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1934664239169006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1934664239169006 | validation: 0.41111727386992425]
	TIME [epoch: 8.1 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14411440426252234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14411440426252234 | validation: 0.40605333732499854]
	TIME [epoch: 8.07 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1385131131758617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1385131131758617 | validation: 0.36762490334138803]
	TIME [epoch: 8.05 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14762716782863147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14762716782863147 | validation: 0.44237934296935344]
	TIME [epoch: 8.05 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1639322414899884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1639322414899884 | validation: 0.37931106264353237]
	TIME [epoch: 8.09 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1708979753696036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1708979753696036 | validation: 0.5039111786866156]
	TIME [epoch: 8.07 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20103583254196772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20103583254196772 | validation: 0.36770120124654365]
	TIME [epoch: 8.06 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13637724937005927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13637724937005927 | validation: 0.3726847303456522]
	TIME [epoch: 8.05 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1223636540220796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1223636540220796 | validation: 0.38594635871479177]
	TIME [epoch: 8.05 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11862048833176177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11862048833176177 | validation: 0.3922517066193935]
	TIME [epoch: 8.04 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12530018856120154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12530018856120154 | validation: 0.33390417675839973]
	TIME [epoch: 8.05 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_512.pth
	Model improved!!!
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14547921428039043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14547921428039043 | validation: 0.6290840681262049]
	TIME [epoch: 8.06 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2537773090240513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2537773090240513 | validation: 0.30498283568352197]
	TIME [epoch: 8.06 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_514.pth
	Model improved!!!
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2624275247678434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2624275247678434 | validation: 0.39480183354956777]
	TIME [epoch: 8.05 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1840703524850083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1840703524850083 | validation: 0.4247049442508164]
	TIME [epoch: 8.03 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21668239707040818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21668239707040818 | validation: 0.3578248013885988]
	TIME [epoch: 8.04 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1325576033050386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1325576033050386 | validation: 0.3616371032664518]
	TIME [epoch: 8.04 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16599680980112028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16599680980112028 | validation: 0.4613820485678387]
	TIME [epoch: 8.07 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2193157952090717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2193157952090717 | validation: 0.39793111658385244]
	TIME [epoch: 8.06 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16529835561879083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16529835561879083 | validation: 0.3453245214017647]
	TIME [epoch: 8.06 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13031581857200675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13031581857200675 | validation: 0.37517447500406753]
	TIME [epoch: 8.06 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1189817530556953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1189817530556953 | validation: 0.3972466294409455]
	TIME [epoch: 8.05 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.136241925983184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.136241925983184 | validation: 0.3070682578668719]
	TIME [epoch: 8.05 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17804919357589252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17804919357589252 | validation: 0.6673890734608244]
	TIME [epoch: 8.08 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2789693616227276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2789693616227276 | validation: 0.3077051024037886]
	TIME [epoch: 8.08 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13611557112517159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13611557112517159 | validation: 0.3812159142217233]
	TIME [epoch: 8.05 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12528624730732313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12528624730732313 | validation: 0.3718436207623637]
	TIME [epoch: 8.05 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14136565383058908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14136565383058908 | validation: 0.35456977086878494]
	TIME [epoch: 8.05 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14820803594744905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14820803594744905 | validation: 0.3677553500504158]
	TIME [epoch: 8.05 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1867307100464405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1867307100464405 | validation: 0.5558077693767258]
	TIME [epoch: 8.07 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20804837020261702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20804837020261702 | validation: 0.3408426095719865]
	TIME [epoch: 8.06 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1448129917211916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1448129917211916 | validation: 0.3743115346551978]
	TIME [epoch: 8.06 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1545108492591831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1545108492591831 | validation: 0.3681826458994973]
	TIME [epoch: 8.05 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1458615444612318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1458615444612318 | validation: 0.3625011980354252]
	TIME [epoch: 8.05 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14054526883821786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14054526883821786 | validation: 0.3125759711626687]
	TIME [epoch: 8.04 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12084297209817897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12084297209817897 | validation: 0.44196282689491845]
	TIME [epoch: 8.07 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15791055523940167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15791055523940167 | validation: 0.28750215787521555]
	TIME [epoch: 8.07 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_538.pth
	Model improved!!!
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.335329113463018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.335329113463018 | validation: 0.47534000489453443]
	TIME [epoch: 8.08 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20490209129290576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20490209129290576 | validation: 0.3946708004047692]
	TIME [epoch: 8.09 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16251928502043178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16251928502043178 | validation: 0.346392632294946]
	TIME [epoch: 8.08 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20558676066487522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20558676066487522 | validation: 0.42999619324197885]
	TIME [epoch: 8.08 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15663697708043653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15663697708043653 | validation: 0.3680107311477186]
	TIME [epoch: 8.1 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12372947524631252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12372947524631252 | validation: 0.30239149597384873]
	TIME [epoch: 8.08 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11831212419042675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11831212419042675 | validation: 0.3683576033447804]
	TIME [epoch: 8.09 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12124883735811284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12124883735811284 | validation: 0.3241708097154927]
	TIME [epoch: 8.06 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11540063886668211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11540063886668211 | validation: 0.3833540597129639]
	TIME [epoch: 8.08 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12542270256963559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12542270256963559 | validation: 0.3581139550053638]
	TIME [epoch: 8.08 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1520156491086253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1520156491086253 | validation: 0.37935916842067435]
	TIME [epoch: 8.1 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16033934300208938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16033934300208938 | validation: 0.3769699226416229]
	TIME [epoch: 8.12 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1615779163259411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1615779163259411 | validation: 0.3689295322142121]
	TIME [epoch: 8.1 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17954625233156904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17954625233156904 | validation: 0.4417258741095818]
	TIME [epoch: 8.07 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19693276792852643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19693276792852643 | validation: 0.31876167222078367]
	TIME [epoch: 8.08 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18577788075519294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18577788075519294 | validation: 0.5288749580983753]
	TIME [epoch: 8.08 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22066976440204022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22066976440204022 | validation: 0.3297856724944441]
	TIME [epoch: 8.09 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14466153474063576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14466153474063576 | validation: 0.36840004982149427]
	TIME [epoch: 8.12 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15553503069433455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15553503069433455 | validation: 0.3726684274610917]
	TIME [epoch: 8.09 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1349800297903419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1349800297903419 | validation: 0.32377413075126227]
	TIME [epoch: 8.1 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11011177962598975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11011177962598975 | validation: 0.3383484070440155]
	TIME [epoch: 8.08 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10343220914714926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10343220914714926 | validation: 0.33327178813847036]
	TIME [epoch: 8.06 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10724283668069912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10724283668069912 | validation: 0.4044495442436179]
	TIME [epoch: 8.06 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13299720923381386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13299720923381386 | validation: 0.3263077141373172]
	TIME [epoch: 8.08 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14832268290628917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14832268290628917 | validation: 0.46624334374998555]
	TIME [epoch: 8.09 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13930814880020945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13930814880020945 | validation: 0.301909274127597]
	TIME [epoch: 8.09 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1327084938247442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1327084938247442 | validation: 0.5694015908855473]
	TIME [epoch: 8.1 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1782822154790312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1782822154790312 | validation: 0.3066707256359015]
	TIME [epoch: 8.09 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23774884545968786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23774884545968786 | validation: 0.46045854449059614]
	TIME [epoch: 8.08 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2830421821536645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2830421821536645 | validation: 0.3967049415032464]
	TIME [epoch: 8.1 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13344448723203248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13344448723203248 | validation: 0.35321088481454255]
	TIME [epoch: 8.1 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17581041414269868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17581041414269868 | validation: 0.3408072538500353]
	TIME [epoch: 8.08 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1685030754368221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1685030754368221 | validation: 0.37324388737269126]
	TIME [epoch: 8.09 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12973443279430538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12973443279430538 | validation: 0.3377930930878581]
	TIME [epoch: 8.08 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11061925587793427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11061925587793427 | validation: 0.31957298656258615]
	TIME [epoch: 8.1 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10875219296076843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10875219296076843 | validation: 0.3304351538790486]
	TIME [epoch: 8.1 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.105491851882218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.105491851882218 | validation: 0.3040776760090656]
	TIME [epoch: 8.09 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09950332666683977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09950332666683977 | validation: 0.3548627167591746]
	TIME [epoch: 8.08 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09988977822698075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09988977822698075 | validation: 0.2991507868314432]
	TIME [epoch: 8.09 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11626163326097626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11626163326097626 | validation: 0.671563995628476]
	TIME [epoch: 8.08 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24779636088268744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24779636088268744 | validation: 0.3145092073364391]
	TIME [epoch: 8.08 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19525481679001627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19525481679001627 | validation: 0.36628274463000343]
	TIME [epoch: 8.12 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2285861648769737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2285861648769737 | validation: 0.516751303871642]
	TIME [epoch: 8.11 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15954634405139448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15954634405139448 | validation: 0.4289311544890076]
	TIME [epoch: 8.1 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12672666607034555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12672666607034555 | validation: 0.31888157904406117]
	TIME [epoch: 8.09 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12032584045636499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12032584045636499 | validation: 0.3000352153881156]
	TIME [epoch: 8.08 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1070770950996345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1070770950996345 | validation: 0.36998003238623617]
	TIME [epoch: 8.09 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1305188902562604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1305188902562604 | validation: 0.28863584335538256]
	TIME [epoch: 8.1 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14913104392919788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14913104392919788 | validation: 0.3936374679649066]
	TIME [epoch: 8.11 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19043348347979042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19043348347979042 | validation: 0.37272731163937367]
	TIME [epoch: 8.1 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1530586586165812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1530586586165812 | validation: 0.3442853536978866]
	TIME [epoch: 8.08 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14354407354335258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14354407354335258 | validation: 0.3250553923777423]
	TIME [epoch: 8.08 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12282295143521477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12282295143521477 | validation: 0.3233540913526733]
	TIME [epoch: 8.1 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10214089898751783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10214089898751783 | validation: 0.2880266577488416]
	TIME [epoch: 8.1 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09536751223599627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09536751223599627 | validation: 0.3318555660493481]
	TIME [epoch: 8.1 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09624266255962312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09624266255962312 | validation: 0.2834193643065534]
	TIME [epoch: 8.08 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_594.pth
	Model improved!!!
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11805249392767371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11805249392767371 | validation: 0.5743226196862777]
	TIME [epoch: 8.1 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2897144560846925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2897144560846925 | validation: 0.27355438626331885]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_596.pth
	Model improved!!!
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14780629529927744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14780629529927744 | validation: 0.29596201463115884]
	TIME [epoch: 8.1 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11121019835798521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11121019835798521 | validation: 0.3691986466073074]
	TIME [epoch: 8.11 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10855466558522774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10855466558522774 | validation: 0.3009411223262868]
	TIME [epoch: 8.11 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1175728771798881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1175728771798881 | validation: 0.34649669933542776]
	TIME [epoch: 8.08 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17714777998694956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17714777998694956 | validation: 0.505588344404999]
	TIME [epoch: 8.07 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20363857739213703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20363857739213703 | validation: 0.31395393034118624]
	TIME [epoch: 8.1 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12197891989200325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12197891989200325 | validation: 0.3687515248053471]
	TIME [epoch: 8.11 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10963781699054542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10963781699054542 | validation: 0.29005220838054196]
	TIME [epoch: 8.09 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10864525335336371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10864525335336371 | validation: 0.3485023818550441]
	TIME [epoch: 8.1 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11748170375818169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11748170375818169 | validation: 0.2873030640003943]
	TIME [epoch: 8.08 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10681321640967918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10681321640967918 | validation: 0.3050044952972073]
	TIME [epoch: 8.08 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10866777036257215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10866777036257215 | validation: 0.38807323834100327]
	TIME [epoch: 8.08 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17213723106490172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17213723106490172 | validation: 0.41941243484557433]
	TIME [epoch: 8.1 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2531396245604934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2531396245604934 | validation: 0.3227809904367197]
	TIME [epoch: 8.09 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09280388786462386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09280388786462386 | validation: 0.2724601279502823]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_611.pth
	Model improved!!!
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13530234331973776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13530234331973776 | validation: 0.3947758465602189]
	TIME [epoch: 8.1 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14568013573573288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14568013573573288 | validation: 0.3321597516430299]
	TIME [epoch: 8.1 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14060195409586673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14060195409586673 | validation: 0.3599078227261414]
	TIME [epoch: 8.1 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17237842993508662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17237842993508662 | validation: 0.34465901183450764]
	TIME [epoch: 8.11 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15029572537951616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15029572537951616 | validation: 0.305831611705338]
	TIME [epoch: 8.11 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10820969619292983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10820969619292983 | validation: 0.39441226131167656]
	TIME [epoch: 8.11 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10777376207820138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10777376207820138 | validation: 0.2689652634542533]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_618.pth
	Model improved!!!
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10179681630245196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10179681630245196 | validation: 0.3902636164383994]
	TIME [epoch: 8.1 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10159176839119383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10159176839119383 | validation: 0.2819717704308388]
	TIME [epoch: 8.1 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09327839164520867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09327839164520867 | validation: 0.393707955451751]
	TIME [epoch: 8.1 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1086341942212869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1086341942212869 | validation: 0.25953326681145933]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_622.pth
	Model improved!!!
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18088833543350707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18088833543350707 | validation: 0.47806017107058296]
	TIME [epoch: 8.09 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23237060342047347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23237060342047347 | validation: 0.38773760864611734]
	TIME [epoch: 8.11 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12367806483688455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12367806483688455 | validation: 0.2963230074487931]
	TIME [epoch: 8.09 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14765140303993457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14765140303993457 | validation: 0.45097051966678103]
	TIME [epoch: 8.09 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12352750715156359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12352750715156359 | validation: 0.39822354677257144]
	TIME [epoch: 8.1 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12213567444189623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12213567444189623 | validation: 0.3015292914712921]
	TIME [epoch: 8.09 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13374803206866484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13374803206866484 | validation: 0.3517565118159787]
	TIME [epoch: 8.09 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10964690346399686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10964690346399686 | validation: 0.3232735523282342]
	TIME [epoch: 8.1 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10151502279266567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10151502279266567 | validation: 0.2864016338866111]
	TIME [epoch: 8.09 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09971964348274356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09971964348274356 | validation: 0.3580062905032798]
	TIME [epoch: 8.1 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.112550453274471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.112550453274471 | validation: 0.28149136166056493]
	TIME [epoch: 8.09 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12454025408508196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12454025408508196 | validation: 0.4407782583020694]
	TIME [epoch: 8.08 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14137662479079613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14137662479079613 | validation: 0.2894219478752169]
	TIME [epoch: 8.1 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1261496704064658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1261496704064658 | validation: 0.3220630476253237]
	TIME [epoch: 8.1 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10742889320065327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10742889320065327 | validation: 0.30340831733645984]
	TIME [epoch: 8.1 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09237903938128428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09237903938128428 | validation: 0.26986883058303385]
	TIME [epoch: 8.09 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0887340269439229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0887340269439229 | validation: 0.42117294741625566]
	TIME [epoch: 8.09 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10655989625184357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10655989625184357 | validation: 0.2722963584138564]
	TIME [epoch: 8.09 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08631764723113221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08631764723113221 | validation: 0.3688091039762951]
	TIME [epoch: 8.1 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10862248632313357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10862248632313357 | validation: 0.3183382538932841]
	TIME [epoch: 8.11 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19416758926068556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19416758926068556 | validation: 0.28426135516765605]
	TIME [epoch: 8.09 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.237053609635962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.237053609635962 | validation: 0.522649634772292]
	TIME [epoch: 8.09 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2358428110106467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2358428110106467 | validation: 0.3320269557501518]
	TIME [epoch: 8.09 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1590370213786341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1590370213786341 | validation: 0.27821354347462196]
	TIME [epoch: 8.09 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18895897731539904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18895897731539904 | validation: 0.2986590338340361]
	TIME [epoch: 8.1 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11163636600771952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11163636600771952 | validation: 0.3177374781501181]
	TIME [epoch: 8.11 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10021960832963327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10021960832963327 | validation: 0.3082949684892406]
	TIME [epoch: 8.1 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08592076826483509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08592076826483509 | validation: 0.2740015507896163]
	TIME [epoch: 8.12 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0863393564837159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0863393564837159 | validation: 0.2794589059909093]
	TIME [epoch: 8.1 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07997581522742551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07997581522742551 | validation: 0.29899682245154896]
	TIME [epoch: 8.09 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08266379898144013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08266379898144013 | validation: 0.26190143291711954]
	TIME [epoch: 8.11 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08666450943058603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08666450943058603 | validation: 0.41135083847734377]
	TIME [epoch: 8.11 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11708107823047001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11708107823047001 | validation: 0.3023815362675632]
	TIME [epoch: 8.09 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1645333762004305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1645333762004305 | validation: 0.3815789626153801]
	TIME [epoch: 8.1 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14046088787901514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14046088787901514 | validation: 0.2784621148863644]
	TIME [epoch: 8.1 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08881203546492535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08881203546492535 | validation: 0.2583743161638154]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_658.pth
	Model improved!!!
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07943415210099623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07943415210099623 | validation: 0.3858252137210573]
	TIME [epoch: 8.07 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10155676831016697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10155676831016697 | validation: 0.23939154245677796]
	TIME [epoch: 8.07 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_660.pth
	Model improved!!!
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10626906163008099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10626906163008099 | validation: 0.3294571561408868]
	TIME [epoch: 8.09 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13366959638711032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13366959638711032 | validation: 0.3269431012936623]
	TIME [epoch: 8.09 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10857378883622416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10857378883622416 | validation: 0.2683288269151527]
	TIME [epoch: 8.09 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12391441860132307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12391441860132307 | validation: 0.40266726840800565]
	TIME [epoch: 8.09 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10600517437611853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10600517437611853 | validation: 0.25827106337481515]
	TIME [epoch: 8.1 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09750024892397552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09750024892397552 | validation: 0.3164123958486276]
	TIME [epoch: 8.09 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12259227919967927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12259227919967927 | validation: 0.32541002962097904]
	TIME [epoch: 8.09 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14901970626613406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14901970626613406 | validation: 0.31576244256973435]
	TIME [epoch: 8.09 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14665607505084277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14665607505084277 | validation: 0.37892404559802717]
	TIME [epoch: 8.1 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10558367170295534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10558367170295534 | validation: 0.252277405536835]
	TIME [epoch: 8.07 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07721647218622044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07721647218622044 | validation: 0.2501141138000868]
	TIME [epoch: 8.09 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10342238597982846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10342238597982846 | validation: 0.4154518837712174]
	TIME [epoch: 8.11 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14025325734483823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14025325734483823 | validation: 0.27857255403304143]
	TIME [epoch: 8.11 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15627277423732977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15627277423732977 | validation: 0.30177035338472535]
	TIME [epoch: 8.09 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17793801335695628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17793801335695628 | validation: 0.28854026585002446]
	TIME [epoch: 8.1 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08265407554198506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08265407554198506 | validation: 0.24543569783948996]
	TIME [epoch: 8.09 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08255744076494398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08255744076494398 | validation: 0.23877523587111849]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_677.pth
	Model improved!!!
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08275300538315353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08275300538315353 | validation: 0.3361314121579798]
	TIME [epoch: 8.1 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08121882284271074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08121882284271074 | validation: 0.24361353544446296]
	TIME [epoch: 8.11 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07886668444464713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07886668444464713 | validation: 0.2672856087204343]
	TIME [epoch: 8.09 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09389677689592585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09389677689592585 | validation: 0.2894952849159327]
	TIME [epoch: 8.09 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09126371587127675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09126371587127675 | validation: 0.2732481959692887]
	TIME [epoch: 8.08 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1110324452909774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1110324452909774 | validation: 0.28593614598065015]
	TIME [epoch: 8.09 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1704130742716878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1704130742716878 | validation: 0.5259445724012574]
	TIME [epoch: 8.12 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16331980519961625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16331980519961625 | validation: 0.24611100729867375]
	TIME [epoch: 8.11 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08652636092145227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08652636092145227 | validation: 0.23465581087355575]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_686.pth
	Model improved!!!
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0769659367705701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0769659367705701 | validation: 0.3110289505461794]
	TIME [epoch: 8.09 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08435545523976443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08435545523976443 | validation: 0.3537956641006689]
	TIME [epoch: 8.07 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1279092184986025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1279092184986025 | validation: 0.24216113845718829]
	TIME [epoch: 8.09 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1723255486201087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1723255486201087 | validation: 0.30502746642581746]
	TIME [epoch: 8.11 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14410939442083173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14410939442083173 | validation: 0.3425272436978392]
	TIME [epoch: 8.08 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08705039969112964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08705039969112964 | validation: 0.22122770974831749]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_692.pth
	Model improved!!!
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08181679909825952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08181679909825952 | validation: 0.24616515008994855]
	TIME [epoch: 8.08 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08171158727773388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08171158727773388 | validation: 0.3474846628250817]
	TIME [epoch: 8.07 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0870126603495757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0870126603495757 | validation: 0.24798917576040333]
	TIME [epoch: 8.08 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07663066919283376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07663066919283376 | validation: 0.24943310762444423]
	TIME [epoch: 8.11 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07728676766965223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07728676766965223 | validation: 0.283678742324922]
	TIME [epoch: 8.09 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09766756204820486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09766756204820486 | validation: 0.2602590815060685]
	TIME [epoch: 8.07 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1349002887892773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1349002887892773 | validation: 0.34525275278336975]
	TIME [epoch: 8.07 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12878637810355892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12878637810355892 | validation: 0.26124631871470066]
	TIME [epoch: 8.07 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13612470529246593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13612470529246593 | validation: 0.32069810454930425]
	TIME [epoch: 8.1 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09141341269983565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09141341269983565 | validation: 0.22806041540693966]
	TIME [epoch: 8.09 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08294093559066461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08294093559066461 | validation: 0.248371752240543]
	TIME [epoch: 8.1 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08101324583655525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08101324583655525 | validation: 0.2840455235785627]
	TIME [epoch: 8.09 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0962120480273465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0962120480273465 | validation: 0.19549261091089942]
	TIME [epoch: 8.08 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_705.pth
	Model improved!!!
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10768083906239909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10768083906239909 | validation: 0.28241263131416955]
	TIME [epoch: 8.1 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1500036265679753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1500036265679753 | validation: 0.25803816931162304]
	TIME [epoch: 8.08 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07184233738767565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07184233738767565 | validation: 0.20042509253446983]
	TIME [epoch: 8.1 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07775498677275003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07775498677275003 | validation: 0.3600711508424448]
	TIME [epoch: 8.1 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0991506653666022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0991506653666022 | validation: 0.2658816950013161]
	TIME [epoch: 8.07 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12363273536983103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12363273536983103 | validation: 0.2917300861148269]
	TIME [epoch: 8.09 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12237167914459325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12237167914459325 | validation: 0.2954096813498982]
	TIME [epoch: 8.08 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1081164662199406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1081164662199406 | validation: 0.23360350250386555]
	TIME [epoch: 8.09 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08886622881675942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08886622881675942 | validation: 0.3082026428867574]
	TIME [epoch: 8.1 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07257024666348312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07257024666348312 | validation: 0.21799905624857177]
	TIME [epoch: 8.11 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06444170057697443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06444170057697443 | validation: 0.24332572512498316]
	TIME [epoch: 8.07 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07015426077458402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07015426077458402 | validation: 0.2354098350228241]
	TIME [epoch: 8.07 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08092890686455181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08092890686455181 | validation: 0.22619353769413625]
	TIME [epoch: 8.05 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11297334653432482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11297334653432482 | validation: 0.32161611194117673]
	TIME [epoch: 8.04 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22618705109560328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22618705109560328 | validation: 0.2845763438497692]
	TIME [epoch: 8.06 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08601903839903521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08601903839903521 | validation: 0.21292181381007902]
	TIME [epoch: 8.08 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12482915852499275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12482915852499275 | validation: 0.28172528268393354]
	TIME [epoch: 8.05 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.155736616372263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.155736616372263 | validation: 0.40543017582440843]
	TIME [epoch: 8.1 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12077820642538659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12077820642538659 | validation: 0.24301490802953413]
	TIME [epoch: 8.08 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07961728545805975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07961728545805975 | validation: 0.22140147018556774]
	TIME [epoch: 8.09 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08465237360638699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08465237360638699 | validation: 0.28241019469559786]
	TIME [epoch: 8.1 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07270682782674161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07270682782674161 | validation: 0.23060120500809342]
	TIME [epoch: 8.13 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07599009097888708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07599009097888708 | validation: 0.2699114114632671]
	TIME [epoch: 8.09 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0858207098555976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0858207098555976 | validation: 0.24188219740448283]
	TIME [epoch: 8.11 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10791896082935026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10791896082935026 | validation: 0.2952385403146029]
	TIME [epoch: 8.1 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11312610646274043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11312610646274043 | validation: 0.24383854125087742]
	TIME [epoch: 8.08 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1058096333384454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1058096333384454 | validation: 0.2531490162636622]
	TIME [epoch: 8.11 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07263843208911062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07263843208911062 | validation: 0.21097779062920308]
	TIME [epoch: 8.11 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05720023604558243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05720023604558243 | validation: 0.2058203658521782]
	TIME [epoch: 8.11 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05360176007969356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05360176007969356 | validation: 0.19538908659021081]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_735.pth
	Model improved!!!
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05115647142656222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05115647142656222 | validation: 0.22788624747811675]
	TIME [epoch: 8.07 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054105826897387965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054105826897387965 | validation: 0.1918778072441963]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_737.pth
	Model improved!!!
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09472607541791722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09472607541791722 | validation: 0.37686163561623576]
	TIME [epoch: 8.1 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15736359950547985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15736359950547985 | validation: 0.3261607796747174]
	TIME [epoch: 8.09 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20934199340580845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20934199340580845 | validation: 0.22134857701848176]
	TIME [epoch: 8.09 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11495914985971326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11495914985971326 | validation: 0.42057118104650404]
	TIME [epoch: 8.12 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1333629708166193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1333629708166193 | validation: 0.2776343812154145]
	TIME [epoch: 8.11 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09802193333050145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09802193333050145 | validation: 0.2671252127200113]
	TIME [epoch: 8.09 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13211244138126707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13211244138126707 | validation: 0.27560959386773337]
	TIME [epoch: 8.1 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13503186684132845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13503186684132845 | validation: 0.2729371093936947]
	TIME [epoch: 8.08 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07581211720488366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07581211720488366 | validation: 0.2175728896713162]
	TIME [epoch: 8.08 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06281979021251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06281979021251 | validation: 0.21363145148778698]
	TIME [epoch: 8.1 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05268573254667017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05268573254667017 | validation: 0.20948711877886664]
	TIME [epoch: 8.08 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053481176025863615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053481176025863615 | validation: 0.20012662366866563]
	TIME [epoch: 8.09 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054428617491514474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054428617491514474 | validation: 0.20128281814507556]
	TIME [epoch: 8.1 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060077879990934946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060077879990934946 | validation: 0.22079878544473092]
	TIME [epoch: 8.09 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08604118857410654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08604118857410654 | validation: 0.29624198031124166]
	TIME [epoch: 8.08 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16182977334501744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16182977334501744 | validation: 0.3319592510422145]
	TIME [epoch: 8.1 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28587454761125136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28587454761125136 | validation: 0.22833904982973507]
	TIME [epoch: 8.08 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07138940775990067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07138940775990067 | validation: 0.2015492389103685]
	TIME [epoch: 8.13 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10722770389075223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10722770389075223 | validation: 0.31022225750642807]
	TIME [epoch: 8.07 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1093163883672729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1093163883672729 | validation: 0.2133760460780333]
	TIME [epoch: 8.1 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08359167088841384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08359167088841384 | validation: 0.27829874776883673]
	TIME [epoch: 8.12 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06560930712544517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06560930712544517 | validation: 0.20313643454093216]
	TIME [epoch: 8.11 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06227412058347312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06227412058347312 | validation: 0.18506842371711685]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_760.pth
	Model improved!!!
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06003169376968927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06003169376968927 | validation: 0.21504309628515084]
	TIME [epoch: 8.1 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06582086996373296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06582086996373296 | validation: 0.24247288806495446]
	TIME [epoch: 8.04 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10043834248922867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10043834248922867 | validation: 0.24575038583175424]
	TIME [epoch: 8.09 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13213542442154974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13213542442154974 | validation: 0.24499552691802318]
	TIME [epoch: 8.09 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09786436209904131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09786436209904131 | validation: 0.17314839628462073]
	TIME [epoch: 8.08 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_765.pth
	Model improved!!!
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06628720349871864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06628720349871864 | validation: 0.21741002526441983]
	TIME [epoch: 8.11 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054460546027002926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054460546027002926 | validation: 0.17108469507044058]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_767.pth
	Model improved!!!
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058520201541548814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058520201541548814 | validation: 0.22620531820989498]
	TIME [epoch: 8.08 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07003228188081224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07003228188081224 | validation: 0.26340861386645475]
	TIME [epoch: 8.1 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1287303439087344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1287303439087344 | validation: 0.1949908761365945]
	TIME [epoch: 8.07 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07931866839122606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07931866839122606 | validation: 0.25910565697641286]
	TIME [epoch: 8.1 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08478349906031502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08478349906031502 | validation: 0.22142859709658383]
	TIME [epoch: 8.12 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08237718183304496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08237718183304496 | validation: 0.2086209622981975]
	TIME [epoch: 8.1 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10423300445153168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10423300445153168 | validation: 0.355993617945106]
	TIME [epoch: 8.1 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11225227247151807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11225227247151807 | validation: 0.18357175267178757]
	TIME [epoch: 8.11 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07478851797595237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07478851797595237 | validation: 0.2402126485759517]
	TIME [epoch: 8.08 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05710384717172346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05710384717172346 | validation: 0.18820130099274518]
	TIME [epoch: 8.09 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05031377686790542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05031377686790542 | validation: 0.18576535051575407]
	TIME [epoch: 8.11 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06313875924711862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06313875924711862 | validation: 0.23483843999689258]
	TIME [epoch: 8.09 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16224383608096746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16224383608096746 | validation: 0.25326640685643315]
	TIME [epoch: 8.09 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07595876934820155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07595876934820155 | validation: 0.15030095046981598]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_781.pth
	Model improved!!!
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054013343774664636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054013343774664636 | validation: 0.25973633931593504]
	TIME [epoch: 8.07 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06431780435080564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06431780435080564 | validation: 0.17547683170792588]
	TIME [epoch: 8.09 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06078468194418675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06078468194418675 | validation: 0.17765020383788954]
	TIME [epoch: 8.11 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0704892309586537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0704892309586537 | validation: 0.3269348000255812]
	TIME [epoch: 8.08 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15958341260892125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15958341260892125 | validation: 0.2955346130580821]
	TIME [epoch: 8.11 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17521690846814558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17521690846814558 | validation: 0.19217563718032388]
	TIME [epoch: 8.09 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08594234826252986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08594234826252986 | validation: 0.30705273065338967]
	TIME [epoch: 8.09 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07390426371922881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07390426371922881 | validation: 0.16459164045608424]
	TIME [epoch: 8.14 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061075348692560885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061075348692560885 | validation: 0.19186532690475444]
	TIME [epoch: 8.1 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05465750207677107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05465750207677107 | validation: 0.17754969261495485]
	TIME [epoch: 8.1 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05561879540002969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05561879540002969 | validation: 0.1773768079180309]
	TIME [epoch: 8.08 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07862778733480498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07862778733480498 | validation: 0.20367304534437675]
	TIME [epoch: 8.07 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07241613815315909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07241613815315909 | validation: 0.20627950629949443]
	TIME [epoch: 8.1 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09188548869269299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09188548869269299 | validation: 0.17242601196990323]
	TIME [epoch: 8.12 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06099262988605104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06099262988605104 | validation: 0.19092702892596475]
	TIME [epoch: 8.1 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04717052035662557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04717052035662557 | validation: 0.15523396729688357]
	TIME [epoch: 8.1 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06101769664201237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06101769664201237 | validation: 0.3711477472991372]
	TIME [epoch: 8.07 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08619985672229082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08619985672229082 | validation: 0.19775552996996174]
	TIME [epoch: 8.1 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0997694555509385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0997694555509385 | validation: 0.3216462804493392]
	TIME [epoch: 8.08 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14451841938675355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14451841938675355 | validation: 0.2450291213935589]
	TIME [epoch: 8.1 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12389524768705512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12389524768705512 | validation: 0.17430451450489926]
	TIME [epoch: 8.08 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07372517582537669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07372517582537669 | validation: 0.2834355851769302]
	TIME [epoch: 8.09 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07474184431918411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07474184431918411 | validation: 0.17192862059984482]
	TIME [epoch: 8.07 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05503955473463631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05503955473463631 | validation: 0.17152214161696705]
	TIME [epoch: 8.09 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0770184306366099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0770184306366099 | validation: 0.2640686774257561]
	TIME [epoch: 8.11 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08048978044308537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08048978044308537 | validation: 0.18981840599647484]
	TIME [epoch: 8.11 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05503107308729053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05503107308729053 | validation: 0.13350444228366554]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_808.pth
	Model improved!!!
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055518425704458696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055518425704458696 | validation: 0.2110881578399834]
	TIME [epoch: 8.09 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049790410151390276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049790410151390276 | validation: 0.1682504802380867]
	TIME [epoch: 8.1 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04936789020635157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04936789020635157 | validation: 0.1838386644102589]
	TIME [epoch: 8.1 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09474690678012536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09474690678012536 | validation: 0.3278726066766793]
	TIME [epoch: 8.13 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20094289362515635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20094289362515635 | validation: 0.2957830906233127]
	TIME [epoch: 8.1 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21745428247425116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21745428247425116 | validation: 0.1904769205728105]
	TIME [epoch: 8.09 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06877674020073181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06877674020073181 | validation: 0.22266634509179759]
	TIME [epoch: 8.08 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10233218351717158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10233218351717158 | validation: 0.19911028440694625]
	TIME [epoch: 8.1 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06406183865128008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06406183865128008 | validation: 0.1841774968803677]
	TIME [epoch: 8.1 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05782733491486933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05782733491486933 | validation: 0.22320167228205812]
	TIME [epoch: 8.1 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04949913269225886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04949913269225886 | validation: 0.1600823891937782]
	TIME [epoch: 8.1 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05179490062659902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05179490062659902 | validation: 0.17277286463682065]
	TIME [epoch: 8.09 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0519782499145866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0519782499145866 | validation: 0.2273692658283819]
	TIME [epoch: 8.09 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06103696554199729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06103696554199729 | validation: 0.18335459163509513]
	TIME [epoch: 8.1 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08798168540865131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08798168540865131 | validation: 0.20858135785806448]
	TIME [epoch: 8.11 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12414521499110777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12414521499110777 | validation: 0.28613015036427253]
	TIME [epoch: 8.11 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09982678422219539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09982678422219539 | validation: 0.15552945122445877]
	TIME [epoch: 8.09 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057169977763724095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057169977763724095 | validation: 0.3714916149812513]
	TIME [epoch: 8.09 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07084703980259416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07084703980259416 | validation: 0.18304560345903406]
	TIME [epoch: 8.08 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05206391281323337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05206391281323337 | validation: 0.13994310554811212]
	TIME [epoch: 8.1 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06339148747626852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06339148747626852 | validation: 0.2428508922051346]
	TIME [epoch: 8.11 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06469724901776122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06469724901776122 | validation: 0.19872833875274887]
	TIME [epoch: 8.09 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08179367460298058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08179367460298058 | validation: 0.1866773913350438]
	TIME [epoch: 8.09 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11694013608302685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11694013608302685 | validation: 0.21695172940760465]
	TIME [epoch: 8.1 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11605318515322692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11605318515322692 | validation: 0.19556407343084986]
	TIME [epoch: 8.1 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05540692821699883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05540692821699883 | validation: 0.13981540157249295]
	TIME [epoch: 8.07 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0501310924022763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0501310924022763 | validation: 0.15800855925800117]
	TIME [epoch: 8.08 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054548233749071785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054548233749071785 | validation: 0.21403568069441775]
	TIME [epoch: 8.1 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05754014335809501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05754014335809501 | validation: 0.15710757600130953]
	TIME [epoch: 8.09 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04921248139649112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04921248139649112 | validation: 0.14656959066821415]
	TIME [epoch: 8.09 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050595108940922236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050595108940922236 | validation: 0.230245909471513]
	TIME [epoch: 8.11 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08299285916024951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08299285916024951 | validation: 0.19464173888729205]
	TIME [epoch: 8.07 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1278917147154796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1278917147154796 | validation: 0.19397198300294743]
	TIME [epoch: 8.09 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10818450142242307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10818450142242307 | validation: 0.5599750206300946]
	TIME [epoch: 8.08 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15544035536903725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15544035536903725 | validation: 0.16129110123660564]
	TIME [epoch: 8.1 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05838857317769119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05838857317769119 | validation: 0.13812422639233596]
	TIME [epoch: 8.09 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06711412789572585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06711412789572585 | validation: 0.24341907064071544]
	TIME [epoch: 8.09 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048095790969192066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048095790969192066 | validation: 0.1833837812391087]
	TIME [epoch: 8.07 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04846470903459693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04846470903459693 | validation: 0.13252414684580638]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_847.pth
	Model improved!!!
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038357622324136066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038357622324136066 | validation: 0.13304845810213287]
	TIME [epoch: 8.12 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039752642755809794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039752642755809794 | validation: 0.16928208586522572]
	TIME [epoch: 8.08 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04481209173352409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04481209173352409 | validation: 0.19478638168453344]
	TIME [epoch: 8.09 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1047242726590666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1047242726590666 | validation: 0.3231091739165375]
	TIME [epoch: 8.09 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24591748288154505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24591748288154505 | validation: 0.23860288754908365]
	TIME [epoch: 8.08 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20796207310901088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20796207310901088 | validation: 0.19988969665531672]
	TIME [epoch: 8.09 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10522524358421599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10522524358421599 | validation: 0.26171971728334203]
	TIME [epoch: 8.09 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0687465550119203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0687465550119203 | validation: 0.1615531666959091]
	TIME [epoch: 8.07 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05846788968985711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05846788968985711 | validation: 0.3114255533208425]
	TIME [epoch: 8.08 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07557116680872908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07557116680872908 | validation: 0.2707272016084794]
	TIME [epoch: 8.06 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06739377909137663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06739377909137663 | validation: 0.17926106256746335]
	TIME [epoch: 8.09 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04384740800316179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04384740800316179 | validation: 0.1345061862023764]
	TIME [epoch: 8.1 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04466655427635061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04466655427635061 | validation: 0.1852758910434645]
	TIME [epoch: 8.08 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03886498229566663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03886498229566663 | validation: 0.15919516709913864]
	TIME [epoch: 8.08 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042221182131761265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042221182131761265 | validation: 0.14844185830547926]
	TIME [epoch: 8.1 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049783213504356646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049783213504356646 | validation: 0.20361857055190113]
	TIME [epoch: 8.07 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07967832648614592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07967832648614592 | validation: 0.2054734059777911]
	TIME [epoch: 8.11 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13420658766256435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13420658766256435 | validation: 0.24958255403287316]
	TIME [epoch: 8.09 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09657101843795898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09657101843795898 | validation: 0.14730780557646236]
	TIME [epoch: 8.09 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0508988274957637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0508988274957637 | validation: 0.16097660739927525]
	TIME [epoch: 8.07 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04107832058872183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04107832058872183 | validation: 0.14358583899390795]
	TIME [epoch: 8.09 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06198582951298717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06198582951298717 | validation: 0.1542519991661044]
	TIME [epoch: 8.1 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07291931412485712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07291931412485712 | validation: 0.17906458561316932]
	TIME [epoch: 8.1 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0882177358514072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0882177358514072 | validation: 0.15339695773543738]
	TIME [epoch: 8.1 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054716348119943456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054716348119943456 | validation: 0.3219196118443473]
	TIME [epoch: 8.08 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08452581361758117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08452581361758117 | validation: 0.14776016713673285]
	TIME [epoch: 8.07 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048754290731212374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048754290731212374 | validation: 0.15473892911735418]
	TIME [epoch: 8.07 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06377923645861128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06377923645861128 | validation: 0.2885045957025536]
	TIME [epoch: 8.1 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07132394558779487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07132394558779487 | validation: 0.18596439067194553]
	TIME [epoch: 8.11 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07181067414355732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07181067414355732 | validation: 0.2193802507073801]
	TIME [epoch: 8.11 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14205552348097036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14205552348097036 | validation: 0.22786488865006307]
	TIME [epoch: 8.09 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10221593899485285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10221593899485285 | validation: 0.1613875074672894]
	TIME [epoch: 8.07 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052216980723826155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052216980723826155 | validation: 0.1258585776843396]
	TIME [epoch: 8.07 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_880.pth
	Model improved!!!
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047550832949730566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047550832949730566 | validation: 0.1361614669645751]
	TIME [epoch: 8.07 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050927821116501945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050927821116501945 | validation: 0.15558082011323243]
	TIME [epoch: 8.1 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06447793145132377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06447793145132377 | validation: 0.13526347921867263]
	TIME [epoch: 8.07 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05273302851744625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05273302851744625 | validation: 0.11758734735984806]
	TIME [epoch: 8.07 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_884.pth
	Model improved!!!
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040582456669182074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040582456669182074 | validation: 0.13633266623912585]
	TIME [epoch: 8.07 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03581557611493264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03581557611493264 | validation: 0.13174639323032347]
	TIME [epoch: 8.08 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052527040406060904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052527040406060904 | validation: 0.3227219006920134]
	TIME [epoch: 8.09 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1206199566985606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1206199566985606 | validation: 0.2539988173582228]
	TIME [epoch: 8.1 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17170383052910698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17170383052910698 | validation: 0.20431242680894438]
	TIME [epoch: 8.08 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06584668974141064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06584668974141064 | validation: 0.19164691629110436]
	TIME [epoch: 8.08 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047609911038324876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047609911038324876 | validation: 0.1383159518680339]
	TIME [epoch: 8.09 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052240071321122815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052240071321122815 | validation: 0.24280116924179077]
	TIME [epoch: 8.08 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05880532128381176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05880532128381176 | validation: 0.16728840796319677]
	TIME [epoch: 8.1 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04856257390184324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04856257390184324 | validation: 0.14787693825138223]
	TIME [epoch: 8.1 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05251680106127588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05251680106127588 | validation: 0.1563233704461059]
	TIME [epoch: 8.06 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0714142291661925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0714142291661925 | validation: 0.1749420025151618]
	TIME [epoch: 8.08 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10180423367073471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10180423367073471 | validation: 0.14875102115827618]
	TIME [epoch: 8.07 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056427044440487586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056427044440487586 | validation: 0.16826873413798157]
	TIME [epoch: 8.09 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07929783903128793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07929783903128793 | validation: 0.16538875803044034]
	TIME [epoch: 8.08 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08926856294885932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08926856294885932 | validation: 0.14981050972670693]
	TIME [epoch: 8.07 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07667102767716824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07667102767716824 | validation: 0.1519152124990886]
	TIME [epoch: 8.07 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06757813688656157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06757813688656157 | validation: 0.14318174797802447]
	TIME [epoch: 8.08 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07244307412988495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07244307412988495 | validation: 0.23089039062035557]
	TIME [epoch: 8.07 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06669087144865461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06669087144865461 | validation: 0.13437987651180125]
	TIME [epoch: 8.08 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045020815888964644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045020815888964644 | validation: 0.11899578660708304]
	TIME [epoch: 8.1 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0408048522044263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0408048522044263 | validation: 0.14040555565489496]
	TIME [epoch: 8.08 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03561925910113588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03561925910113588 | validation: 0.1546905272852316]
	TIME [epoch: 8.07 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03393644074153783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03393644074153783 | validation: 0.12203695482340043]
	TIME [epoch: 8.07 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051013044605164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051013044605164 | validation: 0.3528790220641156]
	TIME [epoch: 8.05 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08524945338600326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08524945338600326 | validation: 0.1773904437624097]
	TIME [epoch: 8.1 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10147460821929974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10147460821929974 | validation: 0.19440185642193097]
	TIME [epoch: 8.08 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09584810641924309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09584810641924309 | validation: 0.18235944042394775]
	TIME [epoch: 8.09 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06685664039702532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06685664039702532 | validation: 0.1488210442383005]
	TIME [epoch: 8.08 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05516348126046154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05516348126046154 | validation: 0.2390650579187339]
	TIME [epoch: 8.09 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05513306445722629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05513306445722629 | validation: 0.13829671711483862]
	TIME [epoch: 8.11 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06101091913698171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06101091913698171 | validation: 0.17486188471079692]
	TIME [epoch: 8.1 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10243962691011388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10243962691011388 | validation: 0.1327862839190448]
	TIME [epoch: 8.1 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05194789406753163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05194789406753163 | validation: 0.12541857436032205]
	TIME [epoch: 8.08 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050936705248214476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050936705248214476 | validation: 0.1485403653086196]
	TIME [epoch: 8.07 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06400323160748786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06400323160748786 | validation: 0.14999696561396472]
	TIME [epoch: 8.1 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07196164021298077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07196164021298077 | validation: 0.15146688236973632]
	TIME [epoch: 8.1 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0634207690860074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0634207690860074 | validation: 0.15542608638602762]
	TIME [epoch: 8.1 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06330592808142238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06330592808142238 | validation: 0.13484435774666825]
	TIME [epoch: 8.09 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05845684946379645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05845684946379645 | validation: 0.1353275107985282]
	TIME [epoch: 8.1 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06952720276154475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06952720276154475 | validation: 0.18492050375321065]
	TIME [epoch: 8.09 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05712194340621592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05712194340621592 | validation: 0.11495841854935565]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_926.pth
	Model improved!!!
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03765594024178053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03765594024178053 | validation: 0.11106263275658251]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_927.pth
	Model improved!!!
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02834680961402368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02834680961402368 | validation: 0.10668140847230467]
	TIME [epoch: 8.13 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_928.pth
	Model improved!!!
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03422838236516034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03422838236516034 | validation: 0.11767786053513417]
	TIME [epoch: 8.1 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049795829928501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049795829928501 | validation: 0.1565519308340083]
	TIME [epoch: 8.12 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045516567031572135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045516567031572135 | validation: 0.1433138079465109]
	TIME [epoch: 8.09 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06677708648268815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06677708648268815 | validation: 0.22835100606512976]
	TIME [epoch: 8.1 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16111912340254278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16111912340254278 | validation: 0.40029476887238435]
	TIME [epoch: 8.1 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15736229735031385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15736229735031385 | validation: 0.12350530856277586]
	TIME [epoch: 8.12 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04311704037636189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04311704037636189 | validation: 0.16479610690658744]
	TIME [epoch: 8.09 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05957367977924724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05957367977924724 | validation: 0.31266208714327054]
	TIME [epoch: 8.09 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07524590384896598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07524590384896598 | validation: 0.1644151943493688]
	TIME [epoch: 8.08 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048061088520017226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048061088520017226 | validation: 0.12228934672122592]
	TIME [epoch: 8.1 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05199393540501098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05199393540501098 | validation: 0.12886652440167673]
	TIME [epoch: 8.11 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04413273470757405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04413273470757405 | validation: 0.15509469843193618]
	TIME [epoch: 8.11 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05691829104596938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05691829104596938 | validation: 0.13343098323722616]
	TIME [epoch: 8.09 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04722668236143367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04722668236143367 | validation: 0.21485238138305762]
	TIME [epoch: 8.1 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06602111340850568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06602111340850568 | validation: 0.15617503696618085]
	TIME [epoch: 8.09 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08725479062362165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08725479062362165 | validation: 0.15888357044243515]
	TIME [epoch: 8.09 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08350954945682208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08350954945682208 | validation: 0.132185940221578]
	TIME [epoch: 8.09 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05278277501043613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05278277501043613 | validation: 0.10500056404155832]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_946.pth
	Model improved!!!
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03476015886623799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03476015886623799 | validation: 0.1013985079639308]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_947.pth
	Model improved!!!
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04039554609534012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04039554609534012 | validation: 0.1233019979967762]
	TIME [epoch: 8.1 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0599978886923096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0599978886923096 | validation: 0.16869950745854145]
	TIME [epoch: 8.09 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12423041445928533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12423041445928533 | validation: 0.20170000740803526]
	TIME [epoch: 8.09 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06697033173709631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06697033173709631 | validation: 0.14842959765582037]
	TIME [epoch: 8.1 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07058819663191555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07058819663191555 | validation: 0.20523194926701438]
	TIME [epoch: 8.08 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06616275163603948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06616275163603948 | validation: 0.13466982513880627]
	TIME [epoch: 8.11 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047990758531546204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047990758531546204 | validation: 0.10369662678992105]
	TIME [epoch: 8.1 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03173912521448016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03173912521448016 | validation: 0.1218135590689843]
	TIME [epoch: 8.09 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033459932764825624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033459932764825624 | validation: 0.1361775173221971]
	TIME [epoch: 8.09 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03669666485919131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03669666485919131 | validation: 0.1004387422972422]
	TIME [epoch: 8.12 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_957.pth
	Model improved!!!
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02741398353771067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02741398353771067 | validation: 0.10275899296880642]
	TIME [epoch: 8.11 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038091713325634574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038091713325634574 | validation: 0.14670236334802916]
	TIME [epoch: 8.09 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058525364522191474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058525364522191474 | validation: 0.18327580720968106]
	TIME [epoch: 8.09 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1268162692471778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1268162692471778 | validation: 0.19351627870225996]
	TIME [epoch: 8.09 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1425793893697895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1425793893697895 | validation: 0.277450082102115]
	TIME [epoch: 8.13 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08408924517297944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08408924517297944 | validation: 0.11044711058443407]
	TIME [epoch: 8.1 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0375333161866363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0375333161866363 | validation: 0.08836538156537031]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_964.pth
	Model improved!!!
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03576157735137031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03576157735137031 | validation: 0.09866121198709361]
	TIME [epoch: 8.1 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035974288328863804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035974288328863804 | validation: 0.10061882616589012]
	TIME [epoch: 8.1 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034944072552649176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034944072552649176 | validation: 0.1621071823480138]
	TIME [epoch: 8.1 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043380836141232657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043380836141232657 | validation: 0.12592965424292246]
	TIME [epoch: 8.11 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09771647705102642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09771647705102642 | validation: 0.14519008619897816]
	TIME [epoch: 8.13 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07940602066592858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07940602066592858 | validation: 0.13933836095312988]
	TIME [epoch: 8.09 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06512877606100412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06512877606100412 | validation: 0.15401686176184415]
	TIME [epoch: 8.12 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058851814549222045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058851814549222045 | validation: 0.24407977112777304]
	TIME [epoch: 8.09 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05394177171687512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05394177171687512 | validation: 0.1627788725855054]
	TIME [epoch: 8.12 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050804898697740936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050804898697740936 | validation: 0.11404618106566478]
	TIME [epoch: 8.11 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055030890815169896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055030890815169896 | validation: 0.14620078710212803]
	TIME [epoch: 8.1 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06748237274456648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06748237274456648 | validation: 0.19708910056104087]
	TIME [epoch: 8.11 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10108896999307508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10108896999307508 | validation: 0.18170235595951648]
	TIME [epoch: 8.11 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12408323072180313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12408323072180313 | validation: 0.1635389877158623]
	TIME [epoch: 8.08 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05450883821518124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05450883821518124 | validation: 0.15878483044121572]
	TIME [epoch: 8.11 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04013860918228818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04013860918228818 | validation: 0.12052079890402845]
	TIME [epoch: 8.14 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0514794203613807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0514794203613807 | validation: 0.49574601967407494]
	TIME [epoch: 8.11 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13999322960646848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13999322960646848 | validation: 0.21729322029009515]
	TIME [epoch: 8.1 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053445763331451734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053445763331451734 | validation: 0.09966115363462436]
	TIME [epoch: 8.08 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04959062825827984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04959062825827984 | validation: 0.09486516555418725]
	TIME [epoch: 8.11 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04178851819940135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04178851819940135 | validation: 0.13122674722005018]
	TIME [epoch: 8.1 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03774029657778768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03774029657778768 | validation: 0.12442599782454455]
	TIME [epoch: 8.11 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03897170213014802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03897170213014802 | validation: 0.11220790338873854]
	TIME [epoch: 8.09 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044036255015090335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044036255015090335 | validation: 0.11839118454323715]
	TIME [epoch: 8.11 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05442987658099901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05442987658099901 | validation: 0.1212748207090843]
	TIME [epoch: 8.09 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07270895861891544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07270895861891544 | validation: 0.15159915565784]
	TIME [epoch: 8.1 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08994724750280064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08994724750280064 | validation: 0.18502544320184075]
	TIME [epoch: 8.13 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10006666593601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10006666593601 | validation: 0.124966187966542]
	TIME [epoch: 8.12 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0538288424531544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0538288424531544 | validation: 0.1013473099738333]
	TIME [epoch: 8.11 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034450834687597943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034450834687597943 | validation: 0.10852140633767715]
	TIME [epoch: 8.1 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03279415386915143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03279415386915143 | validation: 0.10850270788116506]
	TIME [epoch: 8.12 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03792491768908717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03792491768908717 | validation: 0.08898261992279373]
	TIME [epoch: 8.11 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03742720668838289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03742720668838289 | validation: 0.11166119216903235]
	TIME [epoch: 8.13 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04892184691700503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04892184691700503 | validation: 0.12332215703701643]
	TIME [epoch: 8.13 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06346143927188189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06346143927188189 | validation: 0.12536338422600415]
	TIME [epoch: 8.11 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06534680690149097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06534680690149097 | validation: 0.10871550004117486]
	TIME [epoch: 8.09 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0460970220708821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0460970220708821 | validation: 0.10606083981037835]
	TIME [epoch: 59.2 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038095737348995344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038095737348995344 | validation: 0.12412624600647795]
	TIME [epoch: 17.4 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04968675342219407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04968675342219407 | validation: 0.3650134254254771]
	TIME [epoch: 17.4 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08322335460329569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08322335460329569 | validation: 0.1565770887413816]
	TIME [epoch: 17.4 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06398434915676213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06398434915676213 | validation: 0.10487298365559959]
	TIME [epoch: 17.3 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04751977771361645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04751977771361645 | validation: 0.13414051263061402]
	TIME [epoch: 17.4 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06436709513976412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06436709513976412 | validation: 0.16040810192104227]
	TIME [epoch: 17.4 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0859910730379456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0859910730379456 | validation: 0.16521307880834796]
	TIME [epoch: 17.4 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10852119188000549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10852119188000549 | validation: 0.14087785371990008]
	TIME [epoch: 17.3 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06337771668506656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06337771668506656 | validation: 0.11150430360683097]
	TIME [epoch: 17.3 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04309178824666446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04309178824666446 | validation: 0.1078652543469175]
	TIME [epoch: 17.4 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028767469086867815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028767469086867815 | validation: 0.09425052143390254]
	TIME [epoch: 17.4 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02753926447972205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02753926447972205 | validation: 0.12210584482935789]
	TIME [epoch: 17.4 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03097049149692651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03097049149692651 | validation: 0.09040536795395875]
	TIME [epoch: 17.4 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03709290888323323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03709290888323323 | validation: 0.12062389166755866]
	TIME [epoch: 17.4 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06786830614104465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06786830614104465 | validation: 0.2608271560805254]
	TIME [epoch: 17.4 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08310674396782446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08310674396782446 | validation: 0.10236412334235356]
	TIME [epoch: 17.3 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04733026525975987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04733026525975987 | validation: 0.1343583764948051]
	TIME [epoch: 17.4 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05903878278724187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05903878278724187 | validation: 0.16106996772308013]
	TIME [epoch: 17.4 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054640718631198876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054640718631198876 | validation: 0.13991836767446153]
	TIME [epoch: 17.4 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05132533889332847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05132533889332847 | validation: 0.15535174132922955]
	TIME [epoch: 17.4 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0824395028903932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0824395028903932 | validation: 0.17643453095575004]
	TIME [epoch: 17.4 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07697588539943469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07697588539943469 | validation: 0.10668501762034169]
	TIME [epoch: 17.4 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052043462303730775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052043462303730775 | validation: 0.0988583786612188]
	TIME [epoch: 17.4 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03254073687107128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03254073687107128 | validation: 0.09048132998673675]
	TIME [epoch: 17.3 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02765632896608853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02765632896608853 | validation: 0.10341379068455603]
	TIME [epoch: 17.4 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03700711168377563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03700711168377563 | validation: 0.10564058052329361]
	TIME [epoch: 17.4 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05968622425749927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05968622425749927 | validation: 0.15203128233318475]
	TIME [epoch: 17.3 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10066418072771124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10066418072771124 | validation: 0.20393899395796447]
	TIME [epoch: 17.3 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06760411200682957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06760411200682957 | validation: 0.10307910016402336]
	TIME [epoch: 17.4 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04064995151733019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04064995151733019 | validation: 0.12067090484014104]
	TIME [epoch: 17.4 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047479446192233925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047479446192233925 | validation: 0.12259942889082172]
	TIME [epoch: 17.4 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04061705728752038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04061705728752038 | validation: 0.0877786003729853]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_1033.pth
	Model improved!!!
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032984477141105806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032984477141105806 | validation: 0.09832836237232152]
	TIME [epoch: 17.3 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041952199729577595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041952199729577595 | validation: 0.20324045057985068]
	TIME [epoch: 17.4 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0537831136270494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0537831136270494 | validation: 0.09984971942677441]
	TIME [epoch: 17.3 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05275392862274741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05275392862274741 | validation: 0.13451559932663826]
	TIME [epoch: 17.4 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07278317230937704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07278317230937704 | validation: 0.09844350303650212]
	TIME [epoch: 17.4 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049364800614939516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049364800614939516 | validation: 0.08720065274061413]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_1039.pth
	Model improved!!!
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04206990598831152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04206990598831152 | validation: 0.11896584161390987]
	TIME [epoch: 17.4 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0604685313677496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0604685313677496 | validation: 0.1164971727683411]
	TIME [epoch: 17.4 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07043589172262724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07043589172262724 | validation: 0.41729559665756155]
	TIME [epoch: 17.4 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.109702708857203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.109702708857203 | validation: 0.08528869451395173]
	TIME [epoch: 17.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_1043.pth
	Model improved!!!
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04225235952246871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04225235952246871 | validation: 0.09518452621115431]
	TIME [epoch: 17.4 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05375109248488923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05375109248488923 | validation: 0.12003142095322122]
	TIME [epoch: 17.4 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06027932753214652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06027932753214652 | validation: 0.17135632209374843]
	TIME [epoch: 17.4 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06454706177347169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06454706177347169 | validation: 0.11979278215423156]
	TIME [epoch: 17.4 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060833290514456496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060833290514456496 | validation: 0.1078259424812885]
	TIME [epoch: 17.4 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04873973732391067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04873973732391067 | validation: 0.09969628466928036]
	TIME [epoch: 17.4 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031409750473963396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031409750473963396 | validation: 0.08831596011603138]
	TIME [epoch: 17.4 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023788861048491414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023788861048491414 | validation: 0.0710465102867493]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_1051.pth
	Model improved!!!
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019101093334021133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019101093334021133 | validation: 0.07987487854588804]
	TIME [epoch: 17.4 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02393010225632252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02393010225632252 | validation: 0.097434926454672]
	TIME [epoch: 17.3 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034256331298544745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034256331298544745 | validation: 0.23004980598046762]
	TIME [epoch: 17.4 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05248629103495644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05248629103495644 | validation: 0.09136361220321029]
	TIME [epoch: 17.4 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03249745837775493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03249745837775493 | validation: 0.12933292859740855]
	TIME [epoch: 17.4 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060327250219699376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060327250219699376 | validation: 0.17825850229464452]
	TIME [epoch: 17.4 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12244419594564644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12244419594564644 | validation: 0.1634958476495254]
	TIME [epoch: 17.3 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11125420188029442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11125420188029442 | validation: 0.1288888322458266]
	TIME [epoch: 17.3 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060376360532366696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060376360532366696 | validation: 0.41492404445893744]
	TIME [epoch: 17.3 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12470082585170174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12470082585170174 | validation: 0.16283216789002833]
	TIME [epoch: 17.3 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.097883390370747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.097883390370747 | validation: 0.18952937545944581]
	TIME [epoch: 17.3 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10031679182845463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10031679182845463 | validation: 0.10533982782996064]
	TIME [epoch: 17.4 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03473270640321777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03473270640321777 | validation: 0.14740421057784564]
	TIME [epoch: 17.4 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04753873785246523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04753873785246523 | validation: 0.09955141955445344]
	TIME [epoch: 17.4 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029299790132972507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029299790132972507 | validation: 0.0852873888362863]
	TIME [epoch: 17.4 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025821603422719788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025821603422719788 | validation: 0.08396661892256901]
	TIME [epoch: 17.4 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022695577328000677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022695577328000677 | validation: 0.09868740308064829]
	TIME [epoch: 17.4 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024881268573575834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024881268573575834 | validation: 0.08799164459581918]
	TIME [epoch: 17.4 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02370001171864682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02370001171864682 | validation: 0.09398675969059905]
	TIME [epoch: 17.4 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03474232954356267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03474232954356267 | validation: 0.13070359609807194]
	TIME [epoch: 17.4 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07718617434614755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07718617434614755 | validation: 0.16679649314210812]
	TIME [epoch: 17.4 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1265598049456868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1265598049456868 | validation: 0.16316450679097916]
	TIME [epoch: 17.4 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06550533064589208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06550533064589208 | validation: 0.07543389782493352]
	TIME [epoch: 17.3 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024392031262811666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024392031262811666 | validation: 0.07558803784822989]
	TIME [epoch: 17.3 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0219225122447413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0219225122447413 | validation: 0.09305827004801051]
	TIME [epoch: 17.3 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024990221410541236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024990221410541236 | validation: 0.07366796399460843]
	TIME [epoch: 17.3 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023757661528074646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023757661528074646 | validation: 0.07959756750101414]
	TIME [epoch: 17.3 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02563646500190947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02563646500190947 | validation: 0.11254317223196178]
	TIME [epoch: 17.3 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03484337282967013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03484337282967013 | validation: 0.10574652014737024]
	TIME [epoch: 17.3 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05125984858287483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05125984858287483 | validation: 0.20582881169268738]
	TIME [epoch: 17.3 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0570331165037166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0570331165037166 | validation: 0.15785544588372624]
	TIME [epoch: 17.3 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06300693001511082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06300693001511082 | validation: 0.1709421246356521]
	TIME [epoch: 17.4 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05193696418442168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05193696418442168 | validation: 0.13231562193209437]
	TIME [epoch: 17.3 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07621755613840836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07621755613840836 | validation: 0.4230575096156106]
	TIME [epoch: 17.4 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15112595462924525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15112595462924525 | validation: 0.12199141020498648]
	TIME [epoch: 17.4 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07998093521694491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07998093521694491 | validation: 0.08147896218476276]
	TIME [epoch: 17.4 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05016615788606342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05016615788606342 | validation: 0.09932700653685989]
	TIME [epoch: 17.4 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03968043912940797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03968043912940797 | validation: 0.08461174683975968]
	TIME [epoch: 17.4 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025968232425291404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025968232425291404 | validation: 0.07357204993397326]
	TIME [epoch: 17.4 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0197873836379715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0197873836379715 | validation: 0.08400505314303347]
	TIME [epoch: 17.3 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021040518610224393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021040518610224393 | validation: 0.07171295670934584]
	TIME [epoch: 17.4 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02076107871725955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02076107871725955 | validation: 0.07272325297192668]
	TIME [epoch: 17.3 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022365698260088013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022365698260088013 | validation: 0.07862898972529095]
	TIME [epoch: 17.4 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028630917186494208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028630917186494208 | validation: 0.0930540578657798]
	TIME [epoch: 17.4 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05843806898495345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05843806898495345 | validation: 0.1858034612931422]
	TIME [epoch: 17.3 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1397027480403857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1397027480403857 | validation: 0.13538865088357852]
	TIME [epoch: 17.4 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11961884517113124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11961884517113124 | validation: 0.09239750864400109]
	TIME [epoch: 17.4 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03675513916352788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03675513916352788 | validation: 0.1008665830220501]
	TIME [epoch: 17.3 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03727419960445194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03727419960445194 | validation: 0.08047975502972371]
	TIME [epoch: 17.4 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04351046244402941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04351046244402941 | validation: 0.09521341375568272]
	TIME [epoch: 17.4 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04816386480855485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04816386480855485 | validation: 0.1481028635879568]
	TIME [epoch: 17.4 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07227692434773735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07227692434773735 | validation: 0.13654652788375718]
	TIME [epoch: 17.4 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06382334636486038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06382334636486038 | validation: 0.13217347564378956]
	TIME [epoch: 17.4 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034505873590288706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034505873590288706 | validation: 0.09711350343939967]
	TIME [epoch: 17.3 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024586086332036804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024586086332036804 | validation: 0.07966218514828653]
	TIME [epoch: 17.4 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02655845429509145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02655845429509145 | validation: 0.12061006310639905]
	TIME [epoch: 17.4 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034006763874554945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034006763874554945 | validation: 0.08217000183031492]
	TIME [epoch: 17.4 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031089070132125418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031089070132125418 | validation: 0.10800140447051705]
	TIME [epoch: 17.4 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043597439203709225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043597439203709225 | validation: 0.10752521571179074]
	TIME [epoch: 17.4 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06269457462522528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06269457462522528 | validation: 0.11384366308535437]
	TIME [epoch: 17.4 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06933402883913624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06933402883913624 | validation: 0.1957269351576281]
	TIME [epoch: 17.4 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06661479706282172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06661479706282172 | validation: 0.07106882493564039]
	TIME [epoch: 17.4 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02548768084351597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02548768084351597 | validation: 0.0708527211603681]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_1114.pth
	Model improved!!!
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03428190989587954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03428190989587954 | validation: 0.07075180484816389]
	TIME [epoch: 17.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_1115.pth
	Model improved!!!
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028814538017983296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028814538017983296 | validation: 0.10171931703968962]
	TIME [epoch: 17.3 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02495262529405412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02495262529405412 | validation: 0.08270410071971752]
	TIME [epoch: 17.3 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024196035316493766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024196035316493766 | validation: 0.15363925100040654]
	TIME [epoch: 17.3 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03886026420425163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03886026420425163 | validation: 0.08825215514836071]
	TIME [epoch: 17.2 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05057061878660845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05057061878660845 | validation: 0.12472900913032164]
	TIME [epoch: 17.3 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08277264943201662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08277264943201662 | validation: 0.1546294301291945]
	TIME [epoch: 17.3 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10480502094238243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10480502094238243 | validation: 0.10577928951562442]
	TIME [epoch: 17.2 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06962760393315048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06962760393315048 | validation: 0.0946636635249036]
	TIME [epoch: 17.3 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04032866990642435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04032866990642435 | validation: 0.08403715056165417]
	TIME [epoch: 17.3 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052396127819942045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052396127819942045 | validation: 0.12102762819672641]
	TIME [epoch: 17.2 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08671886025959602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08671886025959602 | validation: 0.0969295653681215]
	TIME [epoch: 17.3 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03272292058870137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03272292058870137 | validation: 0.06948219529851205]
	TIME [epoch: 17.2 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_1127.pth
	Model improved!!!
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023695140312688007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023695140312688007 | validation: 0.06909886041805617]
	TIME [epoch: 17.2 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_1128.pth
	Model improved!!!
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02709680291570214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02709680291570214 | validation: 0.08687283818867103]
	TIME [epoch: 17.3 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02182783838131172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02182783838131172 | validation: 0.07008100666981298]
	TIME [epoch: 17.3 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021718200352448397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021718200352448397 | validation: 0.08774643302543253]
	TIME [epoch: 17.3 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024064275481321954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024064275481321954 | validation: 0.0781706281982486]
	TIME [epoch: 17.4 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020311436004651014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020311436004651014 | validation: 0.10535928547052494]
	TIME [epoch: 17.3 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023698066268591764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023698066268591764 | validation: 0.10980713265562181]
	TIME [epoch: 17.3 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04578859266339519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04578859266339519 | validation: 0.19609901509546945]
	TIME [epoch: 17.4 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10039477323940771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10039477323940771 | validation: 0.16879868050927394]
	TIME [epoch: 17.3 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13367993584875126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13367993584875126 | validation: 0.10073744463208101]
	TIME [epoch: 17.3 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04599729596403842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04599729596403842 | validation: 0.1015950605466923]
	TIME [epoch: 17.3 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031059642521223944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031059642521223944 | validation: 0.10284261089926053]
	TIME [epoch: 17.3 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05297484279836977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05297484279836977 | validation: 0.13160033075855548]
	TIME [epoch: 17.3 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06426834465119029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06426834465119029 | validation: 0.08436410070223987]
	TIME [epoch: 17.3 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04180383518557188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04180383518557188 | validation: 0.06442304641380474]
	TIME [epoch: 17.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_1142.pth
	Model improved!!!
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02778585931089026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02778585931089026 | validation: 0.07564453971704282]
	TIME [epoch: 17.3 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021411694083352667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021411694083352667 | validation: 0.07027223698096408]
	TIME [epoch: 17.3 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024774923622276873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024774923622276873 | validation: 0.10993870691880302]
	TIME [epoch: 17.3 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03780422621167079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03780422621167079 | validation: 0.10164991155386237]
	TIME [epoch: 17.3 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041880131406044144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041880131406044144 | validation: 0.0888689414339247]
	TIME [epoch: 17.3 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04378261157830781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04378261157830781 | validation: 0.08802315330223665]
	TIME [epoch: 17.3 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049826127552955936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049826127552955936 | validation: 0.09818197584053459]
	TIME [epoch: 17.3 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05719505764703304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05719505764703304 | validation: 0.1066077369786469]
	TIME [epoch: 17.3 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06352262401054143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06352262401054143 | validation: 0.1720286539098475]
	TIME [epoch: 17.3 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05511622868475776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05511622868475776 | validation: 0.07297250752703824]
	TIME [epoch: 17.3 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022572358036547148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022572358036547148 | validation: 0.07958770339531425]
	TIME [epoch: 17.3 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017902804735561193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017902804735561193 | validation: 0.05727047987629398]
	TIME [epoch: 17.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_1154.pth
	Model improved!!!
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019192761721078685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019192761721078685 | validation: 0.12393668783381423]
	TIME [epoch: 17.3 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028325571159923905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028325571159923905 | validation: 0.08952926846077669]
	TIME [epoch: 17.4 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037013059765727246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037013059765727246 | validation: 0.16916906049722846]
	TIME [epoch: 17.3 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04783727579755249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04783727579755249 | validation: 0.12522638648272363]
	TIME [epoch: 17.3 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05005959547563732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05005959547563732 | validation: 0.09329923333155561]
	TIME [epoch: 17.3 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05285188962967365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05285188962967365 | validation: 0.09288467174732032]
	TIME [epoch: 17.3 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05649502842199696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05649502842199696 | validation: 0.08629537864810062]
	TIME [epoch: 17.4 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05317586393875184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05317586393875184 | validation: 0.08026423834607216]
	TIME [epoch: 17.4 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039975653217855706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039975653217855706 | validation: 0.11025826107301123]
	TIME [epoch: 17.4 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06421889509782369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06421889509782369 | validation: 0.14608665442814675]
	TIME [epoch: 17.4 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09247385493390992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09247385493390992 | validation: 0.3542758564507312]
	TIME [epoch: 17.4 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08423750270253218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08423750270253218 | validation: 0.10200337565828721]
	TIME [epoch: 17.4 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02749709924084325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02749709924084325 | validation: 0.08101369911987472]
	TIME [epoch: 17.4 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032871569005283034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032871569005283034 | validation: 0.0801809035110858]
	TIME [epoch: 17.3 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022700192373004573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022700192373004573 | validation: 0.07504235191325281]
	TIME [epoch: 17.4 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02398912115098824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02398912115098824 | validation: 0.07205188483855193]
	TIME [epoch: 17.4 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020019007716800194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020019007716800194 | validation: 0.06402264397778472]
	TIME [epoch: 17.4 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020805341933213723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020805341933213723 | validation: 0.11441884737046451]
	TIME [epoch: 17.4 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02911294827895282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02911294827895282 | validation: 0.0844441012588214]
	TIME [epoch: 17.4 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05588805063446821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05588805063446821 | validation: 0.11312094420017674]
	TIME [epoch: 17.4 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07744587927149355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07744587927149355 | validation: 0.12359674180853597]
	TIME [epoch: 17.3 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0710879072064165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0710879072064165 | validation: 0.07484314786994438]
	TIME [epoch: 17.4 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03662762923017885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03662762923017885 | validation: 0.06889809598175828]
	TIME [epoch: 17.3 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033466076944812996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033466076944812996 | validation: 0.08913613472167613]
	TIME [epoch: 17.4 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049739036549406104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049739036549406104 | validation: 0.10593641476602372]
	TIME [epoch: 17.3 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05194404896781331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05194404896781331 | validation: 0.10427946240085495]
	TIME [epoch: 17.3 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03637477400905146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03637477400905146 | validation: 0.06629188946615786]
	TIME [epoch: 17.4 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023564555676046282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023564555676046282 | validation: 0.07201423550060893]
	TIME [epoch: 17.4 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030341495300415914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030341495300415914 | validation: 0.07806562020871712]
	TIME [epoch: 17.4 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04736339923735743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04736339923735743 | validation: 0.12787777054015415]
	TIME [epoch: 17.4 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05924961704725242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05924961704725242 | validation: 0.07366319862583043]
	TIME [epoch: 17.3 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04622941391937424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04622941391937424 | validation: 0.06923741096480893]
	TIME [epoch: 17.4 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030060070475536337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030060070475536337 | validation: 0.06288616511215533]
	TIME [epoch: 17.3 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02112828273031134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02112828273031134 | validation: 0.10157227678100773]
	TIME [epoch: 17.4 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028340198583663608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028340198583663608 | validation: 0.09564673723332678]
	TIME [epoch: 17.3 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0382324394549564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0382324394549564 | validation: 0.1467160436512241]
	TIME [epoch: 17.4 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04501701194681686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04501701194681686 | validation: 0.10503175368700768]
	TIME [epoch: 17.3 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054939690710084525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054939690710084525 | validation: 0.20008279112312102]
	TIME [epoch: 17.4 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09680116497563564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09680116497563564 | validation: 0.08913400208844541]
	TIME [epoch: 17.4 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031292876716184505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031292876716184505 | validation: 0.08562121325660153]
	TIME [epoch: 17.4 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04475872577936855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04475872577936855 | validation: 0.2828895306944606]
	TIME [epoch: 17.3 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08638319636887705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08638319636887705 | validation: 0.13469599219044323]
	TIME [epoch: 17.4 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08360946383617872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08360946383617872 | validation: 0.09457906420515144]
	TIME [epoch: 17.3 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05389517571652402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05389517571652402 | validation: 0.07975961210804765]
	TIME [epoch: 17.4 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03565165066376638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03565165066376638 | validation: 0.07489689900405114]
	TIME [epoch: 17.4 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01923282840146846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01923282840146846 | validation: 0.06416739860152672]
	TIME [epoch: 17.4 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019717562352264885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019717562352264885 | validation: 0.07582214905357056]
	TIME [epoch: 17.4 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023003842814761404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023003842814761404 | validation: 0.057651047223923696]
	TIME [epoch: 17.4 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02089087811675346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02089087811675346 | validation: 0.06905806045424116]
	TIME [epoch: 17.4 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01959870424264166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01959870424264166 | validation: 0.06331783013517832]
	TIME [epoch: 17.4 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023615820855600822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023615820855600822 | validation: 0.06657615090529827]
	TIME [epoch: 17.4 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03401100809053574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03401100809053574 | validation: 0.09291782959009304]
	TIME [epoch: 17.3 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0656600695401449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0656600695401449 | validation: 0.16403110424663436]
	TIME [epoch: 17.4 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08121763638856777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08121763638856777 | validation: 0.06335317036138548]
	TIME [epoch: 17.4 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03217074686830013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03217074686830013 | validation: 0.048009437623156875]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_1209.pth
	Model improved!!!
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019069822252956396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019069822252956396 | validation: 0.06379727893156505]
	TIME [epoch: 17.4 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032405580754571645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032405580754571645 | validation: 0.07548288599564251]
	TIME [epoch: 17.4 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052077846931392724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052077846931392724 | validation: 0.08303984291425127]
	TIME [epoch: 17.4 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04710807000798724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04710807000798724 | validation: 0.08553784072165636]
	TIME [epoch: 17.4 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041753703016102296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041753703016102296 | validation: 0.1316917699523004]
	TIME [epoch: 17.4 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053715273878493924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053715273878493924 | validation: 0.5801043467814887]
	TIME [epoch: 17.4 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18851393893290624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18851393893290624 | validation: 0.18155342766072793]
	TIME [epoch: 17.4 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06269894317360312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06269894317360312 | validation: 0.2290851058742611]
	TIME [epoch: 17.4 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08970820418506442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08970820418506442 | validation: 0.13014883433383514]
	TIME [epoch: 17.4 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055502807355968076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055502807355968076 | validation: 0.11903814053564861]
	TIME [epoch: 17.4 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0665491509943782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0665491509943782 | validation: 0.12496691067499653]
	TIME [epoch: 17.4 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03386605673662491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03386605673662491 | validation: 0.11557894848677948]
	TIME [epoch: 17.4 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02340451031901888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02340451031901888 | validation: 0.09740998736555984]
	TIME [epoch: 17.4 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02021126883447006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02021126883447006 | validation: 0.0708265208126823]
	TIME [epoch: 17.4 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018717318045754605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018717318045754605 | validation: 0.06429037419785162]
	TIME [epoch: 17.4 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0173529787324755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0173529787324755 | validation: 0.06582477584400855]
	TIME [epoch: 17.4 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016304456433267483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016304456433267483 | validation: 0.06328302086640858]
	TIME [epoch: 17.4 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01714450313081047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01714450313081047 | validation: 0.07420011490710542]
	TIME [epoch: 17.4 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022850205951565346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022850205951565346 | validation: 0.08731938194190923]
	TIME [epoch: 17.4 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04116585081822406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04116585081822406 | validation: 0.11015106530145996]
	TIME [epoch: 17.4 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07334834069394112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07334834069394112 | validation: 0.111539640639195]
	TIME [epoch: 17.4 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08802804417297307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08802804417297307 | validation: 0.09283127985610658]
	TIME [epoch: 17.4 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052982540247130165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052982540247130165 | validation: 0.0605317392107918]
	TIME [epoch: 17.3 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0243166347874677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0243166347874677 | validation: 0.06211250858985589]
	TIME [epoch: 17.3 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02158722577977043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02158722577977043 | validation: 0.05648601154404375]
	TIME [epoch: 17.4 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02070407680847435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02070407680847435 | validation: 0.04936274753089797]
	TIME [epoch: 17.3 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018894426106230797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018894426106230797 | validation: 0.058848058397284436]
	TIME [epoch: 17.4 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017049427736668843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017049427736668843 | validation: 0.07023007013523296]
	TIME [epoch: 17.3 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015837218112977294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015837218112977294 | validation: 0.055541533322882745]
	TIME [epoch: 17.4 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01951786459164274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01951786459164274 | validation: 0.06391455413613947]
	TIME [epoch: 17.4 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02009792590014051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02009792590014051 | validation: 0.07475129711477113]
	TIME [epoch: 17.4 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025687951768399636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025687951768399636 | validation: 0.0686500009904506]
	TIME [epoch: 17.4 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03576419428362122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03576419428362122 | validation: 0.1022821773713783]
	TIME [epoch: 17.4 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0778925476161466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0778925476161466 | validation: 0.15552416390899457]
	TIME [epoch: 17.4 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1299683247790721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1299683247790721 | validation: 0.19658495719150199]
	TIME [epoch: 17.3 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061395771300605334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061395771300605334 | validation: 0.09879677385992834]
	TIME [epoch: 17.4 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1033537972124207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1033537972124207 | validation: 0.14376362780855706]
	TIME [epoch: 17.4 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.109132595310278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.109132595310278 | validation: 0.08047778347036745]
	TIME [epoch: 17.4 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04781848577672548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04781848577672548 | validation: 0.07253546546297847]
	TIME [epoch: 17.4 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023943995445888277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023943995445888277 | validation: 0.065281040710698]
	TIME [epoch: 17.4 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025337544937371757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025337544937371757 | validation: 0.058088064171348944]
	TIME [epoch: 17.3 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017137788991219337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017137788991219337 | validation: 0.11377646933083274]
	TIME [epoch: 17.4 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02236012471723379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02236012471723379 | validation: 0.05436715436710979]
	TIME [epoch: 17.3 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0152267764092971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0152267764092971 | validation: 0.049198214147873245]
	TIME [epoch: 17.4 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01445397502091536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01445397502091536 | validation: 0.053226267987764024]
	TIME [epoch: 17.3 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014069920345407404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014069920345407404 | validation: 0.05103209173557596]
	TIME [epoch: 17.4 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01715072995502105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01715072995502105 | validation: 0.066556631384404]
	TIME [epoch: 17.3 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03497895294199821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03497895294199821 | validation: 0.11771920928126907]
	TIME [epoch: 17.4 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07731542948535623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07731542948535623 | validation: 0.10664831363447691]
	TIME [epoch: 17.3 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0924252228072238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0924252228072238 | validation: 0.08283220051212895]
	TIME [epoch: 17.4 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03806565586959191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03806565586959191 | validation: 0.05822690650000542]
	TIME [epoch: 17.4 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030756768629030748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030756768629030748 | validation: 0.0646087815920446]
	TIME [epoch: 17.4 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028209589219806402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028209589219806402 | validation: 0.22812324066395348]
	TIME [epoch: 17.4 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08130243667733854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08130243667733854 | validation: 0.13878217201059553]
	TIME [epoch: 17.4 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07013626078158261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07013626078158261 | validation: 0.6254227197973861]
	TIME [epoch: 17.4 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20597485250186306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20597485250186306 | validation: 0.17986537743107223]
	TIME [epoch: 17.4 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042020341991605925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042020341991605925 | validation: 0.11412758394391816]
	TIME [epoch: 17.4 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041406194690545906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041406194690545906 | validation: 0.1308139035771557]
	TIME [epoch: 17.4 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04695663571982488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04695663571982488 | validation: 0.11614784896737623]
	TIME [epoch: 17.3 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04834684833557046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04834684833557046 | validation: 0.09035912178009742]
	TIME [epoch: 17.4 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03447826207955905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03447826207955905 | validation: 0.0940143665602578]
	TIME [epoch: 17.3 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025662726073139384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025662726073139384 | validation: 0.0867201097921846]
	TIME [epoch: 17.4 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0211152150855317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0211152150855317 | validation: 0.068324206318622]
	TIME [epoch: 17.4 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018430350648763354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018430350648763354 | validation: 0.07351499557919201]
	TIME [epoch: 17.4 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016407515053669837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016407515053669837 | validation: 0.06868806560813798]
	TIME [epoch: 17.4 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0158532772178007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0158532772178007 | validation: 0.05908478708529561]
	TIME [epoch: 17.4 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016620149234517805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016620149234517805 | validation: 0.06124290259772073]
	TIME [epoch: 17.4 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018596790347720375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018596790347720375 | validation: 0.07813825074985842]
	TIME [epoch: 17.4 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039529235309396026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039529235309396026 | validation: 0.11694959885408718]
	TIME [epoch: 17.4 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07906884031365055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07906884031365055 | validation: 0.12485437992656144]
	TIME [epoch: 17.4 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08793233431792069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08793233431792069 | validation: 0.07341513173066297]
	TIME [epoch: 17.4 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027800393638702098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027800393638702098 | validation: 0.05999712129607915]
	TIME [epoch: 17.4 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018126184377023558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018126184377023558 | validation: 0.05489367261139543]
	TIME [epoch: 17.4 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024736686021543993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024736686021543993 | validation: 0.049480557171711285]
	TIME [epoch: 17.4 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02289131655823308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02289131655823308 | validation: 0.05599923235159764]
	TIME [epoch: 17.3 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016356259078303573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016356259078303573 | validation: 0.053150362971305855]
	TIME [epoch: 17.3 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014701445253904632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014701445253904632 | validation: 0.050294849146683966]
	TIME [epoch: 17.3 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02066775240239657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02066775240239657 | validation: 0.061225437723166826]
	TIME [epoch: 17.4 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026847672311673343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026847672311673343 | validation: 0.06533936807208017]
	TIME [epoch: 17.3 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03628661472424633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03628661472424633 | validation: 0.06275414556338112]
	TIME [epoch: 17.4 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0465267868754605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0465267868754605 | validation: 0.06090781603047457]
	TIME [epoch: 17.4 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03179415311929023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03179415311929023 | validation: 0.06305680815936697]
	TIME [epoch: 17.4 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03376588799432322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03376588799432322 | validation: 0.08679553863558709]
	TIME [epoch: 17.4 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06305975740787609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06305975740787609 | validation: 0.09751887546392841]
	TIME [epoch: 17.3 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0782900162186262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0782900162186262 | validation: 0.07690477949189081]
	TIME [epoch: 17.4 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03317370334827356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03317370334827356 | validation: 0.04046182821424216]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_1295.pth
	Model improved!!!
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015198401380335156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015198401380335156 | validation: 0.046022551411226714]
	TIME [epoch: 17.3 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01806827655911399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01806827655911399 | validation: 0.0462103311778868]
	TIME [epoch: 17.4 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01954529434355946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01954529434355946 | validation: 0.04233372848139724]
	TIME [epoch: 17.4 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02047027552597311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02047027552597311 | validation: 0.06567325303125116]
	TIME [epoch: 17.3 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02652173116724505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02652173116724505 | validation: 0.05636220868030717]
	TIME [epoch: 17.4 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04159241138057613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04159241138057613 | validation: 0.08092742674132063]
	TIME [epoch: 17.4 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03927282123854843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03927282123854843 | validation: 0.06143874886293415]
	TIME [epoch: 17.4 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03279267354721754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03279267354721754 | validation: 0.05958072837538691]
	TIME [epoch: 17.4 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022010883263386013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022010883263386013 | validation: 0.06469430436841687]
	TIME [epoch: 17.4 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02247806654921702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02247806654921702 | validation: 0.06922558994241979]
	TIME [epoch: 17.4 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03783685963443346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03783685963443346 | validation: 0.08771675533366693]
	TIME [epoch: 17.4 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0624432723702858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0624432723702858 | validation: 0.11333907164274991]
	TIME [epoch: 17.3 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07205258403168302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07205258403168302 | validation: 0.10007329345125444]
	TIME [epoch: 17.4 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041994315391490414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041994315391490414 | validation: 0.06812708700436663]
	TIME [epoch: 17.3 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025301056490469716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025301056490469716 | validation: 0.05526216703333791]
	TIME [epoch: 17.4 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02895316186533089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02895316186533089 | validation: 0.08098531358919553]
	TIME [epoch: 17.3 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046729877451668934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046729877451668934 | validation: 0.071323016794682]
	TIME [epoch: 17.4 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03943897370382469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03943897370382469 | validation: 0.06526036117754698]
	TIME [epoch: 17.3 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03873764890750902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03873764890750902 | validation: 0.05363884376707796]
	TIME [epoch: 17.4 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02220413127362675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02220413127362675 | validation: 0.05297421011552416]
	TIME [epoch: 17.4 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018221171495057243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018221171495057243 | validation: 0.07857132184704865]
	TIME [epoch: 17.3 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02115925246945734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02115925246945734 | validation: 0.05109760576819125]
	TIME [epoch: 17.3 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030485750810498296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030485750810498296 | validation: 0.07721462054284983]
	TIME [epoch: 17.4 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05295279766596003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05295279766596003 | validation: 0.07729145629685845]
	TIME [epoch: 17.3 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04923422921031903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04923422921031903 | validation: 0.047087031038773375]
	TIME [epoch: 17.4 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027885189789780773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027885189789780773 | validation: 0.04603229271350273]
	TIME [epoch: 17.3 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012703801252089704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012703801252089704 | validation: 0.03961839420511684]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_1322.pth
	Model improved!!!
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01098099410023163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01098099410023163 | validation: 0.05158047929427162]
	TIME [epoch: 17.3 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014654691186966249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014654691186966249 | validation: 0.08481291992252142]
	TIME [epoch: 17.3 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026978336539162075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026978336539162075 | validation: 0.13988161425321938]
	TIME [epoch: 17.3 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04657275538327504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04657275538327504 | validation: 0.09518473714199148]
	TIME [epoch: 17.3 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04348522967087478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04348522967087478 | validation: 0.06507682203320463]
	TIME [epoch: 17.3 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03663190874184152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03663190874184152 | validation: 0.06436198338241438]
	TIME [epoch: 17.3 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042160440277303604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042160440277303604 | validation: 0.08632615941613755]
	TIME [epoch: 17.3 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05566272876661294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05566272876661294 | validation: 0.08651318227815225]
	TIME [epoch: 17.3 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05639963141361989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05639963141361989 | validation: 0.11024912254704343]
	TIME [epoch: 17.3 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04250855077567527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04250855077567527 | validation: 0.06214599049580283]
	TIME [epoch: 17.3 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029096589164787218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029096589164787218 | validation: 0.08043157758363376]
	TIME [epoch: 17.3 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047174905142766566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047174905142766566 | validation: 0.11581806937688294]
	TIME [epoch: 17.3 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06413611577983107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06413611577983107 | validation: 0.09948759760483167]
	TIME [epoch: 17.3 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06483110836498131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06483110836498131 | validation: 0.058249041205966134]
	TIME [epoch: 17.3 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029598977532832648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029598977532832648 | validation: 0.046618236037154016]
	TIME [epoch: 17.3 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0229668839623137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0229668839623137 | validation: 0.050405316052023114]
	TIME [epoch: 17.3 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01515477125075163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01515477125075163 | validation: 0.05283649688025961]
	TIME [epoch: 17.3 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015625482088601914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015625482088601914 | validation: 0.04363259859678699]
	TIME [epoch: 17.3 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015376140134337794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015376140134337794 | validation: 0.05099594468191501]
	TIME [epoch: 17.3 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01985436428679962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01985436428679962 | validation: 0.16594596917348095]
	TIME [epoch: 17.3 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06490879465491592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06490879465491592 | validation: 0.07359748146847943]
	TIME [epoch: 17.3 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04405027360739926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04405027360739926 | validation: 0.059400934092402435]
	TIME [epoch: 17.3 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03280708075806071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03280708075806071 | validation: 0.0660212293427626]
	TIME [epoch: 17.3 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02674842704210354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02674842704210354 | validation: 0.07237280316555522]
	TIME [epoch: 17.3 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036234877096434845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036234877096434845 | validation: 0.08928110249054026]
	TIME [epoch: 17.3 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06278792621363648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06278792621363648 | validation: 0.0804228755556619]
	TIME [epoch: 17.3 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05691647925229879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05691647925229879 | validation: 0.04956891829624365]
	TIME [epoch: 17.3 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026034292419550714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026034292419550714 | validation: 0.04666298983384065]
	TIME [epoch: 17.3 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011246235126566721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011246235126566721 | validation: 0.046418157671451826]
	TIME [epoch: 17.3 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01213590427770145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01213590427770145 | validation: 0.04145970480763215]
	TIME [epoch: 17.3 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014073046279596699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014073046279596699 | validation: 0.04428815017623251]
	TIME [epoch: 17.3 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019471830864003965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019471830864003965 | validation: 0.046724821844762926]
	TIME [epoch: 17.3 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023376748222087677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023376748222087677 | validation: 0.06491670649453089]
	TIME [epoch: 17.3 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04062720332450237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04062720332450237 | validation: 0.07996253939306464]
	TIME [epoch: 17.3 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04662204834012515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04662204834012515 | validation: 0.051247239817230894]
	TIME [epoch: 17.3 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034639207034163115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034639207034163115 | validation: 0.07650138596135535]
	TIME [epoch: 17.3 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024783728211260826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024783728211260826 | validation: 0.2146583497730221]
	TIME [epoch: 17.3 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049529892501029134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049529892501029134 | validation: 0.08649792467591516]
	TIME [epoch: 17.3 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040474486014332886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040474486014332886 | validation: 0.1023347163162482]
	TIME [epoch: 17.3 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08098760420493165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08098760420493165 | validation: 0.11130363219705251]
	TIME [epoch: 17.2 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07354164663956815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07354164663956815 | validation: 0.10374021495200131]
	TIME [epoch: 17.3 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042156733893206705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042156733893206705 | validation: 0.04861963361120147]
	TIME [epoch: 17.3 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020444403458213883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020444403458213883 | validation: 0.056633772530401]
	TIME [epoch: 17.3 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02077504922425758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02077504922425758 | validation: 0.06717897200132839]
	TIME [epoch: 17.2 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027404651990353513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027404651990353513 | validation: 0.08876487261490701]
	TIME [epoch: 17.3 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030472069823942807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030472069823942807 | validation: 0.08342064431658187]
	TIME [epoch: 17.3 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027277433815535705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027277433815535705 | validation: 0.03952135791781084]
	TIME [epoch: 17.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_1369.pth
	Model improved!!!
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011580379292488006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011580379292488006 | validation: 0.04778506229657497]
	TIME [epoch: 17.3 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012636385807391693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012636385807391693 | validation: 0.05238556487556966]
	TIME [epoch: 17.3 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014337969697897242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014337969697897242 | validation: 0.03534284286843294]
	TIME [epoch: 17.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_1372.pth
	Model improved!!!
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012778265526973373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012778265526973373 | validation: 0.03927521785796948]
	TIME [epoch: 17.3 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013336936387920708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013336936387920708 | validation: 0.04573524801818474]
	TIME [epoch: 17.3 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02797173659706717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02797173659706717 | validation: 0.08735584273218648]
	TIME [epoch: 17.3 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05891092494171879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05891092494171879 | validation: 0.09793252744115748]
	TIME [epoch: 17.3 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08817313261603016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08817313261603016 | validation: 0.07586065470980342]
	TIME [epoch: 17.2 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05854487345083422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05854487345083422 | validation: 0.10813003543557285]
	TIME [epoch: 17.3 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04384465924272088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04384465924272088 | validation: 0.05849394508534383]
	TIME [epoch: 17.3 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045050128339343236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045050128339343236 | validation: 0.05391537999288052]
	TIME [epoch: 17.2 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0291859820834895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0291859820834895 | validation: 0.0847591635641603]
	TIME [epoch: 17.2 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02492647439904811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02492647439904811 | validation: 0.05717778895642202]
	TIME [epoch: 17.3 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02798833003760218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02798833003760218 | validation: 0.05368009687294772]
	TIME [epoch: 17.2 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025525883663076218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025525883663076218 | validation: 0.05742462738112593]
	TIME [epoch: 17.3 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02951331579632765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02951331579632765 | validation: 0.05193469521028388]
	TIME [epoch: 17.3 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02954251068141959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02954251068141959 | validation: 0.054916270012900704]
	TIME [epoch: 17.3 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029280718001705335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029280718001705335 | validation: 0.06233842920288575]
	TIME [epoch: 17.4 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023624913541715913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023624913541715913 | validation: 0.07149385586333115]
	TIME [epoch: 17.4 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021241133910586952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021241133910586952 | validation: 0.10696822723188062]
	TIME [epoch: 17.3 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02774207235135429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02774207235135429 | validation: 0.07890279130047631]
	TIME [epoch: 17.3 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029928181783484015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029928181783484015 | validation: 0.05203135832333591]
	TIME [epoch: 17.3 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026789660482547335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026789660482547335 | validation: 0.09318704124634417]
	TIME [epoch: 17.3 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0429719402274648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0429719402274648 | validation: 0.07219085721137646]
	TIME [epoch: 17.4 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04587644313870486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04587644313870486 | validation: 0.06502654258318201]
	TIME [epoch: 17.4 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04158099464728764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04158099464728764 | validation: 0.05132458310364477]
	TIME [epoch: 17.3 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024867678657583436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024867678657583436 | validation: 0.046553006115903975]
	TIME [epoch: 17.4 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014530116179739025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014530116179739025 | validation: 0.04633992246446578]
	TIME [epoch: 17.3 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013853963259033923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013853963259033923 | validation: 0.04427502234374412]
	TIME [epoch: 17.4 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017553379308983524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017553379308983524 | validation: 0.06849309737178265]
	TIME [epoch: 17.4 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04626555547399091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04626555547399091 | validation: 0.09558497222977806]
	TIME [epoch: 17.3 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09327435927304775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09327435927304775 | validation: 0.09291272321169636]
	TIME [epoch: 17.3 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05206436109464027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05206436109464027 | validation: 0.058754093864250394]
	TIME [epoch: 17.3 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0214753395323389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0214753395323389 | validation: 0.050796165851241276]
	TIME [epoch: 17.4 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028469115067729263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028469115067729263 | validation: 0.06286751479758516]
	TIME [epoch: 17.3 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032985305741113344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032985305741113344 | validation: 0.09350960119070661]
	TIME [epoch: 17.3 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04010956416149527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04010956416149527 | validation: 0.05335826359888532]
	TIME [epoch: 17.2 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027169172946070238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027169172946070238 | validation: 0.04268494145192592]
	TIME [epoch: 17.2 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016447546016068723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016447546016068723 | validation: 0.06946517095299203]
	TIME [epoch: 17.3 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018470368811896728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018470368811896728 | validation: 0.042430922787787]
	TIME [epoch: 17.2 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021618365988008953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021618365988008953 | validation: 0.05242023467097665]
	TIME [epoch: 17.2 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033481673631538665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033481673631538665 | validation: 0.08102460561949344]
	TIME [epoch: 17.4 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046431376956707744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046431376956707744 | validation: 0.06308336529019509]
	TIME [epoch: 17.3 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037968055794758676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037968055794758676 | validation: 0.05050098598993368]
	TIME [epoch: 17.4 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020565261393584517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020565261393584517 | validation: 0.040367346829437316]
	TIME [epoch: 17.2 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015148799943784432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015148799943784432 | validation: 0.06267526288786789]
	TIME [epoch: 17.3 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026620263210492975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026620263210492975 | validation: 0.24152544096449186]
	TIME [epoch: 17.3 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09439567300944131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09439567300944131 | validation: 0.08428887165333447]
	TIME [epoch: 17.2 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04572142437573172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04572142437573172 | validation: 0.092994134872313]
	TIME [epoch: 17.2 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03848083330454995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03848083330454995 | validation: 0.12678908127119676]
	TIME [epoch: 17.3 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055772142917822246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055772142917822246 | validation: 0.08691325411483702]
	TIME [epoch: 17.3 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052327056218997026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052327056218997026 | validation: 0.08413900568610745]
	TIME [epoch: 17.2 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04699198617537561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04699198617537561 | validation: 0.055237900234775754]
	TIME [epoch: 17.3 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01890017042353532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01890017042353532 | validation: 0.041051023118063204]
	TIME [epoch: 17.2 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01229176761110631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01229176761110631 | validation: 0.03986664218204548]
	TIME [epoch: 17.3 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014820284019261418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014820284019261418 | validation: 0.04566083940441831]
	TIME [epoch: 17.2 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013683795066965362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013683795066965362 | validation: 0.03905730093711077]
	TIME [epoch: 17.2 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01498062146588654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01498062146588654 | validation: 0.043397237401228354]
	TIME [epoch: 17.2 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014481605770096285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014481605770096285 | validation: 0.049337780227494125]
	TIME [epoch: 17.3 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02112078339290461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02112078339290461 | validation: 0.06372671055127055]
	TIME [epoch: 17.2 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04384366424519164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04384366424519164 | validation: 0.06468909790434683]
	TIME [epoch: 17.2 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053669728063083094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053669728063083094 | validation: 0.053414985210311766]
	TIME [epoch: 17.2 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023817906407907503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023817906407907503 | validation: 0.21384127616979895]
	TIME [epoch: 17.2 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04021319362075991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04021319362075991 | validation: 0.04798493999563313]
	TIME [epoch: 17.2 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04078544389358396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04078544389358396 | validation: 0.11036514668407725]
	TIME [epoch: 17.3 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03952066509404487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03952066509404487 | validation: 0.04839369345107775]
	TIME [epoch: 17.2 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03024710973695208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03024710973695208 | validation: 0.08187958785869333]
	TIME [epoch: 17.3 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04455059836839771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04455059836839771 | validation: 0.11683021095508724]
	TIME [epoch: 17.3 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09087744171087074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09087744171087074 | validation: 0.07168103710597273]
	TIME [epoch: 17.3 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04923048450563675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04923048450563675 | validation: 0.053919278658540404]
	TIME [epoch: 17.3 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022497573411573702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022497573411573702 | validation: 0.03366558965019215]
	TIME [epoch: 17.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_1440.pth
	Model improved!!!
EPOCH 1441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012177394024063695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012177394024063695 | validation: 0.03955050553306805]
	TIME [epoch: 17.4 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012855433831031904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012855433831031904 | validation: 0.04607213897244811]
	TIME [epoch: 17.4 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015368149918077388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015368149918077388 | validation: 0.048054378725328334]
	TIME [epoch: 17.4 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023078681391174572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023078681391174572 | validation: 0.07256837131494015]
	TIME [epoch: 17.3 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04311695755176267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04311695755176267 | validation: 0.12029640676720353]
	TIME [epoch: 17.4 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06201693777845909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06201693777845909 | validation: 0.06678492328331968]
	TIME [epoch: 17.4 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03695027570414723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03695027570414723 | validation: 0.042917075283674735]
	TIME [epoch: 17.4 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011494266331302603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011494266331302603 | validation: 0.06024544114196173]
	TIME [epoch: 17.4 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012427236840620158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012427236840620158 | validation: 0.09074013694741034]
	TIME [epoch: 17.3 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021167757360801478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021167757360801478 | validation: 0.0686865287835338]
	TIME [epoch: 17.4 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04719453809937098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04719453809937098 | validation: 0.0904779384906107]
	TIME [epoch: 17.4 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07746598274099326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07746598274099326 | validation: 0.09058447346243793]
	TIME [epoch: 17.4 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07153364031989587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07153364031989587 | validation: 0.06320733677340823]
	TIME [epoch: 17.4 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03298522483582696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03298522483582696 | validation: 0.06397005795557789]
	TIME [epoch: 17.4 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015717183988103917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015717183988103917 | validation: 0.038169767311513496]
	TIME [epoch: 17.4 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019949362281495887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019949362281495887 | validation: 0.03374784888932344]
	TIME [epoch: 17.4 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01789875104028383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01789875104028383 | validation: 0.03984907790998019]
	TIME [epoch: 17.5 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015856909630450607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015856909630450607 | validation: 0.047528853706054336]
	TIME [epoch: 17.4 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018779139549415392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018779139549415392 | validation: 0.04767236335255938]
	TIME [epoch: 17.4 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025875818490855577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025875818490855577 | validation: 0.05087157492390283]
	TIME [epoch: 17.4 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03751543536661376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03751543536661376 | validation: 0.06102631107295782]
	TIME [epoch: 17.4 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03626610926875614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03626610926875614 | validation: 0.04932303444506822]
	TIME [epoch: 17.4 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026098506258074538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026098506258074538 | validation: 0.04897517560709184]
	TIME [epoch: 17.4 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021524363333931235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021524363333931235 | validation: 0.046531041112997246]
	TIME [epoch: 17.4 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02733253939028605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02733253939028605 | validation: 0.051596651074199223]
	TIME [epoch: 17.3 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030390382083590392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030390382083590392 | validation: 0.046136198474990335]
	TIME [epoch: 17.4 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03422251729095659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03422251729095659 | validation: 0.07729085565785199]
	TIME [epoch: 17.4 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04786790853717907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04786790853717907 | validation: 0.06739722450006501]
	TIME [epoch: 17.3 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03038672404584447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03038672404584447 | validation: 0.10408193089261611]
	TIME [epoch: 17.3 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03740040206394022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03740040206394022 | validation: 0.10397073867546898]
	TIME [epoch: 17.3 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03740615943796783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03740615943796783 | validation: 0.038207784031155684]
	TIME [epoch: 17.2 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01814884297150792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01814884297150792 | validation: 0.054659202660931876]
	TIME [epoch: 17.3 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011541032234463604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011541032234463604 | validation: 0.04205338101500392]
	TIME [epoch: 17.3 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011957217767375185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011957217767375185 | validation: 0.07018914481678153]
	TIME [epoch: 17.3 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01670264507370275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01670264507370275 | validation: 0.03906347882517067]
	TIME [epoch: 17.3 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016776818092227065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016776818092227065 | validation: 0.0504762947439102]
	TIME [epoch: 17.3 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027283220369284128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027283220369284128 | validation: 0.07558961539764586]
	TIME [epoch: 17.4 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057068529459383975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057068529459383975 | validation: 0.0923882591477037]
	TIME [epoch: 17.3 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06460067085162237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06460067085162237 | validation: 0.0638052253227384]
	TIME [epoch: 17.3 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04706917038348338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04706917038348338 | validation: 0.04429213006133775]
	TIME [epoch: 17.4 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020219525085463957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020219525085463957 | validation: 0.0395595127818234]
	TIME [epoch: 17.4 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013377442172991501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013377442172991501 | validation: 0.04180819873154386]
	TIME [epoch: 17.3 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01326455853326225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01326455853326225 | validation: 0.034847486706557976]
	TIME [epoch: 17.3 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01978964701222283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01978964701222283 | validation: 0.12264513265257915]
	TIME [epoch: 17.3 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056826877794521134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056826877794521134 | validation: 0.15091135499701017]
	TIME [epoch: 17.3 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07517622505736828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07517622505736828 | validation: 0.055227326422190186]
	TIME [epoch: 17.3 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024540456269800552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024540456269800552 | validation: 0.041688723772650094]
	TIME [epoch: 17.3 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01347482080270366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01347482080270366 | validation: 0.07125363055880762]
	TIME [epoch: 17.3 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026787542186577494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026787542186577494 | validation: 0.06385601215520546]
	TIME [epoch: 17.3 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045293492960884515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045293492960884515 | validation: 0.11190691043224375]
	TIME [epoch: 17.3 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07907726049250241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07907726049250241 | validation: 0.05984084671462539]
	TIME [epoch: 17.3 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03621012052155894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03621012052155894 | validation: 0.04110336745723976]
	TIME [epoch: 17.3 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012400385025516731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012400385025516731 | validation: 0.04658203552365427]
	TIME [epoch: 17.3 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015291500550269234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015291500550269234 | validation: 0.039707287947361086]
	TIME [epoch: 17.2 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013237975503181888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013237975503181888 | validation: 0.027780314650762895]
	TIME [epoch: 17.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_1495.pth
	Model improved!!!
EPOCH 1496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011067145920352779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011067145920352779 | validation: 0.03922319301105618]
	TIME [epoch: 17.4 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011598542723557521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011598542723557521 | validation: 0.04178783334674337]
	TIME [epoch: 17.4 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014041986604862259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014041986604862259 | validation: 0.06291730541051278]
	TIME [epoch: 17.4 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018206853223701257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018206853223701257 | validation: 0.07661923106344667]
	TIME [epoch: 17.4 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02507511494037406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02507511494037406 | validation: 0.0418347101750878]
	TIME [epoch: 17.4 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022554753200887616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022554753200887616 | validation: 0.040838831472576824]
	TIME [epoch: 17.4 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021958404008002017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021958404008002017 | validation: 0.059897361041525876]
	TIME [epoch: 17.4 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03481632241386036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03481632241386036 | validation: 0.08376413632665262]
	TIME [epoch: 17.4 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06431033871159211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06431033871159211 | validation: 0.07980952859241772]
	TIME [epoch: 17.3 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055657714020303416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055657714020303416 | validation: 0.10171614395059746]
	TIME [epoch: 17.4 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054549516380698417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054549516380698417 | validation: 0.10123472711557041]
	TIME [epoch: 17.3 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05018043395234242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05018043395234242 | validation: 0.057639686904286364]
	TIME [epoch: 17.4 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027550257092624673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027550257092624673 | validation: 0.06568087217568455]
	TIME [epoch: 17.3 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031402889738978985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031402889738978985 | validation: 0.07300498916369005]
	TIME [epoch: 17.3 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01764903227859072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01764903227859072 | validation: 0.053380735166485775]
	TIME [epoch: 17.4 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019534649044236664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019534649044236664 | validation: 0.037085008306876936]
	TIME [epoch: 17.3 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017789938875422497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017789938875422497 | validation: 0.05103650063276993]
	TIME [epoch: 17.4 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020808776205438972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020808776205438972 | validation: 0.0721345208262471]
	TIME [epoch: 17.4 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027311170121135816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027311170121135816 | validation: 0.04001732327655375]
	TIME [epoch: 17.3 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025563184138375897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025563184138375897 | validation: 0.045490017446135594]
	TIME [epoch: 17.4 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025518803813208563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025518803813208563 | validation: 0.0432794227295636]
	TIME [epoch: 17.3 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030551015473189343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030551015473189343 | validation: 0.057599540183575985]
	TIME [epoch: 17.4 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03237298802435488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03237298802435488 | validation: 0.049353142526654674]
	TIME [epoch: 17.3 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027536101290096277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027536101290096277 | validation: 0.039494348016296914]
	TIME [epoch: 17.5 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024660808739074246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024660808739074246 | validation: 0.04065401509458562]
	TIME [epoch: 17.4 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022415786294485266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022415786294485266 | validation: 0.047006332988969446]
	TIME [epoch: 17.4 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02557066638756492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02557066638756492 | validation: 0.07226254650917867]
	TIME [epoch: 17.3 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04686674813342537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04686674813342537 | validation: 0.0637358472057465]
	TIME [epoch: 17.4 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05326925753172066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05326925753172066 | validation: 0.07810446224251884]
	TIME [epoch: 17.4 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024846380101894452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024846380101894452 | validation: 0.05720190464795908]
	TIME [epoch: 17.4 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017418461019532257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017418461019532257 | validation: 0.067833228620834]
	TIME [epoch: 17.4 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025601839847043152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025601839847043152 | validation: 0.06413543744933108]
	TIME [epoch: 17.4 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034967216464298005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034967216464298005 | validation: 0.05231133045287261]
	TIME [epoch: 17.3 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03070834695002605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03070834695002605 | validation: 0.057932356544537746]
	TIME [epoch: 17.2 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033995319028766284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033995319028766284 | validation: 0.05196052029217081]
	TIME [epoch: 17.3 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026263633167864883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026263633167864883 | validation: 0.042445206488249655]
	TIME [epoch: 17.4 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01871799700249485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01871799700249485 | validation: 0.03475498341053312]
	TIME [epoch: 17.4 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01574121162685271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01574121162685271 | validation: 0.04488550631153872]
	TIME [epoch: 17.4 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017777558855587287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017777558855587287 | validation: 0.04400164363878707]
	TIME [epoch: 17.3 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025294179820861053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025294179820861053 | validation: 0.05884256168002155]
	TIME [epoch: 17.4 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04067837627678093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04067837627678093 | validation: 0.062130233327119117]
	TIME [epoch: 17.4 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04609817760330184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04609817760330184 | validation: 0.04306830730836726]
	TIME [epoch: 17.4 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02226293812916903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02226293812916903 | validation: 0.04085388837721045]
	TIME [epoch: 17.3 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009941743891929776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009941743891929776 | validation: 0.02221192466324099]
	TIME [epoch: 17.2 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_1539.pth
	Model improved!!!
EPOCH 1540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007793276914150187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.007793276914150187 | validation: 0.02889728195581598]
	TIME [epoch: 17.3 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009136463438481615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009136463438481615 | validation: 0.049533782696015355]
	TIME [epoch: 17.3 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020267138459580527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020267138459580527 | validation: 0.09994053456049991]
	TIME [epoch: 17.3 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04669530928279918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04669530928279918 | validation: 0.09223806793611544]
	TIME [epoch: 17.3 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05233112792984244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05233112792984244 | validation: 0.05661570545429098]
	TIME [epoch: 17.3 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04099678982358686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04099678982358686 | validation: 0.041487300929952266]
	TIME [epoch: 17.3 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027131736900399276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027131736900399276 | validation: 0.040001160273627004]
	TIME [epoch: 17.3 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021395808062558662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021395808062558662 | validation: 0.0659205506217445]
	TIME [epoch: 17.3 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02738827870275831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02738827870275831 | validation: 0.24461333761280743]
	TIME [epoch: 17.4 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12093438305725702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12093438305725702 | validation: 0.059819611554550824]
	TIME [epoch: 17.3 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03482434834150872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03482434834150872 | validation: 0.032351359477115885]
	TIME [epoch: 17.3 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024879130394421793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024879130394421793 | validation: 0.060311960649162416]
	TIME [epoch: 17.3 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019833480644483713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019833480644483713 | validation: 0.06969029847340688]
	TIME [epoch: 17.3 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04445078882223509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04445078882223509 | validation: 0.1001937520928002]
	TIME [epoch: 17.3 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07076723038447856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07076723038447856 | validation: 0.05081441149134072]
	TIME [epoch: 17.3 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025273993827656677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025273993827656677 | validation: 0.03290484724881259]
	TIME [epoch: 17.3 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014425307090861152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014425307090861152 | validation: 0.03324542243837776]
	TIME [epoch: 17.4 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014001831032384655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014001831032384655 | validation: 0.035291775129660134]
	TIME [epoch: 17.3 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014105468028771032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014105468028771032 | validation: 0.04832860577398704]
	TIME [epoch: 17.3 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01561099058306497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01561099058306497 | validation: 0.04263537512672788]
	TIME [epoch: 17.4 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019736273517680245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019736273517680245 | validation: 0.04319261970511663]
	TIME [epoch: 17.3 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020905633664318614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020905633664318614 | validation: 0.04221570513397584]
	TIME [epoch: 17.4 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0256912222301923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0256912222301923 | validation: 0.055551121473775424]
	TIME [epoch: 17.4 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03343662888470371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03343662888470371 | validation: 0.043365758307341365]
	TIME [epoch: 17.3 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03349762009596646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03349762009596646 | validation: 0.07820445070312321]
	TIME [epoch: 17.4 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02257605801861322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02257605801861322 | validation: 0.043641388998605125]
	TIME [epoch: 17.4 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025771012150581224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025771012150581224 | validation: 0.06514827556495033]
	TIME [epoch: 17.4 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04231071565983165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04231071565983165 | validation: 0.07879452060239141]
	TIME [epoch: 17.3 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05215254190760174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05215254190760174 | validation: 0.048348539608096014]
	TIME [epoch: 17.3 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027414814905962454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027414814905962454 | validation: 0.0691530166671508]
	TIME [epoch: 17.3 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023810464803391745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023810464803391745 | validation: 0.04413669557863197]
	TIME [epoch: 17.3 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01528475349925789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01528475349925789 | validation: 0.04569930555002667]
	TIME [epoch: 17.3 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013704990723296437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013704990723296437 | validation: 0.030594391182560066]
	TIME [epoch: 17.3 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013114905221576264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013114905221576264 | validation: 0.04444042731294017]
	TIME [epoch: 17.3 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020366041845029433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020366041845029433 | validation: 0.05693736242546249]
	TIME [epoch: 17.3 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034993154001803625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034993154001803625 | validation: 0.06279464462252803]
	TIME [epoch: 17.3 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03643633866601413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03643633866601413 | validation: 0.040876062741599585]
	TIME [epoch: 17.3 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03192937092229106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03192937092229106 | validation: 0.045238476994323644]
	TIME [epoch: 17.3 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022676159331464964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022676159331464964 | validation: 0.03477534084373921]
	TIME [epoch: 17.3 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017046801184351527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017046801184351527 | validation: 0.04982268932154154]
	TIME [epoch: 17.3 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02449126164446405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02449126164446405 | validation: 0.056228323790223]
	TIME [epoch: 17.3 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04133356940552214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04133356940552214 | validation: 0.0721622307394113]
	TIME [epoch: 17.3 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04869964010606179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04869964010606179 | validation: 0.08742573430503155]
	TIME [epoch: 17.3 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03514411006469568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03514411006469568 | validation: 0.08028980576349914]
	TIME [epoch: 17.3 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017737593007134633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017737593007134633 | validation: 0.09098064950263565]
	TIME [epoch: 17.3 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021471349731652982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021471349731652982 | validation: 0.04731838743917495]
	TIME [epoch: 17.3 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02030684903597746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02030684903597746 | validation: 0.06919213661199419]
	TIME [epoch: 17.3 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02781994778568198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02781994778568198 | validation: 0.1680935388551714]
	TIME [epoch: 17.3 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06158911848559418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06158911848559418 | validation: 0.055797790822884955]
	TIME [epoch: 17.3 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0323005922123666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0323005922123666 | validation: 0.08443357674740366]
	TIME [epoch: 17.3 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01852203993729233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01852203993729233 | validation: 0.03458541980919166]
	TIME [epoch: 17.3 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01795074845947844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01795074845947844 | validation: 0.04486114669698287]
	TIME [epoch: 17.3 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0313630099362516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0313630099362516 | validation: 0.0943948796002683]
	TIME [epoch: 17.3 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05578045364824966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05578045364824966 | validation: 0.07623786436378195]
	TIME [epoch: 17.3 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04532677245878506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04532677245878506 | validation: 0.051373615644955774]
	TIME [epoch: 17.4 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02068021546063857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02068021546063857 | validation: 0.03435333347582192]
	TIME [epoch: 17.3 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012294214790607676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012294214790607676 | validation: 0.030678837480207236]
	TIME [epoch: 17.4 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012832292890380587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012832292890380587 | validation: 0.0502897425539143]
	TIME [epoch: 17.4 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020162318500294287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020162318500294287 | validation: 0.041962473948979534]
	TIME [epoch: 17.3 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026709216410500225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026709216410500225 | validation: 0.04353637867245397]
	TIME [epoch: 17.4 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02748813871824078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02748813871824078 | validation: 0.038271461296618826]
	TIME [epoch: 17.4 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024776840694239254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024776840694239254 | validation: 0.03874979606239827]
	TIME [epoch: 17.4 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017419084577679638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017419084577679638 | validation: 0.05052016189407214]
	TIME [epoch: 17.4 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03057066564607823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03057066564607823 | validation: 0.10349582771839194]
	TIME [epoch: 17.4 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05238528851755392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05238528851755392 | validation: 0.0822474584747182]
	TIME [epoch: 17.3 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035870376041941134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035870376041941134 | validation: 0.04450313354443347]
	TIME [epoch: 17.4 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019645916747863933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019645916747863933 | validation: 0.06660381852348017]
	TIME [epoch: 17.4 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015963558215008472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015963558215008472 | validation: 0.03145707877572747]
	TIME [epoch: 17.4 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01089334685633783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01089334685633783 | validation: 0.03242722730446902]
	TIME [epoch: 17.4 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009253921421571703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009253921421571703 | validation: 0.03469848320567187]
	TIME [epoch: 17.4 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011524753115210715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011524753115210715 | validation: 0.0482672137146018]
	TIME [epoch: 17.4 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033291182221819815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033291182221819815 | validation: 0.08558251308718857]
	TIME [epoch: 17.4 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05717249175973305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05717249175973305 | validation: 0.05564691175021572]
	TIME [epoch: 17.4 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03498601533849301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03498601533849301 | validation: 0.05857432123533284]
	TIME [epoch: 17.4 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035383483708828586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035383483708828586 | validation: 0.04661283946656807]
	TIME [epoch: 17.4 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03439469119885381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03439469119885381 | validation: 0.061379432219879154]
	TIME [epoch: 17.4 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042341755379067425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042341755379067425 | validation: 0.09192519961384471]
	TIME [epoch: 17.4 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043841127038877284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043841127038877284 | validation: 0.039841649092538324]
	TIME [epoch: 17.4 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021628845944614474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021628845944614474 | validation: 0.04465234060278798]
	TIME [epoch: 17.4 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02788995841395133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02788995841395133 | validation: 0.04791316713551566]
	TIME [epoch: 17.4 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021149363749808386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021149363749808386 | validation: 0.032840907270503664]
	TIME [epoch: 17.4 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013602911992945381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013602911992945381 | validation: 0.038785599652552544]
	TIME [epoch: 17.4 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010743403567919505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010743403567919505 | validation: 0.027724487115140264]
	TIME [epoch: 17.4 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011063129632388572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011063129632388572 | validation: 0.06844923935523796]
	TIME [epoch: 17.4 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016248370659265682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016248370659265682 | validation: 0.06831032784383252]
	TIME [epoch: 17.3 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027868076427184474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027868076427184474 | validation: 0.04953177471332765]
	TIME [epoch: 17.4 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025044236071277357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025044236071277357 | validation: 0.04500225851509187]
	TIME [epoch: 17.4 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025901491426463762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025901491426463762 | validation: 0.05194550812188782]
	TIME [epoch: 17.4 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03021288578700996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03021288578700996 | validation: 0.05312485738512297]
	TIME [epoch: 17.4 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03959310250452899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03959310250452899 | validation: 0.03763310117628135]
	TIME [epoch: 17.3 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024794606454655616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024794606454655616 | validation: 0.02702513477711878]
	TIME [epoch: 17.3 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013155895498384805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013155895498384805 | validation: 0.025660753716140107]
	TIME [epoch: 17.4 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009146105996299727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009146105996299727 | validation: 0.030593419286484638]
	TIME [epoch: 17.4 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019790009726112685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019790009726112685 | validation: 0.06517610167874326]
	TIME [epoch: 17.3 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039192070149017634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039192070149017634 | validation: 0.06926185367653032]
	TIME [epoch: 17.4 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05735796100114946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05735796100114946 | validation: 0.07550019093061082]
	TIME [epoch: 17.4 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03905487154602715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03905487154602715 | validation: 0.3284662847792392]
	TIME [epoch: 17.4 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08076133325173782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08076133325173782 | validation: 0.05566383897822202]
	TIME [epoch: 17.4 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01990574396489519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01990574396489519 | validation: 0.09582236281246279]
	TIME [epoch: 17.4 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05085905623789138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05085905623789138 | validation: 0.09108293332352771]
	TIME [epoch: 17.4 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06182672851792065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06182672851792065 | validation: 0.0680944869235871]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144432/states/model_phi1_4b_v_mmd1_1640.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 16970.593 seconds.
