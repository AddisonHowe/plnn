Args:
Namespace(name='model_phi1_4b_v_mmd1', outdir='out/model_training/model_phi1_4b_v_mmd1', training_data='data/training_data/data_phi1_4b/training', validation_data='data/training_data/data_phi1_4b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1100387485

Training model...

Saving initial model state to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.934892410024531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.934892410024531 | validation: 4.9359626513436945]
	TIME [epoch: 46.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.053313767625799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.053313767625799 | validation: 5.6815557307391265]
	TIME [epoch: 1.84 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.792666067553908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.792666067553908 | validation: 5.097329560223212]
	TIME [epoch: 1.82 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.128343765962763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.128343765962763 | validation: 4.994326610724473]
	TIME [epoch: 1.82 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.912400480326788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.912400480326788 | validation: 4.538558875473355]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.58199329894287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.58199329894287 | validation: 4.371850464225616]
	TIME [epoch: 1.81 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.380252902993616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.380252902993616 | validation: 4.4791170616540485]
	TIME [epoch: 1.82 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.205891704590401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.205891704590401 | validation: 4.1170874196949825]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.194452695467687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.194452695467687 | validation: 4.160516936956873]
	TIME [epoch: 1.83 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.051459309591369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.051459309591369 | validation: 4.3595025415672835]
	TIME [epoch: 1.82 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0659472188329895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0659472188329895 | validation: 4.031481374851566]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.075936200337505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.075936200337505 | validation: 4.120241781685387]
	TIME [epoch: 1.83 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.977466228352349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.977466228352349 | validation: 4.167902008201419]
	TIME [epoch: 1.83 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.979185650529023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.979185650529023 | validation: 4.046248921436356]
	TIME [epoch: 1.83 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9326028606891117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9326028606891117 | validation: 3.9776183739900888]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8861334256170985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8861334256170985 | validation: 4.156520865115355]
	TIME [epoch: 1.83 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9517741245796847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9517741245796847 | validation: 4.011474823920705]
	TIME [epoch: 1.84 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.993639319808645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.993639319808645 | validation: 4.029207935777579]
	TIME [epoch: 1.83 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.909816040743934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.909816040743934 | validation: 3.9165009166849787]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7685014412897857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7685014412897857 | validation: 3.8794434180507382]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.761176150003213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.761176150003213 | validation: 3.920015193049176]
	TIME [epoch: 1.83 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7973665923511692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7973665923511692 | validation: 3.8857287301634855]
	TIME [epoch: 1.83 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7994356433093164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7994356433093164 | validation: 3.9720699255168377]
	TIME [epoch: 1.83 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.845359542313997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.845359542313997 | validation: 3.8300095537622454]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7341602565274163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7341602565274163 | validation: 3.768245248842614]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.661822671430775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.661822671430775 | validation: 3.7484396476589676]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6505965229003468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6505965229003468 | validation: 3.7227093640527147]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.630767810973947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.630767810973947 | validation: 3.701076549749358]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6111631745217694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6111631745217694 | validation: 3.719128348560216]
	TIME [epoch: 1.83 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6115619046578544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6115619046578544 | validation: 3.7723656040487175]
	TIME [epoch: 1.82 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.702665549335368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.702665549335368 | validation: 4.051157034630762]
	TIME [epoch: 1.82 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9190275499912834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9190275499912834 | validation: 3.733702661396407]
	TIME [epoch: 1.82 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6337896348604706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6337896348604706 | validation: 3.6800748132951835]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5933133871029117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5933133871029117 | validation: 3.701433071107853]
	TIME [epoch: 1.82 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5718817107874963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5718817107874963 | validation: 3.5188116267892404]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.429939889253526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.429939889253526 | validation: 3.4639573259382974]
	TIME [epoch: 1.81 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.275658917065188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.275658917065188 | validation: 3.1646137306903146]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.117051853234051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.117051853234051 | validation: 2.882457912242361]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7907669456124746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7907669456124746 | validation: 2.358976299251149]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2573081678163316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2573081678163316 | validation: 2.5434653839694943]
	TIME [epoch: 1.84 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5275614802637056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5275614802637056 | validation: 3.3089505849474308]
	TIME [epoch: 1.83 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1310814825265516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1310814825265516 | validation: 1.5728867975410263]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3181598879237786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3181598879237786 | validation: 1.7069097943869758]
	TIME [epoch: 1.83 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6179666189909323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6179666189909323 | validation: 1.3010676911833543]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1614774627994522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1614774627994522 | validation: 0.9524884495736189]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8923079288994193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8923079288994193 | validation: 1.0285351564400942]
	TIME [epoch: 1.82 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.924423776112838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.924423776112838 | validation: 1.265582425824433]
	TIME [epoch: 1.82 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2400113345544082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2400113345544082 | validation: 0.9485292553894372]
	TIME [epoch: 1.81 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8163636164839152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8163636164839152 | validation: 1.1366568621623785]
	TIME [epoch: 1.83 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9781565275561153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9781565275561153 | validation: 1.218810675351351]
	TIME [epoch: 1.82 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.177485371367396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.177485371367396 | validation: 0.9749029228427069]
	TIME [epoch: 1.82 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.842051922634505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.842051922634505 | validation: 1.2450294007993223]
	TIME [epoch: 1.82 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.036170853965311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.036170853965311 | validation: 1.0076269634164683]
	TIME [epoch: 1.82 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9658863268879084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9658863268879084 | validation: 0.8997718390025997]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.833163553499915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.833163553499915 | validation: 0.9884665395468307]
	TIME [epoch: 1.87 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.843925979068433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.843925979068433 | validation: 0.842008176665215]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7898656677422184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7898656677422184 | validation: 0.8430654419067226]
	TIME [epoch: 1.83 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7846288175044205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7846288175044205 | validation: 0.8599138479970526]
	TIME [epoch: 1.83 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7803737505034617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7803737505034617 | validation: 0.8733859251243707]
	TIME [epoch: 1.85 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7850932781159925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7850932781159925 | validation: 0.8469553256367761]
	TIME [epoch: 1.83 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7742277898805112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7742277898805112 | validation: 0.8505161188217132]
	TIME [epoch: 1.85 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.771891814261395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.771891814261395 | validation: 0.8762086442771942]
	TIME [epoch: 1.83 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7695380601369928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7695380601369928 | validation: 0.8466721509960792]
	TIME [epoch: 1.83 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7669895585922427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7669895585922427 | validation: 0.8461880061328169]
	TIME [epoch: 1.82 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7645257593572774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7645257593572774 | validation: 0.8644286640949718]
	TIME [epoch: 1.82 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7667675304162727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7667675304162727 | validation: 0.8575573989213896]
	TIME [epoch: 1.83 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7691322726122023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7691322726122023 | validation: 0.9118276541352923]
	TIME [epoch: 1.83 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8133034395978925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8133034395978925 | validation: 1.0105412940741443]
	TIME [epoch: 1.82 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9265872206028751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9265872206028751 | validation: 1.1710702948993381]
	TIME [epoch: 1.83 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0145748403415917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0145748403415917 | validation: 0.8527083809840962]
	TIME [epoch: 1.83 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7995878906830401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7995878906830401 | validation: 0.9542860408336147]
	TIME [epoch: 1.85 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7799496831141046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7799496831141046 | validation: 0.8585935509013999]
	TIME [epoch: 1.83 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7957403996306139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7957403996306139 | validation: 0.9772029515093504]
	TIME [epoch: 1.83 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7950837422943022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7950837422943022 | validation: 0.8886067702944687]
	TIME [epoch: 1.84 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8262527380761486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8262527380761486 | validation: 1.0901992795679607]
	TIME [epoch: 1.83 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8430876299027201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8430876299027201 | validation: 1.0292642775421894]
	TIME [epoch: 1.83 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9948962416287218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9948962416287218 | validation: 1.1972862843622356]
	TIME [epoch: 1.84 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9591042923773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9591042923773 | validation: 0.9140679002292371]
	TIME [epoch: 1.84 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7668920456883409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7668920456883409 | validation: 0.8964839158782966]
	TIME [epoch: 1.83 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8510673694092246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8510673694092246 | validation: 0.9880272770883156]
	TIME [epoch: 1.83 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8107770993976529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8107770993976529 | validation: 0.8705275934803665]
	TIME [epoch: 1.82 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7573668393927804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7573668393927804 | validation: 0.8106049139745383]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7719422943864973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7719422943864973 | validation: 0.8868949159956934]
	TIME [epoch: 1.82 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7683548577751276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7683548577751276 | validation: 0.8896453832981746]
	TIME [epoch: 1.82 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7841084347000199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7841084347000199 | validation: 0.9456467621830655]
	TIME [epoch: 1.82 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8510267767620117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8510267767620117 | validation: 1.0517555994348868]
	TIME [epoch: 1.82 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8672983087724407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8672983087724407 | validation: 0.9439385947752896]
	TIME [epoch: 1.82 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8692402998846729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8692402998846729 | validation: 0.9161011604952467]
	TIME [epoch: 1.82 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7581173970741035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7581173970741035 | validation: 0.8514970513791177]
	TIME [epoch: 1.82 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.747004915761238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.747004915761238 | validation: 0.8716474481904417]
	TIME [epoch: 1.82 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.752326929324982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.752326929324982 | validation: 0.8225753452076621]
	TIME [epoch: 1.82 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7649298162042407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7649298162042407 | validation: 1.0075654692104141]
	TIME [epoch: 1.82 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8180499624256214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8180499624256214 | validation: 0.8686294244551491]
	TIME [epoch: 1.82 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.843474638684075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.843474638684075 | validation: 1.1047216681349072]
	TIME [epoch: 1.82 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8661794976799189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8661794976799189 | validation: 0.841800353598665]
	TIME [epoch: 1.82 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7432793233157307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7432793233157307 | validation: 0.8433802189456614]
	TIME [epoch: 1.82 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7737002361721446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7737002361721446 | validation: 0.9107035742515042]
	TIME [epoch: 1.81 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8252643817774055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8252643817774055 | validation: 1.2289893219314096]
	TIME [epoch: 1.82 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9419279222861274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9419279222861274 | validation: 1.0423193964526103]
	TIME [epoch: 1.82 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9856206677768882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9856206677768882 | validation: 1.1057438187672297]
	TIME [epoch: 1.82 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8353278384540321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8353278384540321 | validation: 0.9350286835792881]
	TIME [epoch: 1.82 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7928176261518621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7928176261518621 | validation: 0.8709564706628596]
	TIME [epoch: 1.82 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8165885070832551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8165885070832551 | validation: 0.9904391974993116]
	TIME [epoch: 1.82 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7859327523299598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7859327523299598 | validation: 0.8275494866256761]
	TIME [epoch: 1.83 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7630343345639439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7630343345639439 | validation: 0.9240211719555369]
	TIME [epoch: 1.82 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7672882965276259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7672882965276259 | validation: 0.8397916323560056]
	TIME [epoch: 1.82 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.748823246032918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.748823246032918 | validation: 0.8795431174645612]
	TIME [epoch: 1.83 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7516565122028681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7516565122028681 | validation: 0.8206595275521336]
	TIME [epoch: 1.82 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7511377897917786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7511377897917786 | validation: 0.951546184731464]
	TIME [epoch: 1.82 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7749727130969322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7749727130969322 | validation: 0.8346664088563422]
	TIME [epoch: 1.82 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7782381290361987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7782381290361987 | validation: 1.0624775423354043]
	TIME [epoch: 1.83 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8202879054370422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8202879054370422 | validation: 0.8437236491640928]
	TIME [epoch: 1.83 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7659827089820703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7659827089820703 | validation: 0.9325052272722655]
	TIME [epoch: 1.82 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7425609341919597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7425609341919597 | validation: 0.8321802881948138]
	TIME [epoch: 1.82 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.76102672321075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.76102672321075 | validation: 1.184689799858966]
	TIME [epoch: 1.82 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8916965307219116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8916965307219116 | validation: 1.0956992183956356]
	TIME [epoch: 1.82 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.066313484126148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.066313484126148 | validation: 0.9027264971443358]
	TIME [epoch: 1.82 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7394019057759041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7394019057759041 | validation: 1.0389712262430586]
	TIME [epoch: 1.82 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.81272306017842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.81272306017842 | validation: 0.9428075262432447]
	TIME [epoch: 1.82 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8207853933868782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8207853933868782 | validation: 0.8233507924581032]
	TIME [epoch: 1.82 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7766878761060667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7766878761060667 | validation: 1.1474393301735535]
	TIME [epoch: 1.82 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.86144913017184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.86144913017184 | validation: 0.8630441368277149]
	TIME [epoch: 1.82 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8194265674446365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8194265674446365 | validation: 1.0301765840267283]
	TIME [epoch: 1.82 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7869155892072923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7869155892072923 | validation: 0.8350893817542858]
	TIME [epoch: 1.83 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7337679612158339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7337679612158339 | validation: 0.8265910247985611]
	TIME [epoch: 1.84 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.730746074074515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.730746074074515 | validation: 0.9201288337018184]
	TIME [epoch: 1.82 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7486758060458141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7486758060458141 | validation: 0.814953852775933]
	TIME [epoch: 1.83 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7414252390912968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7414252390912968 | validation: 0.9676536123942008]
	TIME [epoch: 1.82 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7517739690549319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7517739690549319 | validation: 0.812165555035479]
	TIME [epoch: 1.82 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.754462884000536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.754462884000536 | validation: 1.1218922607427604]
	TIME [epoch: 1.82 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8114297973795067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8114297973795067 | validation: 0.8843261817886514]
	TIME [epoch: 1.81 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8813694329621841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8813694329621841 | validation: 1.186133878183881]
	TIME [epoch: 1.82 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8575350706777067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8575350706777067 | validation: 0.8290636124036915]
	TIME [epoch: 1.82 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7486200197612256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7486200197612256 | validation: 0.8782505669618863]
	TIME [epoch: 1.82 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7331532077616477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7331532077616477 | validation: 0.880228414695883]
	TIME [epoch: 1.82 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7647773787919182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7647773787919182 | validation: 0.9900056609637968]
	TIME [epoch: 1.82 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.893140089419299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.893140089419299 | validation: 0.9398251062345943]
	TIME [epoch: 1.82 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8850722503969358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8850722503969358 | validation: 1.002983918741273]
	TIME [epoch: 1.81 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8045172286138955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8045172286138955 | validation: 0.8324240084404928]
	TIME [epoch: 1.82 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7239546141009521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7239546141009521 | validation: 0.8069188955670594]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7318256738208047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7318256738208047 | validation: 0.926269438937643]
	TIME [epoch: 1.81 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.753658138353967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.753658138353967 | validation: 0.7925235462626934]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7451992355328273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7451992355328273 | validation: 0.857808726400267]
	TIME [epoch: 1.82 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7334578616220153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7334578616220153 | validation: 0.9002920688439746]
	TIME [epoch: 1.82 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7530722784159533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7530722784159533 | validation: 0.8861108518152719]
	TIME [epoch: 1.82 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8330969902964128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8330969902964128 | validation: 1.2194745769178859]
	TIME [epoch: 1.81 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8594489434578207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8594489434578207 | validation: 0.8484303201799801]
	TIME [epoch: 1.82 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7975977340383336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7975977340383336 | validation: 0.9588266210622298]
	TIME [epoch: 1.82 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7464781192219762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7464781192219762 | validation: 0.8969893169857515]
	TIME [epoch: 1.83 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7925484326516152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7925484326516152 | validation: 1.024182071666687]
	TIME [epoch: 1.83 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8789395120385762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8789395120385762 | validation: 0.8135536644334374]
	TIME [epoch: 1.82 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7693407484267479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7693407484267479 | validation: 0.9681300901466381]
	TIME [epoch: 1.82 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7484991806509299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7484991806509299 | validation: 0.8087452756369058]
	TIME [epoch: 1.82 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7330504669681512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7330504669681512 | validation: 0.9115750667137518]
	TIME [epoch: 1.81 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7264765905076351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7264765905076351 | validation: 0.797119949797049]
	TIME [epoch: 1.83 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7212347941828867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7212347941828867 | validation: 0.8793703667856981]
	TIME [epoch: 1.82 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.715313618489315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.715313618489315 | validation: 0.8127631394335136]
	TIME [epoch: 1.82 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7296832123894808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7296832123894808 | validation: 0.9589067878577517]
	TIME [epoch: 1.83 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7541124675290288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7541124675290288 | validation: 0.8594755080301312]
	TIME [epoch: 1.83 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8277348757734478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8277348757734478 | validation: 1.107366972621526]
	TIME [epoch: 1.83 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7935052466629398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7935052466629398 | validation: 0.8254601679057206]
	TIME [epoch: 1.82 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.823555587046229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.823555587046229 | validation: 1.1611761223495474]
	TIME [epoch: 1.83 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9109914178103569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9109914178103569 | validation: 0.9025408260538472]
	TIME [epoch: 1.82 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7684780284095851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7684780284095851 | validation: 0.8368887227600711]
	TIME [epoch: 1.82 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7986543253507276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7986543253507276 | validation: 0.9326256795216379]
	TIME [epoch: 1.83 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.73291950209584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.73291950209584 | validation: 0.8203098587841625]
	TIME [epoch: 1.82 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7072177288183451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7072177288183451 | validation: 0.824195518604597]
	TIME [epoch: 1.83 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7262429979258137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7262429979258137 | validation: 0.8757125623365124]
	TIME [epoch: 1.82 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7350383650529942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7350383650529942 | validation: 0.9453568108646458]
	TIME [epoch: 1.82 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7918038282864917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7918038282864917 | validation: 0.8738041209603964]
	TIME [epoch: 1.82 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8645529263709233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8645529263709233 | validation: 1.0501154998788398]
	TIME [epoch: 1.82 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8442740550716544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8442740550716544 | validation: 0.8774627352804478]
	TIME [epoch: 1.83 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7116682748195345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7116682748195345 | validation: 0.7729347856556926]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7453870781978299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7453870781978299 | validation: 0.983029662481873]
	TIME [epoch: 1.83 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7558096591677474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7558096591677474 | validation: 0.7778867396994824]
	TIME [epoch: 1.82 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7190515439539257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7190515439539257 | validation: 0.880163591287952]
	TIME [epoch: 1.82 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7303389375477073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7303389375477073 | validation: 0.8901511976036413]
	TIME [epoch: 1.82 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7829007715857549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7829007715857549 | validation: 0.9675636152699472]
	TIME [epoch: 1.83 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8023768543473889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8023768543473889 | validation: 0.9293335129027096]
	TIME [epoch: 1.82 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7828973935466685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7828973935466685 | validation: 0.799205818608648]
	TIME [epoch: 1.82 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.73318867651707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.73318867651707 | validation: 0.8861653828388296]
	TIME [epoch: 1.82 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7055429625077386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7055429625077386 | validation: 0.7796101078306097]
	TIME [epoch: 1.82 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7019626535370739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7019626535370739 | validation: 0.9375740006918989]
	TIME [epoch: 1.82 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7108384639946435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7108384639946435 | validation: 0.7691780044460305]
	TIME [epoch: 1.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_184.pth
	Model improved!!!
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.723247037575116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.723247037575116 | validation: 1.0439357902742241]
	TIME [epoch: 1.83 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7780856156744029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7780856156744029 | validation: 0.7956193611088133]
	TIME [epoch: 1.83 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8006721714898101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8006721714898101 | validation: 1.0626866308083984]
	TIME [epoch: 1.83 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7786668352467256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7786668352467256 | validation: 0.800484817355033]
	TIME [epoch: 1.82 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7278501497272858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7278501497272858 | validation: 0.8394015381392127]
	TIME [epoch: 1.83 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7404080065979881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7404080065979881 | validation: 0.9840525650925689]
	TIME [epoch: 1.83 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8364831425026601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8364831425026601 | validation: 0.8817370159845068]
	TIME [epoch: 1.83 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7865082181062153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7865082181062153 | validation: 0.8592200903667394]
	TIME [epoch: 1.83 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7445660694848807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7445660694848807 | validation: 0.823109985000396]
	TIME [epoch: 1.83 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6984983589153805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6984983589153805 | validation: 0.8180325614821794]
	TIME [epoch: 1.84 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6847237499137182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6847237499137182 | validation: 0.8054607528407853]
	TIME [epoch: 1.84 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6794173975088986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6794173975088986 | validation: 0.8494695241740994]
	TIME [epoch: 1.84 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6790679819245787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6790679819245787 | validation: 0.7501341283458043]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_197.pth
	Model improved!!!
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6826444074548949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6826444074548949 | validation: 0.9854392678109095]
	TIME [epoch: 1.83 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7655849496223899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7655849496223899 | validation: 0.8198555987568241]
	TIME [epoch: 1.82 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8962091020851495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8962091020851495 | validation: 0.9941423884531844]
	TIME [epoch: 1.83 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8547636540798133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8547636540798133 | validation: 1.1209649463822904]
	TIME [epoch: 43.4 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8183051850636178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8183051850636178 | validation: 0.8443069177166582]
	TIME [epoch: 3.62 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8524112915470243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8524112915470243 | validation: 0.8596696865424062]
	TIME [epoch: 3.59 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6825027464866525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6825027464866525 | validation: 0.9502941728396055]
	TIME [epoch: 3.59 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.721076857804124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.721076857804124 | validation: 0.7768898740493619]
	TIME [epoch: 3.59 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7301393711793697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7301393711793697 | validation: 0.8282571084826881]
	TIME [epoch: 3.59 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6738081722517258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6738081722517258 | validation: 0.8340048381348073]
	TIME [epoch: 3.59 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6628565072651935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6628565072651935 | validation: 0.7345208204870367]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6678754120891901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6678754120891901 | validation: 0.8703268599402008]
	TIME [epoch: 3.59 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6819213487696928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6819213487696928 | validation: 0.7324032610496329]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_210.pth
	Model improved!!!
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6952352010737056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6952352010737056 | validation: 0.9285661587391352]
	TIME [epoch: 3.6 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7496214255142882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7496214255142882 | validation: 0.9022121021952384]
	TIME [epoch: 3.6 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8420087604162759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8420087604162759 | validation: 1.0061510119110644]
	TIME [epoch: 3.61 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9218848855726895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9218848855726895 | validation: 0.8905560879762286]
	TIME [epoch: 3.6 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.677914106127629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.677914106127629 | validation: 0.8153654450867251]
	TIME [epoch: 3.59 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6649809189019789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6649809189019789 | validation: 0.891003720358678]
	TIME [epoch: 3.6 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6916454953038185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6916454953038185 | validation: 0.7815516247893023]
	TIME [epoch: 3.6 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.691881599909565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.691881599909565 | validation: 0.8687413242088925]
	TIME [epoch: 3.6 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7011217704569522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7011217704569522 | validation: 0.7562448388046838]
	TIME [epoch: 3.59 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6828749368382129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6828749368382129 | validation: 0.8324508521127783]
	TIME [epoch: 3.6 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7020718371372486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7020718371372486 | validation: 0.9590240039448699]
	TIME [epoch: 3.59 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7218157962151779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7218157962151779 | validation: 0.8181666605311015]
	TIME [epoch: 3.6 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7299261991454491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7299261991454491 | validation: 0.8662302412778495]
	TIME [epoch: 3.59 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6528770928253307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6528770928253307 | validation: 0.7830617537221247]
	TIME [epoch: 3.6 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6275790151597405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6275790151597405 | validation: 0.7649946312537644]
	TIME [epoch: 3.61 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6124323057393695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6124323057393695 | validation: 0.8119764142851917]
	TIME [epoch: 3.61 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6195737601946628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6195737601946628 | validation: 0.6900192078931051]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_227.pth
	Model improved!!!
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6685902838887228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6685902838887228 | validation: 1.1164172483646222]
	TIME [epoch: 3.62 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.926010729225313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.926010729225313 | validation: 0.6659661372199264]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_229.pth
	Model improved!!!
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6621354404223591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6621354404223591 | validation: 0.7232645126428043]
	TIME [epoch: 3.59 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6029260490794812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6029260490794812 | validation: 0.806515282909575]
	TIME [epoch: 3.59 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6046142338347188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6046142338347188 | validation: 0.7209021545696721]
	TIME [epoch: 3.6 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6370945950840159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6370945950840159 | validation: 1.065822595161989]
	TIME [epoch: 3.59 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7512679346951273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7512679346951273 | validation: 0.9109402314803813]
	TIME [epoch: 3.6 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8114398761928943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8114398761928943 | validation: 0.7851516896278268]
	TIME [epoch: 3.59 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7004076900919904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7004076900919904 | validation: 0.956383863915987]
	TIME [epoch: 3.6 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.688943912988029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.688943912988029 | validation: 0.7234576929598048]
	TIME [epoch: 3.59 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5957661730878682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5957661730878682 | validation: 0.7670406504583913]
	TIME [epoch: 3.6 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5687404176674403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5687404176674403 | validation: 0.7352938233204463]
	TIME [epoch: 3.61 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.552557938306326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.552557938306326 | validation: 0.6899070494982831]
	TIME [epoch: 3.59 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5431440436775716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5431440436775716 | validation: 0.6664962729022137]
	TIME [epoch: 3.6 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5381890023041931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5381890023041931 | validation: 0.6868142790304833]
	TIME [epoch: 3.59 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5428531743060644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5428531743060644 | validation: 0.8265487802057012]
	TIME [epoch: 3.59 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7003766346844682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7003766346844682 | validation: 1.0774370502460988]
	TIME [epoch: 3.59 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9205426340686192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9205426340686192 | validation: 0.8015803857211747]
	TIME [epoch: 3.59 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.569500191320204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.569500191320204 | validation: 0.7479919452150253]
	TIME [epoch: 3.59 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5462703696022269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5462703696022269 | validation: 0.8640407590925911]
	TIME [epoch: 3.6 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5752888166269267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5752888166269267 | validation: 0.6311444809735334]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5819104476211939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5819104476211939 | validation: 0.7718510365084925]
	TIME [epoch: 3.6 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5685450708940346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5685450708940346 | validation: 0.5850377490677163]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_250.pth
	Model improved!!!
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5418702012396719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5418702012396719 | validation: 0.706206669871081]
	TIME [epoch: 3.6 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4957930886354258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4957930886354258 | validation: 0.6014996487863502]
	TIME [epoch: 3.6 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47237689324645077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47237689324645077 | validation: 0.6616479759727357]
	TIME [epoch: 3.6 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46770193873196364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46770193873196364 | validation: 0.6057563140942025]
	TIME [epoch: 3.6 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5371310946322163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5371310946322163 | validation: 1.333099092700594]
	TIME [epoch: 3.6 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8131803888835941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8131803888835941 | validation: 0.6627397108497857]
	TIME [epoch: 3.6 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6052726740089296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6052726740089296 | validation: 0.652132465231809]
	TIME [epoch: 3.59 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45629499211398467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45629499211398467 | validation: 0.756213770744909]
	TIME [epoch: 3.6 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5073823137590506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5073823137590506 | validation: 0.6670770769518948]
	TIME [epoch: 3.6 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.509117648580621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.509117648580621 | validation: 0.6249269987776698]
	TIME [epoch: 3.61 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.629144476211063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.629144476211063 | validation: 0.7893011584750098]
	TIME [epoch: 3.6 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5192518542922392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5192518542922392 | validation: 0.6321842790967658]
	TIME [epoch: 3.6 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42116399173941815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42116399173941815 | validation: 0.5669253612553377]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_263.pth
	Model improved!!!
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4099386500307198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4099386500307198 | validation: 0.5254530902235288]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_264.pth
	Model improved!!!
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39947435724438907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39947435724438907 | validation: 0.646882467816319]
	TIME [epoch: 3.62 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41157503931955625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41157503931955625 | validation: 0.49369583060257016]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_266.pth
	Model improved!!!
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5226107127556384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5226107127556384 | validation: 0.7325930865999809]
	TIME [epoch: 3.6 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48302863337843305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48302863337843305 | validation: 0.5762791875020767]
	TIME [epoch: 3.6 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47348816934861776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47348816934861776 | validation: 0.6743015076779909]
	TIME [epoch: 3.59 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.544138371016998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.544138371016998 | validation: 0.9808374461958634]
	TIME [epoch: 3.59 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6147686333068674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6147686333068674 | validation: 0.6205084061795006]
	TIME [epoch: 3.6 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45866308584661397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45866308584661397 | validation: 0.695525941394199]
	TIME [epoch: 3.59 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45690305809267256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45690305809267256 | validation: 0.5030806311978095]
	TIME [epoch: 3.59 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45010691974146244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45010691974146244 | validation: 0.6716606060645991]
	TIME [epoch: 3.59 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5041007823456265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5041007823456265 | validation: 0.6241433914017129]
	TIME [epoch: 3.6 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39760869495429263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39760869495429263 | validation: 0.5455329606446723]
	TIME [epoch: 3.61 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3859583727917354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3859583727917354 | validation: 0.6298713414175707]
	TIME [epoch: 3.59 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3871433050644673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3871433050644673 | validation: 0.46710802404007945]
	TIME [epoch: 3.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_278.pth
	Model improved!!!
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4746994603340657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4746994603340657 | validation: 0.9458468933799491]
	TIME [epoch: 3.61 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5508196195810943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5508196195810943 | validation: 0.5418555507514246]
	TIME [epoch: 3.61 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43936857407831226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43936857407831226 | validation: 0.5435074663790866]
	TIME [epoch: 3.62 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34922495264845865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34922495264845865 | validation: 0.5163339353431127]
	TIME [epoch: 3.62 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3618804942763434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3618804942763434 | validation: 0.49912227850495683]
	TIME [epoch: 3.62 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37344003062765657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37344003062765657 | validation: 0.6533116388879704]
	TIME [epoch: 3.64 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4704469181314695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4704469181314695 | validation: 0.6102748057786117]
	TIME [epoch: 3.62 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.381300172024342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.381300172024342 | validation: 0.4728652642501137]
	TIME [epoch: 3.63 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3552215497937218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3552215497937218 | validation: 0.6825702815241547]
	TIME [epoch: 3.63 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3684091075480688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3684091075480688 | validation: 0.4810387246559351]
	TIME [epoch: 3.63 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3409099482443499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3409099482443499 | validation: 0.5669679488378169]
	TIME [epoch: 3.63 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34146257503755023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34146257503755023 | validation: 0.4895662204417989]
	TIME [epoch: 3.62 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3297352093055079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3297352093055079 | validation: 0.4560449275726038]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4101074045929859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4101074045929859 | validation: 0.6983211070537201]
	TIME [epoch: 3.59 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42810991393831616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42810991393831616 | validation: 0.39898891360019106]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_293.pth
	Model improved!!!
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38953935075143314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38953935075143314 | validation: 0.8636567000561306]
	TIME [epoch: 3.59 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5112853509026216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5112853509026216 | validation: 0.4587906055386512]
	TIME [epoch: 3.6 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3308695215379527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3308695215379527 | validation: 0.49634003165301765]
	TIME [epoch: 3.59 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29390475327845456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29390475327845456 | validation: 0.4748489330197761]
	TIME [epoch: 3.6 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3279101024924526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3279101024924526 | validation: 0.5161061741975262]
	TIME [epoch: 3.59 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31702119061509404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31702119061509404 | validation: 0.45952404006508074]
	TIME [epoch: 3.61 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27546134972277314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27546134972277314 | validation: 0.535626462716784]
	TIME [epoch: 3.62 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28123443613992516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28123443613992516 | validation: 0.4200410288534851]
	TIME [epoch: 3.63 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3087044313367994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3087044313367994 | validation: 0.7556684751499682]
	TIME [epoch: 3.62 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4093421897654584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4093421897654584 | validation: 0.516868247211934]
	TIME [epoch: 3.61 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28648243110956834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28648243110956834 | validation: 0.36630564553545397]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_304.pth
	Model improved!!!
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46270541826437883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46270541826437883 | validation: 0.855078017448268]
	TIME [epoch: 3.62 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6373104143468004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6373104143468004 | validation: 0.567170825183066]
	TIME [epoch: 3.62 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3538243495464311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3538243495464311 | validation: 0.381470020340926]
	TIME [epoch: 3.61 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49381731449600724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49381731449600724 | validation: 0.4740324557067183]
	TIME [epoch: 3.62 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24254254031486114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24254254031486114 | validation: 0.458271519826817]
	TIME [epoch: 3.61 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3036684161704135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3036684161704135 | validation: 0.4922872398619599]
	TIME [epoch: 3.62 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2721260288086035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2721260288086035 | validation: 0.40078101109103437]
	TIME [epoch: 3.62 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2614898743108514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2614898743108514 | validation: 0.6205322842237795]
	TIME [epoch: 3.62 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3026841262550059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3026841262550059 | validation: 0.36868286336916833]
	TIME [epoch: 3.62 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3418295757470218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3418295757470218 | validation: 0.6379089747236533]
	TIME [epoch: 3.62 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.306755307577558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.306755307577558 | validation: 0.383450618310091]
	TIME [epoch: 3.61 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22244354229181693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22244354229181693 | validation: 0.3866656474566573]
	TIME [epoch: 3.6 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20065470255044326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20065470255044326 | validation: 0.43004426004357177]
	TIME [epoch: 3.61 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19633546732666182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19633546732666182 | validation: 0.3890010204692481]
	TIME [epoch: 3.61 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2022700274354092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2022700274354092 | validation: 0.46499187668419845]
	TIME [epoch: 3.61 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2644685478722687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2644685478722687 | validation: 0.5823692889980832]
	TIME [epoch: 3.6 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5306555447835032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5306555447835032 | validation: 0.7235190379089977]
	TIME [epoch: 3.61 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44982753500872585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44982753500872585 | validation: 0.5362596897789004]
	TIME [epoch: 3.6 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40011273252748586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40011273252748586 | validation: 0.5781958390701284]
	TIME [epoch: 3.6 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3457881461765994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3457881461765994 | validation: 0.36436270099999735]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_324.pth
	Model improved!!!
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2537063870976185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2537063870976185 | validation: 0.5510798943981265]
	TIME [epoch: 3.62 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27692905800929757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27692905800929757 | validation: 0.4165964257774197]
	TIME [epoch: 3.63 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24683372025037642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24683372025037642 | validation: 0.42707833274632095]
	TIME [epoch: 3.62 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21511236183442023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21511236183442023 | validation: 0.2918887818326512]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_328.pth
	Model improved!!!
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32896392796367396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32896392796367396 | validation: 0.9411430125060788]
	TIME [epoch: 3.61 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.641143240849565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.641143240849565 | validation: 0.446417563717323]
	TIME [epoch: 3.61 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31737650182630633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31737650182630633 | validation: 0.35312313065336015]
	TIME [epoch: 3.61 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3589966403532167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3589966403532167 | validation: 0.5136615625394179]
	TIME [epoch: 3.6 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2265349246803745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2265349246803745 | validation: 0.3751449780766989]
	TIME [epoch: 3.62 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23387348332383617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23387348332383617 | validation: 0.44509869918553646]
	TIME [epoch: 3.61 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22070916623962256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22070916623962256 | validation: 0.3560640490097058]
	TIME [epoch: 3.61 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19374798677288552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19374798677288552 | validation: 0.3993574675519205]
	TIME [epoch: 3.61 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17822193772445757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17822193772445757 | validation: 0.3476869434265761]
	TIME [epoch: 3.62 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16597357727937304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16597357727937304 | validation: 0.37282094248572095]
	TIME [epoch: 3.62 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15838364126753895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15838364126753895 | validation: 0.3646797801722208]
	TIME [epoch: 3.61 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1557883852305303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1557883852305303 | validation: 0.30467464802236777]
	TIME [epoch: 3.61 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16317059715894477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16317059715894477 | validation: 0.8023039427710341]
	TIME [epoch: 3.61 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46669170746945815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46669170746945815 | validation: 0.44064748820116206]
	TIME [epoch: 3.61 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6179277789213112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6179277789213112 | validation: 0.4582305053432695]
	TIME [epoch: 3.62 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19397790527659176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19397790527659176 | validation: 0.5716907495159615]
	TIME [epoch: 3.61 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24447769846191963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24447769846191963 | validation: 0.34431674889282576]
	TIME [epoch: 3.62 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23403740962169395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23403740962169395 | validation: 0.41665310034411346]
	TIME [epoch: 3.61 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17184773751692428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17184773751692428 | validation: 0.38820664026410934]
	TIME [epoch: 3.61 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18052638608352758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18052638608352758 | validation: 0.34610728409575436]
	TIME [epoch: 3.61 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18646256508670056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18646256508670056 | validation: 0.5671300446650814]
	TIME [epoch: 3.61 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47050451132873833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47050451132873833 | validation: 0.7048419395391887]
	TIME [epoch: 3.62 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3068319779477152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3068319779477152 | validation: 0.3361775426782405]
	TIME [epoch: 3.63 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.303998389006317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.303998389006317 | validation: 0.4124651115332655]
	TIME [epoch: 3.61 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23865039432906046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23865039432906046 | validation: 0.531665526234557]
	TIME [epoch: 3.61 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23157230214217278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23157230214217278 | validation: 0.3227023225457829]
	TIME [epoch: 3.61 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20078163422818548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20078163422818548 | validation: 0.413943703022715]
	TIME [epoch: 3.61 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15762226454318132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15762226454318132 | validation: 0.40954512608081006]
	TIME [epoch: 3.61 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16025996079944718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16025996079944718 | validation: 0.3062122299726298]
	TIME [epoch: 3.62 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1716782495577957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1716782495577957 | validation: 0.6043900462492605]
	TIME [epoch: 3.61 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23713961198948652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23713961198948652 | validation: 0.2920799497648991]
	TIME [epoch: 3.6 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2093008960402799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2093008960402799 | validation: 0.528169830442243]
	TIME [epoch: 3.61 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18659474080232716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18659474080232716 | validation: 0.33608222424210726]
	TIME [epoch: 3.61 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1418438658135208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1418438658135208 | validation: 0.32052246761632064]
	TIME [epoch: 3.61 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14633159299506385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14633159299506385 | validation: 0.48384734058610884]
	TIME [epoch: 3.62 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26372438831216266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26372438831216266 | validation: 0.3659763610116116]
	TIME [epoch: 3.62 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2636841210588545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2636841210588545 | validation: 0.45817110990607013]
	TIME [epoch: 3.61 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2607064009934999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2607064009934999 | validation: 0.49150749656514225]
	TIME [epoch: 3.6 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16191874775881637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16191874775881637 | validation: 0.30590810803353713]
	TIME [epoch: 3.6 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17848963451220495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17848963451220495 | validation: 0.4809134542638502]
	TIME [epoch: 3.61 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19209770327883596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19209770327883596 | validation: 0.31649592640932744]
	TIME [epoch: 3.61 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1481746230459071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1481746230459071 | validation: 0.5024863107499414]
	TIME [epoch: 3.61 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2529490725005559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2529490725005559 | validation: 0.44712979226133576]
	TIME [epoch: 3.61 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2978117489228154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2978117489228154 | validation: 0.6660348502721292]
	TIME [epoch: 3.6 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2619921235908619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2619921235908619 | validation: 0.3118377163583205]
	TIME [epoch: 3.61 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22018857155218624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22018857155218624 | validation: 0.36053096300241383]
	TIME [epoch: 3.6 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15539055910749375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15539055910749375 | validation: 0.4230485191631399]
	TIME [epoch: 3.61 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1576358550769819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1576358550769819 | validation: 0.32392119367654265]
	TIME [epoch: 3.61 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15277329362963823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15277329362963823 | validation: 0.3945897131633633]
	TIME [epoch: 3.62 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13479995712981524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13479995712981524 | validation: 0.32291421717710916]
	TIME [epoch: 3.61 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13635706839559447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13635706839559447 | validation: 0.3922233655717968]
	TIME [epoch: 3.61 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1436265690298825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1436265690298825 | validation: 0.36972648131956354]
	TIME [epoch: 3.6 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15451757061713045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15451757061713045 | validation: 0.3285054290935754]
	TIME [epoch: 3.61 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14956829424022897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14956829424022897 | validation: 0.5848075426181077]
	TIME [epoch: 3.61 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4336850891511328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4336850891511328 | validation: 0.4080625184138435]
	TIME [epoch: 3.61 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14929166319339104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14929166319339104 | validation: 0.31727848148464805]
	TIME [epoch: 3.61 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24033421380664372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24033421380664372 | validation: 0.5154810953193242]
	TIME [epoch: 3.6 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19473903195225856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19473903195225856 | validation: 0.3471373639064013]
	TIME [epoch: 3.6 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23107273412128274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23107273412128274 | validation: 0.6176694918972034]
	TIME [epoch: 3.6 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23911133985589555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23911133985589555 | validation: 0.3218181793993251]
	TIME [epoch: 3.61 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17839905620077523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17839905620077523 | validation: 0.39080045635369376]
	TIME [epoch: 3.62 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14554516253005265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14554516253005265 | validation: 0.33879718892096844]
	TIME [epoch: 3.61 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12483664885707947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12483664885707947 | validation: 0.36426715047662767]
	TIME [epoch: 3.61 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12793512807139862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12793512807139862 | validation: 0.37319676024148907]
	TIME [epoch: 3.61 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1199759792268241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1199759792268241 | validation: 0.2884079661506826]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_393.pth
	Model improved!!!
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11754645194762273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11754645194762273 | validation: 0.4073327898883811]
	TIME [epoch: 3.61 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11415357979411855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11415357979411855 | validation: 0.27587206081863125]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_395.pth
	Model improved!!!
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12207990279560306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12207990279560306 | validation: 0.515917382359505]
	TIME [epoch: 3.6 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15475508045387784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15475508045387784 | validation: 0.25320500110666433]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_397.pth
	Model improved!!!
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1543639864380444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1543639864380444 | validation: 0.6805225793567753]
	TIME [epoch: 3.61 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25427970250110843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25427970250110843 | validation: 0.29135644318998577]
	TIME [epoch: 3.62 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17942145967659306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17942145967659306 | validation: 0.5104797567497166]
	TIME [epoch: 3.61 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2091044136849858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2091044136849858 | validation: 0.5339619337563336]
	TIME [epoch: 3.63 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.341191021618597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.341191021618597 | validation: 0.35809464558963505]
	TIME [epoch: 3.6 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13211047429287284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13211047429287284 | validation: 0.3638103398821011]
	TIME [epoch: 3.6 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20115320055926889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20115320055926889 | validation: 0.3301138430536929]
	TIME [epoch: 3.62 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1290774998641049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1290774998641049 | validation: 0.35274749578043574]
	TIME [epoch: 3.61 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1414924148975883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1414924148975883 | validation: 0.398507442746571]
	TIME [epoch: 3.61 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12933909748007383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12933909748007383 | validation: 0.29103267053295245]
	TIME [epoch: 3.62 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12859709822464893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12859709822464893 | validation: 0.3659159915545111]
	TIME [epoch: 3.61 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11184601634336971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11184601634336971 | validation: 0.36315865550214577]
	TIME [epoch: 3.62 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11840287365263626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11840287365263626 | validation: 0.29619843732464607]
	TIME [epoch: 3.6 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1457760843743476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1457760843743476 | validation: 0.5197786406929533]
	TIME [epoch: 3.61 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18445903521793228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18445903521793228 | validation: 0.3000488256327958]
	TIME [epoch: 3.62 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13562989876325612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13562989876325612 | validation: 0.3896085590674892]
	TIME [epoch: 3.62 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10865205905644264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10865205905644264 | validation: 0.6275836048328277]
	TIME [epoch: 3.63 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6653625601343601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6653625601343601 | validation: 0.5137860449783455]
	TIME [epoch: 3.61 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22578218048486734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22578218048486734 | validation: 0.6543377478878004]
	TIME [epoch: 3.61 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22267601892275501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22267601892275501 | validation: 0.3352912439648532]
	TIME [epoch: 3.63 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14054436571058734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14054436571058734 | validation: 0.29809263421187016]
	TIME [epoch: 3.62 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13925758922022033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13925758922022033 | validation: 0.3739836077052002]
	TIME [epoch: 3.6 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10951723057826554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10951723057826554 | validation: 0.41074348898281593]
	TIME [epoch: 3.6 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11137837186067788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11137837186067788 | validation: 0.3674114654104291]
	TIME [epoch: 3.62 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10164470548345467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10164470548345467 | validation: 0.312565132553316]
	TIME [epoch: 3.63 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10620499523445304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10620499523445304 | validation: 0.32550284294353976]
	TIME [epoch: 3.62 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10416219176416497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10416219176416497 | validation: 0.3534593806203706]
	TIME [epoch: 3.62 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11784794327711198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11784794327711198 | validation: 0.3674347840499704]
	TIME [epoch: 3.63 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13980299058192067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13980299058192067 | validation: 0.3305856669618743]
	TIME [epoch: 3.63 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15509576633704067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15509576633704067 | validation: 0.3412942830139934]
	TIME [epoch: 3.63 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13345634086412048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13345634086412048 | validation: 0.3141976888834102]
	TIME [epoch: 3.64 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11274753907784964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11274753907784964 | validation: 0.3954061889039704]
	TIME [epoch: 3.63 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14577899493791713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14577899493791713 | validation: 0.27998537851695743]
	TIME [epoch: 3.63 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14266427579561675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14266427579561675 | validation: 0.3433397947808199]
	TIME [epoch: 3.62 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1623597325294763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1623597325294763 | validation: 0.3900682646921919]
	TIME [epoch: 3.62 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10527172774195556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10527172774195556 | validation: 0.25322903795928253]
	TIME [epoch: 3.63 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11812885004156076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11812885004156076 | validation: 0.39509479810839]
	TIME [epoch: 3.62 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11434631432565531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11434631432565531 | validation: 0.2648369877339383]
	TIME [epoch: 3.61 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11663125530547025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11663125530547025 | validation: 0.5064760831614272]
	TIME [epoch: 3.62 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14980693543090268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14980693543090268 | validation: 0.31068990705521166]
	TIME [epoch: 3.61 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1391934526115291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1391934526115291 | validation: 0.34211740108551364]
	TIME [epoch: 3.63 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12926275862836198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12926275862836198 | validation: 0.4231529927636006]
	TIME [epoch: 3.62 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12639843799929154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12639843799929154 | validation: 0.2798354068158804]
	TIME [epoch: 3.64 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11954948625222986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11954948625222986 | validation: 0.2971429074199134]
	TIME [epoch: 3.61 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1056118776897709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1056118776897709 | validation: 0.3415266143056288]
	TIME [epoch: 3.62 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09024130346376799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09024130346376799 | validation: 0.2657025701122948]
	TIME [epoch: 3.62 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09179014537768039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09179014537768039 | validation: 0.3676805699143074]
	TIME [epoch: 3.63 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09068839446204052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09068839446204052 | validation: 0.25835214388603067]
	TIME [epoch: 3.63 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10957412227338706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10957412227338706 | validation: 0.5773198929888362]
	TIME [epoch: 3.62 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18544356375479612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18544356375479612 | validation: 0.3392901486491175]
	TIME [epoch: 3.63 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18667774211567945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18667774211567945 | validation: 0.4067204034753205]
	TIME [epoch: 3.63 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17418931461339385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17418931461339385 | validation: 0.47347179924832244]
	TIME [epoch: 3.63 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3314018344293405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3314018344293405 | validation: 0.4199738395643833]
	TIME [epoch: 3.63 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16910418172464883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16910418172464883 | validation: 0.36001685718446086]
	TIME [epoch: 3.64 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11006067786255855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11006067786255855 | validation: 0.3015825362555818]
	TIME [epoch: 3.65 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12146242429650024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12146242429650024 | validation: 0.36223778539768525]
	TIME [epoch: 3.63 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10288535775757357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10288535775757357 | validation: 0.3755463547427176]
	TIME [epoch: 3.62 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11955742167274788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11955742167274788 | validation: 0.26682843211826074]
	TIME [epoch: 3.64 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11837529081822794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11837529081822794 | validation: 0.40384331315882227]
	TIME [epoch: 3.62 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10220965386517211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10220965386517211 | validation: 0.2778922909369333]
	TIME [epoch: 3.62 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08361363032958817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08361363032958817 | validation: 0.29384396601245544]
	TIME [epoch: 3.65 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07905736436977776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07905736436977776 | validation: 0.3638882313493038]
	TIME [epoch: 3.63 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08439038864060532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08439038864060532 | validation: 0.25214296455776086]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_460.pth
	Model improved!!!
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09638759690705097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09638759690705097 | validation: 0.4376998654852055]
	TIME [epoch: 3.6 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1520156092462952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1520156092462952 | validation: 0.29983548624597794]
	TIME [epoch: 3.61 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1722526910852203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1722526910852203 | validation: 0.45206270477604416]
	TIME [epoch: 3.6 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1255173922981427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1255173922981427 | validation: 0.28762944685058506]
	TIME [epoch: 3.62 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07950284083377347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07950284083377347 | validation: 0.24072539393963532]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_465.pth
	Model improved!!!
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0753516584572831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0753516584572831 | validation: 0.41105706699273353]
	TIME [epoch: 3.61 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08782038937054816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08782038937054816 | validation: 0.22500017617180934]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_467.pth
	Model improved!!!
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09197277450697545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09197277450697545 | validation: 0.645619682138614]
	TIME [epoch: 3.61 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1892910471198556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1892910471198556 | validation: 0.25053765259381644]
	TIME [epoch: 3.62 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11775835627042909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11775835627042909 | validation: 0.2851164002665905]
	TIME [epoch: 3.62 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10315031397125018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10315031397125018 | validation: 0.2791712895759233]
	TIME [epoch: 3.62 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12111769440787885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12111769440787885 | validation: 0.32358250558829416]
	TIME [epoch: 3.61 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17501421517405655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17501421517405655 | validation: 0.27920100044473894]
	TIME [epoch: 3.62 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09512137039151806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09512137039151806 | validation: 0.2666624592692013]
	TIME [epoch: 3.62 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07779565297150688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07779565297150688 | validation: 0.2910108909703834]
	TIME [epoch: 3.63 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07891281240774706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07891281240774706 | validation: 0.237039024821092]
	TIME [epoch: 3.62 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10006186986275394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10006186986275394 | validation: 0.4168566845542399]
	TIME [epoch: 3.64 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14181949199996555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14181949199996555 | validation: 0.2576842563152608]
	TIME [epoch: 3.62 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12370728172123631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12370728172123631 | validation: 0.27668307865779157]
	TIME [epoch: 3.62 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08165793515372151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08165793515372151 | validation: 0.3000049870899843]
	TIME [epoch: 3.62 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07847882032571434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07847882032571434 | validation: 0.23483050467358124]
	TIME [epoch: 3.62 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10334782448870038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10334782448870038 | validation: 0.4545989360494713]
	TIME [epoch: 3.61 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11931189389869541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11931189389869541 | validation: 0.24450111874751831]
	TIME [epoch: 3.62 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10385499933548281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10385499933548281 | validation: 0.39095085489374504]
	TIME [epoch: 3.62 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10253982993409011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10253982993409011 | validation: 0.2398259848762586]
	TIME [epoch: 3.62 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08182164385311168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08182164385311168 | validation: 0.26641402525314534]
	TIME [epoch: 3.62 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08955150896314754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08955150896314754 | validation: 0.35530031478631824]
	TIME [epoch: 3.62 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17034480486683964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17034480486683964 | validation: 0.23894341416949594]
	TIME [epoch: 3.62 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09220400959807534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09220400959807534 | validation: 0.3170549906754334]
	TIME [epoch: 3.64 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09723396845342926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09723396845342926 | validation: 0.2435161989530121]
	TIME [epoch: 3.63 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08984483033979772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08984483033979772 | validation: 0.3973060134427016]
	TIME [epoch: 3.62 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1050932087935833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1050932087935833 | validation: 0.23938050114943252]
	TIME [epoch: 3.61 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11936947371244591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11936947371244591 | validation: 0.5413713911839765]
	TIME [epoch: 3.62 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15937681371344845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15937681371344845 | validation: 0.23233386167292125]
	TIME [epoch: 3.61 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08640696436540872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08640696436540872 | validation: 0.1963286721430997]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_495.pth
	Model improved!!!
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10244234087542119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10244234087542119 | validation: 0.46190717446558094]
	TIME [epoch: 3.62 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09694197586294834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09694197586294834 | validation: 0.2537283195756929]
	TIME [epoch: 3.62 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08492407900836095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08492407900836095 | validation: 0.23286121996598746]
	TIME [epoch: 3.63 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08329611221745044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08329611221745044 | validation: 0.20019670893610392]
	TIME [epoch: 3.62 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07746274032853717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07746274032853717 | validation: 0.271152912697286]
	TIME [epoch: 3.62 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1081571852600796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1081571852600796 | validation: 0.18286382009289937]
	TIME [epoch: 48 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_501.pth
	Model improved!!!
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10757855375897454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10757855375897454 | validation: 0.2963721797371283]
	TIME [epoch: 7.83 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1687295513824894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1687295513824894 | validation: 0.39534762758902336]
	TIME [epoch: 7.82 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1538176467150427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1538176467150427 | validation: 0.22153656945677613]
	TIME [epoch: 7.83 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11597294876989722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11597294876989722 | validation: 0.2247781719960198]
	TIME [epoch: 7.81 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09556775885426796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09556775885426796 | validation: 0.39940624742649145]
	TIME [epoch: 7.81 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10117122579193463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10117122579193463 | validation: 0.18986269897598312]
	TIME [epoch: 7.83 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07858467991083061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07858467991083061 | validation: 0.19416177876282537]
	TIME [epoch: 7.82 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08279778297043737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08279778297043737 | validation: 0.3017645893979106]
	TIME [epoch: 7.83 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08098956242964539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08098956242964539 | validation: 0.21672888492360834]
	TIME [epoch: 7.83 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08364907675744924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08364907675744924 | validation: 0.5749253221532088]
	TIME [epoch: 7.82 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14322906521129738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14322906521129738 | validation: 0.2271774921371489]
	TIME [epoch: 7.81 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10978427556176464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10978427556176464 | validation: 0.3203160750361729]
	TIME [epoch: 7.82 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10217806444825239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10217806444825239 | validation: 0.2302591850435305]
	TIME [epoch: 7.81 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08337399758789169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08337399758789169 | validation: 0.20255109252456768]
	TIME [epoch: 7.82 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09022778341465873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09022778341465873 | validation: 0.25434619355624033]
	TIME [epoch: 7.83 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09398010859684348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09398010859684348 | validation: 0.23255008897343546]
	TIME [epoch: 7.83 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10205625115671893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10205625115671893 | validation: 0.2421799963129929]
	TIME [epoch: 7.82 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1446449881414331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1446449881414331 | validation: 0.3812615052422621]
	TIME [epoch: 7.82 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11868837142579727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11868837142579727 | validation: 0.21933865488193158]
	TIME [epoch: 7.82 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07523633907891514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07523633907891514 | validation: 0.2600789465412525]
	TIME [epoch: 7.82 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07792872999393813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07792872999393813 | validation: 0.22706958211216977]
	TIME [epoch: 7.82 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06496129773615156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06496129773615156 | validation: 0.24383765160548299]
	TIME [epoch: 7.83 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05996394965741216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05996394965741216 | validation: 0.2092712010310038]
	TIME [epoch: 7.81 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05639870558590748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05639870558590748 | validation: 0.19150058491178046]
	TIME [epoch: 7.82 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06610343349302034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06610343349302034 | validation: 0.2517546862273514]
	TIME [epoch: 7.81 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09043570614035623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09043570614035623 | validation: 0.32554226167073064]
	TIME [epoch: 7.83 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11267423295758748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11267423295758748 | validation: 0.20846161689452938]
	TIME [epoch: 7.82 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09855292517238706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09855292517238706 | validation: 0.4575781654248582]
	TIME [epoch: 7.84 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11913183094699459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11913183094699459 | validation: 0.23967573012830057]
	TIME [epoch: 7.82 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.128337439425465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.128337439425465 | validation: 0.43901063167610455]
	TIME [epoch: 7.83 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1086500113476744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1086500113476744 | validation: 0.33387740857247883]
	TIME [epoch: 7.82 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07986370446480724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07986370446480724 | validation: 0.23121797047424902]
	TIME [epoch: 7.82 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06664826513639062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06664826513639062 | validation: 0.22348475990872238]
	TIME [epoch: 7.83 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10560431513270169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10560431513270169 | validation: 0.2870935191481534]
	TIME [epoch: 7.83 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12653964188687186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12653964188687186 | validation: 0.2801257297206336]
	TIME [epoch: 7.83 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09218381167890018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09218381167890018 | validation: 0.19316102947255767]
	TIME [epoch: 7.82 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09870545302131448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09870545302131448 | validation: 0.3622271392023757]
	TIME [epoch: 7.83 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09472343271599502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09472343271599502 | validation: 0.17874782916863602]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_539.pth
	Model improved!!!
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06624440458537979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06624440458537979 | validation: 0.25547743862462896]
	TIME [epoch: 7.83 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0746865364822489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0746865364822489 | validation: 0.19419655110717984]
	TIME [epoch: 7.84 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06403988966541294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06403988966541294 | validation: 0.19579853062171682]
	TIME [epoch: 7.83 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05200417969751237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05200417969751237 | validation: 0.18801405813753144]
	TIME [epoch: 7.82 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05416609391413438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05416609391413438 | validation: 0.1664988619391693]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_544.pth
	Model improved!!!
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056638505186340206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056638505186340206 | validation: 0.18940382217023735]
	TIME [epoch: 7.78 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05914742556317574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05914742556317574 | validation: 0.20412951146923108]
	TIME [epoch: 7.81 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08906206772365917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08906206772365917 | validation: 0.23392914724974945]
	TIME [epoch: 7.82 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13473950454781183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13473950454781183 | validation: 0.27100230140122383]
	TIME [epoch: 7.84 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10426621717094925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10426621717094925 | validation: 0.2190725631677819]
	TIME [epoch: 7.81 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.122768946297954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.122768946297954 | validation: 0.5594059836987068]
	TIME [epoch: 7.82 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14973727210914933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14973727210914933 | validation: 0.2836700614837963]
	TIME [epoch: 7.8 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08640921424090311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08640921424090311 | validation: 0.17824868947551495]
	TIME [epoch: 7.81 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07119310342840308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07119310342840308 | validation: 0.32942375166155147]
	TIME [epoch: 7.81 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10851154281551803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10851154281551803 | validation: 0.15349155499646214]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_554.pth
	Model improved!!!
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09974326007101823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09974326007101823 | validation: 0.2345870775161537]
	TIME [epoch: 7.78 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1317547927677193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1317547927677193 | validation: 0.24523210558325204]
	TIME [epoch: 7.81 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07756478504416674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07756478504416674 | validation: 0.1778146088824546]
	TIME [epoch: 7.81 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06995733351525975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06995733351525975 | validation: 0.18942954091350642]
	TIME [epoch: 7.83 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06962146409256546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06962146409256546 | validation: 0.16915834415854655]
	TIME [epoch: 7.83 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06486646124817176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06486646124817176 | validation: 0.1640807432399326]
	TIME [epoch: 7.84 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05481527367305775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05481527367305775 | validation: 0.16246735268362894]
	TIME [epoch: 7.83 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053283468492442296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053283468492442296 | validation: 0.17437304853613364]
	TIME [epoch: 7.84 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05321837542588972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05321837542588972 | validation: 0.21139461432754525]
	TIME [epoch: 7.82 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06455607650650916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06455607650650916 | validation: 0.15190121069963003]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_564.pth
	Model improved!!!
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10003187069155073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10003187069155073 | validation: 0.246171978119424]
	TIME [epoch: 7.79 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14994667744644496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14994667744644496 | validation: 0.16021789554468296]
	TIME [epoch: 7.8 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11448073979860635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11448073979860635 | validation: 0.21107243110671067]
	TIME [epoch: 7.8 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10092872068216625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10092872068216625 | validation: 0.34912024798901004]
	TIME [epoch: 7.79 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06340686578427912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06340686578427912 | validation: 0.26051751293229003]
	TIME [epoch: 7.8 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13642161807888356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13642161807888356 | validation: 0.5429717251759278]
	TIME [epoch: 7.8 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16215157180792883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16215157180792883 | validation: 0.16397589713750577]
	TIME [epoch: 7.8 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11297288453138876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11297288453138876 | validation: 0.1216572638322686]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_572.pth
	Model improved!!!
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0873567718529111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0873567718529111 | validation: 0.24257578090429455]
	TIME [epoch: 7.82 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07296800278057057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07296800278057057 | validation: 0.21630340999663802]
	TIME [epoch: 7.81 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07049158050782146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07049158050782146 | validation: 0.17441317871501694]
	TIME [epoch: 7.84 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051081909522478944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051081909522478944 | validation: 0.1351420959028037]
	TIME [epoch: 7.82 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05119280290541976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05119280290541976 | validation: 0.1421812563756358]
	TIME [epoch: 7.81 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059121323901363106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059121323901363106 | validation: 0.19763492242078817]
	TIME [epoch: 7.82 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07420448970149289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07420448970149289 | validation: 0.23952153372296206]
	TIME [epoch: 7.82 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09316084579749154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09316084579749154 | validation: 0.1959072320809618]
	TIME [epoch: 7.8 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0948907408940226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0948907408940226 | validation: 0.13934506366671418]
	TIME [epoch: 7.8 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08080797219940854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08080797219940854 | validation: 0.17752734125343325]
	TIME [epoch: 7.79 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09091517478719054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09091517478719054 | validation: 0.22332459932579418]
	TIME [epoch: 7.8 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059041202052608266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059041202052608266 | validation: 0.14368489939013576]
	TIME [epoch: 7.81 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04932926625531616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04932926625531616 | validation: 0.20768778599340731]
	TIME [epoch: 7.81 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0506191720442747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0506191720442747 | validation: 0.15845763955760775]
	TIME [epoch: 7.8 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06695935807178002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06695935807178002 | validation: 0.33933921573069087]
	TIME [epoch: 7.81 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0879835724357043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0879835724357043 | validation: 0.1568246426462757]
	TIME [epoch: 7.8 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07488257110626148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07488257110626148 | validation: 0.208317364092603]
	TIME [epoch: 7.78 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046189813271937226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046189813271937226 | validation: 0.16635443739104971]
	TIME [epoch: 7.81 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04116072570063076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04116072570063076 | validation: 0.12486240928459996]
	TIME [epoch: 7.8 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05848668070401786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05848668070401786 | validation: 0.17244174005805415]
	TIME [epoch: 7.82 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0884937991192161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0884937991192161 | validation: 0.1792929337065406]
	TIME [epoch: 7.79 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09141915150824634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09141915150824634 | validation: 0.2947855932165403]
	TIME [epoch: 7.81 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10275634142419345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10275634142419345 | validation: 0.21808941491385078]
	TIME [epoch: 7.79 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13204264207561056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13204264207561056 | validation: 0.16562388812962403]
	TIME [epoch: 7.8 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18220528620643578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18220528620643578 | validation: 0.2952057100318084]
	TIME [epoch: 7.82 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17181073021433513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17181073021433513 | validation: 0.2448739275137043]
	TIME [epoch: 7.81 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08871670707497818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08871670707497818 | validation: 0.24937636547822395]
	TIME [epoch: 7.81 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06979361154210414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06979361154210414 | validation: 0.15014698147258612]
	TIME [epoch: 7.8 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06910010249908491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06910010249908491 | validation: 0.19688489775816895]
	TIME [epoch: 7.82 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06258657297256734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06258657297256734 | validation: 0.14893508217039372]
	TIME [epoch: 7.81 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05989137579652962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05989137579652962 | validation: 0.19070115849056044]
	TIME [epoch: 7.84 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056333511299804795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056333511299804795 | validation: 0.15843287676010342]
	TIME [epoch: 7.84 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058435735383635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058435735383635 | validation: 0.16267232972965015]
	TIME [epoch: 7.84 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06581724957419177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06581724957419177 | validation: 0.141087472841719]
	TIME [epoch: 7.83 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06388971677980743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06388971677980743 | validation: 0.14710067524562717]
	TIME [epoch: 7.82 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06810404839576549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06810404839576549 | validation: 0.20852830195689173]
	TIME [epoch: 7.81 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06658837392248723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06658837392248723 | validation: 0.15819933228286964]
	TIME [epoch: 7.83 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06594845688503506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06594845688503506 | validation: 0.27310000697350256]
	TIME [epoch: 7.83 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07279618568261509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07279618568261509 | validation: 0.1474475598076555]
	TIME [epoch: 7.85 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07577062249715152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07577062249715152 | validation: 0.24296282856250034]
	TIME [epoch: 7.83 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05345327634022341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05345327634022341 | validation: 0.13912664340263972]
	TIME [epoch: 7.83 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05047923958536943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05047923958536943 | validation: 0.19336707832691236]
	TIME [epoch: 7.79 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05404847405169818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05404847405169818 | validation: 0.14855703885480329]
	TIME [epoch: 7.78 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08029212766121104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08029212766121104 | validation: 0.1801491092890281]
	TIME [epoch: 8.22 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0832873092969547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0832873092969547 | validation: 0.11544228350449211]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_617.pth
	Model improved!!!
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08621692290988044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08621692290988044 | validation: 0.24353393664337208]
	TIME [epoch: 7.82 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1263910742149799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1263910742149799 | validation: 0.2309596145202069]
	TIME [epoch: 7.81 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08208398011035857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08208398011035857 | validation: 0.1185207900974834]
	TIME [epoch: 7.81 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04166234768174365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04166234768174365 | validation: 0.16983450954308738]
	TIME [epoch: 7.82 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04127174181418495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04127174181418495 | validation: 0.11797733372250695]
	TIME [epoch: 7.81 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05046252549343571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05046252549343571 | validation: 0.2533858781197017]
	TIME [epoch: 7.82 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06271111156580231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06271111156580231 | validation: 0.19951136770383437]
	TIME [epoch: 7.82 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11021526140151393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11021526140151393 | validation: 0.37306787875076597]
	TIME [epoch: 7.81 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10282078293910521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10282078293910521 | validation: 0.12483829675699919]
	TIME [epoch: 7.82 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054250910023661766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054250910023661766 | validation: 0.1342412002304156]
	TIME [epoch: 7.82 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06216857194630654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06216857194630654 | validation: 0.27849551513981413]
	TIME [epoch: 7.81 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05899951822072804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05899951822072804 | validation: 0.19973025573765266]
	TIME [epoch: 7.83 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05900911396983373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05900911396983373 | validation: 0.13329986874126823]
	TIME [epoch: 7.85 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05792590401286944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05792590401286944 | validation: 0.19328945818554863]
	TIME [epoch: 7.83 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10556733327749498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10556733327749498 | validation: 0.212625202103441]
	TIME [epoch: 7.84 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1868280078827211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1868280078827211 | validation: 0.2385994990058393]
	TIME [epoch: 7.83 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08272420647092077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08272420647092077 | validation: 0.11267068821979981]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_634.pth
	Model improved!!!
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05214291789678056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05214291789678056 | validation: 0.1102380702894414]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_635.pth
	Model improved!!!
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04663960261875283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04663960261875283 | validation: 0.1405347925761735]
	TIME [epoch: 7.84 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04035060923248909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04035060923248909 | validation: 0.15314155956163433]
	TIME [epoch: 7.85 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04048749111584545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04048749111584545 | validation: 0.11043449588182329]
	TIME [epoch: 7.85 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04403625748927238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04403625748927238 | validation: 0.20011095498162962]
	TIME [epoch: 7.85 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048701615250418766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048701615250418766 | validation: 0.15953367034830768]
	TIME [epoch: 7.84 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06766853442962682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06766853442962682 | validation: 0.12168794182445156]
	TIME [epoch: 7.85 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06895258940134369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06895258940134369 | validation: 0.22065617023927994]
	TIME [epoch: 7.84 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07088928913127711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07088928913127711 | validation: 0.11735912830051223]
	TIME [epoch: 7.85 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05501586172651582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05501586172651582 | validation: 0.25974283534620163]
	TIME [epoch: 7.85 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06831570225960727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06831570225960727 | validation: 0.14984348869959607]
	TIME [epoch: 7.85 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08900206926215667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08900206926215667 | validation: 0.39469179832544343]
	TIME [epoch: 7.86 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07142140129423484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07142140129423484 | validation: 0.15467327952854829]
	TIME [epoch: 7.85 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04132397253691903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04132397253691903 | validation: 0.13316387485289374]
	TIME [epoch: 7.85 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05724994742518375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05724994742518375 | validation: 0.35004773124309047]
	TIME [epoch: 7.85 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08330015243526688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08330015243526688 | validation: 0.2809870971887423]
	TIME [epoch: 7.84 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10737294715807799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10737294715807799 | validation: 0.1707140055663508]
	TIME [epoch: 7.84 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10717369598368098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10717369598368098 | validation: 0.227331360720864]
	TIME [epoch: 7.84 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08283833145771556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08283833145771556 | validation: 0.10649724760495033]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_653.pth
	Model improved!!!
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048441829190645366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048441829190645366 | validation: 0.09732847816936235]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_654.pth
	Model improved!!!
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03267440308178363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03267440308178363 | validation: 0.14626588179075403]
	TIME [epoch: 7.84 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0387850368800638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0387850368800638 | validation: 0.1385640976114662]
	TIME [epoch: 7.84 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0395283127542119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0395283127542119 | validation: 0.141248560118601]
	TIME [epoch: 7.84 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04437158324521012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04437158324521012 | validation: 0.18628411075497234]
	TIME [epoch: 7.83 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08448275921629265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08448275921629265 | validation: 0.41597571212435097]
	TIME [epoch: 7.83 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08448187634315382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08448187634315382 | validation: 0.4047714693796728]
	TIME [epoch: 7.83 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10847947763548355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10847947763548355 | validation: 0.4071461466752004]
	TIME [epoch: 7.86 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11293790628194039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11293790628194039 | validation: 0.3372353080335715]
	TIME [epoch: 7.84 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11299129323483306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11299129323483306 | validation: 0.2338644522366664]
	TIME [epoch: 7.84 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08372256453438258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08372256453438258 | validation: 0.18176671456141544]
	TIME [epoch: 7.84 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06920261916814006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06920261916814006 | validation: 0.15894992861660778]
	TIME [epoch: 7.83 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0598235707142874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0598235707142874 | validation: 0.2988380630363679]
	TIME [epoch: 7.84 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059230969792679944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059230969792679944 | validation: 0.18709991790173644]
	TIME [epoch: 7.84 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08694219755812573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08694219755812573 | validation: 0.21142413761894227]
	TIME [epoch: 7.87 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08867077153753983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08867077153753983 | validation: 0.13083670443937595]
	TIME [epoch: 7.83 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05965544294472805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05965544294472805 | validation: 0.13233270026102278]
	TIME [epoch: 7.85 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06021133210630449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06021133210630449 | validation: 0.1482902380311789]
	TIME [epoch: 7.8 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054132944958074386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054132944958074386 | validation: 0.12727060668574275]
	TIME [epoch: 7.79 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059144355931686564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059144355931686564 | validation: 0.21313980534808083]
	TIME [epoch: 7.81 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05973018829996923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05973018829996923 | validation: 0.12834377924912688]
	TIME [epoch: 7.81 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06328529373617726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06328529373617726 | validation: 0.2752333822723531]
	TIME [epoch: 7.78 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058274360469893774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058274360469893774 | validation: 0.13271966452964087]
	TIME [epoch: 7.8 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04952354898979268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04952354898979268 | validation: 0.15740764305348437]
	TIME [epoch: 7.82 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046049865182871796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046049865182871796 | validation: 0.10767140547726806]
	TIME [epoch: 7.82 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05216585860816326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05216585860816326 | validation: 0.170071510652153]
	TIME [epoch: 7.8 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07042434294380823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07042434294380823 | validation: 0.09333522981554411]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_680.pth
	Model improved!!!
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05138515796397982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05138515796397982 | validation: 0.17716034203284137]
	TIME [epoch: 7.82 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047323763935709647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047323763935709647 | validation: 0.15078887811623087]
	TIME [epoch: 7.81 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07765082976056747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07765082976056747 | validation: 0.24750839579276068]
	TIME [epoch: 7.82 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09501384813462155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09501384813462155 | validation: 0.13250767695883178]
	TIME [epoch: 7.83 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07772759910719776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07772759910719776 | validation: 0.1682044388568762]
	TIME [epoch: 7.82 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0441573079508143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0441573079508143 | validation: 0.10253313660345043]
	TIME [epoch: 7.83 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035227120877474365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035227120877474365 | validation: 0.11215793396794847]
	TIME [epoch: 7.82 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06844963042464845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06844963042464845 | validation: 0.1484267209615894]
	TIME [epoch: 7.83 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10204361958358586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10204361958358586 | validation: 0.23013205112935287]
	TIME [epoch: 7.81 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06621082120360999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06621082120360999 | validation: 0.12379603705444067]
	TIME [epoch: 7.82 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06466416763842213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06466416763842213 | validation: 0.11938415796639612]
	TIME [epoch: 7.82 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060615138931139775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060615138931139775 | validation: 0.1579723866746659]
	TIME [epoch: 7.86 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08773734365902479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08773734365902479 | validation: 0.6901290289744273]
	TIME [epoch: 7.8 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20723655809683642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20723655809683642 | validation: 0.32784516455001056]
	TIME [epoch: 7.82 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1322736917337483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1322736917337483 | validation: 0.525934589359639]
	TIME [epoch: 7.81 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13495628102444088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13495628102444088 | validation: 0.3250996849680525]
	TIME [epoch: 7.83 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07516105170873448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07516105170873448 | validation: 0.3119054064682581]
	TIME [epoch: 7.8 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0531867220004358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0531867220004358 | validation: 0.1963684671242778]
	TIME [epoch: 7.81 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04345118061401935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04345118061401935 | validation: 0.13105817617980944]
	TIME [epoch: 7.82 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04881330089419405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04881330089419405 | validation: 0.1466591616981242]
	TIME [epoch: 7.81 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047299868247449396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047299868247449396 | validation: 0.12875055941565536]
	TIME [epoch: 7.84 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052425871631685744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052425871631685744 | validation: 0.1400552190299231]
	TIME [epoch: 7.84 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07306501826205349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07306501826205349 | validation: 0.16680166129380802]
	TIME [epoch: 7.85 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09968190142533327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09968190142533327 | validation: 0.2511712610772814]
	TIME [epoch: 7.86 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08431465314944026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08431465314944026 | validation: 0.18892292003600053]
	TIME [epoch: 7.89 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06876575342006895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06876575342006895 | validation: 0.32382912873001335]
	TIME [epoch: 7.86 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0686484684132982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0686484684132982 | validation: 0.10887211076680132]
	TIME [epoch: 7.87 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05381361635384538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05381361635384538 | validation: 0.12672434877865676]
	TIME [epoch: 7.86 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05364199970497342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05364199970497342 | validation: 0.11850790642717446]
	TIME [epoch: 7.85 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05704230793045749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05704230793045749 | validation: 0.15966506595823093]
	TIME [epoch: 7.86 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07255347508894798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07255347508894798 | validation: 0.10658649166729205]
	TIME [epoch: 7.86 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05509518650432131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05509518650432131 | validation: 0.12428003591457504]
	TIME [epoch: 7.88 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038637872297109216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038637872297109216 | validation: 0.09524891736968716]
	TIME [epoch: 7.86 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036033138680295736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036033138680295736 | validation: 0.10090023733531389]
	TIME [epoch: 7.85 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03733944504458516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03733944504458516 | validation: 0.10619910319455497]
	TIME [epoch: 7.86 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04421956701043632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04421956701043632 | validation: 0.1066411170028474]
	TIME [epoch: 7.86 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06494688463748964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06494688463748964 | validation: 0.12751833285479441]
	TIME [epoch: 7.86 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06728232302781342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06728232302781342 | validation: 0.17003839033246068]
	TIME [epoch: 7.9 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06641260652448928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06641260652448928 | validation: 0.12131008036786513]
	TIME [epoch: 7.86 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03888456078467228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03888456078467228 | validation: 0.14450810637344677]
	TIME [epoch: 7.87 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0550973782617801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0550973782617801 | validation: 0.17198958345462767]
	TIME [epoch: 7.83 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1153163888005041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1153163888005041 | validation: 0.3438371523899201]
	TIME [epoch: 7.87 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1204966914089918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1204966914089918 | validation: 0.06684098792418865]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_723.pth
	Model improved!!!
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04689548479348893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04689548479348893 | validation: 0.14075335008570553]
	TIME [epoch: 7.89 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06402669783256111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06402669783256111 | validation: 0.2831305452559804]
	TIME [epoch: 7.85 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07245756796831382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07245756796831382 | validation: 0.21475880837876923]
	TIME [epoch: 7.88 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06759057166934254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06759057166934254 | validation: 0.14880417280204447]
	TIME [epoch: 7.87 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049782730348654526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049782730348654526 | validation: 0.1301610753194485]
	TIME [epoch: 7.87 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04655499529391438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04655499529391438 | validation: 0.09386861857642716]
	TIME [epoch: 7.88 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039369489040932515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039369489040932515 | validation: 0.08942654610496276]
	TIME [epoch: 7.87 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036543130853658685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036543130853658685 | validation: 0.09739543564207584]
	TIME [epoch: 7.87 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040402878365640785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040402878365640785 | validation: 0.10765918061278856]
	TIME [epoch: 7.87 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047035056078240614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047035056078240614 | validation: 0.20104092732692252]
	TIME [epoch: 7.84 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06088396300750734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06088396300750734 | validation: 0.15110645405903742]
	TIME [epoch: 7.86 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07994775425663192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07994775425663192 | validation: 0.11820385603714463]
	TIME [epoch: 7.86 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06105933554003139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06105933554003139 | validation: 0.4014075700670013]
	TIME [epoch: 7.83 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0746763733651123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0746763733651123 | validation: 0.21795552905396764]
	TIME [epoch: 7.85 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07682931066153305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07682931066153305 | validation: 0.3424938572716602]
	TIME [epoch: 7.85 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07574189701663879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07574189701663879 | validation: 0.1883269350193452]
	TIME [epoch: 7.86 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08049604167432121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08049604167432121 | validation: 0.08251858677206608]
	TIME [epoch: 7.85 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03538057480938555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03538057480938555 | validation: 0.05467541926991844]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_741.pth
	Model improved!!!
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02945264503614121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02945264503614121 | validation: 0.05873384318778509]
	TIME [epoch: 7.83 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030059651314453477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030059651314453477 | validation: 0.12900629276831244]
	TIME [epoch: 7.86 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04324425288226829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04324425288226829 | validation: 0.11075995830365389]
	TIME [epoch: 7.82 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0635265373930442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0635265373930442 | validation: 0.1308293838322608]
	TIME [epoch: 7.84 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05777124783555634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05777124783555634 | validation: 0.13665500831387897]
	TIME [epoch: 7.85 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055312197904431794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055312197904431794 | validation: 0.0916059419084153]
	TIME [epoch: 7.85 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04200884585386252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04200884585386252 | validation: 0.10950392656286721]
	TIME [epoch: 7.85 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04468989745359923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04468989745359923 | validation: 0.12447104174310392]
	TIME [epoch: 7.84 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051346307391902296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051346307391902296 | validation: 0.14010706658399955]
	TIME [epoch: 7.85 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07334374961764949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07334374961764949 | validation: 0.23526807676728093]
	TIME [epoch: 7.82 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07458346184090257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07458346184090257 | validation: 0.12375942349942122]
	TIME [epoch: 7.84 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0633330358224453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0633330358224453 | validation: 0.1502951048356976]
	TIME [epoch: 7.85 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042383625347788716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042383625347788716 | validation: 0.12659917172942733]
	TIME [epoch: 7.86 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045106810711096214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045106810711096214 | validation: 0.18941245377773264]
	TIME [epoch: 7.87 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03932354991245882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03932354991245882 | validation: 0.10602719322645222]
	TIME [epoch: 7.88 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0314577577835331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0314577577835331 | validation: 0.09243226887668038]
	TIME [epoch: 7.85 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03200900078283347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03200900078283347 | validation: 0.15502067885195767]
	TIME [epoch: 7.82 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0841825486531414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0841825486531414 | validation: 0.16708679904344043]
	TIME [epoch: 7.84 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15419844196404894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15419844196404894 | validation: 0.09690801826486595]
	TIME [epoch: 7.87 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08299822895496846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08299822895496846 | validation: 0.20752833988487285]
	TIME [epoch: 7.82 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08338804556757458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08338804556757458 | validation: 0.11047047445240557]
	TIME [epoch: 7.88 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057217776273749285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057217776273749285 | validation: 0.12172670880422708]
	TIME [epoch: 7.82 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036664902951070534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036664902951070534 | validation: 0.10397923679497123]
	TIME [epoch: 7.87 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05151473130215276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05151473130215276 | validation: 0.10729696581905435]
	TIME [epoch: 7.83 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05021175623196509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05021175623196509 | validation: 0.11179699396305698]
	TIME [epoch: 7.81 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05066948419570905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05066948419570905 | validation: 0.09836713221806699]
	TIME [epoch: 7.79 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04822067771354891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04822067771354891 | validation: 0.26840531945839546]
	TIME [epoch: 7.85 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05524540616152869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05524540616152869 | validation: 0.198125693637334]
	TIME [epoch: 7.79 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0438379252132647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0438379252132647 | validation: 0.1635014825638392]
	TIME [epoch: 7.81 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03630958951177691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03630958951177691 | validation: 0.12455434671977571]
	TIME [epoch: 7.8 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05451548282231855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05451548282231855 | validation: 0.22488541450491129]
	TIME [epoch: 7.84 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08109057912008046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08109057912008046 | validation: 0.10675233876817787]
	TIME [epoch: 7.85 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07619419135723836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07619419135723836 | validation: 0.09106385997019514]
	TIME [epoch: 7.83 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02970114400587425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02970114400587425 | validation: 0.17424670385521415]
	TIME [epoch: 7.84 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032724729992447674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032724729992447674 | validation: 0.11575381819094843]
	TIME [epoch: 7.83 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03418151790546554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03418151790546554 | validation: 0.11566000956526397]
	TIME [epoch: 7.81 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026340034264227236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026340034264227236 | validation: 0.07193189770039624]
	TIME [epoch: 7.79 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03225082938606259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03225082938606259 | validation: 0.12196166647890024]
	TIME [epoch: 7.83 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05399137753804944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05399137753804944 | validation: 0.18518572269775202]
	TIME [epoch: 7.83 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11020877065413075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11020877065413075 | validation: 0.2523088086416287]
	TIME [epoch: 7.85 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11093516644632753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11093516644632753 | validation: 0.17403296461743098]
	TIME [epoch: 7.83 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11018271572462648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11018271572462648 | validation: 0.15437663075433614]
	TIME [epoch: 7.82 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09247089843334184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09247089843334184 | validation: 0.2609799101833649]
	TIME [epoch: 7.82 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05979575352132981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05979575352132981 | validation: 0.1127442466944489]
	TIME [epoch: 7.83 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026242865145195537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026242865145195537 | validation: 0.09685076520628993]
	TIME [epoch: 7.84 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032832412490813506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032832412490813506 | validation: 0.1360793476009668]
	TIME [epoch: 7.85 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03162528619509261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03162528619509261 | validation: 0.09456919577338313]
	TIME [epoch: 7.82 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023272357314578614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023272357314578614 | validation: 0.07419290810178311]
	TIME [epoch: 7.81 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024008094488745954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024008094488745954 | validation: 0.08351310388479216]
	TIME [epoch: 7.82 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02798579732445844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02798579732445844 | validation: 0.07782274472254182]
	TIME [epoch: 7.8 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03606375802880885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03606375802880885 | validation: 0.12046730776187452]
	TIME [epoch: 7.84 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05733117931964754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05733117931964754 | validation: 0.09879722283007539]
	TIME [epoch: 7.83 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05951497724550433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05951497724550433 | validation: 0.16401384424315602]
	TIME [epoch: 7.84 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07244459239192548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07244459239192548 | validation: 0.1453763933105482]
	TIME [epoch: 7.84 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10844311706238959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10844311706238959 | validation: 0.10402990292028971]
	TIME [epoch: 7.82 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0739583140686013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0739583140686013 | validation: 0.14440647336622386]
	TIME [epoch: 7.84 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04489701996613619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04489701996613619 | validation: 0.10861818079607598]
	TIME [epoch: 7.82 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057852254289260295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057852254289260295 | validation: 0.19251667022199712]
	TIME [epoch: 7.83 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0380467567313786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0380467567313786 | validation: 0.08875477737555458]
	TIME [epoch: 7.84 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026204237194223396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026204237194223396 | validation: 0.09688932302627982]
	TIME [epoch: 7.84 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027912190594980273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027912190594980273 | validation: 0.07019424585268674]
	TIME [epoch: 7.81 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02785785854315779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02785785854315779 | validation: 0.075494845199696]
	TIME [epoch: 7.8 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03369365094329026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03369365094329026 | validation: 0.11004417936176597]
	TIME [epoch: 7.8 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04944572999838097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04944572999838097 | validation: 0.11106526064610887]
	TIME [epoch: 7.85 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07363908867845757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07363908867845757 | validation: 0.1358618456416217]
	TIME [epoch: 7.85 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0919059296603713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0919059296603713 | validation: 0.09821057578710915]
	TIME [epoch: 7.83 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036950425464147715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036950425464147715 | validation: 0.11512796890607002]
	TIME [epoch: 7.83 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032925520028247716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032925520028247716 | validation: 0.10889855268195746]
	TIME [epoch: 7.83 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060180579140306625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060180579140306625 | validation: 0.24087431292080919]
	TIME [epoch: 7.85 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10131871523169822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10131871523169822 | validation: 0.08845798486869638]
	TIME [epoch: 7.86 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0513000763500319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0513000763500319 | validation: 0.08224575225210284]
	TIME [epoch: 7.85 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03962874222718197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03962874222718197 | validation: 0.11401121116476519]
	TIME [epoch: 7.84 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047450634987149826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047450634987149826 | validation: 0.12693583875355238]
	TIME [epoch: 7.83 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04433704086189497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04433704086189497 | validation: 0.0798190955672787]
	TIME [epoch: 7.82 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029339833753318497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029339833753318497 | validation: 0.07785356157411957]
	TIME [epoch: 7.82 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031134336338323597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031134336338323597 | validation: 0.06928973202647759]
	TIME [epoch: 7.87 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028284802892052784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028284802892052784 | validation: 0.08645394084319989]
	TIME [epoch: 7.87 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032658583402823606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032658583402823606 | validation: 0.10469082134293828]
	TIME [epoch: 7.84 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043743038912193416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043743038912193416 | validation: 0.10868511509633641]
	TIME [epoch: 7.83 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07494414297595019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07494414297595019 | validation: 0.14591686148035984]
	TIME [epoch: 7.82 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0742656300104245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0742656300104245 | validation: 0.11261285641508241]
	TIME [epoch: 7.84 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04584914296841818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04584914296841818 | validation: 0.14152327392223785]
	TIME [epoch: 7.83 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027087312621154047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027087312621154047 | validation: 0.10473673001387984]
	TIME [epoch: 7.85 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031150440624296984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031150440624296984 | validation: 0.11353253785273104]
	TIME [epoch: 7.84 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04325584268646804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04325584268646804 | validation: 0.31502735061577436]
	TIME [epoch: 7.83 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10515142366362575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10515142366362575 | validation: 0.11035338357789798]
	TIME [epoch: 7.82 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10934918944456797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10934918944456797 | validation: 0.07744726319529893]
	TIME [epoch: 7.83 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052685145476980486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052685145476980486 | validation: 0.09317418106065699]
	TIME [epoch: 7.83 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041614991511181004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041614991511181004 | validation: 0.08388134963243676]
	TIME [epoch: 7.83 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05967130911589905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05967130911589905 | validation: 0.10285036486596107]
	TIME [epoch: 7.83 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051106138440404174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051106138440404174 | validation: 0.09688797143840516]
	TIME [epoch: 7.84 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06505714485144384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06505714485144384 | validation: 0.10720115475287072]
	TIME [epoch: 7.83 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05387770655252527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05387770655252527 | validation: 0.09863701411547388]
	TIME [epoch: 7.84 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03485920146211326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03485920146211326 | validation: 0.07410635587554591]
	TIME [epoch: 7.82 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024645083050879465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024645083050879465 | validation: 0.07758707232445862]
	TIME [epoch: 7.86 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020453545295308215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020453545295308215 | validation: 0.07114096083199355]
	TIME [epoch: 7.83 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022511106043821682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022511106043821682 | validation: 0.1258834731903035]
	TIME [epoch: 7.84 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033934948510410956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033934948510410956 | validation: 0.1147771113098151]
	TIME [epoch: 7.8 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05820333321322313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05820333321322313 | validation: 0.1780500675311728]
	TIME [epoch: 7.84 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08046791871793707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08046791871793707 | validation: 0.09539268863823641]
	TIME [epoch: 7.82 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06149869788999561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06149869788999561 | validation: 0.08932173316516613]
	TIME [epoch: 7.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171538/states/model_phi1_4b_v_mmd1_842.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 4324.653 seconds.
