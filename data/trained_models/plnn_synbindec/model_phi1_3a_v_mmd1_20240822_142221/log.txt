Args:
Namespace(name='model_phi1_3a_v_mmd1', outdir='out/model_training/model_phi1_3a_v_mmd1', training_data='data/training_data/data_phi1_3a/training', validation_data='data/training_data/data_phi1_3a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 838108010

Training model...

Saving initial model state to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.055794541202819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.055794541202819 | validation: 4.073378927910595]
	TIME [epoch: 25.8 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7058118708543843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7058118708543843 | validation: 3.576238365024078]
	TIME [epoch: 0.915 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.804852374963664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.804852374963664 | validation: 3.176855662719845]
	TIME [epoch: 0.909 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0577006599335195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0577006599335195 | validation: 2.885551878765167]
	TIME [epoch: 0.908 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.756103180942395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.756103180942395 | validation: 2.3011240390309724]
	TIME [epoch: 0.907 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5867831121289893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5867831121289893 | validation: 4.564693871079126]
	TIME [epoch: 0.907 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.441891395457743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.441891395457743 | validation: 2.156351601693347]
	TIME [epoch: 0.904 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.275211594958813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.275211594958813 | validation: 2.1450830161866117]
	TIME [epoch: 0.904 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.181619808540242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.181619808540242 | validation: 2.301859632748893]
	TIME [epoch: 0.903 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.111025922292522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.111025922292522 | validation: 2.2221761643861973]
	TIME [epoch: 0.904 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0108237038839922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0108237038839922 | validation: 2.1516550185113816]
	TIME [epoch: 0.904 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9084619126704916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9084619126704916 | validation: 2.2360179901383193]
	TIME [epoch: 0.902 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8438509736898285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8438509736898285 | validation: 2.1332528309730656]
	TIME [epoch: 0.904 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7760160187666676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7760160187666676 | validation: 2.089508577064126]
	TIME [epoch: 0.905 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.710867533463202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.710867533463202 | validation: 2.020206679898261]
	TIME [epoch: 0.907 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6521027334712182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6521027334712182 | validation: 1.975657246886847]
	TIME [epoch: 0.907 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5925559807246623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5925559807246623 | validation: 1.991666684450727]
	TIME [epoch: 0.903 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5392598801750648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5392598801750648 | validation: 1.8116634863825074]
	TIME [epoch: 0.903 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5064523050383596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5064523050383596 | validation: 2.2328996648184507]
	TIME [epoch: 0.907 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.620319671888442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.620319671888442 | validation: 1.96606191507255]
	TIME [epoch: 0.903 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0850591980200277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0850591980200277 | validation: 1.875963087224311]
	TIME [epoch: 0.904 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4672729001508917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4672729001508917 | validation: 2.030051980138297]
	TIME [epoch: 0.904 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.490286938400808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.490286938400808 | validation: 1.693689356104159]
	TIME [epoch: 0.904 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5417988715373379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5417988715373379 | validation: 1.769569476037656]
	TIME [epoch: 0.905 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3827667226701794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3827667226701794 | validation: 1.8100124672656883]
	TIME [epoch: 0.907 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3830439399014665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3830439399014665 | validation: 1.5934888026755862]
	TIME [epoch: 0.907 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.313509871256149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.313509871256149 | validation: 1.6641023931168697]
	TIME [epoch: 0.908 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2760448873460104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2760448873460104 | validation: 1.5731675420073448]
	TIME [epoch: 0.904 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2658633377762463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2658633377762463 | validation: 1.6133583738410786]
	TIME [epoch: 0.907 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2913096803825443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2913096803825443 | validation: 1.6002384271590633]
	TIME [epoch: 0.906 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.349552182891298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.349552182891298 | validation: 1.6698531849176974]
	TIME [epoch: 0.912 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2983041654914598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2983041654914598 | validation: 1.4702671225859054]
	TIME [epoch: 0.917 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2619133491432362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2619133491432362 | validation: 1.599226193073493]
	TIME [epoch: 0.903 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.219384687181174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.219384687181174 | validation: 1.3950158953459064]
	TIME [epoch: 0.903 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2125625512369675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2125625512369675 | validation: 1.5635492279079375]
	TIME [epoch: 0.906 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1760288666657246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1760288666657246 | validation: 1.358786383815315]
	TIME [epoch: 0.905 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1308723540598122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1308723540598122 | validation: 1.4241891758174008]
	TIME [epoch: 0.907 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.111012303954932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.111012303954932 | validation: 1.3298686307011318]
	TIME [epoch: 0.911 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0962447380282185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0962447380282185 | validation: 1.3889685195610237]
	TIME [epoch: 0.905 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1017654338737286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1017654338737286 | validation: 1.394276557859242]
	TIME [epoch: 0.905 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1908322682361925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1908322682361925 | validation: 1.604970123561536]
	TIME [epoch: 0.901 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3569914810784094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3569914810784094 | validation: 1.4477671929664018]
	TIME [epoch: 0.902 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2543810768142563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2543810768142563 | validation: 1.334873075759224]
	TIME [epoch: 0.9 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.059662969319429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.059662969319429 | validation: 1.2875450145761222]
	TIME [epoch: 0.902 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.130423114130621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.130423114130621 | validation: 1.3098830952268914]
	TIME [epoch: 0.903 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.095711955608083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.095711955608083 | validation: 1.3232074625468668]
	TIME [epoch: 0.904 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0385961754080162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0385961754080162 | validation: 1.192508230049382]
	TIME [epoch: 0.901 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.027987316418986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.027987316418986 | validation: 1.230891622151363]
	TIME [epoch: 0.905 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0045610441798194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0045610441798194 | validation: 1.2245611452853762]
	TIME [epoch: 0.904 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0035809664220514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0035809664220514 | validation: 1.218951350142993]
	TIME [epoch: 0.901 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9906153048668495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9906153048668495 | validation: 1.1851040208182582]
	TIME [epoch: 0.903 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0156241330316853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0156241330316853 | validation: 1.3481084151611922]
	TIME [epoch: 0.906 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0631559779369315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0631559779369315 | validation: 1.1371950197715666]
	TIME [epoch: 0.903 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0767892230244849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0767892230244849 | validation: 1.3062862779633415]
	TIME [epoch: 0.907 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0309483505033168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0309483505033168 | validation: 1.1916945509186765]
	TIME [epoch: 0.901 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0081757262319844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0081757262319844 | validation: 1.2040439873981876]
	TIME [epoch: 0.954 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0225904956716478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0225904956716478 | validation: 1.1869941411138034]
	TIME [epoch: 0.909 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9854325501122086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9854325501122086 | validation: 1.135654286119351]
	TIME [epoch: 0.902 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9407745033668422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9407745033668422 | validation: 1.0853224695099175]
	TIME [epoch: 0.901 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9339684786064771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9339684786064771 | validation: 1.141784804365526]
	TIME [epoch: 0.903 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9277231514882294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9277231514882294 | validation: 1.0781446852104033]
	TIME [epoch: 0.902 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9237364954973177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9237364954973177 | validation: 1.0985266958884867]
	TIME [epoch: 0.903 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9216045061368954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9216045061368954 | validation: 1.0614674692720911]
	TIME [epoch: 0.901 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9291803668037159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9291803668037159 | validation: 1.1146116522051437]
	TIME [epoch: 0.905 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9631928372942018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9631928372942018 | validation: 1.1235197470533866]
	TIME [epoch: 0.904 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0248573174331903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0248573174331903 | validation: 1.1772506743873383]
	TIME [epoch: 0.904 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0106250405311035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0106250405311035 | validation: 1.0843762780102977]
	TIME [epoch: 0.904 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9494281973449357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9494281973449357 | validation: 1.0653315366768996]
	TIME [epoch: 0.905 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9590259946612447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9590259946612447 | validation: 1.126895012049839]
	TIME [epoch: 0.9 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9458837321642309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9458837321642309 | validation: 0.9763781150294645]
	TIME [epoch: 0.904 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9258361753966727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9258361753966727 | validation: 1.105963617812728]
	TIME [epoch: 0.909 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9156639884823576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9156639884823576 | validation: 0.9967572067983488]
	TIME [epoch: 0.919 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8893411230104907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8893411230104907 | validation: 0.9988047206887697]
	TIME [epoch: 0.905 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8821112388673571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8821112388673571 | validation: 1.0324653189775708]
	TIME [epoch: 0.907 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8873594340813477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8873594340813477 | validation: 0.9691839927277318]
	TIME [epoch: 0.907 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8839888814314338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8839888814314338 | validation: 1.0549764973680837]
	TIME [epoch: 0.912 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8894811921552562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8894811921552562 | validation: 0.9737663877953082]
	TIME [epoch: 0.904 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9114558703283362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9114558703283362 | validation: 1.1175632441756738]
	TIME [epoch: 0.906 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9929430504955369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9929430504955369 | validation: 1.068977027153749]
	TIME [epoch: 0.903 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9560321042650135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9560321042650135 | validation: 0.9774532802971078]
	TIME [epoch: 0.907 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8743620132163752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8743620132163752 | validation: 0.9819783423084133]
	TIME [epoch: 0.902 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8899419050889756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8899419050889756 | validation: 1.0155243349816296]
	TIME [epoch: 0.902 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.924516017707589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.924516017707589 | validation: 1.067312044895386]
	TIME [epoch: 0.905 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9657014812399737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9657014812399737 | validation: 1.0251915638182625]
	TIME [epoch: 0.902 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9646751571622615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9646751571622615 | validation: 1.0326123147389723]
	TIME [epoch: 0.902 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8981174646682715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8981174646682715 | validation: 0.9620331275590689]
	TIME [epoch: 0.902 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8653334895596274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8653334895596274 | validation: 0.9642344003355907]
	TIME [epoch: 0.905 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8739039173050424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8739039173050424 | validation: 0.9504624171255701]
	TIME [epoch: 0.904 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8680430872868224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8680430872868224 | validation: 0.9516420932253065]
	TIME [epoch: 0.907 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8692077721867517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8692077721867517 | validation: 0.9852814577955986]
	TIME [epoch: 0.903 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8769501283171343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8769501283171343 | validation: 0.9648384495020856]
	TIME [epoch: 0.903 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9027033213788522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9027033213788522 | validation: 1.055656146213384]
	TIME [epoch: 0.905 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9674942207588495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9674942207588495 | validation: 1.0439793656947838]
	TIME [epoch: 0.902 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9220372178211198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9220372178211198 | validation: 0.9229504242063223]
	TIME [epoch: 0.906 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8931518032000108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8931518032000108 | validation: 1.0286717099758327]
	TIME [epoch: 0.907 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8896703161070328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8896703161070328 | validation: 0.9475789609706142]
	TIME [epoch: 0.905 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8623937339344875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8623937339344875 | validation: 0.9169975394056487]
	TIME [epoch: 0.9 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8713753138081575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8713753138081575 | validation: 0.9988046322938771]
	TIME [epoch: 0.904 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9003704498364141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9003704498364141 | validation: 0.9945205383201705]
	TIME [epoch: 0.904 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9149295138338507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9149295138338507 | validation: 1.0086512604673807]
	TIME [epoch: 0.901 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.956709553049268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.956709553049268 | validation: 1.0811390743282445]
	TIME [epoch: 0.905 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9397440709759483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9397440709759483 | validation: 0.9134877572507328]
	TIME [epoch: 0.909 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8638345062997209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8638345062997209 | validation: 0.9102635149255246]
	TIME [epoch: 0.906 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8678616113311206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8678616113311206 | validation: 0.9914686806371155]
	TIME [epoch: 0.907 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8810123722617575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8810123722617575 | validation: 0.944675869603602]
	TIME [epoch: 0.906 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8712244887802123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8712244887802123 | validation: 0.9756551376673219]
	TIME [epoch: 0.906 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8997536190843968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8997536190843968 | validation: 1.0201226459593025]
	TIME [epoch: 0.906 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9497816091490813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9497816091490813 | validation: 0.9964505528760598]
	TIME [epoch: 0.904 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8882794880924034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8882794880924034 | validation: 0.9245055914792408]
	TIME [epoch: 0.904 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8481836907709895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8481836907709895 | validation: 0.928549773477362]
	TIME [epoch: 0.905 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8522889875324612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8522889875324612 | validation: 0.9453875525034179]
	TIME [epoch: 0.906 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8568571119394054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8568571119394054 | validation: 0.9369088518823171]
	TIME [epoch: 0.903 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8682679417784565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8682679417784565 | validation: 0.9982891115704663]
	TIME [epoch: 0.903 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9012386689611106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9012386689611106 | validation: 0.9975227365490014]
	TIME [epoch: 0.903 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9407580540147844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9407580540147844 | validation: 0.9962063751334926]
	TIME [epoch: 0.906 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8986625704849556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8986625704849556 | validation: 0.8809342352798423]
	TIME [epoch: 0.913 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8591176302662953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8591176302662953 | validation: 0.9458228114866709]
	TIME [epoch: 0.909 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8682738093084157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8682738093084157 | validation: 0.9053182602577338]
	TIME [epoch: 0.911 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8648472547016351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8648472547016351 | validation: 0.9439129135526495]
	TIME [epoch: 0.908 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9048629532464181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9048629532464181 | validation: 0.9899450303283208]
	TIME [epoch: 0.906 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9163776757404253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9163776757404253 | validation: 0.9467751939692186]
	TIME [epoch: 0.905 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8750556491397342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8750556491397342 | validation: 0.9156738452157257]
	TIME [epoch: 0.905 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8565505133876112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8565505133876112 | validation: 0.9395678804016306]
	TIME [epoch: 0.908 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.868308641932239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.868308641932239 | validation: 0.9387455069125856]
	TIME [epoch: 0.904 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8799126347138528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8799126347138528 | validation: 0.9882384474399925]
	TIME [epoch: 0.904 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8771553135676966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8771553135676966 | validation: 0.9073222364952077]
	TIME [epoch: 0.907 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8566081278923038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8566081278923038 | validation: 0.9348237332570488]
	TIME [epoch: 0.909 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8572116699345764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8572116699345764 | validation: 0.8991548109557672]
	TIME [epoch: 0.905 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.857298455937674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.857298455937674 | validation: 0.9896949372856384]
	TIME [epoch: 0.903 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8731308230103478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8731308230103478 | validation: 0.8861134398442202]
	TIME [epoch: 0.905 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8736610574081374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8736610574081374 | validation: 1.012233706092119]
	TIME [epoch: 0.904 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9087552630826166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9087552630826166 | validation: 0.9323415246191593]
	TIME [epoch: 0.906 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8837863683938169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8837863683938169 | validation: 0.9196660209053334]
	TIME [epoch: 0.904 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9029475037781984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9029475037781984 | validation: 1.0206846918203163]
	TIME [epoch: 0.902 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9046271812857337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9046271812857337 | validation: 0.9065400076038839]
	TIME [epoch: 0.902 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8479983853088642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8479983853088642 | validation: 0.8590099942350902]
	TIME [epoch: 0.904 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8520447266869977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8520447266869977 | validation: 0.9623526718468662]
	TIME [epoch: 0.903 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8704590107260765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8704590107260765 | validation: 0.9057140437041995]
	TIME [epoch: 0.905 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8766434606831917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8766434606831917 | validation: 0.9379958149196569]
	TIME [epoch: 0.908 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.905205761371542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.905205761371542 | validation: 0.9770779487873515]
	TIME [epoch: 0.904 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.891088174687642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.891088174687642 | validation: 0.8762660356468391]
	TIME [epoch: 0.902 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8580779326686707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8580779326686707 | validation: 0.9470041529534525]
	TIME [epoch: 0.9 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8412674064459779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8412674064459779 | validation: 0.8915971085135536]
	TIME [epoch: 0.904 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8445781434496499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8445781434496499 | validation: 0.912448194199413]
	TIME [epoch: 0.901 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8347409454833985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8347409454833985 | validation: 0.9001194719122158]
	TIME [epoch: 0.903 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8368100247443215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8368100247443215 | validation: 0.9249807442680162]
	TIME [epoch: 0.9 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8443472010247104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8443472010247104 | validation: 0.9243757052739046]
	TIME [epoch: 0.901 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8695946197007917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8695946197007917 | validation: 1.01436620793476]
	TIME [epoch: 0.902 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9196112188562967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9196112188562967 | validation: 0.9696964617547128]
	TIME [epoch: 0.902 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9308703850507419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9308703850507419 | validation: 0.9126352664646821]
	TIME [epoch: 0.9 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8805538045581726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8805538045581726 | validation: 0.999371992892627]
	TIME [epoch: 0.9 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9045547475880512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9045547475880512 | validation: 0.9305888640585507]
	TIME [epoch: 0.902 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8767534966457845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8767534966457845 | validation: 0.8699533641507928]
	TIME [epoch: 0.902 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8501839318257884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8501839318257884 | validation: 0.9230536179368984]
	TIME [epoch: 0.903 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8461535847415078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8461535847415078 | validation: 0.8756969606395146]
	TIME [epoch: 0.9 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8297295504745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8297295504745 | validation: 0.9176246340737522]
	TIME [epoch: 0.902 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8268278355980156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8268278355980156 | validation: 0.8758338179049266]
	TIME [epoch: 0.904 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.82597550513878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.82597550513878 | validation: 0.9015913628671719]
	TIME [epoch: 0.905 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.823844354879227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.823844354879227 | validation: 0.8960623786084148]
	TIME [epoch: 0.902 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8410008235744658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8410008235744658 | validation: 0.9271691202255179]
	TIME [epoch: 0.902 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9222889909843138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9222889909843138 | validation: 1.052195490965726]
	TIME [epoch: 0.903 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9579374057721677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9579374057721677 | validation: 0.9079178506387198]
	TIME [epoch: 0.901 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8600813886694377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8600813886694377 | validation: 0.9009179815687663]
	TIME [epoch: 0.906 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8730813995588096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8730813995588096 | validation: 0.9595391483162103]
	TIME [epoch: 0.912 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9074436530771689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9074436530771689 | validation: 0.9351592075339327]
	TIME [epoch: 0.907 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8740588256373019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8740588256373019 | validation: 0.8514384497451758]
	TIME [epoch: 0.904 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_166.pth
	Model improved!!!
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8382895730499926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8382895730499926 | validation: 0.9114003361597739]
	TIME [epoch: 0.913 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8323361673478528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8323361673478528 | validation: 0.8507844451458578]
	TIME [epoch: 0.909 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_168.pth
	Model improved!!!
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8379688119835089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8379688119835089 | validation: 0.9269280256798658]
	TIME [epoch: 0.935 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8388106841878754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8388106841878754 | validation: 0.8479232854326476]
	TIME [epoch: 0.91 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8421852113864459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8421852113864459 | validation: 0.939619462552459]
	TIME [epoch: 0.907 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8470657145006035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8470657145006035 | validation: 0.8538797325701477]
	TIME [epoch: 0.905 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8439101142370337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8439101142370337 | validation: 0.9691161258705532]
	TIME [epoch: 0.907 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8624508844852322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8624508844852322 | validation: 0.8418275081100234]
	TIME [epoch: 0.907 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8404080036075487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8404080036075487 | validation: 0.9260898614733972]
	TIME [epoch: 0.907 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8445045679494046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8445045679494046 | validation: 0.8619915451287716]
	TIME [epoch: 0.904 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8469367174319439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8469367174319439 | validation: 0.9249018384107942]
	TIME [epoch: 0.905 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8852820938726194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8852820938726194 | validation: 1.0229565192408006]
	TIME [epoch: 0.906 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9315352928968405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9315352928968405 | validation: 0.8842503101168]
	TIME [epoch: 0.903 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8981115105810438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8981115105810438 | validation: 0.9135800961663613]
	TIME [epoch: 0.905 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8426051343583393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8426051343583393 | validation: 0.8673453982974602]
	TIME [epoch: 0.905 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8344427708055292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8344427708055292 | validation: 0.8796953964849417]
	TIME [epoch: 0.906 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8452844006779152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8452844006779152 | validation: 0.9317597190843258]
	TIME [epoch: 0.903 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8885635399930104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8885635399930104 | validation: 0.8912379192958577]
	TIME [epoch: 0.903 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8677430206945961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8677430206945961 | validation: 0.8656394009490896]
	TIME [epoch: 0.903 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8413761881157723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8413761881157723 | validation: 0.8952461237302135]
	TIME [epoch: 0.902 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8296847506135311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8296847506135311 | validation: 0.8470621240787992]
	TIME [epoch: 0.906 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8203007179010555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8203007179010555 | validation: 0.9438449798856813]
	TIME [epoch: 0.905 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8415954676266677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8415954676266677 | validation: 0.864688252802138]
	TIME [epoch: 0.905 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.864179633897981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.864179633897981 | validation: 1.0021901959866235]
	TIME [epoch: 0.906 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.899043344697686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.899043344697686 | validation: 0.8869607933523419]
	TIME [epoch: 0.907 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8608112226591135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8608112226591135 | validation: 0.8737280731327459]
	TIME [epoch: 0.904 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8516014665558757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8516014665558757 | validation: 0.9091257850704455]
	TIME [epoch: 0.904 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8417981107843755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8417981107843755 | validation: 0.8430043381961725]
	TIME [epoch: 0.906 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8235898411396158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8235898411396158 | validation: 0.887697924084826]
	TIME [epoch: 0.908 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8207560093919046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8207560093919046 | validation: 0.816197309363659]
	TIME [epoch: 0.904 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8227278152223155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8227278152223155 | validation: 0.9428380555250769]
	TIME [epoch: 0.903 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8625637003191474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8625637003191474 | validation: 0.9252905668226008]
	TIME [epoch: 0.903 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9421471255141995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9421471255141995 | validation: 0.935263709159436]
	TIME [epoch: 0.903 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.859351710393776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.859351710393776 | validation: 0.8784385766142755]
	TIME [epoch: 0.9 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8149080302163435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8149080302163435 | validation: 0.8742592046627142]
	TIME [epoch: 23.9 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8260170072868865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8260170072868865 | validation: 0.885525115483501]
	TIME [epoch: 1.78 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8404791777629569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8404791777629569 | validation: 0.8650120644757344]
	TIME [epoch: 1.78 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8609428518431921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8609428518431921 | validation: 0.942473023319077]
	TIME [epoch: 1.78 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8775945288863102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8775945288863102 | validation: 0.8552935780305084]
	TIME [epoch: 1.78 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8424002461121668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8424002461121668 | validation: 0.8912557008158878]
	TIME [epoch: 1.78 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8291999885107028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8291999885107028 | validation: 0.8543863731075865]
	TIME [epoch: 1.78 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8318877093815581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8318877093815581 | validation: 0.8779168910647306]
	TIME [epoch: 1.78 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8321618557874476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8321618557874476 | validation: 0.8935677836047126]
	TIME [epoch: 1.78 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8387985049814358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8387985049814358 | validation: 0.8293533028652652]
	TIME [epoch: 1.78 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8419726424394122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8419726424394122 | validation: 0.9765685563750561]
	TIME [epoch: 1.78 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8593502201693406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8593502201693406 | validation: 0.8544825234275102]
	TIME [epoch: 1.78 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8456757217024466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8456757217024466 | validation: 0.948153117841096]
	TIME [epoch: 1.78 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8481134865163642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8481134865163642 | validation: 0.8138667433237764]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_214.pth
	Model improved!!!
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.839780838063569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.839780838063569 | validation: 0.9219177706076274]
	TIME [epoch: 1.78 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8490356456095787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8490356456095787 | validation: 0.8596423723360846]
	TIME [epoch: 1.78 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8484224815897389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8484224815897389 | validation: 0.8728748535976998]
	TIME [epoch: 1.78 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8376985099565974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8376985099565974 | validation: 0.9062588118568254]
	TIME [epoch: 1.78 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8526133822383493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8526133822383493 | validation: 0.8417484523094871]
	TIME [epoch: 1.78 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8471155655501414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8471155655501414 | validation: 0.87558700816972]
	TIME [epoch: 1.78 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8340693665844816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8340693665844816 | validation: 0.8487415152759372]
	TIME [epoch: 1.78 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8222622326029755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8222622326029755 | validation: 0.8555492921296963]
	TIME [epoch: 1.78 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8383012231795766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8383012231795766 | validation: 0.9851857358944772]
	TIME [epoch: 1.79 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8743435386558757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8743435386558757 | validation: 0.8521477195394868]
	TIME [epoch: 1.78 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8604037890162439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8604037890162439 | validation: 0.9124769256248538]
	TIME [epoch: 1.78 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.849013936174657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.849013936174657 | validation: 0.8335695359008561]
	TIME [epoch: 1.78 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8158640173674393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8158640173674393 | validation: 0.8388047459040499]
	TIME [epoch: 1.78 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8095403763186942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8095403763186942 | validation: 0.8608482855712417]
	TIME [epoch: 1.78 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8172224782658425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8172224782658425 | validation: 0.8736432035706815]
	TIME [epoch: 1.78 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8372709765540883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8372709765540883 | validation: 0.850329933756945]
	TIME [epoch: 1.78 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8607759094060455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8607759094060455 | validation: 0.9265514173103497]
	TIME [epoch: 1.78 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8716285383522717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8716285383522717 | validation: 0.8245523275170772]
	TIME [epoch: 1.78 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8426260899962794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8426260899962794 | validation: 0.8786356549895711]
	TIME [epoch: 1.78 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8228982463333648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8228982463333648 | validation: 0.8207385658192833]
	TIME [epoch: 1.78 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8300787192846523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8300787192846523 | validation: 0.9072102350559883]
	TIME [epoch: 1.78 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8459547579474724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8459547579474724 | validation: 0.8478584314031701]
	TIME [epoch: 1.78 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8331363312115916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8331363312115916 | validation: 0.8884525560060184]
	TIME [epoch: 1.78 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8458015837282323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8458015837282323 | validation: 0.863352510193219]
	TIME [epoch: 1.78 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8372042471946936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8372042471946936 | validation: 0.848920817447908]
	TIME [epoch: 1.78 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8298466128524686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8298466128524686 | validation: 0.8564872936023344]
	TIME [epoch: 1.78 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8472744193392626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8472744193392626 | validation: 0.894711830619849]
	TIME [epoch: 1.78 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.844365403699981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.844365403699981 | validation: 0.8146503819252913]
	TIME [epoch: 1.78 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8430877958145567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8430877958145567 | validation: 0.9266336523116809]
	TIME [epoch: 1.78 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.835170940004168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.835170940004168 | validation: 0.7935430157178982]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_244.pth
	Model improved!!!
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8186639594813095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8186639594813095 | validation: 0.8480613947095108]
	TIME [epoch: 1.78 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8146665321121503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8146665321121503 | validation: 0.8194520568224237]
	TIME [epoch: 1.78 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8120592293954121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8120592293954121 | validation: 0.8631201121176781]
	TIME [epoch: 1.78 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8142101643007589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8142101643007589 | validation: 0.8055529824136859]
	TIME [epoch: 1.79 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8315080990454601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8315080990454601 | validation: 1.015156941288739]
	TIME [epoch: 1.78 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8873097199783471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8873097199783471 | validation: 0.8204356388004879]
	TIME [epoch: 1.79 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8357691496841789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8357691496841789 | validation: 0.8597250529272437]
	TIME [epoch: 1.79 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8344011481545195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8344011481545195 | validation: 0.8612828241451495]
	TIME [epoch: 1.78 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8390062947709543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8390062947709543 | validation: 0.7983222246322854]
	TIME [epoch: 1.79 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8300025491142554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8300025491142554 | validation: 0.8484682554645616]
	TIME [epoch: 1.78 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8220051041254474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8220051041254474 | validation: 0.8297998269874909]
	TIME [epoch: 1.78 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8659549620175614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8659549620175614 | validation: 0.8833630797036198]
	TIME [epoch: 1.78 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8612476212413558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8612476212413558 | validation: 0.8268298037666466]
	TIME [epoch: 1.78 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8502209210479472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8502209210479472 | validation: 0.8686329785927058]
	TIME [epoch: 1.78 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8250163966703701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8250163966703701 | validation: 0.7967655318110444]
	TIME [epoch: 1.78 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8111216988675497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8111216988675497 | validation: 0.8498282278130832]
	TIME [epoch: 1.78 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8111062640085263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8111062640085263 | validation: 0.8029288167504145]
	TIME [epoch: 1.78 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8068064918022388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8068064918022388 | validation: 0.8033763694198195]
	TIME [epoch: 1.78 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8139142094855256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8139142094855256 | validation: 0.8334152856294863]
	TIME [epoch: 1.78 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.810300700266858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.810300700266858 | validation: 0.8398167063473411]
	TIME [epoch: 1.78 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8260754023467846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8260754023467846 | validation: 0.845213738121608]
	TIME [epoch: 1.78 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.832814018453547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.832814018453547 | validation: 0.8379368224860804]
	TIME [epoch: 1.78 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8136935861748498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8136935861748498 | validation: 0.7815950427282834]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8187268628080532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8187268628080532 | validation: 0.9267152281577471]
	TIME [epoch: 1.78 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8557625018489705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8557625018489705 | validation: 0.7931400904279993]
	TIME [epoch: 1.77 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8453017443065572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8453017443065572 | validation: 0.9368258601412404]
	TIME [epoch: 1.78 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8675474772109874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8675474772109874 | validation: 0.7768238504668533]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_271.pth
	Model improved!!!
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8570964746048731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8570964746048731 | validation: 0.8242932913783836]
	TIME [epoch: 1.78 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.826356589719794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.826356589719794 | validation: 0.8083575953322522]
	TIME [epoch: 1.78 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8093211155698721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8093211155698721 | validation: 0.7918858533791483]
	TIME [epoch: 1.78 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7973958487106921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7973958487106921 | validation: 0.7859865726555294]
	TIME [epoch: 1.78 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7920260409978809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7920260409978809 | validation: 0.774559806167635]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_276.pth
	Model improved!!!
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7908314268926848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7908314268926848 | validation: 0.8158523817518786]
	TIME [epoch: 1.79 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8163448156368831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8163448156368831 | validation: 0.819318678783422]
	TIME [epoch: 1.78 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8551668405127784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8551668405127784 | validation: 0.8507580589523084]
	TIME [epoch: 1.78 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8382191462949778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8382191462949778 | validation: 0.7888152351122204]
	TIME [epoch: 1.78 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8103290117218387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8103290117218387 | validation: 0.8158102875182716]
	TIME [epoch: 1.78 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7858167301630674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7858167301630674 | validation: 0.7864776128331974]
	TIME [epoch: 1.78 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8011004449416268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8011004449416268 | validation: 0.9855299830821131]
	TIME [epoch: 1.78 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8972143466149362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8972143466149362 | validation: 0.8424303415230862]
	TIME [epoch: 1.78 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9253619983407252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9253619983407252 | validation: 0.8921354162655128]
	TIME [epoch: 1.79 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.820496188415319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.820496188415319 | validation: 0.7614743705763918]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_286.pth
	Model improved!!!
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7842485056211865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7842485056211865 | validation: 0.7854299932770541]
	TIME [epoch: 1.77 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8070953403243789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8070953403243789 | validation: 0.8174634394991515]
	TIME [epoch: 1.77 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8128823323052089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8128823323052089 | validation: 0.7815030949439921]
	TIME [epoch: 1.78 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7986711568647743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7986711568647743 | validation: 0.761578700439104]
	TIME [epoch: 1.77 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7771124511277171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7771124511277171 | validation: 0.856538480363049]
	TIME [epoch: 1.77 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8143091111596377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8143091111596377 | validation: 0.8021330220306443]
	TIME [epoch: 1.77 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8282171351910868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8282171351910868 | validation: 0.8683445912167337]
	TIME [epoch: 1.77 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8448747961902265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8448747961902265 | validation: 0.8142845274812089]
	TIME [epoch: 1.77 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8007146809426903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8007146809426903 | validation: 0.7888771635926743]
	TIME [epoch: 1.77 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8185363088605487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8185363088605487 | validation: 0.8749566923710106]
	TIME [epoch: 1.77 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8306376186311046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8306376186311046 | validation: 0.7694709670153016]
	TIME [epoch: 1.78 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7958694200202134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7958694200202134 | validation: 0.8349708473952359]
	TIME [epoch: 1.78 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7960183773104592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7960183773104592 | validation: 0.7877486524490027]
	TIME [epoch: 1.78 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8454968434538106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8454968434538106 | validation: 0.9700671508129775]
	TIME [epoch: 1.77 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8904877897393169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8904877897393169 | validation: 0.732353269667533]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_301.pth
	Model improved!!!
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8055798451376606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8055798451376606 | validation: 0.7962446939082546]
	TIME [epoch: 1.78 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8058527849329096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8058527849329096 | validation: 0.8504118331011121]
	TIME [epoch: 1.77 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8288894918729611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8288894918729611 | validation: 0.7601490148443828]
	TIME [epoch: 1.77 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8107341215698891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8107341215698891 | validation: 0.8059662904329926]
	TIME [epoch: 1.77 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7958017444327772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7958017444327772 | validation: 0.7844874446901572]
	TIME [epoch: 1.77 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7835569259962327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7835569259962327 | validation: 0.785419705763453]
	TIME [epoch: 1.77 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7859400425933092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7859400425933092 | validation: 0.7625135197751972]
	TIME [epoch: 1.77 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.793044351617558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.793044351617558 | validation: 0.8334303255377459]
	TIME [epoch: 1.77 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8065850656980337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8065850656980337 | validation: 0.7952536063858653]
	TIME [epoch: 1.77 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8142935723774829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8142935723774829 | validation: 0.8400199620299095]
	TIME [epoch: 1.78 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8183440580584492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8183440580584492 | validation: 0.7840052921894171]
	TIME [epoch: 1.78 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8125410340959283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8125410340959283 | validation: 0.7429275310871]
	TIME [epoch: 1.77 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8018679528972572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8018679528972572 | validation: 0.8543015532082112]
	TIME [epoch: 1.78 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8336969090990985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8336969090990985 | validation: 0.7796108274585049]
	TIME [epoch: 1.77 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8979042506653753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8979042506653753 | validation: 0.9075737672079441]
	TIME [epoch: 1.77 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8648711224196362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8648711224196362 | validation: 0.7440029177444564]
	TIME [epoch: 1.77 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7853036692213502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7853036692213502 | validation: 0.7920188261161594]
	TIME [epoch: 1.78 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7664083633268604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7664083633268604 | validation: 0.7642106434230262]
	TIME [epoch: 1.77 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7684220160555498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7684220160555498 | validation: 0.7486977559585464]
	TIME [epoch: 1.78 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7685053236946457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7685053236946457 | validation: 0.7806412666132014]
	TIME [epoch: 1.77 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.782408565607958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.782408565607958 | validation: 0.7653784528185626]
	TIME [epoch: 1.78 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8094297119447746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8094297119447746 | validation: 0.8043050934465694]
	TIME [epoch: 1.78 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.838182209775751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.838182209775751 | validation: 0.8244289784914888]
	TIME [epoch: 1.78 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8382553114148696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8382553114148696 | validation: 0.7915591245341853]
	TIME [epoch: 1.77 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7813644198805288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7813644198805288 | validation: 0.849084392672308]
	TIME [epoch: 1.78 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8279558636520343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8279558636520343 | validation: 0.795792287294003]
	TIME [epoch: 1.77 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8638511623841705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8638511623841705 | validation: 0.9044032287166393]
	TIME [epoch: 1.77 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8359005907889828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8359005907889828 | validation: 0.7467370105441123]
	TIME [epoch: 1.78 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7812704117921245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7812704117921245 | validation: 0.7678321637816852]
	TIME [epoch: 1.78 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7915874131103965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7915874131103965 | validation: 0.8044496851491139]
	TIME [epoch: 1.77 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7844383082716959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7844383082716959 | validation: 0.7637142561598325]
	TIME [epoch: 1.77 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7791145215929758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7791145215929758 | validation: 0.7922785842538375]
	TIME [epoch: 1.77 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7791880115206636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7791880115206636 | validation: 0.7647552761071399]
	TIME [epoch: 1.77 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7896392609470212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7896392609470212 | validation: 0.8320395791654246]
	TIME [epoch: 1.77 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8122896469897917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8122896469897917 | validation: 0.7479558512058428]
	TIME [epoch: 1.77 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.777401952539062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.777401952539062 | validation: 0.7787053325187163]
	TIME [epoch: 1.78 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7761573630788097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7761573630788097 | validation: 0.7516227653147568]
	TIME [epoch: 1.78 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7758106390020734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7758106390020734 | validation: 0.8116948417417155]
	TIME [epoch: 1.77 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8003579619929282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8003579619929282 | validation: 0.7710860134535232]
	TIME [epoch: 1.77 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.85151282267841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.85151282267841 | validation: 0.9288172508459276]
	TIME [epoch: 1.77 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8805732447610447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8805732447610447 | validation: 0.7463495506331981]
	TIME [epoch: 1.77 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8453092410026257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8453092410026257 | validation: 0.8467641068550329]
	TIME [epoch: 1.77 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8225402422097936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8225402422097936 | validation: 0.7596437051559162]
	TIME [epoch: 1.77 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.780108660531296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.780108660531296 | validation: 0.7324795167851343]
	TIME [epoch: 1.77 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7734125905559638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7734125905559638 | validation: 0.8096096594552261]
	TIME [epoch: 1.78 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.778606469059244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.778606469059244 | validation: 0.7196084224703019]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_347.pth
	Model improved!!!
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7681212155045845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7681212155045845 | validation: 0.7913208693641587]
	TIME [epoch: 1.79 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7603327274680686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7603327274680686 | validation: 0.7156417594615109]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_349.pth
	Model improved!!!
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7722096376633301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7722096376633301 | validation: 0.8336749035746314]
	TIME [epoch: 1.78 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7890820423971343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7890820423971343 | validation: 0.756964252215472]
	TIME [epoch: 1.78 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8090357116791447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8090357116791447 | validation: 0.8525074409155107]
	TIME [epoch: 1.78 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8305337940042423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8305337940042423 | validation: 0.7716578928369777]
	TIME [epoch: 1.78 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7942471413950708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7942471413950708 | validation: 0.7667750729912379]
	TIME [epoch: 1.78 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8205539913417949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8205539913417949 | validation: 0.868775041345121]
	TIME [epoch: 1.78 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8267725740472373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8267725740472373 | validation: 0.7104840004872716]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_356.pth
	Model improved!!!
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8031963245972104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8031963245972104 | validation: 0.7630671117460617]
	TIME [epoch: 1.78 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7720604549028408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7720604549028408 | validation: 0.7476164241010677]
	TIME [epoch: 1.77 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7635239234445662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7635239234445662 | validation: 0.7478134676950637]
	TIME [epoch: 1.78 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7723901728313521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7723901728313521 | validation: 0.7460233147072034]
	TIME [epoch: 1.77 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7685260759107672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7685260759107672 | validation: 0.8009349609807095]
	TIME [epoch: 1.78 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7962757764388309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7962757764388309 | validation: 0.7592811615187531]
	TIME [epoch: 1.77 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7814989899321034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7814989899321034 | validation: 0.8038406076432658]
	TIME [epoch: 1.78 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7989423063031755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7989423063031755 | validation: 0.7314302298772728]
	TIME [epoch: 1.77 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7611951994921242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7611951994921242 | validation: 0.7516159942908733]
	TIME [epoch: 1.78 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7556361345121494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7556361345121494 | validation: 0.729938076766605]
	TIME [epoch: 1.77 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7600720328348713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7600720328348713 | validation: 0.7584470489590417]
	TIME [epoch: 1.78 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.790979110264136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.790979110264136 | validation: 0.8115438027763681]
	TIME [epoch: 1.77 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8194132960281042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8194132960281042 | validation: 0.7598474152420445]
	TIME [epoch: 1.78 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8070815453998449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8070815453998449 | validation: 0.7497553963866533]
	TIME [epoch: 1.77 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7637284568352475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7637284568352475 | validation: 0.7491629718888344]
	TIME [epoch: 1.78 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7479600691966999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7479600691966999 | validation: 0.7151954706091559]
	TIME [epoch: 1.78 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7480295442528986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7480295442528986 | validation: 0.7888646413288765]
	TIME [epoch: 1.78 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7675598536875461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7675598536875461 | validation: 0.7496942512070293]
	TIME [epoch: 1.78 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.868092255044391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.868092255044391 | validation: 1.1099709408492773]
	TIME [epoch: 1.78 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0073505168021717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0073505168021717 | validation: 0.7563086876959806]
	TIME [epoch: 1.78 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8098925123810208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8098925123810208 | validation: 0.7721014374628977]
	TIME [epoch: 1.78 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8201868224290686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8201868224290686 | validation: 0.8542046644696222]
	TIME [epoch: 1.77 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.810430038307585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.810430038307585 | validation: 0.7189845091850682]
	TIME [epoch: 1.77 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7660735594500335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7660735594500335 | validation: 0.7198216297948148]
	TIME [epoch: 1.77 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7573193633787466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7573193633787466 | validation: 0.7402721364730467]
	TIME [epoch: 1.77 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7515523915122406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7515523915122406 | validation: 0.7302719724263748]
	TIME [epoch: 1.77 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7439384553457936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7439384553457936 | validation: 0.7023124948926909]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_383.pth
	Model improved!!!
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.743815470970871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.743815470970871 | validation: 0.7624875066003738]
	TIME [epoch: 1.78 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7364056817682513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7364056817682513 | validation: 0.7105207597940248]
	TIME [epoch: 1.78 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7621162114953706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7621162114953706 | validation: 0.7784144893724355]
	TIME [epoch: 1.78 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7956979604958432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7956979604958432 | validation: 0.807556340896562]
	TIME [epoch: 1.78 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8750312534793366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8750312534793366 | validation: 0.7560410744176842]
	TIME [epoch: 1.78 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.767039082915191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.767039082915191 | validation: 0.7199241248366276]
	TIME [epoch: 1.78 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7286390789458465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7286390789458465 | validation: 0.6871105875466196]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_390.pth
	Model improved!!!
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7335714517782098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7335714517782098 | validation: 0.7724454845722587]
	TIME [epoch: 1.78 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7536854194685279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7536854194685279 | validation: 0.7273755326724032]
	TIME [epoch: 1.78 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8066592184233973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8066592184233973 | validation: 0.9901077311048461]
	TIME [epoch: 1.78 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9039636416169552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9039636416169552 | validation: 0.7199518139143397]
	TIME [epoch: 1.78 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7691120576070904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7691120576070904 | validation: 0.7075007527048327]
	TIME [epoch: 1.78 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7346734807261192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7346734807261192 | validation: 0.721747622173837]
	TIME [epoch: 1.79 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7303725155264155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7303725155264155 | validation: 0.6941000974331933]
	TIME [epoch: 1.78 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.730979577594198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.730979577594198 | validation: 0.7199903413931197]
	TIME [epoch: 1.79 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7301314346510567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7301314346510567 | validation: 0.7360292129286169]
	TIME [epoch: 1.79 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.761109823845392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.761109823845392 | validation: 0.8655526970706937]
	TIME [epoch: 1.78 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8724849813556392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8724849813556392 | validation: 0.8935753000581093]
	TIME [epoch: 1.78 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9301561863157929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9301561863157929 | validation: 0.7009811635922665]
	TIME [epoch: 1.77 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7205582054051455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7205582054051455 | validation: 0.7502464171310348]
	TIME [epoch: 1.77 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7783135212522047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7783135212522047 | validation: 0.7868874173820453]
	TIME [epoch: 1.77 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8007477833527188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8007477833527188 | validation: 0.7038832328773295]
	TIME [epoch: 1.78 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.73870477533932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.73870477533932 | validation: 0.7642590354122292]
	TIME [epoch: 1.78 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.774456472602995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.774456472602995 | validation: 0.7210284949725907]
	TIME [epoch: 1.77 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8049356319797468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8049356319797468 | validation: 0.8088398366589221]
	TIME [epoch: 1.77 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.773208603189222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.773208603189222 | validation: 0.6869443129977983]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_409.pth
	Model improved!!!
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7463099606277186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7463099606277186 | validation: 0.743914158487311]
	TIME [epoch: 1.78 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7268451645101496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7268451645101496 | validation: 0.6848931712125458]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_411.pth
	Model improved!!!
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.729085681808107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.729085681808107 | validation: 0.7051443629734235]
	TIME [epoch: 1.78 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7276118597874498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7276118597874498 | validation: 0.7304615418161984]
	TIME [epoch: 1.78 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7509622509059952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7509622509059952 | validation: 0.7590187873096864]
	TIME [epoch: 1.78 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7983002041878937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7983002041878937 | validation: 0.7926821530090786]
	TIME [epoch: 1.77 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8132633147593938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8132633147593938 | validation: 0.7867523762432662]
	TIME [epoch: 1.78 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8002523400123007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8002523400123007 | validation: 0.68857303169615]
	TIME [epoch: 1.77 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7343049569416116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7343049569416116 | validation: 0.7039796047503635]
	TIME [epoch: 1.78 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7263300878059952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7263300878059952 | validation: 0.6979502774728653]
	TIME [epoch: 1.78 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7247669097333533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7247669097333533 | validation: 0.6735946664882895]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_420.pth
	Model improved!!!
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7618560611959737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7618560611959737 | validation: 0.8765944390128797]
	TIME [epoch: 1.8 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8209922030546328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8209922030546328 | validation: 0.7062933185754083]
	TIME [epoch: 1.78 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8150213124656717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8150213124656717 | validation: 0.7738748021200611]
	TIME [epoch: 1.78 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7517978960222815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7517978960222815 | validation: 0.6781001118054439]
	TIME [epoch: 1.78 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7186166069784972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7186166069784972 | validation: 0.6806553913502961]
	TIME [epoch: 1.77 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7219483333194515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7219483333194515 | validation: 0.7270938941720602]
	TIME [epoch: 1.78 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7456784750225568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7456784750225568 | validation: 0.7171377867750003]
	TIME [epoch: 1.77 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7817510803396303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7817510803396303 | validation: 0.775544483299705]
	TIME [epoch: 1.78 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7941355228787099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7941355228787099 | validation: 0.7886384506508464]
	TIME [epoch: 1.78 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8052545073962576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8052545073962576 | validation: 0.7057235762501983]
	TIME [epoch: 1.78 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7332561784592533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7332561784592533 | validation: 0.7088087429616367]
	TIME [epoch: 1.78 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7353205344458423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7353205344458423 | validation: 0.6695004310893239]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_432.pth
	Model improved!!!
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7258567227788919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7258567227788919 | validation: 0.7269466710937773]
	TIME [epoch: 1.79 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7186765507562964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7186765507562964 | validation: 0.6723063583978098]
	TIME [epoch: 1.78 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7283317690623651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7283317690623651 | validation: 0.7345953096013469]
	TIME [epoch: 1.78 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7338085761406643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7338085761406643 | validation: 0.697425833025604]
	TIME [epoch: 1.77 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.754548366027028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.754548366027028 | validation: 0.8131068569050325]
	TIME [epoch: 1.78 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.77904785440785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.77904785440785 | validation: 0.7136819288598399]
	TIME [epoch: 1.77 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.752209349231431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.752209349231431 | validation: 0.7470997475975497]
	TIME [epoch: 1.78 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7926066217009538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7926066217009538 | validation: 0.7753422472452283]
	TIME [epoch: 1.77 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8084074635669007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8084074635669007 | validation: 0.67543624818531]
	TIME [epoch: 1.78 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7753622084736094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7753622084736094 | validation: 0.6976896801767334]
	TIME [epoch: 1.77 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7138600840796951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7138600840796951 | validation: 0.7028277991765164]
	TIME [epoch: 1.78 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.698652477485312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.698652477485312 | validation: 0.6684338898690232]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_444.pth
	Model improved!!!
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7055289829377601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7055289829377601 | validation: 0.692690893699502]
	TIME [epoch: 1.78 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7063402419570184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7063402419570184 | validation: 0.6817889468552536]
	TIME [epoch: 1.79 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7100032429475271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7100032429475271 | validation: 0.747177161601417]
	TIME [epoch: 1.78 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7557345803064921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7557345803064921 | validation: 0.9132704830904862]
	TIME [epoch: 1.78 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9233746421383424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9233746421383424 | validation: 0.7369709076957935]
	TIME [epoch: 1.78 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7717200072627856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7717200072627856 | validation: 0.6788807014060148]
	TIME [epoch: 1.78 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7034676186329578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7034676186329578 | validation: 0.673732157061071]
	TIME [epoch: 1.78 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6925534299283328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6925534299283328 | validation: 0.6758694722742887]
	TIME [epoch: 1.78 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7006747286517089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7006747286517089 | validation: 0.6539469228909566]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_453.pth
	Model improved!!!
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6977664931138575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6977664931138575 | validation: 0.7142269428342307]
	TIME [epoch: 1.78 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7217386330783947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7217386330783947 | validation: 0.7419808511771131]
	TIME [epoch: 1.78 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8458089012910691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8458089012910691 | validation: 0.7917739695709902]
	TIME [epoch: 1.78 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7865154547087074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7865154547087074 | validation: 0.7094494487733889]
	TIME [epoch: 1.77 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7473455434168236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7473455434168236 | validation: 0.6862316709351457]
	TIME [epoch: 1.77 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7362150870999746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7362150870999746 | validation: 0.8182771180252859]
	TIME [epoch: 1.77 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7959348118708018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7959348118708018 | validation: 0.6760534176481485]
	TIME [epoch: 1.78 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7457459403668338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7457459403668338 | validation: 0.6851243357039171]
	TIME [epoch: 1.77 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6956365084204077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6956365084204077 | validation: 0.631516123134727]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_462.pth
	Model improved!!!
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6816017998637298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6816017998637298 | validation: 0.6470014113405984]
	TIME [epoch: 1.78 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6792099242010667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6792099242010667 | validation: 0.6461273463355983]
	TIME [epoch: 1.77 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6860438861427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6860438861427 | validation: 0.6759052223419126]
	TIME [epoch: 1.77 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6797445182831204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6797445182831204 | validation: 0.7186431838523419]
	TIME [epoch: 1.77 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7425061002501392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7425061002501392 | validation: 0.9045288999376129]
	TIME [epoch: 1.77 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9043360817896056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9043360817896056 | validation: 0.8592429794658497]
	TIME [epoch: 1.78 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9026998286130131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9026998286130131 | validation: 0.6849408511963851]
	TIME [epoch: 1.78 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6761361360400636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6761361360400636 | validation: 0.7031270914146474]
	TIME [epoch: 1.78 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7307395888745125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7307395888745125 | validation: 0.7195816765421666]
	TIME [epoch: 1.79 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7504811529756457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7504811529756457 | validation: 0.6649171643021138]
	TIME [epoch: 1.78 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6752780732986793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6752780732986793 | validation: 0.6360467688921552]
	TIME [epoch: 1.77 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6669379710235507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6669379710235507 | validation: 0.6351968198043658]
	TIME [epoch: 1.77 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6655831883195807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6655831883195807 | validation: 0.6394391418705571]
	TIME [epoch: 1.77 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.671272147306201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.671272147306201 | validation: 0.6616323602701983]
	TIME [epoch: 1.77 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6961048083419772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6961048083419772 | validation: 0.7720683215231898]
	TIME [epoch: 1.77 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7623708649624712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7623708649624712 | validation: 0.7757229680322666]
	TIME [epoch: 1.78 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8611423451273603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8611423451273603 | validation: 0.7041806565830149]
	TIME [epoch: 1.77 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.71529289909104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.71529289909104 | validation: 0.6152887456909109]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_480.pth
	Model improved!!!
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6684340384300844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6684340384300844 | validation: 0.6479370009796426]
	TIME [epoch: 1.77 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6686519959591718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6686519959591718 | validation: 0.6255470194190211]
	TIME [epoch: 1.77 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.670529030361453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.670529030361453 | validation: 0.7017784846154475]
	TIME [epoch: 1.77 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6964144520843434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6964144520843434 | validation: 0.6518975577144351]
	TIME [epoch: 1.78 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7498186684240844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7498186684240844 | validation: 0.7285147875862537]
	TIME [epoch: 1.78 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7299970922352378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7299970922352378 | validation: 0.6028643821802496]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_486.pth
	Model improved!!!
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6574241872396038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6574241872396038 | validation: 0.5917460219217944]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_487.pth
	Model improved!!!
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6377297015000447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6377297015000447 | validation: 0.6320603249060217]
	TIME [epoch: 1.77 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.650448871450856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.650448871450856 | validation: 0.6601408387475325]
	TIME [epoch: 1.78 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6727144567814182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6727144567814182 | validation: 0.7399310552632271]
	TIME [epoch: 1.78 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8153096959033451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8153096959033451 | validation: 0.9071434593230283]
	TIME [epoch: 1.78 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8907585566153613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8907585566153613 | validation: 0.6494585129226965]
	TIME [epoch: 1.77 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7077824128308987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7077824128308987 | validation: 0.5858302679283892]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_493.pth
	Model improved!!!
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6475578485936625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6475578485936625 | validation: 0.6288001271255669]
	TIME [epoch: 1.79 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6541257359559223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6541257359559223 | validation: 0.6122444541697766]
	TIME [epoch: 1.79 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6605821724419034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6605821724419034 | validation: 0.6210981493914693]
	TIME [epoch: 1.79 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6427169424245295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6427169424245295 | validation: 0.5558868693865352]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_497.pth
	Model improved!!!
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6222678628112405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6222678628112405 | validation: 0.6466813641413051]
	TIME [epoch: 1.78 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6370541997580915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6370541997580915 | validation: 0.6071247824355929]
	TIME [epoch: 1.79 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7027887858552698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7027887858552698 | validation: 0.8518597086647337]
	TIME [epoch: 1.78 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.804617488980918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.804617488980918 | validation: 0.663831395524219]
	TIME [epoch: 25.8 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7106521033551286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7106521033551286 | validation: 0.5761937253471773]
	TIME [epoch: 3.53 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6191811890999184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6191811890999184 | validation: 0.5630375467800806]
	TIME [epoch: 3.52 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.608293464504719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.608293464504719 | validation: 0.5521930820747549]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_504.pth
	Model improved!!!
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5911987863181936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5911987863181936 | validation: 0.5702894237395147]
	TIME [epoch: 3.52 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5913902367203777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5913902367203777 | validation: 0.5790727505436202]
	TIME [epoch: 3.52 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6350911682446381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6350911682446381 | validation: 0.8645636103413606]
	TIME [epoch: 3.51 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8102185681752003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8102185681752003 | validation: 0.5751115224760388]
	TIME [epoch: 3.51 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6654762327733893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6654762327733893 | validation: 0.5228996925294805]
	TIME [epoch: 3.51 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_509.pth
	Model improved!!!
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5617821488742978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5617821488742978 | validation: 0.49937008516953196]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_510.pth
	Model improved!!!
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5374655153317831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5374655153317831 | validation: 0.47865093031819816]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_511.pth
	Model improved!!!
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5327951801050935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5327951801050935 | validation: 0.5857893502736962]
	TIME [epoch: 3.51 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5711058326613616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5711058326613616 | validation: 0.6675192216313978]
	TIME [epoch: 3.52 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8383674170837511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8383674170837511 | validation: 0.8834941875977935]
	TIME [epoch: 3.52 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8345929255716013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8345929255716013 | validation: 0.6840669261766513]
	TIME [epoch: 3.52 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.685140135541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.685140135541 | validation: 0.5619384257323198]
	TIME [epoch: 3.53 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6222264978527511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6222264978527511 | validation: 0.5233388184344391]
	TIME [epoch: 3.51 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5986104263838989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5986104263838989 | validation: 0.5104907151330921]
	TIME [epoch: 3.54 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5329951624985353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5329951624985353 | validation: 0.4831034588674059]
	TIME [epoch: 3.52 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5028897341345987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5028897341345987 | validation: 0.45863862926759913]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_520.pth
	Model improved!!!
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4865912278169164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4865912278169164 | validation: 0.4717665948448049]
	TIME [epoch: 3.51 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47442160530475314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47442160530475314 | validation: 0.49502089528917576]
	TIME [epoch: 3.52 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48290726477864127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48290726477864127 | validation: 0.43410408076899093]
	TIME [epoch: 3.51 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_523.pth
	Model improved!!!
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4922942224015052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4922942224015052 | validation: 0.5878906324783751]
	TIME [epoch: 3.53 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5587649361843424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5587649361843424 | validation: 0.47096237558338405]
	TIME [epoch: 3.58 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5020002456045742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5020002456045742 | validation: 0.45848203774561924]
	TIME [epoch: 3.53 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4342305693769765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4342305693769765 | validation: 0.5165231781105037]
	TIME [epoch: 3.52 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6166884317580594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6166884317580594 | validation: 0.8244026837252503]
	TIME [epoch: 3.53 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7649587648015722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7649587648015722 | validation: 0.6817072607102619]
	TIME [epoch: 3.52 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6680959736805606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6680959736805606 | validation: 0.559438723020697]
	TIME [epoch: 3.53 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6275440223108796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6275440223108796 | validation: 0.5165347940731966]
	TIME [epoch: 3.53 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5698489255514455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5698489255514455 | validation: 0.4719705846805954]
	TIME [epoch: 3.53 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4990617390215961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4990617390215961 | validation: 0.41628113593171734]
	TIME [epoch: 3.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_533.pth
	Model improved!!!
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42805219750739787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42805219750739787 | validation: 0.451516407317882]
	TIME [epoch: 3.53 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44972573220502454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44972573220502454 | validation: 0.5269750812088944]
	TIME [epoch: 3.52 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4987765774330636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4987765774330636 | validation: 0.4839083889310935]
	TIME [epoch: 3.54 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5185401011353497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5185401011353497 | validation: 0.6000026807643968]
	TIME [epoch: 3.52 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5620096920824633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5620096920824633 | validation: 0.3759419816961287]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_538.pth
	Model improved!!!
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39944884330492664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39944884330492664 | validation: 0.37113910120934845]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_539.pth
	Model improved!!!
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38620752313708295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38620752313708295 | validation: 0.5084364529175318]
	TIME [epoch: 3.53 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4862602235461945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4862602235461945 | validation: 0.4456187452339935]
	TIME [epoch: 3.53 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49960888036949785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49960888036949785 | validation: 0.5963923220905198]
	TIME [epoch: 3.52 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5723422058855243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5723422058855243 | validation: 0.4401858928091696]
	TIME [epoch: 3.55 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46951529390140373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46951529390140373 | validation: 0.36842297310223104]
	TIME [epoch: 3.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_544.pth
	Model improved!!!
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41462399751532175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41462399751532175 | validation: 0.387440439786614]
	TIME [epoch: 3.55 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3702758410899284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3702758410899284 | validation: 0.40722937327463904]
	TIME [epoch: 3.51 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39976376126019225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39976376126019225 | validation: 0.6264552711109213]
	TIME [epoch: 3.52 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5851565758696209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5851565758696209 | validation: 0.3893746353492679]
	TIME [epoch: 3.52 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43495611011983315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43495611011983315 | validation: 0.39256873134203324]
	TIME [epoch: 3.52 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4493828402361869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4493828402361869 | validation: 0.5374618908203461]
	TIME [epoch: 3.52 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5044530496919328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5044530496919328 | validation: 0.3975805194717537]
	TIME [epoch: 3.52 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.376067253979289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.376067253979289 | validation: 0.3279012319220409]
	TIME [epoch: 3.51 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_552.pth
	Model improved!!!
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3149311096655519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3149311096655519 | validation: 0.4089349715908885]
	TIME [epoch: 3.51 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3729986029588676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3729986029588676 | validation: 0.5027902212901857]
	TIME [epoch: 3.52 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5588906571241902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5588906571241902 | validation: 0.6150346437349437]
	TIME [epoch: 3.52 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5755812059437136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5755812059437136 | validation: 0.47654539575832633]
	TIME [epoch: 3.52 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49373899515890046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49373899515890046 | validation: 0.33811633800288965]
	TIME [epoch: 3.53 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3812155687508576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3812155687508576 | validation: 0.33682310994485304]
	TIME [epoch: 3.52 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33848386874584263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33848386874584263 | validation: 0.46308244065687465]
	TIME [epoch: 3.51 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40009661478467023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40009661478467023 | validation: 0.44360006484821357]
	TIME [epoch: 3.51 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4584582307195815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4584582307195815 | validation: 0.4139022923011448]
	TIME [epoch: 3.53 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3872717168334513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3872717168334513 | validation: 0.28859661735048475]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_562.pth
	Model improved!!!
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2884495839465476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2884495839465476 | validation: 0.32810841140047153]
	TIME [epoch: 3.52 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3100145709678877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3100145709678877 | validation: 0.45824215603407614]
	TIME [epoch: 3.52 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42190457500461015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42190457500461015 | validation: 0.34906938580053515]
	TIME [epoch: 3.51 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3413487180894332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3413487180894332 | validation: 0.3156402847464206]
	TIME [epoch: 3.52 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28112464937575043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28112464937575043 | validation: 0.32985353206622503]
	TIME [epoch: 3.52 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31388076244267044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31388076244267044 | validation: 0.518701166319972]
	TIME [epoch: 3.51 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48479782131314414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48479782131314414 | validation: 0.29694560153262667]
	TIME [epoch: 3.52 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29792664293527266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29792664293527266 | validation: 0.3288669896264431]
	TIME [epoch: 3.54 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34070586084917925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34070586084917925 | validation: 0.5696612873485541]
	TIME [epoch: 3.51 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5368488091713405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5368488091713405 | validation: 0.3571696628271894]
	TIME [epoch: 3.51 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3605682790177947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3605682790177947 | validation: 0.31159157726001535]
	TIME [epoch: 3.52 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3084377818677892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3084377818677892 | validation: 0.5036820975381451]
	TIME [epoch: 3.52 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4912601468157291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4912601468157291 | validation: 0.2741093418174632]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_575.pth
	Model improved!!!
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27301545105447683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27301545105447683 | validation: 0.24318906114946381]
	TIME [epoch: 3.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_576.pth
	Model improved!!!
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23087098937785056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23087098937785056 | validation: 0.2884496974328289]
	TIME [epoch: 3.52 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23821559760887784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23821559760887784 | validation: 0.3418038032473043]
	TIME [epoch: 3.51 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31802355933039783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31802355933039783 | validation: 0.5587392032657746]
	TIME [epoch: 3.53 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5359994160410494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5359994160410494 | validation: 0.2647430606360383]
	TIME [epoch: 3.51 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28154064491113934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28154064491113934 | validation: 0.4309977200966472]
	TIME [epoch: 3.52 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4250941292563843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4250941292563843 | validation: 0.47775236680954025]
	TIME [epoch: 3.53 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44148277404953706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44148277404953706 | validation: 0.2645013540888072]
	TIME [epoch: 3.53 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2629642988554848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2629642988554848 | validation: 0.27864472518215555]
	TIME [epoch: 3.53 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26549600906000487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26549600906000487 | validation: 0.3725167751204485]
	TIME [epoch: 3.52 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3213430591645782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3213430591645782 | validation: 0.3415354459190554]
	TIME [epoch: 3.52 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2994595938154267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2994595938154267 | validation: 0.2600399392998104]
	TIME [epoch: 3.52 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22929251238796255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22929251238796255 | validation: 0.22770669391094578]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_588.pth
	Model improved!!!
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20543254316522358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20543254316522358 | validation: 0.2506352483152206]
	TIME [epoch: 3.52 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20072900103884028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20072900103884028 | validation: 0.29392633558931064]
	TIME [epoch: 3.52 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2550721209275991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2550721209275991 | validation: 0.4535571078541855]
	TIME [epoch: 3.51 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44263190053030727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44263190053030727 | validation: 0.2735565071904434]
	TIME [epoch: 3.51 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28408445404904686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28408445404904686 | validation: 0.2249102051130738]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_593.pth
	Model improved!!!
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19290767009445112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19290767009445112 | validation: 0.22639031155357703]
	TIME [epoch: 3.53 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18539916608695614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18539916608695614 | validation: 0.32104100421542353]
	TIME [epoch: 3.54 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28011066552055375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28011066552055375 | validation: 0.4829116066499184]
	TIME [epoch: 3.54 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4628653159514051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4628653159514051 | validation: 0.39595182188094447]
	TIME [epoch: 3.53 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38324049421025896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38324049421025896 | validation: 0.2081038970158845]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_598.pth
	Model improved!!!
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21402091290385186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21402091290385186 | validation: 0.31393422845712093]
	TIME [epoch: 3.52 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26574420108311053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26574420108311053 | validation: 0.37049850038073945]
	TIME [epoch: 3.52 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3098099769953415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3098099769953415 | validation: 0.30694346938477024]
	TIME [epoch: 3.52 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28380802816497197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28380802816497197 | validation: 0.2253551063561428]
	TIME [epoch: 3.53 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1756603772094819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1756603772094819 | validation: 0.22303039024577698]
	TIME [epoch: 3.52 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1725615436280568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1725615436280568 | validation: 0.27943728427470566]
	TIME [epoch: 3.53 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24467397475807673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24467397475807673 | validation: 0.3636289327357427]
	TIME [epoch: 3.52 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32475754329327494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32475754329327494 | validation: 0.32988936612891523]
	TIME [epoch: 3.53 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2906446459273459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2906446459273459 | validation: 0.253067637728501]
	TIME [epoch: 3.53 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21520857319650805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21520857319650805 | validation: 0.21177543223036066]
	TIME [epoch: 3.53 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1777057913683244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1777057913683244 | validation: 0.22981207062579842]
	TIME [epoch: 3.54 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19363695239731052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19363695239731052 | validation: 0.3197522379159552]
	TIME [epoch: 3.52 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2711911774120494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2711911774120494 | validation: 0.3525057149185592]
	TIME [epoch: 3.53 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32704349000025906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32704349000025906 | validation: 0.23239964435234164]
	TIME [epoch: 3.51 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20787291619727152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20787291619727152 | validation: 0.22410635822336902]
	TIME [epoch: 3.53 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15685902069992355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15685902069992355 | validation: 0.19816935383506437]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_614.pth
	Model improved!!!
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15425011071941888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15425011071941888 | validation: 0.2054875370511355]
	TIME [epoch: 3.54 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1386977899498205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1386977899498205 | validation: 0.14435079903436437]
	TIME [epoch: 3.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_616.pth
	Model improved!!!
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13268979244689935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13268979244689935 | validation: 0.2334522643490023]
	TIME [epoch: 3.52 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18997294046340696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18997294046340696 | validation: 0.6515334074861875]
	TIME [epoch: 3.53 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6422871666400548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6422871666400548 | validation: 0.27029762827914106]
	TIME [epoch: 3.51 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32291268175224025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32291268175224025 | validation: 0.493545084337619]
	TIME [epoch: 3.54 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46544935705314894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46544935705314894 | validation: 0.4454555921887604]
	TIME [epoch: 3.52 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41606641112744497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41606641112744497 | validation: 0.22587470422699402]
	TIME [epoch: 3.52 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23323204716936607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23323204716936607 | validation: 0.23064059575986784]
	TIME [epoch: 3.52 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1770426708432877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1770426708432877 | validation: 0.3096457829791738]
	TIME [epoch: 3.52 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2497579965418969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2497579965418969 | validation: 0.2590042234504182]
	TIME [epoch: 3.52 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21430268830476887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21430268830476887 | validation: 0.20484399666691613]
	TIME [epoch: 3.52 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1759710735327393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1759710735327393 | validation: 0.17350822262028112]
	TIME [epoch: 3.53 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13417768006008593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13417768006008593 | validation: 0.16325859696103287]
	TIME [epoch: 3.51 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12550422439054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12550422439054 | validation: 0.15227786153353884]
	TIME [epoch: 3.52 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11856072504659353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11856072504659353 | validation: 0.16787461190464306]
	TIME [epoch: 3.51 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1294913018255272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1294913018255272 | validation: 0.38319156803902144]
	TIME [epoch: 3.53 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3354559937895148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3354559937895148 | validation: 0.594024031837939]
	TIME [epoch: 3.52 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5478368132917031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5478368132917031 | validation: 0.15869317271013444]
	TIME [epoch: 3.53 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1975431369980735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1975431369980735 | validation: 0.3552115768937194]
	TIME [epoch: 3.54 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3000576198105332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3000576198105332 | validation: 0.34922200615929877]
	TIME [epoch: 3.53 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31016880520037526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31016880520037526 | validation: 0.25124602087611514]
	TIME [epoch: 3.53 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23388746610933495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23388746610933495 | validation: 0.1963693173788255]
	TIME [epoch: 3.51 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1467840457304235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1467840457304235 | validation: 0.24485840019878277]
	TIME [epoch: 3.52 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19048009483586864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19048009483586864 | validation: 0.2555172351089922]
	TIME [epoch: 3.51 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21998449400549622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21998449400549622 | validation: 0.18736211247567938]
	TIME [epoch: 3.53 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16067290505613424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16067290505613424 | validation: 0.1802758960454579]
	TIME [epoch: 3.51 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13149485875747555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13149485875747555 | validation: 0.16894402704770978]
	TIME [epoch: 3.52 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13318045747947616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13318045747947616 | validation: 0.2510956446512874]
	TIME [epoch: 3.52 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1963190377333861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1963190377333861 | validation: 0.35026257160499263]
	TIME [epoch: 3.52 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35535644762847485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35535644762847485 | validation: 0.23859658397009786]
	TIME [epoch: 3.51 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19799548776246542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19799548776246542 | validation: 0.1509546825352175]
	TIME [epoch: 3.52 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1222355454596158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1222355454596158 | validation: 0.15738153966002438]
	TIME [epoch: 3.52 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11101202975119204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11101202975119204 | validation: 0.13846468618445149]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_648.pth
	Model improved!!!
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10009958154673285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10009958154673285 | validation: 0.13421880951728912]
	TIME [epoch: 3.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_649.pth
	Model improved!!!
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09364311368257713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09364311368257713 | validation: 0.11558366656397086]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_650.pth
	Model improved!!!
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09699747806982273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09699747806982273 | validation: 0.34793151761274993]
	TIME [epoch: 3.52 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29360934375788544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29360934375788544 | validation: 0.7003921290042675]
	TIME [epoch: 3.52 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6993685128020536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6993685128020536 | validation: 0.456196567740319]
	TIME [epoch: 3.52 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5141952464800563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5141952464800563 | validation: 0.36126501562555213]
	TIME [epoch: 3.52 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.318792474316884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.318792474316884 | validation: 0.22480176530397467]
	TIME [epoch: 3.52 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1577857711779402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1577857711779402 | validation: 0.20753803159441928]
	TIME [epoch: 3.52 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.156856780467126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.156856780467126 | validation: 0.21636199270128964]
	TIME [epoch: 3.52 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1577221083504443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1577221083504443 | validation: 0.18968342436376284]
	TIME [epoch: 3.52 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14857805606022645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14857805606022645 | validation: 0.23700997282060268]
	TIME [epoch: 3.53 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19027379670402186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19027379670402186 | validation: 0.21889834295894106]
	TIME [epoch: 3.52 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21007723649651106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21007723649651106 | validation: 0.2376653278170962]
	TIME [epoch: 3.52 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1921393283199376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1921393283199376 | validation: 0.16430783880620317]
	TIME [epoch: 3.52 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14286472931430877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14286472931430877 | validation: 0.17532828435536332]
	TIME [epoch: 3.51 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12837932802771188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12837932802771188 | validation: 0.17095208547207494]
	TIME [epoch: 3.52 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14876148199866568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14876148199866568 | validation: 0.24219257297037952]
	TIME [epoch: 3.52 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19225383793465345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19225383793465345 | validation: 0.23251573545315696]
	TIME [epoch: 3.52 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23580573863918708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23580573863918708 | validation: 0.2411952663404888]
	TIME [epoch: 3.52 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17049400603247708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17049400603247708 | validation: 0.1448863522472953]
	TIME [epoch: 3.52 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12338326805711704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12338326805711704 | validation: 0.15957884431215497]
	TIME [epoch: 3.52 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1049433234164724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1049433234164724 | validation: 0.13451506034952815]
	TIME [epoch: 3.52 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10543484806409653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10543484806409653 | validation: 0.16844253669143294]
	TIME [epoch: 3.51 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1394526576403208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1394526576403208 | validation: 0.29468047577045475]
	TIME [epoch: 3.53 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30043922347942553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30043922347942553 | validation: 0.2860175809834754]
	TIME [epoch: 3.52 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2662750633714469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2662750633714469 | validation: 0.15304686059663555]
	TIME [epoch: 3.52 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12565926708726777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12565926708726777 | validation: 0.12000346438076215]
	TIME [epoch: 3.51 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09821179033899569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09821179033899569 | validation: 0.14882888266020985]
	TIME [epoch: 3.52 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09885321717981178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09885321717981178 | validation: 0.13863063579684518]
	TIME [epoch: 3.52 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14075097269540324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14075097269540324 | validation: 0.286284311631334]
	TIME [epoch: 3.52 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2201662063541251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2201662063541251 | validation: 0.2648191393974321]
	TIME [epoch: 3.51 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2661101917093173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2661101917093173 | validation: 0.16330965575976567]
	TIME [epoch: 3.52 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13079646484717788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13079646484717788 | validation: 0.12985302085980951]
	TIME [epoch: 3.52 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08326878942117936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08326878942117936 | validation: 0.12753250858427162]
	TIME [epoch: 3.52 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0971209763204563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0971209763204563 | validation: 0.19228856757063928]
	TIME [epoch: 3.52 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14800438306537844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14800438306537844 | validation: 0.2359336041692048]
	TIME [epoch: 3.53 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24263841752741222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24263841752741222 | validation: 0.1660831351872926]
	TIME [epoch: 3.53 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1405022690271888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1405022690271888 | validation: 0.17031268807770938]
	TIME [epoch: 3.53 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11196896824025925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11196896824025925 | validation: 0.13963256909261693]
	TIME [epoch: 3.52 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15021283078143532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15021283078143532 | validation: 0.18244630805806802]
	TIME [epoch: 3.53 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13760805604619333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13760805604619333 | validation: 0.08524719209869029]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_689.pth
	Model improved!!!
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08307430453815323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08307430453815323 | validation: 0.12350846286376414]
	TIME [epoch: 3.53 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11641223750984453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11641223750984453 | validation: 0.4235590062412154]
	TIME [epoch: 3.52 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3780323484126839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3780323484126839 | validation: 0.3228322315473315]
	TIME [epoch: 3.52 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3395807505241071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3395807505241071 | validation: 0.09358710543422405]
	TIME [epoch: 3.52 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12130469505084174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12130469505084174 | validation: 0.20972573506726047]
	TIME [epoch: 3.52 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1751387841369462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1751387841369462 | validation: 0.21829444743020077]
	TIME [epoch: 3.52 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19648410533103045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19648410533103045 | validation: 0.17814727508343733]
	TIME [epoch: 3.52 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15571604852857657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15571604852857657 | validation: 0.11707433486527097]
	TIME [epoch: 3.53 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0780809519309663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0780809519309663 | validation: 0.11829787955401048]
	TIME [epoch: 3.53 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0968228096445738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0968228096445738 | validation: 0.15431383746498328]
	TIME [epoch: 3.53 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11706395960314535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11706395960314535 | validation: 0.1426782108128752]
	TIME [epoch: 3.53 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13985351635296372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13985351635296372 | validation: 0.16759224079435145]
	TIME [epoch: 3.52 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1297715206512805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1297715206512805 | validation: 0.13485451214432115]
	TIME [epoch: 3.52 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1224149561447698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1224149561447698 | validation: 0.14132894553212427]
	TIME [epoch: 3.52 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11204457370903875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11204457370903875 | validation: 0.14883386433063492]
	TIME [epoch: 3.52 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1256839806773059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1256839806773059 | validation: 0.1887054703013188]
	TIME [epoch: 3.53 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20419559391282427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20419559391282427 | validation: 0.19360759690449506]
	TIME [epoch: 3.53 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17845087648425342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17845087648425342 | validation: 0.10448238295467421]
	TIME [epoch: 3.54 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09115608735271924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09115608735271924 | validation: 0.08352889990722945]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_708.pth
	Model improved!!!
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.066266786064529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.066266786064529 | validation: 0.15016014621208446]
	TIME [epoch: 3.53 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1023349436441644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1023349436441644 | validation: 0.2093672065001031]
	TIME [epoch: 3.53 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.230161114991824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.230161114991824 | validation: 0.32559641560206826]
	TIME [epoch: 3.54 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26986076241935886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26986076241935886 | validation: 0.14576585136832163]
	TIME [epoch: 3.53 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16096746655687624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16096746655687624 | validation: 0.0814187551909014]
	TIME [epoch: 3.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_713.pth
	Model improved!!!
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06366876881722446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06366876881722446 | validation: 0.1518017499327785]
	TIME [epoch: 3.53 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10152843378172896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10152843378172896 | validation: 0.1378726125734139]
	TIME [epoch: 3.52 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1373717104356723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1373717104356723 | validation: 0.12509404347063874]
	TIME [epoch: 3.52 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09236183478805336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09236183478805336 | validation: 0.11409775178949362]
	TIME [epoch: 3.53 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07718252409730786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07718252409730786 | validation: 0.08909303795669053]
	TIME [epoch: 3.53 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08541315284370068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08541315284370068 | validation: 0.15042412706794953]
	TIME [epoch: 3.52 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13029841645813084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13029841645813084 | validation: 0.14921674957959666]
	TIME [epoch: 3.52 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1883683528324444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1883683528324444 | validation: 0.19061044569031987]
	TIME [epoch: 3.52 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19358088995810824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19358088995810824 | validation: 0.22239099617799551]
	TIME [epoch: 3.52 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1857480514083695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1857480514083695 | validation: 0.13286871223056324]
	TIME [epoch: 3.53 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12117396401992256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12117396401992256 | validation: 0.12865515079772125]
	TIME [epoch: 3.53 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10756698561905986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10756698561905986 | validation: 0.0749150441713919]
	TIME [epoch: 3.54 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_725.pth
	Model improved!!!
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07475205287946078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07475205287946078 | validation: 0.07988062242892585]
	TIME [epoch: 3.51 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05525811156216281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05525811156216281 | validation: 0.06675584445785425]
	TIME [epoch: 3.51 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_727.pth
	Model improved!!!
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05975245424354308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05975245424354308 | validation: 0.09716299946140991]
	TIME [epoch: 3.51 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09230066588228145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09230066588228145 | validation: 0.20441363825973885]
	TIME [epoch: 3.51 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21366933287100298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21366933287100298 | validation: 0.2714877320536809]
	TIME [epoch: 3.51 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23667865062309737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23667865062309737 | validation: 0.09463766326324859]
	TIME [epoch: 3.51 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10756535477125909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10756535477125909 | validation: 0.11305157043122477]
	TIME [epoch: 3.51 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08525856843863278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08525856843863278 | validation: 0.1302341054572625]
	TIME [epoch: 3.51 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12902536464444497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12902536464444497 | validation: 0.15835964558761928]
	TIME [epoch: 3.52 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14512942298605183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14512942298605183 | validation: 0.09674651278816487]
	TIME [epoch: 3.51 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09377617064064793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09377617064064793 | validation: 0.08101212168134825]
	TIME [epoch: 3.52 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06279543916891765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06279543916891765 | validation: 0.0544547144875878]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_737.pth
	Model improved!!!
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04714593038311793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04714593038311793 | validation: 0.05530422122426457]
	TIME [epoch: 3.53 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038274682182749376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038274682182749376 | validation: 0.04541107933379414]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_739.pth
	Model improved!!!
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03861129639424854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03861129639424854 | validation: 0.04994439454063688]
	TIME [epoch: 3.53 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04535155878936499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04535155878936499 | validation: 0.06131947272165333]
	TIME [epoch: 3.52 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0716544883292623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0716544883292623 | validation: 0.2680178309686669]
	TIME [epoch: 3.53 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2814694924025798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2814694924025798 | validation: 0.4037548739456945]
	TIME [epoch: 3.53 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40649657427323355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40649657427323355 | validation: 0.12824521972057965]
	TIME [epoch: 3.52 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11059887112433174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11059887112433174 | validation: 0.23024388770437285]
	TIME [epoch: 3.52 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21703848055051134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21703848055051134 | validation: 0.25105494677359]
	TIME [epoch: 3.52 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1895828870906435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1895828870906435 | validation: 0.10674009115849192]
	TIME [epoch: 3.53 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13256432995734302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13256432995734302 | validation: 0.14282309458728887]
	TIME [epoch: 3.53 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13776404614378351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13776404614378351 | validation: 0.13301254401689905]
	TIME [epoch: 3.53 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11839563385310457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11839563385310457 | validation: 0.07866962485676215]
	TIME [epoch: 3.54 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06754336716497462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06754336716497462 | validation: 0.07360624722467062]
	TIME [epoch: 3.52 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05376537779959943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05376537779959943 | validation: 0.06958854558933103]
	TIME [epoch: 3.52 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05545415152569636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05545415152569636 | validation: 0.09505920904812475]
	TIME [epoch: 3.53 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07208180146226811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07208180146226811 | validation: 0.08447571469462738]
	TIME [epoch: 3.52 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10011916757973786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10011916757973786 | validation: 0.19570047881614397]
	TIME [epoch: 3.52 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1738446447437791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1738446447437791 | validation: 0.24313130875849148]
	TIME [epoch: 3.52 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2780496569854665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2780496569854665 | validation: 0.15363502231624057]
	TIME [epoch: 3.53 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14181740373773025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14181740373773025 | validation: 0.06338330045571117]
	TIME [epoch: 3.51 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051723119773731646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051723119773731646 | validation: 0.07675896137693179]
	TIME [epoch: 3.53 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06383393516939281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06383393516939281 | validation: 0.07577046104582624]
	TIME [epoch: 3.52 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060244318814590914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060244318814590914 | validation: 0.0635149179507074]
	TIME [epoch: 3.53 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06001355473741705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06001355473741705 | validation: 0.10606125955007091]
	TIME [epoch: 3.53 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08325677209304078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08325677209304078 | validation: 0.1514567502264133]
	TIME [epoch: 3.54 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18025764835795982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18025764835795982 | validation: 0.19181871755012292]
	TIME [epoch: 3.54 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21176763089782932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21176763089782932 | validation: 0.08662649546796596]
	TIME [epoch: 3.53 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07838486387619016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07838486387619016 | validation: 0.09083413861108755]
	TIME [epoch: 3.54 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0609323009646779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0609323009646779 | validation: 0.08334095041246668]
	TIME [epoch: 3.53 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08504433280119883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08504433280119883 | validation: 0.13532419810585533]
	TIME [epoch: 3.54 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11188820189403824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11188820189403824 | validation: 0.11770689030945634]
	TIME [epoch: 3.51 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12484841317634321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12484841317634321 | validation: 0.20485899108325017]
	TIME [epoch: 3.52 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1617911902022528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1617911902022528 | validation: 0.109873119528792]
	TIME [epoch: 3.52 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12291727360293082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12291727360293082 | validation: 0.09185668843062655]
	TIME [epoch: 3.52 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06651355210449016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06651355210449016 | validation: 0.0528085170763195]
	TIME [epoch: 3.53 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04197962063323768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04197962063323768 | validation: 0.04340438610175292]
	TIME [epoch: 3.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_774.pth
	Model improved!!!
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03643008478453025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03643008478453025 | validation: 0.04543696237981567]
	TIME [epoch: 3.54 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039059623586912555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039059623586912555 | validation: 0.055004501635775666]
	TIME [epoch: 3.53 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06317529866321044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06317529866321044 | validation: 0.18854338195000167]
	TIME [epoch: 3.52 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20849704777317815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20849704777317815 | validation: 0.28706874658141057]
	TIME [epoch: 3.52 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3191368085913027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3191368085913027 | validation: 0.08588766522071063]
	TIME [epoch: 3.52 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0978029121343758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0978029121343758 | validation: 0.071498489726033]
	TIME [epoch: 3.53 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04826366516636605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04826366516636605 | validation: 0.07759910987762944]
	TIME [epoch: 3.53 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08290767930574668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08290767930574668 | validation: 0.09329456808548059]
	TIME [epoch: 3.54 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09279176923329965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09279176923329965 | validation: 0.07274740257898926]
	TIME [epoch: 3.52 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0705730725225095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0705730725225095 | validation: 0.06875169819541614]
	TIME [epoch: 3.53 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053958670470885525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053958670470885525 | validation: 0.05128655437257321]
	TIME [epoch: 3.53 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05018705740981027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05018705740981027 | validation: 0.0844239266394314]
	TIME [epoch: 3.53 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06676220476131588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06676220476131588 | validation: 0.1337092706338945]
	TIME [epoch: 3.55 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1462698169510734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1462698169510734 | validation: 0.3189632069451278]
	TIME [epoch: 3.53 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30032278571838894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30032278571838894 | validation: 0.21279245429720764]
	TIME [epoch: 3.55 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2483240835511715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2483240835511715 | validation: 0.055612789959026056]
	TIME [epoch: 3.52 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07172980592264562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07172980592264562 | validation: 0.10429590464715062]
	TIME [epoch: 3.53 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08970270299629558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08970270299629558 | validation: 0.09953532902921564]
	TIME [epoch: 3.53 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08751102267038985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08751102267038985 | validation: 0.08535983586296438]
	TIME [epoch: 3.52 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07441445315525547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07441445315525547 | validation: 0.06006776524885058]
	TIME [epoch: 3.53 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04796409263825376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04796409263825376 | validation: 0.05649092077723657]
	TIME [epoch: 3.53 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05147618973633822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05147618973633822 | validation: 0.07271368879937741]
	TIME [epoch: 3.53 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06338961992249706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06338961992249706 | validation: 0.07172530253812463]
	TIME [epoch: 3.52 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08276954260332861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08276954260332861 | validation: 0.12507465316114222]
	TIME [epoch: 3.52 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11519746713108553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11519746713108553 | validation: 0.11772230002039596]
	TIME [epoch: 3.52 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1442990646068021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1442990646068021 | validation: 0.11435083200203662]
	TIME [epoch: 3.51 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12362994360707066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12362994360707066 | validation: 0.126791347607174]
	TIME [epoch: 3.53 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10093083254048774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10093083254048774 | validation: 0.08895164469026678]
	TIME [epoch: 3.54 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08362982825134357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08362982825134357 | validation: 0.14052531246867006]
	TIME [epoch: 3.53 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10860318039832148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10860318039832148 | validation: 0.08505630096298689]
	TIME [epoch: 3.53 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0956221905621572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0956221905621572 | validation: 0.08841115609928052]
	TIME [epoch: 3.53 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0670062102928438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0670062102928438 | validation: 0.06731882159546515]
	TIME [epoch: 3.53 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05111696495311865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05111696495311865 | validation: 0.0708273941167088]
	TIME [epoch: 3.53 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06248470680529826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06248470680529826 | validation: 0.08984170017272636]
	TIME [epoch: 3.53 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10119945799280704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10119945799280704 | validation: 0.11231485224154261]
	TIME [epoch: 3.53 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09776705945829565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09776705945829565 | validation: 0.104046920719103]
	TIME [epoch: 3.54 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10429322798769086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10429322798769086 | validation: 0.08784223105555326]
	TIME [epoch: 3.53 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0773343174718664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0773343174718664 | validation: 0.053880392488858245]
	TIME [epoch: 3.53 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04602586410328735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04602586410328735 | validation: 0.04353229210802392]
	TIME [epoch: 3.54 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04134122515635838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04134122515635838 | validation: 0.08849939063041688]
	TIME [epoch: 3.54 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07410188531912361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07410188531912361 | validation: 0.14932522906291562]
	TIME [epoch: 3.55 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1732879820894167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1732879820894167 | validation: 0.17725553334965866]
	TIME [epoch: 3.53 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15562488111023653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15562488111023653 | validation: 0.05258718209081081]
	TIME [epoch: 3.53 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046718795823617806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046718795823617806 | validation: 0.056520381990089644]
	TIME [epoch: 3.53 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05798843439995269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05798843439995269 | validation: 0.1587590067867811]
	TIME [epoch: 3.54 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18818718641173857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18818718641173857 | validation: 0.14948366909430608]
	TIME [epoch: 3.52 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1543216031105532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1543216031105532 | validation: 0.0603920623281091]
	TIME [epoch: 3.53 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05869259543075352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05869259543075352 | validation: 0.05372370456357163]
	TIME [epoch: 3.53 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03887542662552782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03887542662552782 | validation: 0.04850285749428526]
	TIME [epoch: 3.52 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045686390882597565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045686390882597565 | validation: 0.05896376547826535]
	TIME [epoch: 3.53 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05450363891217846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05450363891217846 | validation: 0.04651247361744998]
	TIME [epoch: 3.53 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052106326388471985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052106326388471985 | validation: 0.06695999056719133]
	TIME [epoch: 3.53 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06152460037642274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06152460037642274 | validation: 0.07730390600386006]
	TIME [epoch: 3.54 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08893253143235397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08893253143235397 | validation: 0.1449761900684758]
	TIME [epoch: 3.54 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.159873883759667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.159873883759667 | validation: 0.17246004068192963]
	TIME [epoch: 3.53 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1458668402463266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1458668402463266 | validation: 0.0892157579828923]
	TIME [epoch: 3.53 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07940940664305837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07940940664305837 | validation: 0.1191844103008612]
	TIME [epoch: 3.53 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10367311141875969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10367311141875969 | validation: 0.11346476077078622]
	TIME [epoch: 3.53 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13836543169003343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13836543169003343 | validation: 0.10793898498883646]
	TIME [epoch: 3.53 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09121227096512191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09121227096512191 | validation: 0.06126403997986272]
	TIME [epoch: 3.53 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05315219616522437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05315219616522437 | validation: 0.058637721640490595]
	TIME [epoch: 3.53 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04683303666786633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04683303666786633 | validation: 0.055429833811247955]
	TIME [epoch: 3.53 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05210921553992419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05210921553992419 | validation: 0.06624014645672041]
	TIME [epoch: 3.53 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0626319373992586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0626319373992586 | validation: 0.0852151762892804]
	TIME [epoch: 3.53 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09121705152325467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09121705152325467 | validation: 0.09912153471741436]
	TIME [epoch: 3.53 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08962566641739443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08962566641739443 | validation: 0.0874539176805971]
	TIME [epoch: 3.54 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08268434895968767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08268434895968767 | validation: 0.10786390858068683]
	TIME [epoch: 3.54 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08322328861461854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08322328861461854 | validation: 0.09310694460733092]
	TIME [epoch: 3.53 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1005777926055779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1005777926055779 | validation: 0.14818601630525616]
	TIME [epoch: 3.53 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13708036769307597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13708036769307597 | validation: 0.058833197608984845]
	TIME [epoch: 3.53 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0799268706287359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0799268706287359 | validation: 0.045136237323473535]
	TIME [epoch: 3.54 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03915368806905196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03915368806905196 | validation: 0.030750303470203535]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_846.pth
	Model improved!!!
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029139096363596746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029139096363596746 | validation: 0.045719398825332325]
	TIME [epoch: 3.54 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03775660751489013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03775660751489013 | validation: 0.08872337220879896]
	TIME [epoch: 3.53 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09933774747832881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09933774747832881 | validation: 0.19395726187425882]
	TIME [epoch: 3.54 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23298963995579114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23298963995579114 | validation: 0.1108644601697591]
	TIME [epoch: 3.52 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09967154242229764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09967154242229764 | validation: 0.06739459699886806]
	TIME [epoch: 3.54 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06832617202970971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06832617202970971 | validation: 0.1279016483338411]
	TIME [epoch: 3.54 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11529683909064829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11529683909064829 | validation: 0.08393398536745789]
	TIME [epoch: 3.53 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1003953349266955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1003953349266955 | validation: 0.06296734577414631]
	TIME [epoch: 3.55 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05673178560166371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05673178560166371 | validation: 0.052342397034038625]
	TIME [epoch: 3.54 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05390280218007515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05390280218007515 | validation: 0.08867404033806835]
	TIME [epoch: 3.54 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06091800446996885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06091800446996885 | validation: 0.07334464465442177]
	TIME [epoch: 3.54 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07649921951208169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07649921951208169 | validation: 0.11995676613094874]
	TIME [epoch: 3.53 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10979840349902795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10979840349902795 | validation: 0.11766885643379275]
	TIME [epoch: 3.53 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13948195167833224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13948195167833224 | validation: 0.10162417924541552]
	TIME [epoch: 3.52 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08953658182558683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08953658182558683 | validation: 0.05007855980036222]
	TIME [epoch: 3.53 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04293955923362752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04293955923362752 | validation: 0.0349569312629139]
	TIME [epoch: 3.52 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026946948133094936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026946948133094936 | validation: 0.02979645643328196]
	TIME [epoch: 3.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_863.pth
	Model improved!!!
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02957661746281321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02957661746281321 | validation: 0.026643581700324666]
	TIME [epoch: 3.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_864.pth
	Model improved!!!
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02930039514559993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02930039514559993 | validation: 0.05035707965086968]
	TIME [epoch: 3.55 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045457681469302284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045457681469302284 | validation: 0.0951037533299626]
	TIME [epoch: 3.53 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12100800355581154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12100800355581154 | validation: 0.206293540502566]
	TIME [epoch: 3.54 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21628908026903942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21628908026903942 | validation: 0.10656914681633584]
	TIME [epoch: 3.53 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11938973168865878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11938973168865878 | validation: 0.08684423151256038]
	TIME [epoch: 3.54 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09846972539769791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09846972539769791 | validation: 0.17592316972323765]
	TIME [epoch: 3.54 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14077111772694476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14077111772694476 | validation: 0.09448664919325589]
	TIME [epoch: 3.53 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11771310949553668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11771310949553668 | validation: 0.049116405186911766]
	TIME [epoch: 3.53 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04155993653204318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04155993653204318 | validation: 0.04218951353288783]
	TIME [epoch: 3.53 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03554912728730014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03554912728730014 | validation: 0.05975522946905453]
	TIME [epoch: 3.54 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05241365868336804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05241365868336804 | validation: 0.0721594326536505]
	TIME [epoch: 3.52 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05614094299597681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05614094299597681 | validation: 0.057561392303009745]
	TIME [epoch: 3.53 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060142029437550044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060142029437550044 | validation: 0.07226623083797278]
	TIME [epoch: 3.54 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06092564085531369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06092564085531369 | validation: 0.0626523089326308]
	TIME [epoch: 3.54 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05572173316517187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05572173316517187 | validation: 0.08001133542175302]
	TIME [epoch: 3.54 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06235632266929998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06235632266929998 | validation: 0.074538690926465]
	TIME [epoch: 3.54 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08604063042106574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08604063042106574 | validation: 0.11300365497527823]
	TIME [epoch: 3.54 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09253507129241591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09253507129241591 | validation: 0.099179461207829]
	TIME [epoch: 3.53 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09640540283455573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09640540283455573 | validation: 0.08610893774843734]
	TIME [epoch: 3.53 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06689001516612866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06689001516612866 | validation: 0.040447829067675814]
	TIME [epoch: 3.53 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04013780196291467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04013780196291467 | validation: 0.03806234921833445]
	TIME [epoch: 3.53 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030532096946416287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030532096946416287 | validation: 0.029250470620139208]
	TIME [epoch: 3.52 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04067806563027694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04067806563027694 | validation: 0.10164892572156661]
	TIME [epoch: 3.53 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11399467906944141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11399467906944141 | validation: 0.20120422819221406]
	TIME [epoch: 3.53 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22267247399449872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22267247399449872 | validation: 0.09737893442014776]
	TIME [epoch: 3.52 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11482316166097216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11482316166097216 | validation: 0.032000964218855586]
	TIME [epoch: 3.53 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028708162006224655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028708162006224655 | validation: 0.0476218647815848]
	TIME [epoch: 3.53 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048755297078276645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048755297078276645 | validation: 0.0575424489595642]
	TIME [epoch: 3.53 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05687802018903064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05687802018903064 | validation: 0.04341503903244416]
	TIME [epoch: 3.54 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038373467684918186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038373467684918186 | validation: 0.04204458693411027]
	TIME [epoch: 3.53 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03621282718045766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03621282718045766 | validation: 0.02984538296125345]
	TIME [epoch: 3.54 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03509804522040746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03509804522040746 | validation: 0.053251314623152635]
	TIME [epoch: 3.52 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0422176614420618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0422176614420618 | validation: 0.06443295380954446]
	TIME [epoch: 3.53 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09195957045463622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09195957045463622 | validation: 0.1760187552859768]
	TIME [epoch: 3.53 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1666013932245529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1666013932245529 | validation: 0.06556762199910242]
	TIME [epoch: 3.53 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09291838255795679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09291838255795679 | validation: 0.03780886519785096]
	TIME [epoch: 3.53 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03155451674237166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03155451674237166 | validation: 0.02515899310896827]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_901.pth
	Model improved!!!
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02833670050010377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02833670050010377 | validation: 0.09492683487335161]
	TIME [epoch: 3.53 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10307259858837757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10307259858837757 | validation: 0.26772476190360966]
	TIME [epoch: 3.53 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31408371135652263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31408371135652263 | validation: 0.14766181362408215]
	TIME [epoch: 3.54 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14917222408425293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14917222408425293 | validation: 0.08100990004737091]
	TIME [epoch: 3.54 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08799989008913191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08799989008913191 | validation: 0.08803798607196385]
	TIME [epoch: 3.54 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07571839851245293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07571839851245293 | validation: 0.05010367989367427]
	TIME [epoch: 3.52 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041148348939874534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041148348939874534 | validation: 0.037401067546516516]
	TIME [epoch: 3.52 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03420936265710834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03420936265710834 | validation: 0.0506775254694267]
	TIME [epoch: 3.54 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040353126077010776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040353126077010776 | validation: 0.029442215067207535]
	TIME [epoch: 3.53 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03963216643596879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03963216643596879 | validation: 0.05848913320061533]
	TIME [epoch: 3.54 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04507343715875841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04507343715875841 | validation: 0.059617912854853496]
	TIME [epoch: 3.52 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06247469536779615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06247469536779615 | validation: 0.110485198072467]
	TIME [epoch: 3.52 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10220257625447836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10220257625447836 | validation: 0.1385549963727053]
	TIME [epoch: 3.52 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16182016511156483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16182016511156483 | validation: 0.11750144298870722]
	TIME [epoch: 3.52 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10626581610149756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10626581610149756 | validation: 0.03631040619684247]
	TIME [epoch: 3.53 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033087982986658486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033087982986658486 | validation: 0.02980444472490461]
	TIME [epoch: 3.53 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025921630408027464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025921630408027464 | validation: 0.031567590775005726]
	TIME [epoch: 3.54 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029560921731150543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029560921731150543 | validation: 0.03060226280873804]
	TIME [epoch: 3.54 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03571598890865918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03571598890865918 | validation: 0.06428050619244337]
	TIME [epoch: 3.53 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05705042842536212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05705042842536212 | validation: 0.0742452908651375]
	TIME [epoch: 3.53 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1078738459379321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1078738459379321 | validation: 0.14032698477619185]
	TIME [epoch: 3.53 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1548111035118432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1548111035118432 | validation: 0.07380027514322762]
	TIME [epoch: 3.53 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0812581219146505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0812581219146505 | validation: 0.05410502841026023]
	TIME [epoch: 3.53 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05031237360879869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05031237360879869 | validation: 0.08793677453304741]
	TIME [epoch: 3.54 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06674894921228203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06674894921228203 | validation: 0.08379259330992556]
	TIME [epoch: 3.54 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09050374434628075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09050374434628075 | validation: 0.11047949501145429]
	TIME [epoch: 3.53 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0947119402505182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0947119402505182 | validation: 0.07350251367856153]
	TIME [epoch: 3.52 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08206915510567761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08206915510567761 | validation: 0.0904461260832193]
	TIME [epoch: 3.53 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06811027990113265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06811027990113265 | validation: 0.04482464338385354]
	TIME [epoch: 3.54 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047354959638571614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047354959638571614 | validation: 0.031320217002294724]
	TIME [epoch: 3.52 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031065814241264846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031065814241264846 | validation: 0.03531804188073835]
	TIME [epoch: 3.53 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03257946221409127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03257946221409127 | validation: 0.03747987689258705]
	TIME [epoch: 3.51 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039017951145681504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039017951145681504 | validation: 0.058724362214580356]
	TIME [epoch: 3.52 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05346602257351601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05346602257351601 | validation: 0.06028552002038983]
	TIME [epoch: 3.53 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07975727395666554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07975727395666554 | validation: 0.10038515343419994]
	TIME [epoch: 3.53 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10014618570986829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10014618570986829 | validation: 0.060804432767069816]
	TIME [epoch: 3.53 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07431983207586214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07431983207586214 | validation: 0.04968984842463978]
	TIME [epoch: 3.52 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045918801596899955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045918801596899955 | validation: 0.060096148478367244]
	TIME [epoch: 3.53 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052839622509199935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052839622509199935 | validation: 0.11327337128929012]
	TIME [epoch: 3.52 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1114592621948066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1114592621948066 | validation: 0.23487380664285984]
	TIME [epoch: 3.52 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20343690898635033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20343690898635033 | validation: 0.09208971115011538]
	TIME [epoch: 3.53 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12256270449320507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12256270449320507 | validation: 0.04565736356882197]
	TIME [epoch: 3.53 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04495988109662148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04495988109662148 | validation: 0.05081084533043383]
	TIME [epoch: 3.54 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0452882448692436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0452882448692436 | validation: 0.0378723414553509]
	TIME [epoch: 3.53 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04275152083920615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04275152083920615 | validation: 0.03550065143531473]
	TIME [epoch: 3.52 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02686864832664675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02686864832664675 | validation: 0.027065780517436955]
	TIME [epoch: 3.53 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02636206232254935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02636206232254935 | validation: 0.03662935943298168]
	TIME [epoch: 3.51 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029290370966356595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029290370966356595 | validation: 0.035983901616590895]
	TIME [epoch: 3.53 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04428276305115541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04428276305115541 | validation: 0.08758587973696441]
	TIME [epoch: 3.51 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07885919711601896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07885919711601896 | validation: 0.08597998117003527]
	TIME [epoch: 3.52 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.122123869771457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.122123869771457 | validation: 0.08279317141818138]
	TIME [epoch: 3.51 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08951891949003946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08951891949003946 | validation: 0.03804635385279462]
	TIME [epoch: 3.54 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03715172779340948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03715172779340948 | validation: 0.04669441554220512]
	TIME [epoch: 3.51 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04100466127625846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04100466127625846 | validation: 0.12488480346131335]
	TIME [epoch: 3.53 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10123532309554933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10123532309554933 | validation: 0.14686362028606642]
	TIME [epoch: 3.52 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17202007176842146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17202007176842146 | validation: 0.12626486196514916]
	TIME [epoch: 3.53 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10853819979634739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10853819979634739 | validation: 0.027278928753591156]
	TIME [epoch: 3.53 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03365849975165086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03365849975165086 | validation: 0.029361659704414306]
	TIME [epoch: 3.53 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02312698989697895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02312698989697895 | validation: 0.03169201692692927]
	TIME [epoch: 3.53 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02827079791171207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02827079791171207 | validation: 0.03365391712830468]
	TIME [epoch: 3.53 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03741900474243969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03741900474243969 | validation: 0.044516422476193945]
	TIME [epoch: 3.52 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04604068896333203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04604068896333203 | validation: 0.059399668642686865]
	TIME [epoch: 3.52 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07133042051977284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07133042051977284 | validation: 0.09566331168181127]
	TIME [epoch: 3.52 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07774982594173795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07774982594173795 | validation: 0.05884154463486668]
	TIME [epoch: 3.53 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06461467831632434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06461467831632434 | validation: 0.053124056969162316]
	TIME [epoch: 3.52 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045731437666732544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045731437666732544 | validation: 0.05042172947733217]
	TIME [epoch: 3.53 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05089444189854712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05089444189854712 | validation: 0.11118411434114933]
	TIME [epoch: 3.52 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0906330262534161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0906330262534161 | validation: 0.08993751755416367]
	TIME [epoch: 3.52 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1184476569370743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1184476569370743 | validation: 0.08743809000335595]
	TIME [epoch: 3.53 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07328007590624312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07328007590624312 | validation: 0.04102598823641901]
	TIME [epoch: 3.53 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04083967876808488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04083967876808488 | validation: 0.04920699273120155]
	TIME [epoch: 3.53 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0655580387241061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0655580387241061 | validation: 0.08649124234917037]
	TIME [epoch: 3.53 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07792698671667671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07792698671667671 | validation: 0.0879399952701701]
	TIME [epoch: 3.51 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09321882693671568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09321882693671568 | validation: 0.07816316151003572]
	TIME [epoch: 3.53 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0691204068464817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0691204068464817 | validation: 0.03493730103580914]
	TIME [epoch: 3.53 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033676591115180746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033676591115180746 | validation: 0.028362940977603935]
	TIME [epoch: 3.52 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024762548457214352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024762548457214352 | validation: 0.025735835352301603]
	TIME [epoch: 3.53 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028782179302111254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028782179302111254 | validation: 0.04523737848696036]
	TIME [epoch: 3.52 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03272482469019554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03272482469019554 | validation: 0.03629357390453225]
	TIME [epoch: 3.52 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052394475379352524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052394475379352524 | validation: 0.10450767190679222]
	TIME [epoch: 3.51 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10830596472678519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10830596472678519 | validation: 0.0848658566892927]
	TIME [epoch: 3.52 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11504426808009238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11504426808009238 | validation: 0.05910049033673964]
	TIME [epoch: 3.54 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06968346272785265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06968346272785265 | validation: 0.04287118200982878]
	TIME [epoch: 3.52 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036242898656591825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036242898656591825 | validation: 0.04848873234452471]
	TIME [epoch: 3.53 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04383453392819047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04383453392819047 | validation: 0.11196973274085958]
	TIME [epoch: 3.53 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08724824827998597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08724824827998597 | validation: 0.11341888188779954]
	TIME [epoch: 3.53 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1251091737933121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1251091737933121 | validation: 0.11614199162834182]
	TIME [epoch: 3.53 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10145231813463192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10145231813463192 | validation: 0.04613670776221049]
	TIME [epoch: 3.53 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04823576951343512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04823576951343512 | validation: 0.0263273240281101]
	TIME [epoch: 3.53 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02407242661299344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02407242661299344 | validation: 0.03321955664776795]
	TIME [epoch: 3.53 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02435604786900635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02435604786900635 | validation: 0.024507248960350194]
	TIME [epoch: 3.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_992.pth
	Model improved!!!
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027768571203927374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027768571203927374 | validation: 0.03812755718923274]
	TIME [epoch: 3.53 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03325510839807637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03325510839807637 | validation: 0.035887537063162375]
	TIME [epoch: 3.52 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04091374476643063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04091374476643063 | validation: 0.06565730531954415]
	TIME [epoch: 3.54 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05933550365162797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05933550365162797 | validation: 0.05161860022625362]
	TIME [epoch: 3.52 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0772805852654746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0772805852654746 | validation: 0.09125886403592248]
	TIME [epoch: 3.54 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09509295967964562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09509295967964562 | validation: 0.07714997218997914]
	TIME [epoch: 3.52 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07781361357931127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07781361357931127 | validation: 0.11643919471979132]
	TIME [epoch: 3.53 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12440985633525635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12440985633525635 | validation: 0.16590671068678492]
	TIME [epoch: 3.52 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1407362211833662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1407362211833662 | validation: 0.041146369845235724]
	TIME [epoch: 29.9 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057990873710609174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057990873710609174 | validation: 0.0582354157484794]
	TIME [epoch: 7.67 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050003570976201187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050003570976201187 | validation: 0.025525234194874305]
	TIME [epoch: 7.64 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029844257892007564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029844257892007564 | validation: 0.029741820708778167]
	TIME [epoch: 7.64 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0255837491356591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0255837491356591 | validation: 0.024356308074461114]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_1005.pth
	Model improved!!!
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026733571721408573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026733571721408573 | validation: 0.020330390251663166]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_1006.pth
	Model improved!!!
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025387865315447657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025387865315447657 | validation: 0.032737159786678974]
	TIME [epoch: 7.64 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03424690528050811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03424690528050811 | validation: 0.05789311798732763]
	TIME [epoch: 7.65 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06642585180718008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06642585180718008 | validation: 0.1366784029744774]
	TIME [epoch: 7.66 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1335536104533894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1335536104533894 | validation: 0.10875550114908328]
	TIME [epoch: 7.64 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12894390197768185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12894390197768185 | validation: 0.07404743875535658]
	TIME [epoch: 7.65 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06407126069895255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06407126069895255 | validation: 0.043683887855803225]
	TIME [epoch: 7.67 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0464377110339613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0464377110339613 | validation: 0.05465196580469131]
	TIME [epoch: 7.64 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05199855238674248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05199855238674248 | validation: 0.04745866716554785]
	TIME [epoch: 7.66 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045967514026270515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045967514026270515 | validation: 0.04418323809753666]
	TIME [epoch: 7.65 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0396423467152442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0396423467152442 | validation: 0.03104768326837284]
	TIME [epoch: 7.65 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030107432407061357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030107432407061357 | validation: 0.03765746656389308]
	TIME [epoch: 7.66 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027983345825299773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027983345825299773 | validation: 0.028489391585785895]
	TIME [epoch: 7.65 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033730131192452675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033730131192452675 | validation: 0.06778281202150016]
	TIME [epoch: 7.65 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05928662834974524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05928662834974524 | validation: 0.0627425291798616]
	TIME [epoch: 7.65 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09176417052255945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09176417052255945 | validation: 0.08403044611768345]
	TIME [epoch: 7.66 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08371364384964182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08371364384964182 | validation: 0.031937306066716104]
	TIME [epoch: 7.65 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03881935427091315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03881935427091315 | validation: 0.028752682215741017]
	TIME [epoch: 7.63 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023232032452221513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023232032452221513 | validation: 0.030804263258431486]
	TIME [epoch: 7.63 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02844468513473394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02844468513473394 | validation: 0.09013344781437582]
	TIME [epoch: 7.63 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07101884532053437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07101884532053437 | validation: 0.18503486409542674]
	TIME [epoch: 7.64 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.198904680114602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.198904680114602 | validation: 0.17676075359910942]
	TIME [epoch: 7.66 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1483290957557977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1483290957557977 | validation: 0.032896549136525496]
	TIME [epoch: 7.63 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03320612798819157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03320612798819157 | validation: 0.05222218950163702]
	TIME [epoch: 7.64 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04734390913761674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04734390913761674 | validation: 0.044128081747518116]
	TIME [epoch: 7.63 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04203062217663064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04203062217663064 | validation: 0.027077586612234406]
	TIME [epoch: 7.63 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023489841985258747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023489841985258747 | validation: 0.023997618454296876]
	TIME [epoch: 7.65 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02193912453538937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02193912453538937 | validation: 0.029761334763036437]
	TIME [epoch: 7.65 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021586435534785987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021586435534785987 | validation: 0.02404602981142258]
	TIME [epoch: 7.68 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019215770953152224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019215770953152224 | validation: 0.028464872178657088]
	TIME [epoch: 7.65 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025698961459076734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025698961459076734 | validation: 0.04224294121361033]
	TIME [epoch: 7.67 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05414206397126913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05414206397126913 | validation: 0.10272376768385066]
	TIME [epoch: 7.65 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11302504225612282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11302504225612282 | validation: 0.16293773181787657]
	TIME [epoch: 7.64 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18062205287298347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18062205287298347 | validation: 0.11087792980440743]
	TIME [epoch: 7.64 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10415307517682484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10415307517682484 | validation: 0.027846351221305785]
	TIME [epoch: 7.66 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025531946119099695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025531946119099695 | validation: 0.036964262670438584]
	TIME [epoch: 7.64 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04146935390358428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04146935390358428 | validation: 0.05885820863812957]
	TIME [epoch: 7.65 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0556573826666294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0556573826666294 | validation: 0.04934590437585884]
	TIME [epoch: 7.64 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05934596896014119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05934596896014119 | validation: 0.06777975642958954]
	TIME [epoch: 7.66 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0671601310265988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0671601310265988 | validation: 0.05634598495884355]
	TIME [epoch: 7.65 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05848372341290186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05848372341290186 | validation: 0.05780202670679002]
	TIME [epoch: 7.67 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05299115767571957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05299115767571957 | validation: 0.03380665483364834]
	TIME [epoch: 7.64 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037121533765254545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037121533765254545 | validation: 0.030955832844742168]
	TIME [epoch: 7.65 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03449778676909398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03449778676909398 | validation: 0.038763727915091]
	TIME [epoch: 7.64 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03353910372330285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03353910372330285 | validation: 0.035707137105485925]
	TIME [epoch: 7.65 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046896055295266866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046896055295266866 | validation: 0.0688253902959135]
	TIME [epoch: 7.64 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07265494505523862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07265494505523862 | validation: 0.07834580695905163]
	TIME [epoch: 7.66 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0945016302084127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0945016302084127 | validation: 0.08275798393933645]
	TIME [epoch: 7.64 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07235731237979072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07235731237979072 | validation: 0.036229425876058076]
	TIME [epoch: 7.65 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03836653052482483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03836653052482483 | validation: 0.03198741449848872]
	TIME [epoch: 7.64 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023838777269809663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023838777269809663 | validation: 0.024506637205197187]
	TIME [epoch: 7.65 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023440750021243163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023440750021243163 | validation: 0.0450516756060678]
	TIME [epoch: 7.63 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035939226579307364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035939226579307364 | validation: 0.04758247309311417]
	TIME [epoch: 7.66 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06819891458122125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06819891458122125 | validation: 0.0973481700726036]
	TIME [epoch: 7.66 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10271120929088717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10271120929088717 | validation: 0.06871386708755607]
	TIME [epoch: 7.64 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06838239178705147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06838239178705147 | validation: 0.05079878211013559]
	TIME [epoch: 7.65 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039329072163615905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039329072163615905 | validation: 0.026761376388043337]
	TIME [epoch: 7.66 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025277790483443256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025277790483443256 | validation: 0.017234159593452775]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_1063.pth
	Model improved!!!
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018569468627332292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018569468627332292 | validation: 0.029725899990955526]
	TIME [epoch: 7.67 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03209244044024005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03209244044024005 | validation: 0.09863646143085818]
	TIME [epoch: 7.65 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08011626936705744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08011626936705744 | validation: 0.14934405468124265]
	TIME [epoch: 7.66 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1694857964021015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1694857964021015 | validation: 0.13619867640649838]
	TIME [epoch: 7.64 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12857000817464986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12857000817464986 | validation: 0.026002488048980907]
	TIME [epoch: 7.66 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03217360742175182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03217360742175182 | validation: 0.04360450203259286]
	TIME [epoch: 7.64 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03396999769084571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03396999769084571 | validation: 0.04044762164018769]
	TIME [epoch: 7.67 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041479630685804396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041479630685804396 | validation: 0.03497173849380079]
	TIME [epoch: 7.64 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04543284242297723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04543284242297723 | validation: 0.023116307704800883]
	TIME [epoch: 7.65 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027263938593638085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027263938593638085 | validation: 0.036216903842788216]
	TIME [epoch: 7.65 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024415975243169703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024415975243169703 | validation: 0.028678346965015283]
	TIME [epoch: 7.63 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034280519141159226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034280519141159226 | validation: 0.07253136312187188]
	TIME [epoch: 7.63 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055717746664083904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055717746664083904 | validation: 0.06758415751367611]
	TIME [epoch: 7.67 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07485509396752099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07485509396752099 | validation: 0.07148435232134574]
	TIME [epoch: 7.65 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06462595655924057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06462595655924057 | validation: 0.03293298608177928]
	TIME [epoch: 7.65 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03905694366784448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03905694366784448 | validation: 0.034851858869959944]
	TIME [epoch: 7.64 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03025210154502125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03025210154502125 | validation: 0.04650797540880209]
	TIME [epoch: 7.67 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04346544813075151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04346544813075151 | validation: 0.09323812575357977]
	TIME [epoch: 7.66 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1051115694927533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1051115694927533 | validation: 0.10053520524562748]
	TIME [epoch: 7.66 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1235369862103522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1235369862103522 | validation: 0.03254490822623159]
	TIME [epoch: 7.66 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0322398148828221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0322398148828221 | validation: 0.032017239942164495]
	TIME [epoch: 7.65 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034317663628641866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034317663628641866 | validation: 0.044650361831324537]
	TIME [epoch: 7.67 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0486305568289744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0486305568289744 | validation: 0.05023826981284413]
	TIME [epoch: 7.64 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042429368848840736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042429368848840736 | validation: 0.04193067844623863]
	TIME [epoch: 7.63 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04157740832307876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04157740832307876 | validation: 0.06816857498473526]
	TIME [epoch: 7.64 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05572671750758398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05572671750758398 | validation: 0.052067761961243623]
	TIME [epoch: 7.66 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06987731453793554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06987731453793554 | validation: 0.07048638894798712]
	TIME [epoch: 7.65 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06405020462340709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06405020462340709 | validation: 0.03268385399355385]
	TIME [epoch: 7.65 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04156474443868307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04156474443868307 | validation: 0.04588627255283417]
	TIME [epoch: 7.65 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027856829074049037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027856829074049037 | validation: 0.022366779079170597]
	TIME [epoch: 7.66 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030200700394640956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030200700394640956 | validation: 0.06620081419371308]
	TIME [epoch: 7.64 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0758727825588794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0758727825588794 | validation: 0.10861334242943624]
	TIME [epoch: 7.65 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09019379896276183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09019379896276183 | validation: 0.06982645205919849]
	TIME [epoch: 7.64 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07364496528299333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07364496528299333 | validation: 0.06254985485757028]
	TIME [epoch: 7.64 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043469392008450035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043469392008450035 | validation: 0.03540510473108028]
	TIME [epoch: 7.64 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038450385591664506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038450385591664506 | validation: 0.049221676557199834]
	TIME [epoch: 7.65 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04338446115485502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04338446115485502 | validation: 0.041497802790189366]
	TIME [epoch: 7.66 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05039955487769038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05039955487769038 | validation: 0.048227452778461424]
	TIME [epoch: 7.67 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046966568212020096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046966568212020096 | validation: 0.030106502498989593]
	TIME [epoch: 7.65 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03311028106708366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03311028106708366 | validation: 0.033576414324395086]
	TIME [epoch: 7.65 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023613145441972377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023613145441972377 | validation: 0.01804696279997785]
	TIME [epoch: 7.65 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021020053430375277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021020053430375277 | validation: 0.028935894861487517]
	TIME [epoch: 7.66 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02133000935018143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02133000935018143 | validation: 0.023636301105619303]
	TIME [epoch: 7.66 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02846996724556929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02846996724556929 | validation: 0.060168229866670454]
	TIME [epoch: 7.66 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055000771691441665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055000771691441665 | validation: 0.0761600095822747]
	TIME [epoch: 7.64 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1040966799748083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1040966799748083 | validation: 0.09023018295225005]
	TIME [epoch: 7.67 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11556824356103942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11556824356103942 | validation: 0.10399675026731117]
	TIME [epoch: 7.64 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07667214525648267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07667214525648267 | validation: 0.0922260012991629]
	TIME [epoch: 7.66 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0859702782531992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0859702782531992 | validation: 0.07315126292315921]
	TIME [epoch: 7.65 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06573627555509297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06573627555509297 | validation: 0.037543038556574815]
	TIME [epoch: 7.67 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04804435444744834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04804435444744834 | validation: 0.04280015090879476]
	TIME [epoch: 7.64 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0353534382525611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0353534382525611 | validation: 0.022492484405782526]
	TIME [epoch: 7.65 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020435918454907966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020435918454907966 | validation: 0.03149628992775928]
	TIME [epoch: 7.64 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02648490407390435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02648490407390435 | validation: 0.0346284580482698]
	TIME [epoch: 7.65 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03381492664483824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03381492664483824 | validation: 0.07142671455132428]
	TIME [epoch: 7.64 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06500408604182342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06500408604182342 | validation: 0.09055887194735915]
	TIME [epoch: 7.66 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10753459122669591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10753459122669591 | validation: 0.08852630530187769]
	TIME [epoch: 7.65 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07934667852482152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07934667852482152 | validation: 0.04081873617111358]
	TIME [epoch: 7.65 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048701641009124205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048701641009124205 | validation: 0.04959779544556713]
	TIME [epoch: 7.64 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03996090437641536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03996090437641536 | validation: 0.03542917730971728]
	TIME [epoch: 7.65 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040243538974960434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040243538974960434 | validation: 0.042099252340027746]
	TIME [epoch: 7.63 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04110595662879986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04110595662879986 | validation: 0.028816626337436802]
	TIME [epoch: 7.66 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02656109639947056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02656109639947056 | validation: 0.027952095680901397]
	TIME [epoch: 7.64 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02221213506789582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02221213506789582 | validation: 0.021966648949702534]
	TIME [epoch: 7.64 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02292986365477799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02292986365477799 | validation: 0.026602901898687727]
	TIME [epoch: 7.65 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028753797846697318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028753797846697318 | validation: 0.04659394375437033]
	TIME [epoch: 7.64 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043235814519692044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043235814519692044 | validation: 0.07818242716685421]
	TIME [epoch: 7.64 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09447689086139441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09447689086139441 | validation: 0.1467158943265947]
	TIME [epoch: 7.66 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11664669718868968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11664669718868968 | validation: 0.07528277378230723]
	TIME [epoch: 7.67 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09473182192499209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09473182192499209 | validation: 0.053996471613167486]
	TIME [epoch: 7.64 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04472998443472143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04472998443472143 | validation: 0.03084069521056343]
	TIME [epoch: 7.64 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03336136634136054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03336136634136054 | validation: 0.030039881901483295]
	TIME [epoch: 7.63 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03351557287407105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03351557287407105 | validation: 0.03907697146715399]
	TIME [epoch: 7.64 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035005949256002755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035005949256002755 | validation: 0.034616675082193335]
	TIME [epoch: 7.65 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035449413521399735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035449413521399735 | validation: 0.021947027718402574]
	TIME [epoch: 7.68 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026857745502867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026857745502867 | validation: 0.03225005362872456]
	TIME [epoch: 7.66 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02886817214264541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02886817214264541 | validation: 0.020362004102569747]
	TIME [epoch: 7.67 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03240330315469846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03240330315469846 | validation: 0.05053770935386373]
	TIME [epoch: 7.66 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04872203191608396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04872203191608396 | validation: 0.06524780154248462]
	TIME [epoch: 7.64 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07906201355736982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07906201355736982 | validation: 0.0735359882377108]
	TIME [epoch: 7.66 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07655913590143537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07655913590143537 | validation: 0.026665961767746556]
	TIME [epoch: 7.65 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03857184686959386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03857184686959386 | validation: 0.0191635687006135]
	TIME [epoch: 7.65 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016328909566130034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016328909566130034 | validation: 0.01167152838396775]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_1146.pth
	Model improved!!!
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015333677338981047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015333677338981047 | validation: 0.03022895867926133]
	TIME [epoch: 7.63 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025175961843729448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025175961843729448 | validation: 0.05189208072040198]
	TIME [epoch: 7.63 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04576277518838534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04576277518838534 | validation: 0.09777804613162783]
	TIME [epoch: 7.63 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07090736872757562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07090736872757562 | validation: 0.11976615370934847]
	TIME [epoch: 7.63 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1298375064858853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1298375064858853 | validation: 0.14726023321316414]
	TIME [epoch: 7.61 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15976655721596308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15976655721596308 | validation: 0.031915468988370825]
	TIME [epoch: 7.62 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052196147083835576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052196147083835576 | validation: 0.02123457717650693]
	TIME [epoch: 7.62 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02425268310452706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02425268310452706 | validation: 0.03926061093801029]
	TIME [epoch: 7.62 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04293654962097243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04293654962097243 | validation: 0.026924036721842987]
	TIME [epoch: 7.64 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031650220550013604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031650220550013604 | validation: 0.012558749324917608]
	TIME [epoch: 7.61 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016785957145585777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016785957145585777 | validation: 0.014383923760165562]
	TIME [epoch: 7.62 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016602804193150852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016602804193150852 | validation: 0.0246278196193198]
	TIME [epoch: 7.62 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024328602747213473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024328602747213473 | validation: 0.02580724981587003]
	TIME [epoch: 7.61 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024400921314853236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024400921314853236 | validation: 0.028772419468153426]
	TIME [epoch: 7.63 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03158868325648869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03158868325648869 | validation: 0.044527874289772035]
	TIME [epoch: 7.63 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04475303420317344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04475303420317344 | validation: 0.061507711721637416]
	TIME [epoch: 7.64 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059264341599322104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059264341599322104 | validation: 0.05502638934608071]
	TIME [epoch: 7.62 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07256154413697737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07256154413697737 | validation: 0.09582356672773566]
	TIME [epoch: 7.62 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10325515278494277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10325515278494277 | validation: 0.1085185418707486]
	TIME [epoch: 7.61 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12180534810601777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12180534810601777 | validation: 0.08307008339964395]
	TIME [epoch: 7.62 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06948909577257627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06948909577257627 | validation: 0.035757753724402486]
	TIME [epoch: 7.62 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03756061767724575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03756061767724575 | validation: 0.019630103109444852]
	TIME [epoch: 7.64 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02856321013335756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02856321013335756 | validation: 0.024034297231139576]
	TIME [epoch: 7.62 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023361629007805793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023361629007805793 | validation: 0.013888365561206479]
	TIME [epoch: 7.61 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016410338920548925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016410338920548925 | validation: 0.016813759004529673]
	TIME [epoch: 7.63 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013011990357956402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013011990357956402 | validation: 0.015745862389796323]
	TIME [epoch: 7.61 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014455598839459413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014455598839459413 | validation: 0.02714178168960806]
	TIME [epoch: 7.63 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018231172361039882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018231172361039882 | validation: 0.03161198413673946]
	TIME [epoch: 7.61 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029603176274584024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029603176274584024 | validation: 0.06199886051990203]
	TIME [epoch: 7.61 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05425599801439467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05425599801439467 | validation: 0.08633218027335221]
	TIME [epoch: 7.61 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09729899520869668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09729899520869668 | validation: 0.07970565726598187]
	TIME [epoch: 7.61 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09251522975663956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09251522975663956 | validation: 0.030883315270692026]
	TIME [epoch: 7.61 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030419408813949184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030419408813949184 | validation: 0.016493477031024613]
	TIME [epoch: 7.61 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01933414787419469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01933414787419469 | validation: 0.04446401832359049]
	TIME [epoch: 7.62 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03945873404013739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03945873404013739 | validation: 0.0771892185078571]
	TIME [epoch: 7.64 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09325762862892142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09325762862892142 | validation: 0.11162032671907246]
	TIME [epoch: 7.61 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09936938366231317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09936938366231317 | validation: 0.04480204797181263]
	TIME [epoch: 7.62 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0514144355924436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0514144355924436 | validation: 0.02507946557243286]
	TIME [epoch: 7.61 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033580590115173364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033580590115173364 | validation: 0.03890693201209938]
	TIME [epoch: 7.63 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048259700525385585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048259700525385585 | validation: 0.0270056545460866]
	TIME [epoch: 7.62 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031075293285518262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031075293285518262 | validation: 0.03026785493832392]
	TIME [epoch: 7.63 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022704820979261377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022704820979261377 | validation: 0.014770709792961662]
	TIME [epoch: 7.61 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019437105714705784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019437105714705784 | validation: 0.021545361763566914]
	TIME [epoch: 7.61 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015529141698688588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015529141698688588 | validation: 0.016887563705401204]
	TIME [epoch: 7.61 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016485034321263577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016485034321263577 | validation: 0.02669529207390713]
	TIME [epoch: 7.62 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019708885143228855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019708885143228855 | validation: 0.02829583671087968]
	TIME [epoch: 7.62 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03517363451753997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03517363451753997 | validation: 0.07102618941164439]
	TIME [epoch: 7.64 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0922123551392665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0922123551392665 | validation: 0.10454319579105409]
	TIME [epoch: 7.62 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09615525238421388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09615525238421388 | validation: 0.06046525505266154]
	TIME [epoch: 7.61 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0645172301443572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0645172301443572 | validation: 0.08712348008396924]
	TIME [epoch: 7.61 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07846812330821726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07846812330821726 | validation: 0.06885002937934477]
	TIME [epoch: 7.62 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0969413177938322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0969413177938322 | validation: 0.04914380541865988]
	TIME [epoch: 7.62 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04152184881811346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04152184881811346 | validation: 0.02085454842516345]
	TIME [epoch: 7.63 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019185022671687094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019185022671687094 | validation: 0.014310909469537447]
	TIME [epoch: 7.62 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018058660462124346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018058660462124346 | validation: 0.02333963656368388]
	TIME [epoch: 7.64 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020967738468450237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020967738468450237 | validation: 0.021179103727572358]
	TIME [epoch: 7.63 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02777325247960577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02777325247960577 | validation: 0.04903555585252699]
	TIME [epoch: 7.63 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0489321635087029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0489321635087029 | validation: 0.06777873098984948]
	TIME [epoch: 7.62 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08785956596772124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08785956596772124 | validation: 0.0634032177129203]
	TIME [epoch: 7.62 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05737810992860899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05737810992860899 | validation: 0.04101990571962333]
	TIME [epoch: 7.63 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036705164540205504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036705164540205504 | validation: 0.040223977949352646]
	TIME [epoch: 7.63 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030599100035919483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030599100035919483 | validation: 0.03114344912619718]
	TIME [epoch: 7.63 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03525125010946444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03525125010946444 | validation: 0.0481367166704606]
	TIME [epoch: 7.64 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04462850692169149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04462850692169149 | validation: 0.04765994927726121]
	TIME [epoch: 7.63 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05754502884293876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05754502884293876 | validation: 0.05006120929053997]
	TIME [epoch: 7.63 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050698782492855914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050698782492855914 | validation: 0.02610217866798467]
	TIME [epoch: 7.62 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03267716932157055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03267716932157055 | validation: 0.03425095881888508]
	TIME [epoch: 7.61 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029299165006995018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029299165006995018 | validation: 0.046614401191316025]
	TIME [epoch: 7.62 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043134267440533364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043134267440533364 | validation: 0.0763376189669972]
	TIME [epoch: 7.62 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07814140874946561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07814140874946561 | validation: 0.11149659356810204]
	TIME [epoch: 7.65 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10422023889657168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10422023889657168 | validation: 0.041963332645506723]
	TIME [epoch: 7.66 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05397703838766564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05397703838766564 | validation: 0.034029450548935]
	TIME [epoch: 7.66 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03039373723373354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03039373723373354 | validation: 0.02343911901119385]
	TIME [epoch: 7.63 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026363902708957776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026363902708957776 | validation: 0.025034440998959663]
	TIME [epoch: 7.65 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01807140834250195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01807140834250195 | validation: 0.01419455316301096]
	TIME [epoch: 7.66 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016419001970635883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016419001970635883 | validation: 0.01701887069786891]
	TIME [epoch: 7.65 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016631710934193526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016631710934193526 | validation: 0.010424630737702584]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_1223.pth
	Model improved!!!
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02215269903123746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02215269903123746 | validation: 0.036307297883105404]
	TIME [epoch: 7.63 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041111094919118525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041111094919118525 | validation: 0.03934504536041537]
	TIME [epoch: 7.61 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05667174834042987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05667174834042987 | validation: 0.06869769259237293]
	TIME [epoch: 7.63 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07413591393567706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07413591393567706 | validation: 0.04619649648184815]
	TIME [epoch: 7.61 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06119441861468152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06119441861468152 | validation: 0.0525295992321404]
	TIME [epoch: 7.63 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046997043663059235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046997043663059235 | validation: 0.035980697161706834]
	TIME [epoch: 7.63 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04025365511492152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04025365511492152 | validation: 0.0746781803575227]
	TIME [epoch: 7.63 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06206352817744045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06206352817744045 | validation: 0.08391271868604402]
	TIME [epoch: 7.63 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08845695026694697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08845695026694697 | validation: 0.07449787698980774]
	TIME [epoch: 7.63 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0740473274931858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0740473274931858 | validation: 0.019564527457141795]
	TIME [epoch: 7.63 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02219969945949388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02219969945949388 | validation: 0.017743142654794847]
	TIME [epoch: 7.63 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017635395728959582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017635395728959582 | validation: 0.02075590437899253]
	TIME [epoch: 7.65 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025031619480128713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025031619480128713 | validation: 0.032471897159900294]
	TIME [epoch: 7.64 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03142277601315504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03142277601315504 | validation: 0.027753655576225063]
	TIME [epoch: 7.61 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03080506260615229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03080506260615229 | validation: 0.03884634020232311]
	TIME [epoch: 7.61 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03401172089904691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03401172089904691 | validation: 0.030849462623066756]
	TIME [epoch: 7.62 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03564512347219344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03564512347219344 | validation: 0.05991844789107677]
	TIME [epoch: 7.62 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05542408376535448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05542408376535448 | validation: 0.05431182483509431]
	TIME [epoch: 7.62 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06946015120543254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06946015120543254 | validation: 0.06540133807353736]
	TIME [epoch: 7.63 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06001768930096543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06001768930096543 | validation: 0.04429626429828804]
	TIME [epoch: 7.63 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04843285668463967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04843285668463967 | validation: 0.04546309964154242]
	TIME [epoch: 7.61 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045434005164290685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045434005164290685 | validation: 0.05512002332275687]
	TIME [epoch: 7.61 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05414869608182205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05414869608182205 | validation: 0.03441148215505856]
	TIME [epoch: 7.62 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03766741754924618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03766741754924618 | validation: 0.02131014991894338]
	TIME [epoch: 7.63 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020818103237252524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020818103237252524 | validation: 0.014837995427059963]
	TIME [epoch: 7.63 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015963416296997296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015963416296997296 | validation: 0.022827951287262616]
	TIME [epoch: 7.65 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014982716600755772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014982716600755772 | validation: 0.01748188175003016]
	TIME [epoch: 7.61 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02099685256682907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02099685256682907 | validation: 0.038343994741286136]
	TIME [epoch: 7.64 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035757762559934854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035757762559934854 | validation: 0.05676359763464034]
	TIME [epoch: 7.61 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06859560758670417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06859560758670417 | validation: 0.10867000199976795]
	TIME [epoch: 7.63 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09658308959019966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09658308959019966 | validation: 0.09024382375735536]
	TIME [epoch: 7.63 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09983972613570265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09983972613570265 | validation: 0.06485050187006221]
	TIME [epoch: 7.62 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059399324254220155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059399324254220155 | validation: 0.03701063030705275]
	TIME [epoch: 7.62 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03044293199607922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03044293199607922 | validation: 0.025210421098825382]
	TIME [epoch: 7.63 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027718257660759683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027718257660759683 | validation: 0.024290757353851895]
	TIME [epoch: 7.62 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02464766959528997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02464766959528997 | validation: 0.020883274769552693]
	TIME [epoch: 7.62 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01818312123200717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01818312123200717 | validation: 0.02218894088507838]
	TIME [epoch: 7.64 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018071023016918566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018071023016918566 | validation: 0.014609674584886169]
	TIME [epoch: 7.63 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01932848338702583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01932848338702583 | validation: 0.03266419592428881]
	TIME [epoch: 7.63 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031186526140430406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031186526140430406 | validation: 0.036906246116965]
	TIME [epoch: 7.65 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05457069340671828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05457069340671828 | validation: 0.06771734966817988]
	TIME [epoch: 7.63 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0825122266869611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0825122266869611 | validation: 0.049168767327452144]
	TIME [epoch: 7.64 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055079090773686536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055079090773686536 | validation: 0.041531808428905595]
	TIME [epoch: 7.63 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0382334431268512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0382334431268512 | validation: 0.023308463748590788]
	TIME [epoch: 7.65 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023122815991299294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023122815991299294 | validation: 0.02054780069211792]
	TIME [epoch: 7.61 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017159789535161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017159789535161 | validation: 0.025346015970376674]
	TIME [epoch: 7.63 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01956821980758085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01956821980758085 | validation: 0.043458232843651114]
	TIME [epoch: 7.62 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03863618129608404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03863618129608404 | validation: 0.09755914701644924]
	TIME [epoch: 7.65 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10917251704923014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10917251704923014 | validation: 0.14873650173131764]
	TIME [epoch: 7.65 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14422182815638465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14422182815638465 | validation: 0.02738638927197759]
	TIME [epoch: 7.64 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03977793421744208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03977793421744208 | validation: 0.02309300819656901]
	TIME [epoch: 7.65 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021065352636270563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021065352636270563 | validation: 0.02773189706813335]
	TIME [epoch: 7.63 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02819630898804804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02819630898804804 | validation: 0.024701735413313827]
	TIME [epoch: 7.63 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02817621184601904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02817621184601904 | validation: 0.01660315226938589]
	TIME [epoch: 7.62 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021122809040382485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021122809040382485 | validation: 0.020973786614172197]
	TIME [epoch: 7.62 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021624528339998648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021624528339998648 | validation: 0.02478354837005395]
	TIME [epoch: 7.62 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02882965778026272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02882965778026272 | validation: 0.05589412482144023]
	TIME [epoch: 7.65 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051936972471980036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051936972471980036 | validation: 0.05759900676506276]
	TIME [epoch: 7.62 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07419860069194055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07419860069194055 | validation: 0.05326756066832249]
	TIME [epoch: 7.66 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05489395004740263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05489395004740263 | validation: 0.019099919158331626]
	TIME [epoch: 7.64 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024753891188517004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024753891188517004 | validation: 0.01799687014291288]
	TIME [epoch: 7.63 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014070928025636547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014070928025636547 | validation: 0.018945462188386376]
	TIME [epoch: 7.65 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01619505769275529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01619505769275529 | validation: 0.02993911173386686]
	TIME [epoch: 7.62 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02656983200454451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02656983200454451 | validation: 0.08121892518038032]
	TIME [epoch: 7.61 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06552061434035282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06552061434035282 | validation: 0.09329268660380358]
	TIME [epoch: 7.62 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10236876671027012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10236876671027012 | validation: 0.07240496494218321]
	TIME [epoch: 7.62 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07543072007014759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07543072007014759 | validation: 0.022753905548009247]
	TIME [epoch: 7.63 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038753003717882685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038753003717882685 | validation: 0.02415936072843446]
	TIME [epoch: 7.62 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019246781518689447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019246781518689447 | validation: 0.017119958120559036]
	TIME [epoch: 7.64 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014873833907354726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014873833907354726 | validation: 0.02173046132562581]
	TIME [epoch: 7.61 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0186795070134434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0186795070134434 | validation: 0.018603121857511503]
	TIME [epoch: 7.62 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022633320753586556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022633320753586556 | validation: 0.03095471821750397]
	TIME [epoch: 7.62 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038609079070752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038609079070752 | validation: 0.0313061450818176]
	TIME [epoch: 7.61 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04731631727472342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04731631727472342 | validation: 0.05935654128387109]
	TIME [epoch: 7.66 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06386000851319105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06386000851319105 | validation: 0.04200641982042791]
	TIME [epoch: 7.61 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05836685706328997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05836685706328997 | validation: 0.04487518120960649]
	TIME [epoch: 7.62 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04400845124353198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04400845124353198 | validation: 0.03096297122412871]
	TIME [epoch: 7.62 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03722300116188263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03722300116188263 | validation: 0.04810202206653207]
	TIME [epoch: 7.63 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04149657296129952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04149657296129952 | validation: 0.046492853555435296]
	TIME [epoch: 7.62 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05558917914437071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05558917914437071 | validation: 0.04728442401537955]
	TIME [epoch: 7.63 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04646936860597853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04646936860597853 | validation: 0.03774570886659462]
	TIME [epoch: 7.62 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026305992575312766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026305992575312766 | validation: 0.02281340637801272]
	TIME [epoch: 7.62 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016463157600511962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016463157600511962 | validation: 0.01751529857634398]
	TIME [epoch: 7.63 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014648502658527778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014648502658527778 | validation: 0.017892210348166238]
	TIME [epoch: 7.62 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01171594572914367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01171594572914367 | validation: 0.01583160062388469]
	TIME [epoch: 7.61 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014381542202073732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014381542202073732 | validation: 0.023058867379475996]
	TIME [epoch: 7.62 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02183575456641978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02183575456641978 | validation: 0.039964119146419674]
	TIME [epoch: 7.62 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045443889385545264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045443889385545264 | validation: 0.10470295397774446]
	TIME [epoch: 7.63 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10819226710174505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10819226710174505 | validation: 0.09078043710111808]
	TIME [epoch: 7.65 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11173172199107001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11173172199107001 | validation: 0.04724330956560809]
	TIME [epoch: 7.63 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05763431683112898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05763431683112898 | validation: 0.05216969708973793]
	TIME [epoch: 7.63 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058894333981649644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058894333981649644 | validation: 0.032521769318999603]
	TIME [epoch: 7.63 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044163395618138604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044163395618138604 | validation: 0.03208661878846674]
	TIME [epoch: 7.64 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02679673634502844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02679673634502844 | validation: 0.015325817188631213]
	TIME [epoch: 7.63 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016017749731428065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016017749731428065 | validation: 0.01633762347952662]
	TIME [epoch: 7.64 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014577160425587637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014577160425587637 | validation: 0.022830969653764695]
	TIME [epoch: 7.62 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01857035434491572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01857035434491572 | validation: 0.01714007124224547]
	TIME [epoch: 7.63 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025323257176586338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025323257176586338 | validation: 0.04363553654044566]
	TIME [epoch: 7.63 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05170051100973055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05170051100973055 | validation: 0.06553016636358171]
	TIME [epoch: 7.64 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06365381231487152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06365381231487152 | validation: 0.055801192736677135]
	TIME [epoch: 7.61 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05136791831016391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05136791831016391 | validation: 0.031617741183914236]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142221/states/model_phi1_3a_v_mmd1_1324.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 5150.107 seconds.
