Args:
Namespace(name='model_phi1_4c_v_mmd1', outdir='out/model_training/model_phi1_4c_v_mmd1', training_data='data/training_data/data_phi1_4c/training', validation_data='data/training_data/data_phi1_4c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2748768668

Training model...

Saving initial model state to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.392507849365533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.392507849365533 | validation: 5.593438448383785]
	TIME [epoch: 46.2 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.403648581728626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.403648581728626 | validation: 5.765337799055453]
	TIME [epoch: 3.73 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.763793456030037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.763793456030037 | validation: 5.192940719957604]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.1721257184458205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.1721257184458205 | validation: 5.459496742292352]
	TIME [epoch: 3.72 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.200499669213619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.200499669213619 | validation: 4.4381296245048265]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.344695421058916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.344695421058916 | validation: 4.698123723222824]
	TIME [epoch: 3.7 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.63881611549648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.63881611549648 | validation: 4.356395128771132]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.287180801857693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.287180801857693 | validation: 4.370351656147583]
	TIME [epoch: 3.72 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.235334787153516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.235334787153516 | validation: 4.184490468701552]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.063514296919838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.063514296919838 | validation: 4.13970511641675]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.061697046620455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.061697046620455 | validation: 4.0436010419417645]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9311406438223813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9311406438223813 | validation: 4.025537636074961]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.899776501090719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.899776501090719 | validation: 3.936692051336043]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8394705505625994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8394705505625994 | validation: 3.8900907726555376]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7779306717139645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7779306717139645 | validation: 3.8427112296446246]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.735817610329142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.735817610329142 | validation: 3.8256939498691453]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7077983894831723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7077983894831723 | validation: 3.78269478937236]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.708395262057186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.708395262057186 | validation: 3.8530726258939296]
	TIME [epoch: 3.71 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.722012564858875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.722012564858875 | validation: 3.7733967079909343]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.740004194399199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.740004194399199 | validation: 3.8089219880256233]
	TIME [epoch: 3.72 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6837055685678037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6837055685678037 | validation: 3.6926022133194736]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5932376288418686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5932376288418686 | validation: 3.6433622318614214]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.556172734871639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.556172734871639 | validation: 3.6329033049865993]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5260572253748625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5260572253748625 | validation: 3.597967753846395]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5040725996623157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5040725996623157 | validation: 3.5922989272764654]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.486182175647997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.486182175647997 | validation: 3.555720962989243]
	TIME [epoch: 3.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.473698172171023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.473698172171023 | validation: 3.6231331859609313]
	TIME [epoch: 3.7 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4872870814236343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4872870814236343 | validation: 3.6040577154223934]
	TIME [epoch: 3.7 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5832390467346262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5832390467346262 | validation: 3.8237625538975375]
	TIME [epoch: 3.7 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6471484187634826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6471484187634826 | validation: 3.4770753972650605]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.411065185131083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.411065185131083 | validation: 3.554998856697506]
	TIME [epoch: 3.72 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4736023623434478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4736023623434478 | validation: 3.5813407989994843]
	TIME [epoch: 3.71 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4470543095737085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4470543095737085 | validation: 3.4186872449128844]
	TIME [epoch: 3.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3531422381794007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3531422381794007 | validation: 3.4781761754927376]
	TIME [epoch: 3.71 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.350151277646235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.350151277646235 | validation: 3.4108699101614306]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3357627886729655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3357627886729655 | validation: 3.419175808473015]
	TIME [epoch: 3.71 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.292624443623763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.292624443623763 | validation: 3.3575495172110266]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.287668031008784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.287668031008784 | validation: 3.4580633410590806]
	TIME [epoch: 3.7 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.308083347423726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.308083347423726 | validation: 3.354148236056423]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3359250732177417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3359250732177417 | validation: 3.504656632660033]
	TIME [epoch: 3.71 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3266519273677706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3266519273677706 | validation: 3.289709621594294]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.239474258016564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.239474258016564 | validation: 3.311984976162964]
	TIME [epoch: 3.72 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.185438211189717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.185438211189717 | validation: 3.276643896324086]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.159972859619628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.159972859619628 | validation: 3.244930357812554]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.139455837324961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.139455837324961 | validation: 3.2086602453720867]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1072055678409107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1072055678409107 | validation: 3.163002814082734]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.065848451109259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.065848451109259 | validation: 3.305567647427705]
	TIME [epoch: 3.72 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1204057101977845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1204057101977845 | validation: 3.994866341613225]
	TIME [epoch: 3.71 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9593738753872243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9593738753872243 | validation: 3.685190943932692]
	TIME [epoch: 3.71 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6216693991858473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6216693991858473 | validation: 3.311860429296762]
	TIME [epoch: 3.72 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2589772037990996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2589772037990996 | validation: 3.3372027264828343]
	TIME [epoch: 3.71 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2029537681122413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2029537681122413 | validation: 3.3933141401101388]
	TIME [epoch: 3.72 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.355842549336204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.355842549336204 | validation: 3.13257921240832]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0351426268332844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0351426268332844 | validation: 3.1602255403001607]
	TIME [epoch: 3.74 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0239756782359315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0239756782359315 | validation: 3.0600442993833012]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0151161773823327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0151161773823327 | validation: 3.1486336663755368]
	TIME [epoch: 3.72 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.985970273961184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.985970273961184 | validation: 3.0603255466028956]
	TIME [epoch: 3.73 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0232917175527483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0232917175527483 | validation: 3.202744771546347]
	TIME [epoch: 3.73 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0120343143443957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0120343143443957 | validation: 3.0323573301399174]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9967826045921333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9967826045921333 | validation: 3.061644796299106]
	TIME [epoch: 3.71 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.912727967456784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.912727967456784 | validation: 2.941822049766941]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.87888303840228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.87888303840228 | validation: 2.9940115991009706]
	TIME [epoch: 3.7 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.870728256730489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.870728256730489 | validation: 3.0091298290793755]
	TIME [epoch: 3.7 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9915259862041204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9915259862041204 | validation: 3.4374138521854265]
	TIME [epoch: 3.7 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1772582944757986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1772582944757986 | validation: 2.9630312809384223]
	TIME [epoch: 3.71 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9505487274904056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9505487274904056 | validation: 2.9225512808305467]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.848877437525215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.848877437525215 | validation: 2.978598186242431]
	TIME [epoch: 3.73 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.846527105014085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.846527105014085 | validation: 2.8892353778303916]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.82708994456979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.82708994456979 | validation: 2.9384011409181836]
	TIME [epoch: 3.72 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7919812277841745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7919812277841745 | validation: 2.846484868979625]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.778839247761241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.778839247761241 | validation: 2.9125590490264472]
	TIME [epoch: 3.73 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.782662755379686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.782662755379686 | validation: 2.8462793543951728]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.835567444399659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.835567444399659 | validation: 3.52312822264001]
	TIME [epoch: 3.72 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.217826899546883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.217826899546883 | validation: 2.7614427176522103]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7384574485905033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7384574485905033 | validation: 2.8131722663116103]
	TIME [epoch: 3.7 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7291851550115314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7291851550115314 | validation: 2.813430050925062]
	TIME [epoch: 3.7 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.728756758199595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.728756758199595 | validation: 2.731710621382138]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.69071123479172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.69071123479172 | validation: 2.7297084931359303]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.652885943158162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.652885943158162 | validation: 2.74606377769145]
	TIME [epoch: 3.7 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6164962735841675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6164962735841675 | validation: 2.7146508222865515]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6009298512033148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6009298512033148 | validation: 2.7104490358847038]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.58753214661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.58753214661 | validation: 2.6955927753574453]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5769501528316896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5769501528316896 | validation: 2.6805736486318454]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5622835948450207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5622835948450207 | validation: 2.661522635764295]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.556971503915171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.556971503915171 | validation: 2.772100154500808]
	TIME [epoch: 3.72 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6023189376344984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6023189376344984 | validation: 2.839558122260257]
	TIME [epoch: 3.71 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8705762802386907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8705762802386907 | validation: 2.8172427279290817]
	TIME [epoch: 3.73 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6201774086992202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6201774086992202 | validation: 2.520614390604535]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.437475839346602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.437475839346602 | validation: 2.022540333955485]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0718978750725845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0718978750725845 | validation: 1.6531327907882833]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.705036857347659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.705036857347659 | validation: 1.3240857727871167]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.409633644161131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.409633644161131 | validation: 1.1448827147569596]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2288062825507333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2288062825507333 | validation: 1.24017032392797]
	TIME [epoch: 3.72 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2714461013115814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2714461013115814 | validation: 1.8375070367048167]
	TIME [epoch: 3.72 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1619709739715263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1619709739715263 | validation: 1.2843002615410497]
	TIME [epoch: 3.73 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3877607572652988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3877607572652988 | validation: 1.1895685486766046]
	TIME [epoch: 3.72 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2214185970613232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2214185970613232 | validation: 0.9305016148484087]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0220598174089293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0220598174089293 | validation: 0.9634188025733028]
	TIME [epoch: 3.72 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1054071905269476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1054071905269476 | validation: 0.8678697907480206]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9604268712342773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9604268712342773 | validation: 0.840216718328334]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9479284562960996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9479284562960996 | validation: 0.8362202713921283]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9364010789853725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9364010789853725 | validation: 0.8218182242188761]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9278738950316603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9278738950316603 | validation: 0.9196800060547736]
	TIME [epoch: 3.72 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9914788695960547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9914788695960547 | validation: 0.8651467655150249]
	TIME [epoch: 3.72 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.985920184689418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.985920184689418 | validation: 0.8187871719951462]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9000369586266146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9000369586266146 | validation: 0.8413011763185527]
	TIME [epoch: 3.72 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9259900170290004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9259900170290004 | validation: 0.7985360688323468]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9144710926532418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9144710926532418 | validation: 0.7898140745938168]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9004615952379743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9004615952379743 | validation: 0.8099439740083061]
	TIME [epoch: 3.72 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8944949222372712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8944949222372712 | validation: 0.8264791693661476]
	TIME [epoch: 3.74 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9553426844577765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9553426844577765 | validation: 0.963087985763799]
	TIME [epoch: 3.73 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.049725391973575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.049725391973575 | validation: 0.8587694536330107]
	TIME [epoch: 3.72 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9932967773149486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9932967773149486 | validation: 0.7857146781803585]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8778439434019749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8778439434019749 | validation: 0.771302754059294]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.876062541700188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.876062541700188 | validation: 0.786704882374464]
	TIME [epoch: 3.72 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8779401654190031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8779401654190031 | validation: 0.8399928301970644]
	TIME [epoch: 3.72 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9314599853075478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9314599853075478 | validation: 0.8733239182840986]
	TIME [epoch: 3.72 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0126322130544971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0126322130544971 | validation: 0.8231030569801243]
	TIME [epoch: 3.72 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8879330216538195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8879330216538195 | validation: 0.7809412055279179]
	TIME [epoch: 3.72 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8667340884474077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8667340884474077 | validation: 0.7807764694028823]
	TIME [epoch: 3.72 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8772374600136845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8772374600136845 | validation: 0.8496521551621701]
	TIME [epoch: 3.72 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9052304445782945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9052304445782945 | validation: 0.7912269613574334]
	TIME [epoch: 3.74 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8562819915559087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8562819915559087 | validation: 0.7999409872239499]
	TIME [epoch: 3.72 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8499067748956928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8499067748956928 | validation: 0.7941771288421502]
	TIME [epoch: 3.72 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.884141195859128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.884141195859128 | validation: 0.9140086668997933]
	TIME [epoch: 3.72 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9552678202132964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9552678202132964 | validation: 0.9783203079912621]
	TIME [epoch: 3.72 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0188315099496104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0188315099496104 | validation: 0.7535943543961835]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8625870825441642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8625870825441642 | validation: 0.8158218397546677]
	TIME [epoch: 3.72 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.882063170269139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.882063170269139 | validation: 0.7810355977054527]
	TIME [epoch: 3.72 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8394795144354612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8394795144354612 | validation: 0.80076519274425]
	TIME [epoch: 3.73 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.844132649603562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.844132649603562 | validation: 0.8783317809707016]
	TIME [epoch: 3.72 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9859214859795735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9859214859795735 | validation: 1.103223417135547]
	TIME [epoch: 3.72 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1955781884151868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1955781884151868 | validation: 0.7688794263319318]
	TIME [epoch: 3.72 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8755042822264513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8755042822264513 | validation: 0.7425068560643049]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8723683539090721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8723683539090721 | validation: 0.7941572923057475]
	TIME [epoch: 3.73 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.848241084198406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.848241084198406 | validation: 0.7776036374729935]
	TIME [epoch: 3.72 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8268000651754622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8268000651754622 | validation: 0.7883295189698051]
	TIME [epoch: 3.73 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8233979605938708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8233979605938708 | validation: 0.797019382349335]
	TIME [epoch: 3.72 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8399852914062885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8399852914062885 | validation: 0.8194477652909624]
	TIME [epoch: 3.72 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8492104523403776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8492104523403776 | validation: 0.7903068380754443]
	TIME [epoch: 3.72 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8604030977124616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8604030977124616 | validation: 0.7785132007575957]
	TIME [epoch: 3.72 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7950218728247811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7950218728247811 | validation: 0.7955379796831981]
	TIME [epoch: 3.72 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8927381057232386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8927381057232386 | validation: 1.192960449139539]
	TIME [epoch: 3.72 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3350849620635865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3350849620635865 | validation: 0.8564889639342145]
	TIME [epoch: 3.72 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0103657108691688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0103657108691688 | validation: 0.8939822644276376]
	TIME [epoch: 3.72 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.001357108025323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.001357108025323 | validation: 0.7753376051188547]
	TIME [epoch: 3.73 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8457236519080528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8457236519080528 | validation: 0.8040513323212184]
	TIME [epoch: 3.73 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.831110840410584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.831110840410584 | validation: 0.7724843060041453]
	TIME [epoch: 3.72 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8272367040166012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8272367040166012 | validation: 0.8005115339877389]
	TIME [epoch: 3.72 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8519559820952805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8519559820952805 | validation: 0.782928613601293]
	TIME [epoch: 3.72 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8838842164581646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8838842164581646 | validation: 0.7964084323350034]
	TIME [epoch: 3.72 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8029708038720551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8029708038720551 | validation: 0.748591225672374]
	TIME [epoch: 3.72 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.765761155907292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.765761155907292 | validation: 0.7650900169820698]
	TIME [epoch: 3.72 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8266557712807918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8266557712807918 | validation: 1.0690244177188386]
	TIME [epoch: 3.72 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1348451553719467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1348451553719467 | validation: 0.774428404123349]
	TIME [epoch: 3.72 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8722459563180819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8722459563180819 | validation: 0.7404171173644182]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8357456897771663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8357456897771663 | validation: 0.8170319175723952]
	TIME [epoch: 3.72 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8300725795003786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8300725795003786 | validation: 0.7335081991846522]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_158.pth
	Model improved!!!
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7830595561177516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7830595561177516 | validation: 0.7422363088029821]
	TIME [epoch: 3.73 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7556505286770081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7556505286770081 | validation: 0.7548486809769667]
	TIME [epoch: 3.69 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7589158916175603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7589158916175603 | validation: 0.8041930626172974]
	TIME [epoch: 3.71 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8135565128938652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8135565128938652 | validation: 1.0305984657182095]
	TIME [epoch: 3.7 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9961146277042118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9961146277042118 | validation: 0.7710186069617091]
	TIME [epoch: 3.7 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7983287912329936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7983287912329936 | validation: 0.7941887197052003]
	TIME [epoch: 3.7 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9297157050242699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9297157050242699 | validation: 0.8342669228576668]
	TIME [epoch: 3.7 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8605751143071478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8605751143071478 | validation: 0.6934182776582287]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_166.pth
	Model improved!!!
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.76702616179374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.76702616179374 | validation: 0.7224821172374098]
	TIME [epoch: 3.72 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7507989824302191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7507989824302191 | validation: 0.8110515494370723]
	TIME [epoch: 3.71 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7968045805647563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7968045805647563 | validation: 0.7572178318080445]
	TIME [epoch: 3.71 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7572130070078987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7572130070078987 | validation: 0.6883641595931262]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7252312085745305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7252312085745305 | validation: 0.8069800948742443]
	TIME [epoch: 3.72 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8352204674420172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8352204674420172 | validation: 1.0564751266943473]
	TIME [epoch: 3.72 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2627657481238512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2627657481238512 | validation: 0.7486320298030668]
	TIME [epoch: 3.72 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7956049822039338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7956049822039338 | validation: 0.7158868764959128]
	TIME [epoch: 3.72 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8089100268348657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8089100268348657 | validation: 0.7151058737431095]
	TIME [epoch: 3.71 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8149135966941122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8149135966941122 | validation: 0.7016935510025389]
	TIME [epoch: 3.71 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7669504475230545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7669504475230545 | validation: 0.7039543532966935]
	TIME [epoch: 3.72 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7476056652912898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7476056652912898 | validation: 0.6875798165747871]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_178.pth
	Model improved!!!
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7348567785901956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7348567785901956 | validation: 0.6873522711360276]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7330526599460672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7330526599460672 | validation: 0.7414052568526541]
	TIME [epoch: 3.71 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8144033952865657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8144033952865657 | validation: 1.022410040152602]
	TIME [epoch: 3.72 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0418070858606623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0418070858606623 | validation: 0.811377514349689]
	TIME [epoch: 3.73 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8040664191142051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8040664191142051 | validation: 0.6671080036182094]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_183.pth
	Model improved!!!
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7332942097441114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7332942097441114 | validation: 0.7393659850881655]
	TIME [epoch: 3.72 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7479973872645899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7479973872645899 | validation: 0.790955379861795]
	TIME [epoch: 3.72 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9271783763593316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9271783763593316 | validation: 0.7964914611427893]
	TIME [epoch: 3.72 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8507811524829048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8507811524829048 | validation: 0.6417082581504969]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_187.pth
	Model improved!!!
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.715934515043673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.715934515043673 | validation: 0.6452707293562945]
	TIME [epoch: 3.7 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6965311171745759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6965311171745759 | validation: 0.6799406371891341]
	TIME [epoch: 3.69 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.719376365581021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.719376365581021 | validation: 0.777856161590586]
	TIME [epoch: 3.69 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.758940479519431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.758940479519431 | validation: 0.788667432972596]
	TIME [epoch: 3.7 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8080816559721729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8080816559721729 | validation: 0.68129926253357]
	TIME [epoch: 3.7 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6783742959194804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6783742959194804 | validation: 0.6799235823348471]
	TIME [epoch: 3.7 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6946377273761652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6946377273761652 | validation: 0.7325378305612107]
	TIME [epoch: 3.72 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8769321883815675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8769321883815675 | validation: 1.053049866342354]
	TIME [epoch: 3.71 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.15635593858856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.15635593858856 | validation: 0.6623830821082909]
	TIME [epoch: 3.7 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.721204977836541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.721204977836541 | validation: 0.7207397593833528]
	TIME [epoch: 3.7 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8087707146646524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8087707146646524 | validation: 0.6257499473863438]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6765356317135327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6765356317135327 | validation: 0.643433863134065]
	TIME [epoch: 3.72 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6918582900723584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6918582900723584 | validation: 0.6575442903556727]
	TIME [epoch: 3.72 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6931060412604936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6931060412604936 | validation: 0.727364332138992]
	TIME [epoch: 48.2 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.753261578861332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.753261578861332 | validation: 0.7828221314126794]
	TIME [epoch: 8.02 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7580751395974282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7580751395974282 | validation: 0.7133607158618607]
	TIME [epoch: 8.09 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7933127805797608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7933127805797608 | validation: 0.8523745146254227]
	TIME [epoch: 8.09 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8765489939014407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8765489939014407 | validation: 0.8453083036332298]
	TIME [epoch: 8.06 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0256017987675239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0256017987675239 | validation: 0.6522650243214018]
	TIME [epoch: 8.11 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6949162390500514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6949162390500514 | validation: 0.6306110094107675]
	TIME [epoch: 8.07 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6719099499815504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6719099499815504 | validation: 0.5953508576636285]
	TIME [epoch: 8.06 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6774757611295364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6774757611295364 | validation: 0.6719678357079869]
	TIME [epoch: 8.06 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6802339498024176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6802339498024176 | validation: 0.6026139975372362]
	TIME [epoch: 8.06 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.712578778697395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.712578778697395 | validation: 0.8058537440530682]
	TIME [epoch: 8.06 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7589767804549152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7589767804549152 | validation: 0.6773823609776669]
	TIME [epoch: 8.08 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7829137684317533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7829137684317533 | validation: 0.6253825383357177]
	TIME [epoch: 8.05 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6287496371793049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6287496371793049 | validation: 0.5906775857976974]
	TIME [epoch: 8.06 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_214.pth
	Model improved!!!
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6148112695295066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6148112695295066 | validation: 0.6129138398753913]
	TIME [epoch: 8.05 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6492398630267312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6492398630267312 | validation: 0.7054776135366853]
	TIME [epoch: 8.06 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.678737319718094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.678737319718094 | validation: 0.682009016529224]
	TIME [epoch: 8.07 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6996379102380609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6996379102380609 | validation: 0.5965663365554196]
	TIME [epoch: 8.07 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6087034224139656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6087034224139656 | validation: 0.6765853584670913]
	TIME [epoch: 8.07 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6954036956732158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6954036956732158 | validation: 0.9353208962834725]
	TIME [epoch: 8.08 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.195049364612183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.195049364612183 | validation: 0.763050046296994]
	TIME [epoch: 8.07 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8388880935185039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8388880935185039 | validation: 0.6347509704527342]
	TIME [epoch: 8.07 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7056462234796337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7056462234796337 | validation: 0.6119300727223712]
	TIME [epoch: 8.1 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7003110157146191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7003110157146191 | validation: 0.5837112438575126]
	TIME [epoch: 8.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_224.pth
	Model improved!!!
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6232097590631777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6232097590631777 | validation: 0.551301674275597]
	TIME [epoch: 8.03 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_225.pth
	Model improved!!!
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6009877280367953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6009877280367953 | validation: 0.5907150447685634]
	TIME [epoch: 8.07 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6033048642552737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6033048642552737 | validation: 0.6531098513106154]
	TIME [epoch: 8.08 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7157473028439421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7157473028439421 | validation: 0.8370128268768788]
	TIME [epoch: 8.07 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7681939910578061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7681939910578061 | validation: 0.6663396675226875]
	TIME [epoch: 8.09 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7175079223140938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7175079223140938 | validation: 0.5579523275534923]
	TIME [epoch: 8.07 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5667041655596818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5667041655596818 | validation: 0.5666478819075588]
	TIME [epoch: 8.06 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5841810108373595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5841810108373595 | validation: 0.6956273936373467]
	TIME [epoch: 8.06 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.71035949946497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.71035949946497 | validation: 0.7874694764613231]
	TIME [epoch: 8.08 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8462677958383864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8462677958383864 | validation: 0.7947583899820162]
	TIME [epoch: 8.1 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8604621208803156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8604621208803156 | validation: 0.5863576397649058]
	TIME [epoch: 8.07 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6673017146338048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6673017146338048 | validation: 0.767514135210849]
	TIME [epoch: 8.08 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7492047368126561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7492047368126561 | validation: 0.5693753450447904]
	TIME [epoch: 8.09 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6180482819053176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6180482819053176 | validation: 0.550437493418304]
	TIME [epoch: 8.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_238.pth
	Model improved!!!
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5594569592244355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5594569592244355 | validation: 0.530845719000827]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5625543420179442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5625543420179442 | validation: 0.5804675514764125]
	TIME [epoch: 8.06 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.590505016134192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.590505016134192 | validation: 0.5941946832383828]
	TIME [epoch: 8.08 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6325360260673771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6325360260673771 | validation: 0.7373344475349605]
	TIME [epoch: 8.08 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7329151204147241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7329151204147241 | validation: 0.5938270243204408]
	TIME [epoch: 8.09 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7182767314420457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7182767314420457 | validation: 0.8427216289850112]
	TIME [epoch: 8.07 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8010711669512405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8010711669512405 | validation: 0.53777805864892]
	TIME [epoch: 8.08 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6051409785830353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6051409785830353 | validation: 0.5329592776743354]
	TIME [epoch: 8.09 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.550243954284206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.550243954284206 | validation: 0.5154192327137516]
	TIME [epoch: 8.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5376276541178148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5376276541178148 | validation: 0.5676285554059218]
	TIME [epoch: 8.01 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5540776164677383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5540776164677383 | validation: 0.5882621565360804]
	TIME [epoch: 8.08 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5899186199503099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5899186199503099 | validation: 0.6320445808986954]
	TIME [epoch: 8.08 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6454128964237265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6454128964237265 | validation: 0.543893223224011]
	TIME [epoch: 8.12 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5598825630834579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5598825630834579 | validation: 0.5042257223480479]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_252.pth
	Model improved!!!
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5114627814282282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5114627814282282 | validation: 0.5723956028001517]
	TIME [epoch: 8.1 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5363117582610188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5363117582610188 | validation: 0.6280964266596714]
	TIME [epoch: 8.08 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7637280438843317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7637280438843317 | validation: 0.9577415987182528]
	TIME [epoch: 8.07 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0083043886138503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0083043886138503 | validation: 0.4913029969720948]
	TIME [epoch: 8.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_256.pth
	Model improved!!!
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5646051806014624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5646051806014624 | validation: 0.4932347791205525]
	TIME [epoch: 8.09 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4979790285697459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4979790285697459 | validation: 0.49477497847878293]
	TIME [epoch: 8.07 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4889228308078501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4889228308078501 | validation: 0.4780074874691925]
	TIME [epoch: 8.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_259.pth
	Model improved!!!
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49995624275104034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49995624275104034 | validation: 0.6694008644489968]
	TIME [epoch: 8.08 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5900815158431815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5900815158431815 | validation: 0.6008137241541486]
	TIME [epoch: 8.08 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7092932654991226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7092932654991226 | validation: 0.6792632399382633]
	TIME [epoch: 8.09 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6295175777347264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6295175777347264 | validation: 0.4749544571460398]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_263.pth
	Model improved!!!
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5318978833501099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5318978833501099 | validation: 0.5911797105724971]
	TIME [epoch: 8.01 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5830390512004962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5830390512004962 | validation: 0.6400838189502257]
	TIME [epoch: 8.04 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.65791345106497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.65791345106497 | validation: 0.6046886086213905]
	TIME [epoch: 8.04 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6277463618979583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6277463618979583 | validation: 0.5440803502052173]
	TIME [epoch: 8.03 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5491828039836197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5491828039836197 | validation: 0.5027239716173443]
	TIME [epoch: 8.03 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6365595982066732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6365595982066732 | validation: 0.6470899797413256]
	TIME [epoch: 8.04 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5762625326663703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5762625326663703 | validation: 0.5234845530609377]
	TIME [epoch: 8.02 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5758870914375263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5758870914375263 | validation: 0.540487672296638]
	TIME [epoch: 8.03 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5061335223392357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5061335223392357 | validation: 0.4848624083456352]
	TIME [epoch: 8.04 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5183493489483803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5183493489483803 | validation: 0.4672855342514417]
	TIME [epoch: 8.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_273.pth
	Model improved!!!
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48294698611904924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48294698611904924 | validation: 0.499976035528602]
	TIME [epoch: 8.1 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4984814969812423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4984814969812423 | validation: 0.46744686533158974]
	TIME [epoch: 8.09 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48918888037021274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48918888037021274 | validation: 0.5328288863484635]
	TIME [epoch: 8.08 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5169069534791535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5169069534791535 | validation: 0.48918716769010717]
	TIME [epoch: 8.1 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.602190175092911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.602190175092911 | validation: 0.9303721400637293]
	TIME [epoch: 8.09 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7903815715005813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7903815715005813 | validation: 0.5016011907083935]
	TIME [epoch: 8.09 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5686414942048151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5686414942048151 | validation: 0.4276156217137538]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_280.pth
	Model improved!!!
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4330063306495602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4330063306495602 | validation: 0.45204752620320526]
	TIME [epoch: 8.07 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4265928789419025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4265928789419025 | validation: 0.43368326882325925]
	TIME [epoch: 8.08 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45777156783299205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45777156783299205 | validation: 0.5565940129515189]
	TIME [epoch: 8.07 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5261188977029596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5261188977029596 | validation: 0.44573619816387855]
	TIME [epoch: 8.06 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5100468896666568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5100468896666568 | validation: 0.43770756671650934]
	TIME [epoch: 8.08 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4029247390107372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4029247390107372 | validation: 0.3847419073403518]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_286.pth
	Model improved!!!
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39111966172757434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39111966172757434 | validation: 0.5142767829113041]
	TIME [epoch: 8.04 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4648007070953393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4648007070953393 | validation: 0.7772254146320287]
	TIME [epoch: 8.07 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8714785877725681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8714785877725681 | validation: 0.7012215190558868]
	TIME [epoch: 8.05 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6923170123196094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6923170123196094 | validation: 0.4518626061169851]
	TIME [epoch: 8.03 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5195590726429689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5195590726429689 | validation: 0.4375712096407515]
	TIME [epoch: 8.07 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4194102359295786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4194102359295786 | validation: 0.4578370628397581]
	TIME [epoch: 8.07 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4100178178177758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4100178178177758 | validation: 0.49457082456525336]
	TIME [epoch: 8.07 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5425741316311347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5425741316311347 | validation: 0.5461424187291865]
	TIME [epoch: 8.06 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46187048158186483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46187048158186483 | validation: 0.3682448891658421]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_295.pth
	Model improved!!!
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3848450411308376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3848450411308376 | validation: 0.4037596912280992]
	TIME [epoch: 8.05 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3683628601736065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3683628601736065 | validation: 0.40802483961648567]
	TIME [epoch: 8.04 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40961031651039503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40961031651039503 | validation: 0.5294239767673148]
	TIME [epoch: 8.07 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5211704348435922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5211704348435922 | validation: 0.5279384980874838]
	TIME [epoch: 8.04 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5704021861023338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5704021861023338 | validation: 0.4628440314142177]
	TIME [epoch: 8.04 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5784364286501038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5784364286501038 | validation: 0.9379924617174304]
	TIME [epoch: 8.08 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7961668526707203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7961668526707203 | validation: 0.4199580781452512]
	TIME [epoch: 8.07 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46561596175975684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46561596175975684 | validation: 0.32935950913262374]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_303.pth
	Model improved!!!
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35513001981588815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35513001981588815 | validation: 0.45959056803856413]
	TIME [epoch: 8.05 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39591768944204936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39591768944204936 | validation: 0.39163132684266344]
	TIME [epoch: 8.07 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4057920584020171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4057920584020171 | validation: 0.5443279718606409]
	TIME [epoch: 8.09 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42310732311064836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42310732311064836 | validation: 0.3448699693485974]
	TIME [epoch: 8.08 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40285108332303876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40285108332303876 | validation: 0.5602558780007243]
	TIME [epoch: 8.12 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46532908388459826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46532908388459826 | validation: 0.31463247182716003]
	TIME [epoch: 8.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_309.pth
	Model improved!!!
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36031864896286137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36031864896286137 | validation: 0.44615610068387906]
	TIME [epoch: 8.06 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35209170050531086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35209170050531086 | validation: 0.31974162592767413]
	TIME [epoch: 8.1 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3773080389923294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3773080389923294 | validation: 0.5470128867025327]
	TIME [epoch: 8.1 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42885500482767797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42885500482767797 | validation: 0.40692342763852035]
	TIME [epoch: 8.11 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4088955467683804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4088955467683804 | validation: 0.39306945275738436]
	TIME [epoch: 8.09 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4111428987007713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4111428987007713 | validation: 0.6454782713039297]
	TIME [epoch: 8.11 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6143271992053767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6143271992053767 | validation: 0.33252634333435993]
	TIME [epoch: 8.09 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36153318889599123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36153318889599123 | validation: 0.5349269854216947]
	TIME [epoch: 8.11 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3993536432384208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3993536432384208 | validation: 0.4818695264705876]
	TIME [epoch: 8.08 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5134719981674896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5134719981674896 | validation: 0.42092937250085893]
	TIME [epoch: 8.11 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3550679328427658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3550679328427658 | validation: 0.2947164510273277]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_320.pth
	Model improved!!!
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28548121314960406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28548121314960406 | validation: 0.34455840463223253]
	TIME [epoch: 8.1 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.316986647344611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.316986647344611 | validation: 0.34304599356895693]
	TIME [epoch: 8.08 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29641424423258733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29641424423258733 | validation: 0.29080698320981807]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_323.pth
	Model improved!!!
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28443345310346013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28443345310346013 | validation: 0.31666816132419257]
	TIME [epoch: 8.1 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2688437398297252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2688437398297252 | validation: 0.271910660993942]
	TIME [epoch: 8.12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_325.pth
	Model improved!!!
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30342221489564736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30342221489564736 | validation: 0.7641964233688898]
	TIME [epoch: 8.11 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5900540672459081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5900540672459081 | validation: 0.5337314822606138]
	TIME [epoch: 8.1 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6474978623722437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6474978623722437 | validation: 0.6688931150003662]
	TIME [epoch: 8.11 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6429100868551083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6429100868551083 | validation: 0.5707271202793897]
	TIME [epoch: 8.1 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48739864256162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48739864256162 | validation: 0.3501557561765086]
	TIME [epoch: 8.11 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4270060045620426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4270060045620426 | validation: 0.3742206443032654]
	TIME [epoch: 8.12 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3138018795205754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3138018795205754 | validation: 0.2885523908712159]
	TIME [epoch: 8.11 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24870731757244877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24870731757244877 | validation: 0.242248695339936]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_333.pth
	Model improved!!!
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25722231985693894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25722231985693894 | validation: 0.4170552042891987]
	TIME [epoch: 8.1 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29549562367523774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29549562367523774 | validation: 0.3217278012299636]
	TIME [epoch: 8.1 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3708477506965317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3708477506965317 | validation: 0.5437822119396073]
	TIME [epoch: 8.1 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38390044235551785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38390044235551785 | validation: 0.2653046605632757]
	TIME [epoch: 8.11 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2555738228374728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2555738228374728 | validation: 0.3003210028129333]
	TIME [epoch: 8.09 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27042205107182127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27042205107182127 | validation: 0.294643929285794]
	TIME [epoch: 8.09 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28292431489581465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28292431489581465 | validation: 0.3005873671796959]
	TIME [epoch: 8.1 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2782992197540844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2782992197540844 | validation: 0.5109740647639333]
	TIME [epoch: 8.1 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3536986629508593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3536986629508593 | validation: 0.6062089036319178]
	TIME [epoch: 8.12 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6729565977550094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6729565977550094 | validation: 0.3430006408298171]
	TIME [epoch: 8.1 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3316384753027859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3316384753027859 | validation: 0.9083115726135422]
	TIME [epoch: 8.09 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0693419252795442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0693419252795442 | validation: 0.7001107304795945]
	TIME [epoch: 8.1 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7080652877325082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7080652877325082 | validation: 0.26806247312061443]
	TIME [epoch: 8.1 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36832905128068916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36832905128068916 | validation: 0.6497610336621752]
	TIME [epoch: 8.1 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4577923270687853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4577923270687853 | validation: 0.504102354840067]
	TIME [epoch: 8.12 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6525373659966832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6525373659966832 | validation: 0.28480380156762847]
	TIME [epoch: 8.1 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2963581290762774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2963581290762774 | validation: 0.3518102957962952]
	TIME [epoch: 8.1 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30678156920861716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30678156920861716 | validation: 0.2502737620685009]
	TIME [epoch: 8.1 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29745314944397533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29745314944397533 | validation: 0.4105933588216621]
	TIME [epoch: 8.1 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2803105970085935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2803105970085935 | validation: 0.2373112924853198]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_353.pth
	Model improved!!!
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24893221253091574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24893221253091574 | validation: 0.2781593974717018]
	TIME [epoch: 8.11 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22026515764552387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22026515764552387 | validation: 0.21298014898909834]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22706809512205922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22706809512205922 | validation: 0.4284747251528339]
	TIME [epoch: 8.04 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3073616103792619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3073616103792619 | validation: 0.33451364239818]
	TIME [epoch: 8.04 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37038740208934934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37038740208934934 | validation: 0.4498674440181086]
	TIME [epoch: 8.05 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3258496530437519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3258496530437519 | validation: 0.22283415829107614]
	TIME [epoch: 8.08 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24509713528392485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24509713528392485 | validation: 0.2833489732569334]
	TIME [epoch: 8.06 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20894003329027364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20894003329027364 | validation: 0.19939476514323717]
	TIME [epoch: 8.05 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_361.pth
	Model improved!!!
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20039556337820813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20039556337820813 | validation: 0.3192529795710475]
	TIME [epoch: 8.08 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.240761866340556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.240761866340556 | validation: 0.34980671414651954]
	TIME [epoch: 8.07 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37354165754184737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37354165754184737 | validation: 0.25719166037808233]
	TIME [epoch: 8.06 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20894745147984708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20894745147984708 | validation: 0.20889786008441913]
	TIME [epoch: 8.03 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17824991266895931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17824991266895931 | validation: 0.23819860942378793]
	TIME [epoch: 8.09 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20415132255995772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20415132255995772 | validation: 0.3093316484679021]
	TIME [epoch: 8.07 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25743221120930676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25743221120930676 | validation: 0.2369997420278748]
	TIME [epoch: 8.07 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24301379207200796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24301379207200796 | validation: 0.6709235727623943]
	TIME [epoch: 8.09 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.516629740213858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.516629740213858 | validation: 0.6181154605821718]
	TIME [epoch: 8.1 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6926413555874456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6926413555874456 | validation: 0.32993183631034184]
	TIME [epoch: 8.08 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3100227702354911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3100227702354911 | validation: 0.6724563953322449]
	TIME [epoch: 8.03 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6133062301242836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6133062301242836 | validation: 0.9821772634358823]
	TIME [epoch: 7.93 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9787908265034535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9787908265034535 | validation: 0.2593856191745208]
	TIME [epoch: 7.91 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3063495822210039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3063495822210039 | validation: 0.5490721155094332]
	TIME [epoch: 7.92 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3832892522446335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3832892522446335 | validation: 0.5271144740998952]
	TIME [epoch: 8.04 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6049803944585402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6049803944585402 | validation: 0.33891138796990594]
	TIME [epoch: 8.04 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2546045853488946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2546045853488946 | validation: 0.19127312112998066]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_378.pth
	Model improved!!!
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19056616966829049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19056616966829049 | validation: 0.2321265740750676]
	TIME [epoch: 8.04 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18114864671970868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18114864671970868 | validation: 0.19831133344881244]
	TIME [epoch: 8.06 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2109914602887092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2109914602887092 | validation: 0.5802005716109151]
	TIME [epoch: 8.07 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45185624871089664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45185624871089664 | validation: 0.5641729658692679]
	TIME [epoch: 8.09 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5960867450947596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5960867450947596 | validation: 0.5391546082827939]
	TIME [epoch: 8.08 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5719258703182488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5719258703182488 | validation: 0.3607369326173558]
	TIME [epoch: 8.07 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38021717464223187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38021717464223187 | validation: 0.2673998270529183]
	TIME [epoch: 8.08 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2505028568138416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2505028568138416 | validation: 0.31908864780904783]
	TIME [epoch: 8.08 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2706667123077362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2706667123077362 | validation: 0.2931216443950949]
	TIME [epoch: 8.08 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2735913515058915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2735913515058915 | validation: 0.23776809059999052]
	TIME [epoch: 8.09 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.177727832354283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.177727832354283 | validation: 0.18748661912958908]
	TIME [epoch: 8.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_389.pth
	Model improved!!!
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17061143530753184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17061143530753184 | validation: 0.47704807170648345]
	TIME [epoch: 8 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32668635465234624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32668635465234624 | validation: 0.5377343134677631]
	TIME [epoch: 8.01 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6280249921687596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6280249921687596 | validation: 0.5482641957911646]
	TIME [epoch: 8 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4773213105180983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4773213105180983 | validation: 0.7367140165150409]
	TIME [epoch: 8.03 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6550291467904732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6550291467904732 | validation: 0.47583870042715176]
	TIME [epoch: 8.05 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.504929518473282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.504929518473282 | validation: 0.37309941684449505]
	TIME [epoch: 8.02 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28348864556521114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28348864556521114 | validation: 0.3066992630187352]
	TIME [epoch: 8.02 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22863262795967895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22863262795967895 | validation: 0.25858180544469006]
	TIME [epoch: 8.02 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33119348644577573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33119348644577573 | validation: 0.33519427868657686]
	TIME [epoch: 8.05 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.238035417869295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.238035417869295 | validation: 0.2625674110318212]
	TIME [epoch: 8.02 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3031318339433416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3031318339433416 | validation: 0.21990173951243666]
	TIME [epoch: 8.03 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1927607016944333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1927607016944333 | validation: 0.19647310051127123]
	TIME [epoch: 8.07 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16135495276540382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16135495276540382 | validation: 0.19201530218665186]
	TIME [epoch: 8.08 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1634346933254929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1634346933254929 | validation: 0.2832132648518753]
	TIME [epoch: 8.06 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19457160084590172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19457160084590172 | validation: 0.25097157466094594]
	TIME [epoch: 8.05 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.296867391856553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.296867391856553 | validation: 0.268918074590147]
	TIME [epoch: 8.1 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20295764194061788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20295764194061788 | validation: 0.18626828193199785]
	TIME [epoch: 8.06 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_406.pth
	Model improved!!!
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17845838496304592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17845838496304592 | validation: 0.328205587794129]
	TIME [epoch: 8.05 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22935144946744643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22935144946744643 | validation: 0.27101395190939676]
	TIME [epoch: 8.06 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2843128691495619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2843128691495619 | validation: 0.3301969101560648]
	TIME [epoch: 8.07 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2420857068026575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2420857068026575 | validation: 0.1992572206678901]
	TIME [epoch: 8.07 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1812664751823324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1812664751823324 | validation: 0.17913162920864445]
	TIME [epoch: 8.06 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_411.pth
	Model improved!!!
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17286109705575545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17286109705575545 | validation: 0.4729157543260451]
	TIME [epoch: 8.06 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3189439102533125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3189439102533125 | validation: 0.5273539279206106]
	TIME [epoch: 8.06 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5555820549629462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5555820549629462 | validation: 0.32164321757142345]
	TIME [epoch: 8.05 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2756472495587688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2756472495587688 | validation: 0.4361238802431414]
	TIME [epoch: 8.04 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.399804599367776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.399804599367776 | validation: 0.4147099603172045]
	TIME [epoch: 8.03 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4123457505902379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4123457505902379 | validation: 0.18919148306551742]
	TIME [epoch: 8.07 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22701583769191658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22701583769191658 | validation: 0.7830430171830152]
	TIME [epoch: 8.06 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5646775786301237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5646775786301237 | validation: 0.3936361667920285]
	TIME [epoch: 8.07 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41052310717376855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41052310717376855 | validation: 0.28771299601018635]
	TIME [epoch: 8.04 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23811966183102173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23811966183102173 | validation: 0.2513723378317937]
	TIME [epoch: 8.05 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20900149132727103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20900149132727103 | validation: 0.24068751052573242]
	TIME [epoch: 8.06 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19907741768228537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19907741768228537 | validation: 0.18026998990741147]
	TIME [epoch: 8.06 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17029574289161598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17029574289161598 | validation: 0.23716393249398396]
	TIME [epoch: 8.03 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16890556106775947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16890556106775947 | validation: 0.2844716783012207]
	TIME [epoch: 8.06 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3014908647196162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3014908647196162 | validation: 0.3939275180018693]
	TIME [epoch: 8.08 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26660374709059254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26660374709059254 | validation: 0.2307427733948938]
	TIME [epoch: 8.06 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2347183739271018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2347183739271018 | validation: 0.2579343674709501]
	TIME [epoch: 8.06 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20291663783146058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20291663783146058 | validation: 0.16762968735327644]
	TIME [epoch: 8.02 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_429.pth
	Model improved!!!
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1712073914773002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1712073914773002 | validation: 0.275936365133063]
	TIME [epoch: 8.08 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20667030873011577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20667030873011577 | validation: 0.2722751555199082]
	TIME [epoch: 8.07 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26947210765741975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26947210765741975 | validation: 0.35406575648433547]
	TIME [epoch: 8.06 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24881636580294583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24881636580294583 | validation: 0.20624243086373886]
	TIME [epoch: 8.08 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20850503161032002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20850503161032002 | validation: 0.20402189217147906]
	TIME [epoch: 8.09 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16051023682883966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16051023682883966 | validation: 0.1524081683282457]
	TIME [epoch: 8.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_435.pth
	Model improved!!!
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1353807898633591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1353807898633591 | validation: 0.1565097542641888]
	TIME [epoch: 8.03 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12469267370576553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12469267370576553 | validation: 0.19986040709916675]
	TIME [epoch: 8.08 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15162039339911051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15162039339911051 | validation: 0.32977353473996046]
	TIME [epoch: 8.05 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37485989682367016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37485989682367016 | validation: 0.3918093525966404]
	TIME [epoch: 8.07 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24951867222071328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24951867222071328 | validation: 0.2269643165274552]
	TIME [epoch: 8.05 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23154414404212972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23154414404212972 | validation: 0.3344472813865397]
	TIME [epoch: 8.06 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27823695171718826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27823695171718826 | validation: 0.4847679952609628]
	TIME [epoch: 8.07 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4296476471306916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4296476471306916 | validation: 0.4547973565153591]
	TIME [epoch: 8.07 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3565641370628505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3565641370628505 | validation: 0.24474660269091542]
	TIME [epoch: 8.08 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3437813391995666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3437813391995666 | validation: 0.6667338771057039]
	TIME [epoch: 8.05 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5072138362746268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5072138362746268 | validation: 0.7486837585913932]
	TIME [epoch: 8.07 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7978267407430636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7978267407430636 | validation: 0.6083929933029973]
	TIME [epoch: 8.04 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5626004870343636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5626004870343636 | validation: 0.484241455869244]
	TIME [epoch: 8.08 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37229336978378125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37229336978378125 | validation: 0.38977085910654496]
	TIME [epoch: 8.08 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3107950591822112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3107950591822112 | validation: 0.35128001879705884]
	TIME [epoch: 8.06 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2832296700334503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2832296700334503 | validation: 0.3026121160319093]
	TIME [epoch: 8.11 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2991248374763505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2991248374763505 | validation: 0.3798485858264828]
	TIME [epoch: 8.05 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3037057546745139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3037057546745139 | validation: 0.26055838255747255]
	TIME [epoch: 8.05 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2583949651481702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2583949651481702 | validation: 0.31146296065128154]
	TIME [epoch: 8.09 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24200074845253497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24200074845253497 | validation: 0.2187532442781615]
	TIME [epoch: 8.08 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2183798953813889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2183798953813889 | validation: 0.18145137814143317]
	TIME [epoch: 8.1 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14642858597893785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14642858597893785 | validation: 0.15910275049415065]
	TIME [epoch: 8.08 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16145548348328062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16145548348328062 | validation: 0.32746769306152895]
	TIME [epoch: 8.1 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22724772071418906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22724772071418906 | validation: 0.2804497246673153]
	TIME [epoch: 8.09 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2993900667354933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2993900667354933 | validation: 0.2524511383836962]
	TIME [epoch: 8.05 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17569994888888416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17569994888888416 | validation: 0.1667450239014486]
	TIME [epoch: 8.09 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1621351743772995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1621351743772995 | validation: 0.2602502165868739]
	TIME [epoch: 8.1 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17348358278608228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17348358278608228 | validation: 0.26576173443211776]
	TIME [epoch: 8.1 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.258970935364618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.258970935364618 | validation: 0.3999449219069289]
	TIME [epoch: 8.07 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28863676054257315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28863676054257315 | validation: 0.1903194492827456]
	TIME [epoch: 8.04 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19178129608046135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19178129608046135 | validation: 0.1846791711384541]
	TIME [epoch: 8.04 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1441974574065144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1441974574065144 | validation: 0.145552145102815]
	TIME [epoch: 8.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_467.pth
	Model improved!!!
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14122690649503997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14122690649503997 | validation: 0.24610667498690422]
	TIME [epoch: 8.06 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.185436102054238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.185436102054238 | validation: 0.18940316461156995]
	TIME [epoch: 8.05 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23721015296298972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23721015296298972 | validation: 0.25757396904605867]
	TIME [epoch: 8.05 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16977594303139742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16977594303139742 | validation: 0.14465939477803924]
	TIME [epoch: 8.07 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_471.pth
	Model improved!!!
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17819182983938206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17819182983938206 | validation: 0.41155829803035965]
	TIME [epoch: 8.02 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27870794881171934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27870794881171934 | validation: 0.46991545890259623]
	TIME [epoch: 8.06 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48324448608655235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48324448608655235 | validation: 0.2610259716394067]
	TIME [epoch: 8.09 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22499442543925355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22499442543925355 | validation: 0.21164003182067115]
	TIME [epoch: 8.05 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15568690872371005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15568690872371005 | validation: 0.18511252819253482]
	TIME [epoch: 8.05 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19135160917476632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19135160917476632 | validation: 0.22815573634995925]
	TIME [epoch: 8.05 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1450046963224046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1450046963224046 | validation: 0.13600374938928464]
	TIME [epoch: 8.03 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_478.pth
	Model improved!!!
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17794515148174353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17794515148174353 | validation: 0.33529100604345774]
	TIME [epoch: 8.03 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20640558215660956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20640558215660956 | validation: 0.19208229474504696]
	TIME [epoch: 8.05 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20365190800483463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20365190800483463 | validation: 0.1827837188623903]
	TIME [epoch: 8 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15396494667041558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15396494667041558 | validation: 0.24921947131458141]
	TIME [epoch: 8.02 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19809382616673074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19809382616673074 | validation: 0.29120912248941766]
	TIME [epoch: 8.02 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29511459275149887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29511459275149887 | validation: 0.592013195325212]
	TIME [epoch: 8.06 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4721563599129084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4721563599129084 | validation: 0.1777021428881425]
	TIME [epoch: 8.05 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25112121483669925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25112121483669925 | validation: 0.22499632994115945]
	TIME [epoch: 8.04 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15434376066637204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15434376066637204 | validation: 0.17235193082291525]
	TIME [epoch: 8.03 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14323585324489366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14323585324489366 | validation: 0.1756635604220234]
	TIME [epoch: 8.01 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14981268880096463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14981268880096463 | validation: 0.13977274197181083]
	TIME [epoch: 8.03 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14141987471962764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14141987471962764 | validation: 0.36248805836933434]
	TIME [epoch: 8.03 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22377411288995425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22377411288995425 | validation: 0.3216632015680727]
	TIME [epoch: 8.04 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3687940089888695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3687940089888695 | validation: 0.46173199345127003]
	TIME [epoch: 8.04 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42242459309281133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42242459309281133 | validation: 0.4322011742257916]
	TIME [epoch: 8 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35409019070270814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35409019070270814 | validation: 0.2062395886991023]
	TIME [epoch: 8.05 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20462815319858252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20462815319858252 | validation: 0.13198599122002083]
	TIME [epoch: 8.04 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_495.pth
	Model improved!!!
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11785578845900202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11785578845900202 | validation: 0.3814098162728146]
	TIME [epoch: 8.08 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22747099050482747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22747099050482747 | validation: 0.5883628852711044]
	TIME [epoch: 8.09 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6842917791805215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6842917791805215 | validation: 0.17734494173097853]
	TIME [epoch: 8.02 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22322439140589165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22322439140589165 | validation: 0.4726607023897537]
	TIME [epoch: 8 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45019607128375727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45019607128375727 | validation: 0.33647267702529005]
	TIME [epoch: 7.98 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2653281026274992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2653281026274992 | validation: 0.39336513349052915]
	TIME [epoch: 58.7 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2625433678213226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2625433678213226 | validation: 0.1999311119661264]
	TIME [epoch: 16.9 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23854062338259108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23854062338259108 | validation: 0.7057062983273088]
	TIME [epoch: 17.1 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5515323131070872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5515323131070872 | validation: 0.22687519154334634]
	TIME [epoch: 17.2 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16692091198856115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16692091198856115 | validation: 0.21864000794668703]
	TIME [epoch: 17.2 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2529518054503046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2529518054503046 | validation: 0.18151964840595936]
	TIME [epoch: 17.2 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1781254200566606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1781254200566606 | validation: 0.18640160074192963]
	TIME [epoch: 17.2 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14113257356328915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14113257356328915 | validation: 0.39094299664720045]
	TIME [epoch: 17.2 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2541293966786716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2541293966786716 | validation: 0.16807958710937773]
	TIME [epoch: 17.3 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18944964847082627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18944964847082627 | validation: 0.14953520469819467]
	TIME [epoch: 17.2 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1393984901048059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1393984901048059 | validation: 0.1806071292858603]
	TIME [epoch: 17.2 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13467527168685936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13467527168685936 | validation: 0.15470113659304163]
	TIME [epoch: 17.1 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11921898220817018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11921898220817018 | validation: 0.13134709773364958]
	TIME [epoch: 17.2 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_513.pth
	Model improved!!!
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12731512852915122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12731512852915122 | validation: 0.2130125409383919]
	TIME [epoch: 17.1 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15301358367427081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15301358367427081 | validation: 0.1428294634319142]
	TIME [epoch: 17.2 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1828501643055644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1828501643055644 | validation: 0.26501439685987604]
	TIME [epoch: 17.1 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16711990940547836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16711990940547836 | validation: 0.1386690671414683]
	TIME [epoch: 17.2 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15197700138182302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15197700138182302 | validation: 0.15876671329693315]
	TIME [epoch: 17.1 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1269790092749916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1269790092749916 | validation: 0.12056984955219643]
	TIME [epoch: 17.2 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_519.pth
	Model improved!!!
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12626121768279272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12626121768279272 | validation: 0.19885880288138613]
	TIME [epoch: 17.2 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14570947195261094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14570947195261094 | validation: 0.18145934480822346]
	TIME [epoch: 17.2 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2041383704791209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2041383704791209 | validation: 0.6247648709109028]
	TIME [epoch: 17.2 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5115747962338234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5115747962338234 | validation: 0.144852740505815]
	TIME [epoch: 17.3 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14678690871278485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14678690871278485 | validation: 0.23535085498707087]
	TIME [epoch: 17.2 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2184265663473731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2184265663473731 | validation: 0.65405466573523]
	TIME [epoch: 17.3 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5651711464190183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5651711464190183 | validation: 0.30728055745864646]
	TIME [epoch: 17.3 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2369222475431392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2369222475431392 | validation: 0.25499706485868784]
	TIME [epoch: 17.3 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30202801959661446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30202801959661446 | validation: 0.2627974959072466]
	TIME [epoch: 17.2 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1698787780005187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1698787780005187 | validation: 0.1400448420149424]
	TIME [epoch: 17.3 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11930068890176283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11930068890176283 | validation: 0.1487206097275589]
	TIME [epoch: 17.2 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13587157367511324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13587157367511324 | validation: 0.10203935917314158]
	TIME [epoch: 17.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_531.pth
	Model improved!!!
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1292426928419766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1292426928419766 | validation: 0.36173147683433765]
	TIME [epoch: 17.2 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22805440654615544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22805440654615544 | validation: 0.22384132073622656]
	TIME [epoch: 17.3 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2574543188568558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2574543188568558 | validation: 0.22940101645024302]
	TIME [epoch: 17.2 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2194668571749685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2194668571749685 | validation: 0.2844525538142221]
	TIME [epoch: 17.3 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2700488073641633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2700488073641633 | validation: 0.1287740445379652]
	TIME [epoch: 17.2 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15215782644812023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15215782644812023 | validation: 0.4100311066987352]
	TIME [epoch: 17.2 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28829843007694794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28829843007694794 | validation: 0.18659960515093876]
	TIME [epoch: 17.2 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1994217565920425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1994217565920425 | validation: 0.26302814189235113]
	TIME [epoch: 17.3 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22136650234802852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22136650234802852 | validation: 0.1611559078473697]
	TIME [epoch: 17.2 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17070236898192798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17070236898192798 | validation: 0.21709583670561244]
	TIME [epoch: 17.2 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15646446396926283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15646446396926283 | validation: 0.3784898009032972]
	TIME [epoch: 17.2 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3621458754100016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3621458754100016 | validation: 0.2176936913927773]
	TIME [epoch: 17.3 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18904657305467296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18904657305467296 | validation: 0.2061141681570665]
	TIME [epoch: 17.3 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18399897048328648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18399897048328648 | validation: 0.14763966574577703]
	TIME [epoch: 17.2 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13454642880742426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13454642880742426 | validation: 0.11807400854928224]
	TIME [epoch: 17.3 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11956835104763482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11956835104763482 | validation: 0.20671418157412957]
	TIME [epoch: 17.3 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13417822045263184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13417822045263184 | validation: 0.20800807407888833]
	TIME [epoch: 17.2 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24078230789435842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24078230789435842 | validation: 0.29744340769305005]
	TIME [epoch: 17.3 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2546367803164028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2546367803164028 | validation: 0.16202279471266512]
	TIME [epoch: 17.2 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17060229088737422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17060229088737422 | validation: 0.18563798466497322]
	TIME [epoch: 17.3 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14334877822554734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14334877822554734 | validation: 0.15337275883721713]
	TIME [epoch: 17.3 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16502775925550367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16502775925550367 | validation: 0.6335474834854748]
	TIME [epoch: 17.3 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41784129387732033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41784129387732033 | validation: 0.11699684593346292]
	TIME [epoch: 17.2 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12401085833548116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12401085833548116 | validation: 0.12127841229098793]
	TIME [epoch: 17.3 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15047289081145584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15047289081145584 | validation: 0.18627916694072122]
	TIME [epoch: 17.2 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14879959475961987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14879959475961987 | validation: 0.1947758969751954]
	TIME [epoch: 17.2 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20916242891521614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20916242891521614 | validation: 0.12764852939162455]
	TIME [epoch: 17.2 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10549447526439462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10549447526439462 | validation: 0.14001535394851558]
	TIME [epoch: 17.3 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10035108631575089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10035108631575089 | validation: 0.14323475852742423]
	TIME [epoch: 17.2 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1832869305368678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1832869305368678 | validation: 0.5601172355901186]
	TIME [epoch: 17.3 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4388245428756449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4388245428756449 | validation: 0.13964000472956828]
	TIME [epoch: 17.2 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14449514401284622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14449514401284622 | validation: 0.11604920905393215]
	TIME [epoch: 17.3 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14475542848615883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14475542848615883 | validation: 0.22359706100677565]
	TIME [epoch: 17.3 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17262493656117855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17262493656117855 | validation: 0.1339544152747569]
	TIME [epoch: 17.3 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1320205022877077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1320205022877077 | validation: 0.1534731507908926]
	TIME [epoch: 17.2 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1348661272382986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1348661272382986 | validation: 0.17824088363710694]
	TIME [epoch: 17.3 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15334376924341903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15334376924341903 | validation: 0.1541514281353349]
	TIME [epoch: 17.2 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13104622159923374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13104622159923374 | validation: 0.11149368981815076]
	TIME [epoch: 17.3 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13140037963175424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13140037963175424 | validation: 0.39878694307614404]
	TIME [epoch: 17.3 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23955544025016412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23955544025016412 | validation: 0.21975383436862195]
	TIME [epoch: 17.3 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2310450687347394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2310450687347394 | validation: 0.15877191096000132]
	TIME [epoch: 17.2 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12433638014074454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12433638014074454 | validation: 0.12343552264644404]
	TIME [epoch: 17.2 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1234744541234722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1234744541234722 | validation: 0.1691328083603103]
	TIME [epoch: 17.2 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1651658137235565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1651658137235565 | validation: 0.16612294647229334]
	TIME [epoch: 17.3 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1572221640733664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1572221640733664 | validation: 0.15110193669391214]
	TIME [epoch: 17.1 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11528194902671253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11528194902671253 | validation: 0.17428775018828518]
	TIME [epoch: 17.2 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20825983111442692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20825983111442692 | validation: 0.5399197845986529]
	TIME [epoch: 17.1 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.364412660099302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.364412660099302 | validation: 0.1712184521701593]
	TIME [epoch: 17.3 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17339053567379686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17339053567379686 | validation: 0.12682504720778484]
	TIME [epoch: 17.2 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10762060592648438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10762060592648438 | validation: 0.10493050389975248]
	TIME [epoch: 17.3 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12212540377668718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12212540377668718 | validation: 0.1520714880400163]
	TIME [epoch: 17.2 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15330868165971684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15330868165971684 | validation: 0.2528292771764148]
	TIME [epoch: 17.3 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1871222098483274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1871222098483274 | validation: 0.09052409801143478]
	TIME [epoch: 17.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_584.pth
	Model improved!!!
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10948078109180022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10948078109180022 | validation: 0.18141856219482058]
	TIME [epoch: 17.1 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11534159097310291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11534159097310291 | validation: 0.12304855754480765]
	TIME [epoch: 17.1 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15013554689667172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15013554689667172 | validation: 0.2697349668283498]
	TIME [epoch: 17.3 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20389818611032762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20389818611032762 | validation: 0.13412231788482473]
	TIME [epoch: 17.2 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15098080321225005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15098080321225005 | validation: 0.1236229991382623]
	TIME [epoch: 17.2 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12196988871454807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12196988871454807 | validation: 0.16383947981017724]
	TIME [epoch: 17.3 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12540938469809207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12540938469809207 | validation: 0.10828990592124021]
	TIME [epoch: 17.3 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14604850312488435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14604850312488435 | validation: 0.16923137801406318]
	TIME [epoch: 17.3 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11752964287994327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11752964287994327 | validation: 0.12813821006127188]
	TIME [epoch: 17.3 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1418444560670073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1418444560670073 | validation: 0.13337126140096642]
	TIME [epoch: 17.2 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11191175075927699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11191175075927699 | validation: 0.11772831354880219]
	TIME [epoch: 17.3 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10988907829278852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10988907829278852 | validation: 0.10934294015449782]
	TIME [epoch: 17.3 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1304846873398375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1304846873398375 | validation: 0.4030715322744729]
	TIME [epoch: 17.3 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23781258884552517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23781258884552517 | validation: 0.27351733203363227]
	TIME [epoch: 17.3 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2539036699249876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2539036699249876 | validation: 0.5078388557280453]
	TIME [epoch: 17.3 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4449666217141418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4449666217141418 | validation: 0.30451446830641427]
	TIME [epoch: 17.3 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27967957988435166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27967957988435166 | validation: 0.13708681334384828]
	TIME [epoch: 17.3 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15956584870566184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15956584870566184 | validation: 0.16056002334970615]
	TIME [epoch: 17.3 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12154260495408493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12154260495408493 | validation: 0.0909494205309872]
	TIME [epoch: 17.3 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10125725229616148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10125725229616148 | validation: 0.0872182852499917]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_604.pth
	Model improved!!!
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08689401720920312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08689401720920312 | validation: 0.19852684363243572]
	TIME [epoch: 17.3 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13291717238253037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13291717238253037 | validation: 0.21803344172340688]
	TIME [epoch: 17.3 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27675774842746914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27675774842746914 | validation: 0.20600705632895086]
	TIME [epoch: 17.3 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13824869824775807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13824869824775807 | validation: 0.10370290415700936]
	TIME [epoch: 17.3 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09843423962477096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09843423962477096 | validation: 0.1412808840380855]
	TIME [epoch: 17.3 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10319851687241428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10319851687241428 | validation: 0.10744090699997876]
	TIME [epoch: 17.4 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12068580221129221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12068580221129221 | validation: 0.18679473424395346]
	TIME [epoch: 17.3 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11930307153664121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11930307153664121 | validation: 0.09253036766224387]
	TIME [epoch: 17.3 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1251017478553674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1251017478553674 | validation: 0.19071014075041293]
	TIME [epoch: 17.3 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12214254305302469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12214254305302469 | validation: 0.10325583923529057]
	TIME [epoch: 17.3 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12499322218606652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12499322218606652 | validation: 2.367581477868633]
	TIME [epoch: 17.3 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0349112051199496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0349112051199496 | validation: 2.0044258746123975]
	TIME [epoch: 17.4 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.630735344051454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.630735344051454 | validation: 0.990453078623051]
	TIME [epoch: 17.3 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8758413001490867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8758413001490867 | validation: 0.5282094076821954]
	TIME [epoch: 17.4 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38156088359084483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38156088359084483 | validation: 0.3351798649636399]
	TIME [epoch: 17.4 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2549472525258896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2549472525258896 | validation: 0.16129479461769575]
	TIME [epoch: 17.3 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17183945634270625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17183945634270625 | validation: 0.17214342450698084]
	TIME [epoch: 17.4 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13348520182947826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13348520182947826 | validation: 0.15627822891408763]
	TIME [epoch: 17.3 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09379655493655069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09379655493655069 | validation: 0.1256774732644956]
	TIME [epoch: 17.4 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09810643236402289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09810643236402289 | validation: 0.1329285019007307]
	TIME [epoch: 17.3 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10108666952986006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10108666952986006 | validation: 0.11076987680911994]
	TIME [epoch: 17.3 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10279117711276672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10279117711276672 | validation: 0.12537607381966043]
	TIME [epoch: 17.2 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0994115135775332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0994115135775332 | validation: 0.08593734779623532]
	TIME [epoch: 17.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_627.pth
	Model improved!!!
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09022868643954372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09022868643954372 | validation: 0.10839793346816201]
	TIME [epoch: 17.3 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08896507485132368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08896507485132368 | validation: 0.08452872927276331]
	TIME [epoch: 17.2 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_629.pth
	Model improved!!!
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09263668026973373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09263668026973373 | validation: 0.12193463932558016]
	TIME [epoch: 17.2 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08965335656699916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08965335656699916 | validation: 0.08303539160119955]
	TIME [epoch: 17.2 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_631.pth
	Model improved!!!
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0884512178184795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0884512178184795 | validation: 0.12077834425478798]
	TIME [epoch: 17.3 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09698839796110793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09698839796110793 | validation: 0.07214020496589456]
	TIME [epoch: 17.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_633.pth
	Model improved!!!
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08801883375150978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08801883375150978 | validation: 0.13042307219864863]
	TIME [epoch: 17.2 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09238412238857052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09238412238857052 | validation: 0.08128513860143044]
	TIME [epoch: 17.3 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10514307352824491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10514307352824491 | validation: 0.17146250958610632]
	TIME [epoch: 17.3 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1131515945221252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1131515945221252 | validation: 0.07463371092437232]
	TIME [epoch: 17.3 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10436085300172263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10436085300172263 | validation: 0.12020866774740294]
	TIME [epoch: 17.3 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09590611114089229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09590611114089229 | validation: 0.09017754354535501]
	TIME [epoch: 17.3 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09969773847762596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09969773847762596 | validation: 0.11349889725496826]
	TIME [epoch: 17.3 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1041158496375835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1041158496375835 | validation: 0.09294563880439871]
	TIME [epoch: 17.3 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10101969698706732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10101969698706732 | validation: 0.10534982121713482]
	TIME [epoch: 17.3 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0882698713992889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0882698713992889 | validation: 0.08642468787635992]
	TIME [epoch: 17.3 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10057089621645895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10057089621645895 | validation: 0.14769128602247444]
	TIME [epoch: 17.4 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10278227220915097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10278227220915097 | validation: 0.11169369447064192]
	TIME [epoch: 17.3 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13899913016978763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13899913016978763 | validation: 0.3038900275251952]
	TIME [epoch: 17.3 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1784884070174649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1784884070174649 | validation: 0.11778652449143068]
	TIME [epoch: 17.4 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13684280810158506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13684280810158506 | validation: 0.13470887992643418]
	TIME [epoch: 17.4 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13439561094728086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13439561094728086 | validation: 0.08189948528682042]
	TIME [epoch: 17.3 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09838846706145325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09838846706145325 | validation: 0.10689734349785698]
	TIME [epoch: 17.4 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07839271936505085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07839271936505085 | validation: 0.055170169813452]
	TIME [epoch: 17.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_651.pth
	Model improved!!!
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05994361469181994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05994361469181994 | validation: 0.07044393085693905]
	TIME [epoch: 17.3 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09421827968264786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09421827968264786 | validation: 0.22475208647756129]
	TIME [epoch: 17.3 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17762878354788075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17762878354788075 | validation: 0.15263139762488956]
	TIME [epoch: 17.3 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1683899922816119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1683899922816119 | validation: 0.10247193075029654]
	TIME [epoch: 17.3 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08122450465268008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08122450465268008 | validation: 0.09346384133179259]
	TIME [epoch: 17.4 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10170434613426778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10170434613426778 | validation: 0.16996337944259696]
	TIME [epoch: 17.3 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11451600858067723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11451600858067723 | validation: 0.12381436890051699]
	TIME [epoch: 17.3 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15269558291660792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15269558291660792 | validation: 0.29764631083696885]
	TIME [epoch: 17.3 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16311492926281115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16311492926281115 | validation: 0.16083787335763958]
	TIME [epoch: 17.4 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18224439723656258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18224439723656258 | validation: 0.3743583604848224]
	TIME [epoch: 17.3 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21984495978032909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21984495978032909 | validation: 0.16083366362970986]
	TIME [epoch: 17.3 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15443962338735606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15443962338735606 | validation: 0.06599380819951238]
	TIME [epoch: 17.4 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12460961706362024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12460961706362024 | validation: 0.2563317164230083]
	TIME [epoch: 17.3 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12174017169115142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12174017169115142 | validation: 0.137853755937926]
	TIME [epoch: 17.3 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13579620454332017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13579620454332017 | validation: 0.11505436132779007]
	TIME [epoch: 17.4 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0960185552518221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0960185552518221 | validation: 0.10127627083037889]
	TIME [epoch: 17.3 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09012569157527282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09012569157527282 | validation: 0.16406687026061798]
	TIME [epoch: 17.4 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15394144552140082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15394144552140082 | validation: 0.15086128302727622]
	TIME [epoch: 17.4 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13406779837135763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13406779837135763 | validation: 0.4417393667137226]
	TIME [epoch: 17.3 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35258030985156197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35258030985156197 | validation: 0.12693599865256958]
	TIME [epoch: 17.3 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1563654124647098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1563654124647098 | validation: 0.3937352235794462]
	TIME [epoch: 17.3 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28552893990893546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28552893990893546 | validation: 0.17235256629793835]
	TIME [epoch: 17.3 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18496650305743187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18496650305743187 | validation: 0.1593968465012971]
	TIME [epoch: 17.4 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1227851183253328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1227851183253328 | validation: 0.09809023840340915]
	TIME [epoch: 17.3 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10832952604723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10832952604723 | validation: 0.10966596936025463]
	TIME [epoch: 17.3 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0831956312582771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0831956312582771 | validation: 0.08226109156798453]
	TIME [epoch: 17.3 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07365083909509786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07365083909509786 | validation: 0.09490582645368663]
	TIME [epoch: 17.3 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08367716162697111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08367716162697111 | validation: 0.2541592933105899]
	TIME [epoch: 17.4 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1388611165361875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1388611165361875 | validation: 0.07645135795149613]
	TIME [epoch: 17.3 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11879463584046271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11879463584046271 | validation: 0.26610614947843914]
	TIME [epoch: 17.3 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15065590420996033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15065590420996033 | validation: 0.20363351860461268]
	TIME [epoch: 17.3 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20528219455345686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20528219455345686 | validation: 0.12092495406361325]
	TIME [epoch: 17.3 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09104298268706046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09104298268706046 | validation: 0.09353272980340013]
	TIME [epoch: 17.3 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08030117578694096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08030117578694096 | validation: 0.06653348220851557]
	TIME [epoch: 17.4 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06931329784214835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06931329784214835 | validation: 0.13184248214423278]
	TIME [epoch: 17.3 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0966326331162679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0966326331162679 | validation: 0.2980304366555512]
	TIME [epoch: 17.4 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3207226781306445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3207226781306445 | validation: 0.28356915593010407]
	TIME [epoch: 17.3 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18334211735260594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18334211735260594 | validation: 0.1181815340710155]
	TIME [epoch: 17.3 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10847567464863944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10847567464863944 | validation: 0.09284028200352666]
	TIME [epoch: 17.3 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10778715062860951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10778715062860951 | validation: 0.34971344771612106]
	TIME [epoch: 17.3 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17255743382941593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17255743382941593 | validation: 0.10802696388202593]
	TIME [epoch: 17.3 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12329319081728782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12329319081728782 | validation: 0.09990476954347588]
	TIME [epoch: 17.3 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10974113512059916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10974113512059916 | validation: 0.15602255282635366]
	TIME [epoch: 17.4 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1339666582566586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1339666582566586 | validation: 0.10790527531870145]
	TIME [epoch: 17.3 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08530483214918792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08530483214918792 | validation: 0.0752066238915915]
	TIME [epoch: 17.4 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09321696369089807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09321696369089807 | validation: 0.30646791243308175]
	TIME [epoch: 17.2 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30332559248165025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30332559248165025 | validation: 0.17383323955529328]
	TIME [epoch: 17.3 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11693971705303245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11693971705303245 | validation: 0.11009807461419077]
	TIME [epoch: 17.3 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11213213764976959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11213213764976959 | validation: 0.13201620667591119]
	TIME [epoch: 17.2 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13637603570928916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13637603570928916 | validation: 0.08644367182475311]
	TIME [epoch: 17.3 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08987369625444633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08987369625444633 | validation: 0.20878848564878247]
	TIME [epoch: 17.3 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1557530340944908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1557530340944908 | validation: 0.11025817348430343]
	TIME [epoch: 17.3 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14584780388380636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14584780388380636 | validation: 0.18338273768150765]
	TIME [epoch: 17.3 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10321921760645537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10321921760645537 | validation: 0.10201963950571585]
	TIME [epoch: 17.3 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12029225217975174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12029225217975174 | validation: 0.1551184615905719]
	TIME [epoch: 17.3 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12467143986158277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12467143986158277 | validation: 0.06723790777402204]
	TIME [epoch: 17.3 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06903697445892827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06903697445892827 | validation: 0.14289788640561799]
	TIME [epoch: 17.2 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08931278872144613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08931278872144613 | validation: 0.15280988591492592]
	TIME [epoch: 17.3 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1986445896331361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1986445896331361 | validation: 0.3095310241860638]
	TIME [epoch: 17.2 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18093890920603614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18093890920603614 | validation: 0.09184502451868237]
	TIME [epoch: 17.3 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11338631497154232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11338631497154232 | validation: 0.19084784751853248]
	TIME [epoch: 17.3 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09856964034564385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09856964034564385 | validation: 0.09527999657246002]
	TIME [epoch: 17.3 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14168504132558613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14168504132558613 | validation: 0.3537175576335332]
	TIME [epoch: 17.2 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24013230068322827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24013230068322827 | validation: 0.08642995980491554]
	TIME [epoch: 17.4 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.095718529650877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.095718529650877 | validation: 0.12372337856827405]
	TIME [epoch: 17.3 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08396885138652248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08396885138652248 | validation: 0.05777798670561945]
	TIME [epoch: 17.3 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07483254879852227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07483254879852227 | validation: 0.0732382003332544]
	TIME [epoch: 17.3 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060854947336209046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060854947336209046 | validation: 0.06013300491560619]
	TIME [epoch: 17.3 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06253614931295101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06253614931295101 | validation: 0.10989992884670885]
	TIME [epoch: 17.3 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10302640134728282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10302640134728282 | validation: 0.1791666235937647]
	TIME [epoch: 17.3 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20025498369070774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20025498369070774 | validation: 0.27436334865109574]
	TIME [epoch: 17.3 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.183001287240957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.183001287240957 | validation: 0.07504020607585117]
	TIME [epoch: 17.4 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0846655214437549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0846655214437549 | validation: 0.19038752755851351]
	TIME [epoch: 17.3 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13306869841349525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13306869841349525 | validation: 0.26692214513742324]
	TIME [epoch: 17.3 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2764291591830271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2764291591830271 | validation: 0.30305947266158795]
	TIME [epoch: 17.3 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19852454344210274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19852454344210274 | validation: 0.07925889126811107]
	TIME [epoch: 17.3 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1008769714542749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1008769714542749 | validation: 0.06689899124139406]
	TIME [epoch: 17.3 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08180549473375333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08180549473375333 | validation: 0.4754577505127234]
	TIME [epoch: 17.3 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27195797162903845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27195797162903845 | validation: 0.15551625680530057]
	TIME [epoch: 17.3 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19688740988210449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19688740988210449 | validation: 0.1896295381184]
	TIME [epoch: 17.3 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12254701379054148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12254701379054148 | validation: 0.09128936706639819]
	TIME [epoch: 17.3 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0795027402167033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0795027402167033 | validation: 0.0758521061921359]
	TIME [epoch: 17.3 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08684446877162079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08684446877162079 | validation: 0.1603763885341769]
	TIME [epoch: 17.3 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12533031595416572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12533031595416572 | validation: 0.1753984173159428]
	TIME [epoch: 17.2 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17764807916079245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17764807916079245 | validation: 0.12740105452784803]
	TIME [epoch: 17.3 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10305541868812856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10305541868812856 | validation: 0.07282815189996303]
	TIME [epoch: 17.2 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07320991961242117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07320991961242117 | validation: 0.16051236811549574]
	TIME [epoch: 17.3 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11031191377412519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11031191377412519 | validation: 0.06999645518014032]
	TIME [epoch: 17.2 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07991834057176395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07991834057176395 | validation: 0.09912028249571941]
	TIME [epoch: 17.3 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06870765039311351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06870765039311351 | validation: 0.0726419606550282]
	TIME [epoch: 17.2 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09038262357704038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09038262357704038 | validation: 0.28095473506520297]
	TIME [epoch: 17.3 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16585337055290636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16585337055290636 | validation: 0.1650718593604526]
	TIME [epoch: 17.3 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20100449520613636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20100449520613636 | validation: 0.20135105186143498]
	TIME [epoch: 17.3 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1421746907724133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1421746907724133 | validation: 0.053391485415694155]
	TIME [epoch: 17.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_745.pth
	Model improved!!!
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06860200945653833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06860200945653833 | validation: 0.0564612000192109]
	TIME [epoch: 17.3 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05988758032951846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05988758032951846 | validation: 0.07686144726582578]
	TIME [epoch: 17.3 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058736141216607723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058736141216607723 | validation: 0.08304028132647955]
	TIME [epoch: 17.3 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0781946619360544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0781946619360544 | validation: 0.10070805524593185]
	TIME [epoch: 17.3 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1170483280392894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1170483280392894 | validation: 0.28461381715802636]
	TIME [epoch: 17.3 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1662840024149255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1662840024149255 | validation: 0.1667101284784853]
	TIME [epoch: 17.3 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16878592232451628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16878592232451628 | validation: 0.25814860995172356]
	TIME [epoch: 17.3 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24123438334125893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24123438334125893 | validation: 0.13601912484951356]
	TIME [epoch: 17.3 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09341817309347782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09341817309347782 | validation: 0.16921067719710142]
	TIME [epoch: 17.3 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.147836688632736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.147836688632736 | validation: 0.10026266549600558]
	TIME [epoch: 17.3 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11067437113026245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11067437113026245 | validation: 0.19180324130113716]
	TIME [epoch: 17.3 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10969412800667128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10969412800667128 | validation: 0.10870855777749432]
	TIME [epoch: 17.3 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1581530055605538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1581530055605538 | validation: 0.32569022382865875]
	TIME [epoch: 17.3 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1680814500439101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1680814500439101 | validation: 0.15390006082999252]
	TIME [epoch: 17.3 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1879117787035263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1879117787035263 | validation: 0.12281980717921051]
	TIME [epoch: 17.3 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12096563533757484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12096563533757484 | validation: 0.0905665601119992]
	TIME [epoch: 17.3 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0874817961611615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0874817961611615 | validation: 0.0902052723025457]
	TIME [epoch: 17.3 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08963445044164757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08963445044164757 | validation: 0.10507200651503723]
	TIME [epoch: 17.3 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09505061463386637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09505061463386637 | validation: 0.10254076488654942]
	TIME [epoch: 17.4 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0961994174085341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0961994174085341 | validation: 0.07545087371598551]
	TIME [epoch: 17.3 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08325042970485018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08325042970485018 | validation: 0.11918670178778241]
	TIME [epoch: 17.4 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08429707208658314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08429707208658314 | validation: 0.06672270959032468]
	TIME [epoch: 17.2 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09539582189278983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09539582189278983 | validation: 0.2299353174711385]
	TIME [epoch: 17.3 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1179505770776303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1179505770776303 | validation: 0.07088965256795686]
	TIME [epoch: 17.2 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10234532370922171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10234532370922171 | validation: 0.18207990976699562]
	TIME [epoch: 17.3 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10325381924281972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10325381924281972 | validation: 0.05776994837739607]
	TIME [epoch: 17.2 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0855536989604261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0855536989604261 | validation: 0.10772209795129234]
	TIME [epoch: 17.3 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07435966695333868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07435966695333868 | validation: 0.08630732534861242]
	TIME [epoch: 17.2 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08434620705437908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08434620705437908 | validation: 0.11143941656394381]
	TIME [epoch: 17.2 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10564501311272118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10564501311272118 | validation: 0.11676870967711128]
	TIME [epoch: 17.3 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10961813214029807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10961813214029807 | validation: 0.10178182932674576]
	TIME [epoch: 17.2 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09139145322342193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09139145322342193 | validation: 0.1501359639202904]
	TIME [epoch: 17.3 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11920482138187839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11920482138187839 | validation: 0.10461647085265272]
	TIME [epoch: 17.3 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10895847837408454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10895847837408454 | validation: 0.260964621039053]
	TIME [epoch: 17.3 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1823507398989396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1823507398989396 | validation: 0.07851800860092711]
	TIME [epoch: 17.3 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16300736590769874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16300736590769874 | validation: 0.23997212689254238]
	TIME [epoch: 17.3 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12986109943649138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12986109943649138 | validation: 0.11803597204368568]
	TIME [epoch: 17.2 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12583368827986238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12583368827986238 | validation: 0.18421245939124123]
	TIME [epoch: 17.3 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1478682896406956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1478682896406956 | validation: 0.06823893354872626]
	TIME [epoch: 17.3 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0761969930521167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0761969930521167 | validation: 0.06932182065602999]
	TIME [epoch: 17.3 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06588164454170511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06588164454170511 | validation: 0.4914587985032993]
	TIME [epoch: 17.3 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5668533943546187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5668533943546187 | validation: 0.32154416725781887]
	TIME [epoch: 17.3 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.253982023024436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.253982023024436 | validation: 0.4043304026005567]
	TIME [epoch: 17.3 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27022131506208236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27022131506208236 | validation: 0.10650412789645294]
	TIME [epoch: 17.3 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11440876190525286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11440876190525286 | validation: 0.09340073212512615]
	TIME [epoch: 17.3 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10069485962234019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10069485962234019 | validation: 0.09675742243542972]
	TIME [epoch: 17.3 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07303255007389453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07303255007389453 | validation: 0.060137125302272866]
	TIME [epoch: 17.4 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05871525045676304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05871525045676304 | validation: 0.09205114132354343]
	TIME [epoch: 17.3 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07063323432494906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07063323432494906 | validation: 0.08496684821703182]
	TIME [epoch: 17.3 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09445032726188561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09445032726188561 | validation: 0.1764213225450714]
	TIME [epoch: 17.3 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10867799702170153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10867799702170153 | validation: 0.21910773119836124]
	TIME [epoch: 17.3 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2307479009651278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2307479009651278 | validation: 0.1405718114563628]
	TIME [epoch: 17.3 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11399619159213958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11399619159213958 | validation: 0.11888955224256113]
	TIME [epoch: 17.3 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12002427362239436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12002427362239436 | validation: 0.08773115680094577]
	TIME [epoch: 17.3 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08288482451385869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08288482451385869 | validation: 1.5177406733539158]
	TIME [epoch: 17.3 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4043340704507077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4043340704507077 | validation: 1.377522945989467]
	TIME [epoch: 17.3 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.379611876915867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.379611876915867 | validation: 0.8821250018791121]
	TIME [epoch: 17.3 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7315881997324233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7315881997324233 | validation: 0.5215539187590776]
	TIME [epoch: 17.3 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5256815406709849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5256815406709849 | validation: 0.3527996546435566]
	TIME [epoch: 17.3 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4242430820176956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4242430820176956 | validation: 0.33048235529545966]
	TIME [epoch: 17.3 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3364040459861156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3364040459861156 | validation: 0.219442537352029]
	TIME [epoch: 17.3 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22130443998056692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22130443998056692 | validation: 0.11301541505199318]
	TIME [epoch: 17.3 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15778213906088598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15778213906088598 | validation: 0.1459964336549587]
	TIME [epoch: 17.3 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19490415035197423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19490415035197423 | validation: 0.13995285617151934]
	TIME [epoch: 17.3 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1889753419248455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1889753419248455 | validation: 0.21249519729934768]
	TIME [epoch: 17.3 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15440518190423885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15440518190423885 | validation: 0.10615997479371542]
	TIME [epoch: 17.3 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09312928302536914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09312928302536914 | validation: 0.08519579348344757]
	TIME [epoch: 17.3 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08776653319105127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08776653319105127 | validation: 0.08258238970752152]
	TIME [epoch: 17.3 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08390318376145789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08390318376145789 | validation: 0.08206605505113526]
	TIME [epoch: 17.2 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08196305568694882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08196305568694882 | validation: 0.08440572886981963]
	TIME [epoch: 17.3 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0850443098118999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0850443098118999 | validation: 0.15236676666076213]
	TIME [epoch: 17.3 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10612542018367402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10612542018367402 | validation: 0.08233215962221185]
	TIME [epoch: 17.3 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08802855204311193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08802855204311193 | validation: 0.2783438260602182]
	TIME [epoch: 17.2 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26455083703817567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26455083703817567 | validation: 0.09685539377868049]
	TIME [epoch: 17.3 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09203676112713509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09203676112713509 | validation: 0.07405425078199356]
	TIME [epoch: 17.3 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06086711911457785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06086711911457785 | validation: 0.07789513463249421]
	TIME [epoch: 17.3 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07360494893339059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07360494893339059 | validation: 0.15940512232552498]
	TIME [epoch: 17.3 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10070112933507815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10070112933507815 | validation: 0.10162308844590023]
	TIME [epoch: 17.3 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11989526081181046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11989526081181046 | validation: 0.14019756084200607]
	TIME [epoch: 17.3 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10260184837939935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10260184837939935 | validation: 0.0694506533262693]
	TIME [epoch: 17.3 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07766523110532234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07766523110532234 | validation: 0.08833805426672825]
	TIME [epoch: 17.3 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07016464813757702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07016464813757702 | validation: 0.05811270563641935]
	TIME [epoch: 17.3 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0742506012342038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0742506012342038 | validation: 0.1336485369218089]
	TIME [epoch: 17.3 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09521308230292695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09521308230292695 | validation: 0.08979697193122553]
	TIME [epoch: 17.2 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10241541224296988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10241541224296988 | validation: 0.10238092373128749]
	TIME [epoch: 17.3 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08905637792111822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08905637792111822 | validation: 0.06440063054333009]
	TIME [epoch: 17.3 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06832377310571737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06832377310571737 | validation: 0.08414132160349896]
	TIME [epoch: 17.3 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07681817285132718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07681817285132718 | validation: 0.0793538466989688]
	TIME [epoch: 17.3 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08491269464988495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08491269464988495 | validation: 0.07538104478322812]
	TIME [epoch: 17.3 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07229278310714897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07229278310714897 | validation: 0.10572985532565343]
	TIME [epoch: 17.2 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10907241535558708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10907241535558708 | validation: 0.31834300750211386]
	TIME [epoch: 17.3 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19033849123857016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19033849123857016 | validation: 0.09687177413660725]
	TIME [epoch: 17.3 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12096868634054245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12096868634054245 | validation: 0.17542244570136087]
	TIME [epoch: 17.3 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0965644959829609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0965644959829609 | validation: 0.08476157668400203]
	TIME [epoch: 17.4 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08339386321603846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08339386321603846 | validation: 0.07331137106785617]
	TIME [epoch: 17.3 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08591450844375917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08591450844375917 | validation: 0.19669906921721442]
	TIME [epoch: 17.3 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13638389580638127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13638389580638127 | validation: 0.13332739959904488]
	TIME [epoch: 17.3 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13979338985376197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13979338985376197 | validation: 0.3065701265896383]
	TIME [epoch: 17.4 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28079044229446826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28079044229446826 | validation: 0.1681714176247081]
	TIME [epoch: 17.4 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14746173196934695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14746173196934695 | validation: 0.6012008599476762]
	TIME [epoch: 17.3 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.664757830590786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.664757830590786 | validation: 0.6382308635957084]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_132805/states/model_phi1_4c_v_mmd1_846.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 9368.919 seconds.
