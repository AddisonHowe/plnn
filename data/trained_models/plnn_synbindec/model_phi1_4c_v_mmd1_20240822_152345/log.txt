Args:
Namespace(name='model_phi1_4c_v_mmd1', outdir='out/model_training/model_phi1_4c_v_mmd1', training_data='data/training_data/data_phi1_4c/training', validation_data='data/training_data/data_phi1_4c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3360546405

Training model...

Saving initial model state to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.623000081138689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.623000081138689 | validation: 4.9314761953386235]
	TIME [epoch: 47.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.072381783709314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.072381783709314 | validation: 5.009296173829485]
	TIME [epoch: 3.79 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.972941154725768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.972941154725768 | validation: 4.9106828874326185]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.789621235249903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.789621235249903 | validation: 4.590588354567538]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.62763250951401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.62763250951401 | validation: 4.421806195401736]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.4401743590228175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4401743590228175 | validation: 4.349296181915114]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.253476856285359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.253476856285359 | validation: 4.284779378091878]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.188878632595761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.188878632595761 | validation: 4.168716167229973]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.1237367187743725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1237367187743725 | validation: 4.1145365817429225]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.03668834635154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.03668834635154 | validation: 4.079502266082559]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9814188446639305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9814188446639305 | validation: 3.9924689517609195]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9233261255907697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9233261255907697 | validation: 3.9526393184922406]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8713783358169405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8713783358169405 | validation: 3.8862362993665003]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8274422260678556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8274422260678556 | validation: 3.860136001740476]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7989432186987844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7989432186987844 | validation: 3.8908066937198265]
	TIME [epoch: 3.79 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8722841086189055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8722841086189055 | validation: 3.950489069291225]
	TIME [epoch: 3.77 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8865121601325088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8865121601325088 | validation: 3.8236454894078875]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.812701429273392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.812701429273392 | validation: 3.7225856990940933]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.696921313883473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.696921313883473 | validation: 3.7569386601150363]
	TIME [epoch: 3.77 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6991113984005195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6991113984005195 | validation: 3.657686022540853]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.640450164325763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.640450164325763 | validation: 3.669573100822956]
	TIME [epoch: 3.78 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6299276399385594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6299276399385594 | validation: 3.6262657075472067]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.614233139140655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.614233139140655 | validation: 3.624430079284764]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5796637404214846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5796637404214846 | validation: 3.5632763918697288]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5539384724028857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5539384724028857 | validation: 3.560404444806471]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.52973980561768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.52973980561768 | validation: 3.526901742741896]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.50499193865422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.50499193865422 | validation: 3.5159964965132007]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.484659497652148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.484659497652148 | validation: 3.4844782431089913]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.468805659430872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.468805659430872 | validation: 3.4809877973215104]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.447318677147128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.447318677147128 | validation: 3.447893226240777]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4319869711437163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4319869711437163 | validation: 3.4457785295127032]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4142773991543915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4142773991543915 | validation: 3.4339048922309985]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4080425592197092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4080425592197092 | validation: 3.4026569497027186]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3757846098231723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3757846098231723 | validation: 3.4182926921653696]
	TIME [epoch: 3.78 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3553075217045256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3553075217045256 | validation: 3.3381545385100133]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3126359568063934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3126359568063934 | validation: 3.347763924273632]
	TIME [epoch: 3.78 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2862186684596866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2862186684596866 | validation: 3.2909057804995925]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2546817093724423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2546817093724423 | validation: 3.2846874270372384]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2247945273134855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2247945273134855 | validation: 3.094450721715313]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.160579045401695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.160579045401695 | validation: 3.1331060803415522]
	TIME [epoch: 3.77 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.111601469520763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.111601469520763 | validation: 2.7697569949470116]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9189951161821135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9189951161821135 | validation: 3.24098702326782]
	TIME [epoch: 3.77 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1253206669756843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1253206669756843 | validation: 2.5281712700648633]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5609016910393416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5609016910393416 | validation: 2.206241521269886]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5999540222787965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5999540222787965 | validation: 1.9479868606251676]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1663469588053905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1663469588053905 | validation: 1.8586311442442498]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.075347189890235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.075347189890235 | validation: 1.617148251646369]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8716301077973077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8716301077973077 | validation: 1.388874349325139]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.661773470523697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.661773470523697 | validation: 1.0908336066330222]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3623854954310406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3623854954310406 | validation: 0.9752134398768739]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1826021220768477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1826021220768477 | validation: 1.6990564868023164]
	TIME [epoch: 3.78 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.239650348391499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.239650348391499 | validation: 1.3331378591493301]
	TIME [epoch: 3.78 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4830800743843457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4830800743843457 | validation: 1.3421004532348053]
	TIME [epoch: 3.77 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4489774526550019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4489774526550019 | validation: 0.9362304781575163]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1525475871421529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1525475871421529 | validation: 0.9325193769009064]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1892892639778205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1892892639778205 | validation: 0.8287048176510781]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0808858106220622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0808858106220622 | validation: 0.8674313579842384]
	TIME [epoch: 3.8 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0508252788439876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0508252788439876 | validation: 0.7986450072439993]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9779068427981383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9779068427981383 | validation: 0.800060709519693]
	TIME [epoch: 3.78 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9930703354789217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9930703354789217 | validation: 0.7982358535646453]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9903734408025378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9903734408025378 | validation: 0.7647232695600842]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9639999616191317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9639999616191317 | validation: 0.7688018927949597]
	TIME [epoch: 3.77 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9645789502052838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9645789502052838 | validation: 0.8258094359412818]
	TIME [epoch: 3.77 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0437831807749243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0437831807749243 | validation: 0.7567503680274982]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9447384951016342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9447384951016342 | validation: 0.7498019615881835]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9264389991803391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9264389991803391 | validation: 0.7495536153839444]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9245166109362193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9245166109362193 | validation: 0.7381780676534797]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9189293794114098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9189293794114098 | validation: 0.7340173561902102]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9139202309019462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9139202309019462 | validation: 0.7682512577046584]
	TIME [epoch: 3.78 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9362360816200795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9362360816200795 | validation: 0.9509636109036785]
	TIME [epoch: 3.78 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2525353521985307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2525353521985307 | validation: 0.8069041444360571]
	TIME [epoch: 3.77 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0223674673192924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0223674673192924 | validation: 0.9969968490939017]
	TIME [epoch: 3.77 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2053305738936173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2053305738936173 | validation: 0.8388998570985731]
	TIME [epoch: 3.77 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.983673491879687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.983673491879687 | validation: 0.8936012182102413]
	TIME [epoch: 3.77 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1149698752208466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1149698752208466 | validation: 0.7550471265616421]
	TIME [epoch: 3.77 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9412143415244169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9412143415244169 | validation: 0.968608627287331]
	TIME [epoch: 3.77 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1531186856733593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1531186856733593 | validation: 0.7904987882689807]
	TIME [epoch: 3.78 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9331463297969069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9331463297969069 | validation: 0.7887377180262269]
	TIME [epoch: 3.78 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9539591434667699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9539591434667699 | validation: 0.8426250262960284]
	TIME [epoch: 3.78 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.001627722462935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.001627722462935 | validation: 0.7647879002268618]
	TIME [epoch: 3.79 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9246721019106967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9246721019106967 | validation: 0.7646999756875088]
	TIME [epoch: 3.78 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9258877035121844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9258877035121844 | validation: 0.8201814897963303]
	TIME [epoch: 3.77 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0001319190874929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0001319190874929 | validation: 0.7639320892583749]
	TIME [epoch: 3.78 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9231472918346546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9231472918346546 | validation: 0.7825330752071518]
	TIME [epoch: 3.78 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.919565855090591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.919565855090591 | validation: 0.7291356049576982]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8896947302159461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8896947302159461 | validation: 0.7503754888164358]
	TIME [epoch: 3.77 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9059375793853562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9059375793853562 | validation: 0.7628287996934138]
	TIME [epoch: 3.77 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8966070947936174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8966070947936174 | validation: 0.7564564849737542]
	TIME [epoch: 3.77 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9220334501530918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9220334501530918 | validation: 0.8619880750934338]
	TIME [epoch: 3.77 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0466454572665669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0466454572665669 | validation: 0.7558611555163881]
	TIME [epoch: 3.77 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8928538449791712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8928538449791712 | validation: 0.7460172330379924]
	TIME [epoch: 3.79 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.911222967839639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.911222967839639 | validation: 0.9227261696524789]
	TIME [epoch: 3.78 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0733818984199441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0733818984199441 | validation: 0.765983366427549]
	TIME [epoch: 3.77 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9245332455751302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9245332455751302 | validation: 0.8146619297876531]
	TIME [epoch: 3.77 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9457668852169367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9457668852169367 | validation: 0.8580396441883166]
	TIME [epoch: 3.77 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9997195477310561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9997195477310561 | validation: 0.7471845108785788]
	TIME [epoch: 3.77 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9061291191529247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9061291191529247 | validation: 0.7919385208207931]
	TIME [epoch: 3.77 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9388925123811638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9388925123811638 | validation: 0.7902400806035019]
	TIME [epoch: 3.76 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9495497644522409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9495497644522409 | validation: 0.7561518682642748]
	TIME [epoch: 3.77 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9001089632746263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9001089632746263 | validation: 0.7516177721803108]
	TIME [epoch: 3.76 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8878651767966372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8878651767966372 | validation: 0.7519204403976499]
	TIME [epoch: 3.77 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8951499940282537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8951499940282537 | validation: 0.8146715715374164]
	TIME [epoch: 3.76 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9950144953204489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9950144953204489 | validation: 0.7626392782421298]
	TIME [epoch: 3.78 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9264413253327621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9264413253327621 | validation: 0.753907756498951]
	TIME [epoch: 3.77 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8902821016544996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8902821016544996 | validation: 0.7755998662798975]
	TIME [epoch: 3.77 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8989781037923561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8989781037923561 | validation: 0.7942946388698614]
	TIME [epoch: 3.77 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9567331423271498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9567331423271498 | validation: 0.9496996154116296]
	TIME [epoch: 3.77 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1807947830962608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1807947830962608 | validation: 0.8406529146099255]
	TIME [epoch: 3.76 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0099515436405073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0099515436405073 | validation: 0.8805035004106551]
	TIME [epoch: 3.77 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0356798480471374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0356798480471374 | validation: 0.7849317690853578]
	TIME [epoch: 3.77 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9115778332956878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9115778332956878 | validation: 0.7477214025622742]
	TIME [epoch: 3.77 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9044779328933572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9044779328933572 | validation: 0.7471515027496701]
	TIME [epoch: 3.77 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8942863535142059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8942863535142059 | validation: 0.7397525714129736]
	TIME [epoch: 3.77 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8964932438663488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8964932438663488 | validation: 0.7331016771546301]
	TIME [epoch: 3.77 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.882586147978127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.882586147978127 | validation: 0.8974856265947784]
	TIME [epoch: 3.79 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0249904791485387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0249904791485387 | validation: 0.7398262409536365]
	TIME [epoch: 3.77 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8984957856602809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8984957856602809 | validation: 0.7853693068430072]
	TIME [epoch: 3.77 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9270425474957327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9270425474957327 | validation: 0.7841634797583836]
	TIME [epoch: 3.77 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9166432198707412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9166432198707412 | validation: 0.7465542170932559]
	TIME [epoch: 3.77 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8770423319505176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8770423319505176 | validation: 0.7506951103641593]
	TIME [epoch: 3.76 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8869570852064059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8869570852064059 | validation: 0.7836357825039243]
	TIME [epoch: 3.77 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9177607000363303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9177607000363303 | validation: 0.7590765037031579]
	TIME [epoch: 3.77 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8775879856521585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8775879856521585 | validation: 0.7517977218243077]
	TIME [epoch: 3.78 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9034388300987695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9034388300987695 | validation: 0.8398346237251233]
	TIME [epoch: 3.77 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9794836098574246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9794836098574246 | validation: 0.7932905940652563]
	TIME [epoch: 3.77 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9526772167099538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9526772167099538 | validation: 0.7670178118479691]
	TIME [epoch: 3.78 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.868959491983577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.868959491983577 | validation: 0.7321159563321848]
	TIME [epoch: 3.79 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8639993147272951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8639993147272951 | validation: 0.7364065519096482]
	TIME [epoch: 3.78 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8599517775001662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8599517775001662 | validation: 0.7316269266162747]
	TIME [epoch: 3.78 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8524511123212082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8524511123212082 | validation: 0.7351375498492897]
	TIME [epoch: 3.77 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8706306911923355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8706306911923355 | validation: 0.7875443788761407]
	TIME [epoch: 3.78 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8860448505739243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8860448505739243 | validation: 0.8727967535789246]
	TIME [epoch: 3.77 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.046483514473204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.046483514473204 | validation: 0.7414473414279419]
	TIME [epoch: 3.78 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8754192455524884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8754192455524884 | validation: 0.778112366327159]
	TIME [epoch: 3.77 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9025930381105606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9025930381105606 | validation: 0.8264598157178219]
	TIME [epoch: 3.77 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9479985438407152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9479985438407152 | validation: 0.7611592566247656]
	TIME [epoch: 3.77 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9261538291434078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9261538291434078 | validation: 0.8019168280291364]
	TIME [epoch: 3.77 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8956156506740834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8956156506740834 | validation: 0.7806024018427893]
	TIME [epoch: 3.77 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9002446549177128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9002446549177128 | validation: 0.7138067298647358]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_139.pth
	Model improved!!!
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8388925759048488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8388925759048488 | validation: 0.7490944697368402]
	TIME [epoch: 3.79 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8608178634346569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8608178634346569 | validation: 0.7779901898296177]
	TIME [epoch: 3.78 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8971822284922938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8971822284922938 | validation: 0.7774478185446898]
	TIME [epoch: 3.79 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9412722903011133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9412722903011133 | validation: 0.828743064848476]
	TIME [epoch: 3.78 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9465072237251225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9465072237251225 | validation: 0.7314268786981349]
	TIME [epoch: 3.78 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9139681615146433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9139681615146433 | validation: 0.7300509363194846]
	TIME [epoch: 3.78 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8200524343364779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8200524343364779 | validation: 0.6669969595312604]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7904764005419181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7904764005419181 | validation: 0.7337716258787791]
	TIME [epoch: 3.77 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7559890354966887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7559890354966887 | validation: 0.9747645805121762]
	TIME [epoch: 3.77 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0466170605441139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0466170605441139 | validation: 0.8787491251084871]
	TIME [epoch: 3.78 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8533671078345859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8533671078345859 | validation: 0.7005913732635718]
	TIME [epoch: 3.77 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7537998489890196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7537998489890196 | validation: 0.6117804040233651]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_151.pth
	Model improved!!!
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.670484579983424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.670484579983424 | validation: 0.7305158894628491]
	TIME [epoch: 3.78 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7201701411079319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7201701411079319 | validation: 1.093319393607019]
	TIME [epoch: 3.77 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2547170904119602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2547170904119602 | validation: 0.8851014233484253]
	TIME [epoch: 3.78 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9651416496272575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9651416496272575 | validation: 0.8728841377830112]
	TIME [epoch: 3.77 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0215007250429453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0215007250429453 | validation: 0.7480702596972222]
	TIME [epoch: 3.78 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.839876496115342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.839876496115342 | validation: 0.7043232052867243]
	TIME [epoch: 3.78 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8337586260780959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8337586260780959 | validation: 0.6549766611186748]
	TIME [epoch: 3.78 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.765599164631277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.765599164631277 | validation: 0.6081837485294973]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_159.pth
	Model improved!!!
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7117467404749818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7117467404749818 | validation: 0.7138021960021252]
	TIME [epoch: 3.78 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6874573210058074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6874573210058074 | validation: 0.8213587182147762]
	TIME [epoch: 3.77 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8979478348367029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8979478348367029 | validation: 0.7697529771001959]
	TIME [epoch: 3.77 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9308840999837882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9308840999837882 | validation: 0.6357030562089169]
	TIME [epoch: 3.78 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.646324905216307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.646324905216307 | validation: 0.8193390265711282]
	TIME [epoch: 3.78 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7378851066391586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7378851066391586 | validation: 0.8332542286699006]
	TIME [epoch: 3.77 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9030842316971806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9030842316971806 | validation: 0.7769843502945467]
	TIME [epoch: 3.78 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8592513506521533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8592513506521533 | validation: 0.7458797559925037]
	TIME [epoch: 3.77 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8360683491849571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8360683491849571 | validation: 0.6678538705190883]
	TIME [epoch: 3.78 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7533575032295587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7533575032295587 | validation: 0.7772506906812316]
	TIME [epoch: 3.77 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8085376210425831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8085376210425831 | validation: 0.7980472262846616]
	TIME [epoch: 3.77 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8899907545717411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8899907545717411 | validation: 0.6305705357529758]
	TIME [epoch: 3.77 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6812988052720439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6812988052720439 | validation: 0.5768832533525086]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6825358479375027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6825358479375027 | validation: 0.5731073568673039]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5843945236769903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5843945236769903 | validation: 0.6104957906598837]
	TIME [epoch: 3.76 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5918758497185809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5918758497185809 | validation: 0.5077368124332732]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_175.pth
	Model improved!!!
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.548693212099714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.548693212099714 | validation: 0.6078068030570799]
	TIME [epoch: 3.76 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5976567606914704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5976567606914704 | validation: 0.6329454005922968]
	TIME [epoch: 3.75 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7066142481415787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7066142481415787 | validation: 0.8711894063636759]
	TIME [epoch: 3.77 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0719152587872747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0719152587872747 | validation: 0.8845772794652538]
	TIME [epoch: 3.76 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0575138114770437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0575138114770437 | validation: 0.7394539305313833]
	TIME [epoch: 4.02 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.856907733180363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.856907733180363 | validation: 0.6616187673300644]
	TIME [epoch: 3.78 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7895486181065354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7895486181065354 | validation: 0.5364575857859016]
	TIME [epoch: 3.78 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6047458907116443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6047458907116443 | validation: 0.7095192591132421]
	TIME [epoch: 3.77 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6653865033064049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6653865033064049 | validation: 0.6199800503690778]
	TIME [epoch: 3.77 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6571141710944989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6571141710944989 | validation: 0.5509306595789938]
	TIME [epoch: 3.77 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6303952810801355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6303952810801355 | validation: 0.5023793509564868]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_186.pth
	Model improved!!!
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5508137881546123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5508137881546123 | validation: 0.6896727192921986]
	TIME [epoch: 3.8 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7076836485989231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7076836485989231 | validation: 1.1201121243084786]
	TIME [epoch: 3.78 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2841398458873254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2841398458873254 | validation: 0.7003400242401248]
	TIME [epoch: 3.78 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7697939356906649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7697939356906649 | validation: 0.7462570217103699]
	TIME [epoch: 3.77 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.844382675148905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.844382675148905 | validation: 0.6707763946049689]
	TIME [epoch: 3.77 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7669639586061697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7669639586061697 | validation: 0.5401843382917781]
	TIME [epoch: 3.78 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6422200456606431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6422200456606431 | validation: 0.4891634108609013]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5137946765369802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5137946765369802 | validation: 0.557948474704614]
	TIME [epoch: 3.78 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5373759487014655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5373759487014655 | validation: 0.5063328195124602]
	TIME [epoch: 3.77 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5168717033467638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5168717033467638 | validation: 0.4728572799301028]
	TIME [epoch: 3.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.53710715322394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.53710715322394 | validation: 0.6342512038583453]
	TIME [epoch: 3.78 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.661532859055678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.661532859055678 | validation: 0.5255530599331849]
	TIME [epoch: 3.79 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5815978692962742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5815978692962742 | validation: 0.5151649107448502]
	TIME [epoch: 3.78 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5701764131163025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5701764131163025 | validation: 0.6456301354517209]
	TIME [epoch: 3.78 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6104552097094035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6104552097094035 | validation: 0.44317255213693985]
	TIME [epoch: 50.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5341120761227611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5341120761227611 | validation: 0.4370377602932791]
	TIME [epoch: 8.24 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5347195303171037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5347195303171037 | validation: 0.41319274078209345]
	TIME [epoch: 8.23 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4359512770773849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4359512770773849 | validation: 0.4595238626724578]
	TIME [epoch: 8.18 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4435024545012894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4435024545012894 | validation: 0.49715626209775066]
	TIME [epoch: 8.17 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5165894301809186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5165894301809186 | validation: 0.6895732435324063]
	TIME [epoch: 8.18 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8467906810363612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8467906810363612 | validation: 0.5904057130764221]
	TIME [epoch: 8.19 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8680678602373452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8680678602373452 | validation: 0.43209714416746037]
	TIME [epoch: 8.18 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5056387739610886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5056387739610886 | validation: 0.46766100504994035]
	TIME [epoch: 8.18 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5090665351723348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5090665351723348 | validation: 0.4321987758648499]
	TIME [epoch: 8.18 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4719360890419635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4719360890419635 | validation: 0.4040422215196934]
	TIME [epoch: 8.18 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42153054614216445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42153054614216445 | validation: 0.39176550888023715]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43505392637621626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43505392637621626 | validation: 0.442602133583746]
	TIME [epoch: 8.22 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4373146361284189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4373146361284189 | validation: 0.3849491602444316]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_214.pth
	Model improved!!!
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45309889641841944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45309889641841944 | validation: 0.45229555204129107]
	TIME [epoch: 8.21 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42512272234860576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42512272234860576 | validation: 0.33807946013002227]
	TIME [epoch: 8.19 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_216.pth
	Model improved!!!
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40248906567365295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40248906567365295 | validation: 0.39516295853744143]
	TIME [epoch: 8.19 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39171150858504084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39171150858504084 | validation: 0.4519166057315229]
	TIME [epoch: 8.2 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5729281834847073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5729281834847073 | validation: 0.5457132673494328]
	TIME [epoch: 8.2 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5678473730187001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5678473730187001 | validation: 0.45615888702893503]
	TIME [epoch: 8.19 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4982803433526557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4982803433526557 | validation: 0.4087303201739877]
	TIME [epoch: 8.19 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4945019904448799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4945019904448799 | validation: 0.5305633094973624]
	TIME [epoch: 8.19 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4820018090961248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4820018090961248 | validation: 0.327330666854051]
	TIME [epoch: 8.18 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3851329772986567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3851329772986567 | validation: 0.3323839121601874]
	TIME [epoch: 8.19 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35891470289246696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35891470289246696 | validation: 0.36645049259280343]
	TIME [epoch: 8.21 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35096158782739806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35096158782739806 | validation: 0.36160345418723316]
	TIME [epoch: 8.2 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42245128878563365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42245128878563365 | validation: 0.5059990668370203]
	TIME [epoch: 8.2 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6216180501432225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6216180501432225 | validation: 0.2822390413678961]
	TIME [epoch: 8.19 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_228.pth
	Model improved!!!
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.380674406564789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.380674406564789 | validation: 0.5234270428717188]
	TIME [epoch: 8.19 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4528257166367044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4528257166367044 | validation: 0.47398422868146195]
	TIME [epoch: 8.2 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5487882438453491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5487882438453491 | validation: 0.6342971031489381]
	TIME [epoch: 8.18 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.823452650025701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.823452650025701 | validation: 0.4184022243158387]
	TIME [epoch: 8.19 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43752334542813454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43752334542813454 | validation: 0.2948295059914147]
	TIME [epoch: 8.21 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36711184707991995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36711184707991995 | validation: 0.48308047340872506]
	TIME [epoch: 8.17 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4037992668625614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4037992668625614 | validation: 0.25712132857485204]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_235.pth
	Model improved!!!
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.318350095423295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.318350095423295 | validation: 0.31945770594230766]
	TIME [epoch: 8.21 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3046082382607308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3046082382607308 | validation: 0.30676928336356246]
	TIME [epoch: 8.19 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3784341431286747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3784341431286747 | validation: 0.5790170504077686]
	TIME [epoch: 8.18 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5093452662061546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5093452662061546 | validation: 0.2825486010830977]
	TIME [epoch: 8.19 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37204136389882564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37204136389882564 | validation: 0.35719646035720465]
	TIME [epoch: 8.19 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3110874076535125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3110874076535125 | validation: 0.24515475435889444]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_241.pth
	Model improved!!!
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27410772478653145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27410772478653145 | validation: 0.2841759301682259]
	TIME [epoch: 8.19 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2829654539877256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2829654539877256 | validation: 0.3258060813105083]
	TIME [epoch: 8.18 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33391930681856535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33391930681856535 | validation: 0.4012292026552105]
	TIME [epoch: 8.18 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49701630611295244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49701630611295244 | validation: 0.6154786698792658]
	TIME [epoch: 8.18 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49397977274623717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49397977274623717 | validation: 0.2717546561154485]
	TIME [epoch: 8.2 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29307186285889303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29307186285889303 | validation: 0.2659498915336236]
	TIME [epoch: 8.19 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27443508329459826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27443508329459826 | validation: 0.2453716956659293]
	TIME [epoch: 8.18 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25023645685288487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25023645685288487 | validation: 0.27368959440424806]
	TIME [epoch: 8.18 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26416239217910636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26416239217910636 | validation: 0.29337736974245554]
	TIME [epoch: 8.18 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35660265254745127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35660265254745127 | validation: 0.7001992790853058]
	TIME [epoch: 8.19 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7142382995715977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7142382995715977 | validation: 0.20971731967572832]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_252.pth
	Model improved!!!
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25746885358291877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25746885358291877 | validation: 0.33616371106508103]
	TIME [epoch: 8.17 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3319325338002802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3319325338002802 | validation: 0.42628929043986796]
	TIME [epoch: 8.17 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40854076685018587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40854076685018587 | validation: 0.29062423182916136]
	TIME [epoch: 8.16 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3941204346097774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3941204346097774 | validation: 0.8569090291402031]
	TIME [epoch: 8.19 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6764778934410326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6764778934410326 | validation: 0.3648164459923855]
	TIME [epoch: 8.18 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2945666082496229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2945666082496229 | validation: 0.3831570864373056]
	TIME [epoch: 8.21 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5025431088311582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5025431088311582 | validation: 0.24458864995546087]
	TIME [epoch: 8.18 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24172493266892414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24172493266892414 | validation: 0.2965997776430414]
	TIME [epoch: 8.19 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25013613621502345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25013613621502345 | validation: 0.2579547490712069]
	TIME [epoch: 8.18 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2705825238528866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2705825238528866 | validation: 0.44718761676867375]
	TIME [epoch: 8.18 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47009768333876933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47009768333876933 | validation: 0.2333765046177849]
	TIME [epoch: 8.21 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32945452187706364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32945452187706364 | validation: 0.4989287197043192]
	TIME [epoch: 8.2 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45335327508856976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45335327508856976 | validation: 0.6513046347991763]
	TIME [epoch: 8.19 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.690046919276281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.690046919276281 | validation: 0.29862071861598327]
	TIME [epoch: 8.18 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4529051652865455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4529051652865455 | validation: 0.26040327832602445]
	TIME [epoch: 8.18 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29386621598997653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29386621598997653 | validation: 0.31774453878491166]
	TIME [epoch: 8.19 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2721257937737875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2721257937737875 | validation: 0.2433578073091368]
	TIME [epoch: 8.22 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3401270078202208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3401270078202208 | validation: 0.38293351992063634]
	TIME [epoch: 8.19 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3135143521965163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3135143521965163 | validation: 0.25447280710642994]
	TIME [epoch: 8.19 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3256159270747638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3256159270747638 | validation: 0.30423182203246113]
	TIME [epoch: 8.19 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26764372256344277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26764372256344277 | validation: 0.1902607619182578]
	TIME [epoch: 8.18 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_273.pth
	Model improved!!!
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20360360785187137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20360360785187137 | validation: 0.19176914306417997]
	TIME [epoch: 8.21 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19199338060978313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19199338060978313 | validation: 0.22262117741878687]
	TIME [epoch: 8.22 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18646537477134012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18646537477134012 | validation: 0.15952915821609842]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_276.pth
	Model improved!!!
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1828489851651412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1828489851651412 | validation: 0.3099167481363547]
	TIME [epoch: 8.2 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23205551134574764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23205551134574764 | validation: 0.5094313953602181]
	TIME [epoch: 8.19 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.650929406800123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.650929406800123 | validation: 0.35783057158883447]
	TIME [epoch: 8.21 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2569946277050367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2569946277050367 | validation: 0.17162156283511376]
	TIME [epoch: 8.23 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18492323493736243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18492323493736243 | validation: 0.16541540783350409]
	TIME [epoch: 8.21 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18605974877398532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18605974877398532 | validation: 0.33578382275223306]
	TIME [epoch: 8.19 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24377268140582747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24377268140582747 | validation: 0.5525662700875111]
	TIME [epoch: 8.21 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6823297792321397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6823297792321397 | validation: 0.23948104273071152]
	TIME [epoch: 8.2 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31679706353365006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31679706353365006 | validation: 0.7580550439256567]
	TIME [epoch: 8.21 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6215178372086427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6215178372086427 | validation: 0.28685750784325953]
	TIME [epoch: 8.22 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2856381444843193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2856381444843193 | validation: 0.18810322534462795]
	TIME [epoch: 8.2 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2802920333416134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2802920333416134 | validation: 0.4849412967349115]
	TIME [epoch: 8.19 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3624390695835577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3624390695835577 | validation: 0.3915864612486876]
	TIME [epoch: 8.21 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4351232111876165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4351232111876165 | validation: 0.28890130399313485]
	TIME [epoch: 8.21 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3129329857302563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3129329857302563 | validation: 0.34819598898115595]
	TIME [epoch: 8.21 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3424020863529721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3424020863529721 | validation: 0.25570073833361456]
	TIME [epoch: 8.22 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32269255357923515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32269255357923515 | validation: 0.30683114744175577]
	TIME [epoch: 8.2 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2421380789321902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2421380789321902 | validation: 0.15120941276036523]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_294.pth
	Model improved!!!
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19614747314900782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19614747314900782 | validation: 0.3116256228764497]
	TIME [epoch: 8.21 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21001600190630995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21001600190630995 | validation: 0.17944552778142164]
	TIME [epoch: 8.2 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24141903459081918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24141903459081918 | validation: 0.33977657759978075]
	TIME [epoch: 8.22 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24479906657152653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24479906657152653 | validation: 0.3812468291563145]
	TIME [epoch: 8.2 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44061691630768707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44061691630768707 | validation: 0.31031698639006794]
	TIME [epoch: 8.22 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2710120612329386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2710120612329386 | validation: 0.1872589524365596]
	TIME [epoch: 8.22 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22172838386044355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22172838386044355 | validation: 0.2313389911932255]
	TIME [epoch: 8.19 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22113732197593083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22113732197593083 | validation: 0.5453994485250332]
	TIME [epoch: 8.2 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4621061547887308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4621061547887308 | validation: 0.22766159540734965]
	TIME [epoch: 8.21 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2805010784536129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2805010784536129 | validation: 0.2944386030557306]
	TIME [epoch: 8.19 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21660995437497776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21660995437497776 | validation: 0.15297194502997316]
	TIME [epoch: 8.18 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22120269490535688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22120269490535688 | validation: 0.8045734824834231]
	TIME [epoch: 8.19 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5864603305371577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5864603305371577 | validation: 0.189435580137806]
	TIME [epoch: 8.18 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17792567055711836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17792567055711836 | validation: 0.26143014422381605]
	TIME [epoch: 8.19 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35700046184748757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35700046184748757 | validation: 0.4846809655788442]
	TIME [epoch: 8.19 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4064967698344751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4064967698344751 | validation: 0.3682994655365335]
	TIME [epoch: 8.18 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28799401315182027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28799401315182027 | validation: 0.2483045631291103]
	TIME [epoch: 8.18 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28682059854440256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28682059854440256 | validation: 0.25847512893109587]
	TIME [epoch: 8.18 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30762643691982866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30762643691982866 | validation: 0.32576154601933066]
	TIME [epoch: 8.18 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4355529617038523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4355529617038523 | validation: 0.1607786949942897]
	TIME [epoch: 8.22 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20950281725705794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20950281725705794 | validation: 0.4866900468609752]
	TIME [epoch: 8.2 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39305940141534995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39305940141534995 | validation: 0.2566479879042087]
	TIME [epoch: 8.19 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3073067704453511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3073067704453511 | validation: 0.164923538346699]
	TIME [epoch: 8.19 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1942920604219462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1942920604219462 | validation: 0.32842096916576197]
	TIME [epoch: 8.19 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23547126480773137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23547126480773137 | validation: 0.3517417964619567]
	TIME [epoch: 8.21 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41895552254563556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41895552254563556 | validation: 0.2264920149841176]
	TIME [epoch: 8.24 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20263788154004672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20263788154004672 | validation: 0.25110871225251563]
	TIME [epoch: 8.2 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21700753137651796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21700753137651796 | validation: 0.23265943778266812]
	TIME [epoch: 8.2 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2000168892040633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2000168892040633 | validation: 0.19427455358007306]
	TIME [epoch: 8.19 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1989854753740246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1989854753740246 | validation: 0.2766441511336897]
	TIME [epoch: 8.2 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22596589243691295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22596589243691295 | validation: 0.20397487611292978]
	TIME [epoch: 8.21 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.254574887105252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.254574887105252 | validation: 0.3905216583366817]
	TIME [epoch: 8.19 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2847681697951779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2847681697951779 | validation: 0.1918448274326615]
	TIME [epoch: 8.2 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26895634168585014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26895634168585014 | validation: 0.38279240616322574]
	TIME [epoch: 8.16 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27199962899587754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27199962899587754 | validation: 0.20711155001273635]
	TIME [epoch: 8.18 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17753070119662354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17753070119662354 | validation: 0.16009062881206604]
	TIME [epoch: 8.18 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2401069562634662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2401069562634662 | validation: 0.3117156080695156]
	TIME [epoch: 8.22 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2199607748562958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2199607748562958 | validation: 0.21147729488720624]
	TIME [epoch: 8.2 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18821821919288667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18821821919288667 | validation: 0.3202366197541463]
	TIME [epoch: 8.2 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3931285649640151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3931285649640151 | validation: 0.39593922237871315]
	TIME [epoch: 8.19 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.316842246854323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.316842246854323 | validation: 0.20129480815899667]
	TIME [epoch: 8.2 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3026042285500662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3026042285500662 | validation: 0.1868052903430944]
	TIME [epoch: 8.21 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24249666231563255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24249666231563255 | validation: 0.3243030737041881]
	TIME [epoch: 8.23 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28493377321017316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28493377321017316 | validation: 0.33337204463296877]
	TIME [epoch: 8.2 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3215788321432649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3215788321432649 | validation: 0.28290351077419523]
	TIME [epoch: 8.2 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34802698977515534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34802698977515534 | validation: 0.26264623547362226]
	TIME [epoch: 8.2 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22635266249362965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22635266249362965 | validation: 0.2281044530092788]
	TIME [epoch: 8.22 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1804156306000206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1804156306000206 | validation: 0.15195507813125284]
	TIME [epoch: 8.22 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2001705059740772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2001705059740772 | validation: 0.14421339799270294]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_343.pth
	Model improved!!!
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14474094256431985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14474094256431985 | validation: 0.27274514562878177]
	TIME [epoch: 8.22 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19445941992497112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19445941992497112 | validation: 0.2819057771867154]
	TIME [epoch: 8.21 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32931462367475206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32931462367475206 | validation: 0.268730111917733]
	TIME [epoch: 8.21 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19461978141140931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19461978141140931 | validation: 0.14123255667417098]
	TIME [epoch: 8.22 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_347.pth
	Model improved!!!
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1640111933986064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1640111933986064 | validation: 0.1653149867095522]
	TIME [epoch: 8.22 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1908356413976797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1908356413976797 | validation: 0.30877539443445556]
	TIME [epoch: 8.21 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25977548851230525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25977548851230525 | validation: 0.475760080901766]
	TIME [epoch: 8.2 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4001825902210575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4001825902210575 | validation: 0.09771905722311047]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_351.pth
	Model improved!!!
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1361087191772809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1361087191772809 | validation: 0.23593056023237857]
	TIME [epoch: 8.2 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20337164304293487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20337164304293487 | validation: 0.2271345201599808]
	TIME [epoch: 8.21 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2617445123157653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2617445123157653 | validation: 0.303820309904575]
	TIME [epoch: 8.21 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19789792528084835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19789792528084835 | validation: 0.20578790142176362]
	TIME [epoch: 8.19 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2852683117599366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2852683117599366 | validation: 1.1146922694859336]
	TIME [epoch: 8.2 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9975864103469296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9975864103469296 | validation: 0.8084644224882096]
	TIME [epoch: 8.19 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7488513572832892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7488513572832892 | validation: 0.6622938939647206]
	TIME [epoch: 8.19 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.568179960389193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.568179960389193 | validation: 0.3476475523730173]
	TIME [epoch: 8.21 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3477132383942277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3477132383942277 | validation: 0.28796624243523083]
	TIME [epoch: 8.2 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38495524929327246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38495524929327246 | validation: 0.1835795560866363]
	TIME [epoch: 8.19 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29386621438146593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29386621438146593 | validation: 0.2293501095153584]
	TIME [epoch: 8.2 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2778623005478498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2778623005478498 | validation: 0.2191666485460825]
	TIME [epoch: 8.2 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23613076289759727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23613076289759727 | validation: 0.23070809830906633]
	TIME [epoch: 8.18 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2419995189796994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2419995189796994 | validation: 0.2063609761333689]
	TIME [epoch: 8.22 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20672011680980262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20672011680980262 | validation: 0.18484282275889843]
	TIME [epoch: 8.21 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21561498089325531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21561498089325531 | validation: 0.21522590628202437]
	TIME [epoch: 8.19 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19017607821271193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19017607821271193 | validation: 0.19678483705068422]
	TIME [epoch: 8.21 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20819851529978045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20819851529978045 | validation: 0.18070464687350793]
	TIME [epoch: 8.19 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1935277935914127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1935277935914127 | validation: 0.1868158088371334]
	TIME [epoch: 8.24 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18674939436943855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18674939436943855 | validation: 0.2680080551775896]
	TIME [epoch: 8.21 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21795925584496512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21795925584496512 | validation: 0.1561466663801436]
	TIME [epoch: 8.21 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19025350598794322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19025350598794322 | validation: 0.23154343338266994]
	TIME [epoch: 8.2 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19204565178570931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19204565178570931 | validation: 0.18656012187171078]
	TIME [epoch: 8.2 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1946425652670665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1946425652670665 | validation: 0.22627737200378034]
	TIME [epoch: 8.2 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2084511197624966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2084511197624966 | validation: 0.2172602754945977]
	TIME [epoch: 8.22 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1837264769955213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1837264769955213 | validation: 0.2155342025870578]
	TIME [epoch: 8.2 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21264506263414412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21264506263414412 | validation: 0.17372392023878716]
	TIME [epoch: 8.19 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13500079833061443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13500079833061443 | validation: 0.09721216197627558]
	TIME [epoch: 8.19 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_379.pth
	Model improved!!!
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13954248850545736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13954248850545736 | validation: 0.22266379554272986]
	TIME [epoch: 8.19 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1740396566428452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1740396566428452 | validation: 0.1610993168017111]
	TIME [epoch: 8.19 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19594070962516064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19594070962516064 | validation: 0.32430080397154876]
	TIME [epoch: 8.22 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21509357981809638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21509357981809638 | validation: 0.3686179898407529]
	TIME [epoch: 8.19 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40842601080986085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40842601080986085 | validation: 0.1732294021659766]
	TIME [epoch: 8.2 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19305618333863692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19305618333863692 | validation: 0.3508024203414003]
	TIME [epoch: 8.19 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22438510898192426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22438510898192426 | validation: 0.32474185443927706]
	TIME [epoch: 8.19 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3039956505092251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3039956505092251 | validation: 0.1645533420404829]
	TIME [epoch: 8.21 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22341673934165132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22341673934165132 | validation: 0.28197652098209103]
	TIME [epoch: 8.21 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19541842850821667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19541842850821667 | validation: 0.13382596165403854]
	TIME [epoch: 8.2 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1717633890148448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1717633890148448 | validation: 0.2787553916994427]
	TIME [epoch: 8.21 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20869849853272437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20869849853272437 | validation: 0.40271793713890347]
	TIME [epoch: 8.21 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44222835783979053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44222835783979053 | validation: 0.14235558448674338]
	TIME [epoch: 8.21 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1835291589850957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1835291589850957 | validation: 0.3691654460484162]
	TIME [epoch: 8.22 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23893064378421222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23893064378421222 | validation: 0.21906014038530472]
	TIME [epoch: 8.2 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2522776847722286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2522776847722286 | validation: 0.3890630895744323]
	TIME [epoch: 8.2 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3456748625660865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3456748625660865 | validation: 0.23658374242272978]
	TIME [epoch: 8.2 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19003349486586202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19003349486586202 | validation: 0.1185542043179586]
	TIME [epoch: 8.2 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18332791057273348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18332791057273348 | validation: 0.142882145613876]
	TIME [epoch: 8.2 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14549015929546458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14549015929546458 | validation: 0.17755522724557846]
	TIME [epoch: 8.22 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15390714579300555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15390714579300555 | validation: 0.14937067475359417]
	TIME [epoch: 8.21 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19873216765572935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19873216765572935 | validation: 0.2022343108082244]
	TIME [epoch: 8.21 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.167221706263355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.167221706263355 | validation: 0.21456699904160814]
	TIME [epoch: 8.2 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17393619325845208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17393619325845208 | validation: 0.24731735639098582]
	TIME [epoch: 8.19 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25708244969276606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25708244969276606 | validation: 0.16510051686241428]
	TIME [epoch: 8.22 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12014287512844828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12014287512844828 | validation: 0.11075299954675509]
	TIME [epoch: 8.2 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12228327385695821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12228327385695821 | validation: 0.34919517910150366]
	TIME [epoch: 8.2 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30229666923675325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30229666923675325 | validation: 0.2848451720185467]
	TIME [epoch: 8.2 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3200916078176989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3200916078176989 | validation: 0.12473847148802422]
	TIME [epoch: 8.2 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11374441005988974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11374441005988974 | validation: 0.21848589112816336]
	TIME [epoch: 8.19 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13518949306260256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13518949306260256 | validation: 0.13403020643267047]
	TIME [epoch: 8.21 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15212726542904892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15212726542904892 | validation: 0.2875854154136362]
	TIME [epoch: 8.2 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21810611564447688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21810611564447688 | validation: 0.331360600896646]
	TIME [epoch: 8.2 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.250504298473289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.250504298473289 | validation: 0.15213928493265072]
	TIME [epoch: 8.2 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18378029221153724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18378029221153724 | validation: 0.2558955825426521]
	TIME [epoch: 8.2 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16787058005610975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16787058005610975 | validation: 0.11728496184627149]
	TIME [epoch: 8.19 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16172978536968372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16172978536968372 | validation: 0.18586688163924545]
	TIME [epoch: 8.21 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17698653028888436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17698653028888436 | validation: 0.2689572536924731]
	TIME [epoch: 8.19 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23227878569665994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23227878569665994 | validation: 0.13527679280023772]
	TIME [epoch: 8.15 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1935520331533776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1935520331533776 | validation: 0.8168584853205219]
	TIME [epoch: 8.19 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6665742623454783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6665742623454783 | validation: 0.4570814680756769]
	TIME [epoch: 8.2 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3894261996952132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3894261996952132 | validation: 0.25043990774194097]
	TIME [epoch: 8.22 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2344411473451079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2344411473451079 | validation: 0.14304965805898995]
	TIME [epoch: 8.2 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2089526198098389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2089526198098389 | validation: 0.17209562221355862]
	TIME [epoch: 8.19 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1638966485981069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1638966485981069 | validation: 0.197746479726292]
	TIME [epoch: 8.19 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1761246221397529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1761246221397529 | validation: 0.16144029768763976]
	TIME [epoch: 8.17 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18509261500218585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18509261500218585 | validation: 0.19114937644541247]
	TIME [epoch: 8.19 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16301569270179664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16301569270179664 | validation: 0.15494833213452766]
	TIME [epoch: 8.19 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16673956483821342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16673956483821342 | validation: 0.20008429218619816]
	TIME [epoch: 8.19 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16253196910259718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16253196910259718 | validation: 0.17706221843993175]
	TIME [epoch: 8.19 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1972154103804192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1972154103804192 | validation: 0.14815111927917995]
	TIME [epoch: 8.19 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14078323189698969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14078323189698969 | validation: 0.14167619718425578]
	TIME [epoch: 8.18 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11418380615150373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11418380615150373 | validation: 0.08097420339063562]
	TIME [epoch: 8.21 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_432.pth
	Model improved!!!
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1042209403848704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1042209403848704 | validation: 0.17236056215305437]
	TIME [epoch: 8.17 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13517721119760157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13517721119760157 | validation: 0.4336461033916581]
	TIME [epoch: 8.2 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5059700430795306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5059700430795306 | validation: 0.28159471527443175]
	TIME [epoch: 8.17 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31469206955553386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31469206955553386 | validation: 0.6219413139778032]
	TIME [epoch: 8.19 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4595237219406053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4595237219406053 | validation: 0.4799690736342349]
	TIME [epoch: 8.17 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35404948803703645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35404948803703645 | validation: 0.22580515163801318]
	TIME [epoch: 8.21 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20495859917065617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20495859917065617 | validation: 0.18286813154375714]
	TIME [epoch: 8.18 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22553145740325536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22553145740325536 | validation: 0.20572545477668125]
	TIME [epoch: 8.19 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15720495176365842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15720495176365842 | validation: 0.1568853231163265]
	TIME [epoch: 8.21 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1672256361356943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1672256361356943 | validation: 0.19871690420744473]
	TIME [epoch: 8.19 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16547936019224666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16547936019224666 | validation: 0.15883540705825583]
	TIME [epoch: 8.21 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17494043329925377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17494043329925377 | validation: 0.5024440391133762]
	TIME [epoch: 8.22 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5730018545350427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5730018545350427 | validation: 0.5187541250012633]
	TIME [epoch: 8.19 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.747787133110447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.747787133110447 | validation: 2.0679532966076737]
	TIME [epoch: 8.19 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6866179950772926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6866179950772926 | validation: 3.1722422649614472]
	TIME [epoch: 8.19 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4831521554609948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4831521554609948 | validation: 3.2809047994151928]
	TIME [epoch: 8.2 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6539865081879417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6539865081879417 | validation: 2.4047890890231534]
	TIME [epoch: 8.21 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4457082465844313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4457082465844313 | validation: 1.3936705919408525]
	TIME [epoch: 8.2 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.511714957568635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.511714957568635 | validation: 1.4688848056697377]
	TIME [epoch: 8.2 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6467997764501388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6467997764501388 | validation: 1.1645254146860562]
	TIME [epoch: 8.2 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2600750207171385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2600750207171385 | validation: 0.9749143371432893]
	TIME [epoch: 8.19 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.084154128136072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.084154128136072 | validation: 1.0221214868864377]
	TIME [epoch: 8.2 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.040339544093474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.040339544093474 | validation: 0.8704737066862998]
	TIME [epoch: 8.22 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9525424675890604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9525424675890604 | validation: 0.7751147326256915]
	TIME [epoch: 8.2 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.858683351098042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.858683351098042 | validation: 0.7177613006270118]
	TIME [epoch: 8.2 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.772990235730928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.772990235730928 | validation: 0.9707913318106146]
	TIME [epoch: 8.2 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9508629735928668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9508629735928668 | validation: 1.1492912566995341]
	TIME [epoch: 8.19 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1050857818438815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1050857818438815 | validation: 1.2321954020578731]
	TIME [epoch: 8.2 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1599840029002633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1599840029002633 | validation: 1.4141999781589538]
	TIME [epoch: 8.21 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3774050008489416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3774050008489416 | validation: 1.7937889040833064]
	TIME [epoch: 8.21 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8078236620627752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8078236620627752 | validation: 1.8453475324337134]
	TIME [epoch: 8.2 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9024341607282227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9024341607282227 | validation: 1.6479961314557874]
	TIME [epoch: 8.2 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7832647459206998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7832647459206998 | validation: 1.6146727860488541]
	TIME [epoch: 8.21 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.655694047173335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.655694047173335 | validation: 1.652590940411341]
	TIME [epoch: 8.22 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.637005517567984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.637005517567984 | validation: 1.6810272798287398]
	TIME [epoch: 8.21 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7296722166983631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7296722166983631 | validation: 1.6673407501090531]
	TIME [epoch: 8.21 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7425159040265363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7425159040265363 | validation: 1.6120875323097998]
	TIME [epoch: 8.2 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6704383920586885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6704383920586885 | validation: 1.5388452381256448]
	TIME [epoch: 8.19 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6394633354273773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6394633354273773 | validation: 1.4203578660509812]
	TIME [epoch: 8.2 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.413958490613083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.413958490613083 | validation: 1.446010417784486]
	TIME [epoch: 8.21 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4581994867728634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4581994867728634 | validation: 1.2394207868992417]
	TIME [epoch: 8.19 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2311355188281283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2311355188281283 | validation: 1.0866460821234252]
	TIME [epoch: 8.19 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0603823924986282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0603823924986282 | validation: 0.9681451841551839]
	TIME [epoch: 8.21 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9717479240039203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9717479240039203 | validation: 0.903144892513097]
	TIME [epoch: 8.19 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9456785110577883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9456785110577883 | validation: 0.7064654664568613]
	TIME [epoch: 8.21 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7723740173359148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7723740173359148 | validation: 0.36910641861343363]
	TIME [epoch: 8.21 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44165257336919317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44165257336919317 | validation: 1.1733934606148264]
	TIME [epoch: 8.2 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0705911120854195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0705911120854195 | validation: 0.4496603921031043]
	TIME [epoch: 8.2 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49070170469773955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49070170469773955 | validation: 0.45183816960642265]
	TIME [epoch: 8.2 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5231993754013642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5231993754013642 | validation: 0.48941727807606683]
	TIME [epoch: 8.2 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5676187252686105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5676187252686105 | validation: 0.39644007765575895]
	TIME [epoch: 8.23 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4665415494441601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4665415494441601 | validation: 0.43196889817551]
	TIME [epoch: 8.17 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48027588018441947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48027588018441947 | validation: 0.3010039902151591]
	TIME [epoch: 8.2 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34346625462169966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34346625462169966 | validation: 0.3243510241000095]
	TIME [epoch: 8.2 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36466672045284165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36466672045284165 | validation: 0.2984657573992386]
	TIME [epoch: 8.18 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30386997852278474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30386997852278474 | validation: 0.2575761582147591]
	TIME [epoch: 8.2 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2785692690198552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2785692690198552 | validation: 0.2538286817710739]
	TIME [epoch: 8.21 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2561075217134391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2561075217134391 | validation: 0.24004997296852293]
	TIME [epoch: 8.21 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2573288251690747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2573288251690747 | validation: 0.23016915403736515]
	TIME [epoch: 8.18 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22891942219723624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22891942219723624 | validation: 0.2549839902803741]
	TIME [epoch: 8.2 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2769028542007837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2769028542007837 | validation: 0.4702641500548266]
	TIME [epoch: 8.2 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3666143396109125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3666143396109125 | validation: 0.41174531743593934]
	TIME [epoch: 8.2 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4283254696474439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4283254696474439 | validation: 0.43578350230682816]
	TIME [epoch: 8.21 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4276428454263474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4276428454263474 | validation: 0.2827215741143774]
	TIME [epoch: 8.17 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31228763185848873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31228763185848873 | validation: 0.2714172344212666]
	TIME [epoch: 8.2 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2831738906092955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2831738906092955 | validation: 0.24254036516380048]
	TIME [epoch: 8.18 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23250793940449668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23250793940449668 | validation: 0.20695686822766804]
	TIME [epoch: 8.21 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21699772492992167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21699772492992167 | validation: 0.19168557701032624]
	TIME [epoch: 8.21 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21477064404811125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21477064404811125 | validation: 0.2233726402738824]
	TIME [epoch: 61.5 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21109487067165097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21109487067165097 | validation: 0.18457765334339396]
	TIME [epoch: 17.3 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21113541347071596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21113541347071596 | validation: 0.46608105004602624]
	TIME [epoch: 17.4 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34099493436622375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34099493436622375 | validation: 0.2842069898705874]
	TIME [epoch: 17.5 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3129186426699155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3129186426699155 | validation: 0.17844131267783764]
	TIME [epoch: 17.5 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19865128241357133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19865128241357133 | validation: 0.1901483409808786]
	TIME [epoch: 17.5 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1838210940073703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1838210940073703 | validation: 0.1429974103325972]
	TIME [epoch: 17.5 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1632059313244697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1632059313244697 | validation: 0.16009793719920162]
	TIME [epoch: 17.5 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15078739781240796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15078739781240796 | validation: 0.20480982920508295]
	TIME [epoch: 17.5 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15728670760652338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15728670760652338 | validation: 0.15723317694532818]
	TIME [epoch: 17.5 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1990166740080061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1990166740080061 | validation: 0.39305672014313076]
	TIME [epoch: 17.5 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2934026314312715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2934026314312715 | validation: 0.21494649633260057]
	TIME [epoch: 17.5 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2681568511541844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2681568511541844 | validation: 0.15751272824032378]
	TIME [epoch: 17.6 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1545864383886469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1545864383886469 | validation: 0.18876435903980388]
	TIME [epoch: 17.5 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16380043831353117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16380043831353117 | validation: 0.14367063326725873]
	TIME [epoch: 17.5 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.186467433172972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.186467433172972 | validation: 0.20109237608299635]
	TIME [epoch: 17.5 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15105643507372624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15105643507372624 | validation: 0.18558333912936392]
	TIME [epoch: 17.5 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14545983411823393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14545983411823393 | validation: 0.10086184744955662]
	TIME [epoch: 17.5 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13044755210831685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13044755210831685 | validation: 0.26025220665288806]
	TIME [epoch: 17.5 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1875988781925456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1875988781925456 | validation: 0.18588448784760012]
	TIME [epoch: 17.5 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2198589743027684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2198589743027684 | validation: 0.2819988483488453]
	TIME [epoch: 17.5 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19299650211733416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19299650211733416 | validation: 0.12703242208382606]
	TIME [epoch: 17.5 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15393833904098508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15393833904098508 | validation: 0.12261373328376342]
	TIME [epoch: 17.5 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14173016002388011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14173016002388011 | validation: 0.16859260209932503]
	TIME [epoch: 17.5 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11971544618503814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11971544618503814 | validation: 0.12787682197109432]
	TIME [epoch: 17.4 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11841261844558293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11841261844558293 | validation: 0.21190390271844045]
	TIME [epoch: 17.5 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15544039877181187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15544039877181187 | validation: 0.18525738490828667]
	TIME [epoch: 17.5 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2623022130315365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2623022130315365 | validation: 0.3114884986960861]
	TIME [epoch: 17.5 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20917223376686472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20917223376686472 | validation: 0.14088611173623186]
	TIME [epoch: 17.5 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15513352660932703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15513352660932703 | validation: 0.12857748721052084]
	TIME [epoch: 17.5 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1485735989044533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1485735989044533 | validation: 0.2475257335556858]
	TIME [epoch: 17.5 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20442273128977495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20442273128977495 | validation: 0.14223920371641707]
	TIME [epoch: 17.4 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16464137339440763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16464137339440763 | validation: 0.22018000682579464]
	TIME [epoch: 17.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240822_152345/states/model_phi1_4c_v_mmd1_533.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 3990.711 seconds.
