Args:
Namespace(name='model_phi1_4c_v_mmd1', outdir='out/model_training/model_phi1_4c_v_mmd1', training_data='data/training_data/data_phi1_4c/training', validation_data='data/training_data/data_phi1_4c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1065247925

Training model...

Saving initial model state to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.398955021265969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.398955021265969 | validation: 2.470371932014873]
	TIME [epoch: 45.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1605798034239934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1605798034239934 | validation: 2.4590830429902826]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9716008933286857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9716008933286857 | validation: 2.689091375429691]
	TIME [epoch: 3.66 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2278108394029177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2278108394029177 | validation: 2.019415911223574]
	TIME [epoch: 3.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5419982514971493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5419982514971493 | validation: 1.8385444020237616]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.333819631266992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.333819631266992 | validation: 1.7074133966395995]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1573049833264144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1573049833264144 | validation: 1.6682708632274033]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.956823227271907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.956823227271907 | validation: 1.8458622655450174]
	TIME [epoch: 3.67 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0672667783148895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0672667783148895 | validation: 2.554251447284616]
	TIME [epoch: 3.67 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.720909123434865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.720909123434865 | validation: 1.735279882358173]
	TIME [epoch: 3.67 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0817749999588324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0817749999588324 | validation: 1.33730498557753]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5919037783929286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5919037783929286 | validation: 1.4619216105165544]
	TIME [epoch: 3.7 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.665748618231044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.665748618231044 | validation: 1.314083728261895]
	TIME [epoch: 3.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5092725989798317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5092725989798317 | validation: 1.2767923986369942]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.415994747470296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.415994747470296 | validation: 1.2442542656560267]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3873251004131006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3873251004131006 | validation: 1.240150089711879]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3497428267676985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3497428267676985 | validation: 1.2452971943544247]
	TIME [epoch: 3.72 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3428057260789712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3428057260789712 | validation: 1.2183400017631638]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.378145022066712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.378145022066712 | validation: 1.1485872185266246]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3076458582703643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3076458582703643 | validation: 1.0591694031990053]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2154384439377448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2154384439377448 | validation: 1.2119469524175615]
	TIME [epoch: 3.67 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.335150285605202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.335150285605202 | validation: 1.2100311908965375]
	TIME [epoch: 3.67 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3059739601533742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3059739601533742 | validation: 1.2002500479777753]
	TIME [epoch: 3.68 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.349725229416658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.349725229416658 | validation: 1.0951172803325535]
	TIME [epoch: 3.67 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2219654071885975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2219654071885975 | validation: 1.0073003682403956]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1422552239756685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1422552239756685 | validation: 1.1070249589984964]
	TIME [epoch: 3.67 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2716559775455218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2716559775455218 | validation: 1.1090793112258213]
	TIME [epoch: 3.67 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2598233504945358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2598233504945358 | validation: 1.1076130755651434]
	TIME [epoch: 3.68 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3016537606312324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3016537606312324 | validation: 0.9877908565962937]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1434645633876417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1434645633876417 | validation: 0.9440851083107701]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0608402901123786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0608402901123786 | validation: 0.9856559013868431]
	TIME [epoch: 3.68 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1065167462127834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1065167462127834 | validation: 1.064945870130704]
	TIME [epoch: 3.69 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2365117181594987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2365117181594987 | validation: 1.0418377258031368]
	TIME [epoch: 3.68 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.225530510798969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.225530510798969 | validation: 0.9732361932408516]
	TIME [epoch: 3.67 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1208149183151173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1208149183151173 | validation: 0.9219123071090055]
	TIME [epoch: 3.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0357427229745622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0357427229745622 | validation: 0.9179906643652768]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0379481052107689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0379481052107689 | validation: 0.9333306454896144]
	TIME [epoch: 3.68 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0518099732214719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0518099732214719 | validation: 0.9968580071368035]
	TIME [epoch: 3.68 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.086688554117257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.086688554117257 | validation: 0.9492371983824999]
	TIME [epoch: 3.68 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0904724060797732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0904724060797732 | validation: 0.9154490792924277]
	TIME [epoch: 3.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0501978765543944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0501978765543944 | validation: 0.8851940134039595]
	TIME [epoch: 3.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0325752070309095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0325752070309095 | validation: 0.8921640865075002]
	TIME [epoch: 3.68 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9970748515964595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9970748515964595 | validation: 0.874236743022032]
	TIME [epoch: 3.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9940973603291585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9940973603291585 | validation: 0.8625925095597742]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.050921813528792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.050921813528792 | validation: 0.9994351205412504]
	TIME [epoch: 3.69 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1394790734434725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1394790734434725 | validation: 0.8601935626456956]
	TIME [epoch: 3.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0105241574731352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0105241574731352 | validation: 0.8606386987231762]
	TIME [epoch: 3.7 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0616542803555098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0616542803555098 | validation: 0.9755578218097694]
	TIME [epoch: 3.69 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1307871111804368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1307871111804368 | validation: 0.8843655029385424]
	TIME [epoch: 3.69 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.050368990679603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.050368990679603 | validation: 0.8424921786594164]
	TIME [epoch: 3.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9655798301049632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9655798301049632 | validation: 0.8353754687488895]
	TIME [epoch: 3.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9535443304885948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9535443304885948 | validation: 0.7963211201193574]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.93265167578306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.93265167578306 | validation: 0.8337841482321302]
	TIME [epoch: 3.65 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.957923243027149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.957923243027149 | validation: 0.7729190771307102]
	TIME [epoch: 3.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9229155024253991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9229155024253991 | validation: 0.8086013872248851]
	TIME [epoch: 3.68 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9193065157620396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9193065157620396 | validation: 0.8352819103309514]
	TIME [epoch: 3.68 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9847077903463219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9847077903463219 | validation: 0.8012403680207545]
	TIME [epoch: 3.69 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9186922249580571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9186922249580571 | validation: 0.8103826538464995]
	TIME [epoch: 3.69 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9413385347115621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9413385347115621 | validation: 0.7614423600873876]
	TIME [epoch: 3.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9494659543498938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9494659543498938 | validation: 0.8115489742750741]
	TIME [epoch: 3.69 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9655768837264532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9655768837264532 | validation: 0.8662701819424362]
	TIME [epoch: 3.69 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0431404501341663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0431404501341663 | validation: 1.20671106718099]
	TIME [epoch: 3.7 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6101450014891998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6101450014891998 | validation: 1.26783638179998]
	TIME [epoch: 3.7 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5701626852850639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5701626852850639 | validation: 1.0382227504720067]
	TIME [epoch: 3.69 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3221829108239775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3221829108239775 | validation: 0.8344601192701031]
	TIME [epoch: 3.69 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0861318318846311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0861318318846311 | validation: 0.7911530834907857]
	TIME [epoch: 3.69 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9603682633824646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9603682633824646 | validation: 0.7910539982630878]
	TIME [epoch: 3.68 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9291661382774671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9291661382774671 | validation: 0.7924602211200907]
	TIME [epoch: 3.69 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8997426484909345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8997426484909345 | validation: 0.8073320782663936]
	TIME [epoch: 3.69 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9161947454667504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9161947454667504 | validation: 0.856047594417737]
	TIME [epoch: 3.69 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0144688589195119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0144688589195119 | validation: 0.7712814224533342]
	TIME [epoch: 3.68 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.923150871468563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.923150871468563 | validation: 0.7548379874068709]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9123490386943545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9123490386943545 | validation: 0.8230640390570856]
	TIME [epoch: 3.69 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9798294006342275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9798294006342275 | validation: 0.7639243217033039]
	TIME [epoch: 3.7 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9137554160258405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9137554160258405 | validation: 0.8150133406508838]
	TIME [epoch: 3.7 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9424057426790772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9424057426790772 | validation: 0.7854093307262885]
	TIME [epoch: 3.69 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.92353849510657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.92353849510657 | validation: 0.7614614760680365]
	TIME [epoch: 3.7 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9072170081556032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9072170081556032 | validation: 0.7935754136742785]
	TIME [epoch: 3.7 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9223326590190728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9223326590190728 | validation: 0.7962121116197594]
	TIME [epoch: 3.69 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.917262132339611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.917262132339611 | validation: 0.7487753544792412]
	TIME [epoch: 3.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8953569207772861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8953569207772861 | validation: 0.7319313612274407]
	TIME [epoch: 3.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8973920404110451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8973920404110451 | validation: 0.7828337447608503]
	TIME [epoch: 3.67 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8910119608070164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8910119608070164 | validation: 0.7595257010535246]
	TIME [epoch: 3.68 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8883707858833184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8883707858833184 | validation: 0.7656948464731681]
	TIME [epoch: 3.69 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8959221714165068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8959221714165068 | validation: 0.7699340333322873]
	TIME [epoch: 3.7 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9161790219941202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9161790219941202 | validation: 0.7649778907381523]
	TIME [epoch: 3.71 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9146405109566594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9146405109566594 | validation: 0.7887482881489072]
	TIME [epoch: 3.7 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9353118613641777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9353118613641777 | validation: 0.811434087247482]
	TIME [epoch: 3.7 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0254911278066492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0254911278066492 | validation: 0.869408608692989]
	TIME [epoch: 3.69 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.069979923451015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.069979923451015 | validation: 0.770404118873528]
	TIME [epoch: 3.69 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9263847524887712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9263847524887712 | validation: 0.8216214000520335]
	TIME [epoch: 3.69 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.987868382707328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.987868382707328 | validation: 0.7616596861339904]
	TIME [epoch: 3.69 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9032448237746102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9032448237746102 | validation: 0.7648944400800262]
	TIME [epoch: 3.69 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9688417870034312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9688417870034312 | validation: 0.7391910331850264]
	TIME [epoch: 3.7 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9445446253208123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9445446253208123 | validation: 0.79362420742341]
	TIME [epoch: 3.69 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.949744469683587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.949744469683587 | validation: 0.8344573958135287]
	TIME [epoch: 3.69 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9594128532387955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9594128532387955 | validation: 0.7530763078707985]
	TIME [epoch: 3.7 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8984574976488605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8984574976488605 | validation: 0.71478419215613]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8951579964691163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8951579964691163 | validation: 0.7670793354086086]
	TIME [epoch: 3.67 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9072146767644423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9072146767644423 | validation: 0.7498173196175326]
	TIME [epoch: 3.68 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8989057561660954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8989057561660954 | validation: 0.7409058294999966]
	TIME [epoch: 3.67 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9019739310388903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9019739310388903 | validation: 0.7626374563891163]
	TIME [epoch: 3.68 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9692157943555489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9692157943555489 | validation: 0.8649438761945079]
	TIME [epoch: 3.69 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0245758255457695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0245758255457695 | validation: 0.7815370943955058]
	TIME [epoch: 3.69 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9265579374656969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9265579374656969 | validation: 0.7126287158140856]
	TIME [epoch: 3.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8923370361393262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8923370361393262 | validation: 0.7647856634546657]
	TIME [epoch: 3.69 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9157569503536387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9157569503536387 | validation: 0.7410241893037193]
	TIME [epoch: 3.68 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8935630214420782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8935630214420782 | validation: 0.7300640957443882]
	TIME [epoch: 3.68 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8958471033737072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8958471033737072 | validation: 0.7323166245353641]
	TIME [epoch: 3.69 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8869964943926281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8869964943926281 | validation: 0.7313925512275712]
	TIME [epoch: 3.69 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8813706414939398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8813706414939398 | validation: 0.7349195003687966]
	TIME [epoch: 3.69 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8839953650689547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8839953650689547 | validation: 0.7285289060390161]
	TIME [epoch: 3.69 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8888732007674398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8888732007674398 | validation: 0.7799744703551261]
	TIME [epoch: 3.67 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9221363386286109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9221363386286109 | validation: 0.8455575506698345]
	TIME [epoch: 3.68 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.031020426423161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.031020426423161 | validation: 0.8183339694819769]
	TIME [epoch: 3.69 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9617896357988412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9617896357988412 | validation: 0.7263371530963678]
	TIME [epoch: 3.69 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8905080083089476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8905080083089476 | validation: 0.753827882869312]
	TIME [epoch: 3.69 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9068294503318651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9068294503318651 | validation: 0.7584463616956785]
	TIME [epoch: 3.69 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9124460224762118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9124460224762118 | validation: 0.7258087465525578]
	TIME [epoch: 3.69 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9053430278767032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9053430278767032 | validation: 0.7236954832232785]
	TIME [epoch: 3.7 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8793087747613463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8793087747613463 | validation: 0.7542962369359056]
	TIME [epoch: 3.69 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8887619718142904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8887619718142904 | validation: 0.7437080530919783]
	TIME [epoch: 3.69 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8713314914480168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8713314914480168 | validation: 0.8283653953901509]
	TIME [epoch: 3.69 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0206709422653346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0206709422653346 | validation: 0.711326650017337]
	TIME [epoch: 3.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8879828025370139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8879828025370139 | validation: 0.7809569215140694]
	TIME [epoch: 3.69 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9438176471841367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9438176471841367 | validation: 0.7632944458238697]
	TIME [epoch: 3.69 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8904093441042739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8904093441042739 | validation: 0.773455761402871]
	TIME [epoch: 3.69 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9467809510599358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9467809510599358 | validation: 0.8472154813490637]
	TIME [epoch: 3.69 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.01658928387453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.01658928387453 | validation: 0.752609050349494]
	TIME [epoch: 3.69 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9130830004769035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9130830004769035 | validation: 0.7513486763764882]
	TIME [epoch: 3.69 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9153668761246866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9153668761246866 | validation: 0.7446661553038354]
	TIME [epoch: 3.7 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9026528239446728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9026528239446728 | validation: 0.717901050513148]
	TIME [epoch: 3.69 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8835959295081247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8835959295081247 | validation: 0.7403144040253117]
	TIME [epoch: 3.68 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8955193016594221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8955193016594221 | validation: 0.7294301644267017]
	TIME [epoch: 3.69 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8818498178159061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8818498178159061 | validation: 0.7717055127848177]
	TIME [epoch: 3.69 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9076253388090655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9076253388090655 | validation: 0.7285167063538494]
	TIME [epoch: 3.69 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8908827365367935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8908827365367935 | validation: 0.7716042164181459]
	TIME [epoch: 3.69 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9300798612778297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9300798612778297 | validation: 0.7797084890097775]
	TIME [epoch: 3.69 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9133901573229278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9133901573229278 | validation: 0.7896148692837017]
	TIME [epoch: 3.69 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9282685724671778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9282685724671778 | validation: 0.7423567895641429]
	TIME [epoch: 3.69 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9070165991927257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9070165991927257 | validation: 0.7615279426379635]
	TIME [epoch: 3.69 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.905615414290131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.905615414290131 | validation: 0.7292588363635799]
	TIME [epoch: 3.71 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8742292782719241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8742292782719241 | validation: 0.7210498891978859]
	TIME [epoch: 3.69 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8660561571722397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8660561571722397 | validation: 0.7636175867809466]
	TIME [epoch: 3.7 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8971389589176618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8971389589176618 | validation: 0.7283053411349947]
	TIME [epoch: 3.7 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8590669902402535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8590669902402535 | validation: 0.7182542323925853]
	TIME [epoch: 3.7 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8803457509176461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8803457509176461 | validation: 0.7616099179866247]
	TIME [epoch: 3.7 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8950917560038225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8950917560038225 | validation: 0.8217045682934292]
	TIME [epoch: 3.7 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.965362938853825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.965362938853825 | validation: 0.9155422420593908]
	TIME [epoch: 3.69 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.028763210144722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.028763210144722 | validation: 0.7286376907452022]
	TIME [epoch: 3.7 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9096202648992815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9096202648992815 | validation: 0.7266689123769299]
	TIME [epoch: 3.7 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.872883184805697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.872883184805697 | validation: 0.7593304870474136]
	TIME [epoch: 3.7 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9043692535744035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9043692535744035 | validation: 0.7306516512336989]
	TIME [epoch: 3.7 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.870425488311293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.870425488311293 | validation: 0.7384717728356313]
	TIME [epoch: 3.7 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8725419136709357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8725419136709357 | validation: 0.726644863042092]
	TIME [epoch: 3.7 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8997076943274547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8997076943274547 | validation: 0.6941184768379213]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8626686930994254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8626686930994254 | validation: 0.7487603411599377]
	TIME [epoch: 3.69 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8844752898239324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8844752898239324 | validation: 0.7095234114945553]
	TIME [epoch: 3.69 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8730431045518466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8730431045518466 | validation: 0.723220727919156]
	TIME [epoch: 3.69 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8626768358553712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8626768358553712 | validation: 0.7334996901964921]
	TIME [epoch: 3.69 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8803087738907814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8803087738907814 | validation: 0.8253098325286573]
	TIME [epoch: 3.69 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9254850041752164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9254850041752164 | validation: 0.7879038681106549]
	TIME [epoch: 3.69 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9238015074294568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9238015074294568 | validation: 0.7303678295419709]
	TIME [epoch: 3.69 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8957570604489001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8957570604489001 | validation: 0.6967344363050911]
	TIME [epoch: 3.69 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8503916141572367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8503916141572367 | validation: 0.720703261392427]
	TIME [epoch: 3.7 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8652445324706364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8652445324706364 | validation: 0.7537754142160479]
	TIME [epoch: 3.7 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8819804566381139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8819804566381139 | validation: 0.7219360412984492]
	TIME [epoch: 3.69 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8816286460352291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8816286460352291 | validation: 0.7931494511780759]
	TIME [epoch: 3.68 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9491758072097446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9491758072097446 | validation: 0.7380611015802543]
	TIME [epoch: 3.69 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8939940560394465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8939940560394465 | validation: 0.7768251445120096]
	TIME [epoch: 3.69 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9620078303660367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9620078303660367 | validation: 0.7205052510245419]
	TIME [epoch: 3.69 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8739120960128908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8739120960128908 | validation: 0.734982319723773]
	TIME [epoch: 3.69 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9858072920659967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9858072920659967 | validation: 0.7544111504205964]
	TIME [epoch: 3.69 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9266547542001357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9266547542001357 | validation: 0.7537910846971646]
	TIME [epoch: 3.69 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9471406754141398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9471406754141398 | validation: 0.6947548919401924]
	TIME [epoch: 3.69 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8573004869969472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8573004869969472 | validation: 0.6994839911002653]
	TIME [epoch: 3.7 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8515196511761193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8515196511761193 | validation: 0.7062165649591567]
	TIME [epoch: 3.7 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8584776018935837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8584776018935837 | validation: 0.7021671203219864]
	TIME [epoch: 3.69 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8467107633552111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8467107633552111 | validation: 0.7323938003681525]
	TIME [epoch: 3.69 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8587607759776452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8587607759776452 | validation: 0.7856659390067201]
	TIME [epoch: 3.69 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9725177290364141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9725177290364141 | validation: 0.7203380751266041]
	TIME [epoch: 3.69 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8996069502134324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8996069502134324 | validation: 0.7819756198816249]
	TIME [epoch: 3.69 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9088177047834709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9088177047834709 | validation: 0.7378121088125384]
	TIME [epoch: 3.69 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9224460588097263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9224460588097263 | validation: 0.7704648874889427]
	TIME [epoch: 3.69 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9072094675153973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9072094675153973 | validation: 0.7541170578096129]
	TIME [epoch: 3.68 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9133065659168943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9133065659168943 | validation: 0.7185771983505028]
	TIME [epoch: 3.69 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8522576716169868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8522576716169868 | validation: 0.7015004762399395]
	TIME [epoch: 3.7 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8438202887788606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8438202887788606 | validation: 0.7089824803489496]
	TIME [epoch: 3.71 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8455102136048737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8455102136048737 | validation: 0.7245239776611299]
	TIME [epoch: 3.7 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.839090130806921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.839090130806921 | validation: 0.7026090247250771]
	TIME [epoch: 3.68 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8352062052570068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8352062052570068 | validation: 0.7052677362998105]
	TIME [epoch: 3.7 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8479206277455725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8479206277455725 | validation: 0.6883332593691125]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_192.pth
	Model improved!!!
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8422331884914224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8422331884914224 | validation: 0.7356872556887091]
	TIME [epoch: 3.69 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8968875214873907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8968875214873907 | validation: 0.8794781475223353]
	TIME [epoch: 3.68 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0321444276309961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0321444276309961 | validation: 0.7234064285577815]
	TIME [epoch: 3.68 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8795213197616221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8795213197616221 | validation: 0.6624690494625051]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8404749559378926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8404749559378926 | validation: 0.7018502069698145]
	TIME [epoch: 3.69 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8644436835070394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8644436835070394 | validation: 0.6737182687930536]
	TIME [epoch: 3.69 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8315933298859907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8315933298859907 | validation: 0.671237050281724]
	TIME [epoch: 3.7 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8229719739145448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8229719739145448 | validation: 0.6582861257731989]
	TIME [epoch: 3.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_200.pth
	Model improved!!!
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8196583348905614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8196583348905614 | validation: 0.7185222334248662]
	TIME [epoch: 47.6 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8736826218306898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8736826218306898 | validation: 0.748933272685886]
	TIME [epoch: 7.9 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9608663736032084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9608663736032084 | validation: 0.7691623282899125]
	TIME [epoch: 7.96 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9099147381179112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9099147381179112 | validation: 0.7026653469216635]
	TIME [epoch: 7.96 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8474490231856701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8474490231856701 | validation: 0.7271074851282614]
	TIME [epoch: 7.96 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.879456533929837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.879456533929837 | validation: 0.6793156027661458]
	TIME [epoch: 7.96 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8281699407975737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8281699407975737 | validation: 0.6652651292169476]
	TIME [epoch: 7.89 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8374709068004137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8374709068004137 | validation: 0.6610017459196736]
	TIME [epoch: 7.9 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8108388800444222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8108388800444222 | validation: 0.6586497815682306]
	TIME [epoch: 7.95 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8096229480554166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8096229480554166 | validation: 0.7304519440691707]
	TIME [epoch: 7.9 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8830169768000574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8830169768000574 | validation: 0.7152298021977248]
	TIME [epoch: 7.96 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8874406505748386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8874406505748386 | validation: 0.6575592867334171]
	TIME [epoch: 7.96 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8541323535148387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8541323535148387 | validation: 0.6473055805450459]
	TIME [epoch: 7.95 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_213.pth
	Model improved!!!
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8187166778709305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8187166778709305 | validation: 0.6758375741714071]
	TIME [epoch: 8 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8176770514640995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8176770514640995 | validation: 0.68752747047755]
	TIME [epoch: 7.97 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8658182892496004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8658182892496004 | validation: 0.7778119027186624]
	TIME [epoch: 7.97 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9393723220092364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9393723220092364 | validation: 0.7328773918541787]
	TIME [epoch: 7.97 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9074282148728838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9074282148728838 | validation: 0.6603715444230076]
	TIME [epoch: 7.96 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8127599042316058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8127599042316058 | validation: 0.6641912440001692]
	TIME [epoch: 7.98 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.79414764225862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.79414764225862 | validation: 0.6520234515567123]
	TIME [epoch: 7.98 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8026867591677409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8026867591677409 | validation: 0.6268189825322971]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7984522602968517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7984522602968517 | validation: 3.1299922778111924]
	TIME [epoch: 7.94 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8962792144719596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8962792144719596 | validation: 0.9416726738710994]
	TIME [epoch: 7.97 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1399370004174987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1399370004174987 | validation: 0.7310267727219802]
	TIME [epoch: 7.94 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.924159323560643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.924159323560643 | validation: 0.7374470566406338]
	TIME [epoch: 7.94 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9244353808745944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9244353808745944 | validation: 0.7219565725348414]
	TIME [epoch: 7.92 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.913104540820149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.913104540820149 | validation: 0.6832230075681022]
	TIME [epoch: 7.88 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8875734421459194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8875734421459194 | validation: 0.718920679589803]
	TIME [epoch: 7.91 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9020441440341976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9020441440341976 | validation: 0.6842155505525584]
	TIME [epoch: 7.92 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.859044016735758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.859044016735758 | validation: 0.6633539311947652]
	TIME [epoch: 7.95 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8520040779698969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8520040779698969 | validation: 0.672595932369445]
	TIME [epoch: 7.94 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8714115098986657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8714115098986657 | validation: 0.674030266708715]
	TIME [epoch: 7.93 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8776827851778845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8776827851778845 | validation: 0.7028106917332321]
	TIME [epoch: 7.96 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.872394273511013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.872394273511013 | validation: 0.6610806926468765]
	TIME [epoch: 7.96 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8274234731233473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8274234731233473 | validation: 0.6792654672676126]
	TIME [epoch: 7.95 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8372603227645512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8372603227645512 | validation: 0.6688499756216137]
	TIME [epoch: 7.95 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8154477039273746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8154477039273746 | validation: 0.6591315176823405]
	TIME [epoch: 7.95 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.815678690652673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.815678690652673 | validation: 0.6360501536288776]
	TIME [epoch: 7.95 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8137495264666764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8137495264666764 | validation: 0.6485016014648454]
	TIME [epoch: 7.94 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8021333778445046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8021333778445046 | validation: 0.6396434738077531]
	TIME [epoch: 7.96 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7872392177261227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7872392177261227 | validation: 0.6189738038000634]
	TIME [epoch: 7.93 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_241.pth
	Model improved!!!
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7806014359984684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7806014359984684 | validation: 0.5975157805717466]
	TIME [epoch: 7.93 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.768372308652401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.768372308652401 | validation: 0.5763549922338175]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_243.pth
	Model improved!!!
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7646199935883797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7646199935883797 | validation: 0.5957408045951058]
	TIME [epoch: 7.99 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7642143687866174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7642143687866174 | validation: 0.787685134574547]
	TIME [epoch: 7.97 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8940486337804834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8940486337804834 | validation: 0.7206670485954048]
	TIME [epoch: 8.01 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8800538256736552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8800538256736552 | validation: 0.6555506993871281]
	TIME [epoch: 7.98 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8361131817207524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8361131817207524 | validation: 0.594236373681025]
	TIME [epoch: 8.02 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8308810718453744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8308810718453744 | validation: 0.5610762124280767]
	TIME [epoch: 8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_249.pth
	Model improved!!!
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8248283367483937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8248283367483937 | validation: 1.1804244745299453]
	TIME [epoch: 7.98 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.385955826885367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.385955826885367 | validation: 0.7289765816246221]
	TIME [epoch: 7.96 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8990069624792474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8990069624792474 | validation: 0.7521795442146889]
	TIME [epoch: 8 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9237399714606866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9237399714606866 | validation: 0.6879394505621914]
	TIME [epoch: 8.01 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8841546256392516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8841546256392516 | validation: 0.7335398240994309]
	TIME [epoch: 8 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9086005372290155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9086005372290155 | validation: 0.656059459104738]
	TIME [epoch: 8.02 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8522959314942871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8522959314942871 | validation: 0.593536399968546]
	TIME [epoch: 7.97 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7877557973353951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7877557973353951 | validation: 0.6069114304389949]
	TIME [epoch: 8.01 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7694831566974183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7694831566974183 | validation: 0.5962133270233034]
	TIME [epoch: 8.01 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7505642829421763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7505642829421763 | validation: 0.5704811430975297]
	TIME [epoch: 7.99 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7377434849469795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7377434849469795 | validation: 0.5466147552196269]
	TIME [epoch: 8.02 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_260.pth
	Model improved!!!
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7148150221126166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7148150221126166 | validation: 0.5156846173752765]
	TIME [epoch: 7.99 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_261.pth
	Model improved!!!
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6338290152835949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6338290152835949 | validation: 1.177300247070584]
	TIME [epoch: 7.92 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2137539871792504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2137539871792504 | validation: 0.8035344455478927]
	TIME [epoch: 7.93 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0784340252301485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0784340252301485 | validation: 0.6972046333788313]
	TIME [epoch: 7.92 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9662830006477097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9662830006477097 | validation: 0.7022445821987348]
	TIME [epoch: 7.96 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9817261814844506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9817261814844506 | validation: 0.7597126775027528]
	TIME [epoch: 7.97 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.976795500890617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.976795500890617 | validation: 0.7686123446206777]
	TIME [epoch: 8 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9764742643213751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9764742643213751 | validation: 0.7252248364615541]
	TIME [epoch: 7.93 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8918625937360888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8918625937360888 | validation: 0.7082017058640394]
	TIME [epoch: 7.92 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9918631147842394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9918631147842394 | validation: 0.7027481623704945]
	TIME [epoch: 7.98 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9576461207741366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9576461207741366 | validation: 0.7115056562822064]
	TIME [epoch: 7.98 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9007391979527511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9007391979527511 | validation: 0.7282414716328289]
	TIME [epoch: 7.97 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8901474018533988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8901474018533988 | validation: 0.7755900716607211]
	TIME [epoch: 7.97 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9152108600417163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9152108600417163 | validation: 0.7110875817222211]
	TIME [epoch: 7.93 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8751822330359218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8751822330359218 | validation: 0.7269358062418342]
	TIME [epoch: 7.93 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8734688132704534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8734688132704534 | validation: 0.7128841717642539]
	TIME [epoch: 7.93 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8716675634191955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8716675634191955 | validation: 0.7122790942297108]
	TIME [epoch: 7.97 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8710117332281216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8710117332281216 | validation: 0.6973374034034677]
	TIME [epoch: 7.96 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8572198881779983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8572198881779983 | validation: 0.7177814474047094]
	TIME [epoch: 7.98 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8724751103843531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8724751103843531 | validation: 0.7322634924808331]
	TIME [epoch: 8.01 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.878576578251448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.878576578251448 | validation: 0.6786750333016937]
	TIME [epoch: 7.97 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8533569300134002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8533569300134002 | validation: 0.705069591151974]
	TIME [epoch: 7.97 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8934865891070021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8934865891070021 | validation: 0.6715400501641757]
	TIME [epoch: 7.97 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8629161498919516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8629161498919516 | validation: 0.7415572759431576]
	TIME [epoch: 8.01 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9444157337079997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9444157337079997 | validation: 0.7318087082171597]
	TIME [epoch: 7.98 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9364394891715404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9364394891715404 | validation: 0.6971651333772473]
	TIME [epoch: 8 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8538655539304719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8538655539304719 | validation: 0.6910500057656903]
	TIME [epoch: 7.99 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8564287646763191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8564287646763191 | validation: 0.7187085422163323]
	TIME [epoch: 7.98 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8868101750664257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8868101750664257 | validation: 0.6988225196337882]
	TIME [epoch: 8.02 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8923214218356691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8923214218356691 | validation: 0.7334567931628442]
	TIME [epoch: 8.03 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9336290549261684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9336290549261684 | validation: 0.7262269984607731]
	TIME [epoch: 8.02 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.891487582959072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.891487582959072 | validation: 0.6950153894730043]
	TIME [epoch: 8 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.836746749113876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.836746749113876 | validation: 0.689632714110628]
	TIME [epoch: 7.99 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8805589016771389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8805589016771389 | validation: 0.6757449535414252]
	TIME [epoch: 7.95 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8859680562895933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8859680562895933 | validation: 0.6763300903646448]
	TIME [epoch: 8.01 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8558577765216865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8558577765216865 | validation: 0.7010572902679882]
	TIME [epoch: 7.98 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8782351445362733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8782351445362733 | validation: 0.7194240974459164]
	TIME [epoch: 7.97 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8528399359041149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8528399359041149 | validation: 0.6923257870230947]
	TIME [epoch: 7.98 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8416753426887177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8416753426887177 | validation: 0.6659240287029691]
	TIME [epoch: 7.98 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8382994553207589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8382994553207589 | validation: 0.6703937373695227]
	TIME [epoch: 8 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.868988650996387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.868988650996387 | validation: 0.6441491594066581]
	TIME [epoch: 7.97 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8362510017485318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8362510017485318 | validation: 0.7258204626019383]
	TIME [epoch: 7.96 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.927856057364904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.927856057364904 | validation: 1.3042253171244935]
	TIME [epoch: 7.97 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.46462283137176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.46462283137176 | validation: 0.719415345290302]
	TIME [epoch: 7.97 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9168311147123868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9168311147123868 | validation: 0.6524004200019811]
	TIME [epoch: 7.97 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8356497640010548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8356497640010548 | validation: 0.6220161789566729]
	TIME [epoch: 7.97 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7904960825905167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7904960825905167 | validation: 0.6088099812086654]
	TIME [epoch: 8.03 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7684353247683054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7684353247683054 | validation: 0.5845653519502788]
	TIME [epoch: 8 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7305081647203931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7305081647203931 | validation: 1.3465427504744658]
	TIME [epoch: 8.02 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4815819425345575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4815819425345575 | validation: 0.6313237100754625]
	TIME [epoch: 7.97 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.744668131636223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.744668131636223 | validation: 0.6078506871255865]
	TIME [epoch: 7.97 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7542538980104203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7542538980104203 | validation: 0.5864233866913603]
	TIME [epoch: 7.97 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6989587045939587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6989587045939587 | validation: 0.5790110099425527]
	TIME [epoch: 7.98 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6240094236709457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6240094236709457 | validation: 0.6947827862331925]
	TIME [epoch: 7.98 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8193284845838577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8193284845838577 | validation: 0.6467318173342694]
	TIME [epoch: 7.97 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8079669034483206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8079669034483206 | validation: 0.5821791355054146]
	TIME [epoch: 8 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7868218192867257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7868218192867257 | validation: 0.5558802057823734]
	TIME [epoch: 7.99 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7448000941777086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7448000941777086 | validation: 0.5720566968423855]
	TIME [epoch: 7.99 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7417898179816393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7417898179816393 | validation: 0.5532668787783117]
	TIME [epoch: 8.02 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7079252044942115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7079252044942115 | validation: 0.6018726556185405]
	TIME [epoch: 7.98 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7282276429965078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7282276429965078 | validation: 0.5414775223136552]
	TIME [epoch: 8 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6579399538492733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6579399538492733 | validation: 0.45566826347888667]
	TIME [epoch: 8.01 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_322.pth
	Model improved!!!
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5735692220384293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5735692220384293 | validation: 0.48318579458174205]
	TIME [epoch: 8 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5679866932903681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5679866932903681 | validation: 0.6125330184909642]
	TIME [epoch: 8.01 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6539462701929546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6539462701929546 | validation: 0.60233982521194]
	TIME [epoch: 8.03 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7035822820236619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7035822820236619 | validation: 0.44177651845975596]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_326.pth
	Model improved!!!
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5996726620360266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5996726620360266 | validation: 0.4867853916671078]
	TIME [epoch: 7.96 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5791951470245529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5791951470245529 | validation: 0.40785387362862036]
	TIME [epoch: 7.96 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_328.pth
	Model improved!!!
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49782916556073636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49782916556073636 | validation: 0.5100967122927528]
	TIME [epoch: 7.91 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.564223485713159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.564223485713159 | validation: 0.426663024551085]
	TIME [epoch: 7.92 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.59183686866791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.59183686866791 | validation: 0.47306622583748636]
	TIME [epoch: 7.92 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5299429644947026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5299429644947026 | validation: 0.7982057572389215]
	TIME [epoch: 7.94 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7907796998668315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7907796998668315 | validation: 0.5749832712455685]
	TIME [epoch: 7.98 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7319911288903168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7319911288903168 | validation: 0.5811157437878682]
	TIME [epoch: 7.92 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7167027739091479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7167027739091479 | validation: 0.4146615127033629]
	TIME [epoch: 7.92 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5393297311192233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5393297311192233 | validation: 0.4478902324470207]
	TIME [epoch: 8.01 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5187933964807825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5187933964807825 | validation: 0.48723561421341866]
	TIME [epoch: 7.92 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5476706288352906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5476706288352906 | validation: 0.41857443891891555]
	TIME [epoch: 7.94 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.528006953299248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.528006953299248 | validation: 0.7514327139805661]
	TIME [epoch: 7.92 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7180606928651555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7180606928651555 | validation: 0.44483422429941105]
	TIME [epoch: 7.94 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5048729500169219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5048729500169219 | validation: 0.37290953068258786]
	TIME [epoch: 7.92 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_341.pth
	Model improved!!!
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47031903140558295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47031903140558295 | validation: 0.36688588320443916]
	TIME [epoch: 8.01 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_342.pth
	Model improved!!!
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5009605537469135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5009605537469135 | validation: 0.33194702934986564]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_343.pth
	Model improved!!!
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45639977299633705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45639977299633705 | validation: 0.3409775848048005]
	TIME [epoch: 7.99 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4304095410162257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4304095410162257 | validation: 0.3752436361053961]
	TIME [epoch: 7.99 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45389208472750087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45389208472750087 | validation: 0.44384019617824466]
	TIME [epoch: 7.98 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5181563148621975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5181563148621975 | validation: 0.35550341564415633]
	TIME [epoch: 8.01 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49159970067293374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49159970067293374 | validation: 0.34686707154877194]
	TIME [epoch: 8.01 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.469707815757364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.469707815757364 | validation: 0.5298334859338714]
	TIME [epoch: 8 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5541445735029072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5541445735029072 | validation: 0.4151606254244493]
	TIME [epoch: 7.99 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46829329112066276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46829329112066276 | validation: 0.3258140031740269]
	TIME [epoch: 7.99 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_351.pth
	Model improved!!!
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41483966925004157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41483966925004157 | validation: 0.433252227243506]
	TIME [epoch: 8 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6235654399949772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6235654399949772 | validation: 0.5769347948639066]
	TIME [epoch: 8.02 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7700543155048161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7700543155048161 | validation: 0.5339744216167109]
	TIME [epoch: 8.02 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7629895739727565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7629895739727565 | validation: 0.39773020680243076]
	TIME [epoch: 7.98 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5950339597856291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5950339597856291 | validation: 0.39913942119753104]
	TIME [epoch: 8 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5541138568805845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5541138568805845 | validation: 0.2912447323025662]
	TIME [epoch: 8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_357.pth
	Model improved!!!
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42393024713437755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42393024713437755 | validation: 0.29494361325757323]
	TIME [epoch: 7.96 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3945286335668072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3945286335668072 | validation: 0.8657534572945892]
	TIME [epoch: 8.02 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8704292299798008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8704292299798008 | validation: 0.5263674765947418]
	TIME [epoch: 8.01 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6514930253730837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6514930253730837 | validation: 0.8690947914296043]
	TIME [epoch: 7.99 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8002970713303705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8002970713303705 | validation: 0.7142726887004914]
	TIME [epoch: 8 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.638530565400979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.638530565400979 | validation: 0.5763153145562397]
	TIME [epoch: 8.01 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5293689738174551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5293689738174551 | validation: 0.3424869127246315]
	TIME [epoch: 8.02 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39681243043828107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39681243043828107 | validation: 0.34853697158719643]
	TIME [epoch: 8.02 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5198742303427182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5198742303427182 | validation: 0.41494157041161606]
	TIME [epoch: 7.99 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6124705754792704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6124705754792704 | validation: 0.3355801180208918]
	TIME [epoch: 8 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4288045025497915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4288045025497915 | validation: 0.5104569281146193]
	TIME [epoch: 8 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7413129498616641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7413129498616641 | validation: 0.7043811354329659]
	TIME [epoch: 7.98 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.016976996786167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.016976996786167 | validation: 0.7940054075041032]
	TIME [epoch: 8 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.151730786115832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.151730786115832 | validation: 0.8267297940854501]
	TIME [epoch: 8.01 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.191532951058794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.191532951058794 | validation: 0.8363354154607268]
	TIME [epoch: 7.99 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1901738540441804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1901738540441804 | validation: 0.6719864146705072]
	TIME [epoch: 7.99 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.058742687594806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.058742687594806 | validation: 0.6941239508755269]
	TIME [epoch: 7.99 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1024344175783005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1024344175783005 | validation: 0.9203539247334064]
	TIME [epoch: 8.01 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2985559809861686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2985559809861686 | validation: 0.958714643528812]
	TIME [epoch: 8.02 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2558027970316705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2558027970316705 | validation: 0.7828797909968661]
	TIME [epoch: 8.01 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9932415821360582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9932415821360582 | validation: 0.6974053147860454]
	TIME [epoch: 7.99 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8475779908125898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8475779908125898 | validation: 0.741008440611039]
	TIME [epoch: 8.01 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9009386335200685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9009386335200685 | validation: 0.6636330047429674]
	TIME [epoch: 8 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8499885021610595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8499885021610595 | validation: 0.6641546230435069]
	TIME [epoch: 8.02 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8119851747615837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8119851747615837 | validation: 0.5659320295529605]
	TIME [epoch: 8.03 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7111839269063502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7111839269063502 | validation: 0.4658827280554439]
	TIME [epoch: 7.98 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5371969005020023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5371969005020023 | validation: 0.46812224493435456]
	TIME [epoch: 7.98 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4750699850653926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4750699850653926 | validation: 0.45449407810038533]
	TIME [epoch: 8.01 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4669915923549955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4669915923549955 | validation: 0.42601081995826845]
	TIME [epoch: 7.96 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4545362278116914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4545362278116914 | validation: 0.407353570718743]
	TIME [epoch: 7.98 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4477398502552994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4477398502552994 | validation: 0.40646896778889796]
	TIME [epoch: 8 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43436816554879853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43436816554879853 | validation: 0.4125337566144269]
	TIME [epoch: 8.02 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43586453628668864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43586453628668864 | validation: 0.38716016593653335]
	TIME [epoch: 7.98 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4238867115734783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4238867115734783 | validation: 0.6639005571526537]
	TIME [epoch: 8.01 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6537435490328326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6537435490328326 | validation: 0.4484471094767306]
	TIME [epoch: 7.99 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4607835901803037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4607835901803037 | validation: 0.42206740233083995]
	TIME [epoch: 8.02 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46749159675359286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46749159675359286 | validation: 0.43235049981841867]
	TIME [epoch: 8.01 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44570720771376343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44570720771376343 | validation: 0.396709689576761]
	TIME [epoch: 8.01 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4207211002251782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4207211002251782 | validation: 0.3809231733607636]
	TIME [epoch: 8 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40585371070014925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40585371070014925 | validation: 0.40703143431466365]
	TIME [epoch: 8 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5113759662036962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5113759662036962 | validation: 0.4169460015222118]
	TIME [epoch: 8 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4590408301701772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4590408301701772 | validation: 0.39165759691608576]
	TIME [epoch: 8.01 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.426622123364526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.426622123364526 | validation: 0.37729891199396753]
	TIME [epoch: 8.04 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4013631845738314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4013631845738314 | validation: 0.32565251287557784]
	TIME [epoch: 8.01 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4041278942894871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4041278942894871 | validation: 0.34416857409176216]
	TIME [epoch: 8.01 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39531923540877956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39531923540877956 | validation: 0.42551525630025777]
	TIME [epoch: 8.03 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4954754446717147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4954754446717147 | validation: 0.5167298095533795]
	TIME [epoch: 8.02 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5984029676640519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5984029676640519 | validation: 0.268978084621383]
	TIME [epoch: 8.05 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_405.pth
	Model improved!!!
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35504644278680836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35504644278680836 | validation: 0.3242398161519908]
	TIME [epoch: 8.01 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.392072690413845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.392072690413845 | validation: 0.327159411338003]
	TIME [epoch: 8 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3878594823677264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3878594823677264 | validation: 0.284832239764493]
	TIME [epoch: 8 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37002039358057565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37002039358057565 | validation: 0.2905635749647159]
	TIME [epoch: 8.02 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3519190134585465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3519190134585465 | validation: 0.290628886224592]
	TIME [epoch: 8.03 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35456149734615466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35456149734615466 | validation: 0.4342949778404501]
	TIME [epoch: 8.03 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44866526704345006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44866526704345006 | validation: 0.41673138014929006]
	TIME [epoch: 8.04 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43398032919659096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43398032919659096 | validation: 0.32890135784575364]
	TIME [epoch: 7.98 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3729433271631737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3729433271631737 | validation: 0.49079060395205726]
	TIME [epoch: 8.03 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5106981997941759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5106981997941759 | validation: 0.580915895433852]
	TIME [epoch: 8.01 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7223660104788011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7223660104788011 | validation: 0.5411331097397735]
	TIME [epoch: 8.02 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6938850127464802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6938850127464802 | validation: 0.5023389397771033]
	TIME [epoch: 8.03 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6467576248722506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6467576248722506 | validation: 0.4574786539540623]
	TIME [epoch: 8.03 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5515443690609001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5515443690609001 | validation: 0.641526472829112]
	TIME [epoch: 8 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8328721898871715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8328721898871715 | validation: 0.6940193753093501]
	TIME [epoch: 8.01 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9542047572807835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9542047572807835 | validation: 0.7352086997396673]
	TIME [epoch: 8 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0650651440223595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0650651440223595 | validation: 0.6802236957681509]
	TIME [epoch: 8.01 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8885976054280817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8885976054280817 | validation: 0.6833155972672345]
	TIME [epoch: 8.04 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9115334739962389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9115334739962389 | validation: 0.6837040436157982]
	TIME [epoch: 8 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9566750238203722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9566750238203722 | validation: 0.6921417069971156]
	TIME [epoch: 8.01 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8315666170534626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8315666170534626 | validation: 0.6656911404132906]
	TIME [epoch: 7.98 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7602455035228262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7602455035228262 | validation: 0.5759586901359833]
	TIME [epoch: 8 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6782672692397079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6782672692397079 | validation: 0.4847480684727705]
	TIME [epoch: 8.02 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5723400020169995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5723400020169995 | validation: 0.40816851776842084]
	TIME [epoch: 7.99 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47983070318085214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47983070318085214 | validation: 0.3668291064284658]
	TIME [epoch: 8.03 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4304758139400673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4304758139400673 | validation: 0.34334796838713033]
	TIME [epoch: 8.01 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40188376365127704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40188376365127704 | validation: 0.3832807083051622]
	TIME [epoch: 8 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.417128152676011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.417128152676011 | validation: 0.39220229130337875]
	TIME [epoch: 8.02 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44493110698934474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44493110698934474 | validation: 0.40122339106972693]
	TIME [epoch: 8.03 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43768273002583485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43768273002583485 | validation: 0.34732212375402566]
	TIME [epoch: 8.03 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41820000948040786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41820000948040786 | validation: 0.77449746640431]
	TIME [epoch: 8.01 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7901514515412807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7901514515412807 | validation: 0.7566129880715311]
	TIME [epoch: 8.03 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6960431030206776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6960431030206776 | validation: 0.6177579949852618]
	TIME [epoch: 8.01 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5629147118716985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5629147118716985 | validation: 0.50824343706993]
	TIME [epoch: 8 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5090080804129898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5090080804129898 | validation: 0.39138690603877724]
	TIME [epoch: 8.02 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5334126176800014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5334126176800014 | validation: 0.3389005020589744]
	TIME [epoch: 8.01 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4009694911432029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4009694911432029 | validation: 0.31688024240255785]
	TIME [epoch: 7.98 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3431731703181133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3431731703181133 | validation: 0.2591399839421421]
	TIME [epoch: 7.95 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_443.pth
	Model improved!!!
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3066098662042976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3066098662042976 | validation: 0.282720345691872]
	TIME [epoch: 7.92 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3463630747525753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3463630747525753 | validation: 0.29892758280063747]
	TIME [epoch: 7.93 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38365547251712856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38365547251712856 | validation: 0.4270264775632633]
	TIME [epoch: 7.95 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4116121715821656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4116121715821656 | validation: 0.515538500896909]
	TIME [epoch: 7.92 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5262307520976519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5262307520976519 | validation: 0.42107879915229224]
	TIME [epoch: 7.93 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4438527580898085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4438527580898085 | validation: 0.4171039008227087]
	TIME [epoch: 7.94 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4499324777614417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4499324777614417 | validation: 0.392021568890499]
	TIME [epoch: 7.92 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4228329704644942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4228329704644942 | validation: 0.4413428627095196]
	TIME [epoch: 7.93 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43930767032079077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43930767032079077 | validation: 0.3452107515296864]
	TIME [epoch: 7.95 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36180053834290243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36180053834290243 | validation: 0.2868932096503603]
	TIME [epoch: 7.94 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2924830393084601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2924830393084601 | validation: 0.3227673223033853]
	TIME [epoch: 7.92 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3860560498394795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3860560498394795 | validation: 0.2755066605720587]
	TIME [epoch: 7.93 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3726413635497283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3726413635497283 | validation: 0.2568319095672673]
	TIME [epoch: 7.96 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_456.pth
	Model improved!!!
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29107441282758895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29107441282758895 | validation: 0.6004224499808907]
	TIME [epoch: 8 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8320281247607011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8320281247607011 | validation: 1.4043351676466227]
	TIME [epoch: 8.02 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6137428311622772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6137428311622772 | validation: 2.039493997627644]
	TIME [epoch: 7.99 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.223460263621767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.223460263621767 | validation: 2.2970274183805364]
	TIME [epoch: 7.99 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5101486372942183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5101486372942183 | validation: 3.1811456008955563]
	TIME [epoch: 7.98 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2349206509234465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2349206509234465 | validation: 3.3531591211120215]
	TIME [epoch: 7.99 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.346023393843641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.346023393843641 | validation: 3.4061691374094067]
	TIME [epoch: 8.01 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3612056483451567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3612056483451567 | validation: 3.4111458717530727]
	TIME [epoch: 8.01 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.359434635064655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.359434635064655 | validation: 3.398380514731261]
	TIME [epoch: 8.01 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.351008635636075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.351008635636075 | validation: 3.38114921826339]
	TIME [epoch: 7.99 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3345470990248396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3345470990248396 | validation: 3.3197488181407806]
	TIME [epoch: 8 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.308279103727971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.308279103727971 | validation: 2.3083620229668713]
	TIME [epoch: 8 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.885305010288172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.885305010288172 | validation: 1.4363901553764287]
	TIME [epoch: 8.01 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.72535433702583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.72535433702583 | validation: 0.9764970403889942]
	TIME [epoch: 7.99 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2185590062065776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2185590062065776 | validation: 0.8802114037451321]
	TIME [epoch: 7.99 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1855994896093964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1855994896093964 | validation: 0.8723844980409313]
	TIME [epoch: 7.99 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1454117645166608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1454117645166608 | validation: 0.7609730549407954]
	TIME [epoch: 7.98 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.951274049370545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.951274049370545 | validation: 0.8090975917826605]
	TIME [epoch: 7.99 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.994281088639632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.994281088639632 | validation: 0.7980654694810672]
	TIME [epoch: 7.99 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0186577174997786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0186577174997786 | validation: 0.7335348840318701]
	TIME [epoch: 7.98 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9056498370800904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9056498370800904 | validation: 0.7365492140114329]
	TIME [epoch: 7.98 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8712515515099054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8712515515099054 | validation: 0.7065332829736503]
	TIME [epoch: 7.97 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8679294260166487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8679294260166487 | validation: 0.7096641451079825]
	TIME [epoch: 7.97 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.919802188292407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.919802188292407 | validation: 0.6982735588700454]
	TIME [epoch: 7.99 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8753251852196592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8753251852196592 | validation: 0.6770962880773145]
	TIME [epoch: 7.99 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8616697457429899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8616697457429899 | validation: 0.6555422389369324]
	TIME [epoch: 8.01 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8134629916614029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8134629916614029 | validation: 0.6799580101076135]
	TIME [epoch: 7.99 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8469321141746018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8469321141746018 | validation: 0.6457458613477475]
	TIME [epoch: 8 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8083996705618433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8083996705618433 | validation: 0.7493590266867267]
	TIME [epoch: 7.99 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9856701223272052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9856701223272052 | validation: 0.7760600013098274]
	TIME [epoch: 8.01 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.019230756429564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.019230756429564 | validation: 0.6954359548196802]
	TIME [epoch: 8.02 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8958934325518005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8958934325518005 | validation: 0.6958128971507825]
	TIME [epoch: 7.99 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8145631420540919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8145631420540919 | validation: 0.6901045133237821]
	TIME [epoch: 8 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8234555206877286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8234555206877286 | validation: 0.7006409249024745]
	TIME [epoch: 8 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.856648621270302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.856648621270302 | validation: 0.683641784185917]
	TIME [epoch: 7.97 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8361166120710155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8361166120710155 | validation: 0.6441087138578525]
	TIME [epoch: 8 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7961877369776715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7961877369776715 | validation: 0.6510352949635181]
	TIME [epoch: 7.99 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7899040541388564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7899040541388564 | validation: 0.6607488538309835]
	TIME [epoch: 7.98 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8044395272291698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8044395272291698 | validation: 0.6645563286187137]
	TIME [epoch: 7.98 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8209516013993059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8209516013993059 | validation: 0.7033061390382712]
	TIME [epoch: 7.98 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8658660486782884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8658660486782884 | validation: 0.6788078613000983]
	TIME [epoch: 7.99 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8742813208730605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8742813208730605 | validation: 0.6077308496462319]
	TIME [epoch: 7.99 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7920865017230275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7920865017230275 | validation: 0.6372325985555206]
	TIME [epoch: 7.97 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7954573265505177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7954573265505177 | validation: 0.6756089970907189]
	TIME [epoch: 7.97 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8076053112136972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8076053112136972 | validation: 0.6209083418112951]
	TIME [epoch: 57.1 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7881605532885745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7881605532885745 | validation: 0.6134954103517922]
	TIME [epoch: 17 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7757036791096874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7757036791096874 | validation: 0.6040039168441462]
	TIME [epoch: 17 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7645148345650569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7645148345650569 | validation: 0.634425971454501]
	TIME [epoch: 17.1 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8161425757834667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8161425757834667 | validation: 0.6473299918461658]
	TIME [epoch: 17.1 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8326846121842977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8326846121842977 | validation: 0.6181788206908566]
	TIME [epoch: 17.1 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8080716694937382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8080716694937382 | validation: 0.5760133267691911]
	TIME [epoch: 17.1 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7652175249344728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7652175249344728 | validation: 0.5790848281135969]
	TIME [epoch: 17.1 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7648335766484349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7648335766484349 | validation: 0.5880562634253933]
	TIME [epoch: 17 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7939665398387723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7939665398387723 | validation: 0.6064903707844398]
	TIME [epoch: 17.1 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8029721367909594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8029721367909594 | validation: 0.5760509463866637]
	TIME [epoch: 17.1 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7740316798897492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7740316798897492 | validation: 0.5844822126471142]
	TIME [epoch: 17.1 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7676651445357535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7676651445357535 | validation: 0.587217615639786]
	TIME [epoch: 17.1 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7812870041295727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7812870041295727 | validation: 0.6433325589881518]
	TIME [epoch: 17.1 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8434577906245311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8434577906245311 | validation: 0.6179260663351798]
	TIME [epoch: 17.1 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7962604225511807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7962604225511807 | validation: 0.6136764093394022]
	TIME [epoch: 17.1 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7619850642455501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7619850642455501 | validation: 0.6412890642577435]
	TIME [epoch: 17.1 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.784863666822857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.784863666822857 | validation: 0.6024204323796103]
	TIME [epoch: 17.1 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7661274038659198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7661274038659198 | validation: 0.5808908333933326]
	TIME [epoch: 17.1 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7524693349015601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7524693349015601 | validation: 0.6024267141551437]
	TIME [epoch: 17.1 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7560547592199758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7560547592199758 | validation: 0.5852779787284678]
	TIME [epoch: 17.1 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7655508767092744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7655508767092744 | validation: 0.5894740681656571]
	TIME [epoch: 17.2 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7430095089258023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7430095089258023 | validation: 0.6074167602494227]
	TIME [epoch: 17.1 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7768631046802099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7768631046802099 | validation: 0.6758751332409526]
	TIME [epoch: 17.1 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8306951509637557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8306951509637557 | validation: 0.6524995279118024]
	TIME [epoch: 17.1 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8186237387869544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8186237387869544 | validation: 0.5885104977753733]
	TIME [epoch: 17.1 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.752814609860458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.752814609860458 | validation: 0.5710438151830384]
	TIME [epoch: 17.2 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7384336270355563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7384336270355563 | validation: 0.5852166429364865]
	TIME [epoch: 17.1 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7558701397443247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7558701397443247 | validation: 0.6169919474116068]
	TIME [epoch: 17.1 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.784513983677865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.784513983677865 | validation: 0.563922896834194]
	TIME [epoch: 17.1 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7351079076559693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7351079076559693 | validation: 0.5630080010276876]
	TIME [epoch: 17.1 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.729275021346235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.729275021346235 | validation: 0.5617878926542819]
	TIME [epoch: 17.1 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7488074409410677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7488074409410677 | validation: 0.5756700663790986]
	TIME [epoch: 17.1 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7403383373759799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7403383373759799 | validation: 0.5631905785631083]
	TIME [epoch: 17.1 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7304969593051388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7304969593051388 | validation: 0.6590027957027984]
	TIME [epoch: 17.1 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8075354146281858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8075354146281858 | validation: 0.5889224764379789]
	TIME [epoch: 17.1 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7809380324926524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7809380324926524 | validation: 0.6286135357007026]
	TIME [epoch: 17.1 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7793232699290985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7793232699290985 | validation: 0.5447378687800104]
	TIME [epoch: 17.1 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7337563502431749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7337563502431749 | validation: 0.5831626775515397]
	TIME [epoch: 17.1 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7507440262290854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7507440262290854 | validation: 0.6410030056660744]
	TIME [epoch: 17.1 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.786423873061313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.786423873061313 | validation: 0.5795992039666199]
	TIME [epoch: 17.1 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7229766558467849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7229766558467849 | validation: 0.5866559671376674]
	TIME [epoch: 17.1 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7562562463038407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7562562463038407 | validation: 0.6316566576374557]
	TIME [epoch: 17.1 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8294742369912822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8294742369912822 | validation: 0.5625096599224565]
	TIME [epoch: 17.1 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7326695752426755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7326695752426755 | validation: 0.5950410918597461]
	TIME [epoch: 17.1 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.725655678865227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.725655678865227 | validation: 0.5846024742447103]
	TIME [epoch: 17.1 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7314237066095622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7314237066095622 | validation: 0.600357209627738]
	TIME [epoch: 17.1 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.764395383355882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.764395383355882 | validation: 0.6653807768468933]
	TIME [epoch: 17.2 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8328188987359042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8328188987359042 | validation: 0.5872828738923268]
	TIME [epoch: 17.1 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7584436548742883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7584436548742883 | validation: 0.5737515151635905]
	TIME [epoch: 17.1 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7701827883718181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7701827883718181 | validation: 0.65979333052336]
	TIME [epoch: 17.1 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8931619676012551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8931619676012551 | validation: 0.7154682895696042]
	TIME [epoch: 17.1 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8600266370181683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8600266370181683 | validation: 0.5473439508183459]
	TIME [epoch: 17.1 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7493798917916823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7493798917916823 | validation: 0.5593355757400839]
	TIME [epoch: 17.2 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7366735789069037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7366735789069037 | validation: 0.5872976326743214]
	TIME [epoch: 17.1 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7421980707943362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7421980707943362 | validation: 0.5613284420292842]
	TIME [epoch: 17.1 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7137834499494689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7137834499494689 | validation: 0.5476412561507521]
	TIME [epoch: 17.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_183416/states/model_phi1_4c_v_mmd1_557.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 4280.258 seconds.
