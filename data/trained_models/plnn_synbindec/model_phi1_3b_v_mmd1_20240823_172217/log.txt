Args:
Namespace(name='model_phi1_3b_v_mmd1', outdir='out/model_training/model_phi1_3b_v_mmd1', training_data='data/training_data/data_phi1_3b/training', validation_data='data/training_data/data_phi1_3b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3800192975

Training model...

Saving initial model state to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.47859334426151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.47859334426151 | validation: 6.299624901903812]
	TIME [epoch: 26.9 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.098297230964354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.098297230964354 | validation: 4.231486370926261]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.901134975373838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.901134975373838 | validation: 4.056491979505739]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.753935156074591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.753935156074591 | validation: 3.3094212787370916]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.405048955577633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.405048955577633 | validation: 3.708530946599762]
	TIME [epoch: 1.89 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.536722651866539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.536722651866539 | validation: 3.451497804788271]
	TIME [epoch: 1.89 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.375394799069201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.375394799069201 | validation: 3.0227571160433313]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.179165617889626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.179165617889626 | validation: 3.063941164578031]
	TIME [epoch: 1.89 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.176157469335472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.176157469335472 | validation: 3.0555936841960247]
	TIME [epoch: 1.89 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.11471334748688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.11471334748688 | validation: 2.871175237455674]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.034862338122105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.034862338122105 | validation: 2.825069373151964]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9824758280769097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9824758280769097 | validation: 2.817105189559292]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.945517232832348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.945517232832348 | validation: 2.765985497833592]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.917067873385705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.917067873385705 | validation: 2.8662000566674903]
	TIME [epoch: 1.89 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9167826005992668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9167826005992668 | validation: 2.8179629190292417]
	TIME [epoch: 1.89 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.938738606036588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.938738606036588 | validation: 3.0087329049235176]
	TIME [epoch: 1.88 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.940115391151595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.940115391151595 | validation: 2.6533441978816974]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8117932662979657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8117932662979657 | validation: 2.6662212482810403]
	TIME [epoch: 1.9 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.784110973824827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.784110973824827 | validation: 2.7096918285843055]
	TIME [epoch: 1.88 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.819751811001826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.819751811001826 | validation: 2.6710660662937595]
	TIME [epoch: 1.88 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7624999188206085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7624999188206085 | validation: 2.610729018385398]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7483547114907005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7483547114907005 | validation: 2.673515733202825]
	TIME [epoch: 1.89 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.719185214291402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.719185214291402 | validation: 2.618870129066549]
	TIME [epoch: 1.89 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7376369088336903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7376369088336903 | validation: 2.827969574511146]
	TIME [epoch: 1.89 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.76942802777956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.76942802777956 | validation: 2.5302720353194754]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.66356511314517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.66356511314517 | validation: 2.523212993983301]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6160493696055585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6160493696055585 | validation: 2.473491584411413]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5976640094222425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5976640094222425 | validation: 2.4745505159407903]
	TIME [epoch: 1.89 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5844901116404793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5844901116404793 | validation: 2.4755697315070155]
	TIME [epoch: 1.88 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.580659889520797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.580659889520797 | validation: 2.486468461724533]
	TIME [epoch: 1.88 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.57180286751741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.57180286751741 | validation: 2.4877020528790825]
	TIME [epoch: 1.88 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5869954659973646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5869954659973646 | validation: 2.577556844343964]
	TIME [epoch: 1.88 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.583111650051508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.583111650051508 | validation: 2.5504301861051446]
	TIME [epoch: 1.89 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6240912803889764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6240912803889764 | validation: 2.6522095431501698]
	TIME [epoch: 1.88 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6091388941654547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6091388941654547 | validation: 2.3697079749923744]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.469879382564234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.469879382564234 | validation: 2.3612633186535756]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4570750995496224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4570750995496224 | validation: 2.410285298666663]
	TIME [epoch: 1.88 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.456696214625821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.456696214625821 | validation: 2.3485200535357116]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4381973508451327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4381973508451327 | validation: 2.3864162166511016]
	TIME [epoch: 1.9 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4278917774764732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4278917774764732 | validation: 2.3451477837839407]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4180697403624403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4180697403624403 | validation: 2.3949837136500434]
	TIME [epoch: 1.9 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.410816046878232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.410816046878232 | validation: 2.3438311950464397]
	TIME [epoch: 1.9 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4057826243318914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4057826243318914 | validation: 2.40744351027255]
	TIME [epoch: 1.89 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3995241413592265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3995241413592265 | validation: 2.329531096106913]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.374206547181676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.374206547181676 | validation: 2.368602704164155]
	TIME [epoch: 1.89 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.357242025258197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.357242025258197 | validation: 2.2963227157213097]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3354906344913027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3354906344913027 | validation: 2.3234668541482715]
	TIME [epoch: 1.88 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3180904567138483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3180904567138483 | validation: 2.2828417814069333]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3089093014157585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3089093014157585 | validation: 2.3052663066464127]
	TIME [epoch: 1.88 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2926371917498085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2926371917498085 | validation: 2.2722261347421684]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.276811284768079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.276811284768079 | validation: 2.292245092019749]
	TIME [epoch: 1.89 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2656788196186746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2656788196186746 | validation: 2.2554470446773442]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.249034762746344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.249034762746344 | validation: 2.282365262209428]
	TIME [epoch: 1.89 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.232454319082239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.232454319082239 | validation: 2.2445556376777396]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.203889022223993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.203889022223993 | validation: 2.266807313707131]
	TIME [epoch: 1.89 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.166440836648486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.166440836648486 | validation: 2.188043325745308]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.103840572344781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.103840572344781 | validation: 2.1978516623910807]
	TIME [epoch: 1.89 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.09668669110556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.09668669110556 | validation: 2.4721506302621545]
	TIME [epoch: 1.89 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3467708658767594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3467708658767594 | validation: 2.263554069924818]
	TIME [epoch: 1.9 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.102271552827757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.102271552827757 | validation: 2.169214795144276]
	TIME [epoch: 1.9 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0320160127800695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0320160127800695 | validation: 2.2892893720756358]
	TIME [epoch: 1.89 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1558270171090452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1558270171090452 | validation: 2.141757442866543]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.984187895757309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.984187895757309 | validation: 2.1321733990541882]
	TIME [epoch: 1.9 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.966508351291609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.966508351291609 | validation: 2.1232060306468328]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9653873365136465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9653873365136465 | validation: 2.1005614524882334]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9373999844959715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9373999844959715 | validation: 2.0817921155878123]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.925193151852337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.925193151852337 | validation: 2.0799302077775965]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.902127382731686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.902127382731686 | validation: 2.078324168375662]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8919077866872858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8919077866872858 | validation: 2.0978809423221043]
	TIME [epoch: 1.89 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.896665793540874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.896665793540874 | validation: 2.1538884236258293]
	TIME [epoch: 1.88 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9586180131184956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9586180131184956 | validation: 2.2260962185767936]
	TIME [epoch: 1.88 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9687295414452035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9687295414452035 | validation: 2.0900524448001963]
	TIME [epoch: 1.88 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8803133040676325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8803133040676325 | validation: 2.060167883181778]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.840257207461381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.840257207461381 | validation: 2.0821055113681366]
	TIME [epoch: 1.88 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.836399303606376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.836399303606376 | validation: 2.0550258116986035]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8310057763049317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8310057763049317 | validation: 2.0802154007784828]
	TIME [epoch: 1.88 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8291307182221437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8291307182221437 | validation: 2.0639008931048424]
	TIME [epoch: 1.88 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8152361958979344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8152361958979344 | validation: 2.0897172530182067]
	TIME [epoch: 1.88 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8279297103453405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8279297103453405 | validation: 2.102847345615163]
	TIME [epoch: 1.88 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8440377706071787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8440377706071787 | validation: 2.13856645478069]
	TIME [epoch: 1.89 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8362184774539196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8362184774539196 | validation: 2.0335646399931604]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7801753475647617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7801753475647617 | validation: 2.040989832142868]
	TIME [epoch: 1.88 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.743402715507755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.743402715507755 | validation: 2.0349966165929483]
	TIME [epoch: 1.88 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.733076827553169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.733076827553169 | validation: 2.0317761663725187]
	TIME [epoch: 1.87 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6638704921828293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6638704921828293 | validation: 1.9633496193086275]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.444337765334787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.444337765334787 | validation: 1.831249414625643]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2676168774698824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2676168774698824 | validation: 2.711315067824929]
	TIME [epoch: 1.89 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.54375751777688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.54375751777688 | validation: 1.3637823510490623]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4213960465990199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4213960465990199 | validation: 1.2807203183185938]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3572727092696024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3572727092696024 | validation: 1.3953588737345897]
	TIME [epoch: 1.89 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2526081640124116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2526081640124116 | validation: 0.9586299775640654]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.03398602087733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.03398602087733 | validation: 0.9084261074327193]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9346203529681009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9346203529681009 | validation: 0.9555702781009977]
	TIME [epoch: 1.89 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9295988819525143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9295988819525143 | validation: 0.8775779061811433]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8967845949323234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8967845949323234 | validation: 0.8643176419253046]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8910875303984043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8910875303984043 | validation: 0.8314291649634802]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8862306018755939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8862306018755939 | validation: 0.8433747988679688]
	TIME [epoch: 1.9 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8728790889499621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8728790889499621 | validation: 0.812367575706179]
	TIME [epoch: 1.9 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8750063510120037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8750063510120037 | validation: 0.8994015815407785]
	TIME [epoch: 1.91 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8810911237296302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8810911237296302 | validation: 0.8185743192962063]
	TIME [epoch: 1.89 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8625288918396606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8625288918396606 | validation: 0.822648481202778]
	TIME [epoch: 1.9 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8418163538136643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8418163538136643 | validation: 0.7654021505440953]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8444790961587478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8444790961587478 | validation: 0.8641228505786387]
	TIME [epoch: 1.88 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8671628842020965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8671628842020965 | validation: 0.8535481562509948]
	TIME [epoch: 1.88 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9320062263992123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9320062263992123 | validation: 1.0163474150240375]
	TIME [epoch: 1.88 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9378143226051486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9378143226051486 | validation: 0.7314167362332125]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7990390135345191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7990390135345191 | validation: 0.7219537563950467]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.798386881924514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.798386881924514 | validation: 0.8470569145110924]
	TIME [epoch: 1.88 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8422989062491957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8422989062491957 | validation: 0.7343198906225421]
	TIME [epoch: 1.88 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8014813400750654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8014813400750654 | validation: 0.7372178279203262]
	TIME [epoch: 1.89 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.777587036017733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.777587036017733 | validation: 0.701321500187874]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7682066773027891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7682066773027891 | validation: 0.6880192245682379]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7683847621951725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7683847621951725 | validation: 0.7016632769623302]
	TIME [epoch: 1.9 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7675393733802094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7675393733802094 | validation: 0.684826342198872]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7618092462363066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7618092462363066 | validation: 0.6961786781542006]
	TIME [epoch: 1.89 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7517398258231309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7517398258231309 | validation: 0.70140365557373]
	TIME [epoch: 1.89 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7601410267067031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7601410267067031 | validation: 0.7681041110029446]
	TIME [epoch: 1.89 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7909729528993898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7909729528993898 | validation: 0.7792631701869093]
	TIME [epoch: 1.89 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8425350290005786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8425350290005786 | validation: 0.7925771117675574]
	TIME [epoch: 1.89 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7897478873639199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7897478873639199 | validation: 0.6857969237363455]
	TIME [epoch: 1.98 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7382651820647036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7382651820647036 | validation: 0.744076559085381]
	TIME [epoch: 1.89 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7589357547531773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7589357547531773 | validation: 0.7742261047911466]
	TIME [epoch: 1.9 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8321486769862787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8321486769862787 | validation: 0.9281624397027702]
	TIME [epoch: 1.9 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8926906528189658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8926906528189658 | validation: 0.7927964051738796]
	TIME [epoch: 1.91 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8594306194608778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8594306194608778 | validation: 0.8533689596775716]
	TIME [epoch: 1.9 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8032835669900763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8032835669900763 | validation: 0.7097009968599304]
	TIME [epoch: 1.9 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7634752858694686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7634752858694686 | validation: 0.7302482297888662]
	TIME [epoch: 1.89 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7435205672146902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7435205672146902 | validation: 0.7675160772518913]
	TIME [epoch: 1.9 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7507238292937819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7507238292937819 | validation: 0.710538488033722]
	TIME [epoch: 1.89 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.748632276168038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.748632276168038 | validation: 0.7559360935514524]
	TIME [epoch: 1.89 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.743073139087467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.743073139087467 | validation: 0.7097080034511278]
	TIME [epoch: 1.89 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7401748419034654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7401748419034654 | validation: 0.727135420878598]
	TIME [epoch: 1.9 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7349699657756142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7349699657756142 | validation: 0.7060484290565565]
	TIME [epoch: 1.9 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7278934853666641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7278934853666641 | validation: 0.7668328799405784]
	TIME [epoch: 1.9 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7370340818428516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7370340818428516 | validation: 0.7764560393011467]
	TIME [epoch: 1.9 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8025978312781602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8025978312781602 | validation: 1.0349514533327278]
	TIME [epoch: 1.9 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.890807648709804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.890807648709804 | validation: 0.7636364744680172]
	TIME [epoch: 1.9 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7918519743697647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7918519743697647 | validation: 0.7241908937990177]
	TIME [epoch: 1.89 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7160747104487521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7160747104487521 | validation: 0.7442976468735027]
	TIME [epoch: 1.9 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7347870595271428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7347870595271428 | validation: 0.7244910853150641]
	TIME [epoch: 1.9 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7331926163953597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7331926163953597 | validation: 0.7293654206306512]
	TIME [epoch: 1.9 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7201102513795175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7201102513795175 | validation: 0.7170308200265807]
	TIME [epoch: 1.9 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7122703644875965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7122703644875965 | validation: 0.7199351274019338]
	TIME [epoch: 1.9 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7150745483585098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7150745483585098 | validation: 0.7488307967963754]
	TIME [epoch: 1.91 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7209031240753302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7209031240753302 | validation: 0.8187842726831538]
	TIME [epoch: 1.89 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8429550441658947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8429550441658947 | validation: 0.9783328235266062]
	TIME [epoch: 1.9 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8752954189500898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8752954189500898 | validation: 0.7162100540365668]
	TIME [epoch: 1.9 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7190610555763887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7190610555763887 | validation: 0.7349346513263411]
	TIME [epoch: 1.9 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7288666615729628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7288666615729628 | validation: 0.854510072282349]
	TIME [epoch: 1.9 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7788437382234303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7788437382234303 | validation: 0.7857842114453453]
	TIME [epoch: 1.89 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7969530264227302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7969530264227302 | validation: 0.7959416082666774]
	TIME [epoch: 1.9 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7757530854055267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7757530854055267 | validation: 0.7725546425726217]
	TIME [epoch: 1.89 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7525758288195772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7525758288195772 | validation: 0.7200664750153606]
	TIME [epoch: 1.9 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.706873448691278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.706873448691278 | validation: 0.7333802599372522]
	TIME [epoch: 1.9 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7084770861540571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7084770861540571 | validation: 0.7341989601024164]
	TIME [epoch: 1.9 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7054104270465572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7054104270465572 | validation: 0.7303269750723089]
	TIME [epoch: 1.9 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.713568590345381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.713568590345381 | validation: 0.7478812090626565]
	TIME [epoch: 1.9 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7197930920995702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7197930920995702 | validation: 0.7217595914167196]
	TIME [epoch: 1.9 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7047638637331677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7047638637331677 | validation: 0.7864896184085005]
	TIME [epoch: 1.9 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7262265772224643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7262265772224643 | validation: 0.7783075275196227]
	TIME [epoch: 1.9 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.785079394863368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.785079394863368 | validation: 0.9423517262246615]
	TIME [epoch: 1.9 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8136211535076763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8136211535076763 | validation: 0.7455417504562755]
	TIME [epoch: 1.9 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7253912280137889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7253912280137889 | validation: 0.7217705086259134]
	TIME [epoch: 1.9 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7059564010657242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7059564010657242 | validation: 0.7638826067154263]
	TIME [epoch: 1.9 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7212432580579522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7212432580579522 | validation: 0.72074334176]
	TIME [epoch: 1.89 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.739057341536419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.739057341536419 | validation: 0.7729484090659648]
	TIME [epoch: 1.91 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7500743254844566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7500743254844566 | validation: 0.7447757474492254]
	TIME [epoch: 1.9 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7247587932974306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7247587932974306 | validation: 0.7317622152607287]
	TIME [epoch: 1.89 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7192756258503943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7192756258503943 | validation: 0.745648215998564]
	TIME [epoch: 1.89 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6851548691265253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6851548691265253 | validation: 0.7270559310737479]
	TIME [epoch: 1.9 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.714418387183174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.714418387183174 | validation: 0.8855279758012977]
	TIME [epoch: 1.89 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7829066758518576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7829066758518576 | validation: 0.7800956284086018]
	TIME [epoch: 1.9 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7742849513588204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7742849513588204 | validation: 0.7784135062426556]
	TIME [epoch: 1.89 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7093578194035314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7093578194035314 | validation: 0.7306151474450058]
	TIME [epoch: 1.9 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.703211832455602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.703211832455602 | validation: 0.7294724267659384]
	TIME [epoch: 1.9 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7237216888692527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7237216888692527 | validation: 0.7654905852038598]
	TIME [epoch: 1.9 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7324887926758428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7324887926758428 | validation: 0.7068321401329419]
	TIME [epoch: 1.9 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7061264567460007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7061264567460007 | validation: 0.7412372708321477]
	TIME [epoch: 1.9 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6972111537874927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6972111537874927 | validation: 0.6944721727210316]
	TIME [epoch: 1.9 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6832354728096559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6832354728096559 | validation: 0.7539191591391438]
	TIME [epoch: 1.9 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6963183458080121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6963183458080121 | validation: 0.7273526457739574]
	TIME [epoch: 1.9 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7382905235915908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7382905235915908 | validation: 0.9981801467979111]
	TIME [epoch: 1.89 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8324395210084208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8324395210084208 | validation: 0.7958795992524639]
	TIME [epoch: 1.9 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8047237932551918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8047237932551918 | validation: 0.7202278610405615]
	TIME [epoch: 1.89 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6830376548041011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6830376548041011 | validation: 0.7366617460060583]
	TIME [epoch: 1.89 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.668582357317597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.668582357317597 | validation: 0.7197257288021908]
	TIME [epoch: 1.9 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6962512148856539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6962512148856539 | validation: 0.7974390738258088]
	TIME [epoch: 1.9 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7175095718910602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7175095718910602 | validation: 0.7218593114441032]
	TIME [epoch: 1.9 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7352164311681737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7352164311681737 | validation: 0.7833184521494245]
	TIME [epoch: 1.9 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.736192046240349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.736192046240349 | validation: 0.7274772051902989]
	TIME [epoch: 1.9 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6996238453001695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6996238453001695 | validation: 0.7064380204645041]
	TIME [epoch: 1.91 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6718803518689509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6718803518689509 | validation: 0.6742244097865359]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_192.pth
	Model improved!!!
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6626400483600753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6626400483600753 | validation: 0.7070969300630984]
	TIME [epoch: 1.89 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6595186227471804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6595186227471804 | validation: 0.6759473206089084]
	TIME [epoch: 1.9 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6633116396519421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6633116396519421 | validation: 0.861054602486761]
	TIME [epoch: 1.9 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.751260428395183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.751260428395183 | validation: 0.8771394694303197]
	TIME [epoch: 1.89 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9372092107014148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9372092107014148 | validation: 0.729228131014768]
	TIME [epoch: 1.9 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.687294229782128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.687294229782128 | validation: 0.7235106140797494]
	TIME [epoch: 1.89 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6667989359593821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6667989359593821 | validation: 0.6993709388236731]
	TIME [epoch: 1.9 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7063079186646973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7063079186646973 | validation: 0.7992595255539151]
	TIME [epoch: 1.89 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.711227376023186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.711227376023186 | validation: 0.7068177314595893]
	TIME [epoch: 26.6 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.699540370125048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.699540370125048 | validation: 0.74496404725955]
	TIME [epoch: 3.74 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6824762516866499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6824762516866499 | validation: 0.6986736922021678]
	TIME [epoch: 3.75 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6809746428778489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6809746428778489 | validation: 0.6629960586716109]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_204.pth
	Model improved!!!
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6782983540298816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6782983540298816 | validation: 0.8036614421227495]
	TIME [epoch: 3.74 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7346267246442241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7346267246442241 | validation: 0.7506080246750573]
	TIME [epoch: 3.75 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7960321097791309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7960321097791309 | validation: 0.7517796451692133]
	TIME [epoch: 3.74 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6797369228775852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6797369228775852 | validation: 0.6495946509511479]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6386487039556367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6386487039556367 | validation: 0.6621735607168729]
	TIME [epoch: 3.74 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6364229417117163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6364229417117163 | validation: 0.6726364682525432]
	TIME [epoch: 3.74 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6351414091278779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6351414091278779 | validation: 0.6802006681542653]
	TIME [epoch: 3.74 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6465788688052231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6465788688052231 | validation: 0.715471038740616]
	TIME [epoch: 3.74 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7038107655950404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7038107655950404 | validation: 0.8449489122683602]
	TIME [epoch: 3.74 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7339494142432327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7339494142432327 | validation: 0.8148465008595756]
	TIME [epoch: 3.75 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8124515140355223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8124515140355223 | validation: 1.1213939142809732]
	TIME [epoch: 3.75 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9248017532431103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9248017532431103 | validation: 0.8225075379856381]
	TIME [epoch: 3.76 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8845769764279036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8845769764279036 | validation: 0.6645052231513034]
	TIME [epoch: 3.76 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6618525650892793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6618525650892793 | validation: 0.767784957278691]
	TIME [epoch: 3.75 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.713967032568547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.713967032568547 | validation: 0.6617442110330238]
	TIME [epoch: 3.76 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.648250521507276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.648250521507276 | validation: 0.6524105858672593]
	TIME [epoch: 3.75 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6346808442045636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6346808442045636 | validation: 0.6699374710516315]
	TIME [epoch: 3.74 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6402589384403052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6402589384403052 | validation: 0.643045290662029]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_222.pth
	Model improved!!!
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6400089993802283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6400089993802283 | validation: 0.6789296535005355]
	TIME [epoch: 3.73 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6298901495047983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6298901495047983 | validation: 0.6343230030480831]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_224.pth
	Model improved!!!
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6331197348282683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6331197348282683 | validation: 0.7786700790890387]
	TIME [epoch: 3.74 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6762825938333825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6762825938333825 | validation: 0.7233408542189591]
	TIME [epoch: 3.74 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7248791904353175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7248791904353175 | validation: 0.776146084310717]
	TIME [epoch: 3.75 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6869771442032514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6869771442032514 | validation: 0.6485659741786867]
	TIME [epoch: 3.77 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6616559880396815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6616559880396815 | validation: 0.6953541878357449]
	TIME [epoch: 3.76 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6419516212997224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6419516212997224 | validation: 0.6334742743142296]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_230.pth
	Model improved!!!
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6586065492247388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6586065492247388 | validation: 0.7370142957955684]
	TIME [epoch: 3.73 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6769831084520718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6769831084520718 | validation: 0.6637293793789791]
	TIME [epoch: 3.75 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7096666951699314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7096666951699314 | validation: 0.7451995232971164]
	TIME [epoch: 3.74 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6652811146302272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6652811146302272 | validation: 0.6396655869627235]
	TIME [epoch: 3.73 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6692877306050237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6692877306050237 | validation: 0.737409808402229]
	TIME [epoch: 3.73 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6662457481838872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6662457481838872 | validation: 0.6376419546851908]
	TIME [epoch: 3.73 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6515253194687952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6515253194687952 | validation: 0.7300309760786541]
	TIME [epoch: 3.74 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6382807415499098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6382807415499098 | validation: 0.6216968915021897]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_238.pth
	Model improved!!!
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6266564531989277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6266564531989277 | validation: 0.7556920821028253]
	TIME [epoch: 3.73 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6374434870966468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6374434870966468 | validation: 0.7229532750077795]
	TIME [epoch: 3.74 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7905905725404233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7905905725404233 | validation: 0.7501670080181054]
	TIME [epoch: 3.74 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7198152650123185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7198152650123185 | validation: 0.596614269871595]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6139639075276391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6139639075276391 | validation: 0.5969360757052763]
	TIME [epoch: 3.73 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5869428763936313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5869428763936313 | validation: 0.5711702158947878]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_244.pth
	Model improved!!!
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5821162821005151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5821162821005151 | validation: 0.6341119028447415]
	TIME [epoch: 3.74 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5716467504492497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5716467504492497 | validation: 0.6057622757626195]
	TIME [epoch: 3.75 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.619985597579573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.619985597579573 | validation: 1.9687366760756895]
	TIME [epoch: 3.76 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.64911239145306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.64911239145306 | validation: 1.2206773154067672]
	TIME [epoch: 3.77 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2712044966046177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2712044966046177 | validation: 0.6891908415124629]
	TIME [epoch: 3.74 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7824631762651081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7824631762651081 | validation: 0.7401945968017016]
	TIME [epoch: 3.77 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7299561134587907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7299561134587907 | validation: 0.736438631028884]
	TIME [epoch: 3.77 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7097853846542317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7097853846542317 | validation: 0.6149548522026761]
	TIME [epoch: 3.78 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6351057637177268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6351057637177268 | validation: 0.617596226352653]
	TIME [epoch: 3.77 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6279115814682282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6279115814682282 | validation: 0.6444975291458998]
	TIME [epoch: 3.77 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6236013849503597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6236013849503597 | validation: 0.6052969761593996]
	TIME [epoch: 3.75 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6161389006382177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6161389006382177 | validation: 0.6128114974413994]
	TIME [epoch: 3.77 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6029875202677074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6029875202677074 | validation: 0.6146009931318122]
	TIME [epoch: 3.76 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5979617661089411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5979617661089411 | validation: 0.5958553025571921]
	TIME [epoch: 3.76 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5964824359548938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5964824359548938 | validation: 0.6645171508388403]
	TIME [epoch: 3.74 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6006460582634756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6006460582634756 | validation: 0.6093600693170554]
	TIME [epoch: 3.77 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.617232579954583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.617232579954583 | validation: 0.7637559531982993]
	TIME [epoch: 3.75 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.65031358712125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.65031358712125 | validation: 0.6705724981705821]
	TIME [epoch: 3.74 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6889430876258195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6889430876258195 | validation: 0.6945475329453634]
	TIME [epoch: 3.75 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.613641820323474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.613641820323474 | validation: 0.5830197922030496]
	TIME [epoch: 4.14 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5948181794418888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5948181794418888 | validation: 0.6774008242481756]
	TIME [epoch: 3.76 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5954025209844989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5954025209844989 | validation: 0.6167198980442775]
	TIME [epoch: 3.76 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6552822708325537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6552822708325537 | validation: 0.781610243605546]
	TIME [epoch: 3.74 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6597738516764106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6597738516764106 | validation: 0.617134172262134]
	TIME [epoch: 3.75 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.688052185190564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.688052185190564 | validation: 0.6426160378532395]
	TIME [epoch: 3.76 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5899212431288602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5899212431288602 | validation: 0.5394637205010524]
	TIME [epoch: 3.77 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_270.pth
	Model improved!!!
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.565557614385357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.565557614385357 | validation: 0.6132117585671495]
	TIME [epoch: 3.74 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5613033973318923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5613033973318923 | validation: 0.5536544028992764]
	TIME [epoch: 3.76 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.599149264638488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.599149264638488 | validation: 0.8978781582542953]
	TIME [epoch: 3.76 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7379769456146668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7379769456146668 | validation: 0.8270457524991657]
	TIME [epoch: 3.74 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9278516615392567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9278516615392567 | validation: 0.5692271921383387]
	TIME [epoch: 3.77 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6040020348594507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6040020348594507 | validation: 0.7584799479272988]
	TIME [epoch: 3.75 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7098628200386109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7098628200386109 | validation: 0.5642763538361291]
	TIME [epoch: 3.75 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.591508504899776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.591508504899776 | validation: 0.5652225924465902]
	TIME [epoch: 3.76 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5896476034013851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5896476034013851 | validation: 0.6176540686891175]
	TIME [epoch: 3.76 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.570846926725056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.570846926725056 | validation: 0.5277150795510862]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_280.pth
	Model improved!!!
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.564407445840213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.564407445840213 | validation: 0.644719670725654]
	TIME [epoch: 3.74 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5898440798901513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5898440798901513 | validation: 0.612522524722622]
	TIME [epoch: 3.76 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7014445582282539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7014445582282539 | validation: 0.7571090424174846]
	TIME [epoch: 3.74 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6498008072873112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6498008072873112 | validation: 0.5904767589994248]
	TIME [epoch: 3.75 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6830824587070999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6830824587070999 | validation: 0.5628439168463878]
	TIME [epoch: 3.75 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5659845593883802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5659845593883802 | validation: 0.5718223752457963]
	TIME [epoch: 3.76 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5480639389357956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5480639389357956 | validation: 0.5280196322690329]
	TIME [epoch: 3.76 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5506623858684095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5506623858684095 | validation: 0.8673346591825535]
	TIME [epoch: 3.76 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6973855822864299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6973855822864299 | validation: 0.8503413348125423]
	TIME [epoch: 3.74 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9739459192802115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9739459192802115 | validation: 0.5821863790690051]
	TIME [epoch: 3.74 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.604666780659113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.604666780659113 | validation: 0.8793111989385629]
	TIME [epoch: 3.74 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7809310203883142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7809310203883142 | validation: 0.5477325460685208]
	TIME [epoch: 3.74 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5725285728827103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5725285728827103 | validation: 0.5549575252355684]
	TIME [epoch: 3.74 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6084717713915541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6084717713915541 | validation: 0.6054303386131134]
	TIME [epoch: 3.74 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5716514596605325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5716514596605325 | validation: 0.5240183217725262]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_295.pth
	Model improved!!!
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5197069391993856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5197069391993856 | validation: 0.5472023571693554]
	TIME [epoch: 3.74 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5033076469308414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5033076469308414 | validation: 0.5292177229328346]
	TIME [epoch: 3.74 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5005209081809581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5005209081809581 | validation: 0.6723833381944258]
	TIME [epoch: 3.77 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5879438125049203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5879438125049203 | validation: 0.9208182056713877]
	TIME [epoch: 3.77 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.027718255944209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.027718255944209 | validation: 0.5474115468499887]
	TIME [epoch: 3.75 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5774435530361359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5774435530361359 | validation: 0.7724581901816138]
	TIME [epoch: 3.72 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7078140491437835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7078140491437835 | validation: 0.5390690859914417]
	TIME [epoch: 3.72 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5871128818694803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5871128818694803 | validation: 0.5471528831520673]
	TIME [epoch: 3.72 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5644734092995904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5644734092995904 | validation: 0.5447969773974973]
	TIME [epoch: 3.74 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5331095335451822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5331095335451822 | validation: 0.48835738300663767]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.506222890309995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.506222890309995 | validation: 0.7834440847358155]
	TIME [epoch: 3.72 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6203126182636975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6203126182636975 | validation: 0.9465081905858043]
	TIME [epoch: 3.73 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.926000583577012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.926000583577012 | validation: 0.48026472177961355]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_308.pth
	Model improved!!!
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5110074081067755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5110074081067755 | validation: 0.7939030468515611]
	TIME [epoch: 3.73 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.694526505950289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.694526505950289 | validation: 0.5262015435568108]
	TIME [epoch: 3.74 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.585099992997958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.585099992997958 | validation: 0.4849204065392459]
	TIME [epoch: 3.74 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5018631698420631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5018631698420631 | validation: 0.5477006067136481]
	TIME [epoch: 3.75 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4920345241825907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4920345241825907 | validation: 0.5158106248706965]
	TIME [epoch: 3.73 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5689438932877986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5689438932877986 | validation: 1.0957953756912013]
	TIME [epoch: 3.74 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9245740978084023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9245740978084023 | validation: 0.8954289529335666]
	TIME [epoch: 3.73 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9865310737633664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9865310737633664 | validation: 0.5545751789139673]
	TIME [epoch: 3.75 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6446337115196766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6446337115196766 | validation: 0.7287800117905783]
	TIME [epoch: 3.72 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7316527725011637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7316527725011637 | validation: 0.6042828805309512]
	TIME [epoch: 3.74 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6253202202719867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6253202202719867 | validation: 0.5307802306031567]
	TIME [epoch: 3.72 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6182359354604379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6182359354604379 | validation: 0.5187524075701981]
	TIME [epoch: 3.73 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.552346850441965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.552346850441965 | validation: 0.5325041333979796]
	TIME [epoch: 3.72 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5259221023144576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5259221023144576 | validation: 0.4899973590749269]
	TIME [epoch: 3.75 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5160811216441931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5160811216441931 | validation: 0.6027044270684385]
	TIME [epoch: 3.72 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5220247145611356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5220247145611356 | validation: 0.5611443318676136]
	TIME [epoch: 3.73 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5542028919059367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5542028919059367 | validation: 0.7734089001055016]
	TIME [epoch: 3.72 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5898852654093715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5898852654093715 | validation: 0.5804557197862563]
	TIME [epoch: 3.72 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6508808098982338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6508808098982338 | validation: 0.5206735672408707]
	TIME [epoch: 3.73 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47718975089645327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47718975089645327 | validation: 0.46185047586774514]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_328.pth
	Model improved!!!
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4528071118103887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4528071118103887 | validation: 0.45426794000252885]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_329.pth
	Model improved!!!
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43893392432633815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43893392432633815 | validation: 0.5111939109098854]
	TIME [epoch: 3.74 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45641441273975975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45641441273975975 | validation: 0.5317336935889939]
	TIME [epoch: 3.74 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5715573210557353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5715573210557353 | validation: 0.8371346670172732]
	TIME [epoch: 3.75 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6922022496001821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6922022496001821 | validation: 0.6422431551862475]
	TIME [epoch: 3.74 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.727025680626742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.727025680626742 | validation: 0.4578896083568933]
	TIME [epoch: 3.74 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4877295940774392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4877295940774392 | validation: 0.5648530688580817]
	TIME [epoch: 3.75 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.537082206375806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.537082206375806 | validation: 0.45329274895670263]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_336.pth
	Model improved!!!
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49241480776595653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49241480776595653 | validation: 0.5279085708929454]
	TIME [epoch: 3.72 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46648985077111593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46648985077111593 | validation: 0.4405970823747367]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_338.pth
	Model improved!!!
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4565041179499383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4565041179499383 | validation: 0.5686074243273732]
	TIME [epoch: 3.75 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4952022148374694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4952022148374694 | validation: 0.5555533611289251]
	TIME [epoch: 3.75 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6269857281156604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6269857281156604 | validation: 0.46984053377897683]
	TIME [epoch: 3.76 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4466888993324723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4466888993324723 | validation: 0.40710073841392325]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_342.pth
	Model improved!!!
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4125828065825624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4125828065825624 | validation: 0.40894571250209116]
	TIME [epoch: 3.75 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4097013634135551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4097013634135551 | validation: 0.43947851026763785]
	TIME [epoch: 3.74 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4353144683350383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4353144683350383 | validation: 0.4767050530870461]
	TIME [epoch: 3.74 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5456330713782633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5456330713782633 | validation: 0.6409045280661808]
	TIME [epoch: 3.75 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5405705163537523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5405705163537523 | validation: 0.730256437813446]
	TIME [epoch: 3.75 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6474689480710498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6474689480710498 | validation: 0.5916242017918718]
	TIME [epoch: 3.77 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5290126803429718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5290126803429718 | validation: 0.4041225194737563]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_349.pth
	Model improved!!!
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41229104366439845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41229104366439845 | validation: 0.4250938025942141]
	TIME [epoch: 3.74 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43912754599028553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43912754599028553 | validation: 0.8952548938610164]
	TIME [epoch: 3.74 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7152986271942626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7152986271942626 | validation: 0.9275208029065054]
	TIME [epoch: 3.76 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8381461737057754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8381461737057754 | validation: 0.4839322368624197]
	TIME [epoch: 3.76 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5263979389186053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5263979389186053 | validation: 0.6895232603262645]
	TIME [epoch: 3.75 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6826728443503669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6826728443503669 | validation: 0.4410828578770333]
	TIME [epoch: 3.74 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4673912798550745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4673912798550745 | validation: 0.4391377454137224]
	TIME [epoch: 3.76 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48708457654633897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48708457654633897 | validation: 0.4613490900191332]
	TIME [epoch: 3.74 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43926108615475995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43926108615475995 | validation: 0.39896504677349776]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_358.pth
	Model improved!!!
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41918915883694224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41918915883694224 | validation: 0.4449855846387788]
	TIME [epoch: 3.75 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41521376947325855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41521376947325855 | validation: 0.4006945442882069]
	TIME [epoch: 3.76 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4343086480380737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4343086480380737 | validation: 0.5035296141981851]
	TIME [epoch: 3.74 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.464065378900833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.464065378900833 | validation: 0.505797147984146]
	TIME [epoch: 3.74 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5135974653730443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5135974653730443 | validation: 0.41088941224225844]
	TIME [epoch: 3.74 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41215232059817153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41215232059817153 | validation: 0.36920576093194196]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_364.pth
	Model improved!!!
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37601894407993725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37601894407993725 | validation: 0.4291391774055353]
	TIME [epoch: 3.74 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39292068942130626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39292068942130626 | validation: 0.4315190463351506]
	TIME [epoch: 3.74 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.419854739232951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.419854739232951 | validation: 0.661238831364978]
	TIME [epoch: 3.74 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4839326128387824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4839326128387824 | validation: 0.6677470651736759]
	TIME [epoch: 3.74 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6137083962313402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6137083962313402 | validation: 0.40558593230382844]
	TIME [epoch: 3.76 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43202326523245504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43202326523245504 | validation: 0.37125072435636464]
	TIME [epoch: 3.75 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3521216284145718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3521216284145718 | validation: 0.37671870347695824]
	TIME [epoch: 3.77 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38107104961980626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38107104961980626 | validation: 0.5304083541827409]
	TIME [epoch: 3.76 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45989240555134386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45989240555134386 | validation: 0.886406083164631]
	TIME [epoch: 3.76 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7657430761692651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7657430761692651 | validation: 0.4605315232816737]
	TIME [epoch: 3.74 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4642568025864206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4642568025864206 | validation: 0.47370693955842713]
	TIME [epoch: 3.76 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44351601201794927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44351601201794927 | validation: 0.48907273798955253]
	TIME [epoch: 3.76 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4460921182889662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4460921182889662 | validation: 0.7475088392424492]
	TIME [epoch: 3.74 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5400670624425197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5400670624425197 | validation: 0.6001653923224111]
	TIME [epoch: 3.76 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5679175672561076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5679175672561076 | validation: 0.38222860731885544]
	TIME [epoch: 3.75 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3831432573069594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3831432573069594 | validation: 0.38479946652537816]
	TIME [epoch: 3.75 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3613289451320908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3613289451320908 | validation: 0.3966099525178672]
	TIME [epoch: 3.74 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3952466586390531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3952466586390531 | validation: 0.6115739799649198]
	TIME [epoch: 3.75 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5020587492433773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5020587492433773 | validation: 0.6285988193261871]
	TIME [epoch: 3.75 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.587979674358992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.587979674358992 | validation: 0.37189900289489275]
	TIME [epoch: 3.75 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3742010538453189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3742010538453189 | validation: 0.3739799150983282]
	TIME [epoch: 3.74 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37217223888602535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37217223888602535 | validation: 0.4330071583366042]
	TIME [epoch: 3.74 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44251152583624204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44251152583624204 | validation: 0.41493958280989196]
	TIME [epoch: 3.74 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4847622220309639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4847622220309639 | validation: 0.3821993451575981]
	TIME [epoch: 3.74 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37320400702204454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37320400702204454 | validation: 0.34833893219828255]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_389.pth
	Model improved!!!
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3287902041386774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3287902041386774 | validation: 0.34230846223587363]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_390.pth
	Model improved!!!
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3227134207824623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3227134207824623 | validation: 0.3377947531970779]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_391.pth
	Model improved!!!
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3228218639566905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3228218639566905 | validation: 0.6074158239598396]
	TIME [epoch: 3.74 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43038292993958027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43038292993958027 | validation: 1.213562843884629]
	TIME [epoch: 3.74 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.886600842892639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.886600842892639 | validation: 0.5492435921656188]
	TIME [epoch: 3.76 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4933541665362817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4933541665362817 | validation: 0.4100927520253082]
	TIME [epoch: 3.77 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4414275554238084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4414275554238084 | validation: 0.5478229034033149]
	TIME [epoch: 3.75 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46694306322979245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46694306322979245 | validation: 0.8075105124619434]
	TIME [epoch: 3.74 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5359542225500611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5359542225500611 | validation: 0.4652964434348338]
	TIME [epoch: 3.74 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4382533570209058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4382533570209058 | validation: 0.4182830373332642]
	TIME [epoch: 3.73 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4222455900209509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4222455900209509 | validation: 0.38568945045227526]
	TIME [epoch: 3.76 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3486856367444544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3486856367444544 | validation: 0.3639170910712741]
	TIME [epoch: 3.73 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3413471852413912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3413471852413912 | validation: 0.5242200320048579]
	TIME [epoch: 3.72 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37481129336787505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37481129336787505 | validation: 0.5521568252803267]
	TIME [epoch: 3.73 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49309982691602855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49309982691602855 | validation: 0.4228236424566969]
	TIME [epoch: 3.73 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3790802054441102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3790802054441102 | validation: 0.3489433594572041]
	TIME [epoch: 3.73 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32719527965985323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32719527965985323 | validation: 0.3944768959906399]
	TIME [epoch: 3.75 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3392936519506256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3392936519506256 | validation: 0.5378312046844477]
	TIME [epoch: 3.76 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49800670422867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49800670422867 | validation: 0.5707192031216373]
	TIME [epoch: 3.72 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4216217344483137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4216217344483137 | validation: 0.4596679458814945]
	TIME [epoch: 3.72 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3912242796966672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3912242796966672 | validation: 0.5859453653570668]
	TIME [epoch: 3.73 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38852784891235387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38852784891235387 | validation: 0.34916509502889764]
	TIME [epoch: 3.71 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3280303904739465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3280303904739465 | validation: 0.3962101784880727]
	TIME [epoch: 3.73 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30689214125042813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30689214125042813 | validation: 0.33083650388053915]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_413.pth
	Model improved!!!
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31844855845823106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31844855845823106 | validation: 0.40714715266181073]
	TIME [epoch: 3.75 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36548604893467146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36548604893467146 | validation: 0.41022306962720284]
	TIME [epoch: 3.76 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46604765503782064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46604765503782064 | validation: 1.048957616266559]
	TIME [epoch: 3.75 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7230063889079571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7230063889079571 | validation: 0.7422823367999208]
	TIME [epoch: 3.76 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6147527344232014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6147527344232014 | validation: 0.40412887182872126]
	TIME [epoch: 3.77 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40538268701420294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40538268701420294 | validation: 0.42812372298231555]
	TIME [epoch: 3.77 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3589066221092972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3589066221092972 | validation: 0.6483905735572781]
	TIME [epoch: 3.76 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41881373784709597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41881373784709597 | validation: 0.3862792890570675]
	TIME [epoch: 3.75 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3621421559122588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3621421559122588 | validation: 0.37866797190010065]
	TIME [epoch: 3.76 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3991322378178252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3991322378178252 | validation: 0.38670919355745703]
	TIME [epoch: 3.76 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33908572424608335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33908572424608335 | validation: 0.37568337094267595]
	TIME [epoch: 3.76 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2992103936659527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2992103936659527 | validation: 0.3402261989937794]
	TIME [epoch: 3.75 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2931342717824187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2931342717824187 | validation: 0.5127687722312061]
	TIME [epoch: 3.75 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.365832696291715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.365832696291715 | validation: 0.7892692290604929]
	TIME [epoch: 3.74 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6234108452291424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6234108452291424 | validation: 0.4894612727356396]
	TIME [epoch: 3.75 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4151611725429008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4151611725429008 | validation: 0.38999300154871064]
	TIME [epoch: 3.76 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42959827976888565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42959827976888565 | validation: 0.38738930802999116]
	TIME [epoch: 3.75 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2883295553026122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2883295553026122 | validation: 0.3829678894634476]
	TIME [epoch: 3.77 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2882377983207152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2882377983207152 | validation: 0.3688489766373187]
	TIME [epoch: 3.76 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3208778036991848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3208778036991848 | validation: 0.506836355520545]
	TIME [epoch: 3.76 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4191046272404752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4191046272404752 | validation: 0.6749011770911841]
	TIME [epoch: 3.76 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5037817672544732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5037817672544732 | validation: 0.593403666898855]
	TIME [epoch: 3.76 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4739119555951936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4739119555951936 | validation: 0.36320568590440605]
	TIME [epoch: 3.75 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4090487354661477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4090487354661477 | validation: 0.34547497888419465]
	TIME [epoch: 3.76 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29440892556029363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29440892556029363 | validation: 0.5790456331396492]
	TIME [epoch: 3.77 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3720670499597191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3720670499597191 | validation: 0.44120152877264956]
	TIME [epoch: 3.77 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35593289553888396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35593289553888396 | validation: 0.5116788624779272]
	TIME [epoch: 3.76 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3444134510634805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3444134510634805 | validation: 0.4079976096261428]
	TIME [epoch: 3.76 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3675899411721575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3675899411721575 | validation: 0.33396230370570584]
	TIME [epoch: 3.77 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34819618582017897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34819618582017897 | validation: 0.349462154871173]
	TIME [epoch: 3.76 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28548839317032587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28548839317032587 | validation: 0.3700930486634394]
	TIME [epoch: 3.76 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31363257663313127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31363257663313127 | validation: 0.520228833927221]
	TIME [epoch: 3.74 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42842687922446937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42842687922446937 | validation: 0.5095224683452798]
	TIME [epoch: 3.76 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.348537809016727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.348537809016727 | validation: 0.31101544274167076]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_447.pth
	Model improved!!!
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27927984928821054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27927984928821054 | validation: 0.3687388679833361]
	TIME [epoch: 3.74 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.286372882171313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.286372882171313 | validation: 0.3725945651450977]
	TIME [epoch: 3.75 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3374080622769937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3374080622769937 | validation: 0.6283194988136251]
	TIME [epoch: 3.75 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.495023815590022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.495023815590022 | validation: 0.6904223131863976]
	TIME [epoch: 3.76 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5218371053539458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5218371053539458 | validation: 0.34428721695055214]
	TIME [epoch: 3.76 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37317367055183964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37317367055183964 | validation: 0.4087095245675051]
	TIME [epoch: 3.75 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34534792444393586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34534792444393586 | validation: 0.7213799150660509]
	TIME [epoch: 3.75 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44551149352782515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44551149352782515 | validation: 0.38418776196814747]
	TIME [epoch: 3.76 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33565904544316255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33565904544316255 | validation: 0.35006836608095837]
	TIME [epoch: 3.76 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3387859443420988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3387859443420988 | validation: 0.38156541224250706]
	TIME [epoch: 3.76 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2751743993188948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2751743993188948 | validation: 0.3248046433834005]
	TIME [epoch: 3.76 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2602313020641713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2602313020641713 | validation: 0.3629514684190952]
	TIME [epoch: 3.74 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2568682250246397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2568682250246397 | validation: 0.31578186060227686]
	TIME [epoch: 3.76 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25413487291114717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25413487291114717 | validation: 0.41546515755808805]
	TIME [epoch: 3.76 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2930701778918331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2930701778918331 | validation: 0.40150146331766995]
	TIME [epoch: 3.77 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3687495028229408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3687495028229408 | validation: 0.3420518793807026]
	TIME [epoch: 3.76 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35129010142779565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35129010142779565 | validation: 0.3517170534552843]
	TIME [epoch: 3.74 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30138121126478457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30138121126478457 | validation: 0.3267506194889807]
	TIME [epoch: 3.75 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25675774573492205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25675774573492205 | validation: 0.33488728733321765]
	TIME [epoch: 3.75 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2761301509567548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2761301509567548 | validation: 0.4659608446133572]
	TIME [epoch: 3.76 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34841297551243144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34841297551243144 | validation: 0.5436828823183053]
	TIME [epoch: 3.75 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45184160784055494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45184160784055494 | validation: 0.4213013302213977]
	TIME [epoch: 3.74 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32951513889312395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32951513889312395 | validation: 0.3534086803550386]
	TIME [epoch: 3.74 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39815098648336217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39815098648336217 | validation: 0.5554250543653944]
	TIME [epoch: 3.74 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36108921717785386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36108921717785386 | validation: 0.3850004587256033]
	TIME [epoch: 3.76 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3139796881038121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3139796881038121 | validation: 0.33311769692247906]
	TIME [epoch: 3.75 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3261620017097478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3261620017097478 | validation: 0.33247232920092573]
	TIME [epoch: 3.76 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27960293333818265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27960293333818265 | validation: 0.2693561771324127]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_475.pth
	Model improved!!!
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21421540770794775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21421540770794775 | validation: 0.30487901938926143]
	TIME [epoch: 3.75 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21659657040019473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21659657040019473 | validation: 0.29131250108122936]
	TIME [epoch: 3.75 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23619967756276872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23619967756276872 | validation: 0.5356542541443672]
	TIME [epoch: 3.76 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34409418550602333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34409418550602333 | validation: 0.35131393916634235]
	TIME [epoch: 3.77 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28968367845198034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28968367845198034 | validation: 0.4211896942445602]
	TIME [epoch: 3.74 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30048074918280804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30048074918280804 | validation: 0.4192834050700676]
	TIME [epoch: 3.75 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4074259050988918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4074259050988918 | validation: 0.321934325417163]
	TIME [epoch: 3.75 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33547672029002784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33547672029002784 | validation: 0.37099433259064635]
	TIME [epoch: 3.76 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25335857984141896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25335857984141896 | validation: 0.34414636253217096]
	TIME [epoch: 3.75 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27414064881830486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27414064881830486 | validation: 0.3850786286284855]
	TIME [epoch: 3.76 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4112062634751203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4112062634751203 | validation: 0.45303706552793876]
	TIME [epoch: 3.76 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38951205683058476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38951205683058476 | validation: 0.38301988648009944]
	TIME [epoch: 3.74 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3081865060978216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3081865060978216 | validation: 0.29573305442521897]
	TIME [epoch: 3.74 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21925299418201982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21925299418201982 | validation: 0.2858028698531581]
	TIME [epoch: 3.75 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2193903379225008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2193903379225008 | validation: 0.3050515228030681]
	TIME [epoch: 3.74 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21267725488928022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21267725488928022 | validation: 0.3081131385886232]
	TIME [epoch: 3.76 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2536482384203121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2536482384203121 | validation: 0.6672554759573776]
	TIME [epoch: 3.75 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4422219910499508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4422219910499508 | validation: 0.2910918549286649]
	TIME [epoch: 3.74 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26622236176968955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26622236176968955 | validation: 0.287443119664688]
	TIME [epoch: 3.75 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2238110412312617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2238110412312617 | validation: 0.26249899289923]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_495.pth
	Model improved!!!
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31537335621411894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31537335621411894 | validation: 0.4710008312817127]
	TIME [epoch: 3.73 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39391311947764446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39391311947764446 | validation: 0.3673370507963879]
	TIME [epoch: 3.73 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28280986046796086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28280986046796086 | validation: 0.28025512522375956]
	TIME [epoch: 3.73 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3180401373781054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3180401373781054 | validation: 0.3541952756255451]
	TIME [epoch: 3.74 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30754532293153974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30754532293153974 | validation: 0.40331229251532275]
	TIME [epoch: 3.73 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3267877917394575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3267877917394575 | validation: 0.4153449094267576]
	TIME [epoch: 31.1 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3341453337625581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3341453337625581 | validation: 0.2963238909638281]
	TIME [epoch: 8.16 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22311856809009847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22311856809009847 | validation: 0.2836652711122904]
	TIME [epoch: 8.16 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30684317538863765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30684317538863765 | validation: 0.286434059451188]
	TIME [epoch: 8.17 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24177354310003146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24177354310003146 | validation: 0.26741287902623617]
	TIME [epoch: 8.15 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24533987990423975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24533987990423975 | validation: 0.3284337848159118]
	TIME [epoch: 8.15 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2688350248838161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2688350248838161 | validation: 0.2979182950209062]
	TIME [epoch: 8.16 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2150870923574363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2150870923574363 | validation: 0.318571173876937]
	TIME [epoch: 8.16 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.243306553594796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.243306553594796 | validation: 0.6374609069914919]
	TIME [epoch: 8.17 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4304812847394834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4304812847394834 | validation: 0.31293372527770674]
	TIME [epoch: 8.15 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.247248052175338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.247248052175338 | validation: 0.33139464753084563]
	TIME [epoch: 8.14 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23045848155487764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23045848155487764 | validation: 0.23283423343040743]
	TIME [epoch: 8.16 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_512.pth
	Model improved!!!
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17953696667480415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17953696667480415 | validation: 0.20295611580425377]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_513.pth
	Model improved!!!
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18065897712507273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18065897712507273 | validation: 0.31619374114851395]
	TIME [epoch: 8.11 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29633748172904983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29633748172904983 | validation: 0.28038468254288135]
	TIME [epoch: 8.15 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3675701148560059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3675701148560059 | validation: 0.29270391413534796]
	TIME [epoch: 8.13 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23164152788167006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23164152788167006 | validation: 0.68501275855329]
	TIME [epoch: 8.15 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4715075382862453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4715075382862453 | validation: 0.25626996237904637]
	TIME [epoch: 8.11 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2634462920711986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2634462920711986 | validation: 0.5145061626967765]
	TIME [epoch: 8.12 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42180127249817245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42180127249817245 | validation: 0.4174657157861349]
	TIME [epoch: 8.09 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33351664635378137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33351664635378137 | validation: 0.3122244391415314]
	TIME [epoch: 8.14 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3701110085506835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3701110085506835 | validation: 0.48085086492040185]
	TIME [epoch: 8.11 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.388741275375977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.388741275375977 | validation: 0.35120538575720417]
	TIME [epoch: 8.13 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30637066934164425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30637066934164425 | validation: 0.31166916030200875]
	TIME [epoch: 8.12 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3019995332504854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3019995332504854 | validation: 0.34273000762933664]
	TIME [epoch: 8.1 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25781043669752857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25781043669752857 | validation: 0.3897248128562037]
	TIME [epoch: 8.11 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28883877277407716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28883877277407716 | validation: 0.21129372742542596]
	TIME [epoch: 8.1 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18878855432553676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18878855432553676 | validation: 0.3044256548757917]
	TIME [epoch: 8.1 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24239423890183634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24239423890183634 | validation: 0.24202430976193778]
	TIME [epoch: 8.12 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17523295833742913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17523295833742913 | validation: 0.20659939052313267]
	TIME [epoch: 8.12 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15549481001023394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15549481001023394 | validation: 0.1929519510142702]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_531.pth
	Model improved!!!
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17014266763546532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17014266763546532 | validation: 0.2480582710887751]
	TIME [epoch: 8.11 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19270709167308872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19270709167308872 | validation: 0.22144838529166144]
	TIME [epoch: 8.12 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2618146986543037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2618146986543037 | validation: 0.30500861149213726]
	TIME [epoch: 8.15 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2477903481248947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2477903481248947 | validation: 0.28321889387072935]
	TIME [epoch: 8.13 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22645216248809227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22645216248809227 | validation: 0.3292121688494109]
	TIME [epoch: 8.1 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25081700452342526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25081700452342526 | validation: 0.29759939315442924]
	TIME [epoch: 8.11 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21880071053984565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21880071053984565 | validation: 0.2636929227833813]
	TIME [epoch: 8.11 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21479640632655347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21479640632655347 | validation: 0.31352752180210103]
	TIME [epoch: 8.13 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2147959974898134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2147959974898134 | validation: 0.2074487924853745]
	TIME [epoch: 8.14 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16957888940359314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16957888940359314 | validation: 0.19993251727459493]
	TIME [epoch: 8.12 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15455510388605218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15455510388605218 | validation: 0.24709948615997668]
	TIME [epoch: 8.12 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19108211131235023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19108211131235023 | validation: 0.4352034052210993]
	TIME [epoch: 8.12 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.355693313522103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.355693313522103 | validation: 0.4594202443892581]
	TIME [epoch: 8.09 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43805611145483214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43805611145483214 | validation: 0.4697027515804857]
	TIME [epoch: 8.11 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3641959004694962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3641959004694962 | validation: 0.3151994827236321]
	TIME [epoch: 8.08 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4462829484209062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4462829484209062 | validation: 0.37174804013052515]
	TIME [epoch: 8.1 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31905188749670754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31905188749670754 | validation: 0.3691064072056188]
	TIME [epoch: 8.08 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2677392566998956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2677392566998956 | validation: 0.23667375193036422]
	TIME [epoch: 8.1 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3106944725390959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3106944725390959 | validation: 0.28393037679820765]
	TIME [epoch: 8.11 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2188196271635344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2188196271635344 | validation: 0.33586545966399917]
	TIME [epoch: 8.11 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2449257866141683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2449257866141683 | validation: 0.18886287582398176]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_552.pth
	Model improved!!!
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17684868086162062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17684868086162062 | validation: 0.23835446110639374]
	TIME [epoch: 8.14 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17970915883955133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17970915883955133 | validation: 0.27087003577821056]
	TIME [epoch: 8.13 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21366096784696914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21366096784696914 | validation: 0.2963451119391984]
	TIME [epoch: 8.13 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22993560622054748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22993560622054748 | validation: 0.19086105715299456]
	TIME [epoch: 8.14 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1422833897865689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1422833897865689 | validation: 0.1594276975425403]
	TIME [epoch: 8.14 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_557.pth
	Model improved!!!
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.136034974602828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.136034974602828 | validation: 0.23930900946511546]
	TIME [epoch: 8.11 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17835383798947457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17835383798947457 | validation: 0.25315958581749004]
	TIME [epoch: 8.07 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22873372319641583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22873372319641583 | validation: 0.48059893603007064]
	TIME [epoch: 8.11 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44495912065949955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44495912065949955 | validation: 0.3697703032002152]
	TIME [epoch: 8.15 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3006095552604163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3006095552604163 | validation: 0.3590286021911645]
	TIME [epoch: 8.15 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24244165827925077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24244165827925077 | validation: 0.22615232895396026]
	TIME [epoch: 8.14 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18927611161062177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18927611161062177 | validation: 0.27856037569667325]
	TIME [epoch: 8.13 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1927748062333118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1927748062333118 | validation: 0.23553297706011903]
	TIME [epoch: 8.12 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21443589825733156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21443589825733156 | validation: 0.2555623918854878]
	TIME [epoch: 8.13 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19285274212818634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19285274212818634 | validation: 0.17721704345949468]
	TIME [epoch: 8.14 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19059734595228073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19059734595228073 | validation: 0.3503134317632096]
	TIME [epoch: 8.14 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2865071877073719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2865071877073719 | validation: 0.19094341809671023]
	TIME [epoch: 8.13 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.144948609782294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.144948609782294 | validation: 0.1884709765440166]
	TIME [epoch: 8.13 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13788469719270427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13788469719270427 | validation: 0.21585117152138933]
	TIME [epoch: 8.14 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15722790552147628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15722790552147628 | validation: 0.29324907970194997]
	TIME [epoch: 8.13 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3200081647548076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3200081647548076 | validation: 0.5533313396704841]
	TIME [epoch: 8.16 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4278035978467413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4278035978467413 | validation: 0.7434853885274824]
	TIME [epoch: 8.13 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5715757865529054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5715757865529054 | validation: 0.3547416208938263]
	TIME [epoch: 8.13 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3263654645220455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3263654645220455 | validation: 0.30572504621117724]
	TIME [epoch: 8.13 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3982226452265914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3982226452265914 | validation: 0.19784806137294042]
	TIME [epoch: 8.14 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1946078386365276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1946078386365276 | validation: 0.357988480438533]
	TIME [epoch: 8.14 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2978346764740912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2978346764740912 | validation: 0.1879813892272541]
	TIME [epoch: 8.15 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16046869255075955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16046869255075955 | validation: 0.2698883867427912]
	TIME [epoch: 8.15 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21093176109278747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21093176109278747 | validation: 1.2304581909966468]
	TIME [epoch: 8.14 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8990066620523409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8990066620523409 | validation: 0.40527525215004356]
	TIME [epoch: 8.14 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2667603904705543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2667603904705543 | validation: 0.4926743624922198]
	TIME [epoch: 8.13 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4217116942895492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4217116942895492 | validation: 0.18930069683499273]
	TIME [epoch: 8.15 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14282375084009338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14282375084009338 | validation: 0.2568835101422254]
	TIME [epoch: 8.17 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17603018721636976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17603018721636976 | validation: 0.19033538397069352]
	TIME [epoch: 8.14 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14246192462750537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14246192462750537 | validation: 0.18554590062640897]
	TIME [epoch: 8.12 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1391020095104507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1391020095104507 | validation: 0.21210810636257627]
	TIME [epoch: 8.13 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14912872119719836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14912872119719836 | validation: 0.16987310339474115]
	TIME [epoch: 8.15 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14295687350644184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14295687350644184 | validation: 0.21250086189012027]
	TIME [epoch: 8.14 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14896764865774867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14896764865774867 | validation: 0.16606361887625112]
	TIME [epoch: 8.1 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15837379751285788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15837379751285788 | validation: 0.2664732573131891]
	TIME [epoch: 8.13 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21848872460324273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21848872460324273 | validation: 0.1775508677532386]
	TIME [epoch: 8.1 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17771503540387204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17771503540387204 | validation: 0.18193588811413863]
	TIME [epoch: 8.1 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12750015218227373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12750015218227373 | validation: 0.1452429716275051]
	TIME [epoch: 8.14 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_595.pth
	Model improved!!!
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.112760072140358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.112760072140358 | validation: 0.14282262417691924]
	TIME [epoch: 8.13 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_596.pth
	Model improved!!!
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11995874195124788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11995874195124788 | validation: 0.22745227848128416]
	TIME [epoch: 8.12 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1700424458781516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1700424458781516 | validation: 0.23633119101723638]
	TIME [epoch: 8.11 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23805906447051117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23805906447051117 | validation: 0.5979984310267848]
	TIME [epoch: 8.14 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4447628316276085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4447628316276085 | validation: 0.38695846039434306]
	TIME [epoch: 8.16 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3445264777192158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3445264777192158 | validation: 0.2876250679983286]
	TIME [epoch: 8.14 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3001277415008178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3001277415008178 | validation: 0.2479676381503725]
	TIME [epoch: 8.13 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1972650842906148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1972650842906148 | validation: 0.28571273892585913]
	TIME [epoch: 8.11 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21866831161245287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21866831161245287 | validation: 0.20204339534468072]
	TIME [epoch: 8.12 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2236316649495154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2236316649495154 | validation: 0.17587747815803723]
	TIME [epoch: 8.12 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13158666976511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13158666976511 | validation: 0.19386566583841436]
	TIME [epoch: 8.13 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.148027535739524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.148027535739524 | validation: 0.21376376745844602]
	TIME [epoch: 8.13 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17482350358792395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17482350358792395 | validation: 0.1953517751235393]
	TIME [epoch: 8.12 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1717443090224487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1717443090224487 | validation: 0.3154582884261756]
	TIME [epoch: 8.12 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.255661147510306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.255661147510306 | validation: 0.18874792286721603]
	TIME [epoch: 8.11 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15553831531418283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15553831531418283 | validation: 0.5437228808299059]
	TIME [epoch: 8.13 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41697620513219835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41697620513219835 | validation: 0.2907906393206316]
	TIME [epoch: 8.13 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28152883663115175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28152883663115175 | validation: 0.40256739774643635]
	TIME [epoch: 8.13 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42310676312508605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42310676312508605 | validation: 0.3171715354310749]
	TIME [epoch: 8.12 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2943114027271852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2943114027271852 | validation: 0.3161020788236024]
	TIME [epoch: 8.12 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3311867759014116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3311867759014116 | validation: 0.14720433667547472]
	TIME [epoch: 8.12 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13781830887640736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13781830887640736 | validation: 0.1663769448445157]
	TIME [epoch: 8.12 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16066993147744946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16066993147744946 | validation: 0.1826757754406399]
	TIME [epoch: 8.12 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15726094569769014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15726094569769014 | validation: 0.14033909081771276]
	TIME [epoch: 8.13 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_619.pth
	Model improved!!!
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12842629143259518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12842629143259518 | validation: 0.16008389632475867]
	TIME [epoch: 8.11 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12977387750305044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12977387750305044 | validation: 0.17925598471063325]
	TIME [epoch: 8.11 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15561373894337333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15561373894337333 | validation: 0.22959971180066982]
	TIME [epoch: 8.12 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1727951847677902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1727951847677902 | validation: 0.15705875120157586]
	TIME [epoch: 8.12 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16220729389354077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16220729389354077 | validation: 0.19232780782191947]
	TIME [epoch: 8.12 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14501181099115543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14501181099115543 | validation: 0.13557575817424108]
	TIME [epoch: 8.12 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_625.pth
	Model improved!!!
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1640823061019099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1640823061019099 | validation: 0.3004633195237727]
	TIME [epoch: 8.1 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22365305560927773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22365305560927773 | validation: 0.21876652800142393]
	TIME [epoch: 8.1 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21259764265339656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21259764265339656 | validation: 0.1944226753163145]
	TIME [epoch: 8.1 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14139486227302464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14139486227302464 | validation: 0.2531786034962412]
	TIME [epoch: 8.1 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18747945021069767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18747945021069767 | validation: 0.2033754549732846]
	TIME [epoch: 8.12 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1892958258986205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1892958258986205 | validation: 0.3319495351248735]
	TIME [epoch: 8.09 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24682867137178768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24682867137178768 | validation: 0.16330815051922848]
	TIME [epoch: 8.1 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1251426074175186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1251426074175186 | validation: 0.14444037230019177]
	TIME [epoch: 8.09 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15693393957241802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15693393957241802 | validation: 0.2646174895358075]
	TIME [epoch: 8.1 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21411799952623195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21411799952623195 | validation: 0.13701020919023815]
	TIME [epoch: 8.11 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11373372196944907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11373372196944907 | validation: 0.1506244016458633]
	TIME [epoch: 8.11 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12200476508949219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12200476508949219 | validation: 0.2507082916483312]
	TIME [epoch: 8.09 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20727670106195237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20727670106195237 | validation: 0.18590115803411622]
	TIME [epoch: 8.09 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15550279658078192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15550279658078192 | validation: 0.26072109395979615]
	TIME [epoch: 8.09 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2206987642816178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2206987642816178 | validation: 0.21849295805916658]
	TIME [epoch: 8.1 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1739844691610544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1739844691610544 | validation: 0.20582907853392451]
	TIME [epoch: 8.1 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17589908022596362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17589908022596362 | validation: 0.22597272106595648]
	TIME [epoch: 8.1 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18025140222412844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18025140222412844 | validation: 0.14982262251014286]
	TIME [epoch: 8.09 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14584596237608152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14584596237608152 | validation: 0.3189997816080728]
	TIME [epoch: 8.09 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27386531262376074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27386531262376074 | validation: 0.2059951212063038]
	TIME [epoch: 8.08 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18010905000520647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18010905000520647 | validation: 0.48216758543415034]
	TIME [epoch: 8.1 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3946913444003501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3946913444003501 | validation: 0.3472671965706147]
	TIME [epoch: 8.12 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3737276665514686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3737276665514686 | validation: 0.2491583245749736]
	TIME [epoch: 8.09 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22697108535667102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22697108535667102 | validation: 0.1639289472208717]
	TIME [epoch: 8.1 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17219715937622154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17219715937622154 | validation: 0.21342789409186158]
	TIME [epoch: 8.1 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1972274599754497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1972274599754497 | validation: 0.15766713574256272]
	TIME [epoch: 8.09 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15373694325059709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15373694325059709 | validation: 0.1232238344092147]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_652.pth
	Model improved!!!
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11786172630211944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11786172630211944 | validation: 0.1534185755959007]
	TIME [epoch: 8.11 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13196901960292187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13196901960292187 | validation: 0.14567134273220753]
	TIME [epoch: 8.09 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12732194176499903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12732194176499903 | validation: 0.19723260814639612]
	TIME [epoch: 8.09 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14460795178551553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14460795178551553 | validation: 0.1677205789486063]
	TIME [epoch: 8.1 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13868461322657055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13868461322657055 | validation: 0.1913896058162736]
	TIME [epoch: 8.1 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15415168420488398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15415168420488398 | validation: 0.11281111822476847]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_658.pth
	Model improved!!!
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1019344710008524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1019344710008524 | validation: 0.1255348750317696]
	TIME [epoch: 8.15 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11664171196335907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11664171196335907 | validation: 0.13917330846816245]
	TIME [epoch: 8.14 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12749788414605248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12749788414605248 | validation: 0.20308353518066657]
	TIME [epoch: 8.15 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19183453369983133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19183453369983133 | validation: 0.5949792871941976]
	TIME [epoch: 8.15 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4493080574008219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4493080574008219 | validation: 0.8584276498523553]
	TIME [epoch: 8.1 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6394367854966367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6394367854966367 | validation: 0.8453289033147574]
	TIME [epoch: 8.15 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5932221393276078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5932221393276078 | validation: 0.5273996975323916]
	TIME [epoch: 8.15 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4509189155179061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4509189155179061 | validation: 0.5235135038611824]
	TIME [epoch: 8.14 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5654893371710163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5654893371710163 | validation: 0.20549934258826452]
	TIME [epoch: 8.14 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18928677184383766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18928677184383766 | validation: 0.26636789625031326]
	TIME [epoch: 8.07 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2198204121401338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2198204121401338 | validation: 0.1620422946200996]
	TIME [epoch: 8.08 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16047267735227863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16047267735227863 | validation: 0.1514759251302481]
	TIME [epoch: 8.11 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1367327021651275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1367327021651275 | validation: 0.12480343157008976]
	TIME [epoch: 8.09 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12303525402494521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12303525402494521 | validation: 0.12879351703074257]
	TIME [epoch: 8.08 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11760805433542165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11760805433542165 | validation: 0.12490955589868391]
	TIME [epoch: 8.08 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1202750468197958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1202750468197958 | validation: 0.18484693167537625]
	TIME [epoch: 8.11 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14560841153082746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14560841153082746 | validation: 0.14577891909593796]
	TIME [epoch: 8.1 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13077470866512667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13077470866512667 | validation: 0.16358656061450552]
	TIME [epoch: 8.11 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13311265119782179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13311265119782179 | validation: 0.1511509517968812]
	TIME [epoch: 8.08 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11499550629465606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11499550629465606 | validation: 0.14755885119007292]
	TIME [epoch: 8.11 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11867423163148035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11867423163148035 | validation: 0.636762119659245]
	TIME [epoch: 8.11 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7800545891152534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7800545891152534 | validation: 0.5773552927391425]
	TIME [epoch: 8.11 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6310390464783564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6310390464783564 | validation: 0.2538589809275379]
	TIME [epoch: 8.13 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19448120801516944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19448120801516944 | validation: 0.28344463503441636]
	TIME [epoch: 8.13 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23106175392136646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23106175392136646 | validation: 0.20875903112928393]
	TIME [epoch: 8.11 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19376211829303117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19376211829303117 | validation: 0.18569443421322274]
	TIME [epoch: 8.14 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14158967720975943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14158967720975943 | validation: 0.1752067281915815]
	TIME [epoch: 8.13 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14134878104703671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14134878104703671 | validation: 0.1686097603216556]
	TIME [epoch: 8.12 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12062347098783864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12062347098783864 | validation: 0.15859626769265595]
	TIME [epoch: 8.11 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10929176770678928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10929176770678928 | validation: 0.171169205642669]
	TIME [epoch: 8.14 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10860561310795533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10860561310795533 | validation: 0.13701963425973224]
	TIME [epoch: 8.12 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09972574215130153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09972574215130153 | validation: 0.13933965900522083]
	TIME [epoch: 8.11 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10928042573842837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10928042573842837 | validation: 0.29474870855954144]
	TIME [epoch: 8.1 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2210519451091263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2210519451091263 | validation: 0.3508164022733248]
	TIME [epoch: 8.1 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28533984068108903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28533984068108903 | validation: 0.2653541986908004]
	TIME [epoch: 8.11 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21681295851067414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21681295851067414 | validation: 0.22921146187943534]
	TIME [epoch: 8.12 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34977827448719595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34977827448719595 | validation: 0.187388236102616]
	TIME [epoch: 8.1 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2336591462467083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2336591462467083 | validation: 0.19195008698869342]
	TIME [epoch: 8.12 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16789138024183145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16789138024183145 | validation: 0.17077667062328566]
	TIME [epoch: 8.12 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16593549805456392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16593549805456392 | validation: 0.13634814852276828]
	TIME [epoch: 8.11 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09901803720139989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09901803720139989 | validation: 0.1627336877898185]
	TIME [epoch: 8.12 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12656059252437402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12656059252437402 | validation: 0.11916522463836624]
	TIME [epoch: 8.13 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10140512116209287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10140512116209287 | validation: 0.15564372626924447]
	TIME [epoch: 8.07 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1129167605015171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1129167605015171 | validation: 0.132618388505228]
	TIME [epoch: 8.08 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1222856941851677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1222856941851677 | validation: 0.26429033898086146]
	TIME [epoch: 8.08 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19044434015512288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19044434015512288 | validation: 0.24752372634426678]
	TIME [epoch: 8.09 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18844459700070215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18844459700070215 | validation: 0.2223057136779147]
	TIME [epoch: 8.07 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16553415485830977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16553415485830977 | validation: 0.11579801229902059]
	TIME [epoch: 8.08 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09629634164591748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09629634164591748 | validation: 0.11126353236655917]
	TIME [epoch: 8.07 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_707.pth
	Model improved!!!
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0885681074017721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0885681074017721 | validation: 0.146643725828098]
	TIME [epoch: 8.11 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10913958046399973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10913958046399973 | validation: 0.13895912744696354]
	TIME [epoch: 8.12 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12911488174763208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12911488174763208 | validation: 0.3401589837567607]
	TIME [epoch: 8.11 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27242229229772125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27242229229772125 | validation: 0.3038860395659442]
	TIME [epoch: 8.1 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2417450796645119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2417450796645119 | validation: 0.24995467283859446]
	TIME [epoch: 8.1 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22440185892095849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22440185892095849 | validation: 0.115293936331341]
	TIME [epoch: 8.09 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10865096568979135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10865096568979135 | validation: 0.10800172106324545]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_714.pth
	Model improved!!!
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1315477870584104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1315477870584104 | validation: 0.22216016040168257]
	TIME [epoch: 8.13 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1859053267445327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1859053267445327 | validation: 0.21659037444413648]
	TIME [epoch: 8.08 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27013936221986806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27013936221986806 | validation: 0.21626731526466864]
	TIME [epoch: 8.12 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16501459613424987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16501459613424987 | validation: 0.16775539290720434]
	TIME [epoch: 8.08 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14365848784838417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14365848784838417 | validation: 0.1643074516182287]
	TIME [epoch: 8.09 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18559122245551493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18559122245551493 | validation: 0.27813884980259146]
	TIME [epoch: 8.09 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2111140535074774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2111140535074774 | validation: 0.12805168760018829]
	TIME [epoch: 8.13 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1128169528853292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1128169528853292 | validation: 0.1629072321083887]
	TIME [epoch: 8.08 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15929874689257414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15929874689257414 | validation: 0.24956698656005566]
	TIME [epoch: 8.09 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1946343038041286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1946343038041286 | validation: 0.15868297752045893]
	TIME [epoch: 8.09 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11000259935086529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11000259935086529 | validation: 0.16535094766594557]
	TIME [epoch: 8.1 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15253331672248469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15253331672248469 | validation: 0.2630062441003874]
	TIME [epoch: 8.1 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2020346694528576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2020346694528576 | validation: 0.10699922760605829]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_727.pth
	Model improved!!!
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10590881074568864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10590881074568864 | validation: 0.11692289544077257]
	TIME [epoch: 8.1 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13985610662478892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13985610662478892 | validation: 0.26949323936415653]
	TIME [epoch: 8.09 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20916544834786194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20916544834786194 | validation: 0.11450056706111096]
	TIME [epoch: 8.08 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09288913780399101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09288913780399101 | validation: 0.11006750554524146]
	TIME [epoch: 8.09 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08172943860342326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08172943860342326 | validation: 0.17852757691730267]
	TIME [epoch: 8.09 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12853921359538537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12853921359538537 | validation: 0.2529478235898851]
	TIME [epoch: 8.1 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21829336293250431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21829336293250431 | validation: 0.3491540608234249]
	TIME [epoch: 8.1 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2715176776000795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2715176776000795 | validation: 0.18318722666667106]
	TIME [epoch: 8.09 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1638391823840745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1638391823840745 | validation: 0.3187172972406209]
	TIME [epoch: 8.1 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35900070988810434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35900070988810434 | validation: 0.24045877425264514]
	TIME [epoch: 8.1 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18940003873896885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18940003873896885 | validation: 0.9003986386924547]
	TIME [epoch: 8.11 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8998047751424366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8998047751424366 | validation: 0.6930731542169419]
	TIME [epoch: 8.09 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7202145423156247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7202145423156247 | validation: 0.24335923113539712]
	TIME [epoch: 8.09 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35485476347924905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35485476347924905 | validation: 0.631736538177114]
	TIME [epoch: 8.11 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6368636073461598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6368636073461598 | validation: 0.5968597685112986]
	TIME [epoch: 8.1 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5931554337665434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5931554337665434 | validation: 0.3956573015816079]
	TIME [epoch: 8.09 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3791776589459863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3791776589459863 | validation: 0.23574393754432832]
	TIME [epoch: 8.11 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20630451009175121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20630451009175121 | validation: 0.18789632980139476]
	TIME [epoch: 8.09 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16788602223746607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16788602223746607 | validation: 0.1715732254935155]
	TIME [epoch: 8.1 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13675245545469078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13675245545469078 | validation: 0.14782060775294997]
	TIME [epoch: 8.09 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1172910181212258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1172910181212258 | validation: 0.14289526889180545]
	TIME [epoch: 8.1 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10619184344155383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10619184344155383 | validation: 0.11636538583300626]
	TIME [epoch: 8.09 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09841465479597716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09841465479597716 | validation: 0.10906607657015793]
	TIME [epoch: 8.1 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09257621061257236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09257621061257236 | validation: 0.12334053334920125]
	TIME [epoch: 8.08 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09562731851285973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09562731851285973 | validation: 0.1223856545290269]
	TIME [epoch: 8.1 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10246445983821996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10246445983821996 | validation: 0.19301996899396748]
	TIME [epoch: 8.09 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1522424863759286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1522424863759286 | validation: 0.19613169379961756]
	TIME [epoch: 8.1 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16307221013736378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16307221013736378 | validation: 0.13866031362981654]
	TIME [epoch: 8.1 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11111780302047931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11111780302047931 | validation: 0.10233620014657224]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_756.pth
	Model improved!!!
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09242970978978228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09242970978978228 | validation: 0.1847046851313138]
	TIME [epoch: 8.12 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13908520769967797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13908520769967797 | validation: 0.25870322133710266]
	TIME [epoch: 8.13 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2642766752701304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2642766752701304 | validation: 0.20728644624233664]
	TIME [epoch: 8.17 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1558475594573193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1558475594573193 | validation: 0.17554958315235475]
	TIME [epoch: 8.14 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12888326137935813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12888326137935813 | validation: 0.1668827263920274]
	TIME [epoch: 8.17 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16245878732380667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16245878732380667 | validation: 0.14467572992082908]
	TIME [epoch: 8.13 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1288012578271844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1288012578271844 | validation: 0.13108492054410667]
	TIME [epoch: 8.14 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11015363267036406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11015363267036406 | validation: 0.20460738009160706]
	TIME [epoch: 8.15 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2784917901579705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2784917901579705 | validation: 0.20122363984737915]
	TIME [epoch: 8.14 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14677026888637298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14677026888637298 | validation: 0.152608148305817]
	TIME [epoch: 8.16 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11974472568085802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11974472568085802 | validation: 0.16534827249188133]
	TIME [epoch: 8.14 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11868939162721028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11868939162721028 | validation: 0.21929291789931954]
	TIME [epoch: 8.14 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15708918787438778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15708918787438778 | validation: 0.16474275318584275]
	TIME [epoch: 8.14 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11906506961748742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11906506961748742 | validation: 0.16241537204664064]
	TIME [epoch: 8.14 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1401944390607125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1401944390607125 | validation: 0.30457134738860464]
	TIME [epoch: 8.14 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2418720794032035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2418720794032035 | validation: 0.23042614987657745]
	TIME [epoch: 8.17 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.184145446487142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.184145446487142 | validation: 0.19975894279507964]
	TIME [epoch: 8.15 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18158098098211803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18158098098211803 | validation: 0.17571288590186623]
	TIME [epoch: 8.14 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13037394810023833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13037394810023833 | validation: 0.1455943661235457]
	TIME [epoch: 8.14 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10487207296413989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10487207296413989 | validation: 0.14077273692806969]
	TIME [epoch: 8.14 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11176170985134626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11176170985134626 | validation: 0.119971847513195]
	TIME [epoch: 8.15 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09339653847589677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09339653847589677 | validation: 0.11897490614461058]
	TIME [epoch: 8.15 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08302754759620512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08302754759620512 | validation: 0.10206045266648521]
	TIME [epoch: 8.13 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_779.pth
	Model improved!!!
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07485083870751683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07485083870751683 | validation: 0.09075728711003488]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_780.pth
	Model improved!!!
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09315620581645917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09315620581645917 | validation: 0.27630131660558965]
	TIME [epoch: 8.13 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2155567486070855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2155567486070855 | validation: 0.15639124187135192]
	TIME [epoch: 8.14 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10634890997102975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10634890997102975 | validation: 0.1410677751435973]
	TIME [epoch: 8.16 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17313289593291786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17313289593291786 | validation: 0.28958244726931565]
	TIME [epoch: 8.15 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23671517620846558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23671517620846558 | validation: 0.18401946662417676]
	TIME [epoch: 8.14 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1399608127047214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1399608127047214 | validation: 0.3117001638669278]
	TIME [epoch: 8.15 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28194283715632373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28194283715632373 | validation: 0.18282351546689962]
	TIME [epoch: 8.13 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1503437963778398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1503437963778398 | validation: 0.14014465155172248]
	TIME [epoch: 8.14 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11496101266888968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11496101266888968 | validation: 0.09068886157206996]
	TIME [epoch: 8.14 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_789.pth
	Model improved!!!
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1288591150336415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1288591150336415 | validation: 0.16436413878882902]
	TIME [epoch: 8.13 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14967486377377767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14967486377377767 | validation: 0.10925083245552775]
	TIME [epoch: 8.13 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11045073246378945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11045073246378945 | validation: 0.08732689368550639]
	TIME [epoch: 8.13 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_792.pth
	Model improved!!!
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09317066794799478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09317066794799478 | validation: 0.07838221486826993]
	TIME [epoch: 8.12 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_793.pth
	Model improved!!!
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0743360488365593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0743360488365593 | validation: 0.10986345068977377]
	TIME [epoch: 8.14 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07557792167315207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07557792167315207 | validation: 0.1652909578216683]
	TIME [epoch: 8.14 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12832425202457967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12832425202457967 | validation: 0.27543044366063857]
	TIME [epoch: 8.15 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20398052132191374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20398052132191374 | validation: 0.16677728355644314]
	TIME [epoch: 8.15 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16363852974142643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16363852974142643 | validation: 0.136636941384035]
	TIME [epoch: 8.13 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09990331594879859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09990331594879859 | validation: 0.12115347847756311]
	TIME [epoch: 8.15 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11067492014100523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11067492014100523 | validation: 0.23530527731060952]
	TIME [epoch: 8.15 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17105685931133788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17105685931133788 | validation: 0.12391040690257019]
	TIME [epoch: 8.14 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08146252918732987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08146252918732987 | validation: 0.11608367423947219]
	TIME [epoch: 8.13 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08105502909435174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08105502909435174 | validation: 0.1622363018932767]
	TIME [epoch: 8.14 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11945376380901211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11945376380901211 | validation: 0.10224616304144568]
	TIME [epoch: 8.14 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07009861867320803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07009861867320803 | validation: 0.08820308249599729]
	TIME [epoch: 8.16 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06096393908216689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06096393908216689 | validation: 0.08597116479670239]
	TIME [epoch: 8.14 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06213990807220771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06213990807220771 | validation: 0.0852193582355183]
	TIME [epoch: 8.14 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07855633047353001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07855633047353001 | validation: 0.23846923644343693]
	TIME [epoch: 8.13 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18214801731737715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18214801731737715 | validation: 0.16737128275540414]
	TIME [epoch: 8.13 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1683079466445573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1683079466445573 | validation: 0.20437389356486643]
	TIME [epoch: 8.13 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18167539806832433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18167539806832433 | validation: 0.37349091697602504]
	TIME [epoch: 8.15 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2998778886061009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2998778886061009 | validation: 0.1739972195850879]
	TIME [epoch: 8.15 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13875507160156558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13875507160156558 | validation: 0.13940084513376896]
	TIME [epoch: 8.16 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18291530169614376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18291530169614376 | validation: 0.3884631257018749]
	TIME [epoch: 8.13 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40247265103992264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40247265103992264 | validation: 0.35041623226494795]
	TIME [epoch: 8.14 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37547157247735974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37547157247735974 | validation: 0.19724727653657143]
	TIME [epoch: 8.15 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14052447797582956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14052447797582956 | validation: 0.13461091366120687]
	TIME [epoch: 8.16 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14062201275783262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14062201275783262 | validation: 0.13697141978070895]
	TIME [epoch: 8.14 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1140884181623062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1140884181623062 | validation: 0.13450420117603776]
	TIME [epoch: 8.15 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09257887107183732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09257887107183732 | validation: 0.13752284707650744]
	TIME [epoch: 8.13 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10702404752199028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10702404752199028 | validation: 0.22978606870972149]
	TIME [epoch: 8.14 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15695227526215177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15695227526215177 | validation: 0.2225747216804321]
	TIME [epoch: 8.14 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2375729035322442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2375729035322442 | validation: 0.35363756940410873]
	TIME [epoch: 8.16 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4215315075660763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4215315075660763 | validation: 0.15694373395410674]
	TIME [epoch: 8.15 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12332949035941906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12332949035941906 | validation: 0.25432854482364237]
	TIME [epoch: 8.14 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1837636125676208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1837636125676208 | validation: 0.13722879753015743]
	TIME [epoch: 8.14 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11192790363353218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11192790363353218 | validation: 0.1451080094878918]
	TIME [epoch: 8.13 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13626670233077978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13626670233077978 | validation: 0.17937216412768225]
	TIME [epoch: 8.15 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1279223392412753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1279223392412753 | validation: 0.12337404055130952]
	TIME [epoch: 8.15 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07537920380223884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07537920380223884 | validation: 0.13527132577736709]
	TIME [epoch: 8.16 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09818113551057693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09818113551057693 | validation: 0.13629123220985542]
	TIME [epoch: 8.13 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10300835341438543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10300835341438543 | validation: 0.17935493175018627]
	TIME [epoch: 8.14 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11246919737919393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11246919737919393 | validation: 0.1852252232418047]
	TIME [epoch: 8.15 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1901108701906247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1901108701906247 | validation: 0.2244083731395673]
	TIME [epoch: 8.18 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15593151776509934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15593151776509934 | validation: 0.1502896959968356]
	TIME [epoch: 8.14 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09814519269898528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09814519269898528 | validation: 0.13419839291911095]
	TIME [epoch: 8.13 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08441620854418927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08441620854418927 | validation: 0.1254139846170202]
	TIME [epoch: 8.18 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08263067051688433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08263067051688433 | validation: 0.08963528091446818]
	TIME [epoch: 8.14 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.069695035517405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.069695035517405 | validation: 0.0981660832571623]
	TIME [epoch: 8.14 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05545732014877024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05545732014877024 | validation: 0.11140484835383511]
	TIME [epoch: 8.16 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062178953768116045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062178953768116045 | validation: 0.14043898177360528]
	TIME [epoch: 8.17 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09244777800074061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09244777800074061 | validation: 0.20153364627212214]
	TIME [epoch: 8.14 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14692473509734463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14692473509734463 | validation: 0.2057366327551282]
	TIME [epoch: 8.16 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16474557990263963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16474557990263963 | validation: 0.13560487824224493]
	TIME [epoch: 8.14 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13179443611536693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13179443611536693 | validation: 0.12218991814206015]
	TIME [epoch: 8.15 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08709894070642277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08709894070642277 | validation: 0.10612407665222344]
	TIME [epoch: 8.15 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08753906736071898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08753906736071898 | validation: 0.1082483283643505]
	TIME [epoch: 8.15 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0797044452743122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0797044452743122 | validation: 0.09456321357069811]
	TIME [epoch: 8.14 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0648579091589805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0648579091589805 | validation: 0.09550957405839844]
	TIME [epoch: 8.14 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05644739721781801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05644739721781801 | validation: 0.08812206120888652]
	TIME [epoch: 8.16 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06727229655660072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06727229655660072 | validation: 0.09837397697906217]
	TIME [epoch: 8.16 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09123481168405687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09123481168405687 | validation: 0.25642105568219775]
	TIME [epoch: 8.16 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19811142033982707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19811142033982707 | validation: 0.14657554742628737]
	TIME [epoch: 8.14 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11534639910821146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11534639910821146 | validation: 0.11277889127486303]
	TIME [epoch: 8.15 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11580937947681548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11580937947681548 | validation: 0.28781463846401295]
	TIME [epoch: 8.16 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24560958484892517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24560958484892517 | validation: 0.13927918493792288]
	TIME [epoch: 8.18 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09765919487010255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09765919487010255 | validation: 0.16229403962830055]
	TIME [epoch: 8.17 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12641548917816456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12641548917816456 | validation: 0.2171487296988414]
	TIME [epoch: 8.15 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1688304561213873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1688304561213873 | validation: 0.147486396752306]
	TIME [epoch: 8.15 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1287865500991724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1287865500991724 | validation: 0.10981218212414007]
	TIME [epoch: 8.16 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09182741074059407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09182741074059407 | validation: 0.1545816600890355]
	TIME [epoch: 8.13 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10147765454409523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10147765454409523 | validation: 0.08708625010342443]
	TIME [epoch: 8.17 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06881128018247427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06881128018247427 | validation: 0.0855696465675693]
	TIME [epoch: 8.16 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09188136179381666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09188136179381666 | validation: 0.1954475119786261]
	TIME [epoch: 8.15 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1406234431364691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1406234431364691 | validation: 0.13346299595643535]
	TIME [epoch: 8.13 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07422235184759687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07422235184759687 | validation: 0.13501259768371776]
	TIME [epoch: 8.18 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11572842057419934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11572842057419934 | validation: 0.2040238412704464]
	TIME [epoch: 8.14 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1637327739101366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1637327739101366 | validation: 0.12938612791183793]
	TIME [epoch: 8.16 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12141322664564291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12141322664564291 | validation: 0.11235667155406676]
	TIME [epoch: 8.15 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10830134642755379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10830134642755379 | validation: 0.13024801822956586]
	TIME [epoch: 8.15 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09873161552700818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09873161552700818 | validation: 0.06724499148476958]
	TIME [epoch: 8.13 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_871.pth
	Model improved!!!
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05985301895658806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05985301895658806 | validation: 0.057297982248178]
	TIME [epoch: 8.12 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_872.pth
	Model improved!!!
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05485086847079911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05485086847079911 | validation: 0.06305928800112993]
	TIME [epoch: 8.1 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04776535378367754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04776535378367754 | validation: 0.12814086865513893]
	TIME [epoch: 8.12 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06870804936036122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06870804936036122 | validation: 0.0996023071657662]
	TIME [epoch: 8.09 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1374528505352132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1374528505352132 | validation: 0.23924379780587757]
	TIME [epoch: 8.12 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1746746500813957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1746746500813957 | validation: 0.17125527044336686]
	TIME [epoch: 8.11 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11712598904610054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11712598904610054 | validation: 0.1894307898296284]
	TIME [epoch: 8.13 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12370499752101029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12370499752101029 | validation: 0.24568165496562908]
	TIME [epoch: 8.1 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17821685087423672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17821685087423672 | validation: 0.184249432263037]
	TIME [epoch: 8.13 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14654542663408548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14654542663408548 | validation: 0.14682531487430361]
	TIME [epoch: 8.09 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10217391160373553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10217391160373553 | validation: 0.09185410068351686]
	TIME [epoch: 8.12 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05171570382656862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05171570382656862 | validation: 0.07986463371747228]
	TIME [epoch: 8.13 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04978772470692126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04978772470692126 | validation: 0.07890245327768505]
	TIME [epoch: 8.12 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07833833424177304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07833833424177304 | validation: 0.24187682237467334]
	TIME [epoch: 8.13 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1915829482525283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1915829482525283 | validation: 0.17186393209372341]
	TIME [epoch: 8.16 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15519259355705894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15519259355705894 | validation: 0.19125190427246097]
	TIME [epoch: 8.1 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16129817583354475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16129817583354475 | validation: 0.14736633002523503]
	TIME [epoch: 8.13 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10732768238155611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10732768238155611 | validation: 0.09477439497457874]
	TIME [epoch: 8.14 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11144308435944078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11144308435944078 | validation: 0.07115261457407701]
	TIME [epoch: 8.17 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04915370744678674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04915370744678674 | validation: 0.09003806523817759]
	TIME [epoch: 8.12 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050870471771579155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050870471771579155 | validation: 0.06500727536603151]
	TIME [epoch: 8.15 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05932223376854319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05932223376854319 | validation: 0.09326871524695697]
	TIME [epoch: 8.12 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07899372432131267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07899372432131267 | validation: 0.09691295541871177]
	TIME [epoch: 8.16 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0487144153189888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0487144153189888 | validation: 0.13809115202179842]
	TIME [epoch: 8.13 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09094143721871645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09094143721871645 | validation: 0.18323082011545788]
	TIME [epoch: 8.15 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12327050712635959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12327050712635959 | validation: 0.14406627875347564]
	TIME [epoch: 8.12 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12178727537389435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12178727537389435 | validation: 0.11822350250019867]
	TIME [epoch: 8.17 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09080668738367301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09080668738367301 | validation: 0.09218492608614687]
	TIME [epoch: 8.11 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051938494067958224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051938494067958224 | validation: 0.0685981272721135]
	TIME [epoch: 8.16 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039162905227482496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039162905227482496 | validation: 0.18502207927855918]
	TIME [epoch: 8.11 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14553866907710958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14553866907710958 | validation: 0.34968825782397234]
	TIME [epoch: 8.15 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4758354737030861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4758354737030861 | validation: 0.38377385806491426]
	TIME [epoch: 8.11 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3395453520292112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3395453520292112 | validation: 0.5906117212715807]
	TIME [epoch: 8.12 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4336620709296567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4336620709296567 | validation: 0.4337628655851381]
	TIME [epoch: 8.11 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31721467597625536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31721467597625536 | validation: 0.24101015460244268]
	TIME [epoch: 8.1 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18619430430617256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18619430430617256 | validation: 0.12166330499294298]
	TIME [epoch: 8.12 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09311116841185893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09311116841185893 | validation: 0.06515148605333562]
	TIME [epoch: 8.11 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08053798092439954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08053798092439954 | validation: 0.07414630138085933]
	TIME [epoch: 8.11 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07584628001855986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07584628001855986 | validation: 0.0682817793387625]
	TIME [epoch: 8.12 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05567561076134089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05567561076134089 | validation: 0.055162265011725525]
	TIME [epoch: 8.13 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_911.pth
	Model improved!!!
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0500615839583427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0500615839583427 | validation: 0.07932620385382298]
	TIME [epoch: 8.11 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0636144309152353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0636144309152353 | validation: 0.1631233181836846]
	TIME [epoch: 8.13 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12482248332922587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12482248332922587 | validation: 0.161997746003892]
	TIME [epoch: 8.11 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14355830224446048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14355830224446048 | validation: 0.10120463794903078]
	TIME [epoch: 8.12 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09865814341548211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09865814341548211 | validation: 0.06333304161053192]
	TIME [epoch: 8.13 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06096219744747277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06096219744747277 | validation: 0.06384986936711548]
	TIME [epoch: 8.12 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04267441844379992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04267441844379992 | validation: 0.0494276463156158]
	TIME [epoch: 8.2 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_918.pth
	Model improved!!!
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04001153020339331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04001153020339331 | validation: 0.05405047392458345]
	TIME [epoch: 8.1 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04068537302084842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04068537302084842 | validation: 0.06385853477043768]
	TIME [epoch: 8.1 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04599168079198059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04599168079198059 | validation: 0.09747460137270525]
	TIME [epoch: 8.11 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0641978578661263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0641978578661263 | validation: 0.09531118922023596]
	TIME [epoch: 8.12 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10931697816860528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10931697816860528 | validation: 0.2649671610716188]
	TIME [epoch: 8.12 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17752562349700388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17752562349700388 | validation: 0.5068784985653164]
	TIME [epoch: 8.11 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3463945850458898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3463945850458898 | validation: 0.39710990679015523]
	TIME [epoch: 8.12 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3121632463039304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3121632463039304 | validation: 0.19326750085196318]
	TIME [epoch: 8.11 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19004457286077348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19004457286077348 | validation: 0.1325198693449266]
	TIME [epoch: 8.14 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1051552996632728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1051552996632728 | validation: 0.11684535547522744]
	TIME [epoch: 8.15 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09279134767634757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09279134767634757 | validation: 0.1176005114064373]
	TIME [epoch: 8.15 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09254250762595831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09254250762595831 | validation: 0.33754018752440507]
	TIME [epoch: 8.17 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3347886329050285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3347886329050285 | validation: 0.45850243144148656]
	TIME [epoch: 8.14 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35611458486733455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35611458486733455 | validation: 0.21657101448609484]
	TIME [epoch: 8.14 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17564457894834629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17564457894834629 | validation: 0.127298926337389]
	TIME [epoch: 8.14 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10890528469477172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10890528469477172 | validation: 0.11152562284756254]
	TIME [epoch: 8.13 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09551556833848875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09551556833848875 | validation: 0.10077087169071529]
	TIME [epoch: 8.13 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08091240454092301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08091240454092301 | validation: 0.07652665541557163]
	TIME [epoch: 8.14 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06652864412466665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06652864412466665 | validation: 0.1078784885905486]
	TIME [epoch: 8.13 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0711744041104407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0711744041104407 | validation: 0.11132922118162221]
	TIME [epoch: 8.13 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08555224083413354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08555224083413354 | validation: 0.13019347738448392]
	TIME [epoch: 8.15 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10179962041275285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10179962041275285 | validation: 0.09940488637085065]
	TIME [epoch: 8.16 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07819797854729293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07819797854729293 | validation: 0.08033556321558256]
	TIME [epoch: 8.14 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06199098903860609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06199098903860609 | validation: 0.05812041828398208]
	TIME [epoch: 8.14 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05611179478291065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05611179478291065 | validation: 0.06894932697659341]
	TIME [epoch: 8.13 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04714346249169263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04714346249169263 | validation: 0.06032181619933248]
	TIME [epoch: 8.12 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04173291175102144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04173291175102144 | validation: 0.05963123668523878]
	TIME [epoch: 8.15 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044277664781972974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044277664781972974 | validation: 0.08570819673003988]
	TIME [epoch: 8.12 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059273621667560394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059273621667560394 | validation: 0.09555579188086666]
	TIME [epoch: 8.14 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08162559677373724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08162559677373724 | validation: 0.11938389567292479]
	TIME [epoch: 8.14 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09275052740559081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09275052740559081 | validation: 0.1337318091435641]
	TIME [epoch: 8.13 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09648651297265001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09648651297265001 | validation: 0.08854434078424707]
	TIME [epoch: 8.15 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.103252498574379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.103252498574379 | validation: 0.16308897939402311]
	TIME [epoch: 8.14 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10863540926618537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10863540926618537 | validation: 0.08118480903410827]
	TIME [epoch: 8.14 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0534372842723468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0534372842723468 | validation: 0.070474839768162]
	TIME [epoch: 8.13 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044219393166651966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044219393166651966 | validation: 0.06263187321420373]
	TIME [epoch: 8.14 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04638545340105367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04638545340105367 | validation: 0.08741817589083467]
	TIME [epoch: 8.17 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1197484854242142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1197484854242142 | validation: 0.1944351730987619]
	TIME [epoch: 8.13 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12736774333563933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12736774333563933 | validation: 0.13708361023799462]
	TIME [epoch: 8.14 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08203219559558612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08203219559558612 | validation: 0.10760530047598592]
	TIME [epoch: 8.16 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08711927623176526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08711927623176526 | validation: 0.14193241972183243]
	TIME [epoch: 8.14 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09122227106100489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09122227106100489 | validation: 0.08783896381101064]
	TIME [epoch: 8.12 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05433641870617171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05433641870617171 | validation: 0.0737940212901735]
	TIME [epoch: 8.13 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03859058199926424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03859058199926424 | validation: 0.06470133801599386]
	TIME [epoch: 8.16 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04208323828972277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04208323828972277 | validation: 0.05518805424167727]
	TIME [epoch: 8.17 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037429161980219096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037429161980219096 | validation: 0.07667364501999335]
	TIME [epoch: 8.16 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04009224680769176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04009224680769176 | validation: 0.06917627656241578]
	TIME [epoch: 8.16 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05907791254410064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05907791254410064 | validation: 0.11337248913964466]
	TIME [epoch: 8.15 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07543043196540429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07543043196540429 | validation: 0.07900513516430102]
	TIME [epoch: 8.15 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08262962076383117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08262962076383117 | validation: 0.14304626599541823]
	TIME [epoch: 8.13 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14540668071766905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14540668071766905 | validation: 0.20583141048594122]
	TIME [epoch: 8.14 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17636350177475557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17636350177475557 | validation: 0.135707662480873]
	TIME [epoch: 8.14 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09240268800832763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09240268800832763 | validation: 0.05841745839720145]
	TIME [epoch: 8.12 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04231646497166496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04231646497166496 | validation: 0.05719723376778059]
	TIME [epoch: 8.14 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037113909506038614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037113909506038614 | validation: 0.05801271732627268]
	TIME [epoch: 8.14 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034763957897197495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034763957897197495 | validation: 0.058859051751237006]
	TIME [epoch: 8.13 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04613475950510356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04613475950510356 | validation: 0.10229101696404684]
	TIME [epoch: 8.15 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07317429361238911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07317429361238911 | validation: 0.04499168604184905]
	TIME [epoch: 8.16 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_976.pth
	Model improved!!!
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05078643829213563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05078643829213563 | validation: 0.07036497516211017]
	TIME [epoch: 8.12 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05696575404642235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05696575404642235 | validation: 0.06268430036842469]
	TIME [epoch: 8.13 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06825357504114483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06825357504114483 | validation: 0.16925504490546422]
	TIME [epoch: 8.12 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12486473709945173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12486473709945173 | validation: 0.08481500817329096]
	TIME [epoch: 8.14 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07274957738061659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07274957738061659 | validation: 0.13644946750454467]
	TIME [epoch: 8.14 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1387524151432524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1387524151432524 | validation: 0.17694941650013113]
	TIME [epoch: 8.15 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12402180010223703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12402180010223703 | validation: 0.1066041663754243]
	TIME [epoch: 8.13 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06910923828196021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06910923828196021 | validation: 0.05266039783501511]
	TIME [epoch: 8.12 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0531507062824501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0531507062824501 | validation: 0.06490935161855675]
	TIME [epoch: 8.15 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05244388350366646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05244388350366646 | validation: 0.05301523367787603]
	TIME [epoch: 8.13 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040183614504834535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040183614504834535 | validation: 0.08377399249237799]
	TIME [epoch: 8.17 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05245554793390593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05245554793390593 | validation: 0.07059706726639435]
	TIME [epoch: 8.13 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06328130715220157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06328130715220157 | validation: 0.1281853748869067]
	TIME [epoch: 8.17 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10746505643609151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10746505643609151 | validation: 0.12120185808333235]
	TIME [epoch: 8.15 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12293725869998653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12293725869998653 | validation: 0.1380684546220706]
	TIME [epoch: 8.15 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09132177130123297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09132177130123297 | validation: 0.06367447431631715]
	TIME [epoch: 8.14 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0440635083284381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0440635083284381 | validation: 0.0410643053905859]
	TIME [epoch: 8.13 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_993.pth
	Model improved!!!
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03287543388203843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03287543388203843 | validation: 0.05999573902282168]
	TIME [epoch: 8.12 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04097204322057773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04097204322057773 | validation: 0.04309749596571071]
	TIME [epoch: 8.12 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05350966438980752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05350966438980752 | validation: 0.10621299817781563]
	TIME [epoch: 8.12 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07464180539697783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07464180539697783 | validation: 0.05428865121112968]
	TIME [epoch: 8.11 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05816486568481248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05816486568481248 | validation: 0.04773449811616437]
	TIME [epoch: 8.14 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032334673246338985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032334673246338985 | validation: 0.04868404205608987]
	TIME [epoch: 8.15 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03578379590402248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03578379590402248 | validation: 0.06560905527544716]
	TIME [epoch: 8.15 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04438204697462596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04438204697462596 | validation: 0.07169512979723588]
	TIME [epoch: 39.9 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06694221500723453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06694221500723453 | validation: 0.18941359889992776]
	TIME [epoch: 17.4 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1458850987242875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1458850987242875 | validation: 0.11488960416298262]
	TIME [epoch: 17.4 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09571911909070238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09571911909070238 | validation: 0.0489744386730773]
	TIME [epoch: 17.4 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0586787426453373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0586787426453373 | validation: 0.12191002633795095]
	TIME [epoch: 17.4 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08728473041303463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08728473041303463 | validation: 0.07528348430037107]
	TIME [epoch: 17.4 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059778603445834105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059778603445834105 | validation: 0.06391675996677935]
	TIME [epoch: 17.4 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042158275454097255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042158275454097255 | validation: 0.067326801729829]
	TIME [epoch: 17.4 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062307361712306636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062307361712306636 | validation: 0.14636285559816117]
	TIME [epoch: 17.4 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11340353992962615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11340353992962615 | validation: 0.16884214925937205]
	TIME [epoch: 17.4 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14604528682128445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14604528682128445 | validation: 0.09406692192739471]
	TIME [epoch: 17.4 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07073573721482986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07073573721482986 | validation: 0.033536784199959646]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_1012.pth
	Model improved!!!
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030146546049064523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030146546049064523 | validation: 0.044391487734828254]
	TIME [epoch: 17.4 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032060942636123045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032060942636123045 | validation: 0.06028951415598504]
	TIME [epoch: 17.4 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0807173306653916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0807173306653916 | validation: 0.1399243148587826]
	TIME [epoch: 17.4 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09949038107934145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09949038107934145 | validation: 0.06470555024412374]
	TIME [epoch: 17.4 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04578965513310521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04578965513310521 | validation: 0.05077247680055659]
	TIME [epoch: 17.4 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048752523030820394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048752523030820394 | validation: 0.0730217558820219]
	TIME [epoch: 17.4 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04576236879704515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04576236879704515 | validation: 0.055016926651731746]
	TIME [epoch: 17.5 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03468336312949037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03468336312949037 | validation: 0.06071951730923874]
	TIME [epoch: 17.4 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036510495030324164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036510495030324164 | validation: 0.06274096613984506]
	TIME [epoch: 17.5 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06053142096583672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06053142096583672 | validation: 0.166311014066286]
	TIME [epoch: 17.4 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13482277353079958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13482277353079958 | validation: 0.16445272236126351]
	TIME [epoch: 17.4 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1736735131475813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1736735131475813 | validation: 0.11019914398942526]
	TIME [epoch: 17.4 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08442245898648537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08442245898648537 | validation: 0.03039643640968639]
	TIME [epoch: 17.5 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_1025.pth
	Model improved!!!
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02558474393425111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02558474393425111 | validation: 0.03686793413941166]
	TIME [epoch: 17.4 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03996903227890687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03996903227890687 | validation: 0.0528139531437343]
	TIME [epoch: 17.4 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04300080097515698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04300080097515698 | validation: 0.039125989288046906]
	TIME [epoch: 17.4 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03418658653563188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03418658653563188 | validation: 0.04503772118071556]
	TIME [epoch: 17.4 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029410478122308566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029410478122308566 | validation: 0.04222036127346334]
	TIME [epoch: 17.4 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03327393214208777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03327393214208777 | validation: 0.07373897069016168]
	TIME [epoch: 17.4 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0441445156873181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0441445156873181 | validation: 0.08960433510190714]
	TIME [epoch: 17.4 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0763673048104325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0763673048104325 | validation: 0.1323956001613523]
	TIME [epoch: 17.4 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10758452344052995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10758452344052995 | validation: 0.08068391533671893]
	TIME [epoch: 17.4 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06801671365429922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06801671365429922 | validation: 0.09675713667946656]
	TIME [epoch: 17.4 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06314496599021183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06314496599021183 | validation: 0.07392372589788182]
	TIME [epoch: 17.4 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08289490195540708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08289490195540708 | validation: 0.14950034157358333]
	TIME [epoch: 17.4 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10485592757646002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10485592757646002 | validation: 0.0845039991489511]
	TIME [epoch: 17.4 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07491804334032351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07491804334032351 | validation: 0.06988393762144533]
	TIME [epoch: 17.4 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055360755139213354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055360755139213354 | validation: 0.09949736228295754]
	TIME [epoch: 17.4 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05778192615370113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05778192615370113 | validation: 0.0632076559458259]
	TIME [epoch: 17.4 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05093911899752685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05093911899752685 | validation: 0.08061535541385074]
	TIME [epoch: 17.4 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055090070022884206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055090070022884206 | validation: 0.05841557066224723]
	TIME [epoch: 17.4 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06260958918478869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06260958918478869 | validation: 0.08131009797787965]
	TIME [epoch: 17.4 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053983076232706345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053983076232706345 | validation: 0.06905489907587001]
	TIME [epoch: 17.4 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054670845167552024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054670845167552024 | validation: 0.07758361058038188]
	TIME [epoch: 17.4 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05884036536245077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05884036536245077 | validation: 0.08440949773640577]
	TIME [epoch: 17.4 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06115318594769507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06115318594769507 | validation: 0.08341401389830176]
	TIME [epoch: 17.4 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05609013209564944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05609013209564944 | validation: 0.06048526567996995]
	TIME [epoch: 17.4 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039216229515434735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039216229515434735 | validation: 0.04750089898546161]
	TIME [epoch: 17.4 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030830642665523805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030830642665523805 | validation: 0.045317581961070025]
	TIME [epoch: 17.4 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023763258744632764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023763258744632764 | validation: 0.03995925595921473]
	TIME [epoch: 17.4 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02318899854798554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02318899854798554 | validation: 0.05349185011172796]
	TIME [epoch: 17.4 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03159915743711036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03159915743711036 | validation: 0.05191666291143311]
	TIME [epoch: 17.4 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061725197639839566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061725197639839566 | validation: 0.1874665178467953]
	TIME [epoch: 17.4 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14827594731722932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14827594731722932 | validation: 0.0933497499813885]
	TIME [epoch: 17.4 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09749814374617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09749814374617 | validation: 0.03474623348254381]
	TIME [epoch: 17.4 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04979268899422174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04979268899422174 | validation: 0.07697499749395519]
	TIME [epoch: 17.4 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05392174093149274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05392174093149274 | validation: 0.06633113234923262]
	TIME [epoch: 17.4 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04879437798401494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04879437798401494 | validation: 0.04300974701970857]
	TIME [epoch: 17.4 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02809468609232496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02809468609232496 | validation: 0.044527590737346634]
	TIME [epoch: 17.4 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038446692626565374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038446692626565374 | validation: 0.12035842474843905]
	TIME [epoch: 17.4 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08614603793817853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08614603793817853 | validation: 0.17631539660110393]
	TIME [epoch: 17.4 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1503394904393842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1503394904393842 | validation: 0.10435440565717627]
	TIME [epoch: 17.4 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10001569676594066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10001569676594066 | validation: 0.04381752396578723]
	TIME [epoch: 17.4 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04114592058424146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04114592058424146 | validation: 0.045343176770583506]
	TIME [epoch: 17.4 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029758675361680382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029758675361680382 | validation: 0.02402645131373392]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_1067.pth
	Model improved!!!
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026644357891610963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026644357891610963 | validation: 0.07541601374395365]
	TIME [epoch: 17.4 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052653430273008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052653430273008 | validation: 0.04516008748622588]
	TIME [epoch: 17.4 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03862716574443369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03862716574443369 | validation: 0.05561040491508515]
	TIME [epoch: 17.4 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036726995550851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036726995550851 | validation: 0.037855783238519625]
	TIME [epoch: 17.4 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040966788487083454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040966788487083454 | validation: 0.12475249525334342]
	TIME [epoch: 17.4 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0968538358755525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0968538358755525 | validation: 0.0773094777492423]
	TIME [epoch: 17.4 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06937629173949969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06937629173949969 | validation: 0.07296536061902413]
	TIME [epoch: 17.4 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06603824714413373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06603824714413373 | validation: 0.1078171449617194]
	TIME [epoch: 17.4 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08115992432700761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08115992432700761 | validation: 0.08341411176501955]
	TIME [epoch: 17.4 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06868545161819259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06868545161819259 | validation: 0.06044657509186782]
	TIME [epoch: 17.4 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047637024014017135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047637024014017135 | validation: 0.04419187031951696]
	TIME [epoch: 17.4 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029246397481020054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029246397481020054 | validation: 0.020408871672341455]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_1079.pth
	Model improved!!!
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01895579258018392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01895579258018392 | validation: 0.03312812826393352]
	TIME [epoch: 17.4 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02226184034273019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02226184034273019 | validation: 0.027723784164361134]
	TIME [epoch: 17.4 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02911593234595148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02911593234595148 | validation: 0.1154702530615086]
	TIME [epoch: 17.4 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0916999230122144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0916999230122144 | validation: 0.12777803707373928]
	TIME [epoch: 17.4 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14019233254711533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14019233254711533 | validation: 0.15775215637861198]
	TIME [epoch: 17.4 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11919062186895193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11919062186895193 | validation: 0.06523698979359209]
	TIME [epoch: 17.4 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06478046932070157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06478046932070157 | validation: 0.0637447536265909]
	TIME [epoch: 17.4 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04328481199520851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04328481199520851 | validation: 0.03504299897664727]
	TIME [epoch: 17.4 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026057035159659216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026057035159659216 | validation: 0.03246902601345286]
	TIME [epoch: 17.4 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03728752701846106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03728752701846106 | validation: 0.07641288509394584]
	TIME [epoch: 17.4 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052597919207499885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052597919207499885 | validation: 0.05019416341685601]
	TIME [epoch: 17.4 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04940027072654767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04940027072654767 | validation: 0.0641763501475328]
	TIME [epoch: 17.4 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05153988111432739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05153988111432739 | validation: 0.0488253163606405]
	TIME [epoch: 17.4 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05417627089267387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05417627089267387 | validation: 0.08164337276325181]
	TIME [epoch: 17.4 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05886231809037436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05886231809037436 | validation: 0.04463460433563291]
	TIME [epoch: 17.4 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04377390765423523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04377390765423523 | validation: 0.07694325891615014]
	TIME [epoch: 17.4 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04675926633500749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04675926633500749 | validation: 0.06560481937597948]
	TIME [epoch: 17.4 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05543268663598429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05543268663598429 | validation: 0.06531402682430686]
	TIME [epoch: 17.4 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05858305647630646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05858305647630646 | validation: 0.072052573507541]
	TIME [epoch: 17.4 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05156683091618045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05156683091618045 | validation: 0.03747090461811128]
	TIME [epoch: 17.4 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038370976468199446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038370976468199446 | validation: 0.0741103874665439]
	TIME [epoch: 17.4 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05744547808426591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05744547808426591 | validation: 0.02682479427520922]
	TIME [epoch: 17.3 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035451667839354384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035451667839354384 | validation: 0.05160628874641349]
	TIME [epoch: 17.4 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04456922951983181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04456922951983181 | validation: 0.05593856949537695]
	TIME [epoch: 17.3 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07414200025248055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07414200025248055 | validation: 0.11233794776076117]
	TIME [epoch: 17.4 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09271910770058277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09271910770058277 | validation: 0.0928908215814701]
	TIME [epoch: 17.4 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07289579692707679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07289579692707679 | validation: 0.07675136982950817]
	TIME [epoch: 17.3 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06179634039895349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06179634039895349 | validation: 0.05124740646530349]
	TIME [epoch: 17.3 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0399685440729778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0399685440729778 | validation: 0.028805602391291252]
	TIME [epoch: 17.3 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025834094264787888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025834094264787888 | validation: 0.018190031633451044]
	TIME [epoch: 17.3 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_1109.pth
	Model improved!!!
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020760753856275377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020760753856275377 | validation: 0.04111207720587706]
	TIME [epoch: 17.4 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028806841076412813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028806841076412813 | validation: 0.03352067377421593]
	TIME [epoch: 17.4 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06601219739270324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06601219739270324 | validation: 0.1407479744450941]
	TIME [epoch: 17.4 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1268647743015875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1268647743015875 | validation: 0.07990219424629728]
	TIME [epoch: 17.4 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08141521544135474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08141521544135474 | validation: 0.07652179087548036]
	TIME [epoch: 17.4 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07023704170461756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07023704170461756 | validation: 0.0686957446116629]
	TIME [epoch: 17.4 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05495567932019677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05495567932019677 | validation: 0.036667639282495244]
	TIME [epoch: 17.3 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03371038563235266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03371038563235266 | validation: 0.01974971850080467]
	TIME [epoch: 17.4 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020984091186805998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020984091186805998 | validation: 0.03487231664865781]
	TIME [epoch: 17.4 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021087117565434712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021087117565434712 | validation: 0.023200620373548343]
	TIME [epoch: 17.4 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02731959093642576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02731959093642576 | validation: 0.04309709543675177]
	TIME [epoch: 17.4 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04089053529209913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04089053529209913 | validation: 0.05320091361760168]
	TIME [epoch: 17.3 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06737322381741245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06737322381741245 | validation: 0.12534089549376468]
	TIME [epoch: 17.4 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09563047519650644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09563047519650644 | validation: 0.11113272664780355]
	TIME [epoch: 17.4 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09301209266841182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09301209266841182 | validation: 0.11036943024408515]
	TIME [epoch: 17.4 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09133285764818597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09133285764818597 | validation: 0.05163486594509823]
	TIME [epoch: 17.3 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03616526118444243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03616526118444243 | validation: 0.02009023824628391]
	TIME [epoch: 17.4 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01937774646663275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01937774646663275 | validation: 0.02508015820919728]
	TIME [epoch: 17.3 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019810679343221838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019810679343221838 | validation: 0.02930761759897528]
	TIME [epoch: 17.4 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026972796491936846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026972796491936846 | validation: 0.04819678215234089]
	TIME [epoch: 17.3 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037163659328005194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037163659328005194 | validation: 0.05135458844683427]
	TIME [epoch: 17.4 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06465703903455726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06465703903455726 | validation: 0.13101737509983044]
	TIME [epoch: 17.3 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11087880936950606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11087880936950606 | validation: 0.07543974678166337]
	TIME [epoch: 17.3 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0749199791032231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0749199791032231 | validation: 0.04885262062017677]
	TIME [epoch: 17.3 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03892530442181194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03892530442181194 | validation: 0.03766790188061267]
	TIME [epoch: 17.4 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031494122932374254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031494122932374254 | validation: 0.026219750024891464]
	TIME [epoch: 17.3 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04150756209956102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04150756209956102 | validation: 0.09034299006403357]
	TIME [epoch: 17.4 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06431807604481811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06431807604481811 | validation: 0.0351431413228638]
	TIME [epoch: 17.3 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03023811781941663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03023811781941663 | validation: 0.026910448185432125]
	TIME [epoch: 17.4 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032295680492871855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032295680492871855 | validation: 0.029329876743528382]
	TIME [epoch: 17.3 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02236451626439819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02236451626439819 | validation: 0.03492452004012262]
	TIME [epoch: 17.4 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025201542460299008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025201542460299008 | validation: 0.0302174746753631]
	TIME [epoch: 17.4 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026485428734564415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026485428734564415 | validation: 0.039867556842031165]
	TIME [epoch: 17.4 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031401655379091196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031401655379091196 | validation: 0.07181049799923013]
	TIME [epoch: 17.3 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05137090958507133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05137090958507133 | validation: 0.10946118014051659]
	TIME [epoch: 17.4 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08096276515917739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08096276515917739 | validation: 0.09535973274180345]
	TIME [epoch: 17.3 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11071301452200243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11071301452200243 | validation: 0.16971061822853453]
	TIME [epoch: 17.4 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16011051254844247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16011051254844247 | validation: 0.06862353674291548]
	TIME [epoch: 17.3 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05449345912464381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05449345912464381 | validation: 0.048967622500678355]
	TIME [epoch: 17.4 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05440216442369554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05440216442369554 | validation: 0.04308394960934522]
	TIME [epoch: 17.4 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04307695682876527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04307695682876527 | validation: 0.03032328498065682]
	TIME [epoch: 17.3 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03179928075533864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03179928075533864 | validation: 0.029474784406788856]
	TIME [epoch: 17.3 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025340747999010007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025340747999010007 | validation: 0.03127982146122537]
	TIME [epoch: 17.4 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031824765456143815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031824765456143815 | validation: 0.02943714004816721]
	TIME [epoch: 17.4 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03087533679641724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03087533679641724 | validation: 0.05415086792243771]
	TIME [epoch: 17.3 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03477502654095551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03477502654095551 | validation: 0.04680984486727542]
	TIME [epoch: 17.4 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038056396532309404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038056396532309404 | validation: 0.056787194020194345]
	TIME [epoch: 17.4 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03814899704703133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03814899704703133 | validation: 0.04852167068863389]
	TIME [epoch: 17.4 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037003389561730854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037003389561730854 | validation: 0.06456894242822514]
	TIME [epoch: 17.4 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060777654242131045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060777654242131045 | validation: 0.14141613347732107]
	TIME [epoch: 17.4 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12160073669378614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12160073669378614 | validation: 0.09473855527419155]
	TIME [epoch: 17.4 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1095390888729188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1095390888729188 | validation: 0.055789479324119574]
	TIME [epoch: 17.4 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07217045896306248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07217045896306248 | validation: 0.04482520393832227]
	TIME [epoch: 17.4 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042906182080683614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042906182080683614 | validation: 0.030047438199082177]
	TIME [epoch: 17.4 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039337785382255915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039337785382255915 | validation: 0.02889037018839845]
	TIME [epoch: 17.4 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028379078848215375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028379078848215375 | validation: 0.024298706454019714]
	TIME [epoch: 17.4 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02302635201553274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02302635201553274 | validation: 0.03302306430127472]
	TIME [epoch: 17.3 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026765811485981415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026765811485981415 | validation: 0.03524554189169191]
	TIME [epoch: 17.4 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02913256842643884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02913256842643884 | validation: 0.0297667937782638]
	TIME [epoch: 17.3 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030949245770221796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030949245770221796 | validation: 0.05214033857821555]
	TIME [epoch: 17.4 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03770443069608186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03770443069608186 | validation: 0.04629964568092209]
	TIME [epoch: 19.3 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03937398776132159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03937398776132159 | validation: 0.08471545122232493]
	TIME [epoch: 17.4 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06929662109311145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06929662109311145 | validation: 0.05749593317116648]
	TIME [epoch: 17.4 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05729125313385602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05729125313385602 | validation: 0.05140883967956934]
	TIME [epoch: 17.4 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048153573108590905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048153573108590905 | validation: 0.034252905526283926]
	TIME [epoch: 17.4 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04569237976048226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04569237976048226 | validation: 0.04455714404514819]
	TIME [epoch: 17.4 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051607843191881816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051607843191881816 | validation: 0.055909073869681185]
	TIME [epoch: 17.4 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05205082393825799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05205082393825799 | validation: 0.030423621396611547]
	TIME [epoch: 17.4 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03357267326293668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03357267326293668 | validation: 0.02713210909591414]
	TIME [epoch: 17.4 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0263400749004607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0263400749004607 | validation: 0.03597584290818569]
	TIME [epoch: 17.4 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02952895658942828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02952895658942828 | validation: 0.029818544237959932]
	TIME [epoch: 17.3 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030480480947674762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030480480947674762 | validation: 0.025611483627542866]
	TIME [epoch: 17.4 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033542145077143294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033542145077143294 | validation: 0.033762667769986925]
	TIME [epoch: 17.3 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03644081835406387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03644081835406387 | validation: 0.026490607314506656]
	TIME [epoch: 17.4 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041480396653190345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041480396653190345 | validation: 0.10666201061255087]
	TIME [epoch: 17.3 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08799584881325483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08799584881325483 | validation: 0.10155126983309626]
	TIME [epoch: 17.4 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1164295399384517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1164295399384517 | validation: 0.16605119877563262]
	TIME [epoch: 17.4 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1303700643239638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1303700643239638 | validation: 0.0880518849712729]
	TIME [epoch: 17.4 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07122413808545883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07122413808545883 | validation: 0.020232698960121944]
	TIME [epoch: 17.3 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024831648846573007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024831648846573007 | validation: 0.02513528454067825]
	TIME [epoch: 17.4 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020820731995195994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020820731995195994 | validation: 0.028924713693776152]
	TIME [epoch: 17.3 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029067733800129225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029067733800129225 | validation: 0.038574015765598595]
	TIME [epoch: 17.3 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031266219812513006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031266219812513006 | validation: 0.022712542536198966]
	TIME [epoch: 17.3 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033396884511592964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033396884511592964 | validation: 0.05307665938233942]
	TIME [epoch: 17.4 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04292347497043358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04292347497043358 | validation: 0.0377990290021073]
	TIME [epoch: 17.4 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06345951663096122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06345951663096122 | validation: 0.04698063492923457]
	TIME [epoch: 17.3 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05005595161815382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05005595161815382 | validation: 0.05579915163143763]
	TIME [epoch: 17.4 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04631485302227014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04631485302227014 | validation: 0.04562376303121253]
	TIME [epoch: 17.4 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04134442356814469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04134442356814469 | validation: 0.05674727366722424]
	TIME [epoch: 17.3 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04816829376598331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04816829376598331 | validation: 0.07505005471084919]
	TIME [epoch: 17.4 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06379546368520238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06379546368520238 | validation: 0.03835278379564695]
	TIME [epoch: 17.3 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03778967661993411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03778967661993411 | validation: 0.025966619132796843]
	TIME [epoch: 17.4 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023944595938607646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023944595938607646 | validation: 0.020072637989861444]
	TIME [epoch: 17.4 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025461623621274784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025461623621274784 | validation: 0.04099792302463652]
	TIME [epoch: 17.4 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03554132757925694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03554132757925694 | validation: 0.0394181481910032]
	TIME [epoch: 17.3 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06042141837608412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06042141837608412 | validation: 0.08375761572906908]
	TIME [epoch: 17.4 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07801878068680233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07801878068680233 | validation: 0.05734469967361486]
	TIME [epoch: 17.4 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05555520175433915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05555520175433915 | validation: 0.06843771862592922]
	TIME [epoch: 17.4 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054508192779769725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054508192779769725 | validation: 0.06138209334318903]
	TIME [epoch: 17.4 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04257549639323633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04257549639323633 | validation: 0.0374789487357906]
	TIME [epoch: 17.4 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028115239106006513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028115239106006513 | validation: 0.022375091203183962]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240823_172217/states/model_phi1_3b_v_mmd1_1210.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 9435.081 seconds.
