Args:
Namespace(name='model_phi1_3a_v_mmd1', outdir='out/model_training/model_phi1_3a_v_mmd1', training_data='data/training_data/data_phi1_3a/training', validation_data='data/training_data/data_phi1_3a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2749618715

Training model...

Saving initial model state to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.44423212619724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.44423212619724 | validation: 5.8595102934955285]
	TIME [epoch: 26.3 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.252433513097415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.252433513097415 | validation: 5.0785993608563915]
	TIME [epoch: 0.989 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.775561155752684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.775561155752684 | validation: 6.548451703485117]
	TIME [epoch: 0.937 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.4384640010651015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.4384640010651015 | validation: 5.910287563005517]
	TIME [epoch: 0.934 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.794115412260785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.794115412260785 | validation: 5.140655969940351]
	TIME [epoch: 0.933 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.84504504307567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.84504504307567 | validation: 5.100344852422489]
	TIME [epoch: 0.934 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.378814438163297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.378814438163297 | validation: 4.91652734352584]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.076496521858306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.076496521858306 | validation: 4.787773455731547]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.147556715617592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.147556715617592 | validation: 4.811392886075196]
	TIME [epoch: 0.938 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9480532644549737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9480532644549737 | validation: 4.742421086527985]
	TIME [epoch: 0.941 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.904164549773068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.904164549773068 | validation: 4.597640906300355]
	TIME [epoch: 0.94 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8426804658889298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8426804658889298 | validation: 4.682910678852645]
	TIME [epoch: 0.937 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.78391885478369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.78391885478369 | validation: 4.556255918796478]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7212478669831617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7212478669831617 | validation: 4.5484844388930155]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.664596620325708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.664596620325708 | validation: 4.5129184787565775]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6167695132538142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6167695132538142 | validation: 4.475155420723971]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5681928811630104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5681928811630104 | validation: 4.432140344515569]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5615416258807957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5615416258807957 | validation: 4.57535727545679]
	TIME [epoch: 0.94 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8270445633118833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8270445633118833 | validation: 4.4200056369653]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.586703293382452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.586703293382452 | validation: 4.3272448763549844]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5738661141868944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5738661141868944 | validation: 4.53163980392147]
	TIME [epoch: 0.94 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5587918816961173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5587918816961173 | validation: 4.344953904349452]
	TIME [epoch: 0.937 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9018058033271634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9018058033271634 | validation: 4.238275349047579]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4433076751330383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4433076751330383 | validation: 4.566484152641947]
	TIME [epoch: 0.936 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6495839080368975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6495839080368975 | validation: 4.15776403142591]
	TIME [epoch: 0.939 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4940515103022323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4940515103022323 | validation: 4.178514899567475]
	TIME [epoch: 0.937 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3824002776961892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3824002776961892 | validation: 4.30097414191636]
	TIME [epoch: 0.934 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.393251661879862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.393251661879862 | validation: 4.140792499214117]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3283194428746485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3283194428746485 | validation: 4.139066248560953]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.296999948405786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.296999948405786 | validation: 4.174741497363097]
	TIME [epoch: 0.939 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.293524432137823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.293524432137823 | validation: 4.084291153348587]
	TIME [epoch: 0.94 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.26861484361424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.26861484361424 | validation: 4.11088064258075]
	TIME [epoch: 0.937 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2450017876229094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2450017876229094 | validation: 4.0971040316952365]
	TIME [epoch: 0.937 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2617106627184076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2617106627184076 | validation: 4.17932161661641]
	TIME [epoch: 0.934 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.523378348383342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.523378348383342 | validation: 4.064328711051647]
	TIME [epoch: 0.939 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2438163741177437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2438163741177437 | validation: 4.039957895559366]
	TIME [epoch: 0.944 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.198598305192078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.198598305192078 | validation: 3.994227834665803]
	TIME [epoch: 0.951 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1753615990255724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1753615990255724 | validation: 4.045236913394932]
	TIME [epoch: 0.939 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1876630830017807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1876630830017807 | validation: 3.9462912904762413]
	TIME [epoch: 0.939 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2178400078466347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2178400078466347 | validation: 4.1429175275169845]
	TIME [epoch: 0.939 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2803039304884534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2803039304884534 | validation: 3.888743638651617]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2435571910271053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2435571910271053 | validation: 4.001943979310581]
	TIME [epoch: 0.938 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.182070494975037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.182070494975037 | validation: 3.9618571962622315]
	TIME [epoch: 0.933 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.299797409336576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.299797409336576 | validation: 3.880628914410508]
	TIME [epoch: 0.933 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0855857754259364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0855857754259364 | validation: 3.9152242069758856]
	TIME [epoch: 0.934 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1016454490237666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1016454490237666 | validation: 3.8298440797078115]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.107834370636936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.107834370636936 | validation: 3.8535325029860736]
	TIME [epoch: 0.934 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0477688072485547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0477688072485547 | validation: 3.8252160355826526]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.031493289085882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.031493289085882 | validation: 3.7899657780730034]
	TIME [epoch: 0.946 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0278596710961607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0278596710961607 | validation: 3.831936309970338]
	TIME [epoch: 0.937 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.017582081447511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.017582081447511 | validation: 3.734864053678908]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.010455751350784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.010455751350784 | validation: 3.863475234746315]
	TIME [epoch: 0.935 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0430007693099514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0430007693099514 | validation: 3.715084627754027]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1506008886621437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1506008886621437 | validation: 3.795529492495646]
	TIME [epoch: 0.937 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.994929038580409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.994929038580409 | validation: 3.698161275114904]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9563769472596153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9563769472596153 | validation: 3.6733587261467786]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9464829820054614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9464829820054614 | validation: 3.718423375824219]
	TIME [epoch: 0.937 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9540316302245464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9540316302245464 | validation: 3.653833876633855]
	TIME [epoch: 0.933 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.99165721402901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.99165721402901 | validation: 3.678314848699511]
	TIME [epoch: 0.935 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.957791893317441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.957791893317441 | validation: 3.6493010871473026]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9318971446417117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9318971446417117 | validation: 3.606350520229224]
	TIME [epoch: 0.939 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9062972236249496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9062972236249496 | validation: 3.6716185088476196]
	TIME [epoch: 0.939 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9136304269076323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9136304269076323 | validation: 3.551483776051309]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.929012817475641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.929012817475641 | validation: 3.6765674972256934]
	TIME [epoch: 0.95 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9141505862901136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9141505862901136 | validation: 3.503815906950944]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.887387703165257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.887387703165257 | validation: 3.571773201275315]
	TIME [epoch: 0.938 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8437489903019753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8437489903019753 | validation: 3.5043247063568943]
	TIME [epoch: 0.938 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8172064464397013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8172064464397013 | validation: 3.5102778746417145]
	TIME [epoch: 0.936 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8078385243444344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8078385243444344 | validation: 3.4906856433037294]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7993613870354763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7993613870354763 | validation: 3.468275509657761]
	TIME [epoch: 0.939 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7893778131626834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7893778131626834 | validation: 3.4815379809548777]
	TIME [epoch: 0.939 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7869645296372108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7869645296372108 | validation: 3.4037118855106088]
	TIME [epoch: 0.948 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.831227751698234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.831227751698234 | validation: 3.7112557847331575]
	TIME [epoch: 0.942 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0108517261298644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0108517261298644 | validation: 3.4613095048658864]
	TIME [epoch: 0.938 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.039367103625774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.039367103625774 | validation: 3.405966830334304]
	TIME [epoch: 0.937 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.841115501582417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.841115501582417 | validation: 3.472906494371204]
	TIME [epoch: 0.937 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7920792457240915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7920792457240915 | validation: 3.4167764825652918]
	TIME [epoch: 0.934 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7611782984852007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7611782984852007 | validation: 3.3588276512272404]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.750348211724567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.750348211724567 | validation: 3.3312095808850644]
	TIME [epoch: 0.942 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.736266679430478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.736266679430478 | validation: 3.3409771600167346]
	TIME [epoch: 0.94 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7250427077157666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7250427077157666 | validation: 3.3449914182367433]
	TIME [epoch: 0.938 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7237549272415875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7237549272415875 | validation: 3.3217777049110633]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.709243641903395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.709243641903395 | validation: 3.29209668325727]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7011415927935385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7011415927935385 | validation: 3.3013360204976196]
	TIME [epoch: 0.934 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.704813563075735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.704813563075735 | validation: 3.304107239775368]
	TIME [epoch: 0.935 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.700340766741784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.700340766741784 | validation: 3.282005677101754]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7029048876683124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7029048876683124 | validation: 3.2693871519293163]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7460036050400753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7460036050400753 | validation: 3.421197332956571]
	TIME [epoch: 0.937 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.842012928688522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.842012928688522 | validation: 3.21375328121105]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6687833743861877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6687833743861877 | validation: 3.2280507478957094]
	TIME [epoch: 0.934 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6896465317899954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6896465317899954 | validation: 3.296774250241621]
	TIME [epoch: 0.936 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7119549214392693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7119549214392693 | validation: 3.1801164634322627]
	TIME [epoch: 0.933 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6608700569336383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6608700569336383 | validation: 3.195587899010656]
	TIME [epoch: 0.941 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.639147530116208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.639147530116208 | validation: 3.1978195156357274]
	TIME [epoch: 0.937 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.629189599932554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.629189599932554 | validation: 3.153939433235248]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6277833854315977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6277833854315977 | validation: 3.1550057166529184]
	TIME [epoch: 0.936 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.631461589385939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.631461589385939 | validation: 3.13267289300339]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.615918258996388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.615918258996388 | validation: 3.168278756789036]
	TIME [epoch: 0.936 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.620303799456706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.620303799456706 | validation: 3.125975394906928]
	TIME [epoch: 0.955 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.674616073955021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.674616073955021 | validation: 3.3070886272956077]
	TIME [epoch: 0.935 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7773195803237356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7773195803237356 | validation: 3.055371807807779]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5857966471827485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5857966471827485 | validation: 3.069707856608037]
	TIME [epoch: 0.939 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.589931006044563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.589931006044563 | validation: 2.561507622945142]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2023977950698486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2023977950698486 | validation: 2.8374563152448173]
	TIME [epoch: 0.938 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4473162152992725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4473162152992725 | validation: 1.668783076688252]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8940057936838401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8940057936838401 | validation: 3.4326603846074373]
	TIME [epoch: 0.937 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3087382958175535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3087382958175535 | validation: 1.8972452484417]
	TIME [epoch: 0.942 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9375066051763434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9375066051763434 | validation: 1.6020923417814346]
	TIME [epoch: 0.939 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6142611182838942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6142611182838942 | validation: 1.5091918448002042]
	TIME [epoch: 0.948 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.524283077434617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.524283077434617 | validation: 1.3527864534504803]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3847022260110091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3847022260110091 | validation: 1.3008057677447245]
	TIME [epoch: 0.94 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2923564072317137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2923564072317137 | validation: 1.2057974229607586]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2159101199495936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2159101199495936 | validation: 1.1419053371751635]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1902830852250565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1902830852250565 | validation: 1.2784485315977654]
	TIME [epoch: 0.935 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.253252630831994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.253252630831994 | validation: 1.1684588748748617]
	TIME [epoch: 0.933 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1677579363417623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1677579363417623 | validation: 1.074433221521257]
	TIME [epoch: 0.933 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.129689859162618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.129689859162618 | validation: 2.041771252890775]
	TIME [epoch: 0.934 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6513737161753146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6513737161753146 | validation: 1.1919950391995202]
	TIME [epoch: 0.933 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5031300860605625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5031300860605625 | validation: 1.4381919352356152]
	TIME [epoch: 1.18 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3815913582517554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3815913582517554 | validation: 1.2756619438632226]
	TIME [epoch: 0.939 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2854367524083388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2854367524083388 | validation: 1.0273142419162864]
	TIME [epoch: 0.94 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1691523161394701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1691523161394701 | validation: 1.0643530817046822]
	TIME [epoch: 0.942 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0939483449366951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0939483449366951 | validation: 1.0921731856273558]
	TIME [epoch: 0.946 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0773008286245032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0773008286245032 | validation: 1.0554744728701426]
	TIME [epoch: 0.935 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0751176087556837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0751176087556837 | validation: 0.9839019345034132]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0509561756602737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0509561756602737 | validation: 0.9771835357643628]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0291210442683782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0291210442683782 | validation: 0.9617990573340507]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0188644258410897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0188644258410897 | validation: 0.9800475505735016]
	TIME [epoch: 0.936 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0192871942273414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0192871942273414 | validation: 0.9332026784461388]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_129.pth
	Model improved!!!
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.000857577599095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.000857577599095 | validation: 0.9206895769425745]
	TIME [epoch: 0.941 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9889339355451446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9889339355451446 | validation: 0.8851118356030899]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.012488468444365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.012488468444365 | validation: 1.1833794972299985]
	TIME [epoch: 0.941 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1291266449757498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1291266449757498 | validation: 0.9614227475501992]
	TIME [epoch: 0.939 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.024065101279449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.024065101279449 | validation: 0.899243527212068]
	TIME [epoch: 0.935 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0936534331860275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0936534331860275 | validation: 1.4024183938251915]
	TIME [epoch: 0.935 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3112219930455873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3112219930455873 | validation: 1.0120689264850107]
	TIME [epoch: 0.934 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.000735936693258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.000735936693258 | validation: 1.088249472312766]
	TIME [epoch: 0.936 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.282105994264734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.282105994264734 | validation: 0.9240234085298811]
	TIME [epoch: 0.938 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9557948655568036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9557948655568036 | validation: 0.9009630756146546]
	TIME [epoch: 0.935 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9520655842560096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9520655842560096 | validation: 0.8703209694160805]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9409509183604752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9409509183604752 | validation: 0.893627757732121]
	TIME [epoch: 0.938 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9238244289716235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9238244289716235 | validation: 0.9023718846795479]
	TIME [epoch: 0.935 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9204640818808261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9204640818808261 | validation: 0.881633901540457]
	TIME [epoch: 0.934 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9130734783492592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9130734783492592 | validation: 0.9162734985674806]
	TIME [epoch: 0.935 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8996221497143244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8996221497143244 | validation: 0.8451544254403205]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8997438728125449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8997438728125449 | validation: 0.9476699827677204]
	TIME [epoch: 0.949 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9147318356618772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9147318356618772 | validation: 0.8560745259613818]
	TIME [epoch: 0.935 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9594860242755163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9594860242755163 | validation: 1.1813550797642545]
	TIME [epoch: 0.934 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0609342982536238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0609342982536238 | validation: 0.8899970934352149]
	TIME [epoch: 0.936 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9905421572303876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9905421572303876 | validation: 0.9087974069627638]
	TIME [epoch: 0.936 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8878608710712148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8878608710712148 | validation: 0.8264106916492129]
	TIME [epoch: 0.933 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_151.pth
	Model improved!!!
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8574845642504616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8574845642504616 | validation: 0.8668669299916861]
	TIME [epoch: 0.937 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8657739921780359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8657739921780359 | validation: 0.8471775004725584]
	TIME [epoch: 0.94 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9033573172402848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9033573172402848 | validation: 0.943086076521405]
	TIME [epoch: 0.936 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9081401341239672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9081401341239672 | validation: 0.8013855602611127]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8381531663442476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8381531663442476 | validation: 0.8298828186997569]
	TIME [epoch: 0.938 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8185806300789551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8185806300789551 | validation: 0.8004223438180476]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8221814326824752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8221814326824752 | validation: 0.8803528019217395]
	TIME [epoch: 0.94 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8669371676478375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8669371676478375 | validation: 0.9173326623046698]
	TIME [epoch: 0.939 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9682424992458554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9682424992458554 | validation: 0.9611213676922887]
	TIME [epoch: 0.939 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9115218786333804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9115218786333804 | validation: 0.8605025157803177]
	TIME [epoch: 0.935 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8623145752277586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8623145752277586 | validation: 0.9036040419808844]
	TIME [epoch: 0.933 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.848379445633438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.848379445633438 | validation: 0.8042968584215132]
	TIME [epoch: 0.933 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8297260673713719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8297260673713719 | validation: 0.8536127191679438]
	TIME [epoch: 0.934 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.830077093024778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.830077093024778 | validation: 0.789526326400558]
	TIME [epoch: 0.933 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_165.pth
	Model improved!!!
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.810812357927413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.810812357927413 | validation: 0.8218397910624772]
	TIME [epoch: 0.941 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8090635307814782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8090635307814782 | validation: 0.8065640935395422]
	TIME [epoch: 0.941 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7953650077757558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7953650077757558 | validation: 0.8300149784016227]
	TIME [epoch: 0.943 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8030710870090253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8030710870090253 | validation: 0.8101400128004418]
	TIME [epoch: 0.942 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7986389124592339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7986389124592339 | validation: 0.835647809451168]
	TIME [epoch: 0.943 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8227546074989971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8227546074989971 | validation: 0.7938770799698718]
	TIME [epoch: 0.941 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8105563139591955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8105563139591955 | validation: 0.8344413898004653]
	TIME [epoch: 0.943 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8175723235178294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8175723235178294 | validation: 0.7875200889915766]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7856118301977599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7856118301977599 | validation: 0.8465846581600762]
	TIME [epoch: 0.937 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8244781043113474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8244781043113474 | validation: 1.3796800334772021]
	TIME [epoch: 0.938 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1730647703783117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1730647703783117 | validation: 0.8540304083364192]
	TIME [epoch: 0.938 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8538018836923934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8538018836923934 | validation: 0.8100202915852139]
	TIME [epoch: 0.933 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7694324871801783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7694324871801783 | validation: 0.7493227955558317]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_178.pth
	Model improved!!!
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7483495219227487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7483495219227487 | validation: 0.7420062088530428]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7525996173604784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7525996173604784 | validation: 0.7568742035802805]
	TIME [epoch: 0.937 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7608931588094103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7608931588094103 | validation: 0.7734491332434009]
	TIME [epoch: 0.935 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8013907631633541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8013907631633541 | validation: 0.8561749815993794]
	TIME [epoch: 0.932 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.859099317376667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.859099317376667 | validation: 0.8405618916592075]
	TIME [epoch: 0.933 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8559036630458832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8559036630458832 | validation: 0.7577290984515277]
	TIME [epoch: 0.932 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.750298049757665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.750298049757665 | validation: 0.9393707858620677]
	TIME [epoch: 0.932 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8446573700012928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8446573700012928 | validation: 0.8844069181891311]
	TIME [epoch: 0.931 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.872838508694799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.872838508694799 | validation: 0.8156176549697229]
	TIME [epoch: 0.936 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.757723477323014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.757723477323014 | validation: 0.7331499997921184]
	TIME [epoch: 0.944 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7250349011913906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7250349011913906 | validation: 0.7274343281000446]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_189.pth
	Model improved!!!
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7543991712662598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7543991712662598 | validation: 0.7849368194357504]
	TIME [epoch: 0.939 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.805254613922086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.805254613922086 | validation: 0.8843801032332941]
	TIME [epoch: 0.937 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8676239447585289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8676239447585289 | validation: 0.818403418035285]
	TIME [epoch: 0.934 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8363229963175752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8363229963175752 | validation: 0.8174975463823739]
	TIME [epoch: 0.933 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7543708882947867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7543708882947867 | validation: 0.7079779137351466]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7007155580815695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7007155580815695 | validation: 0.6834634144574038]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7017469784162939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7017469784162939 | validation: 0.7417504851743927]
	TIME [epoch: 0.935 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7136258835428625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7136258835428625 | validation: 0.7473212481433432]
	TIME [epoch: 0.934 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7638349266040539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7638349266040539 | validation: 0.9068357980795828]
	TIME [epoch: 0.933 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8923183299449323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8923183299449323 | validation: 0.8810776395531605]
	TIME [epoch: 0.935 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8774071324814344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8774071324814344 | validation: 0.7904830071328408]
	TIME [epoch: 0.937 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.733450269659504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.733450269659504 | validation: 0.6669268303280846]
	TIME [epoch: 24.3 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6739209861248958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6739209861248958 | validation: 0.6530865334934037]
	TIME [epoch: 1.85 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6925837406927952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6925837406927952 | validation: 0.720546137138934]
	TIME [epoch: 1.84 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6915195580936773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6915195580936773 | validation: 0.7045517109559415]
	TIME [epoch: 1.84 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7022461789577216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7022461789577216 | validation: 0.7123544558949183]
	TIME [epoch: 1.84 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7362281299357832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7362281299357832 | validation: 0.7554343002049247]
	TIME [epoch: 1.84 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7737329947357443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7737329947357443 | validation: 0.7177929518326404]
	TIME [epoch: 1.86 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7395101412792675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7395101412792675 | validation: 0.6887105451612149]
	TIME [epoch: 1.84 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6809725264706472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6809725264706472 | validation: 0.7858373562425354]
	TIME [epoch: 1.84 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7295843125182829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7295843125182829 | validation: 0.7643187065435587]
	TIME [epoch: 1.84 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7852204349468016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7852204349468016 | validation: 0.7444211682383908]
	TIME [epoch: 1.84 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.691187103890667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.691187103890667 | validation: 0.590362356088777]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6398316082020964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6398316082020964 | validation: 0.6186545236161192]
	TIME [epoch: 1.84 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6254577442682708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6254577442682708 | validation: 0.6475061061484091]
	TIME [epoch: 1.84 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6158564264289055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6158564264289055 | validation: 0.5575971574636922]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_215.pth
	Model improved!!!
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6502177600423258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6502177600423258 | validation: 0.8234702147632043]
	TIME [epoch: 1.84 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7791259027304304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7791259027304304 | validation: 0.7059181708385558]
	TIME [epoch: 1.84 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8097097834682854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8097097834682854 | validation: 0.6789451551441613]
	TIME [epoch: 1.84 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7264803870845671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7264803870845671 | validation: 0.8144715340525495]
	TIME [epoch: 1.84 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7140693309820142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7140693309820142 | validation: 0.5828207700371081]
	TIME [epoch: 1.84 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5776501829078271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5776501829078271 | validation: 0.4840832559881847]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5815106439236367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5815106439236367 | validation: 0.7886575681610932]
	TIME [epoch: 1.84 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6958996129610117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6958996129610117 | validation: 0.5253904978854689]
	TIME [epoch: 1.85 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5498716143672638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5498716143672638 | validation: 0.5136439964708464]
	TIME [epoch: 1.84 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5179638202249427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5179638202249427 | validation: 0.5421989450843137]
	TIME [epoch: 1.84 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5146334079645183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5146334079645183 | validation: 0.5867904540159494]
	TIME [epoch: 1.84 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6128682255731568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6128682255731568 | validation: 0.7881450578808891]
	TIME [epoch: 1.85 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7004371974279328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7004371974279328 | validation: 0.5952891455599785]
	TIME [epoch: 1.84 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6264186326515514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6264186326515514 | validation: 0.634274963274128]
	TIME [epoch: 1.84 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7312598839908541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7312598839908541 | validation: 1.2023638436156356]
	TIME [epoch: 1.84 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0281739832319665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0281739832319665 | validation: 0.7045761643866009]
	TIME [epoch: 1.85 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5935893637389503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5935893637389503 | validation: 0.6224778096897933]
	TIME [epoch: 1.84 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6763618440101467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6763618440101467 | validation: 0.5173404634677015]
	TIME [epoch: 1.84 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48433795399745466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48433795399745466 | validation: 0.580416438692687]
	TIME [epoch: 1.85 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5320611634299383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5320611634299383 | validation: 0.48345947971745434]
	TIME [epoch: 1.85 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_235.pth
	Model improved!!!
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46772648281269386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46772648281269386 | validation: 0.4748619382371854]
	TIME [epoch: 1.85 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4469942859377837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4469942859377837 | validation: 0.4602630963455613]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_237.pth
	Model improved!!!
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4163386714365011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4163386714365011 | validation: 0.4283848515080255]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_238.pth
	Model improved!!!
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3990014816633736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3990014816633736 | validation: 0.43837736282170847]
	TIME [epoch: 1.84 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3949873636168082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3949873636168082 | validation: 0.5028817748673833]
	TIME [epoch: 1.84 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5190833569327785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5190833569327785 | validation: 1.072384121788177]
	TIME [epoch: 1.84 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9014870507409875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9014870507409875 | validation: 0.5125498807104919]
	TIME [epoch: 1.84 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5489501237464323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5489501237464323 | validation: 0.5392550743954222]
	TIME [epoch: 1.84 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5193884802070408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5193884802070408 | validation: 0.5323251049964017]
	TIME [epoch: 1.84 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5302775452648928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5302775452648928 | validation: 0.6348909087320844]
	TIME [epoch: 1.84 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5976654084071472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5976654084071472 | validation: 0.5702831676580067]
	TIME [epoch: 1.84 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5982407612411266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5982407612411266 | validation: 0.6525106814256387]
	TIME [epoch: 1.84 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5009779062471141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5009779062471141 | validation: 0.47355880215770113]
	TIME [epoch: 1.83 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42654154726241744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42654154726241744 | validation: 0.5340248302845422]
	TIME [epoch: 1.84 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5502629412153088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5502629412153088 | validation: 0.4594608737945824]
	TIME [epoch: 1.84 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3792083263155718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3792083263155718 | validation: 0.5258112661771633]
	TIME [epoch: 1.84 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47320454756784275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47320454756784275 | validation: 0.7670166431568083]
	TIME [epoch: 1.84 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6829699564229634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6829699564229634 | validation: 0.43567151307861995]
	TIME [epoch: 1.84 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4731614054437768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4731614054437768 | validation: 0.6436957269247485]
	TIME [epoch: 1.84 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48447546439279393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48447546439279393 | validation: 0.45271419749628133]
	TIME [epoch: 1.84 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3860409873830173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3860409873830173 | validation: 0.43638024789473673]
	TIME [epoch: 1.84 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.444733475276574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.444733475276574 | validation: 0.523408810759422]
	TIME [epoch: 1.84 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4093752853401442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4093752853401442 | validation: 0.4585750793671995]
	TIME [epoch: 1.84 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.387393164696492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.387393164696492 | validation: 0.5422514265555979]
	TIME [epoch: 1.85 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47261309532521917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47261309532521917 | validation: 0.44200246109807806]
	TIME [epoch: 1.84 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.364746463571626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.364746463571626 | validation: 0.44841821623561434]
	TIME [epoch: 1.84 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3578220877804792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3578220877804792 | validation: 0.4459210073736099]
	TIME [epoch: 1.83 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37920083394290743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37920083394290743 | validation: 0.5710392474232123]
	TIME [epoch: 1.84 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47056497737859415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47056497737859415 | validation: 0.513834014842084]
	TIME [epoch: 1.84 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4990249603171958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4990249603171958 | validation: 0.4897235407858144]
	TIME [epoch: 1.84 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41212210992923487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41212210992923487 | validation: 0.4826123427874347]
	TIME [epoch: 1.84 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38076966985068367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38076966985068367 | validation: 0.43974798982625923]
	TIME [epoch: 1.84 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4307755186274133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4307755186274133 | validation: 0.6817830192838821]
	TIME [epoch: 1.84 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5647802317064151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5647802317064151 | validation: 0.5763420912296557]
	TIME [epoch: 1.84 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4660711004583208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4660711004583208 | validation: 0.35260517048479745]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_270.pth
	Model improved!!!
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3083253027159592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3083253027159592 | validation: 0.41509376307542856]
	TIME [epoch: 1.84 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3221131624807776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3221131624807776 | validation: 0.46483065950397817]
	TIME [epoch: 1.84 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4027782699384354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4027782699384354 | validation: 0.6704680717747292]
	TIME [epoch: 1.84 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.530274312464893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.530274312464893 | validation: 1.0134418361295936]
	TIME [epoch: 1.84 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8464741561160836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8464741561160836 | validation: 0.5246187881489534]
	TIME [epoch: 1.84 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5360507891246266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5360507891246266 | validation: 0.4664231531719824]
	TIME [epoch: 1.85 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.388929151381325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.388929151381325 | validation: 0.5145093639644909]
	TIME [epoch: 1.85 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3769133188601548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3769133188601548 | validation: 0.35030837211774996]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_278.pth
	Model improved!!!
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2905407407016313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2905407407016313 | validation: 0.3855133324776747]
	TIME [epoch: 1.83 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29201397690180125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29201397690180125 | validation: 0.3270894253591829]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_280.pth
	Model improved!!!
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2645863108676251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2645863108676251 | validation: 0.38882253484908624]
	TIME [epoch: 1.84 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.304768199268019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.304768199268019 | validation: 0.4367407923752724]
	TIME [epoch: 1.85 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40134902902000913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40134902902000913 | validation: 0.6476793393139876]
	TIME [epoch: 1.84 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5501583744562667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5501583744562667 | validation: 0.7554010455702379]
	TIME [epoch: 1.83 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7558526602707986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7558526602707986 | validation: 0.6536128982690415]
	TIME [epoch: 1.83 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5216593932045224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5216593932045224 | validation: 0.36866488705709377]
	TIME [epoch: 1.83 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35883774445738925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35883774445738925 | validation: 0.51716872192488]
	TIME [epoch: 1.83 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41280775110130946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41280775110130946 | validation: 0.44926972493034634]
	TIME [epoch: 1.83 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3518571162841077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3518571162841077 | validation: 0.37722727383387594]
	TIME [epoch: 1.84 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33944353373910574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33944353373910574 | validation: 0.34248823698382713]
	TIME [epoch: 1.84 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2585529990511941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2585529990511941 | validation: 0.3112921068690988]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2341789142011299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2341789142011299 | validation: 0.28897758905619364]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_292.pth
	Model improved!!!
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.233744647749325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.233744647749325 | validation: 0.5073473794712298]
	TIME [epoch: 1.84 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3582917403455781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3582917403455781 | validation: 0.562695942047525]
	TIME [epoch: 1.84 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4810998718560992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4810998718560992 | validation: 0.4989592370312883]
	TIME [epoch: 1.84 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5252193957354773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5252193957354773 | validation: 0.6606217867964587]
	TIME [epoch: 1.84 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4843673640297661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4843673640297661 | validation: 0.35788348255968966]
	TIME [epoch: 1.84 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26928892671198607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26928892671198607 | validation: 0.31875881066559414]
	TIME [epoch: 1.84 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29592549159214293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29592549159214293 | validation: 0.4040046726530306]
	TIME [epoch: 1.84 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3091267600435662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3091267600435662 | validation: 0.4814470952396083]
	TIME [epoch: 1.84 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3668235142747885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3668235142747885 | validation: 0.4068106953383362]
	TIME [epoch: 1.84 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3475238744724903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3475238744724903 | validation: 0.3575966790098682]
	TIME [epoch: 1.84 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3003666677486022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3003666677486022 | validation: 0.677951832306168]
	TIME [epoch: 1.84 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.503838478277781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.503838478277781 | validation: 0.4037451742125546]
	TIME [epoch: 1.84 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3082201802638474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3082201802638474 | validation: 0.47911387254194904]
	TIME [epoch: 1.85 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46610003293667507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46610003293667507 | validation: 0.2554107665564341]
	TIME [epoch: 1.85 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_306.pth
	Model improved!!!
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2516589121545076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2516589121545076 | validation: 0.44180463311854096]
	TIME [epoch: 1.84 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3607616869299352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3607616869299352 | validation: 0.3344849447807178]
	TIME [epoch: 1.84 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27234672374240015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27234672374240015 | validation: 0.29477055497712706]
	TIME [epoch: 1.83 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2538407088918968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2538407088918968 | validation: 0.45705725087607274]
	TIME [epoch: 1.84 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3255182727076909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3255182727076909 | validation: 0.3332991803986962]
	TIME [epoch: 1.84 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2791772417189195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2791772417189195 | validation: 0.39643907488515395]
	TIME [epoch: 1.84 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3075223089733201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3075223089733201 | validation: 0.4497598257464457]
	TIME [epoch: 1.84 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37091756588940156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37091756588940156 | validation: 0.4676828242408094]
	TIME [epoch: 1.83 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3682621629318395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3682621629318395 | validation: 0.2657970249460906]
	TIME [epoch: 1.84 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21687304124439613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21687304124439613 | validation: 0.38646929520018936]
	TIME [epoch: 1.84 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28851848115960854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28851848115960854 | validation: 0.48179988462567985]
	TIME [epoch: 1.84 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4013033343625155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4013033343625155 | validation: 0.37987219573998315]
	TIME [epoch: 1.84 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2634264733438323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2634264733438323 | validation: 0.45694950742501145]
	TIME [epoch: 1.84 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37494868755783395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37494868755783395 | validation: 0.2811441692141393]
	TIME [epoch: 1.84 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26511919513782334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26511919513782334 | validation: 0.31803958714035135]
	TIME [epoch: 1.84 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2348536031990764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2348536031990764 | validation: 0.21156334477501326]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_322.pth
	Model improved!!!
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18774948990433415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18774948990433415 | validation: 0.29337368290844934]
	TIME [epoch: 1.84 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1863552510372847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1863552510372847 | validation: 0.2418395607880265]
	TIME [epoch: 1.84 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2326943391062575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2326943391062575 | validation: 0.3892390525084402]
	TIME [epoch: 1.83 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2784530572058626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2784530572058626 | validation: 0.2861366163614611]
	TIME [epoch: 1.83 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2060043170281718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2060043170281718 | validation: 0.5269397191395385]
	TIME [epoch: 1.84 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4351628518910111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4351628518910111 | validation: 0.5710445723932841]
	TIME [epoch: 1.84 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6124217796265999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6124217796265999 | validation: 0.46746656761871375]
	TIME [epoch: 1.84 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34860139819609565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34860139819609565 | validation: 0.42596510440272883]
	TIME [epoch: 1.85 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3588853341214236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3588853341214236 | validation: 0.38000972908355124]
	TIME [epoch: 1.84 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.301610415428491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.301610415428491 | validation: 0.29971165790499565]
	TIME [epoch: 1.83 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2618087756375553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2618087756375553 | validation: 0.32409200833752516]
	TIME [epoch: 1.84 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2728151617471536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2728151617471536 | validation: 0.32933613622038527]
	TIME [epoch: 1.83 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23042415811241246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23042415811241246 | validation: 0.23074321707467488]
	TIME [epoch: 1.84 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2130516051472564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2130516051472564 | validation: 0.2747699602556723]
	TIME [epoch: 1.84 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19336557800434762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19336557800434762 | validation: 0.27195122422095747]
	TIME [epoch: 1.84 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1985002699793861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1985002699793861 | validation: 0.4426148469354235]
	TIME [epoch: 1.84 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33619941855372887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33619941855372887 | validation: 0.34682983601988965]
	TIME [epoch: 1.83 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28959447791047943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28959447791047943 | validation: 0.37820588757497603]
	TIME [epoch: 1.84 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29724260581110523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29724260581110523 | validation: 0.27249846799225147]
	TIME [epoch: 1.84 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2272913093380231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2272913093380231 | validation: 0.28599050873900816]
	TIME [epoch: 1.83 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20890436848694058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20890436848694058 | validation: 0.26355389740793544]
	TIME [epoch: 1.84 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18574024663796745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18574024663796745 | validation: 0.25648380482314537]
	TIME [epoch: 1.83 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20072776898895672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20072776898895672 | validation: 0.34805821918944885]
	TIME [epoch: 1.84 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2852696240577042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2852696240577042 | validation: 0.47194448128273525]
	TIME [epoch: 1.83 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3474257110038181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3474257110038181 | validation: 0.2783390225224977]
	TIME [epoch: 1.84 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19156173076589525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19156173076589525 | validation: 0.620580394080394]
	TIME [epoch: 1.83 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5467781666098263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5467781666098263 | validation: 0.24319411530203028]
	TIME [epoch: 1.84 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23320249668892545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23320249668892545 | validation: 0.3514743855390372]
	TIME [epoch: 1.84 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3119085693506204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3119085693506204 | validation: 0.32846136021347694]
	TIME [epoch: 1.84 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24991288389776045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24991288389776045 | validation: 0.2870791035667959]
	TIME [epoch: 1.84 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21669413446809396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21669413446809396 | validation: 0.22252500269364686]
	TIME [epoch: 1.84 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2108696837155369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2108696837155369 | validation: 0.2469453427406262]
	TIME [epoch: 1.85 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17591029628215346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17591029628215346 | validation: 0.19977532487986038]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16293519514885013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16293519514885013 | validation: 0.25398820718433235]
	TIME [epoch: 1.84 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16903151597872232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16903151597872232 | validation: 0.27498962885174605]
	TIME [epoch: 1.84 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21876084523330735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21876084523330735 | validation: 0.4303404490141481]
	TIME [epoch: 1.84 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.339319097654131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.339319097654131 | validation: 0.6745004790769382]
	TIME [epoch: 1.84 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5372599208649812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5372599208649812 | validation: 0.23890455654112774]
	TIME [epoch: 1.83 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23162120603938058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23162120603938058 | validation: 0.2381388415287142]
	TIME [epoch: 1.84 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19105772920810374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19105772920810374 | validation: 0.40123478001672574]
	TIME [epoch: 1.84 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30401816100896273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30401816100896273 | validation: 0.23402172241343966]
	TIME [epoch: 1.84 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19022537662244113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19022537662244113 | validation: 0.4794373772021654]
	TIME [epoch: 1.84 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3735078157365922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3735078157365922 | validation: 0.348710859929355]
	TIME [epoch: 1.84 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2889526827708839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2889526827708839 | validation: 0.24005799077833273]
	TIME [epoch: 1.84 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16485292538752017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16485292538752017 | validation: 0.18767136418363034]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_367.pth
	Model improved!!!
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17099423349430254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17099423349430254 | validation: 0.31461738701658604]
	TIME [epoch: 1.84 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.231924618845827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.231924618845827 | validation: 0.18014325897275751]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_369.pth
	Model improved!!!
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11238392869761704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11238392869761704 | validation: 0.2508441360025757]
	TIME [epoch: 1.85 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1902265067948536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1902265067948536 | validation: 0.2978558886672706]
	TIME [epoch: 1.85 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26095049374589707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26095049374589707 | validation: 0.44735783350686226]
	TIME [epoch: 1.85 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36415653857323177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36415653857323177 | validation: 0.29511127459249753]
	TIME [epoch: 1.84 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.258296387999829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.258296387999829 | validation: 0.24022644209274924]
	TIME [epoch: 1.84 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16530565993159857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16530565993159857 | validation: 0.328880111544947]
	TIME [epoch: 1.84 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2881070861267672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2881070861267672 | validation: 0.53283259669833]
	TIME [epoch: 1.84 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3904514835663058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3904514835663058 | validation: 0.29882423327012536]
	TIME [epoch: 1.84 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2481240154739309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2481240154739309 | validation: 0.20008615436245336]
	TIME [epoch: 1.85 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13279287626307096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13279287626307096 | validation: 0.1788120486589905]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_379.pth
	Model improved!!!
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12993919582032634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12993919582032634 | validation: 0.2404928133162507]
	TIME [epoch: 1.84 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1493493260665803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1493493260665803 | validation: 0.2516264805949646]
	TIME [epoch: 1.84 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18763811779635334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18763811779635334 | validation: 0.34853582080324935]
	TIME [epoch: 1.85 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2555239044459397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2555239044459397 | validation: 0.3434330012842039]
	TIME [epoch: 1.84 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31692900493741877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31692900493741877 | validation: 0.294868489667318]
	TIME [epoch: 1.84 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20623823864097834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20623823864097834 | validation: 0.19345841937712127]
	TIME [epoch: 1.84 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1418706018320419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1418706018320419 | validation: 0.1965160738034457]
	TIME [epoch: 1.85 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13272756433943309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13272756433943309 | validation: 0.22096797111703734]
	TIME [epoch: 1.85 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1498310580969197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1498310580969197 | validation: 0.258797485967988]
	TIME [epoch: 1.85 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17929242046060975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17929242046060975 | validation: 0.2888758030498833]
	TIME [epoch: 1.85 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23719495351404124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23719495351404124 | validation: 0.3034334020682383]
	TIME [epoch: 1.84 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21069564921847644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21069564921847644 | validation: 0.21827377159822695]
	TIME [epoch: 1.84 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1511645426985085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1511645426985085 | validation: 0.21036369798300805]
	TIME [epoch: 1.84 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1477156348131434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1477156348131434 | validation: 0.219732802569263]
	TIME [epoch: 1.85 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1669441859882264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1669441859882264 | validation: 0.29204525460352326]
	TIME [epoch: 1.84 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23073965520797862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23073965520797862 | validation: 0.2957094513905753]
	TIME [epoch: 1.84 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.282287513750068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.282287513750068 | validation: 0.4184161607207282]
	TIME [epoch: 1.84 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32074242760832966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32074242760832966 | validation: 0.3018944783474572]
	TIME [epoch: 1.84 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2346470554135265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2346470554135265 | validation: 0.17660968106030772]
	TIME [epoch: 1.85 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_398.pth
	Model improved!!!
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10771813064297553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10771813064297553 | validation: 0.22087601607891294]
	TIME [epoch: 1.84 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19641340326398626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19641340326398626 | validation: 0.23321070421253323]
	TIME [epoch: 1.84 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16390331965174773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16390331965174773 | validation: 0.15216541960024463]
	TIME [epoch: 1.85 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_401.pth
	Model improved!!!
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08828742145525588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08828742145525588 | validation: 0.16641158660249655]
	TIME [epoch: 1.84 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1251514921092629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1251514921092629 | validation: 0.3771454878580842]
	TIME [epoch: 1.84 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2532870258083702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2532870258083702 | validation: 0.4547811386811176]
	TIME [epoch: 1.84 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38963232779913937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38963232779913937 | validation: 0.3354213515769754]
	TIME [epoch: 1.84 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22060892855777872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22060892855777872 | validation: 0.19815475786193504]
	TIME [epoch: 1.84 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1498212992905574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1498212992905574 | validation: 0.23901627372460657]
	TIME [epoch: 1.84 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19668685320064336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19668685320064336 | validation: 0.20321822288811897]
	TIME [epoch: 1.85 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20368678631689896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20368678631689896 | validation: 0.16616521331528356]
	TIME [epoch: 1.84 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1112264657514014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1112264657514014 | validation: 0.12896415535627123]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_410.pth
	Model improved!!!
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09369493903431764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09369493903431764 | validation: 0.15867241275622312]
	TIME [epoch: 1.84 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10026257507316597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10026257507316597 | validation: 0.23467322109963146]
	TIME [epoch: 1.84 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18827848712380088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18827848712380088 | validation: 0.37874116113101863]
	TIME [epoch: 1.84 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2922881013416726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2922881013416726 | validation: 0.3601274126898288]
	TIME [epoch: 1.85 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3485141689532656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3485141689532656 | validation: 0.2999525949739441]
	TIME [epoch: 1.84 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21584959503590795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21584959503590795 | validation: 0.21592003704169122]
	TIME [epoch: 1.84 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1624093226217996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1624093226217996 | validation: 0.18575030412433616]
	TIME [epoch: 1.85 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14535995549596337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14535995549596337 | validation: 0.20473038923691111]
	TIME [epoch: 1.84 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1285904844676571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1285904844676571 | validation: 0.1357873591605044]
	TIME [epoch: 1.85 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1128343780831291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1128343780831291 | validation: 0.170261283718169]
	TIME [epoch: 1.84 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11951703430317456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11951703430317456 | validation: 0.1475912864125278]
	TIME [epoch: 1.84 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11696651362307972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11696651362307972 | validation: 0.15674765952079592]
	TIME [epoch: 1.84 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09731165427435415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09731165427435415 | validation: 0.1547995250799387]
	TIME [epoch: 1.84 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10673527398574162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10673527398574162 | validation: 0.23795965611025183]
	TIME [epoch: 1.85 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19032196070238308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19032196070238308 | validation: 0.44696262493946315]
	TIME [epoch: 1.86 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3454321034706543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3454321034706543 | validation: 0.3080720547827467]
	TIME [epoch: 1.84 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3315962857392385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3315962857392385 | validation: 0.20813873312787232]
	TIME [epoch: 1.84 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13825568012251777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13825568012251777 | validation: 0.14501191804909708]
	TIME [epoch: 1.84 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10566378940635666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10566378940635666 | validation: 0.1897922896180037]
	TIME [epoch: 1.84 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12986721124851147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12986721124851147 | validation: 0.24780348048425266]
	TIME [epoch: 1.85 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1820104716483087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1820104716483087 | validation: 0.16990108412870583]
	TIME [epoch: 1.85 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11259871411708405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11259871411708405 | validation: 0.16013111400735114]
	TIME [epoch: 1.85 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16357034659339909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16357034659339909 | validation: 0.25149980627784524]
	TIME [epoch: 1.84 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18995602130984657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18995602130984657 | validation: 0.22441112533218854]
	TIME [epoch: 1.84 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20869361301197387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20869361301197387 | validation: 0.32372367358139653]
	TIME [epoch: 1.84 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23151433614508712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23151433614508712 | validation: 0.29615453281372744]
	TIME [epoch: 1.85 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21747858639849824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21747858639849824 | validation: 0.2181546155098517]
	TIME [epoch: 1.85 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17181260808174717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17181260808174717 | validation: 0.15716181851250927]
	TIME [epoch: 1.84 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10732310096326814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10732310096326814 | validation: 0.118697915732704]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_439.pth
	Model improved!!!
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11073967577324523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11073967577324523 | validation: 0.12382977904332998]
	TIME [epoch: 1.84 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09328104896226226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09328104896226226 | validation: 0.11486467666291744]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_441.pth
	Model improved!!!
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07799010668947476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07799010668947476 | validation: 0.10762435732034498]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_442.pth
	Model improved!!!
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07341450158358907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07341450158358907 | validation: 0.14249305854165467]
	TIME [epoch: 1.84 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10396927772842936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10396927772842936 | validation: 0.19508776936366523]
	TIME [epoch: 1.84 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15524003353423457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15524003353423457 | validation: 0.23289638995801948]
	TIME [epoch: 1.84 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1957557391875684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1957557391875684 | validation: 0.2865658144291638]
	TIME [epoch: 1.84 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21996019188193094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21996019188193094 | validation: 0.2856090476209663]
	TIME [epoch: 1.84 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2376167836769436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2376167836769436 | validation: 0.4327816972449794]
	TIME [epoch: 1.85 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.328938344028172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.328938344028172 | validation: 0.23827515243012443]
	TIME [epoch: 1.84 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23005426309208696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23005426309208696 | validation: 0.15820408449233975]
	TIME [epoch: 1.84 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11200522019215021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11200522019215021 | validation: 0.12347289977023722]
	TIME [epoch: 1.84 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08924395259136807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08924395259136807 | validation: 0.15091621117884008]
	TIME [epoch: 1.84 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09009300515803961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09009300515803961 | validation: 0.1377836663544446]
	TIME [epoch: 1.84 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10995925364378986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10995925364378986 | validation: 0.20375547016662188]
	TIME [epoch: 1.84 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1450961815693277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1450961815693277 | validation: 0.2482435784015159]
	TIME [epoch: 1.84 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22333107972667485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22333107972667485 | validation: 0.3114391880743801]
	TIME [epoch: 1.84 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24639620492300623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24639620492300623 | validation: 0.23974048462444839]
	TIME [epoch: 1.84 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18385062676194047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18385062676194047 | validation: 0.19367518317198626]
	TIME [epoch: 1.84 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14140921639698975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14140921639698975 | validation: 0.15389481685104137]
	TIME [epoch: 1.84 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10606849567306972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10606849567306972 | validation: 0.1289506702667101]
	TIME [epoch: 1.85 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11256629119543313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11256629119543313 | validation: 0.15041567084753082]
	TIME [epoch: 1.84 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09920734542497965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09920734542497965 | validation: 0.11241026093611746]
	TIME [epoch: 1.84 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09303700743199453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09303700743199453 | validation: 0.15477060104623655]
	TIME [epoch: 1.84 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11052502185158462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11052502185158462 | validation: 0.15121074509997434]
	TIME [epoch: 1.84 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1324854617362812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1324854617362812 | validation: 0.21309003220647105]
	TIME [epoch: 1.85 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.148676677978944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.148676677978944 | validation: 0.17171709940425173]
	TIME [epoch: 1.84 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18023978877323973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18023978877323973 | validation: 0.20470525323893018]
	TIME [epoch: 1.84 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1618962037116961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1618962037116961 | validation: 0.20451082855242878]
	TIME [epoch: 1.84 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1814326315665157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1814326315665157 | validation: 0.4059877767206137]
	TIME [epoch: 1.84 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3194375107585406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3194375107585406 | validation: 0.19803804313555579]
	TIME [epoch: 1.84 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16909011555484482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16909011555484482 | validation: 0.11623227025968426]
	TIME [epoch: 1.84 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08815989190029107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08815989190029107 | validation: 0.11013790553421483]
	TIME [epoch: 1.84 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08467580311853527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08467580311853527 | validation: 0.16426941116013533]
	TIME [epoch: 1.85 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10602101437920752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10602101437920752 | validation: 0.15991704581709437]
	TIME [epoch: 1.84 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12315291849002559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12315291849002559 | validation: 0.19966435852844647]
	TIME [epoch: 1.84 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14281116164336724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14281116164336724 | validation: 0.23173323713842064]
	TIME [epoch: 1.84 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18923984028525845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18923984028525845 | validation: 0.2272487368268731]
	TIME [epoch: 1.84 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1870560503105587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1870560503105587 | validation: 0.16488135244399144]
	TIME [epoch: 1.84 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14960427386394062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14960427386394062 | validation: 0.18250554667491714]
	TIME [epoch: 1.84 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.139992539120647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.139992539120647 | validation: 0.1599830483040597]
	TIME [epoch: 1.84 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1543790702284428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1543790702284428 | validation: 0.19503891695696463]
	TIME [epoch: 1.84 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1340505749119025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1340505749119025 | validation: 0.1171635764551136]
	TIME [epoch: 1.85 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11739833960082706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11739833960082706 | validation: 0.15396092486966922]
	TIME [epoch: 1.84 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10652384529221348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10652384529221348 | validation: 0.18758924713268593]
	TIME [epoch: 1.84 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.163135886775584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.163135886775584 | validation: 0.21618409978179245]
	TIME [epoch: 1.84 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17833008370155573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17833008370155573 | validation: 0.14448759047205517]
	TIME [epoch: 1.84 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11338502582787907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11338502582787907 | validation: 0.15844012946233618]
	TIME [epoch: 1.84 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11850842716054522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11850842716054522 | validation: 0.17378051436515973]
	TIME [epoch: 1.84 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13314471955125398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13314471955125398 | validation: 0.19607986092157895]
	TIME [epoch: 1.85 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14837777724721823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14837777724721823 | validation: 0.19158606184655652]
	TIME [epoch: 1.85 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16708115660596728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16708115660596728 | validation: 0.19940853063269634]
	TIME [epoch: 1.85 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14731871888680495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14731871888680495 | validation: 0.15044162929579827]
	TIME [epoch: 1.84 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13831554484852623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13831554484852623 | validation: 0.19615373970990221]
	TIME [epoch: 1.84 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15048614719015016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15048614719015016 | validation: 0.15345732411201196]
	TIME [epoch: 1.84 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15646048952335675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15646048952335675 | validation: 0.13906132113747746]
	TIME [epoch: 1.85 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10022718976105555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10022718976105555 | validation: 0.09015910764554894]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_496.pth
	Model improved!!!
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0771518746090676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0771518746090676 | validation: 0.12945242527704034]
	TIME [epoch: 1.85 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08954376964565995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08954376964565995 | validation: 0.17214811734995078]
	TIME [epoch: 1.84 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12721993819241187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12721993819241187 | validation: 0.2502284330671737]
	TIME [epoch: 1.84 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.201064581697873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.201064581697873 | validation: 0.2482553210536664]
	TIME [epoch: 1.84 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19039542373121035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19039542373121035 | validation: 0.1450984735969246]
	TIME [epoch: 26.2 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11860082986799447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11860082986799447 | validation: 0.11144315415706695]
	TIME [epoch: 3.65 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08596210732955163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08596210732955163 | validation: 0.15013489020481363]
	TIME [epoch: 3.64 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10928665129493581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10928665129493581 | validation: 0.17979613216796322]
	TIME [epoch: 3.65 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1743016247101218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1743016247101218 | validation: 0.22966544839760933]
	TIME [epoch: 3.65 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1778208932240527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1778208932240527 | validation: 0.12372264115135723]
	TIME [epoch: 3.66 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13178183809115923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13178183809115923 | validation: 0.12144881165348763]
	TIME [epoch: 3.64 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09095867532309454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09095867532309454 | validation: 0.10019627374256922]
	TIME [epoch: 3.64 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08745535788776952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08745535788776952 | validation: 0.15540926915374179]
	TIME [epoch: 3.64 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11678468225537487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11678468225537487 | validation: 0.2437428899216307]
	TIME [epoch: 3.64 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18980461777234428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18980461777234428 | validation: 0.20251288843121423]
	TIME [epoch: 3.65 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16287202029034878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16287202029034878 | validation: 0.12094188352453401]
	TIME [epoch: 3.64 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08618725846251632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08618725846251632 | validation: 0.07655572261382293]
	TIME [epoch: 3.64 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_513.pth
	Model improved!!!
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06744338683325918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06744338683325918 | validation: 0.09232591140941578]
	TIME [epoch: 3.65 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06310868882001269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06310868882001269 | validation: 0.09416713542988477]
	TIME [epoch: 3.65 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08652386847530256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08652386847530256 | validation: 0.1575303928846883]
	TIME [epoch: 3.65 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15037752038809493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15037752038809493 | validation: 0.24532546201556826]
	TIME [epoch: 3.65 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23754047192821282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23754047192821282 | validation: 0.3233321127157414]
	TIME [epoch: 3.65 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22841505206529014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22841505206529014 | validation: 0.17371600607079396]
	TIME [epoch: 3.66 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13779479614595036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13779479614595036 | validation: 0.129165926712986]
	TIME [epoch: 3.66 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09542971491025606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09542971491025606 | validation: 0.08835988096468875]
	TIME [epoch: 3.65 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08323413219377253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08323413219377253 | validation: 0.1528841916903767]
	TIME [epoch: 3.65 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10884108030518831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10884108030518831 | validation: 0.11960708993686052]
	TIME [epoch: 3.66 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10845624666571398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10845624666571398 | validation: 0.12732949573149863]
	TIME [epoch: 3.66 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0860750209017701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0860750209017701 | validation: 0.1131701101373063]
	TIME [epoch: 3.65 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08984125503921737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08984125503921737 | validation: 0.18495692492310828]
	TIME [epoch: 3.66 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14405874548422726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14405874548422726 | validation: 0.2761739697384062]
	TIME [epoch: 3.65 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22972933159140696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22972933159140696 | validation: 0.1946357956571798]
	TIME [epoch: 3.66 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17604985003708032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17604985003708032 | validation: 0.12269439034278018]
	TIME [epoch: 3.65 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10808862007425858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10808862007425858 | validation: 0.11000898777754534]
	TIME [epoch: 3.66 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08582863537222477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08582863537222477 | validation: 0.12039129351256939]
	TIME [epoch: 3.65 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11603135557354582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11603135557354582 | validation: 0.17859434425791298]
	TIME [epoch: 3.66 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13577547795276215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13577547795276215 | validation: 0.1294307900689837]
	TIME [epoch: 3.67 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12856674640918508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12856674640918508 | validation: 0.12928339100394867]
	TIME [epoch: 3.66 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09425476246097227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09425476246097227 | validation: 0.0836386858828176]
	TIME [epoch: 3.65 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08856145876073342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08856145876073342 | validation: 0.11515879140107643]
	TIME [epoch: 3.65 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09774365777929436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09774365777929436 | validation: 0.1848448073014981]
	TIME [epoch: 3.65 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15516904338723236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15516904338723236 | validation: 0.2296111180480317]
	TIME [epoch: 3.65 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22600590705409448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22600590705409448 | validation: 0.1786438754131666]
	TIME [epoch: 3.65 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12545838617683705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12545838617683705 | validation: 0.10933022935400452]
	TIME [epoch: 3.66 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0633965463249656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0633965463249656 | validation: 0.09079903076181627]
	TIME [epoch: 3.65 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07726162247491637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07726162247491637 | validation: 0.13201035567168518]
	TIME [epoch: 3.66 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09219389472995342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09219389472995342 | validation: 0.11886436872095402]
	TIME [epoch: 3.66 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11783288648327211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11783288648327211 | validation: 0.17974557653385517]
	TIME [epoch: 3.66 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14366257926088213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14366257926088213 | validation: 0.1456022581453158]
	TIME [epoch: 3.65 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12304581967953589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12304581967953589 | validation: 0.1157648975149232]
	TIME [epoch: 3.66 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08541294137725278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08541294137725278 | validation: 0.17470830245128846]
	TIME [epoch: 3.65 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14050681803002943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14050681803002943 | validation: 0.2689263497125496]
	TIME [epoch: 3.66 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27636500638107936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27636500638107936 | validation: 0.20398466977592122]
	TIME [epoch: 3.65 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17625561665548042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17625561665548042 | validation: 0.08408312847025218]
	TIME [epoch: 3.65 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0732351303760289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0732351303760289 | validation: 0.06784774508784763]
	TIME [epoch: 3.65 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_551.pth
	Model improved!!!
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04779228814933502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04779228814933502 | validation: 0.07754087093535736]
	TIME [epoch: 3.65 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05220221207756003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05220221207756003 | validation: 0.0856330026475991]
	TIME [epoch: 3.65 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07500316976007111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07500316976007111 | validation: 0.12851077638463196]
	TIME [epoch: 3.65 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10259743469533071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10259743469533071 | validation: 0.14923881311018813]
	TIME [epoch: 3.65 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12068709402998083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12068709402998083 | validation: 0.13148955325582629]
	TIME [epoch: 3.65 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09448972505221147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09448972505221147 | validation: 0.09666778185187928]
	TIME [epoch: 3.66 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08286706497896379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08286706497896379 | validation: 0.10836016361780279]
	TIME [epoch: 3.66 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06818931238262323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06818931238262323 | validation: 0.09196663812403513]
	TIME [epoch: 3.65 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09445830862602989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09445830862602989 | validation: 0.2146240997139743]
	TIME [epoch: 3.65 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17076857704073845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17076857704073845 | validation: 0.13283401320897378]
	TIME [epoch: 3.65 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1439883344318297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1439883344318297 | validation: 0.09299125127177282]
	TIME [epoch: 3.65 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0586271725615863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0586271725615863 | validation: 0.058309706702614594]
	TIME [epoch: 3.65 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_563.pth
	Model improved!!!
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04286878801700115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04286878801700115 | validation: 0.1030895530914022]
	TIME [epoch: 3.65 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08177791469235021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08177791469235021 | validation: 0.24617327298512587]
	TIME [epoch: 3.64 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20699666189047128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20699666189047128 | validation: 0.4091509573507736]
	TIME [epoch: 3.64 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39473247484147606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39473247484147606 | validation: 0.18283785526070484]
	TIME [epoch: 3.64 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1847506652564944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1847506652564944 | validation: 0.08640788662536351]
	TIME [epoch: 3.65 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06470418294092757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06470418294092757 | validation: 0.08933368418177988]
	TIME [epoch: 3.65 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05916736877169232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05916736877169232 | validation: 0.0873668360213673]
	TIME [epoch: 3.65 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08247772302588059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08247772302588059 | validation: 0.11382249455134386]
	TIME [epoch: 3.64 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07883171820312349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07883171820312349 | validation: 0.10420846953084806]
	TIME [epoch: 3.64 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08057802466110456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08057802466110456 | validation: 0.1248096754109131]
	TIME [epoch: 3.65 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08397884130691334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08397884130691334 | validation: 0.12168355891393655]
	TIME [epoch: 3.66 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1041034205428182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1041034205428182 | validation: 0.14933953306357067]
	TIME [epoch: 3.65 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12504781046257724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12504781046257724 | validation: 0.17147175897930303]
	TIME [epoch: 3.65 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16472688577551245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16472688577551245 | validation: 0.19268520406300418]
	TIME [epoch: 3.65 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20305834315712884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20305834315712884 | validation: 0.19271755808723756]
	TIME [epoch: 3.65 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15799742757545265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15799742757545265 | validation: 0.12297527545173566]
	TIME [epoch: 3.65 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11673105041100126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11673105041100126 | validation: 0.08394497160965511]
	TIME [epoch: 3.66 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05839694079894581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05839694079894581 | validation: 0.06954046836455195]
	TIME [epoch: 3.65 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058379388387088955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058379388387088955 | validation: 0.10733056867150353]
	TIME [epoch: 3.66 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07722842621272355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07722842621272355 | validation: 0.13705440593017618]
	TIME [epoch: 3.65 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1266021510908103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1266021510908103 | validation: 0.1854098200092616]
	TIME [epoch: 3.65 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1558001803184707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1558001803184707 | validation: 0.12392424838109578]
	TIME [epoch: 3.65 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10408306419837292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10408306419837292 | validation: 0.09181171104668086]
	TIME [epoch: 3.64 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07904014451147041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07904014451147041 | validation: 0.11733775118553376]
	TIME [epoch: 3.65 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07870327270868974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07870327270868974 | validation: 0.08943169400491412]
	TIME [epoch: 3.65 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09159625340381436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09159625340381436 | validation: 0.1223884916542406]
	TIME [epoch: 3.65 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10093245674764234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10093245674764234 | validation: 0.0870591197974705]
	TIME [epoch: 3.65 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0902279190255564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0902279190255564 | validation: 0.09803677534175531]
	TIME [epoch: 3.64 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07409967781967185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07409967781967185 | validation: 0.09371048421281347]
	TIME [epoch: 3.65 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07940032560701432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07940032560701432 | validation: 0.15555031235404468]
	TIME [epoch: 3.65 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10629744641765604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10629744641765604 | validation: 0.19243810565310743]
	TIME [epoch: 3.66 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16524499416763305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16524499416763305 | validation: 0.22541155755644088]
	TIME [epoch: 3.65 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22121374964980933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22121374964980933 | validation: 0.1710784361978347]
	TIME [epoch: 3.65 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1802720964645772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1802720964645772 | validation: 0.10892513757092637]
	TIME [epoch: 3.65 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09634356386879592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09634356386879592 | validation: 0.10072589774882466]
	TIME [epoch: 3.64 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0797275538624166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0797275538624166 | validation: 0.07922585100754678]
	TIME [epoch: 3.64 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06512558176200466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06512558176200466 | validation: 0.08666846060654698]
	TIME [epoch: 3.65 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06380374339613747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06380374339613747 | validation: 0.06254834046258655]
	TIME [epoch: 3.64 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07274686568288606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07274686568288606 | validation: 0.12358450535262185]
	TIME [epoch: 3.64 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0927645714622101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0927645714622101 | validation: 0.09435280692195198]
	TIME [epoch: 3.64 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09565732331706726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09565732331706726 | validation: 0.10676057300651186]
	TIME [epoch: 3.64 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07837898928043617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07837898928043617 | validation: 0.10921839964791652]
	TIME [epoch: 3.63 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09510755777708951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09510755777708951 | validation: 0.1648185914344864]
	TIME [epoch: 3.64 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11536683519267843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11536683519267843 | validation: 0.15874758328143773]
	TIME [epoch: 3.65 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1353597693170486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1353597693170486 | validation: 0.18382435885987675]
	TIME [epoch: 3.64 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17214742400425578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17214742400425578 | validation: 0.23211115703605512]
	TIME [epoch: 3.64 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20760092564526939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20760092564526939 | validation: 0.14708697583845864]
	TIME [epoch: 3.64 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14059547842537934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14059547842537934 | validation: 0.09690363391413825]
	TIME [epoch: 3.64 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07470767938183807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07470767938183807 | validation: 0.07365529846566875]
	TIME [epoch: 3.65 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06752113197514825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06752113197514825 | validation: 0.08291049568870329]
	TIME [epoch: 3.65 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06485615584673564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06485615584673564 | validation: 0.08700009586108721]
	TIME [epoch: 3.64 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07040197668706893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07040197668706893 | validation: 0.09126750938550676]
	TIME [epoch: 3.66 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07802171899758503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07802171899758503 | validation: 0.12491716009030503]
	TIME [epoch: 3.63 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08368303187605587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08368303187605587 | validation: 0.1284037115407788]
	TIME [epoch: 3.66 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10346015780439291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10346015780439291 | validation: 0.132452515404622]
	TIME [epoch: 3.64 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10163733206958166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10163733206958166 | validation: 0.1424679427582696]
	TIME [epoch: 3.65 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11882936313553462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11882936313553462 | validation: 0.15672272724259184]
	TIME [epoch: 3.64 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15209005267466094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15209005267466094 | validation: 0.15137674238225465]
	TIME [epoch: 3.65 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12832333361115736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12832333361115736 | validation: 0.07965178139280613]
	TIME [epoch: 3.64 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06698249983487631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06698249983487631 | validation: 0.06433685438843231]
	TIME [epoch: 3.64 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044218821002598205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044218821002598205 | validation: 0.08048227057921595]
	TIME [epoch: 3.66 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06125834835567576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06125834835567576 | validation: 0.10052659334646777]
	TIME [epoch: 3.64 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09034390432941049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09034390432941049 | validation: 0.1437731848674912]
	TIME [epoch: 3.66 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12385278658685972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12385278658685972 | validation: 0.11449603458161524]
	TIME [epoch: 3.65 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10993307960355664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10993307960355664 | validation: 0.08602141285944737]
	TIME [epoch: 3.65 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06046624980738747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06046624980738747 | validation: 0.07879062904155319]
	TIME [epoch: 3.65 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05623611645868632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05623611645868632 | validation: 0.14923264540529277]
	TIME [epoch: 3.65 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13420897386748665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13420897386748665 | validation: 0.2941039945565003]
	TIME [epoch: 3.66 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2598032344912609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2598032344912609 | validation: 0.1647532871674707]
	TIME [epoch: 3.66 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1680960090711156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1680960090711156 | validation: 0.0885532716885698]
	TIME [epoch: 3.64 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08241495678033693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08241495678033693 | validation: 0.07796434327864597]
	TIME [epoch: 3.64 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06546509970300918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06546509970300918 | validation: 0.06256502997398666]
	TIME [epoch: 3.64 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06668416995845122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06668416995845122 | validation: 0.09213129402000647]
	TIME [epoch: 3.65 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07259098997039154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07259098997039154 | validation: 0.11753045434132284]
	TIME [epoch: 3.65 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10121943214445078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10121943214445078 | validation: 0.10346220334472563]
	TIME [epoch: 3.64 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07133549574128421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07133549574128421 | validation: 0.07665826287704239]
	TIME [epoch: 3.64 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07286656083866848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07286656083866848 | validation: 0.13965969767443837]
	TIME [epoch: 3.64 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10833411706796234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10833411706796234 | validation: 0.16425234806942093]
	TIME [epoch: 3.64 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15644029828949024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15644029828949024 | validation: 0.12608475031391292]
	TIME [epoch: 3.64 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09153602000009531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09153602000009531 | validation: 0.10117169549527127]
	TIME [epoch: 3.64 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09191281453240986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09191281453240986 | validation: 0.14659191619202225]
	TIME [epoch: 3.63 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14152873640300548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14152873640300548 | validation: 0.17614265855609632]
	TIME [epoch: 3.64 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16923349726475878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16923349726475878 | validation: 0.15545576605980682]
	TIME [epoch: 3.64 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1523408824299229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1523408824299229 | validation: 0.10115338784757442]
	TIME [epoch: 3.63 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0774761878561401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0774761878561401 | validation: 0.053212010087999834]
	TIME [epoch: 3.64 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_648.pth
	Model improved!!!
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044371596161991676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044371596161991676 | validation: 0.058857312846700476]
	TIME [epoch: 3.65 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04727352661581447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04727352661581447 | validation: 0.0943424585657423]
	TIME [epoch: 3.66 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07240830218437486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07240830218437486 | validation: 0.09511508710461913]
	TIME [epoch: 3.65 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08648223763275305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08648223763275305 | validation: 0.10368441738039905]
	TIME [epoch: 3.65 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08130432707567234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08130432707567234 | validation: 0.06412569420209295]
	TIME [epoch: 3.65 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06699788797236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06699788797236 | validation: 0.08253579025706001]
	TIME [epoch: 3.66 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06753504771589344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06753504771589344 | validation: 0.08525943213629533]
	TIME [epoch: 3.66 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08451408315890448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08451408315890448 | validation: 0.1562661203890071]
	TIME [epoch: 3.65 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12935701697576638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12935701697576638 | validation: 0.17192484683903655]
	TIME [epoch: 3.67 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16142849674643317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16142849674643317 | validation: 0.17372000079775393]
	TIME [epoch: 3.66 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14856201202664726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14856201202664726 | validation: 0.1445702714352285]
	TIME [epoch: 3.66 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1190706333309183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1190706333309183 | validation: 0.0822927731281707]
	TIME [epoch: 3.65 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09480597736114132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09480597736114132 | validation: 0.0962444339130457]
	TIME [epoch: 3.64 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08706585472212293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08706585472212293 | validation: 0.11365423851186485]
	TIME [epoch: 3.64 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10046672393578834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10046672393578834 | validation: 0.07595947336907463]
	TIME [epoch: 3.64 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06497655006725515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06497655006725515 | validation: 0.08726092186074666]
	TIME [epoch: 3.64 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06989987606279974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06989987606279974 | validation: 0.13539555645039125]
	TIME [epoch: 3.64 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11693790043898147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11693790043898147 | validation: 0.20956424158043713]
	TIME [epoch: 3.64 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18434681324036384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18434681324036384 | validation: 0.12386364286085388]
	TIME [epoch: 3.65 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11975544579073662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11975544579073662 | validation: 0.09907349872754972]
	TIME [epoch: 3.65 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07631042094971632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07631042094971632 | validation: 0.06798853946748644]
	TIME [epoch: 3.65 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0663616932100363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0663616932100363 | validation: 0.08898218804290467]
	TIME [epoch: 3.64 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.068652726240054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.068652726240054 | validation: 0.06373895629383104]
	TIME [epoch: 3.66 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060142785119752705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060142785119752705 | validation: 0.07298097864559018]
	TIME [epoch: 3.64 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055913125133756736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055913125133756736 | validation: 0.07757582326965036]
	TIME [epoch: 3.63 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06615276721846401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06615276721846401 | validation: 0.11967380394588122]
	TIME [epoch: 3.65 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0897601658603309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0897601658603309 | validation: 0.13447072410219604]
	TIME [epoch: 3.64 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13124954715950654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13124954715950654 | validation: 0.15511901472831766]
	TIME [epoch: 3.64 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15458797542014022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15458797542014022 | validation: 0.12488965774538735]
	TIME [epoch: 3.64 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11141298557255802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11141298557255802 | validation: 0.1292676733220674]
	TIME [epoch: 3.65 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12116485631286338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12116485631286338 | validation: 0.11998436696382019]
	TIME [epoch: 3.65 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10643145087922086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10643145087922086 | validation: 0.09760922362575604]
	TIME [epoch: 3.65 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09820595555023673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09820595555023673 | validation: 0.06633745484625231]
	TIME [epoch: 3.67 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07460154728279904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07460154728279904 | validation: 0.12698949844010357]
	TIME [epoch: 3.65 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0954159912284866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0954159912284866 | validation: 0.11989029018975553]
	TIME [epoch: 3.67 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.110682998330957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.110682998330957 | validation: 0.08763577705523382]
	TIME [epoch: 3.67 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06711225106372416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06711225106372416 | validation: 0.06344416970638604]
	TIME [epoch: 3.63 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04599105596451392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04599105596451392 | validation: 0.058966729022402256]
	TIME [epoch: 3.65 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04490618058251926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04490618058251926 | validation: 0.08454664158692171]
	TIME [epoch: 3.65 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07240257697708105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07240257697708105 | validation: 0.10992082441701462]
	TIME [epoch: 3.64 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10244555940946203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10244555940946203 | validation: 0.10877198807208824]
	TIME [epoch: 3.63 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09032252586551924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09032252586551924 | validation: 0.07520324790926454]
	TIME [epoch: 3.65 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0943372197313678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0943372197313678 | validation: 0.12991878927016484]
	TIME [epoch: 3.63 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10620378816558432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10620378816558432 | validation: 0.09428632418674598]
	TIME [epoch: 3.64 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10899616059572162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10899616059572162 | validation: 0.12635971557521675]
	TIME [epoch: 3.63 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09717132754252092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09717132754252092 | validation: 0.14545559877308617]
	TIME [epoch: 3.65 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13164126378207885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13164126378207885 | validation: 0.19372161744887048]
	TIME [epoch: 3.66 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1888472051287172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1888472051287172 | validation: 0.1278951752734197]
	TIME [epoch: 3.65 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11742266840463639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11742266840463639 | validation: 0.0668895754659221]
	TIME [epoch: 3.63 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059587355963069744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059587355963069744 | validation: 0.05399573891821061]
	TIME [epoch: 3.65 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04786466981835851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04786466981835851 | validation: 0.07078826477502435]
	TIME [epoch: 3.65 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04804558539738379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04804558539738379 | validation: 0.05878692689143586]
	TIME [epoch: 3.65 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060706975970555334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060706975970555334 | validation: 0.10889773750109347]
	TIME [epoch: 3.63 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0699056349164558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0699056349164558 | validation: 0.08511437813199457]
	TIME [epoch: 3.63 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07624574201091312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07624574201091312 | validation: 0.08707838540160388]
	TIME [epoch: 3.64 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07574297394118486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07574297394118486 | validation: 0.0883647798961999]
	TIME [epoch: 3.63 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09027286698977857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09027286698977857 | validation: 0.13539287144066595]
	TIME [epoch: 3.63 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1267790327206999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1267790327206999 | validation: 0.19194457925728475]
	TIME [epoch: 3.64 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15407469411039398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15407469411039398 | validation: 0.12279634932320045]
	TIME [epoch: 3.64 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10267202958903304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10267202958903304 | validation: 0.061671877639430966]
	TIME [epoch: 3.65 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05088410126166739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05088410126166739 | validation: 0.058282866351973356]
	TIME [epoch: 3.65 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043331633491561314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043331633491561314 | validation: 0.061676367618488076]
	TIME [epoch: 3.64 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06703381945813776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06703381945813776 | validation: 0.11985816102081706]
	TIME [epoch: 3.63 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09414472248314994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09414472248314994 | validation: 0.12383093091845265]
	TIME [epoch: 3.63 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14417104542505457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14417104542505457 | validation: 0.1661792279472586]
	TIME [epoch: 3.64 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13512509898146616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13512509898146616 | validation: 0.0652641211783081]
	TIME [epoch: 3.64 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07709732638016925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07709732638016925 | validation: 0.06358703759408312]
	TIME [epoch: 3.64 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0445330350020995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0445330350020995 | validation: 0.05226978690207129]
	TIME [epoch: 3.64 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_716.pth
	Model improved!!!
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046499354752128665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046499354752128665 | validation: 0.06268084040520885]
	TIME [epoch: 3.64 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04768478895027178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04768478895027178 | validation: 0.08005829781220825]
	TIME [epoch: 3.64 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06477600016533291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06477600016533291 | validation: 0.09649315209673573]
	TIME [epoch: 3.64 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10108950517202793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10108950517202793 | validation: 0.1576455371274029]
	TIME [epoch: 3.66 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1528752095844813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1528752095844813 | validation: 0.17062537369136438]
	TIME [epoch: 3.67 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17040562494783537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17040562494783537 | validation: 0.1570922644654566]
	TIME [epoch: 3.63 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13600222987012536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13600222987012536 | validation: 0.1436903799922735]
	TIME [epoch: 3.64 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11598564992478984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11598564992478984 | validation: 0.08712079154753807]
	TIME [epoch: 3.62 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08234386500017181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08234386500017181 | validation: 0.1122237010410073]
	TIME [epoch: 3.63 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08590420718389359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08590420718389359 | validation: 0.07196069770636122]
	TIME [epoch: 3.64 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06712738557230118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06712738557230118 | validation: 0.051950631442355816]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_727.pth
	Model improved!!!
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040311649643885596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040311649643885596 | validation: 0.0433981963893793]
	TIME [epoch: 3.65 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_728.pth
	Model improved!!!
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033302882677552714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033302882677552714 | validation: 0.04894143074975502]
	TIME [epoch: 3.65 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04396210720978616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04396210720978616 | validation: 0.07199037082105675]
	TIME [epoch: 3.65 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06637977671124179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06637977671124179 | validation: 0.08114969728049118]
	TIME [epoch: 3.65 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07662147273412598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07662147273412598 | validation: 0.11079700076933141]
	TIME [epoch: 3.65 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10939782165153435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10939782165153435 | validation: 0.14486125883181145]
	TIME [epoch: 3.67 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13943202840461616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13943202840461616 | validation: 0.15199929202889895]
	TIME [epoch: 3.65 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11264581994747835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11264581994747835 | validation: 0.097691212252962]
	TIME [epoch: 3.65 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0825871665464856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0825871665464856 | validation: 0.0876370671282797]
	TIME [epoch: 3.66 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11951246233531526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11951246233531526 | validation: 0.1501818693326813]
	TIME [epoch: 3.67 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1272250381781748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1272250381781748 | validation: 0.07027126054163624]
	TIME [epoch: 3.66 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07439878045011263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07439878045011263 | validation: 0.05971815278195952]
	TIME [epoch: 3.63 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03980686066860116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03980686066860116 | validation: 0.045177315228393175]
	TIME [epoch: 3.64 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04186996515563384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04186996515563384 | validation: 0.08975642105512122]
	TIME [epoch: 3.65 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07276764786207067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07276764786207067 | validation: 0.13219018758086332]
	TIME [epoch: 3.64 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.130932048805588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.130932048805588 | validation: 0.15483018551290487]
	TIME [epoch: 3.64 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14607393273292288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14607393273292288 | validation: 0.13299217315445938]
	TIME [epoch: 3.64 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1419274255031657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1419274255031657 | validation: 0.10585319788778032]
	TIME [epoch: 3.66 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09334358389903702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09334358389903702 | validation: 0.060528957905979786]
	TIME [epoch: 3.65 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0607003162786789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0607003162786789 | validation: 0.07598399177725486]
	TIME [epoch: 3.65 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05794137274968301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05794137274968301 | validation: 0.06624879565031139]
	TIME [epoch: 3.64 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07719754668087944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07719754668087944 | validation: 0.10718998929214962]
	TIME [epoch: 3.66 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07760905256633885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07760905256633885 | validation: 0.0642902919110067]
	TIME [epoch: 3.64 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06332345591857658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06332345591857658 | validation: 0.0661763208182458]
	TIME [epoch: 3.64 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04745539936392382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04745539936392382 | validation: 0.051900160892663166]
	TIME [epoch: 3.65 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057861059261479754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057861059261479754 | validation: 0.11360273796649896]
	TIME [epoch: 3.64 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09450365467288695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09450365467288695 | validation: 0.10192000945206735]
	TIME [epoch: 3.65 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12804643927106285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12804643927106285 | validation: 0.13894886388325225]
	TIME [epoch: 3.65 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1254444963470486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1254444963470486 | validation: 0.10301582058503317]
	TIME [epoch: 3.65 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10145422401942834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10145422401942834 | validation: 0.1264362670314986]
	TIME [epoch: 3.65 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10348700890628314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10348700890628314 | validation: 0.11471589853129792]
	TIME [epoch: 3.66 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10910234353086203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10910234353086203 | validation: 0.0816238925889796]
	TIME [epoch: 3.66 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09985372737851382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09985372737851382 | validation: 0.06400916264658099]
	TIME [epoch: 3.65 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05940285480451886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05940285480451886 | validation: 0.04452513629210207]
	TIME [epoch: 3.65 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030149689233146076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030149689233146076 | validation: 0.03349124497652145]
	TIME [epoch: 3.66 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_762.pth
	Model improved!!!
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026142268092258156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026142268092258156 | validation: 0.04770666140135953]
	TIME [epoch: 3.64 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03391908498636606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03391908498636606 | validation: 0.0694507277373048]
	TIME [epoch: 3.65 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06071205518841183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06071205518841183 | validation: 0.11556968278561648]
	TIME [epoch: 3.64 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10893239261181638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10893239261181638 | validation: 0.19410037033228908]
	TIME [epoch: 3.65 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19633570698193217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19633570698193217 | validation: 0.1661982513457619]
	TIME [epoch: 3.65 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1641292845287394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1641292845287394 | validation: 0.09886421051826516]
	TIME [epoch: 3.64 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08958269381944417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08958269381944417 | validation: 0.10783698483287858]
	TIME [epoch: 3.65 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11040722861954361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11040722861954361 | validation: 0.06697318098955587]
	TIME [epoch: 3.66 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07714182147718267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07714182147718267 | validation: 0.06081951627959053]
	TIME [epoch: 3.66 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043130566332629065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043130566332629065 | validation: 0.04115615318887377]
	TIME [epoch: 3.64 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03721733038211107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03721733038211107 | validation: 0.06852097336585421]
	TIME [epoch: 3.64 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051451741315977635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051451741315977635 | validation: 0.10781202535597237]
	TIME [epoch: 3.65 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10062696404738365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10062696404738365 | validation: 0.13501815574352252]
	TIME [epoch: 3.64 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09519526488580642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09519526488580642 | validation: 0.09952034727534437]
	TIME [epoch: 3.64 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0836960814163162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0836960814163162 | validation: 0.11644466790985562]
	TIME [epoch: 3.66 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11037407147215099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11037407147215099 | validation: 0.13097058437592962]
	TIME [epoch: 3.66 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15528540621190307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15528540621190307 | validation: 0.11645219235728202]
	TIME [epoch: 3.65 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11324351569245554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11324351569245554 | validation: 0.08341865316901625]
	TIME [epoch: 3.64 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06667395522366115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06667395522366115 | validation: 0.05779024908238306]
	TIME [epoch: 3.65 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06412125584933584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06412125584933584 | validation: 0.06778843557298402]
	TIME [epoch: 3.65 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06783315792626812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06783315792626812 | validation: 0.06754540864897281]
	TIME [epoch: 3.64 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0656899055304717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0656899055304717 | validation: 0.0617399141545508]
	TIME [epoch: 3.66 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05629152713950667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05629152713950667 | validation: 0.060116002601304855]
	TIME [epoch: 3.64 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041611046062014795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041611046062014795 | validation: 0.04892356139704294]
	TIME [epoch: 3.64 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03858056757981824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03858056757981824 | validation: 0.07810881812216858]
	TIME [epoch: 3.66 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05986112838974223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05986112838974223 | validation: 0.1355617074690117]
	TIME [epoch: 3.65 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11493028214318139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11493028214318139 | validation: 0.18960997937777757]
	TIME [epoch: 3.65 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17695709093871137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17695709093871137 | validation: 0.1415309653298756]
	TIME [epoch: 3.66 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14434149975418697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14434149975418697 | validation: 0.07233717292277347]
	TIME [epoch: 3.66 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0637862310050512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0637862310050512 | validation: 0.04259457112190709]
	TIME [epoch: 3.65 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036622173423179825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036622173423179825 | validation: 0.05365327075274606]
	TIME [epoch: 3.65 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04965218729075963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04965218729075963 | validation: 0.07257118811108797]
	TIME [epoch: 3.65 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0940321065715688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0940321065715688 | validation: 0.1014192663501534]
	TIME [epoch: 3.66 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10400713180965596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10400713180965596 | validation: 0.07422427978273602]
	TIME [epoch: 3.66 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0690973283558196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0690973283558196 | validation: 0.09764070485857101]
	TIME [epoch: 3.66 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06879600491922386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06879600491922386 | validation: 0.0562291690215481]
	TIME [epoch: 3.65 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061967267433716756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061967267433716756 | validation: 0.0800501370167293]
	TIME [epoch: 3.66 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06736125117814704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06736125117814704 | validation: 0.05965232351590244]
	TIME [epoch: 3.65 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0738189527727358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0738189527727358 | validation: 0.07329214915462076]
	TIME [epoch: 3.64 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05196567080982678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05196567080982678 | validation: 0.11278376519798486]
	TIME [epoch: 3.63 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09928816100407768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09928816100407768 | validation: 0.21518963878242314]
	TIME [epoch: 3.63 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19538372799983414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19538372799983414 | validation: 0.14732558916689872]
	TIME [epoch: 3.65 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1886672676269091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1886672676269091 | validation: 0.07274066829605129]
	TIME [epoch: 3.64 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06386951073642237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06386951073642237 | validation: 0.050240271334929566]
	TIME [epoch: 3.65 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03975040455789627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03975040455789627 | validation: 0.052011077997025305]
	TIME [epoch: 3.64 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0529143068793975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0529143068793975 | validation: 0.07689677030307146]
	TIME [epoch: 3.64 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06056936662156849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06056936662156849 | validation: 0.06918880606353137]
	TIME [epoch: 3.65 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06976726418086888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06976726418086888 | validation: 0.08319620455234054]
	TIME [epoch: 3.64 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06810621139133552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06810621139133552 | validation: 0.059697758273044826]
	TIME [epoch: 3.64 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06088768194558992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06088768194558992 | validation: 0.05808172994764939]
	TIME [epoch: 3.64 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045563720940022936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045563720940022936 | validation: 0.04339529299952254]
	TIME [epoch: 3.63 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04539218314976118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04539218314976118 | validation: 0.06645401573323684]
	TIME [epoch: 3.64 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05709471970321433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05709471970321433 | validation: 0.08833858750454336]
	TIME [epoch: 3.64 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08515149391196027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08515149391196027 | validation: 0.13818194907120077]
	TIME [epoch: 3.65 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12594400630165595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12594400630165595 | validation: 0.14307195258546024]
	TIME [epoch: 3.64 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10813553203897457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10813553203897457 | validation: 0.09491041850228366]
	TIME [epoch: 3.64 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07967240729470414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07967240729470414 | validation: 0.1174606668594406]
	TIME [epoch: 3.64 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09780676610157894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09780676610157894 | validation: 0.1748331903519045]
	TIME [epoch: 3.65 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18759841220201726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18759841220201726 | validation: 0.13779756111363942]
	TIME [epoch: 3.65 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12457971940659109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12457971940659109 | validation: 0.046563187898848266]
	TIME [epoch: 3.64 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03881053181349017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03881053181349017 | validation: 0.041757277500193965]
	TIME [epoch: 3.64 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03317308816516328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03317308816516328 | validation: 0.03320158610007152]
	TIME [epoch: 3.64 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_824.pth
	Model improved!!!
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032899884697292735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032899884697292735 | validation: 0.04468481434117487]
	TIME [epoch: 3.63 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03996759213696795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03996759213696795 | validation: 0.06336379797856094]
	TIME [epoch: 3.64 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0670996967484713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0670996967484713 | validation: 0.06966168045914962]
	TIME [epoch: 3.63 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053940033005220336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053940033005220336 | validation: 0.05130609236897635]
	TIME [epoch: 3.63 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04378749167059961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04378749167059961 | validation: 0.057706758865127085]
	TIME [epoch: 3.64 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04651059146641251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04651059146641251 | validation: 0.08499239535758166]
	TIME [epoch: 3.65 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07622386675379722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07622386675379722 | validation: 0.11028864906528545]
	TIME [epoch: 3.65 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13376624169826457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13376624169826457 | validation: 0.10475441175834499]
	TIME [epoch: 3.65 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10383389655590597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10383389655590597 | validation: 0.08419922458425097]
	TIME [epoch: 3.67 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.071234945267044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.071234945267044 | validation: 0.0818356006401449]
	TIME [epoch: 3.65 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053036453316276495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053036453316276495 | validation: 0.1115721051299281]
	TIME [epoch: 3.64 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07807554874824725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07807554874824725 | validation: 0.1792709426300608]
	TIME [epoch: 3.64 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16795306997205925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16795306997205925 | validation: 0.19028264932804795]
	TIME [epoch: 3.64 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22137112364516653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22137112364516653 | validation: 0.06355447607968912]
	TIME [epoch: 3.64 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05644895651826076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05644895651826076 | validation: 0.03641588710032057]
	TIME [epoch: 3.64 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024937149990933005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024937149990933005 | validation: 0.04914970565260287]
	TIME [epoch: 3.66 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040465887846071184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040465887846071184 | validation: 0.03676045189971563]
	TIME [epoch: 3.64 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03968535892088743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03968535892088743 | validation: 0.06815425140018669]
	TIME [epoch: 3.65 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047506995417148645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047506995417148645 | validation: 0.09984098330010786]
	TIME [epoch: 3.65 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0997126970926438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0997126970926438 | validation: 0.140819825992667]
	TIME [epoch: 3.65 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13785748905647938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13785748905647938 | validation: 0.05834592227490172]
	TIME [epoch: 3.65 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06833140244755014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06833140244755014 | validation: 0.07668824744596957]
	TIME [epoch: 3.64 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05924365182121502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05924365182121502 | validation: 0.08107651644266847]
	TIME [epoch: 3.65 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08242439044647824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08242439044647824 | validation: 0.08697728559497585]
	TIME [epoch: 3.64 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09182169924477748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09182169924477748 | validation: 0.04908516907754416]
	TIME [epoch: 3.64 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06281701430938966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06281701430938966 | validation: 0.07517235794550484]
	TIME [epoch: 3.64 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053100851264254685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053100851264254685 | validation: 0.07274297051308394]
	TIME [epoch: 3.65 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06753413085333083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06753413085333083 | validation: 0.08980465598124665]
	TIME [epoch: 3.65 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06629554662657154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06629554662657154 | validation: 0.09187377148014526]
	TIME [epoch: 3.66 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10009700125183203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10009700125183203 | validation: 0.12976482561483357]
	TIME [epoch: 3.66 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1590306600644951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1590306600644951 | validation: 0.1237321686772549]
	TIME [epoch: 3.65 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1001936308471275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1001936308471275 | validation: 0.06037802430400846]
	TIME [epoch: 3.66 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053083569559614074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053083569559614074 | validation: 0.043028204202101725]
	TIME [epoch: 3.66 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039147678456013184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039147678456013184 | validation: 0.07440152707490914]
	TIME [epoch: 3.66 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06523662690025944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06523662690025944 | validation: 0.07650407538255713]
	TIME [epoch: 3.64 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10464439703518977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10464439703518977 | validation: 0.14585268238170995]
	TIME [epoch: 3.65 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.125765966807727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.125765966807727 | validation: 0.10099524262123634]
	TIME [epoch: 3.64 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09846468296875052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09846468296875052 | validation: 0.05818382865300015]
	TIME [epoch: 3.64 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04235462905569443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04235462905569443 | validation: 0.032503957338929614]
	TIME [epoch: 3.64 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_863.pth
	Model improved!!!
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027022289641227402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027022289641227402 | validation: 0.03354477368865714]
	TIME [epoch: 3.64 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03441766672134016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03441766672134016 | validation: 0.05002577233851391]
	TIME [epoch: 3.63 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04897717666323787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04897717666323787 | validation: 0.07294575724058135]
	TIME [epoch: 3.65 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07122419907584732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07122419907584732 | validation: 0.08723151539977769]
	TIME [epoch: 3.63 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07980564464868026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07980564464868026 | validation: 0.10866249669940474]
	TIME [epoch: 3.65 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09684751406708433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09684751406708433 | validation: 0.09786484549684524]
	TIME [epoch: 3.64 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08722460507263588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08722460507263588 | validation: 0.12355455793868274]
	TIME [epoch: 3.66 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1022658560363519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1022658560363519 | validation: 0.12140900617896616]
	TIME [epoch: 3.65 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13756405726696894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13756405726696894 | validation: 0.11682425972482809]
	TIME [epoch: 3.64 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09732697447813739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09732697447813739 | validation: 0.039212390805098855]
	TIME [epoch: 3.64 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04530572517445612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04530572517445612 | validation: 0.041745993676495875]
	TIME [epoch: 3.63 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03645470321347579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03645470321347579 | validation: 0.054682706124889874]
	TIME [epoch: 3.65 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06046754116052929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06046754116052929 | validation: 0.07396518882129947]
	TIME [epoch: 3.64 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05764071339487705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05764071339487705 | validation: 0.07814061758488783]
	TIME [epoch: 3.64 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07525138151673172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07525138151673172 | validation: 0.11680515481986314]
	TIME [epoch: 3.64 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11334896461948495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11334896461948495 | validation: 0.11148561379296013]
	TIME [epoch: 3.64 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12869025652634875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12869025652634875 | validation: 0.10829349780847895]
	TIME [epoch: 3.65 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08692189818888103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08692189818888103 | validation: 0.038631110700158545]
	TIME [epoch: 3.64 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04481161370189288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04481161370189288 | validation: 0.05743941435749549]
	TIME [epoch: 3.64 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04786176546278442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04786176546278442 | validation: 0.049278235154882506]
	TIME [epoch: 3.65 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06433060260414164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06433060260414164 | validation: 0.06550195428347286]
	TIME [epoch: 3.65 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06252970747987989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06252970747987989 | validation: 0.045197179205976296]
	TIME [epoch: 3.64 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05328905232821831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05328905232821831 | validation: 0.06721476674481341]
	TIME [epoch: 3.63 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050258299544431714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050258299544431714 | validation: 0.05751472672642728]
	TIME [epoch: 3.63 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05664832856120888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05664832856120888 | validation: 0.08443050093090552]
	TIME [epoch: 3.63 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07019250701545589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07019250701545589 | validation: 0.07017453591241878]
	TIME [epoch: 3.64 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07325607282535151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07325607282535151 | validation: 0.09676259202789174]
	TIME [epoch: 3.64 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09753201788423145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09753201788423145 | validation: 0.06979054482448159]
	TIME [epoch: 3.64 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09024616617969582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09024616617969582 | validation: 0.0896846074452326]
	TIME [epoch: 3.64 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07373082255440265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07373082255440265 | validation: 0.12246714236412384]
	TIME [epoch: 3.64 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11344681863219996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11344681863219996 | validation: 0.14207664384186158]
	TIME [epoch: 3.64 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12012408020605907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12012408020605907 | validation: 0.06431567695793153]
	TIME [epoch: 3.65 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06598073352081847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06598073352081847 | validation: 0.0361753839279769]
	TIME [epoch: 3.65 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026012101959205502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026012101959205502 | validation: 0.03327754760064476]
	TIME [epoch: 3.65 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02293504923147241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02293504923147241 | validation: 0.0329055227041744]
	TIME [epoch: 3.65 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03245119894227399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03245119894227399 | validation: 0.06760451147591494]
	TIME [epoch: 3.63 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07036085491339737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07036085491339737 | validation: 0.11713834880533135]
	TIME [epoch: 3.63 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1506331724282476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1506331724282476 | validation: 0.11199994146200593]
	TIME [epoch: 3.63 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10762234698753384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10762234698753384 | validation: 0.05811453859706416]
	TIME [epoch: 3.64 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053365219689640656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053365219689640656 | validation: 0.06419806967585982]
	TIME [epoch: 3.64 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049960399029451555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049960399029451555 | validation: 0.11351988681033805]
	TIME [epoch: 3.64 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0902657080141724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0902657080141724 | validation: 0.13620994551775986]
	TIME [epoch: 3.64 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13846046021600195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13846046021600195 | validation: 0.10708482420753339]
	TIME [epoch: 3.63 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10071545621801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10071545621801 | validation: 0.035232076385494214]
	TIME [epoch: 3.64 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03225203261802266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03225203261802266 | validation: 0.027267479343808177]
	TIME [epoch: 3.64 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_908.pth
	Model improved!!!
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019500577467081396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019500577467081396 | validation: 0.03304554623176011]
	TIME [epoch: 3.65 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026852593450637858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026852593450637858 | validation: 0.05128884537725625]
	TIME [epoch: 3.63 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054104861257118254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054104861257118254 | validation: 0.06633889715205954]
	TIME [epoch: 3.64 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07530945170992913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07530945170992913 | validation: 0.08966957818269623]
	TIME [epoch: 3.64 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07161306665148758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07161306665148758 | validation: 0.07917839747693001]
	TIME [epoch: 3.63 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08383641543701766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08383641543701766 | validation: 0.11137099518052627]
	TIME [epoch: 3.64 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10373520118922042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10373520118922042 | validation: 0.12911441248265018]
	TIME [epoch: 3.63 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16034313760428284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16034313760428284 | validation: 0.15891221541436773]
	TIME [epoch: 3.63 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16042385362689782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16042385362689782 | validation: 0.051350101007472754]
	TIME [epoch: 3.63 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05777592805517075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05777592805517075 | validation: 0.04131633276186103]
	TIME [epoch: 3.64 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03997049322523723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03997049322523723 | validation: 0.04168890241387108]
	TIME [epoch: 3.63 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043683294576988184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043683294576988184 | validation: 0.061418532164038225]
	TIME [epoch: 3.64 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045704483418560406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045704483418560406 | validation: 0.049569056277276785]
	TIME [epoch: 3.64 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043752331753637554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043752331753637554 | validation: 0.055688778150942066]
	TIME [epoch: 3.64 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03614665345158406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03614665345158406 | validation: 0.03503768142949875]
	TIME [epoch: 3.64 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03791398092171601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03791398092171601 | validation: 0.061990555290843456]
	TIME [epoch: 3.64 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053537620590322875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053537620590322875 | validation: 0.06144370504065747]
	TIME [epoch: 3.64 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07631753181379214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07631753181379214 | validation: 0.08870749677829011]
	TIME [epoch: 3.63 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07508389559148639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07508389559148639 | validation: 0.07546645935361185]
	TIME [epoch: 3.63 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07713136475530703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07713136475530703 | validation: 0.1077728006391495]
	TIME [epoch: 3.63 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08857097185524618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08857097185524618 | validation: 0.10726470714923682]
	TIME [epoch: 3.64 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09983968217560825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09983968217560825 | validation: 0.09898862004238687]
	TIME [epoch: 3.64 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11175794769737206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11175794769737206 | validation: 0.0746317190463096]
	TIME [epoch: 3.63 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07286346565095532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07286346565095532 | validation: 0.043112559746672136]
	TIME [epoch: 3.64 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053388561409016884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053388561409016884 | validation: 0.07166011580192526]
	TIME [epoch: 3.65 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06776934559146748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06776934559146748 | validation: 0.039176024678291003]
	TIME [epoch: 3.65 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06331509342472164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06331509342472164 | validation: 0.06631912899534091]
	TIME [epoch: 3.63 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03798541533491534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03798541533491534 | validation: 0.04204881766607616]
	TIME [epoch: 3.64 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04568530064801415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04568530064801415 | validation: 0.11290020594833504]
	TIME [epoch: 3.64 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07516053580199417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07516053580199417 | validation: 0.0985450977675646]
	TIME [epoch: 3.64 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09865241365898321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09865241365898321 | validation: 0.11074451946593128]
	TIME [epoch: 3.63 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08920956217567726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08920956217567726 | validation: 0.08236297063991166]
	TIME [epoch: 3.63 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10905102076769176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10905102076769176 | validation: 0.08811544949255057]
	TIME [epoch: 3.64 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07719516402549012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07719516402549012 | validation: 0.03819659909138626]
	TIME [epoch: 3.64 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029909990119496143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029909990119496143 | validation: 0.03055774654391684]
	TIME [epoch: 3.63 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020165888531709614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020165888531709614 | validation: 0.028351245745780282]
	TIME [epoch: 3.64 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030531430602387794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030531430602387794 | validation: 0.05265806272226697]
	TIME [epoch: 3.64 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05182477401168823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05182477401168823 | validation: 0.05222940230472031]
	TIME [epoch: 3.64 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06456219470891905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06456219470891905 | validation: 0.05376473920060868]
	TIME [epoch: 3.65 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05299232150191716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05299232150191716 | validation: 0.07129718697188436]
	TIME [epoch: 3.64 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0651174899044501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0651174899044501 | validation: 0.09762209791233345]
	TIME [epoch: 3.65 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11612814564232515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11612814564232515 | validation: 0.11555333464567785]
	TIME [epoch: 3.64 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11240790711482912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11240790711482912 | validation: 0.08416104814407527]
	TIME [epoch: 3.63 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08297654218889607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08297654218889607 | validation: 0.12860994180825489]
	TIME [epoch: 3.63 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12380440350219556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12380440350219556 | validation: 0.16292649246056584]
	TIME [epoch: 3.63 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14195850988451486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14195850988451486 | validation: 0.05124713922989928]
	TIME [epoch: 3.64 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06975539500002566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06975539500002566 | validation: 0.04459022703276179]
	TIME [epoch: 3.65 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028444026931948165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028444026931948165 | validation: 0.028350649522556556]
	TIME [epoch: 3.64 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025269574064044946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025269574064044946 | validation: 0.032216523771950625]
	TIME [epoch: 3.64 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032355922650883946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032355922650883946 | validation: 0.04991584817870091]
	TIME [epoch: 3.64 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053356489006256495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053356489006256495 | validation: 0.07315148180070069]
	TIME [epoch: 3.65 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07120737429904594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07120737429904594 | validation: 0.07922454669508751]
	TIME [epoch: 3.64 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07950144721322824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07950144721322824 | validation: 0.0816208237135328]
	TIME [epoch: 3.64 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07500530909771977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07500530909771977 | validation: 0.06863119143334087]
	TIME [epoch: 3.63 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056181717335453234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056181717335453234 | validation: 0.06982548275373737]
	TIME [epoch: 3.65 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053837819790013286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053837819790013286 | validation: 0.07050237388646322]
	TIME [epoch: 3.63 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09135621233598608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09135621233598608 | validation: 0.09640776112917471]
	TIME [epoch: 3.64 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09874488331630339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09874488331630339 | validation: 0.03792071505216668]
	TIME [epoch: 3.64 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06347061577425402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06347061577425402 | validation: 0.065294903900882]
	TIME [epoch: 3.64 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04539389254985966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04539389254985966 | validation: 0.0689203514889261]
	TIME [epoch: 3.64 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059631074487197074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059631074487197074 | validation: 0.09622509443811347]
	TIME [epoch: 3.64 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08972281489446662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08972281489446662 | validation: 0.08816852976678559]
	TIME [epoch: 3.64 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09629373097744609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09629373097744609 | validation: 0.09416859859040168]
	TIME [epoch: 3.65 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0860165552989859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0860165552989859 | validation: 0.04316895845365287]
	TIME [epoch: 3.65 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046977833627107994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046977833627107994 | validation: 0.05101393240214889]
	TIME [epoch: 3.65 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0368975370306861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0368975370306861 | validation: 0.056233257718081256]
	TIME [epoch: 3.64 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055910928971120905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055910928971120905 | validation: 0.0719584560698349]
	TIME [epoch: 3.64 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06228784673333454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06228784673333454 | validation: 0.050523003467143716]
	TIME [epoch: 3.63 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054683518221433455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054683518221433455 | validation: 0.05592852427329731]
	TIME [epoch: 3.64 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054803935398431494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054803935398431494 | validation: 0.07163443250110328]
	TIME [epoch: 3.64 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06979361218062959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06979361218062959 | validation: 0.0735111458573545]
	TIME [epoch: 3.64 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08815604559619043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08815604559619043 | validation: 0.07007410423672884]
	TIME [epoch: 3.64 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06279538007143942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06279538007143942 | validation: 0.04772879355654805]
	TIME [epoch: 3.64 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049055814398335174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049055814398335174 | validation: 0.07594013084311951]
	TIME [epoch: 3.64 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060904320962541836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060904320962541836 | validation: 0.10152862329407131]
	TIME [epoch: 3.64 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12289800849753828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12289800849753828 | validation: 0.14378112828354556]
	TIME [epoch: 3.65 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1443975980937015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1443975980937015 | validation: 0.07314420005668387]
	TIME [epoch: 3.66 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0795871373910051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0795871373910051 | validation: 0.043310506166236794]
	TIME [epoch: 3.65 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034486819894341825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034486819894341825 | validation: 0.030115374757473695]
	TIME [epoch: 3.64 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029402245490960252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029402245490960252 | validation: 0.04107542064259677]
	TIME [epoch: 3.63 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03028796299282595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03028796299282595 | validation: 0.04043978513667198]
	TIME [epoch: 3.64 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04142211085279873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04142211085279873 | validation: 0.05699605407782198]
	TIME [epoch: 3.64 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04934474568889269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04934474568889269 | validation: 0.05270310376629617]
	TIME [epoch: 3.64 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06482849314720213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06482849314720213 | validation: 0.08711634446362466]
	TIME [epoch: 3.64 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08554977094425972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08554977094425972 | validation: 0.0843953137635243]
	TIME [epoch: 3.64 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09510712134826971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09510712134826971 | validation: 0.089702207864783]
	TIME [epoch: 3.64 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07550544820893522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07550544820893522 | validation: 0.06173468925793329]
	TIME [epoch: 3.64 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056634588886323556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056634588886323556 | validation: 0.05217342598933475]
	TIME [epoch: 3.65 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06187004620907126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06187004620907126 | validation: 0.07995315487073573]
	TIME [epoch: 3.66 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09299963147907762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09299963147907762 | validation: 0.0743996582561721]
	TIME [epoch: 3.64 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09044584277041486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09044584277041486 | validation: 0.06909956148225531]
	TIME [epoch: 3.64 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06828289152987836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06828289152987836 | validation: 0.0706660986515131]
	TIME [epoch: 3.63 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05664880068414334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05664880068414334 | validation: 0.05237519699494744]
	TIME [epoch: 30.2 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04242482842013621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04242482842013621 | validation: 0.06375879633325075]
	TIME [epoch: 7.9 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04817443601541552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04817443601541552 | validation: 0.05984788230168643]
	TIME [epoch: 7.91 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04833161408966269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04833161408966269 | validation: 0.10067455182270124]
	TIME [epoch: 7.89 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08509660322176071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08509660322176071 | validation: 0.10217130637085292]
	TIME [epoch: 7.88 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12128182663428762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12128182663428762 | validation: 0.08537409347263854]
	TIME [epoch: 7.89 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06820637770142918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06820637770142918 | validation: 0.05899696328792512]
	TIME [epoch: 7.89 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057299023802784024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057299023802784024 | validation: 0.052805810265982314]
	TIME [epoch: 7.9 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05051482989038556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05051482989038556 | validation: 0.0500707075746381]
	TIME [epoch: 7.91 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240822_142013/states/model_phi1_3a_v_mmd1_1009.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 2830.708 seconds.
