Args:
Namespace(name='model_phi1_3b_v_mmd1', outdir='out/model_training/model_phi1_3b_v_mmd1', training_data='data/training_data/data_phi1_3b/training', validation_data='data/training_data/data_phi1_3b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 4041621988

Training model...

Saving initial model state to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.793537090515689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.793537090515689 | validation: 2.8416425930745697]
	TIME [epoch: 28.8 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6138713780794904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6138713780794904 | validation: 2.874906792599662]
	TIME [epoch: 1.74 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4549201259537607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4549201259537607 | validation: 2.085255932506252]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.282020546084853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.282020546084853 | validation: 2.0841685134648764]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.093682268682474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.093682268682474 | validation: 2.354056630817515]
	TIME [epoch: 1.73 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.14003161554624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.14003161554624 | validation: 1.9476877923244662]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6494980441223204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6494980441223204 | validation: 2.0226079682658753]
	TIME [epoch: 1.73 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.102108641980323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.102108641980323 | validation: 1.5156257278026677]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5406418568336588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5406418568336588 | validation: 1.8230565653217121]
	TIME [epoch: 1.73 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.454365208532792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.454365208532792 | validation: 1.7215097682200367]
	TIME [epoch: 1.73 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5011114575754243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5011114575754243 | validation: 1.4000910585335355]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2170537713800509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2170537713800509 | validation: 1.3659186411632342]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.188604645802311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.188604645802311 | validation: 1.399301435714036]
	TIME [epoch: 1.74 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2067583844360628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2067583844360628 | validation: 1.3726804421260614]
	TIME [epoch: 1.73 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1736309634540731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1736309634540731 | validation: 1.3461667926874823]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1476964191274026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1476964191274026 | validation: 1.305168113676533]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0932697080461262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0932697080461262 | validation: 1.247865902928799]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.082643974561237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.082643974561237 | validation: 1.2688768768152479]
	TIME [epoch: 1.73 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0663141694811327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0663141694811327 | validation: 1.2199667566654684]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1008222775532208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1008222775532208 | validation: 1.1814190988687792]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0461147096152947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0461147096152947 | validation: 1.1223414236850944]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0002474705967344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0002474705967344 | validation: 1.1275797921208288]
	TIME [epoch: 1.73 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9859709784256278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9859709784256278 | validation: 1.1396136010698619]
	TIME [epoch: 1.73 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1972692701931464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1972692701931464 | validation: 1.0522000631655295]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9274347727223756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9274347727223756 | validation: 0.9976708277098958]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8884192590014388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8884192590014388 | validation: 0.9733845963202749]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.865270869259018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.865270869259018 | validation: 1.0881947165129284]
	TIME [epoch: 1.74 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9311089863404736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9311089863404736 | validation: 0.9670565553825672]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.011919306927867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.011919306927867 | validation: 0.9714316998776366]
	TIME [epoch: 1.74 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8632911295800061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8632911295800061 | validation: 0.9471502420652271]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8605834860628881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8605834860628881 | validation: 1.0280753649294783]
	TIME [epoch: 1.73 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9600037418143015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9600037418143015 | validation: 1.0568448117034566]
	TIME [epoch: 1.73 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0259249210653296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0259249210653296 | validation: 1.0793867201980796]
	TIME [epoch: 1.74 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.991614072559195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.991614072559195 | validation: 0.8919423445700535]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.822724055890156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.822724055890156 | validation: 0.8518369649896214]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7937180304148964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7937180304148964 | validation: 0.8564095431535836]
	TIME [epoch: 1.75 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7955124686373969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7955124686373969 | validation: 0.8528668776940421]
	TIME [epoch: 1.74 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.803323161623077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.803323161623077 | validation: 0.9158852498676533]
	TIME [epoch: 1.73 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8858794236530594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8858794236530594 | validation: 0.8948992461728278]
	TIME [epoch: 1.74 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8366423363599037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8366423363599037 | validation: 0.8956009128096124]
	TIME [epoch: 1.73 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8663550818150703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8663550818150703 | validation: 0.8408860240561573]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7803596907704093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7803596907704093 | validation: 0.8468162260580878]
	TIME [epoch: 1.73 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7744810916791437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7744810916791437 | validation: 0.8278336185648061]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.760867452378769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.760867452378769 | validation: 0.8510830169270158]
	TIME [epoch: 1.73 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7934726170997634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7934726170997634 | validation: 0.8873951725538686]
	TIME [epoch: 1.73 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7978443648469753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7978443648469753 | validation: 0.8835460316173958]
	TIME [epoch: 1.73 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9035378197837137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9035378197837137 | validation: 1.136792065090089]
	TIME [epoch: 1.73 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9774901663682587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9774901663682587 | validation: 0.9538893279830873]
	TIME [epoch: 1.73 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9325080672302453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9325080672302453 | validation: 0.9731764503751048]
	TIME [epoch: 1.73 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8744477170902971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8744477170902971 | validation: 0.8310149557400228]
	TIME [epoch: 1.73 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7591526500973146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7591526500973146 | validation: 0.8220415543390669]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7465897595870993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7465897595870993 | validation: 0.8262551361198928]
	TIME [epoch: 1.73 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.754076445872637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.754076445872637 | validation: 0.8266243569975384]
	TIME [epoch: 1.74 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7764779815848791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7764779815848791 | validation: 0.8419535112729557]
	TIME [epoch: 1.74 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7591102627584124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7591102627584124 | validation: 0.809620369404493]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.758447468086923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.758447468086923 | validation: 0.8321154160519568]
	TIME [epoch: 1.73 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7476182861224715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7476182861224715 | validation: 0.8088104946969192]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7643267535885946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7643267535885946 | validation: 0.8470975944272002]
	TIME [epoch: 1.73 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7655683398289158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7655683398289158 | validation: 0.8100252510602692]
	TIME [epoch: 1.73 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7789022054363747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7789022054363747 | validation: 0.9054452779870265]
	TIME [epoch: 1.73 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8015312240153809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8015312240153809 | validation: 0.8155594456727995]
	TIME [epoch: 1.73 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8074769150962723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8074769150962723 | validation: 0.9478118605944703]
	TIME [epoch: 1.73 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8638136544513122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8638136544513122 | validation: 0.9043359477225327]
	TIME [epoch: 1.73 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8808671741432025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8808671741432025 | validation: 0.9386402338867375]
	TIME [epoch: 1.73 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9219709429081118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9219709429081118 | validation: 0.8113867431700691]
	TIME [epoch: 1.73 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7257780827604938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7257780827604938 | validation: 0.8777839785734914]
	TIME [epoch: 1.73 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8055182181462479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8055182181462479 | validation: 0.8615155051213931]
	TIME [epoch: 1.73 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8270269222535683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8270269222535683 | validation: 0.7698792948128982]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7269052051319994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7269052051319994 | validation: 0.8758182299608471]
	TIME [epoch: 1.73 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7738908782131881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7738908782131881 | validation: 0.8080412595237807]
	TIME [epoch: 1.73 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7901688444748901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7901688444748901 | validation: 0.7914484542795062]
	TIME [epoch: 1.73 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7289254874614376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7289254874614376 | validation: 0.7818583441939625]
	TIME [epoch: 1.73 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7307962321487006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7307962321487006 | validation: 0.8387855704565812]
	TIME [epoch: 1.74 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7647869745116818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7647869745116818 | validation: 0.8209999742143546]
	TIME [epoch: 1.75 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7841607192585588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7841607192585588 | validation: 0.9293015154901835]
	TIME [epoch: 1.73 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8336575959885648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8336575959885648 | validation: 0.8088529702311412]
	TIME [epoch: 1.73 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7710056087328289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7710056087328289 | validation: 0.9355857631580857]
	TIME [epoch: 1.73 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8021769366889165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8021769366889165 | validation: 0.7892812348749215]
	TIME [epoch: 1.78 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7538339424317262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7538339424317262 | validation: 0.8003360970189463]
	TIME [epoch: 1.73 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7309306119549334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7309306119549334 | validation: 0.7708539630189454]
	TIME [epoch: 1.73 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7211348514511431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7211348514511431 | validation: 0.7906228023223726]
	TIME [epoch: 1.73 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7196022079169757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7196022079169757 | validation: 0.8274617962667613]
	TIME [epoch: 1.73 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7505215721119712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7505215721119712 | validation: 0.8013080777965629]
	TIME [epoch: 1.73 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7678971076066698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7678971076066698 | validation: 0.9153070848629569]
	TIME [epoch: 1.73 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.789780699671756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.789780699671756 | validation: 0.7915043537056893]
	TIME [epoch: 1.73 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7474317287047001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7474317287047001 | validation: 0.8367025720130787]
	TIME [epoch: 1.73 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.762296737359759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.762296737359759 | validation: 0.8521509583595078]
	TIME [epoch: 1.73 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.842806061107903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.842806061107903 | validation: 0.961739761062924]
	TIME [epoch: 1.73 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9148455881563766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9148455881563766 | validation: 0.7656857702012214]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7390471288048605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7390471288048605 | validation: 0.8297220676914332]
	TIME [epoch: 1.73 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7564251358692698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7564251358692698 | validation: 0.7938492615501536]
	TIME [epoch: 1.74 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7574914370147278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7574914370147278 | validation: 0.752410336863682]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7246080538268155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7246080538268155 | validation: 0.8078134100296506]
	TIME [epoch: 1.75 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7319245335490419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7319245335490419 | validation: 0.7959266182795214]
	TIME [epoch: 1.74 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7695601446578755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7695601446578755 | validation: 0.9292752586642511]
	TIME [epoch: 1.74 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8093723268768753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8093723268768753 | validation: 0.7902452109927907]
	TIME [epoch: 1.73 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7654583332327839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7654583332327839 | validation: 0.8423336671758581]
	TIME [epoch: 1.74 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7653742932643033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7653742932643033 | validation: 0.8107449201774133]
	TIME [epoch: 1.73 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7612890043839162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7612890043839162 | validation: 0.814792964636456]
	TIME [epoch: 1.73 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7780163982849739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7780163982849739 | validation: 0.7647307627353741]
	TIME [epoch: 1.73 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7266214260474386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7266214260474386 | validation: 0.7698848414714199]
	TIME [epoch: 1.73 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7147341131046286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7147341131046286 | validation: 0.7341090053413513]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7250317687937338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7250317687937338 | validation: 0.8865419438614972]
	TIME [epoch: 1.73 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7740945506900296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7740945506900296 | validation: 0.8005470279995287]
	TIME [epoch: 1.73 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7983891370773131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7983891370773131 | validation: 0.8653793504655817]
	TIME [epoch: 1.73 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7736072046848085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7736072046848085 | validation: 0.8060826497007558]
	TIME [epoch: 1.73 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7859748148249149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7859748148249149 | validation: 0.9103620549951827]
	TIME [epoch: 1.73 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8449809025072239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8449809025072239 | validation: 0.7789973960580848]
	TIME [epoch: 1.73 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7361703172730273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7361703172730273 | validation: 0.8303615990604514]
	TIME [epoch: 1.73 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7398362452169772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7398362452169772 | validation: 0.753775500578853]
	TIME [epoch: 1.73 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7217972512943842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7217972512943842 | validation: 0.7701617103995453]
	TIME [epoch: 1.74 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7074570017577176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7074570017577176 | validation: 0.773718576726352]
	TIME [epoch: 1.74 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7143608992178337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7143608992178337 | validation: 0.7655645238771716]
	TIME [epoch: 1.74 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7312274014866071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7312274014866071 | validation: 0.8586414236670781]
	TIME [epoch: 1.73 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7582169017527417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7582169017527417 | validation: 0.7967331788211134]
	TIME [epoch: 1.73 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7774962336486878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7774962336486878 | validation: 0.9266821427851437]
	TIME [epoch: 1.73 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8112892502599266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8112892502599266 | validation: 0.8606833428072935]
	TIME [epoch: 1.73 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8565497127198203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8565497127198203 | validation: 0.9507153173163423]
	TIME [epoch: 1.73 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8943101808529688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8943101808529688 | validation: 0.7814889740309415]
	TIME [epoch: 1.73 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.718695014193739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.718695014193739 | validation: 0.8497107244351114]
	TIME [epoch: 1.73 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7846652261638798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7846652261638798 | validation: 0.8229006695354452]
	TIME [epoch: 1.73 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7406392850910638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7406392850910638 | validation: 0.7655631964234232]
	TIME [epoch: 1.73 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7236790224343198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7236790224343198 | validation: 0.8257447466452672]
	TIME [epoch: 1.73 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7249014199901106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7249014199901106 | validation: 0.7628111107027895]
	TIME [epoch: 1.73 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7250047059777299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7250047059777299 | validation: 0.8068709042575238]
	TIME [epoch: 1.73 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7222803715131062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7222803715131062 | validation: 0.7841857277148615]
	TIME [epoch: 1.73 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7543775919035659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7543775919035659 | validation: 0.8918449895351863]
	TIME [epoch: 1.73 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7927134023603075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7927134023603075 | validation: 0.792879876679155]
	TIME [epoch: 1.73 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7681377565634522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7681377565634522 | validation: 0.8413984600496776]
	TIME [epoch: 1.74 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7470925899301762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7470925899301762 | validation: 0.7507983985623179]
	TIME [epoch: 1.73 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.71419425039288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.71419425039288 | validation: 0.7639094692156552]
	TIME [epoch: 1.74 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7020107074575168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7020107074575168 | validation: 0.7637642489075601]
	TIME [epoch: 1.75 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.71782626937369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.71782626937369 | validation: 0.8085624388446901]
	TIME [epoch: 1.74 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7204955724392512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7204955724392512 | validation: 0.7616690871394239]
	TIME [epoch: 1.73 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7246756530731833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7246756530731833 | validation: 0.8094968622415429]
	TIME [epoch: 1.73 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7211609059068803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7211609059068803 | validation: 0.7376822723311642]
	TIME [epoch: 1.73 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7154166063659747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7154166063659747 | validation: 0.8578151449034895]
	TIME [epoch: 1.73 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7537884807400808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7537884807400808 | validation: 0.864600309584321]
	TIME [epoch: 1.73 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.87682467394171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.87682467394171 | validation: 1.0093658818265694]
	TIME [epoch: 1.74 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9355147807060488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9355147807060488 | validation: 0.740134408664498]
	TIME [epoch: 1.73 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7003047535360202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7003047535360202 | validation: 0.7619024747792923]
	TIME [epoch: 1.73 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7490302004393702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7490302004393702 | validation: 0.8658861721037152]
	TIME [epoch: 1.73 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7901661852661716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7901661852661716 | validation: 0.7398320689601698]
	TIME [epoch: 1.73 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7142230080979308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7142230080979308 | validation: 0.7494264100271807]
	TIME [epoch: 1.73 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6927958996349544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6927958996349544 | validation: 0.7557462451606987]
	TIME [epoch: 1.74 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7058355066574201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7058355066574201 | validation: 0.7508774934407683]
	TIME [epoch: 1.73 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7109883571808626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7109883571808626 | validation: 0.7867586017007181]
	TIME [epoch: 1.73 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7179124984551755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7179124984551755 | validation: 0.7588362011369547]
	TIME [epoch: 1.73 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7341730285967927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7341730285967927 | validation: 0.8427099448774421]
	TIME [epoch: 1.73 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7605051157521143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7605051157521143 | validation: 0.7753316515337092]
	TIME [epoch: 1.73 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7444349411233898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7444349411233898 | validation: 0.8276341897736782]
	TIME [epoch: 1.74 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7566915908337296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7566915908337296 | validation: 0.7870813713730842]
	TIME [epoch: 1.74 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.765466250123156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.765466250123156 | validation: 0.8098830833135384]
	TIME [epoch: 1.75 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7760381115716701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7760381115716701 | validation: 0.7279359637885229]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6867100637231255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6867100637231255 | validation: 0.7413055339457506]
	TIME [epoch: 1.74 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6936354110377585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6936354110377585 | validation: 0.7576653275047729]
	TIME [epoch: 1.73 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7052109860267409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7052109860267409 | validation: 0.7269194225405425]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7299810109965889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7299810109965889 | validation: 0.9171869560193451]
	TIME [epoch: 1.73 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8118485247887648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8118485247887648 | validation: 0.7801020709153937]
	TIME [epoch: 1.73 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7730336180720357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7730336180720357 | validation: 0.8269681284901313]
	TIME [epoch: 1.73 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.740391748185035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.740391748185035 | validation: 0.7478081738734648]
	TIME [epoch: 1.73 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7075730610423915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7075730610423915 | validation: 0.7472320762873029]
	TIME [epoch: 1.73 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6969453755355772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6969453755355772 | validation: 0.7294960933452229]
	TIME [epoch: 1.73 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6817722653192417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6817722653192417 | validation: 0.7193268522774396]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.683964183065979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.683964183065979 | validation: 0.7349872983374914]
	TIME [epoch: 1.74 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6817446633562912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6817446633562912 | validation: 0.7288158628047042]
	TIME [epoch: 1.73 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6881770739885431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6881770739885431 | validation: 0.7629222650171213]
	TIME [epoch: 1.74 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7050114372084315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7050114372084315 | validation: 0.791660790023152]
	TIME [epoch: 1.74 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7437472644860639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7437472644860639 | validation: 0.8314057656796239]
	TIME [epoch: 1.74 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8484130215749573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8484130215749573 | validation: 1.274462284802678]
	TIME [epoch: 1.74 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0803862724596887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0803862724596887 | validation: 0.7309274846710259]
	TIME [epoch: 1.75 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.687034239217497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.687034239217497 | validation: 0.8182230551272203]
	TIME [epoch: 1.74 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.805551024428586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.805551024428586 | validation: 0.8926493248212272]
	TIME [epoch: 1.74 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7895962606460678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7895962606460678 | validation: 0.7465310654950731]
	TIME [epoch: 1.73 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6978480542570816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6978480542570816 | validation: 0.7392750488836755]
	TIME [epoch: 1.74 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7038051540058956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7038051540058956 | validation: 0.7484983082462991]
	TIME [epoch: 1.74 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6954837423463641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6954837423463641 | validation: 0.7426116041980642]
	TIME [epoch: 1.74 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.70426493464602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.70426493464602 | validation: 0.7540300589290193]
	TIME [epoch: 1.74 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7124133980641949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7124133980641949 | validation: 0.7083866555609314]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6776667860678753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6776667860678753 | validation: 0.7349045935364489]
	TIME [epoch: 1.74 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6806892198500145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6806892198500145 | validation: 0.7010951890359177]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6861537697916138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6861537697916138 | validation: 0.7798214188748688]
	TIME [epoch: 1.74 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7134447275903314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7134447275903314 | validation: 0.7540042882560384]
	TIME [epoch: 1.74 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7778222419773672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7778222419773672 | validation: 0.9339531266704922]
	TIME [epoch: 1.74 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8504750512641619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8504750512641619 | validation: 0.7113950748940584]
	TIME [epoch: 1.74 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6999428867492317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6999428867492317 | validation: 0.7042868468595178]
	TIME [epoch: 1.74 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6674198667409139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6674198667409139 | validation: 0.6961569055607271]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_187.pth
	Model improved!!!
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6690246092049267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6690246092049267 | validation: 0.6875952996526069]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6804559373715466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6804559373715466 | validation: 0.7995163048703239]
	TIME [epoch: 1.74 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7256340572017278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7256340572017278 | validation: 0.7547385130141696]
	TIME [epoch: 1.75 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7545476767367165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7545476767367165 | validation: 0.880550642084661]
	TIME [epoch: 1.74 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.77800596289383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.77800596289383 | validation: 0.7209492488529268]
	TIME [epoch: 1.74 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7160109267747304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7160109267747304 | validation: 0.7580126521180283]
	TIME [epoch: 1.74 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7205070685158154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7205070685158154 | validation: 0.6934733918446168]
	TIME [epoch: 1.74 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6693989453396284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6693989453396284 | validation: 0.7005353705373165]
	TIME [epoch: 1.74 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6554387967445574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6554387967445574 | validation: 0.6754366078302004]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6499286460452038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6499286460452038 | validation: 0.7167368427818128]
	TIME [epoch: 1.73 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6589777238688168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6589777238688168 | validation: 0.7143710494455838]
	TIME [epoch: 1.73 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7254354591035197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7254354591035197 | validation: 1.0430669715985783]
	TIME [epoch: 1.73 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9211574739663755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9211574739663755 | validation: 0.7500091942901942]
	TIME [epoch: 1.73 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7620274525946655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7620274525946655 | validation: 0.7241700359071528]
	TIME [epoch: 28.1 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6835644603496291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6835644603496291 | validation: 0.67202517779885]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.647097856372416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.647097856372416 | validation: 0.679654514422256]
	TIME [epoch: 3.45 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6528878584258467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6528878584258467 | validation: 0.6969306599286581]
	TIME [epoch: 3.44 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6681777156052375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6681777156052375 | validation: 0.6772074712864617]
	TIME [epoch: 3.43 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6623883641661544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6623883641661544 | validation: 0.760584279456168]
	TIME [epoch: 3.43 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6979982193967732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6979982193967732 | validation: 0.7487039245869134]
	TIME [epoch: 3.43 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7604207773652192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7604207773652192 | validation: 1.0436402644405811]
	TIME [epoch: 3.44 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8802215133097793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8802215133097793 | validation: 0.7020802305848008]
	TIME [epoch: 3.44 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6671559355589589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6671559355589589 | validation: 0.6787722749331252]
	TIME [epoch: 3.45 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6617811551140622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6617811551140622 | validation: 0.6872043518478317]
	TIME [epoch: 3.43 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6508266562478741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6508266562478741 | validation: 0.6464116279337909]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6448162277047979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6448162277047979 | validation: 0.7148840965293339]
	TIME [epoch: 3.44 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6723932123032316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6723932123032316 | validation: 0.7038320194779433]
	TIME [epoch: 3.43 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7275581635913898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7275581635913898 | validation: 0.9559118727770652]
	TIME [epoch: 3.44 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8400048778740592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8400048778740592 | validation: 0.6929047917814992]
	TIME [epoch: 3.44 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.711568414636146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.711568414636146 | validation: 0.6888741715323351]
	TIME [epoch: 3.44 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6551979993827517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6551979993827517 | validation: 0.6423724303041584]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_218.pth
	Model improved!!!
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6339123159683574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6339123159683574 | validation: 0.6465452140281489]
	TIME [epoch: 3.45 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6327842519810429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6327842519810429 | validation: 0.6921210172772996]
	TIME [epoch: 3.45 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6437822949280207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6437822949280207 | validation: 0.6477714177050535]
	TIME [epoch: 3.44 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6808755587748765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6808755587748765 | validation: 0.8674018710799466]
	TIME [epoch: 3.44 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7889685298584211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7889685298584211 | validation: 0.7285474073587779]
	TIME [epoch: 3.44 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7848525689502713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7848525689502713 | validation: 0.7598634869403958]
	TIME [epoch: 3.43 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7016733183682351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7016733183682351 | validation: 0.6531534497519257]
	TIME [epoch: 3.43 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6394813294420648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6394813294420648 | validation: 0.6585832164061847]
	TIME [epoch: 3.44 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6335273495424554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6335273495424554 | validation: 0.6658440346090693]
	TIME [epoch: 3.43 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6444691595959237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6444691595959237 | validation: 0.6372233292810008]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_228.pth
	Model improved!!!
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6426553500939082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6426553500939082 | validation: 0.7144377889330419]
	TIME [epoch: 3.44 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6680131996318773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6680131996318773 | validation: 0.7204487171199511]
	TIME [epoch: 3.46 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7481780114498205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7481780114498205 | validation: 0.9975066735641618]
	TIME [epoch: 3.44 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8398071525957969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8398071525957969 | validation: 0.6452223388857061]
	TIME [epoch: 3.44 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.658612046402041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.658612046402041 | validation: 0.658381385734839]
	TIME [epoch: 3.44 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6413082240778977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6413082240778977 | validation: 0.6551936728767617]
	TIME [epoch: 3.43 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6255188442559941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6255188442559941 | validation: 0.614869145367554]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_235.pth
	Model improved!!!
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6322979815721159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6322979815721159 | validation: 0.7225090073493696]
	TIME [epoch: 3.43 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6563932234456553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6563932234456553 | validation: 0.6739818187421724]
	TIME [epoch: 3.43 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7414432277717679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7414432277717679 | validation: 0.8561293154221974]
	TIME [epoch: 3.44 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7887923093881618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7887923093881618 | validation: 0.6429025288489125]
	TIME [epoch: 3.44 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6551520941410095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6551520941410095 | validation: 0.660445447118109]
	TIME [epoch: 3.44 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6290764206662256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6290764206662256 | validation: 0.6239745194339031]
	TIME [epoch: 3.45 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6169502830190898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6169502830190898 | validation: 0.6211093634499565]
	TIME [epoch: 3.43 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6112597552075384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6112597552075384 | validation: 0.6064285205704936]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_243.pth
	Model improved!!!
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5992700547834965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5992700547834965 | validation: 0.6225079193020993]
	TIME [epoch: 3.43 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5920408851958411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5920408851958411 | validation: 0.6047214050413294]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_245.pth
	Model improved!!!
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6130699028766402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6130699028766402 | validation: 0.8946283060451784]
	TIME [epoch: 3.44 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7914072116822499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7914072116822499 | validation: 0.9730762602994791]
	TIME [epoch: 3.43 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0458760060750953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0458760060750953 | validation: 0.693799011887825]
	TIME [epoch: 3.43 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6567625880534366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6567625880534366 | validation: 0.6209483842529997]
	TIME [epoch: 3.44 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6220652660414809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6220652660414809 | validation: 0.627396833316481]
	TIME [epoch: 3.45 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.661057701787132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.661057701787132 | validation: 0.7377230357261828]
	TIME [epoch: 3.45 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6647540244112871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6647540244112871 | validation: 0.5886858202234757]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_252.pth
	Model improved!!!
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6131733438996275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6131733438996275 | validation: 0.6025948420523064]
	TIME [epoch: 3.44 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5906352052960077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5906352052960077 | validation: 0.5894362475815421]
	TIME [epoch: 3.44 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5905385325643261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5905385325643261 | validation: 0.598885924216432]
	TIME [epoch: 3.44 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.582566575649193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.582566575649193 | validation: 0.5812651659235959]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_256.pth
	Model improved!!!
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.590573918050777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.590573918050777 | validation: 0.6729604349584408]
	TIME [epoch: 3.43 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6404217397480284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6404217397480284 | validation: 0.7630495130074098]
	TIME [epoch: 3.43 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8604309198410107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8604309198410107 | validation: 0.9914863224032765]
	TIME [epoch: 3.43 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8597952459440771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8597952459440771 | validation: 0.583140546711942]
	TIME [epoch: 3.44 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.612022823934735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.612022823934735 | validation: 0.5940331016237747]
	TIME [epoch: 3.44 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5944386065747364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5944386065747364 | validation: 0.648702058426768]
	TIME [epoch: 3.43 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6180360360415794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6180360360415794 | validation: 0.6000723067081397]
	TIME [epoch: 3.43 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6196017748879422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6196017748879422 | validation: 0.6687166198913546]
	TIME [epoch: 3.43 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6220078144336479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6220078144336479 | validation: 0.5757858376617796]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_265.pth
	Model improved!!!
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6218468994643382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6218468994643382 | validation: 0.686439886296419]
	TIME [epoch: 3.44 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6359800039510882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6359800039510882 | validation: 0.5864873190708209]
	TIME [epoch: 3.43 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.642972831923743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.642972831923743 | validation: 0.6958132559978847]
	TIME [epoch: 3.43 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6573831162733892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6573831162733892 | validation: 0.5845213916851671]
	TIME [epoch: 3.43 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6432776602102183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6432776602102183 | validation: 0.6703828953762274]
	TIME [epoch: 3.45 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6250881333519958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6250881333519958 | validation: 0.5656411208772284]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_271.pth
	Model improved!!!
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6092110290505411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6092110290505411 | validation: 0.6264848832697116]
	TIME [epoch: 3.44 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6005152710565738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6005152710565738 | validation: 0.5457585131163771]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_273.pth
	Model improved!!!
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6046882355739247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6046882355739247 | validation: 0.6962022715651804]
	TIME [epoch: 3.43 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6269883852953573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6269883852953573 | validation: 0.6162851336856847]
	TIME [epoch: 3.43 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.664599991103672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.664599991103672 | validation: 0.7456910895178767]
	TIME [epoch: 3.43 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6772275269139354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6772275269139354 | validation: 0.5502573872483231]
	TIME [epoch: 3.43 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6117149634758892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6117149634758892 | validation: 0.6028309379143222]
	TIME [epoch: 3.44 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5965704740231981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5965704740231981 | validation: 0.5461769622947605]
	TIME [epoch: 3.43 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5794202642791167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5794202642791167 | validation: 0.5902273659655232]
	TIME [epoch: 3.43 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5838337492484805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5838337492484805 | validation: 0.5495615577425081]
	TIME [epoch: 3.82 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6006496897321983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6006496897321983 | validation: 0.7096107592194763]
	TIME [epoch: 3.44 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6414860822160479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6414860822160479 | validation: 0.671909375900015]
	TIME [epoch: 3.44 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7211463539342056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7211463539342056 | validation: 0.698051973073625]
	TIME [epoch: 3.44 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6390381454310625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6390381454310625 | validation: 0.5170081367958476]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_285.pth
	Model improved!!!
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5659529356210746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5659529356210746 | validation: 0.5525643703736892]
	TIME [epoch: 3.44 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5550066475555041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5550066475555041 | validation: 0.5396709780898892]
	TIME [epoch: 3.44 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5440982855929956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5440982855929956 | validation: 0.5212496293508897]
	TIME [epoch: 3.44 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5410408805871393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5410408805871393 | validation: 0.5376870065266756]
	TIME [epoch: 3.44 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5279514374996311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5279514374996311 | validation: 0.5077929049033175]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_290.pth
	Model improved!!!
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5191975046637174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5191975046637174 | validation: 0.5840273329054714]
	TIME [epoch: 3.46 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5431797652748126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5431797652748126 | validation: 0.6761051753214521]
	TIME [epoch: 3.44 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7545752123355252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7545752123355252 | validation: 0.99141042359113]
	TIME [epoch: 3.44 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8827281114442886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8827281114442886 | validation: 0.498841026472417]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_294.pth
	Model improved!!!
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5737813727749199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5737813727749199 | validation: 0.48052843169106974]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_295.pth
	Model improved!!!
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5445565486449467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5445565486449467 | validation: 0.5783684842528037]
	TIME [epoch: 3.43 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5665792424035617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5665792424035617 | validation: 0.491085627004129]
	TIME [epoch: 3.43 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5545042185520098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5545042185520098 | validation: 0.5783030926133738]
	TIME [epoch: 3.43 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5379882750644608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5379882750644608 | validation: 0.485650923525176]
	TIME [epoch: 3.44 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5594538530524922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5594538530524922 | validation: 0.6343006680065902]
	TIME [epoch: 3.44 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5852028441029818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5852028441029818 | validation: 0.5568220811469485]
	TIME [epoch: 3.45 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6297793812166648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6297793812166648 | validation: 0.64862903541999]
	TIME [epoch: 3.43 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5926784683995002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5926784683995002 | validation: 0.4798340398858891]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_303.pth
	Model improved!!!
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5415208252839605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5415208252839605 | validation: 0.510021264272859]
	TIME [epoch: 3.44 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4982445406615768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4982445406615768 | validation: 0.4661076133556426]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49719330812962936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49719330812962936 | validation: 0.5083371349714974]
	TIME [epoch: 3.43 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49712621909707394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49712621909707394 | validation: 0.46590114169116537]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_307.pth
	Model improved!!!
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5438701707058023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5438701707058023 | validation: 0.6376797017904714]
	TIME [epoch: 3.44 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5859552259194999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5859552259194999 | validation: 0.6003404301860634]
	TIME [epoch: 3.43 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6613918251104459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6613918251104459 | validation: 0.6667161232616046]
	TIME [epoch: 3.44 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6109525585196306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6109525585196306 | validation: 0.4486025587006879]
	TIME [epoch: 3.45 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_311.pth
	Model improved!!!
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4998545447770861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4998545447770861 | validation: 0.44849708992865295]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_312.pth
	Model improved!!!
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4741175717782194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4741175717782194 | validation: 0.4587143193566062]
	TIME [epoch: 3.44 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4572283673917886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4572283673917886 | validation: 0.4433575974667955]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_314.pth
	Model improved!!!
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4539512293704074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4539512293704074 | validation: 0.4426349487143226]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_315.pth
	Model improved!!!
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4449977154312566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4449977154312566 | validation: 0.41190820161287667]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_316.pth
	Model improved!!!
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44282652213950424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44282652213950424 | validation: 0.5578340283513561]
	TIME [epoch: 3.44 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4975386809571956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4975386809571956 | validation: 0.7479963606092613]
	TIME [epoch: 3.44 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7906869122700552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7906869122700552 | validation: 0.6122735480437076]
	TIME [epoch: 3.44 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6000249573657479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6000249573657479 | validation: 0.42289822115756925]
	TIME [epoch: 3.45 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4673811744653618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4673811744653618 | validation: 0.42046019851204053]
	TIME [epoch: 3.45 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4249945485804406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4249945485804406 | validation: 0.4182656584905015]
	TIME [epoch: 3.44 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42589583341446896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42589583341446896 | validation: 0.39104996783316537]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_323.pth
	Model improved!!!
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4211161262627113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4211161262627113 | validation: 0.5390750582088663]
	TIME [epoch: 3.44 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.475011345686067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.475011345686067 | validation: 0.6091019330137493]
	TIME [epoch: 3.44 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6744243003544458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6744243003544458 | validation: 0.5770101118588989]
	TIME [epoch: 3.44 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5635371382171277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5635371382171277 | validation: 0.4079312558256919]
	TIME [epoch: 3.44 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44002684109571394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44002684109571394 | validation: 0.40906929097911154]
	TIME [epoch: 3.44 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4040827422297953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4040827422297953 | validation: 0.37785204422100893]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_329.pth
	Model improved!!!
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40145045315889605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40145045315889605 | validation: 0.43627479689148263]
	TIME [epoch: 3.45 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4212496823141329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4212496823141329 | validation: 0.4249871793221591]
	TIME [epoch: 3.45 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48486782062814243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48486782062814243 | validation: 0.5566757323187012]
	TIME [epoch: 3.44 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49976094728059284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49976094728059284 | validation: 0.4834408092584969]
	TIME [epoch: 3.44 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5009081467698212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5009081467698212 | validation: 0.4708389789877323]
	TIME [epoch: 3.44 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4398451409134344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4398451409134344 | validation: 0.36637931763454223]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_335.pth
	Model improved!!!
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41979404026696626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41979404026696626 | validation: 0.3972662080393268]
	TIME [epoch: 3.43 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38039976540357556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38039976540357556 | validation: 0.3319343061073233]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_337.pth
	Model improved!!!
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36648974844614673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36648974844614673 | validation: 0.39050765613252275]
	TIME [epoch: 3.43 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3598619009572683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3598619009572683 | validation: 0.37090580513393007]
	TIME [epoch: 3.43 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39189529310491295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39189529310491295 | validation: 0.49726347483722944]
	TIME [epoch: 3.49 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43457764332277693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43457764332277693 | validation: 0.5549136407267765]
	TIME [epoch: 3.44 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5159431399030592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5159431399030592 | validation: 0.5236661947067196]
	TIME [epoch: 3.43 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45281340208192933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45281340208192933 | validation: 0.33470997707764155]
	TIME [epoch: 3.43 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34362002477693293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34362002477693293 | validation: 0.3341259937066973]
	TIME [epoch: 3.43 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3334556455457494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3334556455457494 | validation: 0.34667151653589007]
	TIME [epoch: 3.43 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32944002242220327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32944002242220327 | validation: 0.31780681482893897]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_346.pth
	Model improved!!!
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32370738326614756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32370738326614756 | validation: 0.3501366789560497]
	TIME [epoch: 3.44 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34030836646980483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34030836646980483 | validation: 0.4060689184234514]
	TIME [epoch: 3.44 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42080751945312117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42080751945312117 | validation: 0.35362496826652956]
	TIME [epoch: 3.44 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3193436099382805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3193436099382805 | validation: 0.3355638874906621]
	TIME [epoch: 3.45 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3092907786787456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3092907786787456 | validation: 0.3251553183295615]
	TIME [epoch: 3.45 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3089866005515372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3089866005515372 | validation: 0.5587682806327089]
	TIME [epoch: 3.44 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4362878394616419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4362878394616419 | validation: 0.5567208535387888]
	TIME [epoch: 3.44 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5269186433127323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5269186433127323 | validation: 0.4678982971231886]
	TIME [epoch: 3.44 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.370889126755158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.370889126755158 | validation: 0.32402105484854604]
	TIME [epoch: 3.44 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3793703701557988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3793703701557988 | validation: 0.31022951167454793]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_356.pth
	Model improved!!!
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.292913193075604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.292913193075604 | validation: 0.35721024798136874]
	TIME [epoch: 3.43 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34241331161454314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34241331161454314 | validation: 0.29728508889163996]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_358.pth
	Model improved!!!
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32695351259425154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32695351259425154 | validation: 0.29185388726138195]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_359.pth
	Model improved!!!
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27798912206019344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27798912206019344 | validation: 0.3163092890996536]
	TIME [epoch: 3.45 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2925595786833456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2925595786833456 | validation: 0.2993376852461539]
	TIME [epoch: 3.45 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.300710668871647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.300710668871647 | validation: 0.3202693205323249]
	TIME [epoch: 3.44 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2808815360957555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2808815360957555 | validation: 0.31428808526992597]
	TIME [epoch: 3.44 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29007521344822373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29007521344822373 | validation: 0.36652568654711853]
	TIME [epoch: 3.44 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2955848507734079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2955848507734079 | validation: 0.34263328119429615]
	TIME [epoch: 3.44 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31372321760558397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31372321760558397 | validation: 0.5611835397482615]
	TIME [epoch: 3.44 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4607365200409331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4607365200409331 | validation: 0.5748122649398982]
	TIME [epoch: 3.44 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5084823480749374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5084823480749374 | validation: 0.5369982690883361]
	TIME [epoch: 3.44 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49304323802868616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49304323802868616 | validation: 0.39817246906520387]
	TIME [epoch: 3.44 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37123197908186834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37123197908186834 | validation: 0.35497049184254476]
	TIME [epoch: 3.44 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37137437753479613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37137437753479613 | validation: 0.290273703042579]
	TIME [epoch: 3.45 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_371.pth
	Model improved!!!
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2633619098452398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2633619098452398 | validation: 0.28666715670786547]
	TIME [epoch: 3.45 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_372.pth
	Model improved!!!
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2821202787768972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2821202787768972 | validation: 0.28726641745309406]
	TIME [epoch: 3.43 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25277439960017717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25277439960017717 | validation: 0.2851374794716603]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_374.pth
	Model improved!!!
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2502837138412052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2502837138412052 | validation: 0.264393130178302]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_375.pth
	Model improved!!!
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2369057645724449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2369057645724449 | validation: 0.2712460463625109]
	TIME [epoch: 3.44 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23367683421698723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23367683421698723 | validation: 0.25765256890381355]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_377.pth
	Model improved!!!
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22415707896405507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22415707896405507 | validation: 0.257958353444891]
	TIME [epoch: 3.44 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21257848575971017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21257848575971017 | validation: 0.2623781938923499]
	TIME [epoch: 3.45 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21068294257517642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21068294257517642 | validation: 0.2747781718201972]
	TIME [epoch: 3.45 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21734136242944474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21734136242944474 | validation: 0.49361921792579705]
	TIME [epoch: 3.46 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3408454137303432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3408454137303432 | validation: 0.6991791252493083]
	TIME [epoch: 3.46 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6347422487737161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6347422487737161 | validation: 0.43341088146365037]
	TIME [epoch: 3.45 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36249933793702704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36249933793702704 | validation: 0.2701218212454433]
	TIME [epoch: 3.44 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3255919228066151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3255919228066151 | validation: 0.27127456688834306]
	TIME [epoch: 3.45 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2854287722231337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2854287722231337 | validation: 0.3009620653200463]
	TIME [epoch: 3.44 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26003560136606574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26003560136606574 | validation: 0.2725301951689218]
	TIME [epoch: 3.44 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23759881242201858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23759881242201858 | validation: 0.25574182979586724]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_388.pth
	Model improved!!!
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2307626827867471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2307626827867471 | validation: 0.27240252306069496]
	TIME [epoch: 3.43 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2280803283392892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2280803283392892 | validation: 0.2473987662063201]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_390.pth
	Model improved!!!
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2167396176267932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2167396176267932 | validation: 0.2568517713348511]
	TIME [epoch: 3.46 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20467018285811975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20467018285811975 | validation: 0.24517702628094093]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_392.pth
	Model improved!!!
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2143685180321135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2143685180321135 | validation: 0.27386132842670674]
	TIME [epoch: 3.44 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2200316420200462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2200316420200462 | validation: 0.3594346685707484]
	TIME [epoch: 3.44 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3248282999708454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3248282999708454 | validation: 0.8716074403056511]
	TIME [epoch: 3.43 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6350814751914482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6350814751914482 | validation: 0.472513539507286]
	TIME [epoch: 3.44 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43742863064973775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43742863064973775 | validation: 0.40719678656708364]
	TIME [epoch: 3.44 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39482636257690373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39482636257690373 | validation: 0.25100471063124713]
	TIME [epoch: 3.43 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23177313543422656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23177313543422656 | validation: 0.31001163813066157]
	TIME [epoch: 3.44 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2778351938887858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2778351938887858 | validation: 0.2504338489778918]
	TIME [epoch: 3.44 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.205429429347102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.205429429347102 | validation: 0.24213530373191272]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_401.pth
	Model improved!!!
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2124207172348133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2124207172348133 | validation: 0.26042186106835713]
	TIME [epoch: 3.43 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20833883376243187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20833883376243187 | validation: 0.2488131624738796]
	TIME [epoch: 3.44 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19494392674499364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19494392674499364 | validation: 0.2517524696502159]
	TIME [epoch: 3.43 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18828799222841444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18828799222841444 | validation: 0.23127960604132705]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_405.pth
	Model improved!!!
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18415909925468918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18415909925468918 | validation: 0.2390045521930757]
	TIME [epoch: 3.43 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.191051296192368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.191051296192368 | validation: 0.2837191220432455]
	TIME [epoch: 3.44 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24369253023498794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24369253023498794 | validation: 0.3893576656192437]
	TIME [epoch: 3.43 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33729690363202275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33729690363202275 | validation: 0.28734746083561064]
	TIME [epoch: 3.44 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2530558567217068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2530558567217068 | validation: 0.2487915002144063]
	TIME [epoch: 3.44 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20580739386796068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20580739386796068 | validation: 0.2536239905992887]
	TIME [epoch: 3.45 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.252870530661372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.252870530661372 | validation: 0.2334621042961974]
	TIME [epoch: 3.43 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17490242126354996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17490242126354996 | validation: 0.2249254767302901]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_413.pth
	Model improved!!!
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17136042428258663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17136042428258663 | validation: 0.22985347880372606]
	TIME [epoch: 3.43 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1834800900706564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1834800900706564 | validation: 0.2556396079228693]
	TIME [epoch: 3.44 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24200752816526863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24200752816526863 | validation: 0.2791594017860632]
	TIME [epoch: 3.43 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26975178639463276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26975178639463276 | validation: 0.3213954856205959]
	TIME [epoch: 3.43 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.285075509336914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.285075509336914 | validation: 0.2555013320919343]
	TIME [epoch: 3.44 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1992492875308607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1992492875308607 | validation: 0.28051491287443603]
	TIME [epoch: 3.43 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2325695994830909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2325695994830909 | validation: 0.2792733151105864]
	TIME [epoch: 3.44 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2305899095460333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2305899095460333 | validation: 0.3369947879316417]
	TIME [epoch: 3.44 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31832563068809394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31832563068809394 | validation: 0.21641328182461442]
	TIME [epoch: 3.45 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_422.pth
	Model improved!!!
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18171537542318206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18171537542318206 | validation: 0.2870885685668955]
	TIME [epoch: 3.43 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28584650979644816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28584650979644816 | validation: 0.3630911943379987]
	TIME [epoch: 3.43 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3205096829784765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3205096829784765 | validation: 0.5270732465891872]
	TIME [epoch: 3.43 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42306489599428954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42306489599428954 | validation: 0.2723972950182804]
	TIME [epoch: 3.43 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23663671723137722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23663671723137722 | validation: 0.25749721584230295]
	TIME [epoch: 3.43 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21049936405604963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21049936405604963 | validation: 0.23795491154930365]
	TIME [epoch: 3.43 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20461451833683292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20461451833683292 | validation: 0.2082644412357836]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_429.pth
	Model improved!!!
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18318180911091134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18318180911091134 | validation: 0.20279520544786034]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_430.pth
	Model improved!!!
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18166302955507532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18166302955507532 | validation: 0.20359033092256473]
	TIME [epoch: 3.44 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1669419362056304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1669419362056304 | validation: 0.21499786829670242]
	TIME [epoch: 3.45 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16639478630740484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16639478630740484 | validation: 0.20604900313484717]
	TIME [epoch: 3.44 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16657833255773447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16657833255773447 | validation: 0.25714841352150053]
	TIME [epoch: 3.44 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2196082623915603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2196082623915603 | validation: 0.4004845036213208]
	TIME [epoch: 3.44 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3839374750510314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3839374750510314 | validation: 0.43166990795691523]
	TIME [epoch: 3.43 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3830796013979655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3830796013979655 | validation: 0.24480021422582335]
	TIME [epoch: 3.44 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19487678626691454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19487678626691454 | validation: 0.22141563802362177]
	TIME [epoch: 3.44 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18247296204194985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18247296204194985 | validation: 0.25734244089885006]
	TIME [epoch: 3.44 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23636042846065233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23636042846065233 | validation: 0.21574740093120204]
	TIME [epoch: 3.44 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16930102279693848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16930102279693848 | validation: 0.2238856688568654]
	TIME [epoch: 3.45 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1738269704227305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1738269704227305 | validation: 0.23462240054558692]
	TIME [epoch: 3.45 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20078409089046298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20078409089046298 | validation: 0.23884004880128773]
	TIME [epoch: 3.44 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19365269159808932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19365269159808932 | validation: 0.21484530842772412]
	TIME [epoch: 3.44 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18317752182890812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18317752182890812 | validation: 0.22977370300797478]
	TIME [epoch: 3.44 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.182965353845429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.182965353845429 | validation: 0.2258841845127095]
	TIME [epoch: 3.44 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18336423773048716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18336423773048716 | validation: 0.31710876544783506]
	TIME [epoch: 3.44 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2757790547102669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2757790547102669 | validation: 0.2399670228846542]
	TIME [epoch: 3.44 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23368236981114757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23368236981114757 | validation: 0.22081693100659516]
	TIME [epoch: 3.44 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18148770657208196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18148770657208196 | validation: 0.21520933605068338]
	TIME [epoch: 3.44 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15565713215402416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15565713215402416 | validation: 0.21955069436973088]
	TIME [epoch: 3.44 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1650620460353718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1650620460353718 | validation: 0.19721123310832694]
	TIME [epoch: 3.45 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_452.pth
	Model improved!!!
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14652791793920295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14652791793920295 | validation: 0.20715735163248397]
	TIME [epoch: 3.44 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14373812965411925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14373812965411925 | validation: 0.19754995193342537]
	TIME [epoch: 3.43 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1501210822699802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1501210822699802 | validation: 0.24052209516774284]
	TIME [epoch: 3.44 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20546972186422094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20546972186422094 | validation: 0.41520381750242735]
	TIME [epoch: 3.44 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33323660745078715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33323660745078715 | validation: 0.2668275603637092]
	TIME [epoch: 3.44 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24830440128718104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24830440128718104 | validation: 0.26157673776005813]
	TIME [epoch: 3.44 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2275745617967884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2275745617967884 | validation: 0.18840086931661282]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_459.pth
	Model improved!!!
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16297282206180136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16297282206180136 | validation: 0.1791120146634289]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_460.pth
	Model improved!!!
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1747518976002835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1747518976002835 | validation: 0.22561710939662014]
	TIME [epoch: 3.44 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.234935418378847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.234935418378847 | validation: 0.2819750165485928]
	TIME [epoch: 3.47 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24615151192414644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24615151192414644 | validation: 0.1940293872229212]
	TIME [epoch: 3.44 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16894851504034847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16894851504034847 | validation: 0.24218314096832894]
	TIME [epoch: 3.45 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22906783764383762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22906783764383762 | validation: 0.28352867433468526]
	TIME [epoch: 3.45 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23868543283524696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23868543283524696 | validation: 0.30144126966906015]
	TIME [epoch: 3.44 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23967496186560766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23967496186560766 | validation: 0.1792211527723046]
	TIME [epoch: 3.44 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1446710957942596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1446710957942596 | validation: 0.21490597760645072]
	TIME [epoch: 3.45 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1681651318949907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1681651318949907 | validation: 0.17903413803249002]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_469.pth
	Model improved!!!
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1498460883699728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1498460883699728 | validation: 0.18744924179232345]
	TIME [epoch: 3.44 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13725521543953037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13725521543953037 | validation: 0.1808588671353458]
	TIME [epoch: 3.44 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1418468677244185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1418468677244185 | validation: 0.18012427320409619]
	TIME [epoch: 3.45 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16118063748307332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16118063748307332 | validation: 0.21330789704464848]
	TIME [epoch: 3.45 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19561233966829675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19561233966829675 | validation: 0.20645168081471388]
	TIME [epoch: 3.44 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1825866051029811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1825866051029811 | validation: 0.2189099321099656]
	TIME [epoch: 3.43 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18031675897409075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18031675897409075 | validation: 0.23197467228542779]
	TIME [epoch: 3.44 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1715970196453683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1715970196453683 | validation: 0.24983914465001114]
	TIME [epoch: 3.44 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2010957972824192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2010957972824192 | validation: 0.20765993834198038]
	TIME [epoch: 3.44 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1831295623381496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1831295623381496 | validation: 0.1829786968391899]
	TIME [epoch: 3.43 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14087001114301947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14087001114301947 | validation: 0.1900136971348733]
	TIME [epoch: 3.44 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1317686680081929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1317686680081929 | validation: 0.19296984013565197]
	TIME [epoch: 3.44 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14879903724728044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14879903724728044 | validation: 0.3380742169971118]
	TIME [epoch: 3.44 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29384865973864066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29384865973864066 | validation: 0.4212942189560789]
	TIME [epoch: 3.45 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3432200329372243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3432200329372243 | validation: 0.18200754363305385]
	TIME [epoch: 3.43 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14773153296804653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14773153296804653 | validation: 0.22680556841973543]
	TIME [epoch: 3.43 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17312192625511152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17312192625511152 | validation: 0.1860376177973989]
	TIME [epoch: 3.44 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13992526954108464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13992526954108464 | validation: 0.17753906433456534]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_487.pth
	Model improved!!!
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14823167871430387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14823167871430387 | validation: 0.20422915644673592]
	TIME [epoch: 3.43 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.200752508361245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.200752508361245 | validation: 0.25431467370481897]
	TIME [epoch: 3.44 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28100401959749377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28100401959749377 | validation: 0.21419712580838235]
	TIME [epoch: 3.43 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16833968697220436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16833968697220436 | validation: 0.16893429168727692]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_491.pth
	Model improved!!!
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1541401017248399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1541401017248399 | validation: 0.1785020881464525]
	TIME [epoch: 3.45 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15196159246237692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15196159246237692 | validation: 0.20006334171416096]
	TIME [epoch: 3.45 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15624756061750172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15624756061750172 | validation: 0.17867820525365952]
	TIME [epoch: 3.44 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14119371141657028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14119371141657028 | validation: 0.18489455033853153]
	TIME [epoch: 3.44 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1373715649683904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1373715649683904 | validation: 0.17901338686044202]
	TIME [epoch: 3.44 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1286740992875161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1286740992875161 | validation: 0.20492498335647708]
	TIME [epoch: 3.44 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15439185673637026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15439185673637026 | validation: 0.23782909966922663]
	TIME [epoch: 3.44 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20190444778904706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20190444778904706 | validation: 0.28739987974817355]
	TIME [epoch: 3.44 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24260018154551158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24260018154551158 | validation: 0.19320741395245977]
	TIME [epoch: 3.44 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1630225461700887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1630225461700887 | validation: 0.2161654917167386]
	TIME [epoch: 33.3 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20024764559597608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20024764559597608 | validation: 0.32511178617961384]
	TIME [epoch: 7.48 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26637699961059985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26637699961059985 | validation: 0.2093330921935325]
	TIME [epoch: 7.47 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2170161286100412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2170161286100412 | validation: 0.2768664831914924]
	TIME [epoch: 7.47 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.265212840949231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.265212840949231 | validation: 0.24579079469023288]
	TIME [epoch: 7.49 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1896356040723155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1896356040723155 | validation: 0.1906996990845077]
	TIME [epoch: 7.47 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1556469230704362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1556469230704362 | validation: 0.18273815600784526]
	TIME [epoch: 7.47 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12965309076901205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12965309076901205 | validation: 0.19309811541502012]
	TIME [epoch: 7.47 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12873689679236863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12873689679236863 | validation: 0.19602652580034613]
	TIME [epoch: 7.48 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15616971860321024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15616971860321024 | validation: 0.22126863783639694]
	TIME [epoch: 7.48 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1586291011849767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1586291011849767 | validation: 0.1874704550903346]
	TIME [epoch: 7.47 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13288239896918463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13288239896918463 | validation: 0.17832293010070446]
	TIME [epoch: 7.46 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12165601813696356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12165601813696356 | validation: 0.19404214147815246]
	TIME [epoch: 7.47 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13432298465052628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13432298465052628 | validation: 0.19817339488577296]
	TIME [epoch: 7.47 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14790863042167052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14790863042167052 | validation: 0.21727207117104386]
	TIME [epoch: 7.48 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16823500862612853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16823500862612853 | validation: 0.4572491407975048]
	TIME [epoch: 7.47 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.566337111118544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.566337111118544 | validation: 0.49760459798141804]
	TIME [epoch: 7.47 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.534118371595919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.534118371595919 | validation: 0.25746384522428784]
	TIME [epoch: 7.47 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21006630222333386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21006630222333386 | validation: 0.17545749404311642]
	TIME [epoch: 7.48 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1288056356486932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1288056356486932 | validation: 0.17908686276409178]
	TIME [epoch: 7.48 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15111618529929347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15111618529929347 | validation: 0.18818860373916244]
	TIME [epoch: 7.52 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13117368095573143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13117368095573143 | validation: 0.19476327667744778]
	TIME [epoch: 7.47 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12027528369108784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12027528369108784 | validation: 0.1811024098775038]
	TIME [epoch: 7.47 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1161621520086042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1161621520086042 | validation: 0.1777709627490335]
	TIME [epoch: 7.47 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11292698736472488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11292698736472488 | validation: 0.19105635450893288]
	TIME [epoch: 7.48 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12380142449561664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12380142449561664 | validation: 0.21290385590270766]
	TIME [epoch: 7.47 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14683960031299662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14683960031299662 | validation: 0.22210434094492212]
	TIME [epoch: 7.47 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17919042989256517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17919042989256517 | validation: 0.2304981407024378]
	TIME [epoch: 7.47 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16698170471137413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16698170471137413 | validation: 0.1908959405615996]
	TIME [epoch: 7.48 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13970471020045572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13970471020045572 | validation: 0.1992499127504802]
	TIME [epoch: 7.48 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12657525736518327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12657525736518327 | validation: 0.15528817600812572]
	TIME [epoch: 7.46 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_531.pth
	Model improved!!!
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10029279346305589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10029279346305589 | validation: 0.17208040526219723]
	TIME [epoch: 7.48 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09310061845076024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09310061845076024 | validation: 0.15582717273909702]
	TIME [epoch: 7.5 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09397203243185814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09397203243185814 | validation: 0.1658884046153057]
	TIME [epoch: 7.51 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10210829625078997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10210829625078997 | validation: 0.2085733733406778]
	TIME [epoch: 7.48 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1582152079978897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1582152079978897 | validation: 0.44052362477769247]
	TIME [epoch: 7.49 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4089718724086172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4089718724086172 | validation: 0.21119574160575053]
	TIME [epoch: 7.47 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1766575017428117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1766575017428117 | validation: 0.14492745964890502]
	TIME [epoch: 7.51 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_538.pth
	Model improved!!!
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10554843930859635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10554843930859635 | validation: 0.1691381733884756]
	TIME [epoch: 7.49 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12178670756962316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12178670756962316 | validation: 0.20450291002009113]
	TIME [epoch: 7.48 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1447008091522265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1447008091522265 | validation: 0.19010197120120179]
	TIME [epoch: 7.47 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1584903844083691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1584903844083691 | validation: 0.2755405843527579]
	TIME [epoch: 7.48 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22739024465244748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22739024465244748 | validation: 0.17071872776980432]
	TIME [epoch: 7.48 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12646729763386777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12646729763386777 | validation: 0.14624484431259963]
	TIME [epoch: 7.49 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08839897760602089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08839897760602089 | validation: 0.16797791918945984]
	TIME [epoch: 7.48 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10340206392378042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10340206392378042 | validation: 0.15503584785121136]
	TIME [epoch: 7.47 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13126951346155608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13126951346155608 | validation: 0.30381771630890714]
	TIME [epoch: 7.47 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2478173344052753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2478173344052753 | validation: 0.17495456101920667]
	TIME [epoch: 7.49 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11192444762613658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11192444762613658 | validation: 0.17471993929946214]
	TIME [epoch: 7.48 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15474074121906495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15474074121906495 | validation: 0.33977552847632203]
	TIME [epoch: 7.47 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27814877340427885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27814877340427885 | validation: 0.21696414490322535]
	TIME [epoch: 7.47 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22791525378313285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22791525378313285 | validation: 0.2685199868186885]
	TIME [epoch: 7.47 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2662845441308574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2662845441308574 | validation: 0.23173670901421764]
	TIME [epoch: 7.48 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18226087489315268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18226087489315268 | validation: 0.17841285830599196]
	TIME [epoch: 7.47 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13432275564152768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13432275564152768 | validation: 0.15993365589910513]
	TIME [epoch: 7.46 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10746817038310293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10746817038310293 | validation: 0.15934454832296724]
	TIME [epoch: 7.47 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11095249942726712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11095249942726712 | validation: 0.12989980646102106]
	TIME [epoch: 7.47 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_557.pth
	Model improved!!!
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09946807390293813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09946807390293813 | validation: 0.15715669590344886]
	TIME [epoch: 7.48 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09906642826102204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09906642826102204 | validation: 0.13939744840055845]
	TIME [epoch: 7.46 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08705245104290998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08705245104290998 | validation: 0.13940931870991155]
	TIME [epoch: 7.46 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09641447782351634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09641447782351634 | validation: 0.1667003386918805]
	TIME [epoch: 7.46 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12087991785177306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12087991785177306 | validation: 0.3008512538812122]
	TIME [epoch: 7.46 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23792376427424386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23792376427424386 | validation: 0.28313694113859555]
	TIME [epoch: 7.48 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2577526210271246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2577526210271246 | validation: 0.21498198004514527]
	TIME [epoch: 7.46 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15721309079503293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15721309079503293 | validation: 0.19341135874769588]
	TIME [epoch: 7.46 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1247059695817508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1247059695817508 | validation: 0.16370074274987456]
	TIME [epoch: 7.46 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1333984870686628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1333984870686628 | validation: 0.14523873205406176]
	TIME [epoch: 7.48 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08989170158432458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08989170158432458 | validation: 0.1355730653086149]
	TIME [epoch: 7.47 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08359938133422613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08359938133422613 | validation: 0.1217061757471937]
	TIME [epoch: 7.47 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_569.pth
	Model improved!!!
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0835428618408488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0835428618408488 | validation: 0.14673188842993018]
	TIME [epoch: 7.47 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08724997016432968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08724997016432968 | validation: 0.158387012252091]
	TIME [epoch: 7.47 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11665222020857315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11665222020857315 | validation: 0.3348643725600543]
	TIME [epoch: 7.48 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28170057356171485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28170057356171485 | validation: 0.28580996082837234]
	TIME [epoch: 7.47 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.233865863741978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.233865863741978 | validation: 0.13235012939792104]
	TIME [epoch: 7.47 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08042194778736624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08042194778736624 | validation: 0.15330696144806813]
	TIME [epoch: 7.47 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10441144817620142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10441144817620142 | validation: 0.1837242754457428]
	TIME [epoch: 7.48 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1474452230375339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1474452230375339 | validation: 0.28361459926651117]
	TIME [epoch: 7.47 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2255407463802703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2255407463802703 | validation: 0.21577768417083518]
	TIME [epoch: 7.47 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22706427126652406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22706427126652406 | validation: 0.1426600539896888]
	TIME [epoch: 7.47 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11385944197182721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11385944197182721 | validation: 0.18458800855193727]
	TIME [epoch: 7.47 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12652671754520628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12652671754520628 | validation: 0.17482454786667612]
	TIME [epoch: 7.49 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13767743628806606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13767743628806606 | validation: 0.16992292513386378]
	TIME [epoch: 7.47 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12230563530394552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12230563530394552 | validation: 0.150619237236564]
	TIME [epoch: 7.47 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10925084424988349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10925084424988349 | validation: 0.13980144313142004]
	TIME [epoch: 7.47 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09504083803791188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09504083803791188 | validation: 0.13150120064103407]
	TIME [epoch: 7.48 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07689502597543425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07689502597543425 | validation: 0.119238348218768]
	TIME [epoch: 7.48 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_586.pth
	Model improved!!!
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07851110531045119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07851110531045119 | validation: 0.14107496001929729]
	TIME [epoch: 7.47 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08407051087482613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08407051087482613 | validation: 0.19528362536473548]
	TIME [epoch: 7.47 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1773823108907599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1773823108907599 | validation: 0.4652395344530936]
	TIME [epoch: 7.48 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41495892079735347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41495892079735347 | validation: 0.26323635734112805]
	TIME [epoch: 7.48 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2816634889145108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2816634889145108 | validation: 0.1419277582463051]
	TIME [epoch: 7.47 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10410055345300796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10410055345300796 | validation: 0.1688353457464602]
	TIME [epoch: 7.47 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13258441430240406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13258441430240406 | validation: 0.11692354572547958]
	TIME [epoch: 7.48 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_593.pth
	Model improved!!!
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08589357798087457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08589357798087457 | validation: 0.1258042077553904]
	TIME [epoch: 7.48 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08184145832826127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08184145832826127 | validation: 0.11831618746278588]
	TIME [epoch: 7.48 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07852755038283622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07852755038283622 | validation: 0.11409369541342636]
	TIME [epoch: 7.47 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_596.pth
	Model improved!!!
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09614441792424806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09614441792424806 | validation: 0.22795468896037763]
	TIME [epoch: 7.48 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17422176459552038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17422176459552038 | validation: 0.2951305499276988]
	TIME [epoch: 7.48 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.252986306102221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.252986306102221 | validation: 0.2090713003591861]
	TIME [epoch: 7.48 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15967146996325107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15967146996325107 | validation: 0.13517411803326926]
	TIME [epoch: 7.47 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11130003792717652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11130003792717652 | validation: 0.18712672349446755]
	TIME [epoch: 7.47 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14627427468663975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14627427468663975 | validation: 0.23589775314598735]
	TIME [epoch: 7.46 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.169936614000527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.169936614000527 | validation: 0.1570309502921736]
	TIME [epoch: 7.47 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09916530186802376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09916530186802376 | validation: 0.11718963005639163]
	TIME [epoch: 7.48 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0806727635473494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0806727635473494 | validation: 0.147869583439337]
	TIME [epoch: 7.46 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08577552945972397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08577552945972397 | validation: 0.12833744072990458]
	TIME [epoch: 7.47 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07684114898138589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07684114898138589 | validation: 0.12633099757458754]
	TIME [epoch: 7.46 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11227227482219082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11227227482219082 | validation: 0.31438789208523454]
	TIME [epoch: 7.49 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2747705359897482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2747705359897482 | validation: 0.28551893990150834]
	TIME [epoch: 7.47 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22096384141320755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22096384141320755 | validation: 0.12936158601407696]
	TIME [epoch: 7.47 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11536795992886824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11536795992886824 | validation: 0.1678732278707134]
	TIME [epoch: 7.46 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13293300117553428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13293300117553428 | validation: 0.23297300811805322]
	TIME [epoch: 7.47 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19929136676883402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19929136676883402 | validation: 0.18794086827808262]
	TIME [epoch: 7.48 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12305399037316445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12305399037316445 | validation: 0.11603418457357839]
	TIME [epoch: 7.47 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08863950350033081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08863950350033081 | validation: 0.14581486890277118]
	TIME [epoch: 7.46 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10833538889993577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10833538889993577 | validation: 0.1975611727495007]
	TIME [epoch: 7.46 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14143483422762823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14143483422762823 | validation: 0.18578371922500403]
	TIME [epoch: 7.47 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13637555952565825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13637555952565825 | validation: 0.14190454169359912]
	TIME [epoch: 7.47 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09036682990844731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09036682990844731 | validation: 0.10568691305759033]
	TIME [epoch: 7.46 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_619.pth
	Model improved!!!
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06612023815109172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06612023815109172 | validation: 0.10797680017631009]
	TIME [epoch: 7.47 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06345333705702266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06345333705702266 | validation: 0.10806313593262247]
	TIME [epoch: 7.47 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06296603093323787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06296603093323787 | validation: 0.1390138245657487]
	TIME [epoch: 7.49 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08323638188013936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08323638188013936 | validation: 0.2570155731355121]
	TIME [epoch: 7.47 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2966587482077676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2966587482077676 | validation: 0.5332637901875611]
	TIME [epoch: 7.47 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48229866360182805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48229866360182805 | validation: 0.318970568086123]
	TIME [epoch: 7.47 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23953267568771544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23953267568771544 | validation: 0.15431043812545608]
	TIME [epoch: 7.47 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10441871956687707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10441871956687707 | validation: 0.15064067770378042]
	TIME [epoch: 7.48 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12791071497897333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12791071497897333 | validation: 0.15055055044999383]
	TIME [epoch: 7.46 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09781267873535023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09781267873535023 | validation: 0.180352470113617]
	TIME [epoch: 7.47 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1266262002204185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1266262002204185 | validation: 0.13073273194937354]
	TIME [epoch: 7.47 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08639566655371987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08639566655371987 | validation: 0.10077185820202963]
	TIME [epoch: 7.48 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_631.pth
	Model improved!!!
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08508747479102503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08508747479102503 | validation: 0.11601895220042313]
	TIME [epoch: 7.46 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0790457736050149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0790457736050149 | validation: 0.13485151417146277]
	TIME [epoch: 7.46 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09421266573865092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09421266573865092 | validation: 0.16262047148014158]
	TIME [epoch: 7.46 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11146647552323152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11146647552323152 | validation: 0.18502876523825018]
	TIME [epoch: 7.47 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13183593522488443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13183593522488443 | validation: 0.14930029697005068]
	TIME [epoch: 7.48 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1316707257192292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1316707257192292 | validation: 0.25214679676044105]
	TIME [epoch: 7.47 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19769924234464245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19769924234464245 | validation: 0.1933865700165602]
	TIME [epoch: 7.46 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16516100489006633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16516100489006633 | validation: 0.1899691607718264]
	TIME [epoch: 7.47 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1312109650188692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1312109650188692 | validation: 0.1417130199051917]
	TIME [epoch: 7.48 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08674887544989375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08674887544989375 | validation: 0.11142624625647508]
	TIME [epoch: 7.44 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07980339955296209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07980339955296209 | validation: 0.13789011750164856]
	TIME [epoch: 7.44 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08957679584957016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08957679584957016 | validation: 0.14913565670816645]
	TIME [epoch: 7.44 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09521446486929792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09521446486929792 | validation: 0.14855015330775898]
	TIME [epoch: 7.46 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10954505079512292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10954505079512292 | validation: 0.1500991806417591]
	TIME [epoch: 7.48 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11435217296322148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11435217296322148 | validation: 0.19221363467324487]
	TIME [epoch: 7.46 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13603008299504787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13603008299504787 | validation: 0.10820237083898601]
	TIME [epoch: 7.44 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08601554636174852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08601554636174852 | validation: 0.11988597429606436]
	TIME [epoch: 7.45 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.081041913167787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.081041913167787 | validation: 0.11406991971431209]
	TIME [epoch: 7.45 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08007250913336213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08007250913336213 | validation: 0.19816910719563952]
	TIME [epoch: 7.47 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1375444733502211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1375444733502211 | validation: 0.15716906716249818]
	TIME [epoch: 7.44 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11602518008310472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11602518008310472 | validation: 0.2074535496801637]
	TIME [epoch: 7.52 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1957832569347057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1957832569347057 | validation: 0.30443537836446577]
	TIME [epoch: 7.45 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23714508645220156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23714508645220156 | validation: 0.2333541034651857]
	TIME [epoch: 7.46 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16921724803767282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16921724803767282 | validation: 0.1020334674347656]
	TIME [epoch: 7.44 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08062797649462099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08062797649462099 | validation: 0.10475508662132382]
	TIME [epoch: 7.45 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08328668013716566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08328668013716566 | validation: 0.11000163981463576]
	TIME [epoch: 7.44 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07420273202802725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07420273202802725 | validation: 0.10495517354404466]
	TIME [epoch: 7.46 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07089816083856952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07089816083856952 | validation: 0.10268472074591713]
	TIME [epoch: 7.48 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08093203331918346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08093203331918346 | validation: 0.20953176877955185]
	TIME [epoch: 7.47 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1571111449717969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1571111449717969 | validation: 0.2054218880378429]
	TIME [epoch: 7.47 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16194821967058676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16194821967058676 | validation: 0.15462874881591948]
	TIME [epoch: 7.44 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12228581757205527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12228581757205527 | validation: 0.12871005376336409]
	TIME [epoch: 7.45 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09730402193608295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09730402193608295 | validation: 0.17416228495319422]
	TIME [epoch: 7.46 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10568363835806954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10568363835806954 | validation: 0.11545288511693874]
	TIME [epoch: 7.45 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06924484087593887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06924484087593887 | validation: 0.11160704172324698]
	TIME [epoch: 7.45 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0744595669462501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0744595669462501 | validation: 0.10968674521411988]
	TIME [epoch: 7.48 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07468956253881881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07468956253881881 | validation: 0.1875488008497268]
	TIME [epoch: 7.46 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13390444351673997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13390444351673997 | validation: 0.1876633988037974]
	TIME [epoch: 7.44 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20153200461752782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20153200461752782 | validation: 0.3286218804192561]
	TIME [epoch: 7.46 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.261316824645625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.261316824645625 | validation: 0.230489460717216]
	TIME [epoch: 7.47 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15252087508187046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15252087508187046 | validation: 0.12141523765678541]
	TIME [epoch: 7.47 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08888144163824649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08888144163824649 | validation: 0.13988492024665297]
	TIME [epoch: 7.49 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.118749914706432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.118749914706432 | validation: 0.1271856798324444]
	TIME [epoch: 7.47 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09120475666547315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09120475666547315 | validation: 0.14213353158072334]
	TIME [epoch: 7.47 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09288752938539978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09288752938539978 | validation: 0.10853857791238669]
	TIME [epoch: 7.47 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07930183364179808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07930183364179808 | validation: 0.1115337640413192]
	TIME [epoch: 7.49 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0862132333100157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0862132333100157 | validation: 0.18149405010759273]
	TIME [epoch: 7.47 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12618287450531387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12618287450531387 | validation: 0.14253923930518644]
	TIME [epoch: 7.47 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08529400429176136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08529400429176136 | validation: 0.0981314900941116]
	TIME [epoch: 7.47 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_680.pth
	Model improved!!!
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08745799236113323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08745799236113323 | validation: 0.15207496711637314]
	TIME [epoch: 7.45 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.087143632674372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.087143632674372 | validation: 0.12757061399909483]
	TIME [epoch: 7.45 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1020634514126962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1020634514126962 | validation: 0.1493827303672544]
	TIME [epoch: 7.44 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13015209143996248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13015209143996248 | validation: 0.2115562050936377]
	TIME [epoch: 7.44 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2480161824752225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2480161824752225 | validation: 0.33886442024216695]
	TIME [epoch: 7.44 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2574944240753592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2574944240753592 | validation: 0.27721929697763886]
	TIME [epoch: 7.44 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20585011085754945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20585011085754945 | validation: 0.14082732062320397]
	TIME [epoch: 7.46 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09654750802101723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09654750802101723 | validation: 0.11797556599815691]
	TIME [epoch: 7.44 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10721764217814056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10721764217814056 | validation: 0.11301969459505115]
	TIME [epoch: 7.44 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07823463393671488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07823463393671488 | validation: 0.12120344489141682]
	TIME [epoch: 7.44 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07774675605041824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07774675605041824 | validation: 0.10662222062284732]
	TIME [epoch: 7.45 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0678253654426108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0678253654426108 | validation: 0.08466485520859827]
	TIME [epoch: 7.44 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_692.pth
	Model improved!!!
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06442036217583776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06442036217583776 | validation: 0.10590870808367453]
	TIME [epoch: 7.47 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0737337182980668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0737337182980668 | validation: 0.12987825918646986]
	TIME [epoch: 7.47 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10789217136306002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10789217136306002 | validation: 0.22218680196723]
	TIME [epoch: 7.47 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1682838246518942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1682838246518942 | validation: 0.15799241617835547]
	TIME [epoch: 7.47 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15822215996222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15822215996222 | validation: 0.2555347557066057]
	TIME [epoch: 7.44 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1903938700656373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1903938700656373 | validation: 0.23586931154285484]
	TIME [epoch: 7.45 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17663334752063997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17663334752063997 | validation: 0.09492205656033753]
	TIME [epoch: 7.45 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08460798394164615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08460798394164615 | validation: 0.09476588983342105]
	TIME [epoch: 7.48 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08742749984307985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08742749984307985 | validation: 0.12120030106144918]
	TIME [epoch: 7.45 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07647186522710067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07647186522710067 | validation: 0.1002513345746669]
	TIME [epoch: 7.46 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07176252258535312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07176252258535312 | validation: 0.12631907571092219]
	TIME [epoch: 7.46 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08884855984366072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08884855984366072 | validation: 0.14840836354928721]
	TIME [epoch: 7.45 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15823260499921882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15823260499921882 | validation: 0.3363637034185298]
	TIME [epoch: 7.47 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2576864264312593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2576864264312593 | validation: 0.24662349713864962]
	TIME [epoch: 7.45 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17057614826846518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17057614826846518 | validation: 0.08893867268853553]
	TIME [epoch: 7.45 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06150224102631597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06150224102631597 | validation: 0.10796609353677344]
	TIME [epoch: 7.45 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11384462122560784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11384462122560784 | validation: 0.11051309445198153]
	TIME [epoch: 7.45 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07718724879506085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07718724879506085 | validation: 0.15942188431584864]
	TIME [epoch: 7.46 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11780861126887757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11780861126887757 | validation: 0.17614905157514912]
	TIME [epoch: 7.44 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15241768565299563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15241768565299563 | validation: 0.1933639644429219]
	TIME [epoch: 7.45 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21776062128777185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21776062128777185 | validation: 0.22299684051905536]
	TIME [epoch: 7.45 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14610123317205342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14610123317205342 | validation: 0.17727042150973826]
	TIME [epoch: 7.46 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12484604789710473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12484604789710473 | validation: 0.10638984274621777]
	TIME [epoch: 7.44 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07197527071968585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07197527071968585 | validation: 0.10059028481367674]
	TIME [epoch: 7.44 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09264521452580438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09264521452580438 | validation: 0.122302951035958]
	TIME [epoch: 7.44 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09165816714679373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09165816714679373 | validation: 0.14853482809917842]
	TIME [epoch: 7.46 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1027663876904996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1027663876904996 | validation: 0.1063394742174487]
	TIME [epoch: 7.46 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0801110964408327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0801110964408327 | validation: 0.0908402951648445]
	TIME [epoch: 7.44 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07118285010714635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07118285010714635 | validation: 0.1046827822074969]
	TIME [epoch: 7.47 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07593525656432291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07593525656432291 | validation: 0.10151081560523392]
	TIME [epoch: 7.45 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06250309437900702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06250309437900702 | validation: 0.10487974025374439]
	TIME [epoch: 7.47 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06688243503505463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06688243503505463 | validation: 0.09622167990729698]
	TIME [epoch: 7.47 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08737129264724976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08737129264724976 | validation: 0.26409232810531796]
	TIME [epoch: 7.45 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20655697286139865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20655697286139865 | validation: 0.2341787606431348]
	TIME [epoch: 7.47 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19073719831460825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19073719831460825 | validation: 0.13820725793742977]
	TIME [epoch: 7.47 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11266712171478843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11266712171478843 | validation: 0.11693203983014869]
	TIME [epoch: 7.48 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11502984064195125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11502984064195125 | validation: 0.12483770073210375]
	TIME [epoch: 7.47 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06929148874435122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06929148874435122 | validation: 0.06869833397381622]
	TIME [epoch: 7.47 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_730.pth
	Model improved!!!
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052406921698991714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052406921698991714 | validation: 0.08781778697275187]
	TIME [epoch: 7.45 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05628758367219034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05628758367219034 | validation: 0.10339754051134538]
	TIME [epoch: 7.45 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10901231564675935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10901231564675935 | validation: 0.34749214443317733]
	TIME [epoch: 7.46 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2632658313154294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2632658313154294 | validation: 0.2548292615350642]
	TIME [epoch: 7.44 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19074919388931932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19074919388931932 | validation: 0.10694519156241089]
	TIME [epoch: 7.45 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06444975171765753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06444975171765753 | validation: 0.14954675212334653]
	TIME [epoch: 7.45 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17062418007101635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17062418007101635 | validation: 0.1981165294351136]
	TIME [epoch: 7.46 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.151390923053344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.151390923053344 | validation: 0.21327425425721042]
	TIME [epoch: 7.5 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16625403707044414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16625403707044414 | validation: 0.11613848229646694]
	TIME [epoch: 7.44 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0880133176773365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0880133176773365 | validation: 0.08684741056499197]
	TIME [epoch: 7.44 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0802435299491178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0802435299491178 | validation: 0.07785693094971331]
	TIME [epoch: 7.45 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05343376892226145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05343376892226145 | validation: 0.0728144305651071]
	TIME [epoch: 7.46 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05206722168146328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05206722168146328 | validation: 0.0783931527934072]
	TIME [epoch: 7.45 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0486755123979664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0486755123979664 | validation: 0.07778672259158598]
	TIME [epoch: 7.44 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0496152236357594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0496152236357594 | validation: 0.11000957274285983]
	TIME [epoch: 7.44 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0716778315507633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0716778315507633 | validation: 0.1459485943808168]
	TIME [epoch: 7.46 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14981461599537846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14981461599537846 | validation: 0.3150935997727361]
	TIME [epoch: 7.45 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2545789075690147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2545789075690147 | validation: 0.22892254993054484]
	TIME [epoch: 7.44 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1632058965788137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1632058965788137 | validation: 0.07053832499705809]
	TIME [epoch: 7.45 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05368059810514143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05368059810514143 | validation: 0.08527141413922414]
	TIME [epoch: 7.44 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07883053983170128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07883053983170128 | validation: 0.08258445308303076]
	TIME [epoch: 7.46 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05908396452771067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05908396452771067 | validation: 0.08300688617466484]
	TIME [epoch: 7.43 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05915327628969258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05915327628969258 | validation: 0.12766362794922623]
	TIME [epoch: 7.45 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10031793112131913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10031793112131913 | validation: 0.2366196175791239]
	TIME [epoch: 7.44 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19311663826657224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19311663826657224 | validation: 0.1746524214324525]
	TIME [epoch: 7.46 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13386350750314915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13386350750314915 | validation: 0.06164577558504877]
	TIME [epoch: 7.45 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_756.pth
	Model improved!!!
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05681894562116925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05681894562116925 | validation: 0.11273325449611478]
	TIME [epoch: 7.45 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0807328878705396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0807328878705396 | validation: 0.13957970423842425]
	TIME [epoch: 7.44 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1191309270180769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1191309270180769 | validation: 0.1788137343445689]
	TIME [epoch: 7.44 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13080315244864985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13080315244864985 | validation: 0.14860182740294176]
	TIME [epoch: 7.46 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12383979916823956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12383979916823956 | validation: 0.1641009585228507]
	TIME [epoch: 7.45 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13580338578976656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13580338578976656 | validation: 0.1190941767892848]
	TIME [epoch: 7.45 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14599482015631238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14599482015631238 | validation: 0.20195757236294654]
	TIME [epoch: 7.46 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14026848809378525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14026848809378525 | validation: 0.16705073238711365]
	TIME [epoch: 7.45 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11816938758806073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11816938758806073 | validation: 0.06793508328269723]
	TIME [epoch: 7.47 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05667025501167679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05667025501167679 | validation: 0.07859728132879656]
	TIME [epoch: 7.44 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0758162396711603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0758162396711603 | validation: 0.08499823753338011]
	TIME [epoch: 7.45 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06309084018080581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06309084018080581 | validation: 0.07190285361412221]
	TIME [epoch: 7.45 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05473215104335616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05473215104335616 | validation: 0.07007385136927954]
	TIME [epoch: 7.47 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05911746293734114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05911746293734114 | validation: 0.09024973916418656]
	TIME [epoch: 7.45 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06823903531370011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06823903531370011 | validation: 0.17890899570315566]
	TIME [epoch: 7.46 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1270231674744975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1270231674744975 | validation: 0.14958684735855501]
	TIME [epoch: 7.44 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11261341850839401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11261341850839401 | validation: 0.09880800926478382]
	TIME [epoch: 7.45 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0990252959611292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0990252959611292 | validation: 0.12890298462535554]
	TIME [epoch: 7.46 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09724119766010687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09724119766010687 | validation: 0.17598356094012935]
	TIME [epoch: 7.47 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1320660117366706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1320660117366706 | validation: 0.10783916600177253]
	TIME [epoch: 7.45 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08234366929305231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08234366929305231 | validation: 0.0703052395398204]
	TIME [epoch: 7.47 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052377421737316965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052377421737316965 | validation: 0.0881291373340522]
	TIME [epoch: 7.45 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05700956165229243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05700956165229243 | validation: 0.054718511162472894]
	TIME [epoch: 7.47 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_779.pth
	Model improved!!!
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038614842219889124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038614842219889124 | validation: 0.09572940677745959]
	TIME [epoch: 7.46 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06773868034503465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06773868034503465 | validation: 0.16789722537744226]
	TIME [epoch: 7.47 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20839165195425305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20839165195425305 | validation: 0.4174966712251329]
	TIME [epoch: 7.46 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33710843474367336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33710843474367336 | validation: 0.3695242192112847]
	TIME [epoch: 7.48 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2539157141270757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2539157141270757 | validation: 0.2008311089268954]
	TIME [epoch: 7.47 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13439020408114868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13439020408114868 | validation: 0.07088705616964008]
	TIME [epoch: 7.46 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054963905977077426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054963905977077426 | validation: 0.0636497883895825]
	TIME [epoch: 7.46 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07177296079896515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07177296079896515 | validation: 0.0847790171783864]
	TIME [epoch: 7.46 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0740581791652119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0740581791652119 | validation: 0.08944268778982484]
	TIME [epoch: 7.49 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0717031382616117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0717031382616117 | validation: 0.09184844993688272]
	TIME [epoch: 7.47 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07259783357110929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07259783357110929 | validation: 0.07725375128586544]
	TIME [epoch: 7.46 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08205262659461113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08205262659461113 | validation: 0.16111630887523096]
	TIME [epoch: 7.5 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12189999796378959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12189999796378959 | validation: 0.10952417487557159]
	TIME [epoch: 7.48 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09109591019235579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09109591019235579 | validation: 0.1096531762429764]
	TIME [epoch: 7.46 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10196960687336269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10196960687336269 | validation: 0.16159354193649927]
	TIME [epoch: 7.46 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12862665898584266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12862665898584266 | validation: 0.14000515809048628]
	TIME [epoch: 7.47 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11017987469887086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11017987469887086 | validation: 0.0744368153721256]
	TIME [epoch: 7.46 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056045345603557756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056045345603557756 | validation: 0.06454934970446623]
	TIME [epoch: 7.49 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05390766232259676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05390766232259676 | validation: 0.14161344887630892]
	TIME [epoch: 7.46 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09279269675587369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09279269675587369 | validation: 0.11111275035395114]
	TIME [epoch: 7.46 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09195257580460284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09195257580460284 | validation: 0.14304674170551182]
	TIME [epoch: 7.46 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12059973046023313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12059973046023313 | validation: 0.11114269442951459]
	TIME [epoch: 7.47 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13091347309654136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13091347309654136 | validation: 0.24966598329349857]
	TIME [epoch: 7.47 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16940224096191348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16940224096191348 | validation: 0.19069404768297005]
	TIME [epoch: 7.46 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13151032217839018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13151032217839018 | validation: 0.07755413834521782]
	TIME [epoch: 7.46 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05509464130238682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05509464130238682 | validation: 0.08961763250668796]
	TIME [epoch: 7.46 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10837371901133754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10837371901133754 | validation: 0.13507257462583497]
	TIME [epoch: 7.47 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09818931430282472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09818931430282472 | validation: 0.16572083994408318]
	TIME [epoch: 7.46 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11259908767764422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11259908767764422 | validation: 0.1007328062052765]
	TIME [epoch: 7.46 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08081885012357054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08081885012357054 | validation: 0.09210478776715532]
	TIME [epoch: 7.45 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1028942325375024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1028942325375024 | validation: 0.1353369937142305]
	TIME [epoch: 7.47 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11346163761216932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11346163761216932 | validation: 0.175219406899038]
	TIME [epoch: 7.47 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13897325137526442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13897325137526442 | validation: 0.1348666266314313]
	TIME [epoch: 7.46 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1064773536957773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1064773536957773 | validation: 0.07716322818516136]
	TIME [epoch: 7.46 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07412596558516599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07412596558516599 | validation: 0.06980056119293841]
	TIME [epoch: 7.45 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05443202666693356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05443202666693356 | validation: 0.05399702890059286]
	TIME [epoch: 7.48 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_815.pth
	Model improved!!!
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04092500112013738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04092500112013738 | validation: 0.052123306691182994]
	TIME [epoch: 7.46 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_816.pth
	Model improved!!!
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0397433249568423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0397433249568423 | validation: 0.06056383293601765]
	TIME [epoch: 7.45 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036443278179134043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036443278179134043 | validation: 0.0651835495591089]
	TIME [epoch: 7.45 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09432587148294015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09432587148294015 | validation: 0.2735850040758192]
	TIME [epoch: 7.45 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19314333959553875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19314333959553875 | validation: 0.4039855094745212]
	TIME [epoch: 7.47 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3160067835048248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3160067835048248 | validation: 0.16794702818463458]
	TIME [epoch: 7.46 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12916523281034706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12916523281034706 | validation: 0.09313078059148909]
	TIME [epoch: 7.46 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10810360163440606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10810360163440606 | validation: 0.10243151515296792]
	TIME [epoch: 7.46 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07638410314230598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07638410314230598 | validation: 0.09693015935761394]
	TIME [epoch: 7.48 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0828076244151048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0828076244151048 | validation: 0.07518161273536726]
	TIME [epoch: 7.45 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06534437481708238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06534437481708238 | validation: 0.10161011333886615]
	TIME [epoch: 7.47 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10728915745653314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10728915745653314 | validation: 0.1398190803964112]
	TIME [epoch: 7.45 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1155516606881708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1155516606881708 | validation: 0.12006625582058253]
	TIME [epoch: 7.47 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09336881719846048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09336881719846048 | validation: 0.07295140733762853]
	TIME [epoch: 7.47 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05612843251779886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05612843251779886 | validation: 0.05664135023262571]
	TIME [epoch: 7.46 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04365540619076649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04365540619076649 | validation: 0.06350361766365178]
	TIME [epoch: 7.45 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04784043914636788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04784043914636788 | validation: 0.05530272001403574]
	TIME [epoch: 7.46 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04498792355004686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04498792355004686 | validation: 0.140922252155257]
	TIME [epoch: 7.45 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0915886300800971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0915886300800971 | validation: 0.20001814250493144]
	TIME [epoch: 7.46 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17287667344124305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17287667344124305 | validation: 0.1968641578415317]
	TIME [epoch: 7.46 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1522609598726385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1522609598726385 | validation: 0.0915134694372147]
	TIME [epoch: 7.46 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059453165247556464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059453165247556464 | validation: 0.054270083526492756]
	TIME [epoch: 7.46 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05231134784537078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05231134784537078 | validation: 0.08493908436914677]
	TIME [epoch: 7.47 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06847339853744912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06847339853744912 | validation: 0.09327782076460994]
	TIME [epoch: 7.46 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0774621960572969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0774621960572969 | validation: 0.10527352408616611]
	TIME [epoch: 7.45 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07720616838926929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07720616838926929 | validation: 0.06843570672834286]
	TIME [epoch: 7.45 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053553346234948285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053553346234948285 | validation: 0.10863404580874306]
	TIME [epoch: 7.46 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08083397854296276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08083397854296276 | validation: 0.1735325225461939]
	TIME [epoch: 7.47 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1823714863948932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1823714863948932 | validation: 0.2526765051494961]
	TIME [epoch: 7.45 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20171160996180162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20171160996180162 | validation: 0.11642193117230773]
	TIME [epoch: 7.45 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07925074091760062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07925074091760062 | validation: 0.06434620133500578]
	TIME [epoch: 7.45 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06390926356198635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06390926356198635 | validation: 0.0919126368083265]
	TIME [epoch: 7.46 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07632741154806154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07632741154806154 | validation: 0.08606619241754256]
	TIME [epoch: 7.46 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06582127311110626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06582127311110626 | validation: 0.07104185086979943]
	TIME [epoch: 7.45 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0470051066879149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0470051066879149 | validation: 0.05487763418137163]
	TIME [epoch: 7.45 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04383889324878311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04383889324878311 | validation: 0.08411517341782437]
	TIME [epoch: 7.46 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052252843037087805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052252843037087805 | validation: 0.052037769584496324]
	TIME [epoch: 7.47 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_852.pth
	Model improved!!!
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040746700880528515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040746700880528515 | validation: 0.06024608642082875]
	TIME [epoch: 7.46 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04868567866122743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04868567866122743 | validation: 0.10927116255364516]
	TIME [epoch: 7.46 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07715514231403636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07715514231403636 | validation: 0.22619188985017305]
	TIME [epoch: 7.46 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1711386117420274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1711386117420274 | validation: 0.14577183041238276]
	TIME [epoch: 7.46 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10696338565538721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10696338565538721 | validation: 0.06733929334194656]
	TIME [epoch: 7.47 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05149112763257778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05149112763257778 | validation: 0.059867945200943885]
	TIME [epoch: 7.46 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03512551628813769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03512551628813769 | validation: 0.050511047594369675]
	TIME [epoch: 7.45 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_859.pth
	Model improved!!!
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05159737024330187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05159737024330187 | validation: 0.1953150794067954]
	TIME [epoch: 7.46 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.141879523423169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.141879523423169 | validation: 0.1524110524933307]
	TIME [epoch: 7.47 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15357264004683413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15357264004683413 | validation: 0.16876987286380396]
	TIME [epoch: 7.45 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1316357067380744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1316357067380744 | validation: 0.10512858661762273]
	TIME [epoch: 7.45 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10834365842306784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10834365842306784 | validation: 0.16913125561991155]
	TIME [epoch: 7.46 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.115333277539192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.115333277539192 | validation: 0.185789964799974]
	TIME [epoch: 7.45 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12781879151111045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12781879151111045 | validation: 0.08070980758715829]
	TIME [epoch: 7.47 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06353605574017603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06353605574017603 | validation: 0.1030952468722156]
	TIME [epoch: 7.45 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12349065000262406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12349065000262406 | validation: 0.09577038368194293]
	TIME [epoch: 7.46 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07796240945520155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07796240945520155 | validation: 0.12131630212644683]
	TIME [epoch: 7.46 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0820857168582978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0820857168582978 | validation: 0.08265177785761862]
	TIME [epoch: 7.47 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06177417378361645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06177417378361645 | validation: 0.08707273661084264]
	TIME [epoch: 7.46 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0915123707623524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0915123707623524 | validation: 0.10567231310834845]
	TIME [epoch: 7.45 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09641793308893924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09641793308893924 | validation: 0.10739791784083226]
	TIME [epoch: 7.46 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09880684403133859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09880684403133859 | validation: 0.08647563500234316]
	TIME [epoch: 7.46 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07698699505004447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07698699505004447 | validation: 0.06389390993235265]
	TIME [epoch: 7.47 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06086044013114323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06086044013114323 | validation: 0.09920174596443472]
	TIME [epoch: 7.45 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06456308570216333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06456308570216333 | validation: 0.066338691632287]
	TIME [epoch: 7.45 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08023360766975392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08023360766975392 | validation: 0.20704026956433808]
	TIME [epoch: 7.46 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14309049805976198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14309049805976198 | validation: 0.1705331870232666]
	TIME [epoch: 7.46 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12185514909469472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12185514909469472 | validation: 0.0768490072675172]
	TIME [epoch: 7.47 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06163696472338288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06163696472338288 | validation: 0.06632631008111672]
	TIME [epoch: 7.45 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06996459658843784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06996459658843784 | validation: 0.13924385655223123]
	TIME [epoch: 7.45 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09976966935531748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09976966935531748 | validation: 0.15106134951104516]
	TIME [epoch: 7.46 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.104180335631886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.104180335631886 | validation: 0.09087286813774798]
	TIME [epoch: 7.47 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07493176238131376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07493176238131376 | validation: 0.08979652299177726]
	TIME [epoch: 7.46 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07984681751473012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07984681751473012 | validation: 0.09028747570579793]
	TIME [epoch: 7.46 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0917411831343634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0917411831343634 | validation: 0.08625926075505401]
	TIME [epoch: 7.46 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06574247998666805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06574247998666805 | validation: 0.06870473622595268]
	TIME [epoch: 7.46 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07225333884331223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07225333884331223 | validation: 0.15756665786515356]
	TIME [epoch: 7.48 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12533299787189242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12533299787189242 | validation: 0.31210172532158187]
	TIME [epoch: 7.46 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.364515753658674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.364515753658674 | validation: 0.13807421859364344]
	TIME [epoch: 7.46 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11327713264392182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11327713264392182 | validation: 0.10992915188298996]
	TIME [epoch: 7.45 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07638653132210588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07638653132210588 | validation: 0.082917672149541]
	TIME [epoch: 7.47 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0658400164098296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0658400164098296 | validation: 0.08441826326570213]
	TIME [epoch: 7.48 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0622682975295491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0622682975295491 | validation: 0.06583326260636364]
	TIME [epoch: 7.46 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04699404302729194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04699404302729194 | validation: 0.0570361857225057]
	TIME [epoch: 7.46 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04573854340798758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04573854340798758 | validation: 0.0526793423870465]
	TIME [epoch: 7.45 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04396596177858176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04396596177858176 | validation: 0.056864395130534574]
	TIME [epoch: 7.47 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048570600687244725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048570600687244725 | validation: 0.07828642084149795]
	TIME [epoch: 7.46 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0659569716567925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0659569716567925 | validation: 0.15757955223719475]
	TIME [epoch: 7.46 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12611919428401752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12611919428401752 | validation: 0.13346986254194207]
	TIME [epoch: 7.45 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1089429504747275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1089429504747275 | validation: 0.09692443429979067]
	TIME [epoch: 7.46 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07141210493160101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07141210493160101 | validation: 0.09229270982296395]
	TIME [epoch: 7.46 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1450237175356558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1450237175356558 | validation: 0.17974785804246682]
	TIME [epoch: 7.46 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13379780835821903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13379780835821903 | validation: 0.16969545843272066]
	TIME [epoch: 7.45 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12226633374443711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12226633374443711 | validation: 0.07290108838269031]
	TIME [epoch: 7.47 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05059115298961918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05059115298961918 | validation: 0.08628744583887876]
	TIME [epoch: 7.46 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10655246034620305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10655246034620305 | validation: 0.1033333520046948]
	TIME [epoch: 7.46 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07805760871991071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07805760871991071 | validation: 0.11778600449856047]
	TIME [epoch: 7.45 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07742874876609313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07742874876609313 | validation: 0.054337105235677065]
	TIME [epoch: 7.46 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04768874480823259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04768874480823259 | validation: 0.05751330815010558]
	TIME [epoch: 7.46 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05576563978948697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05576563978948697 | validation: 0.05226681053760458]
	TIME [epoch: 7.48 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05715599742452923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05715599742452923 | validation: 0.09697222530581534]
	TIME [epoch: 7.45 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08191380353043336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08191380353043336 | validation: 0.14586004889366713]
	TIME [epoch: 7.46 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13096460002695093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13096460002695093 | validation: 0.17905681606227494]
	TIME [epoch: 7.45 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13873663102041542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13873663102041542 | validation: 0.07442619763043913]
	TIME [epoch: 7.47 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0650645393721699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0650645393721699 | validation: 0.045439471214465066]
	TIME [epoch: 7.46 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_917.pth
	Model improved!!!
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03329191268749314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03329191268749314 | validation: 0.0493769235880101]
	TIME [epoch: 7.47 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03574333686882985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03574333686882985 | validation: 0.06033463533511582]
	TIME [epoch: 7.45 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05201265185524654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05201265185524654 | validation: 0.08047012116025154]
	TIME [epoch: 7.47 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06979971877700347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06979971877700347 | validation: 0.13352597401971308]
	TIME [epoch: 7.47 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11466873738128282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11466873738128282 | validation: 0.1087486137657912]
	TIME [epoch: 7.46 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0876057759207215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0876057759207215 | validation: 0.07706519980537181]
	TIME [epoch: 7.45 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06256910746797371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06256910746797371 | validation: 0.07079832435184227]
	TIME [epoch: 7.46 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059101855789626755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059101855789626755 | validation: 0.13532703959329193]
	TIME [epoch: 7.46 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10461568512669413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10461568512669413 | validation: 0.1007871232254694]
	TIME [epoch: 7.47 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09117939497158169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09117939497158169 | validation: 0.08968033820164273]
	TIME [epoch: 7.45 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06641170942945086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06641170942945086 | validation: 0.04637202776100022]
	TIME [epoch: 7.45 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04033774272789724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04033774272789724 | validation: 0.2025716906202908]
	TIME [epoch: 7.46 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22514709779915104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22514709779915104 | validation: 0.174691763851539]
	TIME [epoch: 7.46 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12317020411742252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12317020411742252 | validation: 0.06385860288612132]
	TIME [epoch: 7.45 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06382232792366187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06382232792366187 | validation: 0.07650718691298564]
	TIME [epoch: 7.46 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06511529041249293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06511529041249293 | validation: 0.08936473670256916]
	TIME [epoch: 7.45 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06960817457836552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06960817457836552 | validation: 0.059420582139258785]
	TIME [epoch: 7.49 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05705642547524091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05705642547524091 | validation: 0.04097265621915104]
	TIME [epoch: 7.46 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_935.pth
	Model improved!!!
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03629145762498359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03629145762498359 | validation: 0.07108372304621977]
	TIME [epoch: 7.46 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060024038636231694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060024038636231694 | validation: 0.05216948601919611]
	TIME [epoch: 7.45 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05304384277800867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05304384277800867 | validation: 0.1323141386392452]
	TIME [epoch: 7.45 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11596637843150495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11596637843150495 | validation: 0.1268019728842149]
	TIME [epoch: 7.47 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13728230410333161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13728230410333161 | validation: 0.19700582081342471]
	TIME [epoch: 7.45 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13583110449117494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13583110449117494 | validation: 0.11424450902343926]
	TIME [epoch: 7.45 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07913801199088677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07913801199088677 | validation: 0.05866624110802206]
	TIME [epoch: 7.45 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04414849046634158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04414849046634158 | validation: 0.07673970584468474]
	TIME [epoch: 7.45 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061283691025714786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061283691025714786 | validation: 0.08846678912723335]
	TIME [epoch: 7.48 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06000699718220585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06000699718220585 | validation: 0.07526749333092832]
	TIME [epoch: 7.45 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06214159918920746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06214159918920746 | validation: 0.09217166189457732]
	TIME [epoch: 7.46 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09695505527417918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09695505527417918 | validation: 0.1522891693212444]
	TIME [epoch: 7.45 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12073521018160825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12073521018160825 | validation: 0.08710435709930879]
	TIME [epoch: 7.46 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07021679066769597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07021679066769597 | validation: 0.04446564472303463]
	TIME [epoch: 7.47 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05317759017884181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05317759017884181 | validation: 0.06758322989028519]
	TIME [epoch: 7.45 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045719354175752305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045719354175752305 | validation: 0.054067556619000784]
	TIME [epoch: 7.55 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041048410362386545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041048410362386545 | validation: 0.0461270796011544]
	TIME [epoch: 7.45 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04377254132185676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04377254132185676 | validation: 0.05471624407565502]
	TIME [epoch: 7.47 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04301281987334368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04301281987334368 | validation: 0.06934341225732957]
	TIME [epoch: 7.46 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05567688927576321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05567688927576321 | validation: 0.13771583027922546]
	TIME [epoch: 7.45 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14158079969504553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14158079969504553 | validation: 0.3745965680836321]
	TIME [epoch: 7.45 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30612350687816864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30612350687816864 | validation: 0.24323081849661696]
	TIME [epoch: 7.46 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17416645410240306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17416645410240306 | validation: 0.10724016004836923]
	TIME [epoch: 7.47 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07951179241332465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07951179241332465 | validation: 0.08414047081729698]
	TIME [epoch: 7.45 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06929577120802276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06929577120802276 | validation: 0.04470440958910965]
	TIME [epoch: 7.46 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03228483324722659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03228483324722659 | validation: 0.040728880730186184]
	TIME [epoch: 7.45 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_961.pth
	Model improved!!!
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03221288867801198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03221288867801198 | validation: 0.03647316668899221]
	TIME [epoch: 7.47 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_962.pth
	Model improved!!!
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037674761989461195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037674761989461195 | validation: 0.06089843187666255]
	TIME [epoch: 7.47 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043970867509392074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043970867509392074 | validation: 0.0865879864135779]
	TIME [epoch: 7.46 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13426114645522827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13426114645522827 | validation: 0.24995839310589218]
	TIME [epoch: 7.46 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18799467806137013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18799467806137013 | validation: 0.2179845433358623]
	TIME [epoch: 7.47 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15462238324324584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15462238324324584 | validation: 0.07976820537684658]
	TIME [epoch: 7.48 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07407282506680389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07407282506680389 | validation: 0.06189304295076925]
	TIME [epoch: 7.45 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05742624256381126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05742624256381126 | validation: 0.05675412689522929]
	TIME [epoch: 7.46 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043995560264873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043995560264873 | validation: 0.05753350459609352]
	TIME [epoch: 7.46 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053291659546960614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053291659546960614 | validation: 0.08461959849803859]
	TIME [epoch: 7.47 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0668386620871269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0668386620871269 | validation: 0.08004744789047955]
	TIME [epoch: 7.48 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07370420603193432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07370420603193432 | validation: 0.06463939045551245]
	TIME [epoch: 7.46 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05621436044536878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05621436044536878 | validation: 0.03902882143867145]
	TIME [epoch: 7.45 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03416593012868543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03416593012868543 | validation: 0.036785088816255675]
	TIME [epoch: 7.47 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051440679543830414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051440679543830414 | validation: 0.12356556689597964]
	TIME [epoch: 7.47 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09085975375063725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09085975375063725 | validation: 0.10652784729250353]
	TIME [epoch: 7.46 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07923434895508061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07923434895508061 | validation: 0.11951882129867793]
	TIME [epoch: 7.46 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10760742055379333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10760742055379333 | validation: 0.061888962303328625]
	TIME [epoch: 7.45 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058194934499247744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058194934499247744 | validation: 0.08717375940142799]
	TIME [epoch: 7.46 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07087541631988777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07087541631988777 | validation: 0.0915287918753695]
	TIME [epoch: 7.47 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11354407114294407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11354407114294407 | validation: 0.17788241203167524]
	TIME [epoch: 7.46 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13867153986335137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13867153986335137 | validation: 0.1522881200229081]
	TIME [epoch: 7.45 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12022828074146977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12022828074146977 | validation: 0.06608378338063133]
	TIME [epoch: 7.45 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0486323705784044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0486323705784044 | validation: 0.07081833917849704]
	TIME [epoch: 7.47 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09051559255502102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09051559255502102 | validation: 0.12066794784294917]
	TIME [epoch: 7.47 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10181256354581883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10181256354581883 | validation: 0.1466634994783536]
	TIME [epoch: 7.45 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10695156455846853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10695156455846853 | validation: 0.06033475857541697]
	TIME [epoch: 7.45 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04962746731019024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04962746731019024 | validation: 0.07842144009991159]
	TIME [epoch: 7.46 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08963030897073193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08963030897073193 | validation: 0.15825549516568294]
	TIME [epoch: 7.47 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14512690911171425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14512690911171425 | validation: 0.18183953522016266]
	TIME [epoch: 7.46 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16201408266043413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16201408266043413 | validation: 0.08722066288090999]
	TIME [epoch: 7.46 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0791501700799099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0791501700799099 | validation: 0.0948279573899186]
	TIME [epoch: 7.45 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11215896317250433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11215896317250433 | validation: 0.09630489300008649]
	TIME [epoch: 7.47 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08371366488229455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08371366488229455 | validation: 0.0685849298800962]
	TIME [epoch: 7.47 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06928110788294264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06928110788294264 | validation: 0.044907950743182136]
	TIME [epoch: 7.46 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03979185133899615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03979185133899615 | validation: 0.04638112228643282]
	TIME [epoch: 7.46 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04493021728808046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04493021728808046 | validation: 0.05547726953247206]
	TIME [epoch: 7.46 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04562376925174465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04562376925174465 | validation: 0.11970513177249023]
	TIME [epoch: 7.47 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11064370414968529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11064370414968529 | validation: 0.15501892993770144]
	TIME [epoch: 7.46 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12447324059851803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12447324059851803 | validation: 0.08913883668809285]
	TIME [epoch: 41.1 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0742773896718682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0742773896718682 | validation: 0.04868649865171301]
	TIME [epoch: 16 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03393020218431996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03393020218431996 | validation: 0.05776444050756229]
	TIME [epoch: 16 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09111495232112268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09111495232112268 | validation: 0.1148088479359881]
	TIME [epoch: 16 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08354173774705682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08354173774705682 | validation: 0.1118203417740058]
	TIME [epoch: 16 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09499637392340543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09499637392340543 | validation: 0.0881513303633871]
	TIME [epoch: 16 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07173180258755588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07173180258755588 | validation: 0.09218491485705708]
	TIME [epoch: 16 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10063637423855516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10063637423855516 | validation: 0.0869436634876457]
	TIME [epoch: 16 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06983722900812964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06983722900812964 | validation: 0.05859932922821044]
	TIME [epoch: 16 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058315308782346434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058315308782346434 | validation: 0.05154997689099076]
	TIME [epoch: 16 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04872125223051883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04872125223051883 | validation: 0.062024789511828576]
	TIME [epoch: 16 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04341016074319742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04341016074319742 | validation: 0.0946219435520066]
	TIME [epoch: 16 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07507043337139065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07507043337139065 | validation: 0.08487249297506297]
	TIME [epoch: 16 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06710902092623046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06710902092623046 | validation: 0.12178357305838394]
	TIME [epoch: 16 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09681494718766265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09681494718766265 | validation: 0.10647599943492257]
	TIME [epoch: 16 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0847141480838759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0847141480838759 | validation: 0.07612095742871394]
	TIME [epoch: 16 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059500094007915026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059500094007915026 | validation: 0.05401376131680996]
	TIME [epoch: 16 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05487685100103862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05487685100103862 | validation: 0.12697796137435935]
	TIME [epoch: 16 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10031458982175316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10031458982175316 | validation: 0.0969141079793655]
	TIME [epoch: 16 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08546658958598355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08546658958598355 | validation: 0.09867408612228164]
	TIME [epoch: 16 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08457253708453706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08457253708453706 | validation: 0.07055788531591461]
	TIME [epoch: 16 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05543919056630161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05543919056630161 | validation: 0.06645646568044669]
	TIME [epoch: 16 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043465768271681436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043465768271681436 | validation: 0.047597395798969044]
	TIME [epoch: 16 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051642765151473286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051642765151473286 | validation: 0.06161225063019095]
	TIME [epoch: 16 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04863411466521091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04863411466521091 | validation: 0.030693029434630162]
	TIME [epoch: 16 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_1025.pth
	Model improved!!!
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032392554086027786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032392554086027786 | validation: 0.05078312559962292]
	TIME [epoch: 16 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04437738475040847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04437738475040847 | validation: 0.0769482666247296]
	TIME [epoch: 16 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06670890497970898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06670890497970898 | validation: 0.18435190708947186]
	TIME [epoch: 16 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15684751600944308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15684751600944308 | validation: 0.13490553948327033]
	TIME [epoch: 16 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12649390072940805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12649390072940805 | validation: 0.05668544407918739]
	TIME [epoch: 16 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05165685369573333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05165685369573333 | validation: 0.05101345296657178]
	TIME [epoch: 16 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05013646878583433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05013646878583433 | validation: 0.09497761424808433]
	TIME [epoch: 16 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0671155881000119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0671155881000119 | validation: 0.049893155334094136]
	TIME [epoch: 16 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040161609378818214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040161609378818214 | validation: 0.05698226619278551]
	TIME [epoch: 16 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05356164773278392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05356164773278392 | validation: 0.10702480899418987]
	TIME [epoch: 16 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08391875831235295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08391875831235295 | validation: 0.05264587817395157]
	TIME [epoch: 16 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0443357680260475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0443357680260475 | validation: 0.05394244934664938]
	TIME [epoch: 16 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04859720097394261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04859720097394261 | validation: 0.07014585923528543]
	TIME [epoch: 16 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05118909839578602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05118909839578602 | validation: 0.03530437217792274]
	TIME [epoch: 16 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027964414125429923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027964414125429923 | validation: 0.06092897134596229]
	TIME [epoch: 16 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10204419124165093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10204419124165093 | validation: 0.1933290447349502]
	TIME [epoch: 16 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13349613791198586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13349613791198586 | validation: 0.28791097050429376]
	TIME [epoch: 16 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18845208809916955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18845208809916955 | validation: 0.1625682073970654]
	TIME [epoch: 16 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12388881262212217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12388881262212217 | validation: 0.1330188184736105]
	TIME [epoch: 16 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12897171980592348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12897171980592348 | validation: 0.10886265389355963]
	TIME [epoch: 16 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11106145144090043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11106145144090043 | validation: 0.17767949768308441]
	TIME [epoch: 16 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20794897030484027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20794897030484027 | validation: 0.18873117702225964]
	TIME [epoch: 16 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14938480059047174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14938480059047174 | validation: 0.036912966562547424]
	TIME [epoch: 16 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06706740853995193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06706740853995193 | validation: 0.04681497533578876]
	TIME [epoch: 16 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04763500569777914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04763500569777914 | validation: 0.062325959531316755]
	TIME [epoch: 16 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05255366996518413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05255366996518413 | validation: 0.04296687473448529]
	TIME [epoch: 16 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04041868768815462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04041868768815462 | validation: 0.03817679546066232]
	TIME [epoch: 16 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03551817931617614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03551817931617614 | validation: 0.04533338005791]
	TIME [epoch: 16 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03822569722673566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03822569722673566 | validation: 0.05130842348463597]
	TIME [epoch: 16 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05242701612326503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05242701612326503 | validation: 0.08954317759997935]
	TIME [epoch: 16 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.084162079039469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.084162079039469 | validation: 0.0978705509509444]
	TIME [epoch: 16 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10169973915037153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10169973915037153 | validation: 0.13760770919879659]
	TIME [epoch: 16 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11098808230156436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11098808230156436 | validation: 0.06824112416955716]
	TIME [epoch: 16 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05416535356466778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05416535356466778 | validation: 0.03587319882431619]
	TIME [epoch: 16 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0345672201299993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0345672201299993 | validation: 0.04455995814936117]
	TIME [epoch: 16 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03177562797678728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03177562797678728 | validation: 0.03388658281038318]
	TIME [epoch: 16 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03497712558809885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03497712558809885 | validation: 0.08162014456132004]
	TIME [epoch: 16 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06258184244804196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06258184244804196 | validation: 0.07301745305504352]
	TIME [epoch: 16 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06972698459018252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06972698459018252 | validation: 0.11841985958363109]
	TIME [epoch: 16 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09724511903610729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09724511903610729 | validation: 0.09052653684999379]
	TIME [epoch: 16 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07579477286557539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07579477286557539 | validation: 0.08765505812963284]
	TIME [epoch: 16 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07635333731287125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07635333731287125 | validation: 0.12925776931903357]
	TIME [epoch: 16 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14834878780067035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14834878780067035 | validation: 0.1879765485418915]
	TIME [epoch: 16 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13394248619086718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13394248619086718 | validation: 0.3096609803437365]
	TIME [epoch: 16 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17717231884152987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17717231884152987 | validation: 0.190600594997952]
	TIME [epoch: 16 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12377370317345836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12377370317345836 | validation: 0.08838658454167994]
	TIME [epoch: 16 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061598066605520314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061598066605520314 | validation: 0.05193648874162186]
	TIME [epoch: 16 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05806515181700702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05806515181700702 | validation: 0.05039059497090934]
	TIME [epoch: 16 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0532341834723703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0532341834723703 | validation: 0.06066060047847445]
	TIME [epoch: 16 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07430647833463017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07430647833463017 | validation: 0.08465230593155888]
	TIME [epoch: 16 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08352960148150172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08352960148150172 | validation: 0.07936733320480141]
	TIME [epoch: 16 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06888339496177975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06888339496177975 | validation: 0.05448455150945029]
	TIME [epoch: 16 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05180277618237584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05180277618237584 | validation: 0.06023207847965489]
	TIME [epoch: 16 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044893073242528454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044893073242528454 | validation: 0.07334519566687292]
	TIME [epoch: 16 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06222231463355174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06222231463355174 | validation: 0.09199107596352331]
	TIME [epoch: 16 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07104400363808115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07104400363808115 | validation: 0.06719992689133845]
	TIME [epoch: 16 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05785290347294348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05785290347294348 | validation: 0.06802509306360752]
	TIME [epoch: 16 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056903380525037815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056903380525037815 | validation: 0.052465099031646006]
	TIME [epoch: 16 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05088163241928598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05088163241928598 | validation: 0.040618079023493715]
	TIME [epoch: 16 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03197546915513122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03197546915513122 | validation: 0.041273119039493045]
	TIME [epoch: 16 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0371272433819978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0371272433819978 | validation: 0.057107436761973523]
	TIME [epoch: 16 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054563689691751904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054563689691751904 | validation: 0.1389177996613969]
	TIME [epoch: 16 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12766302259883264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12766302259883264 | validation: 0.1308074763003748]
	TIME [epoch: 16 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13607741298740636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13607741298740636 | validation: 0.12374761803417794]
	TIME [epoch: 16 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09769228065416431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09769228065416431 | validation: 0.05920683783479222]
	TIME [epoch: 16 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044869584129841406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044869584129841406 | validation: 0.04945593185000385]
	TIME [epoch: 16 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06142442506649813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06142442506649813 | validation: 0.08013273291895892]
	TIME [epoch: 16 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07554467106627767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07554467106627767 | validation: 0.05262675424732031]
	TIME [epoch: 16 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051169473969778556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051169473969778556 | validation: 0.059077981809830565]
	TIME [epoch: 16 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09160746835082362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09160746835082362 | validation: 0.06702071106613053]
	TIME [epoch: 16.1 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05357511280699637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05357511280699637 | validation: 0.0504787429785906]
	TIME [epoch: 16 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04316425274774598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04316425274774598 | validation: 0.041614637265361513]
	TIME [epoch: 16 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0419061134722814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0419061134722814 | validation: 0.035632160237323705]
	TIME [epoch: 16 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029360735654085906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029360735654085906 | validation: 0.02763853088627879]
	TIME [epoch: 16 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_1099.pth
	Model improved!!!
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027480232849060985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027480232849060985 | validation: 0.03339110098192111]
	TIME [epoch: 16 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03279325912286522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03279325912286522 | validation: 0.127848995349625]
	TIME [epoch: 16 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10938960017707526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10938960017707526 | validation: 0.17546670616922092]
	TIME [epoch: 16 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15024049858727995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15024049858727995 | validation: 0.08087526424615284]
	TIME [epoch: 16 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07683414763365085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07683414763365085 | validation: 0.028240904478286467]
	TIME [epoch: 16 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025902569461270588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025902569461270588 | validation: 0.05418339424593741]
	TIME [epoch: 16 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036301097472876485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036301097472876485 | validation: 0.03237032053260386]
	TIME [epoch: 16 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03435975359334918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03435975359334918 | validation: 0.04976027841505779]
	TIME [epoch: 16 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05121003724926031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05121003724926031 | validation: 0.13973471330871676]
	TIME [epoch: 16 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11045875288663372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11045875288663372 | validation: 0.11045817949198655]
	TIME [epoch: 16 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10087381246322018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10087381246322018 | validation: 0.050696897870046664]
	TIME [epoch: 16 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0419249790457089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0419249790457089 | validation: 0.028407966796565078]
	TIME [epoch: 16 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02542888482084873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02542888482084873 | validation: 0.034065545658930566]
	TIME [epoch: 16 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027947149692101842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027947149692101842 | validation: 0.07549208246816591]
	TIME [epoch: 16 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06494378924414203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06494378924414203 | validation: 0.08095335736330649]
	TIME [epoch: 16 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0776225796590553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0776225796590553 | validation: 0.13973294210714593]
	TIME [epoch: 16 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12315765020410957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12315765020410957 | validation: 0.11211519570411915]
	TIME [epoch: 16 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11824091953792987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11824091953792987 | validation: 0.1354817496774635]
	TIME [epoch: 16 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10104219654816386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10104219654816386 | validation: 0.09052725732964027]
	TIME [epoch: 16 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07433402611038882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07433402611038882 | validation: 0.035097196738614056]
	TIME [epoch: 16 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02748083312536677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02748083312536677 | validation: 0.05312662111189847]
	TIME [epoch: 16 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07201655172999354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07201655172999354 | validation: 0.07375941315934781]
	TIME [epoch: 16 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05332849877648261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05332849877648261 | validation: 0.06618772164622391]
	TIME [epoch: 16 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053525031627443695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053525031627443695 | validation: 0.03614356569651247]
	TIME [epoch: 16 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031039535723885445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031039535723885445 | validation: 0.03301787462532513]
	TIME [epoch: 16 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03430360374827745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03430360374827745 | validation: 0.061495145456212076]
	TIME [epoch: 16 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061993975338956155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061993975338956155 | validation: 0.08876151698816527]
	TIME [epoch: 16 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10047109066982762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10047109066982762 | validation: 0.11823752599655563]
	TIME [epoch: 16 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10517035760779447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10517035760779447 | validation: 0.05287888784943526]
	TIME [epoch: 16 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053137463910285974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053137463910285974 | validation: 0.047400906420087]
	TIME [epoch: 16 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04684031440478474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04684031440478474 | validation: 0.14043085614051348]
	TIME [epoch: 16 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1474240999344552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1474240999344552 | validation: 0.17102053234251569]
	TIME [epoch: 16 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14894279629437918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14894279629437918 | validation: 0.14607620239817218]
	TIME [epoch: 16 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11438145424375488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11438145424375488 | validation: 0.026771350029390874]
	TIME [epoch: 16 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_1133.pth
	Model improved!!!
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02374153730398942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02374153730398942 | validation: 0.05342290838249078]
	TIME [epoch: 16 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08769408238751346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08769408238751346 | validation: 0.06682819257385066]
	TIME [epoch: 16 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056261310989416655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056261310989416655 | validation: 0.05665824387105972]
	TIME [epoch: 16 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05478185550864355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05478185550864355 | validation: 0.041162787540142645]
	TIME [epoch: 16 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03531657971990562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03531657971990562 | validation: 0.049458056653525984]
	TIME [epoch: 16 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048725333268815466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048725333268815466 | validation: 0.0811530621177521]
	TIME [epoch: 16 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06487585326840403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06487585326840403 | validation: 0.08447473977261122]
	TIME [epoch: 16 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07458773036445633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07458773036445633 | validation: 0.07501739841287429]
	TIME [epoch: 16 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06533398088029595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06533398088029595 | validation: 0.052374306205216895]
	TIME [epoch: 16 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04752483467407216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04752483467407216 | validation: 0.0725500262495544]
	TIME [epoch: 16 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06334874343625548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06334874343625548 | validation: 0.05704969804902204]
	TIME [epoch: 16 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04898699794855894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04898699794855894 | validation: 0.07053791955369941]
	TIME [epoch: 16 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06625895835381401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06625895835381401 | validation: 0.09331213388260855]
	TIME [epoch: 16 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0890783193174108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0890783193174108 | validation: 0.08800688554243273]
	TIME [epoch: 16 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0811502243428916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0811502243428916 | validation: 0.046415654667557094]
	TIME [epoch: 16 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040297705904656936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040297705904656936 | validation: 0.026203744608088497]
	TIME [epoch: 16 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_1149.pth
	Model improved!!!
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027474280695539543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027474280695539543 | validation: 0.030123440677067393]
	TIME [epoch: 16 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025275620558489233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025275620558489233 | validation: 0.03775514320716929]
	TIME [epoch: 16 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03296561536141599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03296561536141599 | validation: 0.06054287700813811]
	TIME [epoch: 16 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0598318747297046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0598318747297046 | validation: 0.1317706381083989]
	TIME [epoch: 16 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10469860156267853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10469860156267853 | validation: 0.07378271078640553]
	TIME [epoch: 16 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08487980140544142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08487980140544142 | validation: 0.09702688344906625]
	TIME [epoch: 16 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08369800442813391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08369800442813391 | validation: 0.06516151797650102]
	TIME [epoch: 16 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049592725362300846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049592725362300846 | validation: 0.06967244593698216]
	TIME [epoch: 16 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0775104504511584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0775104504511584 | validation: 0.07300035292053102]
	TIME [epoch: 16 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05598123309346291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05598123309346291 | validation: 0.043422882964017785]
	TIME [epoch: 16 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03664996701987199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03664996701987199 | validation: 0.05105111827298987]
	TIME [epoch: 16 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051064611027666705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051064611027666705 | validation: 0.04342194478867764]
	TIME [epoch: 16 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04592719617786488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04592719617786488 | validation: 0.02807488305764192]
	TIME [epoch: 16 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025120002498768813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025120002498768813 | validation: 0.03388448712639709]
	TIME [epoch: 16 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031217098425423685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031217098425423685 | validation: 0.07802005302021241]
	TIME [epoch: 16 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08871608825316801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08871608825316801 | validation: 0.2707508739972964]
	TIME [epoch: 16 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20915428332493477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20915428332493477 | validation: 0.2237536093469145]
	TIME [epoch: 16 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17848663646915763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17848663646915763 | validation: 0.08637643692274377]
	TIME [epoch: 16 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0767938137734246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0767938137734246 | validation: 0.06365268020187069]
	TIME [epoch: 16 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05950184751493591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05950184751493591 | validation: 0.045699492326953056]
	TIME [epoch: 16 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03545912191712353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03545912191712353 | validation: 0.03374162539925087]
	TIME [epoch: 16 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039533147617076166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039533147617076166 | validation: 0.026386639189004548]
	TIME [epoch: 16 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023845967324554424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023845967324554424 | validation: 0.023651792169041896]
	TIME [epoch: 16 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_1172.pth
	Model improved!!!
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02091232568288694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02091232568288694 | validation: 0.025060358437374197]
	TIME [epoch: 16 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01961454853262431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01961454853262431 | validation: 0.04135260297522408]
	TIME [epoch: 16 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03893283207256369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03893283207256369 | validation: 0.07170713055954632]
	TIME [epoch: 16 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1272856011158848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1272856011158848 | validation: 0.23177862325615145]
	TIME [epoch: 16 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19042441461942772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19042441461942772 | validation: 0.23227241030241053]
	TIME [epoch: 16 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18428200307610362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18428200307610362 | validation: 0.09530757446153268]
	TIME [epoch: 16 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0896474770182971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0896474770182971 | validation: 0.05589130775081017]
	TIME [epoch: 16 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05895807042457049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05895807042457049 | validation: 0.035220459742987875]
	TIME [epoch: 16 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030228343586035402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030228343586035402 | validation: 0.034969415229512295]
	TIME [epoch: 16 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03446335059468455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03446335059468455 | validation: 0.04784232538376634]
	TIME [epoch: 16 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05086939653766481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05086939653766481 | validation: 0.06893363637594661]
	TIME [epoch: 16 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06358913655425372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06358913655425372 | validation: 0.06277503983239402]
	TIME [epoch: 16 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057757802399787325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057757802399787325 | validation: 0.05347156815799611]
	TIME [epoch: 16 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04931939755288193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04931939755288193 | validation: 0.03493212607567083]
	TIME [epoch: 16 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036318941813842584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036318941813842584 | validation: 0.019440755883943117]
	TIME [epoch: 16 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_1187.pth
	Model improved!!!
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024154358035850608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024154358035850608 | validation: 0.02761233521416987]
	TIME [epoch: 16 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028026623445079314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028026623445079314 | validation: 0.04027804543824246]
	TIME [epoch: 16 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03645620496575679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03645620496575679 | validation: 0.06450628473036676]
	TIME [epoch: 16 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050536742497939095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050536742497939095 | validation: 0.09051625089858734]
	TIME [epoch: 16 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08541513621620275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08541513621620275 | validation: 0.11945168996461314]
	TIME [epoch: 16 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09419526408827235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09419526408827235 | validation: 0.06993257306509386]
	TIME [epoch: 16 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0666695935217915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0666695935217915 | validation: 0.04157558511131363]
	TIME [epoch: 16 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031781127346145965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031781127346145965 | validation: 0.02706026504239474]
	TIME [epoch: 16 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023943728402657866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023943728402657866 | validation: 0.03389640979028381]
	TIME [epoch: 16 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030197083595056073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030197083595056073 | validation: 0.07694645259700379]
	TIME [epoch: 16.5 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1390332763686975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1390332763686975 | validation: 0.1275130017613181]
	TIME [epoch: 16 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09854781722674286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09854781722674286 | validation: 0.1920612373472537]
	TIME [epoch: 16 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16193542304056605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16193542304056605 | validation: 0.159965537958989]
	TIME [epoch: 16 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14237152420909877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14237152420909877 | validation: 0.1192844345968403]
	TIME [epoch: 16 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15545395096344566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15545395096344566 | validation: 0.07680530815407705]
	TIME [epoch: 16 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06891650887369233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06891650887369233 | validation: 0.08805392709744611]
	TIME [epoch: 16 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06697169898578985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06697169898578985 | validation: 0.04766863864048607]
	TIME [epoch: 16 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05088862624400481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05088862624400481 | validation: 0.04463363654526738]
	TIME [epoch: 16 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037075936090501804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037075936090501804 | validation: 0.03406563498123246]
	TIME [epoch: 16 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0419077534424284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0419077534424284 | validation: 0.03774446954787561]
	TIME [epoch: 16 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03961308688415693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03961308688415693 | validation: 0.07459388976934121]
	TIME [epoch: 16 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06229304258109835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06229304258109835 | validation: 0.05413005146805852]
	TIME [epoch: 16 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05240883298921893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05240883298921893 | validation: 0.07301978278252308]
	TIME [epoch: 16 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07047025685201398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07047025685201398 | validation: 0.05488814416821685]
	TIME [epoch: 16 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05831462175517166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05831462175517166 | validation: 0.07104839581923246]
	TIME [epoch: 16 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062279342068255805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062279342068255805 | validation: 0.058395362318066815]
	TIME [epoch: 16 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05362133118285261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05362133118285261 | validation: 0.044962531263363026]
	TIME [epoch: 16 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05352361757628616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05352361757628616 | validation: 0.09717105662666725]
	TIME [epoch: 16 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09163088484493798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09163088484493798 | validation: 0.08333666412764253]
	TIME [epoch: 16 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08873040555379628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08873040555379628 | validation: 0.06133373497879513]
	TIME [epoch: 16 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05231513133944185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05231513133944185 | validation: 0.028034008984700976]
	TIME [epoch: 16 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04375844412538612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04375844412538612 | validation: 0.046797809136955706]
	TIME [epoch: 16 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047610280830626246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047610280830626246 | validation: 0.029251814442910154]
	TIME [epoch: 16 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025959257509176947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025959257509176947 | validation: 0.03684140482025402]
	TIME [epoch: 16 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04850721306646984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04850721306646984 | validation: 0.06786122795547876]
	TIME [epoch: 16 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05209974276507314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05209974276507314 | validation: 0.026938559865640255]
	TIME [epoch: 16 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02524094493418327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02524094493418327 | validation: 0.04821805125254729]
	TIME [epoch: 16 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05516120619390591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05516120619390591 | validation: 0.08166643357962534]
	TIME [epoch: 16 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07906032565681048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07906032565681048 | validation: 0.08933555000156801]
	TIME [epoch: 16 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0751376176163963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0751376176163963 | validation: 0.08951112144272684]
	TIME [epoch: 16 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09170203116363744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09170203116363744 | validation: 0.054619534922255145]
	TIME [epoch: 16 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04962566664866008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04962566664866008 | validation: 0.03740286845649337]
	TIME [epoch: 16 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038642924791865756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038642924791865756 | validation: 0.056925766098551406]
	TIME [epoch: 16 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07117918612456847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07117918612456847 | validation: 0.03352976355394618]
	TIME [epoch: 16 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035220563514104616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035220563514104616 | validation: 0.029003803165447706]
	TIME [epoch: 16 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033819536182537656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033819536182537656 | validation: 0.025867239331748573]
	TIME [epoch: 16 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04342315177225744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04342315177225744 | validation: 0.11612065696086789]
	TIME [epoch: 16 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11674504763405756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11674504763405756 | validation: 0.18053624157376857]
	TIME [epoch: 16 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17072453006045776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17072453006045776 | validation: 0.08083643797498546]
	TIME [epoch: 16 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06394201234289659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06394201234289659 | validation: 0.028266661981286845]
	TIME [epoch: 16 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03264110403943242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03264110403943242 | validation: 0.09126128518182727]
	TIME [epoch: 16 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08568771823495816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08568771823495816 | validation: 0.05789433608872564]
	TIME [epoch: 16 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047582137099735425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047582137099735425 | validation: 0.5666608012202969]
	TIME [epoch: 16 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6158587663761995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6158587663761995 | validation: 0.36650414575683965]
	TIME [epoch: 16 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45465545786534833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45465545786534833 | validation: 0.11029978424164183]
	TIME [epoch: 16 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20107195119361312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20107195119361312 | validation: 0.10395581845441049]
	TIME [epoch: 16 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11032650983549527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11032650983549527 | validation: 0.0871664885254866]
	TIME [epoch: 16 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08478741240193806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08478741240193806 | validation: 0.046856701593792904]
	TIME [epoch: 16 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04700058644947859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04700058644947859 | validation: 0.03885029351791924]
	TIME [epoch: 16 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036656328723835334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036656328723835334 | validation: 0.045100922984819825]
	TIME [epoch: 16 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040527958131035706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040527958131035706 | validation: 0.034029514086119386]
	TIME [epoch: 16 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031035325097393915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031035325097393915 | validation: 0.04069298160461297]
	TIME [epoch: 16 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043011990074538556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043011990074538556 | validation: 0.07174030200788577]
	TIME [epoch: 16 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06864750102168296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06864750102168296 | validation: 0.11233440353690892]
	TIME [epoch: 16 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10562519040441197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10562519040441197 | validation: 0.07655165132564679]
	TIME [epoch: 16 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07560914562054313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07560914562054313 | validation: 0.040728677003647185]
	TIME [epoch: 16 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04547956148838344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04547956148838344 | validation: 0.025059870453463296]
	TIME [epoch: 16 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024574804176151727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024574804176151727 | validation: 0.024728180561676263]
	TIME [epoch: 16 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023362726129098857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023362726129098857 | validation: 0.03752603553676487]
	TIME [epoch: 16 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026516834996760403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026516834996760403 | validation: 0.03508007506575672]
	TIME [epoch: 16 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028795708415559666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028795708415559666 | validation: 0.047474713834364295]
	TIME [epoch: 16 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038576425651782535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038576425651782535 | validation: 0.04961737665693503]
	TIME [epoch: 16 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05427152993868521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05427152993868521 | validation: 0.07240260901233025]
	TIME [epoch: 16 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06457248221960044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06457248221960044 | validation: 0.08015708844945822]
	TIME [epoch: 16 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07100758489508009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07100758489508009 | validation: 0.050368863796783174]
	TIME [epoch: 16 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03757008934632234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03757008934632234 | validation: 0.034145987781364316]
	TIME [epoch: 16 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03304436497956072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03304436497956072 | validation: 0.033815082666048914]
	TIME [epoch: 16 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03141583026888022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03141583026888022 | validation: 0.058105612075577275]
	TIME [epoch: 16 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05681844783186905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05681844783186905 | validation: 0.10472316277833965]
	TIME [epoch: 16 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12643060500535697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12643060500535697 | validation: 0.21951013352927407]
	TIME [epoch: 16 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15808438806171837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15808438806171837 | validation: 0.18178885115809773]
	TIME [epoch: 16 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12088909261001318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12088909261001318 | validation: 0.04655101595211634]
	TIME [epoch: 16 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04696373833080478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04696373833080478 | validation: 0.06058851262536798]
	TIME [epoch: 16 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05831227581831663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05831227581831663 | validation: 0.031181495091219282]
	TIME [epoch: 16 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039121497614450246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039121497614450246 | validation: 0.03294357024535127]
	TIME [epoch: 16 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037598216883288495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037598216883288495 | validation: 0.036634120403293315]
	TIME [epoch: 16 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03782402980339528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03782402980339528 | validation: 0.03828690596110473]
	TIME [epoch: 16 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03202867644549912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03202867644549912 | validation: 0.029919146682950273]
	TIME [epoch: 16 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03298220522205602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03298220522205602 | validation: 0.036707297430119815]
	TIME [epoch: 16 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03242606104549325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03242606104549325 | validation: 0.023061012280311523]
	TIME [epoch: 16 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02186760510902852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02186760510902852 | validation: 0.040901952855866446]
	TIME [epoch: 16 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03670568851833623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03670568851833623 | validation: 0.08628604561378822]
	TIME [epoch: 16 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08636554083500496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08636554083500496 | validation: 0.1507485946775094]
	TIME [epoch: 16 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14652127344848712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14652127344848712 | validation: 0.10837074534426981]
	TIME [epoch: 16 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12265349013521917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12265349013521917 | validation: 0.08024651974547654]
	TIME [epoch: 16 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07879809660234428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07879809660234428 | validation: 0.08198759706248226]
	TIME [epoch: 16 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06941204303832248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06941204303832248 | validation: 0.054687035833394654]
	TIME [epoch: 16 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058455103190634614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058455103190634614 | validation: 0.03783257396305574]
	TIME [epoch: 16 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041907450115134325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041907450115134325 | validation: 0.04664477121922577]
	TIME [epoch: 16 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03761410600956743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03761410600956743 | validation: 0.03344245507790862]
	TIME [epoch: 16 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03438004561635622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03438004561635622 | validation: 0.027475110715509444]
	TIME [epoch: 16 sec]
	Saving model to: out/model_training/model_phi1_3b_v_mmd1_20240822_142553/states/model_phi1_3b_v_mmd1_1288.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 9955.156 seconds.
