Args:
Namespace(name='model_phi1_4b_v_mmd1', outdir='out/model_training/model_phi1_4b_v_mmd1', training_data='data/training_data/data_phi1_4b/training', validation_data='data/training_data/data_phi1_4b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2579470394

Training model...

Saving initial model state to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.706628313402261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.706628313402261 | validation: 6.2452932873532845]
	TIME [epoch: 48.5 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.525432001290628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.525432001290628 | validation: 5.8137932303836415]
	TIME [epoch: 1.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.295842978905823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.295842978905823 | validation: 4.870968880301781]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.566997442212276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.566997442212276 | validation: 6.137174169195608]
	TIME [epoch: 1.73 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.45380477530729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.45380477530729 | validation: 5.139482104993078]
	TIME [epoch: 1.73 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.659087336112724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.659087336112724 | validation: 4.441362375355335]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.5568916041269105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5568916041269105 | validation: 5.23034696176819]
	TIME [epoch: 1.75 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.010907596182413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.010907596182413 | validation: 4.671373859707153]
	TIME [epoch: 1.74 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.414351816464639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.414351816464639 | validation: 4.321129813492917]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.603759519287429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.603759519287429 | validation: 4.277834832193007]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.340863380089058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.340863380089058 | validation: 4.736686152398874]
	TIME [epoch: 1.72 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.433890176343773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.433890176343773 | validation: 4.289249568599502]
	TIME [epoch: 1.73 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.261012255879025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.261012255879025 | validation: 4.26328622462373]
	TIME [epoch: 1.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.226417284785139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.226417284785139 | validation: 4.448537770498717]
	TIME [epoch: 1.73 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.219828307687944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.219828307687944 | validation: 4.241821868238365]
	TIME [epoch: 1.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.188735946681811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.188735946681811 | validation: 4.301010265891375]
	TIME [epoch: 1.72 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.1589385051858745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1589385051858745 | validation: 4.298775800852385]
	TIME [epoch: 1.72 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.14643239605358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.14643239605358 | validation: 4.230034867210622]
	TIME [epoch: 1.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.134217788705137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.134217788705137 | validation: 4.29317078257723]
	TIME [epoch: 1.73 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.123601636519894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.123601636519894 | validation: 4.166959181154775]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.124690969210798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.124690969210798 | validation: 4.368289816019936]
	TIME [epoch: 1.73 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.145580091224295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.145580091224295 | validation: 4.155467860136883]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.225671420015598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.225671420015598 | validation: 4.346973004066135]
	TIME [epoch: 1.73 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.1307857433029325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1307857433029325 | validation: 4.12815535157483]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.093326263055293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.093326263055293 | validation: 4.2224543721714936]
	TIME [epoch: 1.74 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.060905422620903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.060905422620903 | validation: 4.135098258544224]
	TIME [epoch: 1.73 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.044708293511641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.044708293511641 | validation: 4.174761601642594]
	TIME [epoch: 1.73 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.03400883753071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.03400883753071 | validation: 4.101962423467189]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0247298094075905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0247298094075905 | validation: 4.178794812082561]
	TIME [epoch: 1.73 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.022309984918071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.022309984918071 | validation: 4.068917641024418]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.047056285552834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.047056285552834 | validation: 4.290128280745482]
	TIME [epoch: 1.73 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.088080896941188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.088080896941188 | validation: 4.088574909733238]
	TIME [epoch: 1.73 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.108967726694401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.108967726694401 | validation: 4.114600241813766]
	TIME [epoch: 1.73 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9713375082030633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9713375082030633 | validation: 4.078414646819829]
	TIME [epoch: 1.73 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.952915887176522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.952915887176522 | validation: 4.019178771363307]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9526232909508283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9526232909508283 | validation: 4.089957600953461]
	TIME [epoch: 1.73 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.941356688426671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.941356688426671 | validation: 3.9864033563013956]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.926807954130827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.926807954130827 | validation: 4.031685396042779]
	TIME [epoch: 1.73 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.895563528981944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.895563528981944 | validation: 3.9345027926820553]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8518507745984394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8518507745984394 | validation: 3.9297148927839376]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.794891797901961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.794891797901961 | validation: 3.845995835598678]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.758260314637559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.758260314637559 | validation: 3.8537305825063246]
	TIME [epoch: 1.74 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7286450523279338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7286450523279338 | validation: 3.784662098639174]
	TIME [epoch: 1.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.730465994758869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.730465994758869 | validation: 3.8960667419837076]
	TIME [epoch: 1.74 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7680250685883867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7680250685883867 | validation: 3.81914393925235]
	TIME [epoch: 1.73 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.818205854966511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.818205854966511 | validation: 3.7811952670774334]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6627308386959228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6627308386959228 | validation: 3.7299854068725007]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6401606604584735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6401606604584735 | validation: 3.7058419940736607]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6354314813616013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6354314813616013 | validation: 3.7391667430173388]
	TIME [epoch: 1.73 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.622499860331501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.622499860331501 | validation: 3.6774660551709446]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6083858531670785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6083858531670785 | validation: 3.6961641251403687]
	TIME [epoch: 1.73 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5954382071295705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5954382071295705 | validation: 3.6318999585560854]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.584613928214526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.584613928214526 | validation: 3.6764568309081795]
	TIME [epoch: 1.73 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.572465045156199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.572465045156199 | validation: 3.6263970530343728]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.568626478985816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.568626478985816 | validation: 3.6397668187861303]
	TIME [epoch: 1.72 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5470877970902497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5470877970902497 | validation: 3.614049400766873]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.544617801681021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.544617801681021 | validation: 3.585749158999766]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5110290367106827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5110290367106827 | validation: 3.5558475065183544]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4864343403659253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4864343403659253 | validation: 3.5513376215762054]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.463831884255037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.463831884255037 | validation: 3.5453051587000064]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4531784898403637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4531784898403637 | validation: 3.5288195290295477]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.445301246000783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.445301246000783 | validation: 3.504524688518343]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4432847309403805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4432847309403805 | validation: 3.539840678233081]
	TIME [epoch: 1.73 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.458236968446687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.458236968446687 | validation: 3.533794778011496]
	TIME [epoch: 1.73 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4718201026422735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4718201026422735 | validation: 3.497919316366426]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.447580161618942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.447580161618942 | validation: 3.497592060155382]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4087635001039804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4087635001039804 | validation: 3.443230278060101]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.395072657205026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.395072657205026 | validation: 3.443680609793484]
	TIME [epoch: 1.73 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3683499801921917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3683499801921917 | validation: 3.412154163602107]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3542706740826707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3542706740826707 | validation: 3.3987759461850056]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3424787275675487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3424787275675487 | validation: 3.4077017308946767]
	TIME [epoch: 1.75 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.334185517976056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.334185517976056 | validation: 3.383551639736865]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.332871582249737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.332871582249737 | validation: 3.417263742209832]
	TIME [epoch: 1.72 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3470132215854553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3470132215854553 | validation: 3.432489682881159]
	TIME [epoch: 1.72 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4030214439660744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4030214439660744 | validation: 3.365361808901517]
	TIME [epoch: 1.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.328841314742013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.328841314742013 | validation: 3.337246771685822]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.285845604108766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.285845604108766 | validation: 3.3539839309776056]
	TIME [epoch: 1.74 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.288635313156748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.288635313156748 | validation: 3.3211328811678076]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.28078482426579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.28078482426579 | validation: 3.300096345559254]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.264855315602257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.264855315602257 | validation: 3.3002059952010967]
	TIME [epoch: 1.73 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.24589859771341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.24589859771341 | validation: 3.282939512947273]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2400993917462277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2400993917462277 | validation: 3.2710591123167476]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2222757657687033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2222757657687033 | validation: 3.266741315233015]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2160643927510417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2160643927510417 | validation: 3.258234600575946]
	TIME [epoch: 1.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2076313383077366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2076313383077366 | validation: 3.253185659173468]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2116502710024166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2116502710024166 | validation: 3.3023017187387596]
	TIME [epoch: 1.74 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2816341334856034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2816341334856034 | validation: 3.2334023899060913]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2223394699132304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2223394699132304 | validation: 3.1678808370182687]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.157675876603087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.157675876603087 | validation: 3.1121434336395586]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.099140024314428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.099140024314428 | validation: 2.9542961340676137]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9185253852486164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9185253852486164 | validation: 2.568478394899957]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.649237166868589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.649237166868589 | validation: 2.167572013218322]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1191323749660858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1191323749660858 | validation: 1.778414633334856]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7858192493301204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7858192493301204 | validation: 2.2928322798360425]
	TIME [epoch: 1.74 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1521041361493904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1521041361493904 | validation: 1.7222759488293353]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8297033122344517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8297033122344517 | validation: 1.4060657793471691]
	TIME [epoch: 1.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3923131289247994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3923131289247994 | validation: 1.3400918939615574]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3038960102733177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3038960102733177 | validation: 1.0955457977356329]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0406993892126177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0406993892126177 | validation: 1.106644596905477]
	TIME [epoch: 1.73 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0793181412459336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0793181412459336 | validation: 1.1351663405991819]
	TIME [epoch: 1.73 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1406755869493688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1406755869493688 | validation: 1.0599283967361781]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9949086403742254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9949086403742254 | validation: 0.9688440127184258]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9465464338747097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9465464338747097 | validation: 0.9468937686373846]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9408980895368229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9408980895368229 | validation: 1.016235275878363]
	TIME [epoch: 1.73 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9646409652776922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9646409652776922 | validation: 0.9548203754124945]
	TIME [epoch: 1.73 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9662410685039643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9662410685039643 | validation: 0.9703576534544894]
	TIME [epoch: 1.74 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8991761811437354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8991761811437354 | validation: 0.9026635658649268]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8802474903512904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8802474903512904 | validation: 0.9618896721465071]
	TIME [epoch: 1.75 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8964784939487629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8964784939487629 | validation: 0.9522794493903256]
	TIME [epoch: 1.73 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9578482762826396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9578482762826396 | validation: 0.9962800801861094]
	TIME [epoch: 1.73 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.894772583623363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.894772583623363 | validation: 0.8892021398798043]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8683681624209254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8683681624209254 | validation: 0.9510282887616701]
	TIME [epoch: 1.73 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8406783917462136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8406783917462136 | validation: 0.8806528345148528]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8326733330609895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8326733330609895 | validation: 0.9031012125787593]
	TIME [epoch: 1.73 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8229177553443784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8229177553443784 | validation: 0.8600903108791427]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_115.pth
	Model improved!!!
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8129196250483013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8129196250483013 | validation: 0.8672638170636056]
	TIME [epoch: 1.73 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8068356724313283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8068356724313283 | validation: 0.8523507535228343]
	TIME [epoch: 1.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8158512227531699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8158512227531699 | validation: 0.9747105170477586]
	TIME [epoch: 1.73 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8443224116965479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8443224116965479 | validation: 1.0207485634893254]
	TIME [epoch: 1.73 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0292419459330864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0292419459330864 | validation: 1.1958919611182552]
	TIME [epoch: 1.73 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9768300451093092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9768300451093092 | validation: 0.9345133328510906]
	TIME [epoch: 1.73 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8851648303487999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8851648303487999 | validation: 0.8921614269265845]
	TIME [epoch: 1.73 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7986951545030826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7986951545030826 | validation: 0.9184822510636788]
	TIME [epoch: 1.73 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.812081148750524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.812081148750524 | validation: 0.8553165212967118]
	TIME [epoch: 1.73 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8190757678049551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8190757678049551 | validation: 0.8644609857525434]
	TIME [epoch: 1.73 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7963499721404393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7963499721404393 | validation: 0.898581899158611]
	TIME [epoch: 1.73 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.786896908435212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.786896908435212 | validation: 0.8343922780783981]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7923691096289894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7923691096289894 | validation: 0.9272482011601796]
	TIME [epoch: 1.73 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8036231914967132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8036231914967132 | validation: 0.8546249228744798]
	TIME [epoch: 1.73 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8007194531594098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8007194531594098 | validation: 0.9829191597586195]
	TIME [epoch: 1.72 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8072965602543716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8072965602543716 | validation: 0.8600003735483803]
	TIME [epoch: 1.72 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8004858988132995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8004858988132995 | validation: 1.0630501562541432]
	TIME [epoch: 1.72 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8531342817949183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8531342817949183 | validation: 1.0214775375913878]
	TIME [epoch: 1.73 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9796586870317481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9796586870317481 | validation: 0.9839720057210866]
	TIME [epoch: 1.72 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8015073215709453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8015073215709453 | validation: 0.8777024656536583]
	TIME [epoch: 1.72 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7692800911479739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7692800911479739 | validation: 0.8401037951211414]
	TIME [epoch: 1.72 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7777334488074346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7777334488074346 | validation: 0.8906329153722958]
	TIME [epoch: 1.72 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7724173977941315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7724173977941315 | validation: 0.849850539708275]
	TIME [epoch: 1.72 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7785216037221769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7785216037221769 | validation: 0.9536323241095055]
	TIME [epoch: 1.72 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7794849816954852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7794849816954852 | validation: 0.8725139733144008]
	TIME [epoch: 1.72 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8145750919609573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8145750919609573 | validation: 1.0352253988457414]
	TIME [epoch: 1.73 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8150019429513483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8150019429513483 | validation: 0.8777299805707474]
	TIME [epoch: 1.72 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.819316472153228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.819316472153228 | validation: 1.0748748913497737]
	TIME [epoch: 1.72 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8168994865611373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8168994865611373 | validation: 0.9036456022078824]
	TIME [epoch: 1.72 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8802079485864954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8802079485864954 | validation: 1.2572728688340735]
	TIME [epoch: 1.73 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9780113505486304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9780113505486304 | validation: 0.9413657806922471]
	TIME [epoch: 1.72 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7734714933229456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7734714933229456 | validation: 0.8890549970126991]
	TIME [epoch: 1.73 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8557908349441344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8557908349441344 | validation: 0.976013511670246]
	TIME [epoch: 1.74 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7964634212445161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7964634212445161 | validation: 0.8741475858033706]
	TIME [epoch: 1.73 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7655726651663036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7655726651663036 | validation: 0.8405119669501524]
	TIME [epoch: 1.73 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7746501965930555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7746501965930555 | validation: 0.8987983359028153]
	TIME [epoch: 1.73 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7610248346822366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7610248346822366 | validation: 0.8683355066986987]
	TIME [epoch: 1.72 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7611397398517642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7611397398517642 | validation: 1.0005706399000882]
	TIME [epoch: 1.72 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.800947043025477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.800947043025477 | validation: 0.9738031561504017]
	TIME [epoch: 1.72 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8877856472670371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8877856472670371 | validation: 1.1300627841549342]
	TIME [epoch: 1.72 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8237365412229987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8237365412229987 | validation: 0.8515940633512458]
	TIME [epoch: 1.73 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7597713851523116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7597713851523116 | validation: 0.913659387518727]
	TIME [epoch: 1.72 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7470725981570219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7470725981570219 | validation: 0.9097955263112523]
	TIME [epoch: 1.72 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7529046653888498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7529046653888498 | validation: 0.878482235341863]
	TIME [epoch: 1.72 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.792791977515089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.792791977515089 | validation: 1.2337124837428897]
	TIME [epoch: 1.72 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8612609593450472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8612609593450472 | validation: 0.919007987161296]
	TIME [epoch: 1.72 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8630680137361733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8630680137361733 | validation: 1.089544014216019]
	TIME [epoch: 1.72 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8092927008285251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8092927008285251 | validation: 0.8826156389480266]
	TIME [epoch: 1.72 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7518670852793136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7518670852793136 | validation: 0.8580113348635989]
	TIME [epoch: 1.72 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7886497190462504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7886497190462504 | validation: 1.0526816380022552]
	TIME [epoch: 1.72 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7916216895182427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7916216895182427 | validation: 0.8554711571461223]
	TIME [epoch: 1.72 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7617986918404711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7617986918404711 | validation: 0.8577799538333791]
	TIME [epoch: 1.72 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7547565557025181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7547565557025181 | validation: 0.9143389525222979]
	TIME [epoch: 1.73 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7611152980373598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7611152980373598 | validation: 0.8883800773663275]
	TIME [epoch: 1.74 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7884134398703131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7884134398703131 | validation: 1.0098916831874198]
	TIME [epoch: 1.73 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8512879899609175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8512879899609175 | validation: 0.9131756434845113]
	TIME [epoch: 1.73 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7862845232363409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7862845232363409 | validation: 0.8743871319529565]
	TIME [epoch: 1.73 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7638119287395131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7638119287395131 | validation: 1.0041619237250996]
	TIME [epoch: 1.72 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7688636377203778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7688636377203778 | validation: 0.8831370708800215]
	TIME [epoch: 1.72 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8282776623465832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8282776623465832 | validation: 1.2532940890731332]
	TIME [epoch: 1.72 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8745989956851394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8745989956851394 | validation: 0.8685756237732489]
	TIME [epoch: 1.72 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7922524667050047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7922524667050047 | validation: 0.9009795372973266]
	TIME [epoch: 1.72 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7653431514094701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7653431514094701 | validation: 0.8906855482749239]
	TIME [epoch: 1.73 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.768496422353924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.768496422353924 | validation: 0.8495495153859282]
	TIME [epoch: 1.72 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7589141303116024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7589141303116024 | validation: 0.9047418094411557]
	TIME [epoch: 1.73 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7541228469062387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7541228469062387 | validation: 0.8616231867156275]
	TIME [epoch: 1.72 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.744171813605443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.744171813605443 | validation: 0.8714260243297591]
	TIME [epoch: 1.72 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7460644240517047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7460644240517047 | validation: 0.8359892319924764]
	TIME [epoch: 1.72 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7581634080528957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7581634080528957 | validation: 0.9385308003388151]
	TIME [epoch: 1.72 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8026313220048715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8026313220048715 | validation: 0.9772222561433725]
	TIME [epoch: 1.72 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8608268706396982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8608268706396982 | validation: 0.969919844999803]
	TIME [epoch: 1.72 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.86554678807817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.86554678807817 | validation: 1.1112187663653301]
	TIME [epoch: 1.72 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8099108057877663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8099108057877663 | validation: 0.8337733112963633]
	TIME [epoch: 1.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7964201190462614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7964201190462614 | validation: 1.0618860843228461]
	TIME [epoch: 1.74 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8066529677127321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8066529677127321 | validation: 0.8595576923615108]
	TIME [epoch: 1.72 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7366276617780392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7366276617780392 | validation: 0.8292959553801851]
	TIME [epoch: 1.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_191.pth
	Model improved!!!
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7439507811258497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7439507811258497 | validation: 1.0207463646645996]
	TIME [epoch: 1.72 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7775013607424661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7775013607424661 | validation: 0.8350100469385269]
	TIME [epoch: 1.72 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7646320868589004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7646320868589004 | validation: 1.0056017209466934]
	TIME [epoch: 1.73 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7794976164543701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7794976164543701 | validation: 0.8988768970720843]
	TIME [epoch: 1.73 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8122627431833458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8122627431833458 | validation: 1.0368437459409112]
	TIME [epoch: 1.73 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.798758242901256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.798758242901256 | validation: 0.8436264381618774]
	TIME [epoch: 1.72 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.751079983785946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.751079983785946 | validation: 0.8527785041768093]
	TIME [epoch: 1.72 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7415657260282787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7415657260282787 | validation: 0.9237908864196931]
	TIME [epoch: 1.72 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7419070504432892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7419070504432892 | validation: 0.8161555317300835]
	TIME [epoch: 1.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_200.pth
	Model improved!!!
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7502609014646616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7502609014646616 | validation: 1.0768829569766776]
	TIME [epoch: 48.8 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8168657956212871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8168657956212871 | validation: 0.8354597607222547]
	TIME [epoch: 3.45 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7827582437497377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7827582437497377 | validation: 0.9938111580828987]
	TIME [epoch: 3.42 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7723332443180629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7723332443180629 | validation: 0.813520936820491]
	TIME [epoch: 3.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_204.pth
	Model improved!!!
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7342083100957413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7342083100957413 | validation: 0.9130245601684192]
	TIME [epoch: 3.41 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7665312285832415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7665312285832415 | validation: 1.0022608464569707]
	TIME [epoch: 3.43 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.951445059750497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.951445059750497 | validation: 1.1117866326882857]
	TIME [epoch: 3.43 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8249136333511824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8249136333511824 | validation: 0.914064451990779]
	TIME [epoch: 3.42 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7290735933905836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7290735933905836 | validation: 0.8360266252487247]
	TIME [epoch: 3.42 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7486779684389512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7486779684389512 | validation: 0.9477229554815376]
	TIME [epoch: 3.41 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7451986209189874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7451986209189874 | validation: 0.8258670799517529]
	TIME [epoch: 3.42 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7276742918703645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7276742918703645 | validation: 0.8344507836179447]
	TIME [epoch: 3.41 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.730576692109704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.730576692109704 | validation: 1.0134063465103798]
	TIME [epoch: 3.42 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.761737120881651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.761737120881651 | validation: 0.8675083047834662]
	TIME [epoch: 3.41 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8456730990765875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8456730990765875 | validation: 1.2641613609487423]
	TIME [epoch: 3.42 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8981771721962288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8981771721962288 | validation: 0.8456582348262315]
	TIME [epoch: 3.41 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7199779556524061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7199779556524061 | validation: 0.8344202617810699]
	TIME [epoch: 3.42 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7683877224834105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7683877224834105 | validation: 0.9492994830307234]
	TIME [epoch: 3.43 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.807558527195345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.807558527195345 | validation: 0.9676260719745278]
	TIME [epoch: 3.43 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7946037778972146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7946037778972146 | validation: 0.8839317209873503]
	TIME [epoch: 3.42 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8309307698789276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8309307698789276 | validation: 0.9410929002614599]
	TIME [epoch: 3.42 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7551174414207867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7551174414207867 | validation: 0.8590367272399817]
	TIME [epoch: 3.42 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7274411443241808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7274411443241808 | validation: 0.8088795796409827]
	TIME [epoch: 3.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.744388380266404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.744388380266404 | validation: 0.8557913440743928]
	TIME [epoch: 3.42 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7259378149811757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7259378149811757 | validation: 0.8452721050513468]
	TIME [epoch: 3.45 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7181157539151011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7181157539151011 | validation: 0.7914663890621015]
	TIME [epoch: 3.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7331868020709738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7331868020709738 | validation: 0.9567370812480317]
	TIME [epoch: 3.41 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7573926823932755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7573926823932755 | validation: 0.7979229205246948]
	TIME [epoch: 3.42 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7552283421287108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7552283421287108 | validation: 1.0550206609500261]
	TIME [epoch: 3.43 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.802856479063598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.802856479063598 | validation: 0.7991394447063803]
	TIME [epoch: 3.43 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7282899212750067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7282899212750067 | validation: 0.8163575292272164]
	TIME [epoch: 3.42 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7138406456894508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7138406456894508 | validation: 0.794288110351256]
	TIME [epoch: 3.42 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7079798509861075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7079798509861075 | validation: 0.7827088808053357]
	TIME [epoch: 3.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_233.pth
	Model improved!!!
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.745086690316544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.745086690316544 | validation: 1.1932270427999574]
	TIME [epoch: 3.41 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1474734396651345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1474734396651345 | validation: 1.1723829024014234]
	TIME [epoch: 3.41 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8922357634731091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8922357634731091 | validation: 0.9779411231244008]
	TIME [epoch: 3.42 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7396101382032535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7396101382032535 | validation: 0.9023048563352968]
	TIME [epoch: 3.41 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.759589518185002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.759589518185002 | validation: 0.8686183667833526]
	TIME [epoch: 3.41 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7463228055431336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7463228055431336 | validation: 0.8944492646247743]
	TIME [epoch: 3.41 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7262572349309946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7262572349309946 | validation: 0.8288553306122761]
	TIME [epoch: 3.43 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7199844050846943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7199844050846943 | validation: 0.824684088263756]
	TIME [epoch: 3.42 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7117863587368547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7117863587368547 | validation: 0.8239035056496434]
	TIME [epoch: 3.42 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7051877957278206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7051877957278206 | validation: 0.7947776109246004]
	TIME [epoch: 3.43 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7022861351097122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7022861351097122 | validation: 0.814217666837318]
	TIME [epoch: 3.42 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6956164099201334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6956164099201334 | validation: 0.7628795784448082]
	TIME [epoch: 3.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_245.pth
	Model improved!!!
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7037201363102132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7037201363102132 | validation: 1.148875057584192]
	TIME [epoch: 3.43 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8372103330213315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8372103330213315 | validation: 0.916193623064099]
	TIME [epoch: 3.43 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9588743191481277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9588743191481277 | validation: 1.1153787779051176]
	TIME [epoch: 3.43 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7971975727347009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7971975727347009 | validation: 0.975600276519211]
	TIME [epoch: 3.43 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7474669425247177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7474669425247177 | validation: 0.8699314362044253]
	TIME [epoch: 3.43 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7841199228317981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7841199228317981 | validation: 0.7969752971382192]
	TIME [epoch: 3.44 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7231194230674334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7231194230674334 | validation: 0.8571632543990195]
	TIME [epoch: 3.43 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7277828884663609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7277828884663609 | validation: 0.7782928520525145]
	TIME [epoch: 3.43 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7253363167419593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7253363167419593 | validation: 0.8533910922479075]
	TIME [epoch: 3.42 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7388646965335444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7388646965335444 | validation: 0.8538190524249153]
	TIME [epoch: 3.43 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7677392443037221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7677392443037221 | validation: 0.853983743423412]
	TIME [epoch: 3.42 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7605469196437477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7605469196437477 | validation: 0.8591785327420176]
	TIME [epoch: 3.42 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7359891528254702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7359891528254702 | validation: 0.8126997852800653]
	TIME [epoch: 3.43 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7056518705908645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7056518705908645 | validation: 0.775264574132375]
	TIME [epoch: 3.43 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7011899737202651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7011899737202651 | validation: 0.853741423017697]
	TIME [epoch: 3.43 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7173719677752564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7173719677752564 | validation: 0.7917276161895999]
	TIME [epoch: 3.43 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7671308782583772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7671308782583772 | validation: 0.9420234174049802]
	TIME [epoch: 3.43 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8139407470151148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8139407470151148 | validation: 0.876162679820411]
	TIME [epoch: 3.45 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7531960222365492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7531960222365492 | validation: 0.7628766289494346]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_264.pth
	Model improved!!!
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7033654341378551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7033654341378551 | validation: 0.8474094556261411]
	TIME [epoch: 3.43 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7022829765626557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7022829765626557 | validation: 0.7315274239160666]
	TIME [epoch: 3.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_266.pth
	Model improved!!!
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6823085664500832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6823085664500832 | validation: 0.8006620883151719]
	TIME [epoch: 3.43 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6880689614812795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6880689614812795 | validation: 0.873021684970609]
	TIME [epoch: 3.42 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7360111909225324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7360111909225324 | validation: 0.8209382653869082]
	TIME [epoch: 3.42 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7513442090015343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7513442090015343 | validation: 0.9711029461245554]
	TIME [epoch: 3.42 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7382395505324479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7382395505324479 | validation: 0.9148813847455817]
	TIME [epoch: 3.42 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7203526886624755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7203526886624755 | validation: 0.7439983308887375]
	TIME [epoch: 3.42 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7906253050863009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7906253050863009 | validation: 0.9991121535287637]
	TIME [epoch: 3.42 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7939396276346452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7939396276346452 | validation: 0.7465188365701186]
	TIME [epoch: 3.44 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6643273940689882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6643273940689882 | validation: 0.7514271475835836]
	TIME [epoch: 3.43 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7021331915632143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7021331915632143 | validation: 1.0138940264993013]
	TIME [epoch: 3.43 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7302734505420273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7302734505420273 | validation: 0.7664162924622686]
	TIME [epoch: 3.42 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6597015367058364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6597015367058364 | validation: 0.679463444809779]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_278.pth
	Model improved!!!
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6604967579956876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6604967579956876 | validation: 0.8453874840438929]
	TIME [epoch: 3.43 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.686491201758944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.686491201758944 | validation: 0.6690735013866927]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_280.pth
	Model improved!!!
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.711540727773986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.711540727773986 | validation: 0.9135888936627803]
	TIME [epoch: 3.43 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.775698019750038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.775698019750038 | validation: 0.945545542757566]
	TIME [epoch: 3.43 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7757618304413402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7757618304413402 | validation: 0.7824347608983662]
	TIME [epoch: 3.43 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.656425603300537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.656425603300537 | validation: 0.7340622697710699]
	TIME [epoch: 3.43 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.635317659615682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.635317659615682 | validation: 0.6911646890004426]
	TIME [epoch: 3.44 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6326109983888151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6326109983888151 | validation: 0.7532667457441709]
	TIME [epoch: 3.44 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6879653623977221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6879653623977221 | validation: 1.0265948401013616]
	TIME [epoch: 3.44 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8011757628858257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8011757628858257 | validation: 0.9511241319264215]
	TIME [epoch: 3.43 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6711962836338993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6711962836338993 | validation: 0.6407160673741995]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_289.pth
	Model improved!!!
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6563797579081481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6563797579081481 | validation: 0.7542248048856363]
	TIME [epoch: 3.43 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5957403705245851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5957403705245851 | validation: 0.631407516098731]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5844476698445328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5844476698445328 | validation: 0.7056316091148743]
	TIME [epoch: 3.42 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5746207133184094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5746207133184094 | validation: 0.7692543730618775]
	TIME [epoch: 3.42 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6279223448830336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6279223448830336 | validation: 0.8521945869473555]
	TIME [epoch: 3.45 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8059109286957369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8059109286957369 | validation: 0.9411470742219255]
	TIME [epoch: 3.42 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.643775308466619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.643775308466619 | validation: 0.6261466442856082]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_296.pth
	Model improved!!!
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5413745542483763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5413745542483763 | validation: 0.6294927710870002]
	TIME [epoch: 3.43 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5168838474893137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5168838474893137 | validation: 0.7074667090496016]
	TIME [epoch: 3.42 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5112785304773605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5112785304773605 | validation: 0.5535108129388612]
	TIME [epoch: 3.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_299.pth
	Model improved!!!
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5606915681162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5606915681162 | validation: 0.8268306046760415]
	TIME [epoch: 3.42 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.596529565134021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.596529565134021 | validation: 0.6037666802555561]
	TIME [epoch: 3.42 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6279993463831403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6279993463831403 | validation: 0.6533938351496773]
	TIME [epoch: 3.42 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49842057445214616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49842057445214616 | validation: 0.8526250099962142]
	TIME [epoch: 3.42 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5347208825646947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5347208825646947 | validation: 0.5705996370413765]
	TIME [epoch: 3.43 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6561166115575082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6561166115575082 | validation: 0.7373326837424498]
	TIME [epoch: 3.42 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46276315490068015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46276315490068015 | validation: 0.7224148870428392]
	TIME [epoch: 3.44 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46097029955528385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46097029955528385 | validation: 0.5145884035983473]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_307.pth
	Model improved!!!
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5240433256167107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5240433256167107 | validation: 0.7125400642636832]
	TIME [epoch: 3.41 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4316684626406483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4316684626406483 | validation: 0.590027484659371]
	TIME [epoch: 3.41 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4020575184703516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4020575184703516 | validation: 0.6642731619666308]
	TIME [epoch: 3.41 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38702629457856297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38702629457856297 | validation: 0.5007222916458975]
	TIME [epoch: 3.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_311.pth
	Model improved!!!
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4334989249142494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4334989249142494 | validation: 0.8545846024758753]
	TIME [epoch: 3.44 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.492864814693866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.492864814693866 | validation: 0.5075724466181074]
	TIME [epoch: 3.43 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5097718215355022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5097718215355022 | validation: 0.7227126196013942]
	TIME [epoch: 3.42 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37685685176622735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37685685176622735 | validation: 0.5782235504735694]
	TIME [epoch: 3.43 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36782780478624305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36782780478624305 | validation: 0.6883696733895726]
	TIME [epoch: 3.42 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3920563464092109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3920563464092109 | validation: 0.6493760406145773]
	TIME [epoch: 3.43 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37807029278933996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37807029278933996 | validation: 0.5057882780498482]
	TIME [epoch: 3.45 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4806197785109743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4806197785109743 | validation: 1.0381720347619272]
	TIME [epoch: 3.43 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.652049245829065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.652049245829065 | validation: 0.5209294643365547]
	TIME [epoch: 3.43 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35793311163946717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35793311163946717 | validation: 0.5985634952726555]
	TIME [epoch: 3.42 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32175212741777187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32175212741777187 | validation: 0.6063522249165456]
	TIME [epoch: 3.43 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3361765377929429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3361765377929429 | validation: 0.7947694634810295]
	TIME [epoch: 3.42 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4191809906831651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4191809906831651 | validation: 0.4825746708678558]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_324.pth
	Model improved!!!
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5944545094610251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5944545094610251 | validation: 0.7493434831311658]
	TIME [epoch: 3.41 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35492496172839494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35492496172839494 | validation: 0.5485052873529321]
	TIME [epoch: 3.41 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29699708160893834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29699708160893834 | validation: 0.5387358092551562]
	TIME [epoch: 3.4 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3008026083039426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3008026083039426 | validation: 0.8038209180742243]
	TIME [epoch: 3.41 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.391727770811955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.391727770811955 | validation: 0.47836463667545703]
	TIME [epoch: 3.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_329.pth
	Model improved!!!
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45421902287552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45421902287552 | validation: 0.8432112498391018]
	TIME [epoch: 3.43 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5065113988297183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5065113988297183 | validation: 0.6019233088668436]
	TIME [epoch: 3.43 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2884263096380594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2884263096380594 | validation: 0.4905281693905539]
	TIME [epoch: 3.43 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3214851673916388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3214851673916388 | validation: 0.8602979638272297]
	TIME [epoch: 3.42 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4474544972424721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4474544972424721 | validation: 0.5059555796051312]
	TIME [epoch: 3.42 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2991246895572121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2991246895572121 | validation: 0.596179063972169]
	TIME [epoch: 3.43 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2751397057383297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2751397057383297 | validation: 0.5016271496232935]
	TIME [epoch: 3.42 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30321317216503935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30321317216503935 | validation: 0.8337008006748999]
	TIME [epoch: 3.42 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4771348501570942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4771348501570942 | validation: 0.5330072209544784]
	TIME [epoch: 3.41 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31506984331244936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31506984331244936 | validation: 0.7179015218451845]
	TIME [epoch: 3.42 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32959715542774404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32959715542774404 | validation: 0.473367195904161]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_340.pth
	Model improved!!!
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.332955522137147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.332955522137147 | validation: 0.8123452119556571]
	TIME [epoch: 3.44 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3978232238664585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3978232238664585 | validation: 0.5684570767645065]
	TIME [epoch: 3.41 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3543197248769285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3543197248769285 | validation: 0.638622096646291]
	TIME [epoch: 3.41 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28695642623142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28695642623142 | validation: 0.47941835275981104]
	TIME [epoch: 3.42 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4130252210427583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4130252210427583 | validation: 0.828269303714531]
	TIME [epoch: 3.43 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39791322754418723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39791322754418723 | validation: 0.5579877345994645]
	TIME [epoch: 3.43 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25632430510076476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25632430510076476 | validation: 0.4774953397079113]
	TIME [epoch: 3.45 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26538188859477535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26538188859477535 | validation: 0.6491309569611858]
	TIME [epoch: 3.42 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29729699547945343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29729699547945343 | validation: 0.4487799465899778]
	TIME [epoch: 3.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_349.pth
	Model improved!!!
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2830757229335636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2830757229335636 | validation: 0.6462142840489703]
	TIME [epoch: 3.42 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3011232034943651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3011232034943651 | validation: 0.41645992325115205]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_351.pth
	Model improved!!!
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3922450783407289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3922450783407289 | validation: 0.8859985658444733]
	TIME [epoch: 3.44 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5189882178869601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5189882178869601 | validation: 0.7579149025642762]
	TIME [epoch: 3.43 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34296963977334904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34296963977334904 | validation: 0.4227165590888749]
	TIME [epoch: 3.43 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36800053769628854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36800053769628854 | validation: 0.6223589281822738]
	TIME [epoch: 3.42 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3630530472551991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3630530472551991 | validation: 0.5903624699868306]
	TIME [epoch: 3.43 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2837372356340651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2837372356340651 | validation: 0.4213858096820536]
	TIME [epoch: 3.43 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2710058808318795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2710058808318795 | validation: 0.563414400044769]
	TIME [epoch: 3.43 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29281137017047043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29281137017047043 | validation: 0.47782019426825156]
	TIME [epoch: 3.42 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28452314001823686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28452314001823686 | validation: 0.5524559190706841]
	TIME [epoch: 3.43 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23673388291033726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23673388291033726 | validation: 0.4303736035204551]
	TIME [epoch: 3.42 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.240982402854564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.240982402854564 | validation: 0.6929253405750586]
	TIME [epoch: 3.44 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3480417381787316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3480417381787316 | validation: 0.4601686212181768]
	TIME [epoch: 3.43 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3506851596562284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3506851596562284 | validation: 0.7002323193748684]
	TIME [epoch: 3.43 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3692199982034423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3692199982034423 | validation: 0.44127737491228686]
	TIME [epoch: 3.42 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22927941858213505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22927941858213505 | validation: 0.47181967947134373]
	TIME [epoch: 3.42 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2058627338153621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2058627338153621 | validation: 0.5020702709908959]
	TIME [epoch: 3.42 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20865582214240008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20865582214240008 | validation: 0.47497263031490117]
	TIME [epoch: 3.42 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.227512736575639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.227512736575639 | validation: 0.5047139631806928]
	TIME [epoch: 3.42 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24860403250976176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24860403250976176 | validation: 0.5239733696865526]
	TIME [epoch: 3.42 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.221188825455879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.221188825455879 | validation: 0.4485044468718769]
	TIME [epoch: 3.42 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2419675302722835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2419675302722835 | validation: 0.8384635255047688]
	TIME [epoch: 3.42 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47382063908914507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47382063908914507 | validation: 0.38230921353371355]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_373.pth
	Model improved!!!
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26028581406094536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26028581406094536 | validation: 0.6200656444980865]
	TIME [epoch: 3.44 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2553070265326775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2553070265326775 | validation: 0.43308503967533185]
	TIME [epoch: 3.42 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2696221579091307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2696221579091307 | validation: 0.6749826983479299]
	TIME [epoch: 3.42 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3392848480264333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3392848480264333 | validation: 0.4084443901214499]
	TIME [epoch: 3.42 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27512403526840473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27512403526840473 | validation: 0.4694913985362512]
	TIME [epoch: 3.42 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2062155516853477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2062155516853477 | validation: 0.4266564886247148]
	TIME [epoch: 3.42 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20783847190515498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20783847190515498 | validation: 0.4965609589162684]
	TIME [epoch: 3.42 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23949604341639588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23949604341639588 | validation: 0.38843280461751395]
	TIME [epoch: 3.42 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3117188464688531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3117188464688531 | validation: 0.6884697238090388]
	TIME [epoch: 3.42 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2974950491972023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2974950491972023 | validation: 0.40504733831400613]
	TIME [epoch: 3.42 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2462496624269715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2462496624269715 | validation: 0.4715167086168595]
	TIME [epoch: 3.42 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23445377440134552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23445377440134552 | validation: 0.4848924396355136]
	TIME [epoch: 3.43 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22892431135604818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22892431135604818 | validation: 0.46520640505934935]
	TIME [epoch: 3.43 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2024738396670351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2024738396670351 | validation: 0.4038650054502213]
	TIME [epoch: 3.42 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.227814596350848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.227814596350848 | validation: 0.6507397892214278]
	TIME [epoch: 3.42 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32219521239369076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32219521239369076 | validation: 0.4543157271915259]
	TIME [epoch: 3.42 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2669761189384153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2669761189384153 | validation: 0.5089226615959679]
	TIME [epoch: 3.42 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19770455075385243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19770455075385243 | validation: 0.42023976019874126]
	TIME [epoch: 3.42 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2778003524659809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2778003524659809 | validation: 0.6569271142240771]
	TIME [epoch: 3.42 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26123234754997077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26123234754997077 | validation: 0.5510594448610878]
	TIME [epoch: 3.42 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2654526747051428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2654526747051428 | validation: 0.5196105284222233]
	TIME [epoch: 3.42 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22985862763224038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22985862763224038 | validation: 0.4334708892256554]
	TIME [epoch: 3.42 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39149134657962964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39149134657962964 | validation: 0.47622149365776356]
	TIME [epoch: 3.43 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21843506055517284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21843506055517284 | validation: 0.520563924521011]
	TIME [epoch: 3.43 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23633830980616766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23633830980616766 | validation: 0.4516969950048544]
	TIME [epoch: 3.42 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21148744433726052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21148744433726052 | validation: 0.36793814304202765]
	TIME [epoch: 3.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_399.pth
	Model improved!!!
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27704133976178863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27704133976178863 | validation: 0.6285022445796522]
	TIME [epoch: 3.41 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3179674564188579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3179674564188579 | validation: 0.45708260035349274]
	TIME [epoch: 3.42 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2216881616102697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2216881616102697 | validation: 0.386844297982323]
	TIME [epoch: 3.42 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.177923024034039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.177923024034039 | validation: 0.3942995092951077]
	TIME [epoch: 3.42 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.178840600142322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.178840600142322 | validation: 0.41473942874791736]
	TIME [epoch: 3.42 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17497330106817244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17497330106817244 | validation: 0.4296350435634517]
	TIME [epoch: 3.42 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1677475188786083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1677475188786083 | validation: 0.4484116057828021]
	TIME [epoch: 3.42 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16167080662244332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16167080662244332 | validation: 0.42149035426908876]
	TIME [epoch: 3.44 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2180742111216837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2180742111216837 | validation: 0.66142439931368]
	TIME [epoch: 3.44 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38489411213808844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38489411213808844 | validation: 0.4395837060329467]
	TIME [epoch: 3.42 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2700355036799406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2700355036799406 | validation: 0.5825399873143299]
	TIME [epoch: 3.42 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2223287619923361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2223287619923361 | validation: 0.36454740589881096]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_411.pth
	Model improved!!!
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24023584655641692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24023584655641692 | validation: 0.5505024359175257]
	TIME [epoch: 3.42 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2892100468424677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2892100468424677 | validation: 0.5095736211699562]
	TIME [epoch: 3.42 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25012499133091465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25012499133091465 | validation: 0.4030361599894494]
	TIME [epoch: 3.42 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19915387088766168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19915387088766168 | validation: 0.4369345986110962]
	TIME [epoch: 3.42 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16739529466341616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16739529466341616 | validation: 0.35575056677725725]
	TIME [epoch: 3.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_416.pth
	Model improved!!!
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16287209924510176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16287209924510176 | validation: 0.4104294479978764]
	TIME [epoch: 3.43 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1887936904390844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1887936904390844 | validation: 0.5385056536513437]
	TIME [epoch: 3.43 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27037293228918374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27037293228918374 | validation: 0.3958241338283326]
	TIME [epoch: 3.43 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27705149253071937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27705149253071937 | validation: 0.5196205362679361]
	TIME [epoch: 3.42 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23102754860136926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23102754860136926 | validation: 0.38785270832341384]
	TIME [epoch: 3.42 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1953884078319544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1953884078319544 | validation: 0.49810048665868634]
	TIME [epoch: 3.42 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20013846577993996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20013846577993996 | validation: 0.44906235907183234]
	TIME [epoch: 3.41 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2525638986697523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2525638986697523 | validation: 0.5199230168712483]
	TIME [epoch: 3.42 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2018173753771341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2018173753771341 | validation: 0.367059507210481]
	TIME [epoch: 3.41 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1808897114798854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1808897114798854 | validation: 0.42785323383584456]
	TIME [epoch: 3.42 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18352171403987377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18352171403987377 | validation: 0.41364986071867094]
	TIME [epoch: 3.42 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1636740022616175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1636740022616175 | validation: 0.4221652375834654]
	TIME [epoch: 3.42 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1524142062965138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1524142062965138 | validation: 0.34095924006960093]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_429.pth
	Model improved!!!
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21562953891537662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21562953891537662 | validation: 0.6485896472789868]
	TIME [epoch: 3.46 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3504570692049384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3504570692049384 | validation: 0.4863883054667266]
	TIME [epoch: 3.43 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23697772932346503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23697772932346503 | validation: 0.3787781390268475]
	TIME [epoch: 3.43 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14252768052954906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14252768052954906 | validation: 0.41736299118764963]
	TIME [epoch: 3.44 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18240696953138183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18240696953138183 | validation: 0.5441330584804039]
	TIME [epoch: 3.43 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28791067810566545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28791067810566545 | validation: 0.42961800395629457]
	TIME [epoch: 3.43 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22264580669064032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22264580669064032 | validation: 0.4079303254492286]
	TIME [epoch: 3.43 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19429919455247727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19429919455247727 | validation: 0.4809261767915784]
	TIME [epoch: 3.43 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29028967939227474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29028967939227474 | validation: 0.4982337012735369]
	TIME [epoch: 3.43 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19569246224469758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19569246224469758 | validation: 0.3475227320664164]
	TIME [epoch: 3.43 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17106944762922774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17106944762922774 | validation: 0.500046941599802]
	TIME [epoch: 3.44 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15461434892021203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15461434892021203 | validation: 0.4204728949406878]
	TIME [epoch: 3.45 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13412904626898478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13412904626898478 | validation: 0.3482254353361651]
	TIME [epoch: 3.45 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17695490187475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17695490187475 | validation: 0.5035930073136182]
	TIME [epoch: 3.43 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22875970054651773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22875970054651773 | validation: 0.44131020217049244]
	TIME [epoch: 3.43 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3752626191836215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3752626191836215 | validation: 0.3663424688208083]
	TIME [epoch: 3.43 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13914049975718026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13914049975718026 | validation: 0.41125399307388016]
	TIME [epoch: 3.43 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2140533402537975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2140533402537975 | validation: 0.4128081134030563]
	TIME [epoch: 3.43 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28352630226747194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28352630226747194 | validation: 0.5103400836636928]
	TIME [epoch: 3.44 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1854112382301677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1854112382301677 | validation: 0.35761787534713574]
	TIME [epoch: 3.43 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16253334574044026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16253334574044026 | validation: 0.40974133910861854]
	TIME [epoch: 3.43 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19081711241590782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19081711241590782 | validation: 0.4517262647672189]
	TIME [epoch: 3.43 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21851211374018412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21851211374018412 | validation: 0.42532775283451046]
	TIME [epoch: 3.45 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2152155334080328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2152155334080328 | validation: 0.40316782502522114]
	TIME [epoch: 3.44 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17376460114673234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17376460114673234 | validation: 0.36018175320114865]
	TIME [epoch: 3.43 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15813256268250261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15813256268250261 | validation: 0.5212515401206743]
	TIME [epoch: 3.43 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.158325199603117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.158325199603117 | validation: 0.3208818943312237]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_456.pth
	Model improved!!!
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12690538875958282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12690538875958282 | validation: 0.3779254469351827]
	TIME [epoch: 3.44 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13937169108489392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13937169108489392 | validation: 0.870221728547027]
	TIME [epoch: 3.43 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.862354606067845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.862354606067845 | validation: 0.4407131192607084]
	TIME [epoch: 3.44 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6565410219763076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6565410219763076 | validation: 0.4903740609586706]
	TIME [epoch: 3.44 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22046571455083347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22046571455083347 | validation: 0.5919244889828785]
	TIME [epoch: 3.44 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19716609614321443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19716609614321443 | validation: 0.44316074004054506]
	TIME [epoch: 3.44 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15776873840988148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15776873840988148 | validation: 0.3493255779865394]
	TIME [epoch: 3.45 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18952924087018092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18952924087018092 | validation: 0.27746752225985777]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_464.pth
	Model improved!!!
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29646864103504195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29646864103504195 | validation: 0.39260016329849284]
	TIME [epoch: 3.44 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1958235988718225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1958235988718225 | validation: 0.30584220923900496]
	TIME [epoch: 3.43 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13961107852135748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13961107852135748 | validation: 0.3293269878381933]
	TIME [epoch: 3.43 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16973242746775116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16973242746775116 | validation: 0.33895819171511365]
	TIME [epoch: 3.43 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18247788355819222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18247788355819222 | validation: 0.35199842156982647]
	TIME [epoch: 3.43 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15640196141795246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15640196141795246 | validation: 0.35968650299068744]
	TIME [epoch: 3.43 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1486348510688778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1486348510688778 | validation: 0.330124455896517]
	TIME [epoch: 3.43 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16244810118124595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16244810118124595 | validation: 0.33667824416754466]
	TIME [epoch: 3.43 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14695648930027058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14695648930027058 | validation: 0.3354681584782143]
	TIME [epoch: 3.43 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14316670302150328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14316670302150328 | validation: 0.36032658178323745]
	TIME [epoch: 3.45 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16373179188832057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16373179188832057 | validation: 0.37997149214802073]
	TIME [epoch: 3.44 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20520794668116124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20520794668116124 | validation: 0.3683977996107721]
	TIME [epoch: 3.43 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1819602029366925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1819602029366925 | validation: 0.33479803217854914]
	TIME [epoch: 3.43 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1653518169058412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1653518169058412 | validation: 0.3400323471812023]
	TIME [epoch: 3.44 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15272664772784106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15272664772784106 | validation: 0.33051718724686385]
	TIME [epoch: 3.43 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16641451313596464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16641451313596464 | validation: 0.4526610970291226]
	TIME [epoch: 3.43 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19601124057475383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19601124057475383 | validation: 0.29909303459589903]
	TIME [epoch: 3.43 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15675406465057903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15675406465057903 | validation: 0.3762338171490326]
	TIME [epoch: 3.43 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18624278096892774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18624278096892774 | validation: 0.3574368034727969]
	TIME [epoch: 3.43 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1626197622657179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1626197622657179 | validation: 0.37922703246731015]
	TIME [epoch: 3.43 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13532217667934718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13532217667934718 | validation: 0.329328767332806]
	TIME [epoch: 3.43 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1656812271325537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1656812271325537 | validation: 0.40860271754648836]
	TIME [epoch: 3.45 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18820558589621159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18820558589621159 | validation: 0.3360316463839861]
	TIME [epoch: 3.44 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15332988680642604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15332988680642604 | validation: 0.4246299164188333]
	TIME [epoch: 3.43 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22876330256202415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22876330256202415 | validation: 0.3333307085377095]
	TIME [epoch: 3.43 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18215560277649787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18215560277649787 | validation: 0.35997351196999205]
	TIME [epoch: 3.42 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1366793608895139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1366793608895139 | validation: 0.4597229421630804]
	TIME [epoch: 3.43 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15753371899093455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15753371899093455 | validation: 0.3052237262325229]
	TIME [epoch: 3.43 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17053499183664322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17053499183664322 | validation: 0.42370268415036955]
	TIME [epoch: 3.43 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18950990525570738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18950990525570738 | validation: 0.2768652266040938]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_494.pth
	Model improved!!!
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18457218863360794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18457218863360794 | validation: 0.31044183846048906]
	TIME [epoch: 3.42 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15260236062002017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15260236062002017 | validation: 0.44167558568612436]
	TIME [epoch: 3.43 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17474852271850794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17474852271850794 | validation: 0.32759334075021546]
	TIME [epoch: 3.44 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16752240226329973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16752240226329973 | validation: 0.34567181349593423]
	TIME [epoch: 3.43 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17779700605847212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17779700605847212 | validation: 0.369324100521873]
	TIME [epoch: 3.42 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1463529749484295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1463529749484295 | validation: 0.30854363151190206]
	TIME [epoch: 3.43 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10590411281351651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10590411281351651 | validation: 0.2759865245916083]
	TIME [epoch: 53.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_501.pth
	Model improved!!!
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11426271246514429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11426271246514429 | validation: 0.45936557234267683]
	TIME [epoch: 7.49 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14755856603995327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14755856603995327 | validation: 0.3623199840486664]
	TIME [epoch: 7.47 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16740541379018964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16740541379018964 | validation: 0.25961271972708916]
	TIME [epoch: 7.46 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_504.pth
	Model improved!!!
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21063496739353277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21063496739353277 | validation: 0.41675438178729735]
	TIME [epoch: 7.47 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20876280887336168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20876280887336168 | validation: 0.36436120894595697]
	TIME [epoch: 7.44 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14357834032721625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14357834032721625 | validation: 0.32243207531343077]
	TIME [epoch: 7.44 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16196459528763874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16196459528763874 | validation: 0.48185917180268656]
	TIME [epoch: 7.44 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17290003038125534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17290003038125534 | validation: 0.37176329329889496]
	TIME [epoch: 7.45 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.180934723071128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.180934723071128 | validation: 0.4015179568483159]
	TIME [epoch: 7.49 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1304752494405245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1304752494405245 | validation: 0.5000651377111716]
	TIME [epoch: 7.47 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1594459772116557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1594459772116557 | validation: 0.3612014119141784]
	TIME [epoch: 7.45 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12520029608575883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12520029608575883 | validation: 0.3113145762881752]
	TIME [epoch: 7.45 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1610309360828412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1610309360828412 | validation: 0.3948200384460139]
	TIME [epoch: 7.45 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19061946726022747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19061946726022747 | validation: 0.43804430069823286]
	TIME [epoch: 7.46 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22432883971915288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22432883971915288 | validation: 0.3464292577971416]
	TIME [epoch: 7.44 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19839021336106097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19839021336106097 | validation: 0.5939044022263035]
	TIME [epoch: 7.47 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3905847625550938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3905847625550938 | validation: 0.3354185611425578]
	TIME [epoch: 7.44 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26400697686060814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26400697686060814 | validation: 0.7219658988894793]
	TIME [epoch: 7.45 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30978946751392494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30978946751392494 | validation: 0.48350286974183315]
	TIME [epoch: 7.47 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15074693505257997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15074693505257997 | validation: 0.28763556155297565]
	TIME [epoch: 7.48 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1358309116993235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1358309116993235 | validation: 0.33576709140564853]
	TIME [epoch: 7.44 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20341679040154076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20341679040154076 | validation: 0.3040794175078052]
	TIME [epoch: 7.45 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13903855451388564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13903855451388564 | validation: 0.35888739641399986]
	TIME [epoch: 7.48 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12449092897052821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12449092897052821 | validation: 0.24741789894625305]
	TIME [epoch: 7.5 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_525.pth
	Model improved!!!
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11213686537762463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11213686537762463 | validation: 0.2762552527228979]
	TIME [epoch: 7.44 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11410908952932164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11410908952932164 | validation: 0.3903364107022969]
	TIME [epoch: 7.43 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11968780530764833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11968780530764833 | validation: 0.367933487048838]
	TIME [epoch: 7.43 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10891525912014625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10891525912014625 | validation: 0.26416976067630077]
	TIME [epoch: 7.43 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2139265154973852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2139265154973852 | validation: 0.5627248709141109]
	TIME [epoch: 7.45 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3227220335998291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3227220335998291 | validation: 0.40371391345028623]
	TIME [epoch: 7.43 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15348315733798976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15348315733798976 | validation: 0.25054761945358256]
	TIME [epoch: 7.43 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16499840586656164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16499840586656164 | validation: 0.35279303781413174]
	TIME [epoch: 7.43 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19972068797141673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19972068797141673 | validation: 0.2809243796576692]
	TIME [epoch: 7.44 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09385663797707633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09385663797707633 | validation: 0.23090276433217874]
	TIME [epoch: 7.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_535.pth
	Model improved!!!
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08444312307405787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08444312307405787 | validation: 0.26138691496426125]
	TIME [epoch: 7.4 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08921261406233504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08921261406233504 | validation: 0.24079551668529856]
	TIME [epoch: 7.4 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0924816791193025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0924816791193025 | validation: 0.38697297280458287]
	TIME [epoch: 7.4 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14003870636699953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14003870636699953 | validation: 0.26058092291815427]
	TIME [epoch: 7.41 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17017130320021562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17017130320021562 | validation: 0.36000456069284886]
	TIME [epoch: 7.42 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.243678751654143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.243678751654143 | validation: 0.5183805553488077]
	TIME [epoch: 7.4 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24822413792643183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24822413792643183 | validation: 0.29255338982045276]
	TIME [epoch: 7.43 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15121383702567096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15121383702567096 | validation: 0.3039273014335043]
	TIME [epoch: 7.43 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11105227244685092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11105227244685092 | validation: 0.2524849262964951]
	TIME [epoch: 7.44 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08714051907411291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08714051907411291 | validation: 0.24399349942450135]
	TIME [epoch: 7.45 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09222087441773351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09222087441773351 | validation: 0.27593959334745083]
	TIME [epoch: 7.43 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11800297403567162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11800297403567162 | validation: 0.30000825278312]
	TIME [epoch: 7.43 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17947614758362804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17947614758362804 | validation: 0.2673072974639862]
	TIME [epoch: 7.43 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1677725139289025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1677725139289025 | validation: 0.3185805972896243]
	TIME [epoch: 7.44 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1551925763833133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1551925763833133 | validation: 0.23612676281847325]
	TIME [epoch: 7.44 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15135552064194252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15135552064194252 | validation: 0.27939118551106373]
	TIME [epoch: 7.43 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1206696547425236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1206696547425236 | validation: 0.30246531121701603]
	TIME [epoch: 7.43 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11772050707795029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11772050707795029 | validation: 0.2899082946677305]
	TIME [epoch: 7.43 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09681884481762315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09681884481762315 | validation: 0.2536530505196111]
	TIME [epoch: 7.43 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09467908075038133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09467908075038133 | validation: 0.41249543840869674]
	TIME [epoch: 7.44 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13723187769557826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13723187769557826 | validation: 0.35183639646285764]
	TIME [epoch: 7.45 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22133452431547163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22133452431547163 | validation: 0.3387554099635335]
	TIME [epoch: 7.43 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18038421943687954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18038421943687954 | validation: 0.34704759188360784]
	TIME [epoch: 7.42 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13736460076412135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13736460076412135 | validation: 0.2384852755329636]
	TIME [epoch: 7.44 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1280659477046011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1280659477046011 | validation: 0.277269820390636]
	TIME [epoch: 7.45 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17288164004900275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17288164004900275 | validation: 0.2984177879979535]
	TIME [epoch: 7.43 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19882815943116847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19882815943116847 | validation: 0.38872687642951126]
	TIME [epoch: 7.43 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17869510912776143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17869510912776143 | validation: 0.2387275340555362]
	TIME [epoch: 7.43 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09113281290986844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09113281290986844 | validation: 0.23077565007470852]
	TIME [epoch: 7.44 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_564.pth
	Model improved!!!
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10505294201827221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10505294201827221 | validation: 0.2909526684819249]
	TIME [epoch: 7.45 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12490536802177755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12490536802177755 | validation: 0.2488149264787364]
	TIME [epoch: 7.42 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12356267766260323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12356267766260323 | validation: 0.3199664886291338]
	TIME [epoch: 7.43 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13204306203433044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13204306203433044 | validation: 0.3040394760385423]
	TIME [epoch: 7.4 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11944133865516023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11944133865516023 | validation: 0.23221073998258232]
	TIME [epoch: 7.44 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09340640076219663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09340640076219663 | validation: 0.2906263510738793]
	TIME [epoch: 7.45 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1113034467025033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1113034467025033 | validation: 0.27049172070840877]
	TIME [epoch: 7.43 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1389457352973491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1389457352973491 | validation: 0.28313611155498797]
	TIME [epoch: 7.43 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11605907907869156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11605907907869156 | validation: 0.20411343615154476]
	TIME [epoch: 7.44 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_573.pth
	Model improved!!!
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09304762426818199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09304762426818199 | validation: 0.2987251650863776]
	TIME [epoch: 7.45 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17915143118061244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17915143118061244 | validation: 0.2930982140667056]
	TIME [epoch: 7.44 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2180517626105322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2180517626105322 | validation: 0.24016226799417062]
	TIME [epoch: 7.43 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09446791946103547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09446791946103547 | validation: 0.19631654518442354]
	TIME [epoch: 7.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_577.pth
	Model improved!!!
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06397160203359561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06397160203359561 | validation: 0.282775625261568]
	TIME [epoch: 7.47 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08271471666882221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08271471666882221 | validation: 0.24541741562824068]
	TIME [epoch: 7.49 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13356823180462674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13356823180462674 | validation: 0.4218480252119132]
	TIME [epoch: 7.47 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3036867184571481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3036867184571481 | validation: 0.3280920011590429]
	TIME [epoch: 7.45 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18525124080337166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18525124080337166 | validation: 0.23903619806823695]
	TIME [epoch: 7.45 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11411764831402253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11411764831402253 | validation: 0.2247168543481573]
	TIME [epoch: 7.46 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09887594243146497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09887594243146497 | validation: 0.3003232609185437]
	TIME [epoch: 7.48 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16130750870454172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16130750870454172 | validation: 0.22208338790442594]
	TIME [epoch: 7.48 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1107714608364574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1107714608364574 | validation: 0.22703344982833526]
	TIME [epoch: 7.47 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09440293496371437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09440293496371437 | validation: 0.25214034451370654]
	TIME [epoch: 7.47 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08091234108926411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08091234108926411 | validation: 0.22033182965527864]
	TIME [epoch: 7.47 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0688914284335994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0688914284335994 | validation: 0.2155138113838894]
	TIME [epoch: 7.47 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0669630888159682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0669630888159682 | validation: 0.5788636697483492]
	TIME [epoch: 7.49 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1813737905943548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1813737905943548 | validation: 0.2948873309936648]
	TIME [epoch: 7.47 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15505885542280243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15505885542280243 | validation: 0.24222005180787312]
	TIME [epoch: 7.47 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3178797871505045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3178797871505045 | validation: 0.4568148128397793]
	TIME [epoch: 7.47 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34266637870232997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34266637870232997 | validation: 0.458554782114389]
	TIME [epoch: 7.48 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1331070118452388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1331070118452388 | validation: 0.17794361699442707]
	TIME [epoch: 7.47 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_595.pth
	Model improved!!!
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1589390452208297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1589390452208297 | validation: 0.48287890669429834]
	TIME [epoch: 7.42 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2926221814722376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2926221814722376 | validation: 0.6662019730767392]
	TIME [epoch: 7.41 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2659772705424727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2659772705424727 | validation: 0.6433147930494072]
	TIME [epoch: 7.41 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28177451800838255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28177451800838255 | validation: 0.5722248211102331]
	TIME [epoch: 7.43 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2312449022557671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2312449022557671 | validation: 0.4195583665750053]
	TIME [epoch: 7.43 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15424758857727083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15424758857727083 | validation: 0.31965646699264744]
	TIME [epoch: 7.43 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09099646356666352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09099646356666352 | validation: 0.24427599723000082]
	TIME [epoch: 7.42 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07746932236869737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07746932236869737 | validation: 0.17527264381287097]
	TIME [epoch: 7.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_603.pth
	Model improved!!!
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07412858628491019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07412858628491019 | validation: 0.2205606410973104]
	TIME [epoch: 7.44 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10610916549119734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10610916549119734 | validation: 0.272645139341516]
	TIME [epoch: 7.45 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21920770237618697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21920770237618697 | validation: 0.3410178378030504]
	TIME [epoch: 7.43 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2867054517494422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2867054517494422 | validation: 0.3427230037784734]
	TIME [epoch: 7.43 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13822082234260233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13822082234260233 | validation: 0.17154765934771699]
	TIME [epoch: 7.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_608.pth
	Model improved!!!
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0989154707864221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0989154707864221 | validation: 0.21436625395394293]
	TIME [epoch: 7.46 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12685735838092396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12685735838092396 | validation: 0.3865974821436704]
	TIME [epoch: 7.44 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0983646579177334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0983646579177334 | validation: 0.29796187298666826]
	TIME [epoch: 7.44 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09245885995216817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09245885995216817 | validation: 0.19678198835037236]
	TIME [epoch: 7.43 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07810344910784153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07810344910784153 | validation: 0.15755303266844814]
	TIME [epoch: 7.44 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_613.pth
	Model improved!!!
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06412161614119243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06412161614119243 | validation: 0.2069357034767595]
	TIME [epoch: 7.42 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06346758207337053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06346758207337053 | validation: 0.19422967155539617]
	TIME [epoch: 7.42 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07859366825841872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07859366825841872 | validation: 0.23905912302283558]
	TIME [epoch: 7.4 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12222685752795108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12222685752795108 | validation: 0.2192704467382403]
	TIME [epoch: 7.41 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14083798032895814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14083798032895814 | validation: 0.28457239225802883]
	TIME [epoch: 7.4 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1495829905842467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1495829905842467 | validation: 0.29708023164451564]
	TIME [epoch: 7.43 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2126794757609844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2126794757609844 | validation: 0.20549089611938567]
	TIME [epoch: 7.41 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17666405967110813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17666405967110813 | validation: 0.26672718381897526]
	TIME [epoch: 7.41 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14930882472947274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14930882472947274 | validation: 0.15735429884473312]
	TIME [epoch: 7.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_622.pth
	Model improved!!!
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059114400170077366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059114400170077366 | validation: 0.16708491108161524]
	TIME [epoch: 7.44 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0676684790861199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0676684790861199 | validation: 0.18291832349151568]
	TIME [epoch: 7.46 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07890502956158627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07890502956158627 | validation: 0.202618944428965]
	TIME [epoch: 7.42 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07821177945650976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07821177945650976 | validation: 0.18573025728134482]
	TIME [epoch: 7.42 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09000990030525896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09000990030525896 | validation: 0.2389921211968659]
	TIME [epoch: 7.42 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11213854791003675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11213854791003675 | validation: 0.25695053387837524]
	TIME [epoch: 7.42 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1300549449829433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1300549449829433 | validation: 0.22909928095696191]
	TIME [epoch: 7.44 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13466163241662288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13466163241662288 | validation: 0.2213753475526517]
	TIME [epoch: 7.42 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15205842637817982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15205842637817982 | validation: 0.32290998191339104]
	TIME [epoch: 7.43 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12376723419638307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12376723419638307 | validation: 0.17426960058714602]
	TIME [epoch: 7.41 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0752588536052674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0752588536052674 | validation: 0.15758273410850238]
	TIME [epoch: 7.42 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05458504990001526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05458504990001526 | validation: 0.1795232194897917]
	TIME [epoch: 7.42 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05945086553455104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05945086553455104 | validation: 0.22902815959760003]
	TIME [epoch: 7.45 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08568014641850759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08568014641850759 | validation: 0.23630308911220554]
	TIME [epoch: 7.41 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15178089233988382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15178089233988382 | validation: 0.24454461116347687]
	TIME [epoch: 7.43 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17414614466297967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17414614466297967 | validation: 0.29999003134716323]
	TIME [epoch: 7.42 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1584084528469739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1584084528469739 | validation: 0.17175786187176406]
	TIME [epoch: 7.43 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08523398562820857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08523398562820857 | validation: 0.18706329085688]
	TIME [epoch: 7.44 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12385750069708952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12385750069708952 | validation: 0.2491689647262636]
	TIME [epoch: 7.42 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.139290523475554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.139290523475554 | validation: 0.18390054749629733]
	TIME [epoch: 7.47 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11340285872298952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11340285872298952 | validation: 0.3158664523287925]
	TIME [epoch: 7.44 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10305257073257071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10305257073257071 | validation: 0.26783982487660546]
	TIME [epoch: 7.44 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14198476513846095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14198476513846095 | validation: 0.34659568820251535]
	TIME [epoch: 7.45 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10760398453008777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10760398453008777 | validation: 0.26080929189085506]
	TIME [epoch: 7.44 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0861389827608457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0861389827608457 | validation: 0.16997545919783386]
	TIME [epoch: 7.42 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06332820041371184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06332820041371184 | validation: 0.159891262507082]
	TIME [epoch: 7.44 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07344863852372417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07344863852372417 | validation: 0.1391365620907841]
	TIME [epoch: 7.44 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_649.pth
	Model improved!!!
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06761007753013658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06761007753013658 | validation: 0.20088736114797745]
	TIME [epoch: 7.41 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08406059742442704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08406059742442704 | validation: 0.1674991902759568]
	TIME [epoch: 7.39 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11730889489980867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11730889489980867 | validation: 0.30079178160077097]
	TIME [epoch: 7.39 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19166410079638355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19166410079638355 | validation: 0.1972442895399893]
	TIME [epoch: 7.39 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1113480972856504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1113480972856504 | validation: 0.17054947904516574]
	TIME [epoch: 7.4 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08662746119338487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08662746119338487 | validation: 0.16719574434760753]
	TIME [epoch: 7.41 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1030130938000756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1030130938000756 | validation: 0.22410675016340434]
	TIME [epoch: 7.39 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1890355105339296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1890355105339296 | validation: 0.24369329176401058]
	TIME [epoch: 7.39 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10038732428685325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10038732428685325 | validation: 0.14545393968300685]
	TIME [epoch: 7.39 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09902623147993339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09902623147993339 | validation: 0.206389245516033]
	TIME [epoch: 7.4 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09927860563893857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09927860563893857 | validation: 0.2338670384126699]
	TIME [epoch: 7.44 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08025501970384162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08025501970384162 | validation: 0.14885122883349]
	TIME [epoch: 7.39 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06829900393493558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06829900393493558 | validation: 0.1369557929336846]
	TIME [epoch: 7.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_662.pth
	Model improved!!!
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056921005902148124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056921005902148124 | validation: 0.24440445981124825]
	TIME [epoch: 7.39 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07411315851146318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07411315851146318 | validation: 0.16530962232016438]
	TIME [epoch: 7.39 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10635108254352962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10635108254352962 | validation: 0.2642418232666221]
	TIME [epoch: 7.41 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19623289708292152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19623289708292152 | validation: 0.6454549372637818]
	TIME [epoch: 7.4 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38490321757033835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38490321757033835 | validation: 0.251854126939238]
	TIME [epoch: 7.4 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13270751395526612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13270751395526612 | validation: 0.15595797964378472]
	TIME [epoch: 7.39 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18753892128582159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18753892128582159 | validation: 0.2639170185453341]
	TIME [epoch: 7.4 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12003401722695771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12003401722695771 | validation: 0.5494931637591448]
	TIME [epoch: 7.4 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1605521750196612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1605521750196612 | validation: 0.40544030253347413]
	TIME [epoch: 7.41 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1029719625850959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1029719625850959 | validation: 0.17669930496853947]
	TIME [epoch: 7.39 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06927042392187861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06927042392187861 | validation: 0.11858738110613239]
	TIME [epoch: 7.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_673.pth
	Model improved!!!
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0655155624825448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0655155624825448 | validation: 0.15497267718789665]
	TIME [epoch: 7.43 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0583754391612494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0583754391612494 | validation: 0.17233122434840983]
	TIME [epoch: 7.44 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0794541217401891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0794541217401891 | validation: 0.21782540181440213]
	TIME [epoch: 7.45 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10677336702054728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10677336702054728 | validation: 0.2683856898119353]
	TIME [epoch: 7.43 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14792172317421223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14792172317421223 | validation: 0.4178089842238377]
	TIME [epoch: 7.44 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24109322120723056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24109322120723056 | validation: 0.2366705350729623]
	TIME [epoch: 7.43 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17392219938322315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17392219938322315 | validation: 0.23018633022717327]
	TIME [epoch: 7.44 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06588575775196481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06588575775196481 | validation: 0.1813962896940555]
	TIME [epoch: 7.44 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08296777993979738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08296777993979738 | validation: 0.16309509106461056]
	TIME [epoch: 7.44 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07508591484384468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07508591484384468 | validation: 0.2845798651989453]
	TIME [epoch: 7.42 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0661643157318004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0661643157318004 | validation: 0.20338972913272732]
	TIME [epoch: 7.42 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050631379727135994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050631379727135994 | validation: 0.15848673333755928]
	TIME [epoch: 7.43 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06319562018801804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06319562018801804 | validation: 0.22230941388098557]
	TIME [epoch: 7.43 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07941553870975929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07941553870975929 | validation: 0.24157558656033828]
	TIME [epoch: 7.46 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11598185535887058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11598185535887058 | validation: 0.21431054858582083]
	TIME [epoch: 7.42 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1401565755131744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1401565755131744 | validation: 0.26407459936139105]
	TIME [epoch: 7.43 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.181608158508217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.181608158508217 | validation: 0.15306216020297092]
	TIME [epoch: 7.43 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06143778080337173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06143778080337173 | validation: 0.21055642036417327]
	TIME [epoch: 7.43 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0762870871330884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0762870871330884 | validation: 0.29836499552151985]
	TIME [epoch: 7.46 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06812387403464185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06812387403464185 | validation: 0.20962814710212263]
	TIME [epoch: 7.42 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06518101499729699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06518101499729699 | validation: 0.18046214327099674]
	TIME [epoch: 7.43 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0874125281207134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0874125281207134 | validation: 0.19775606861820863]
	TIME [epoch: 7.43 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11849478269103436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11849478269103436 | validation: 0.29712509962884726]
	TIME [epoch: 7.43 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16177660026242888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16177660026242888 | validation: 0.2536300313303298]
	TIME [epoch: 7.5 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23478196453667544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23478196453667544 | validation: 0.20159390611853817]
	TIME [epoch: 7.44 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041054329811315286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041054329811315286 | validation: 0.1086997408504413]
	TIME [epoch: 7.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_699.pth
	Model improved!!!
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07444875775648707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07444875775648707 | validation: 0.2305169123241962]
	TIME [epoch: 7.42 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13751628111881314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13751628111881314 | validation: 0.38576607071269414]
	TIME [epoch: 7.46 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10606537474589306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10606537474589306 | validation: 0.19748307981196456]
	TIME [epoch: 7.46 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060455512827036005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060455512827036005 | validation: 0.14492423799032647]
	TIME [epoch: 7.46 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06050490868309584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06050490868309584 | validation: 0.14402862529120816]
	TIME [epoch: 7.45 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06481078341214568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06481078341214568 | validation: 0.20054210010341178]
	TIME [epoch: 7.45 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11415341082149955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11415341082149955 | validation: 0.20757051465108992]
	TIME [epoch: 7.45 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13810484524465674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13810484524465674 | validation: 0.17390910531761186]
	TIME [epoch: 7.45 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09396686976445638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09396686976445638 | validation: 0.14502742245602934]
	TIME [epoch: 7.47 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04689984403478133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04689984403478133 | validation: 0.1377560794581618]
	TIME [epoch: 7.45 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05422145504506792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05422145504506792 | validation: 0.1833905924179665]
	TIME [epoch: 7.45 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06986197160334337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06986197160334337 | validation: 0.23021418325975365]
	TIME [epoch: 7.44 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14749726998383872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14749726998383872 | validation: 0.32562923471330774]
	TIME [epoch: 7.45 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0757741521039213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0757741521039213 | validation: 0.21861222465043087]
	TIME [epoch: 7.49 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05778021346904209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05778021346904209 | validation: 0.24589447484306062]
	TIME [epoch: 7.47 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14312799345670996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14312799345670996 | validation: 0.36784533261652225]
	TIME [epoch: 7.46 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1757386796221681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1757386796221681 | validation: 0.3353573976725468]
	TIME [epoch: 7.44 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14504707657460864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14504707657460864 | validation: 0.15518140594867663]
	TIME [epoch: 7.44 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07537559208666014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07537559208666014 | validation: 0.12568032213158484]
	TIME [epoch: 7.45 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04938617849915759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04938617849915759 | validation: 0.12064193906940882]
	TIME [epoch: 7.47 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04211820810852339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04211820810852339 | validation: 0.12065406763611441]
	TIME [epoch: 7.44 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042752788734672295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042752788734672295 | validation: 0.14947127687227396]
	TIME [epoch: 7.45 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06277462121093948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06277462121093948 | validation: 0.2245640237506764]
	TIME [epoch: 7.44 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17820334128191234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17820334128191234 | validation: 0.24947984259989076]
	TIME [epoch: 7.45 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09313112590101383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09313112590101383 | validation: 0.2831709858234901]
	TIME [epoch: 7.47 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1359980448795976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1359980448795976 | validation: 0.18158675476772396]
	TIME [epoch: 7.44 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12007518453650591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12007518453650591 | validation: 0.1552467500964258]
	TIME [epoch: 7.45 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06062899314500852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06062899314500852 | validation: 0.13085202269550594]
	TIME [epoch: 7.44 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05326074759759898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05326074759759898 | validation: 0.12925559032453512]
	TIME [epoch: 7.44 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08005024602772604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08005024602772604 | validation: 0.21031463262163907]
	TIME [epoch: 7.45 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12894852383430735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12894852383430735 | validation: 0.1743763997946828]
	TIME [epoch: 7.45 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11653495388696569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11653495388696569 | validation: 0.17479726223629596]
	TIME [epoch: 7.48 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08099228553122277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08099228553122277 | validation: 0.15156655478420578]
	TIME [epoch: 7.44 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09619311585428662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09619311585428662 | validation: 0.22537529490268526]
	TIME [epoch: 7.44 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07182629871759395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07182629871759395 | validation: 0.24471913969102974]
	TIME [epoch: 7.44 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1668941418343607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1668941418343607 | validation: 0.21870545176490938]
	TIME [epoch: 7.46 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052287860350322804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052287860350322804 | validation: 0.23421338896865118]
	TIME [epoch: 7.44 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09394615801987158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09394615801987158 | validation: 0.25083920417861355]
	TIME [epoch: 7.44 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12756776856127944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12756776856127944 | validation: 0.2686359332335729]
	TIME [epoch: 7.44 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05363826300078545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05363826300078545 | validation: 0.2235379554031776]
	TIME [epoch: 7.44 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08295913250575004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08295913250575004 | validation: 0.1672587287230526]
	TIME [epoch: 7.45 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07569117191458538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07569117191458538 | validation: 0.12022298598367881]
	TIME [epoch: 7.44 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05028887619771002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05028887619771002 | validation: 0.17814919675133184]
	TIME [epoch: 7.43 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12220771794700287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12220771794700287 | validation: 0.2635810677521731]
	TIME [epoch: 7.43 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20636151053767035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20636151053767035 | validation: 0.20457863470365523]
	TIME [epoch: 7.43 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12132433649482906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12132433649482906 | validation: 0.10222398509900602]
	TIME [epoch: 7.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_745.pth
	Model improved!!!
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05059539396246313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05059539396246313 | validation: 0.1855299477395993]
	TIME [epoch: 7.45 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07329130036437974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07329130036437974 | validation: 0.47177909954915453]
	TIME [epoch: 7.44 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5647634097912981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5647634097912981 | validation: 0.34165092484139375]
	TIME [epoch: 7.45 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4290624660626473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4290624660626473 | validation: 0.26584040128244546]
	TIME [epoch: 7.44 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2176908387557004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2176908387557004 | validation: 0.3263022093014032]
	TIME [epoch: 7.44 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16561855836501033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16561855836501033 | validation: 0.37911179761971114]
	TIME [epoch: 7.46 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13305325622755754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13305325622755754 | validation: 0.3178179492270194]
	TIME [epoch: 7.44 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08455794923155038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08455794923155038 | validation: 0.16917896056144943]
	TIME [epoch: 7.42 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05420723358185079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05420723358185079 | validation: 0.2407805858354021]
	TIME [epoch: 7.44 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06840501657026637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06840501657026637 | validation: 0.20785079659813324]
	TIME [epoch: 7.43 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056161828418716635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056161828418716635 | validation: 0.19872807584549787]
	TIME [epoch: 7.46 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04218984620067098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04218984620067098 | validation: 0.19377983365794735]
	TIME [epoch: 7.44 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04260862326374765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04260862326374765 | validation: 0.21535408863506347]
	TIME [epoch: 7.44 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05561420498274131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05561420498274131 | validation: 0.2240317733613106]
	TIME [epoch: 7.43 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10583219698577571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10583219698577571 | validation: 0.2571254497133489]
	TIME [epoch: 7.44 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15608230450965624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15608230450965624 | validation: 0.39117786220121564]
	TIME [epoch: 7.43 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15824602749174624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15824602749174624 | validation: 0.18771574653872306]
	TIME [epoch: 7.46 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051789810818773034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051789810818773034 | validation: 0.12241159098918836]
	TIME [epoch: 7.43 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03900566712734591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03900566712734591 | validation: 0.14707493282762468]
	TIME [epoch: 7.43 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05011549428467994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05011549428467994 | validation: 0.24468750101599046]
	TIME [epoch: 7.43 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09018034162090004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09018034162090004 | validation: 0.21544908042117364]
	TIME [epoch: 7.43 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17440720108559493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17440720108559493 | validation: 0.3226735246630815]
	TIME [epoch: 7.46 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06507147917744233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06507147917744233 | validation: 0.1722828172572712]
	TIME [epoch: 7.43 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06259728891355512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06259728891355512 | validation: 0.2285657680300306]
	TIME [epoch: 7.42 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10368881215468641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10368881215468641 | validation: 0.26496526223293326]
	TIME [epoch: 7.44 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08790864661320591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08790864661320591 | validation: 0.2180544369950851]
	TIME [epoch: 7.42 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07464148088672254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07464148088672254 | validation: 0.17083991618271702]
	TIME [epoch: 7.45 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07160860053226188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07160860053226188 | validation: 0.16434325555308635]
	TIME [epoch: 7.44 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07681231521443435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07681231521443435 | validation: 0.1466870664720828]
	TIME [epoch: 7.44 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08747741326766065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08747741326766065 | validation: 0.29808692487797955]
	TIME [epoch: 7.43 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14041353460406386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14041353460406386 | validation: 0.18526257266864243]
	TIME [epoch: 7.43 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11581017207660736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11581017207660736 | validation: 0.14479415461236175]
	TIME [epoch: 7.44 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04745624351123613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04745624351123613 | validation: 0.12205323451288703]
	TIME [epoch: 7.45 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03754794261114987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03754794261114987 | validation: 0.10575689352243514]
	TIME [epoch: 7.43 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03952405780953651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03952405780953651 | validation: 0.1647832809845088]
	TIME [epoch: 7.44 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06254963583279782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06254963583279782 | validation: 0.14496973928331602]
	TIME [epoch: 7.46 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09734076088186441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09734076088186441 | validation: 0.19962640096378437]
	TIME [epoch: 7.44 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07248428271238487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07248428271238487 | validation: 0.14590667449839775]
	TIME [epoch: 7.44 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08577204735209515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08577204735209515 | validation: 0.331054830916508]
	TIME [epoch: 7.45 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10991033408713044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10991033408713044 | validation: 0.26318628725783144]
	TIME [epoch: 7.46 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09151412846845096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09151412846845096 | validation: 0.1336957031624848]
	TIME [epoch: 7.45 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052108459880750094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052108459880750094 | validation: 0.1262949174364379]
	TIME [epoch: 7.44 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06701373992169252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06701373992169252 | validation: 0.18437807368140818]
	TIME [epoch: 7.45 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10657935052609428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10657935052609428 | validation: 0.28563473785851035]
	TIME [epoch: 7.45 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18197154460664736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18197154460664736 | validation: 0.39486032070484867]
	TIME [epoch: 7.44 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08829493075910202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08829493075910202 | validation: 0.4988651993035544]
	TIME [epoch: 7.43 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5308821270181331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5308821270181331 | validation: 0.4273718569206606]
	TIME [epoch: 7.44 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5858028799627218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5858028799627218 | validation: 0.31068138619962743]
	TIME [epoch: 7.43 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3041227744765356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3041227744765356 | validation: 0.24442479131917969]
	TIME [epoch: 7.46 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08824789295435576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08824789295435576 | validation: 0.35879759899603036]
	TIME [epoch: 7.44 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07893371987988941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07893371987988941 | validation: 0.4011131578271253]
	TIME [epoch: 7.44 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0719646155882833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0719646155882833 | validation: 0.2838080779166972]
	TIME [epoch: 7.43 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04360943170706258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04360943170706258 | validation: 0.18919057830832472]
	TIME [epoch: 7.48 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03671277430177856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03671277430177856 | validation: 0.15807114385743512]
	TIME [epoch: 7.45 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03874389358758952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03874389358758952 | validation: 0.1628556215769316]
	TIME [epoch: 7.45 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03715552955217799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03715552955217799 | validation: 0.1772781531291137]
	TIME [epoch: 7.44 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043955030221250885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043955030221250885 | validation: 0.3183354829777971]
	TIME [epoch: 7.44 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09921329079011322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09921329079011322 | validation: 0.3247638458322051]
	TIME [epoch: 7.46 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18969253294349894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18969253294349894 | validation: 0.2903269519146577]
	TIME [epoch: 7.44 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1284806841371104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1284806841371104 | validation: 0.14383121112125152]
	TIME [epoch: 7.44 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04611816533317469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04611816533317469 | validation: 0.15894582958738948]
	TIME [epoch: 7.43 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038832919789269005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038832919789269005 | validation: 0.15020569475069734]
	TIME [epoch: 7.42 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0386467368689847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0386467368689847 | validation: 0.13938554379979298]
	TIME [epoch: 7.43 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03859256795889719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03859256795889719 | validation: 0.13996406564530917]
	TIME [epoch: 7.43 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03660876048206768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03660876048206768 | validation: 0.12307003168736347]
	TIME [epoch: 7.45 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03773590314264619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03773590314264619 | validation: 0.17425876444510757]
	TIME [epoch: 7.42 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06743036039893643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06743036039893643 | validation: 0.22294186304517558]
	TIME [epoch: 7.43 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2273205699915838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2273205699915838 | validation: 0.21084543602706987]
	TIME [epoch: 7.43 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14990821453745629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14990821453745629 | validation: 0.2235705397115457]
	TIME [epoch: 7.43 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05847593967665498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05847593967665498 | validation: 0.1963310264005811]
	TIME [epoch: 7.45 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07557891686017654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07557891686017654 | validation: 0.18514886413460144]
	TIME [epoch: 7.45 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0902435812982663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0902435812982663 | validation: 0.1828295370061892]
	TIME [epoch: 7.43 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11524410268354761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11524410268354761 | validation: 0.282584446403168]
	TIME [epoch: 7.44 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12253690600207531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12253690600207531 | validation: 0.14808096678601476]
	TIME [epoch: 7.43 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07397725591180945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07397725591180945 | validation: 0.13836509540527708]
	TIME [epoch: 7.45 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03727638601229599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03727638601229599 | validation: 0.11092572399832756]
	TIME [epoch: 7.46 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03249381577277172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03249381577277172 | validation: 0.1073519828697184]
	TIME [epoch: 7.44 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04144238121691953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04144238121691953 | validation: 0.19447054440654835]
	TIME [epoch: 7.44 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0489438861316626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0489438861316626 | validation: 0.11603919506725327]
	TIME [epoch: 7.43 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05732687447003432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05732687447003432 | validation: 0.26718938693798633]
	TIME [epoch: 7.44 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07337061534662842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07337061534662842 | validation: 0.16369545464312416]
	TIME [epoch: 7.44 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07413784880933537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07413784880933537 | validation: 0.15712666010906853]
	TIME [epoch: 7.44 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07663846733173311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07663846733173311 | validation: 0.12093915116588456]
	TIME [epoch: 7.42 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06682519532679157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06682519532679157 | validation: 0.26009797795120965]
	TIME [epoch: 7.44 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07961965973881167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07961965973881167 | validation: 0.15217283503949905]
	TIME [epoch: 7.42 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10018478755600434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10018478755600434 | validation: 0.1925928976236731]
	TIME [epoch: 7.44 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1264184232177507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1264184232177507 | validation: 0.15079630581758668]
	TIME [epoch: 7.44 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09970310556777688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09970310556777688 | validation: 0.1268090285443815]
	TIME [epoch: 7.44 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06670641678422494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06670641678422494 | validation: 0.09470018577560178]
	TIME [epoch: 7.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_834.pth
	Model improved!!!
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04110824625739726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04110824625739726 | validation: 0.08596524436459418]
	TIME [epoch: 7.44 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_835.pth
	Model improved!!!
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028500874185224748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028500874185224748 | validation: 0.09182606642370383]
	TIME [epoch: 7.43 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03898176380859935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03898176380859935 | validation: 0.15986138310627768]
	TIME [epoch: 7.44 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09381998602458266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09381998602458266 | validation: 0.34306784506658916]
	TIME [epoch: 7.42 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17448501517915044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17448501517915044 | validation: 0.19163894103153523]
	TIME [epoch: 7.42 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10626811383346908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10626811383346908 | validation: 0.148522145358537]
	TIME [epoch: 7.43 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05256218287136233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05256218287136233 | validation: 0.23503055246057186]
	TIME [epoch: 7.43 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06655343874799081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06655343874799081 | validation: 0.1598866771885637]
	TIME [epoch: 7.45 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06088610345641088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06088610345641088 | validation: 0.1515459775743253]
	TIME [epoch: 7.44 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07388897149397183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07388897149397183 | validation: 0.16483324719715436]
	TIME [epoch: 7.43 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10290503377674691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10290503377674691 | validation: 0.1560388291943502]
	TIME [epoch: 7.43 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08475215643868059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08475215643868059 | validation: 0.13834595311316153]
	TIME [epoch: 7.43 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057637733469221956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057637733469221956 | validation: 0.1470765353539282]
	TIME [epoch: 7.43 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04707340706323031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04707340706323031 | validation: 0.10020322954935984]
	TIME [epoch: 7.45 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043612910671624566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043612910671624566 | validation: 0.11104675592729635]
	TIME [epoch: 7.42 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031538661041005986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031538661041005986 | validation: 0.12599342185158993]
	TIME [epoch: 7.43 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03681190227595672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03681190227595672 | validation: 0.09972026022954614]
	TIME [epoch: 7.42 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058657534828718064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058657534828718064 | validation: 0.29086829321620866]
	TIME [epoch: 7.44 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22464215030530438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22464215030530438 | validation: 0.4089992201033315]
	TIME [epoch: 7.44 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10980133138302099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10980133138302099 | validation: 0.27003425461884056]
	TIME [epoch: 7.44 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0979446431183775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0979446431183775 | validation: 0.28286493972271487]
	TIME [epoch: 7.42 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20058388395433077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20058388395433077 | validation: 0.15022339323749873]
	TIME [epoch: 7.45 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0617641757610385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0617641757610385 | validation: 0.12415227124166309]
	TIME [epoch: 7.43 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09858047106933071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09858047106933071 | validation: 0.16641349049756438]
	TIME [epoch: 7.44 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08272484912215948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08272484912215948 | validation: 0.1300776567569942]
	TIME [epoch: 7.43 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048512309332892974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048512309332892974 | validation: 0.17321678092015158]
	TIME [epoch: 7.42 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06044778355883119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06044778355883119 | validation: 0.12215854940202116]
	TIME [epoch: 7.42 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05529511340443096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05529511340443096 | validation: 0.13228555302070508]
	TIME [epoch: 7.42 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0543915523617053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0543915523617053 | validation: 0.11687492544142906]
	TIME [epoch: 7.43 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06612295228080864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06612295228080864 | validation: 0.1159618324084474]
	TIME [epoch: 7.44 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07117616665086599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07117616665086599 | validation: 0.1259247437232059]
	TIME [epoch: 7.42 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06918236070688813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06918236070688813 | validation: 0.34085835074032683]
	TIME [epoch: 7.42 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10536792898780263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10536792898780263 | validation: 0.16307880395471064]
	TIME [epoch: 7.42 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05336052468792543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05336052468792543 | validation: 0.10899169683083226]
	TIME [epoch: 7.43 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045096176411605536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045096176411605536 | validation: 0.10309900741847039]
	TIME [epoch: 7.45 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049846972455055656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049846972455055656 | validation: 0.12126627103766588]
	TIME [epoch: 7.42 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08031781438935905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08031781438935905 | validation: 0.2566815814655993]
	TIME [epoch: 7.42 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1104428397066787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1104428397066787 | validation: 0.14509729995308046]
	TIME [epoch: 7.42 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0862610397739844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0862610397739844 | validation: 0.1473720713543418]
	TIME [epoch: 7.42 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05815718568210274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05815718568210274 | validation: 0.17318923707737335]
	TIME [epoch: 7.42 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05509876011206233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05509876011206233 | validation: 0.1338052990309588]
	TIME [epoch: 7.47 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049987056557315945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049987056557315945 | validation: 0.1569557672896854]
	TIME [epoch: 7.42 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0497081007191792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0497081007191792 | validation: 0.1516649116169705]
	TIME [epoch: 7.42 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08166474452065764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08166474452065764 | validation: 0.17572817248213307]
	TIME [epoch: 7.43 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11838526416899647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11838526416899647 | validation: 0.13405251036105853]
	TIME [epoch: 7.44 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09043661594212137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09043661594212137 | validation: 0.1502692559670093]
	TIME [epoch: 7.45 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04863693724109333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04863693724109333 | validation: 0.10705356358058865]
	TIME [epoch: 7.42 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03542667006009556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03542667006009556 | validation: 0.10226620896478195]
	TIME [epoch: 7.42 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024674380391355097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024674380391355097 | validation: 0.07709517087024398]
	TIME [epoch: 7.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_883.pth
	Model improved!!!
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023649080422921014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023649080422921014 | validation: 0.08332201862719801]
	TIME [epoch: 7.43 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02831614337297836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02831614337297836 | validation: 0.12963238427173335]
	TIME [epoch: 7.42 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04543914806652687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04543914806652687 | validation: 0.17840866426854682]
	TIME [epoch: 7.42 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11409250453005498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11409250453005498 | validation: 0.2831085564140599]
	TIME [epoch: 7.42 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1597690915648559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1597690915648559 | validation: 0.2653936640390341]
	TIME [epoch: 7.43 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16551721540654835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16551721540654835 | validation: 0.18510666242515353]
	TIME [epoch: 7.42 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11646258811197345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11646258811197345 | validation: 0.29062143035379406]
	TIME [epoch: 7.42 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2502989364458089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2502989364458089 | validation: 0.239390547690135]
	TIME [epoch: 7.44 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14733239315984883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14733239315984883 | validation: 0.3214414810918219]
	TIME [epoch: 7.44 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049747570665900645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049747570665900645 | validation: 0.3318266707581458]
	TIME [epoch: 7.42 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06182604904913958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06182604904913958 | validation: 0.12594783863268863]
	TIME [epoch: 7.42 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046463213973964294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046463213973964294 | validation: 0.11759246648247479]
	TIME [epoch: 7.41 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033568829134703417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033568829134703417 | validation: 0.10332857661251671]
	TIME [epoch: 7.44 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03777498952002421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03777498952002421 | validation: 0.11897632260355016]
	TIME [epoch: 7.42 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0625806581542904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0625806581542904 | validation: 0.1585591916759709]
	TIME [epoch: 7.42 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09939997244198859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09939997244198859 | validation: 0.1840451479714019]
	TIME [epoch: 7.42 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10421519832189638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10421519832189638 | validation: 0.821855903027331]
	TIME [epoch: 7.42 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30162930615551165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30162930615551165 | validation: 0.5973522695373383]
	TIME [epoch: 7.45 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2034023128432634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2034023128432634 | validation: 0.5737204023309337]
	TIME [epoch: 7.43 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6229391276552292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6229391276552292 | validation: 0.644275154094242]
	TIME [epoch: 7.43 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6438837536158292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6438837536158292 | validation: 0.6570453650947239]
	TIME [epoch: 7.41 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6614700034936396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6614700034936396 | validation: 0.6434197385184031]
	TIME [epoch: 7.42 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6657722853758452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6657722853758452 | validation: 0.3561138587209409]
	TIME [epoch: 7.43 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5182630152713562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5182630152713562 | validation: 0.2359887195293295]
	TIME [epoch: 7.44 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25167630189760937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25167630189760937 | validation: 0.6456851055294853]
	TIME [epoch: 7.42 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3748048305092623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3748048305092623 | validation: 0.3142635629528857]
	TIME [epoch: 7.42 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25057122915645424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25057122915645424 | validation: 0.39685428194637073]
	TIME [epoch: 7.43 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17004839570773095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17004839570773095 | validation: 0.23364432334546428]
	TIME [epoch: 7.43 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11855869592565663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11855869592565663 | validation: 0.24224321703167387]
	TIME [epoch: 7.45 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07223588282859016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07223588282859016 | validation: 0.21706753831357203]
	TIME [epoch: 7.42 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06609601005820695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06609601005820695 | validation: 0.20702040886359108]
	TIME [epoch: 7.42 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060666650898780344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060666650898780344 | validation: 0.21258710270512995]
	TIME [epoch: 7.43 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07474425617601402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07474425617601402 | validation: 0.2968737826915711]
	TIME [epoch: 7.43 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10031912693626918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10031912693626918 | validation: 0.22133669349265095]
	TIME [epoch: 7.44 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10373989554643252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10373989554643252 | validation: 0.20467978556929498]
	TIME [epoch: 7.43 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07198335097463149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07198335097463149 | validation: 0.16619931974597676]
	TIME [epoch: 7.42 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048388396657058595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048388396657058595 | validation: 0.16552636058744913]
	TIME [epoch: 7.42 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036326535244871144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036326535244871144 | validation: 0.15921296203459218]
	TIME [epoch: 7.42 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04484656672715417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04484656672715417 | validation: 0.1708784404935994]
	TIME [epoch: 7.43 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03484270916005766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03484270916005766 | validation: 0.13942010009905847]
	TIME [epoch: 7.45 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03584055849891727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03584055849891727 | validation: 0.13011015079935143]
	TIME [epoch: 7.43 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04958204396913594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04958204396913594 | validation: 0.21985782277107227]
	TIME [epoch: 7.43 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13031398451131104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13031398451131104 | validation: 0.2568534850026207]
	TIME [epoch: 7.43 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06870585314822286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06870585314822286 | validation: 0.14816866837384274]
	TIME [epoch: 7.43 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04648324594751266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04648324594751266 | validation: 0.19556780458266934]
	TIME [epoch: 7.48 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07478748549103745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07478748549103745 | validation: 0.18527543704340774]
	TIME [epoch: 7.44 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08960756871982073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08960756871982073 | validation: 0.15232182659577345]
	TIME [epoch: 7.41 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05529785429168186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05529785429168186 | validation: 0.11762851310172051]
	TIME [epoch: 7.43 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034810229475796095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034810229475796095 | validation: 0.10595202716042579]
	TIME [epoch: 7.41 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0354366273008979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0354366273008979 | validation: 0.1471167945559566]
	TIME [epoch: 7.44 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0777799955812048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0777799955812048 | validation: 0.30753197478528205]
	TIME [epoch: 7.43 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1640532393463328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1640532393463328 | validation: 0.21767554248532736]
	TIME [epoch: 7.43 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15845139358167365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15845139358167365 | validation: 0.16771675657058768]
	TIME [epoch: 7.41 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04994953300168972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04994953300168972 | validation: 0.1291004034554469]
	TIME [epoch: 7.4 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024445856943706715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024445856943706715 | validation: 0.10136881749332992]
	TIME [epoch: 7.42 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03027693751307403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03027693751307403 | validation: 0.11920050563348182]
	TIME [epoch: 7.45 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037612819720280584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037612819720280584 | validation: 0.11016942706901518]
	TIME [epoch: 7.41 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04820585094042107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04820585094042107 | validation: 0.1523165832933028]
	TIME [epoch: 7.42 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09066663948367594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09066663948367594 | validation: 0.18667639500578295]
	TIME [epoch: 7.42 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16812607725380474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16812607725380474 | validation: 0.17842514981719468]
	TIME [epoch: 7.43 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08080861046821275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08080861046821275 | validation: 0.12506247493820985]
	TIME [epoch: 7.43 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06118904872870488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06118904872870488 | validation: 0.11341979634632993]
	TIME [epoch: 7.43 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053558278118864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053558278118864 | validation: 0.12506412502617087]
	TIME [epoch: 7.42 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03439953875233998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03439953875233998 | validation: 0.13429259503832958]
	TIME [epoch: 7.42 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03708716748172248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03708716748172248 | validation: 0.40003745487463843]
	TIME [epoch: 7.42 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0894367613002409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0894367613002409 | validation: 0.16034382432949712]
	TIME [epoch: 7.4 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04261018112848918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04261018112848918 | validation: 0.09377329390885154]
	TIME [epoch: 7.45 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033792326791878206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033792326791878206 | validation: 0.11468119947469141]
	TIME [epoch: 7.45 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08164055240352039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08164055240352039 | validation: 0.255511550492164]
	TIME [epoch: 7.42 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1756785801093567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1756785801093567 | validation: 0.21779489498493124]
	TIME [epoch: 7.44 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11245806671802623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11245806671802623 | validation: 0.14465898921894244]
	TIME [epoch: 7.43 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06642254192753377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06642254192753377 | validation: 0.14146849459469354]
	TIME [epoch: 7.45 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06429983995817605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06429983995817605 | validation: 0.19566108206391233]
	TIME [epoch: 7.44 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04565371182449088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04565371182449088 | validation: 0.102018932403787]
	TIME [epoch: 7.46 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02522192543402425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02522192543402425 | validation: 0.0870407175844713]
	TIME [epoch: 7.42 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019995605698527333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019995605698527333 | validation: 0.13425461240108666]
	TIME [epoch: 7.44 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02417804833266991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02417804833266991 | validation: 0.10208440405515865]
	TIME [epoch: 7.43 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02946450223521321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02946450223521321 | validation: 0.16572790673162238]
	TIME [epoch: 7.46 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04844987542422157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04844987542422157 | validation: 0.09974650148721892]
	TIME [epoch: 7.43 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06991267856037013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06991267856037013 | validation: 0.15576815370946243]
	TIME [epoch: 7.44 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11079203090924193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11079203090924193 | validation: 0.17813524560123328]
	TIME [epoch: 7.42 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16396047069173658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16396047069173658 | validation: 0.17939383472347276]
	TIME [epoch: 7.48 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06475909154506691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06475909154506691 | validation: 0.21557075686131963]
	TIME [epoch: 7.45 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07620835148312213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07620835148312213 | validation: 0.11711986895959184]
	TIME [epoch: 7.43 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057297455493500266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057297455493500266 | validation: 0.20990502893566126]
	TIME [epoch: 7.43 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1612694001581663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1612694001581663 | validation: 0.21982146055646698]
	TIME [epoch: 7.43 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07601366789726838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07601366789726838 | validation: 0.10945205301684127]
	TIME [epoch: 7.43 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03945202021994452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03945202021994452 | validation: 0.09909738976352955]
	TIME [epoch: 7.44 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04288717971565546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04288717971565546 | validation: 0.10911354434821324]
	TIME [epoch: 7.44 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028884030492832002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028884030492832002 | validation: 0.09327886411092703]
	TIME [epoch: 7.43 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02780119086946356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02780119086946356 | validation: 0.07886106255553454]
	TIME [epoch: 7.43 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028709963520795904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028709963520795904 | validation: 0.0896005485620599]
	TIME [epoch: 7.43 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040025576254478015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040025576254478015 | validation: 0.10073633426365991]
	TIME [epoch: 7.44 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05815668349077685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05815668349077685 | validation: 0.16252844550294274]
	TIME [epoch: 7.44 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0719315552645276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0719315552645276 | validation: 0.11852127052073023]
	TIME [epoch: 7.44 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06351534624206165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06351534624206165 | validation: 0.11996106161881735]
	TIME [epoch: 7.39 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08283667633365954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08283667633365954 | validation: 0.22013231721028126]
	TIME [epoch: 7.4 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09441010678273759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09441010678273759 | validation: 0.11548858294329159]
	TIME [epoch: 7.42 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08031122544698326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08031122544698326 | validation: 0.11520543979181462]
	TIME [epoch: 7.44 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0583142508556998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0583142508556998 | validation: 0.09242930514333517]
	TIME [epoch: 7.43 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045251117209653816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045251117209653816 | validation: 0.09892999284312352]
	TIME [epoch: 7.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240822_144632/states/model_phi1_4b_v_mmd1_984.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 5238.082 seconds.
