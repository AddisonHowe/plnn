Args:
Namespace(name='model_phi1_4b_v_mmd1', outdir='out/model_training/model_phi1_4b_v_mmd1', training_data='data/training_data/data_phi1_4b/training', validation_data='data/training_data/data_phi1_4b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2334906167

Training model...

Saving initial model state to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.760963865324145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.760963865324145 | validation: 6.102315808860397]
	TIME [epoch: 44.7 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.621315693739921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.621315693739921 | validation: 4.869321659094954]
	TIME [epoch: 1.9 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.350020970144615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.350020970144615 | validation: 5.907082685243646]
	TIME [epoch: 1.89 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.911574840122379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.911574840122379 | validation: 6.0784031756041745]
	TIME [epoch: 1.89 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.032545405643514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.032545405643514 | validation: 5.524911685517609]
	TIME [epoch: 1.88 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.3433877286714555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.3433877286714555 | validation: 4.400353344440872]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.59648125517939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.59648125517939 | validation: 4.752807818208725]
	TIME [epoch: 1.88 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.897621505958459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.897621505958459 | validation: 4.666924164957735]
	TIME [epoch: 1.88 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.513427919250362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.513427919250362 | validation: 4.690298975173326]
	TIME [epoch: 1.88 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.631978112733927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.631978112733927 | validation: 4.502819175807987]
	TIME [epoch: 1.88 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.3100250136157925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3100250136157925 | validation: 4.502511299015622]
	TIME [epoch: 1.88 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.370403608065992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.370403608065992 | validation: 4.416935354860549]
	TIME [epoch: 1.88 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.288448885756552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.288448885756552 | validation: 4.493428960712127]
	TIME [epoch: 1.88 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.298565824621481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.298565824621481 | validation: 4.43576528772432]
	TIME [epoch: 1.88 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.2657585596318395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2657585596318395 | validation: 4.397760064152706]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.252652222332901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.252652222332901 | validation: 4.426433970451348]
	TIME [epoch: 1.89 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.24226193048572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.24226193048572 | validation: 4.3699085919656495]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.228068755923359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.228068755923359 | validation: 4.378149525491437]
	TIME [epoch: 1.87 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.2142155811050666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2142155811050666 | validation: 4.34068225963702]
	TIME [epoch: 1.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.203353121834507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.203353121834507 | validation: 4.354349830139066]
	TIME [epoch: 1.9 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.192310946743685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.192310946743685 | validation: 4.282683219724315]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.182227392149511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.182227392149511 | validation: 4.394068719650231]
	TIME [epoch: 1.89 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.18281203562781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.18281203562781 | validation: 4.1808503446387775]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.231729962271666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.231729962271666 | validation: 4.497134244403911]
	TIME [epoch: 1.89 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.234558621796171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.234558621796171 | validation: 4.166993732017696]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.192286359938098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.192286359938098 | validation: 4.263212265695286]
	TIME [epoch: 1.88 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.121683756315404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.121683756315404 | validation: 4.280609785009476]
	TIME [epoch: 1.88 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.113861084000188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.113861084000188 | validation: 4.158128829592953]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.111442862100846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.111442862100846 | validation: 4.265445247699373]
	TIME [epoch: 1.88 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.095898675370241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.095898675370241 | validation: 4.14833719012074]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.083088307238602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.083088307238602 | validation: 4.214777286045282]
	TIME [epoch: 1.88 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0743579202072295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0743579202072295 | validation: 4.168614518741498]
	TIME [epoch: 1.88 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.083745615592744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.083745615592744 | validation: 4.1599586454827735]
	TIME [epoch: 1.87 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.058052921646288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.058052921646288 | validation: 4.164349714886025]
	TIME [epoch: 1.88 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.044880656693741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.044880656693741 | validation: 4.089162772420095]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.022778835562752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.022778835562752 | validation: 4.187308952182392]
	TIME [epoch: 1.88 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0188032651908845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0188032651908845 | validation: 4.04760906442732]
	TIME [epoch: 1.9 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.042402127308068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.042402127308068 | validation: 4.204747230718953]
	TIME [epoch: 1.88 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.025416971085236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.025416971085236 | validation: 4.028555236164398]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.993103275305373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.993103275305373 | validation: 4.079509461699476]
	TIME [epoch: 1.89 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.953741757978386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.953741757978386 | validation: 4.043149968953881]
	TIME [epoch: 1.89 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.935203915681074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.935203915681074 | validation: 4.019194448547606]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.925824755498512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.925824755498512 | validation: 4.032539733103495]
	TIME [epoch: 1.88 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.915018337120258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.915018337120258 | validation: 3.988923252829469]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9021206523690286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9021206523690286 | validation: 4.019795382394186]
	TIME [epoch: 1.89 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.893916116944124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.893916116944124 | validation: 3.953805237239842]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8913926866904296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8913926866904296 | validation: 4.031339579578337]
	TIME [epoch: 1.88 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8891029066912863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8891029066912863 | validation: 3.9356384313405983]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.89385054419959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.89385054419959 | validation: 4.01468447848488]
	TIME [epoch: 1.88 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.869672974122401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.869672974122401 | validation: 3.9047227319428255]
	TIME [epoch: 1.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8395410733369686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8395410733369686 | validation: 3.9308237558073404]
	TIME [epoch: 1.88 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.813421233439501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.813421233439501 | validation: 3.892184416562424]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7970201068989593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7970201068989593 | validation: 3.87788820248433]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7869633437757524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7869633437757524 | validation: 3.8765339445620928]
	TIME [epoch: 1.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7713358193028355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7713358193028355 | validation: 3.854733133339556]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7579094061354064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7579094061354064 | validation: 3.8517845410861926]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.739706023506571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.739706023506571 | validation: 3.81045515047541]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7274244639269694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7274244639269694 | validation: 3.815884264752167]
	TIME [epoch: 1.89 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.70744588395022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.70744588395022 | validation: 3.755885068092137]
	TIME [epoch: 1.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6775764558257094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6775764558257094 | validation: 3.73707705457542]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6350126085488466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6350126085488466 | validation: 3.6522133999331245]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5808832066020986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5808832066020986 | validation: 3.616704764422106]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.532587990444197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.532587990444197 | validation: 3.5815166721757943]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.498246508923885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.498246508923885 | validation: 3.552871761701343]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4765280251146486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4765280251146486 | validation: 3.537477257600584]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4622645374151317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4622645374151317 | validation: 3.54648223631133]
	TIME [epoch: 1.88 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5220468675835996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5220468675835996 | validation: 3.6836908282943583]
	TIME [epoch: 1.88 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.600883703946936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.600883703946936 | validation: 3.50852298737819]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.510351297935377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.510351297935377 | validation: 3.4764565104670546]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.417997930835643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.417997930835643 | validation: 3.5287775720563213]
	TIME [epoch: 1.89 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4298289143038323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4298289143038323 | validation: 3.429856769253485]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.382661848749033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.382661848749033 | validation: 3.4278994410319004]
	TIME [epoch: 1.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.376908032235371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.376908032235371 | validation: 3.439673065192227]
	TIME [epoch: 1.88 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.364659905219617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.364659905219617 | validation: 3.4115013064536086]
	TIME [epoch: 1.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3507100758128776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3507100758128776 | validation: 3.399923161149131]
	TIME [epoch: 1.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.339773880463223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.339773880463223 | validation: 3.381402113289643]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3272657127062457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3272657127062457 | validation: 3.3775951078966244]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3173341951911977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3173341951911977 | validation: 3.3592911877062948]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.310645232727444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.310645232727444 | validation: 3.329492490912391]
	TIME [epoch: 1.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2973665568692003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2973665568692003 | validation: 3.3392800215826832]
	TIME [epoch: 1.88 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.291560929943704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.291560929943704 | validation: 3.3886877140760525]
	TIME [epoch: 1.88 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3714761511263704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3714761511263704 | validation: 3.4031900344517343]
	TIME [epoch: 1.88 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3575855110604667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3575855110604667 | validation: 3.2933849904477244]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2872736379576577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2872736379576577 | validation: 3.285047691417392]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2516806383287373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2516806383287373 | validation: 3.310827918807607]
	TIME [epoch: 1.88 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.258860808263142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.258860808263142 | validation: 3.26831576576109]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.229526689671493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.229526689671493 | validation: 3.248813287175507]
	TIME [epoch: 1.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2169206836078197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2169206836078197 | validation: 3.2363003858597703]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.209944236411509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.209944236411509 | validation: 3.1995981132292055]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1991334486119944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1991334486119944 | validation: 3.2187000575464513]
	TIME [epoch: 1.88 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1829753112772665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1829753112772665 | validation: 3.199907054507559]
	TIME [epoch: 1.88 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.196809109691653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.196809109691653 | validation: 3.3340923058302154]
	TIME [epoch: 1.88 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3138572730914992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3138572730914992 | validation: 3.1162054225504074]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.136520695249368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.136520695249368 | validation: 2.706466661604224]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9371420961802777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9371420961802777 | validation: 2.8892883295444]
	TIME [epoch: 1.89 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7619483949876673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7619483949876673 | validation: 2.1464508773217035]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1556187594371816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1556187594371816 | validation: 1.7929525979995882]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7876071449316608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7876071449316608 | validation: 2.3162144570878818]
	TIME [epoch: 1.88 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1602048885742384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1602048885742384 | validation: 1.9572115372513388]
	TIME [epoch: 1.88 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.034925746088637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.034925746088637 | validation: 1.587222838736384]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5980878230793385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5980878230793385 | validation: 1.4352090150373265]
	TIME [epoch: 1.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.335969641969545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.335969641969545 | validation: 1.2730200608338265]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2149615953069188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2149615953069188 | validation: 1.2648359276010694]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1948200858525968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1948200858525968 | validation: 1.0561471164337932]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.03278556069941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.03278556069941 | validation: 0.9865147585494869]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9922931480401593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9922931480401593 | validation: 1.0435885577912332]
	TIME [epoch: 1.88 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9981200056465792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9981200056465792 | validation: 0.983893735038604]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.010722151388757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.010722151388757 | validation: 1.1141153191561577]
	TIME [epoch: 1.88 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0149407368964969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0149407368964969 | validation: 1.0024118325217324]
	TIME [epoch: 1.88 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9705990102680077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9705990102680077 | validation: 1.0156188901404237]
	TIME [epoch: 1.88 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9182486565804755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9182486565804755 | validation: 0.97345241931257]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.905234070926947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.905234070926947 | validation: 0.9310849285987385]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8966098009259517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8966098009259517 | validation: 0.9736811556475828]
	TIME [epoch: 1.89 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8933291335895931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8933291335895931 | validation: 0.9353398924698202]
	TIME [epoch: 1.89 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8951215925475668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8951215925475668 | validation: 1.0461664015953147]
	TIME [epoch: 1.88 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9093598596895336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9093598596895336 | validation: 0.9178618839470613]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9094383502245853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9094383502245853 | validation: 0.9944194567482605]
	TIME [epoch: 1.89 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8744388457480041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8744388457480041 | validation: 0.925161238577434]
	TIME [epoch: 1.89 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8649701075670233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8649701075670233 | validation: 0.9636009852994274]
	TIME [epoch: 1.89 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8644750808946406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8644750808946406 | validation: 0.9478202951594655]
	TIME [epoch: 1.88 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8935228539512358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8935228539512358 | validation: 0.9601606440146103]
	TIME [epoch: 1.88 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9036787945496423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9036787945496423 | validation: 1.0000523626470537]
	TIME [epoch: 1.89 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9093000393899512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9093000393899512 | validation: 0.926033411632274]
	TIME [epoch: 1.89 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8741846452089312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8741846452089312 | validation: 1.0333120032583971]
	TIME [epoch: 1.89 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8741801521941313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8741801521941313 | validation: 0.9068781837098193]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8916381887768126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8916381887768126 | validation: 1.1436901707209532]
	TIME [epoch: 1.88 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9281477196796115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9281477196796115 | validation: 0.8935065098885242]
	TIME [epoch: 1.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8151024717502666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8151024717502666 | validation: 0.8776049825059706]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8172370877823797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8172370877823797 | validation: 0.9575053691539015]
	TIME [epoch: 1.88 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8229854428406644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8229854428406644 | validation: 0.8590621263744013]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8087823738988662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8087823738988662 | validation: 0.9436132568760229]
	TIME [epoch: 1.88 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8222441214272036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8222441214272036 | validation: 0.9352086368843562]
	TIME [epoch: 1.89 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.871543198026313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.871543198026313 | validation: 1.0368284837916129]
	TIME [epoch: 1.9 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9131166004463174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9131166004463174 | validation: 0.9197786857690645]
	TIME [epoch: 1.89 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8214862927202237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8214862927202237 | validation: 0.9374123947647717]
	TIME [epoch: 1.89 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7994818514917986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7994818514917986 | validation: 0.8802761987851675]
	TIME [epoch: 1.88 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.801283697418304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.801283697418304 | validation: 1.04298627304868]
	TIME [epoch: 1.88 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8234796644678825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8234796644678825 | validation: 0.8838358349009341]
	TIME [epoch: 1.88 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8379413050819662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8379413050819662 | validation: 1.088305813801016]
	TIME [epoch: 1.88 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8663398931948212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8663398931948212 | validation: 0.8844138464871533]
	TIME [epoch: 1.88 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8176701659239524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8176701659239524 | validation: 0.9425143103436183]
	TIME [epoch: 1.88 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8065375263543562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8065375263543562 | validation: 0.9113543473099028]
	TIME [epoch: 1.88 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8218021547215494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8218021547215494 | validation: 0.9813314676847539]
	TIME [epoch: 1.88 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8889727564895111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8889727564895111 | validation: 0.9286511949019305]
	TIME [epoch: 1.88 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8452154368188733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8452154368188733 | validation: 0.9589717337901966]
	TIME [epoch: 1.88 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8076020825767996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8076020825767996 | validation: 0.8643504193973955]
	TIME [epoch: 1.89 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7802500291371771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7802500291371771 | validation: 0.9790134518675311]
	TIME [epoch: 1.88 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7971917387905012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7971917387905012 | validation: 0.8936235882823402]
	TIME [epoch: 1.88 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8340074232240328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8340074232240328 | validation: 1.177519037531065]
	TIME [epoch: 1.88 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8767481268758269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8767481268758269 | validation: 0.8845084157288503]
	TIME [epoch: 1.89 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7882717064771271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7882717064771271 | validation: 0.867258963652025]
	TIME [epoch: 1.88 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7744877572630122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7744877572630122 | validation: 1.0608929151247934]
	TIME [epoch: 1.88 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8172981698992302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8172981698992302 | validation: 0.8778681154519398]
	TIME [epoch: 1.88 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8057470259414505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8057470259414505 | validation: 0.9439444466590283]
	TIME [epoch: 1.88 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7766083957965515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7766083957965515 | validation: 0.918877047501717]
	TIME [epoch: 1.89 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8084123078598193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8084123078598193 | validation: 1.0626944893073362]
	TIME [epoch: 1.89 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9309393897298525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9309393897298525 | validation: 0.9752730198786499]
	TIME [epoch: 1.89 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8747240604394013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8747240604394013 | validation: 0.9724163645832647]
	TIME [epoch: 1.89 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8051946029309818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8051946029309818 | validation: 0.8853557587956307]
	TIME [epoch: 1.88 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.773143757090293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.773143757090293 | validation: 0.8863567496639018]
	TIME [epoch: 1.88 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7774841527729264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7774841527729264 | validation: 0.9271479406961746]
	TIME [epoch: 1.88 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7807128006747489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7807128006747489 | validation: 0.8876061237462601]
	TIME [epoch: 1.88 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7860354784844192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7860354784844192 | validation: 1.034519642248395]
	TIME [epoch: 1.88 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8108441872977744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8108441872977744 | validation: 0.8999889772319067]
	TIME [epoch: 1.88 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8446658574641132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8446658574641132 | validation: 1.2694286919352413]
	TIME [epoch: 1.88 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9113000144617996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9113000144617996 | validation: 0.9112262230983253]
	TIME [epoch: 1.88 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7807249094332329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7807249094332329 | validation: 0.8858663658655097]
	TIME [epoch: 1.89 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8434550748683232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8434550748683232 | validation: 1.1109083862482714]
	TIME [epoch: 1.88 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8776516517762523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8776516517762523 | validation: 0.9674699417375404]
	TIME [epoch: 1.88 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7936862924863658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7936862924863658 | validation: 0.8959830549882032]
	TIME [epoch: 1.88 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8082939062583765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8082939062583765 | validation: 0.9577773735547783]
	TIME [epoch: 1.89 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7854985102570702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7854985102570702 | validation: 0.8940039148863502]
	TIME [epoch: 1.88 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7818952093342065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7818952093342065 | validation: 0.9113676966137385]
	TIME [epoch: 1.88 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7999482557869831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7999482557869831 | validation: 0.9590368552192177]
	TIME [epoch: 1.88 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8180738351623135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8180738351623135 | validation: 0.9135177500246805]
	TIME [epoch: 1.88 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8009821004837757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8009821004837757 | validation: 0.9658264423270652]
	TIME [epoch: 1.88 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7858736334955471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7858736334955471 | validation: 0.8767823356637149]
	TIME [epoch: 1.89 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7743341100925574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7743341100925574 | validation: 0.9342461911596829]
	TIME [epoch: 1.89 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7606005170779426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7606005170779426 | validation: 0.8701012758666856]
	TIME [epoch: 1.89 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7684465872010042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7684465872010042 | validation: 1.0935550405299193]
	TIME [epoch: 1.88 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8114708867101146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8114708867101146 | validation: 0.9850596918125124]
	TIME [epoch: 1.88 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8780692850918062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8780692850918062 | validation: 1.1234480063611805]
	TIME [epoch: 1.88 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8418020970634242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8418020970634242 | validation: 0.9741321978142662]
	TIME [epoch: 1.88 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8062104458252232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8062104458252232 | validation: 0.8728509971243694]
	TIME [epoch: 1.88 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8041873842831287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8041873842831287 | validation: 1.0858128202551889]
	TIME [epoch: 1.88 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8080641608489677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8080641608489677 | validation: 0.9099603953792127]
	TIME [epoch: 1.89 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7655341626232703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7655341626232703 | validation: 0.8589640372110104]
	TIME [epoch: 1.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_187.pth
	Model improved!!!
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7940095207644098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7940095207644098 | validation: 1.0870421704132374]
	TIME [epoch: 1.88 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8061971328147629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8061971328147629 | validation: 0.9113647806904542]
	TIME [epoch: 1.87 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.756856224674819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.756856224674819 | validation: 0.8911928135659546]
	TIME [epoch: 1.88 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7999097919772022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7999097919772022 | validation: 1.0608108443813329]
	TIME [epoch: 1.87 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.801524481029499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.801524481029499 | validation: 0.9067428915915123]
	TIME [epoch: 1.87 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7912091510446746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7912091510446746 | validation: 0.9280961119938109]
	TIME [epoch: 1.88 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8403496610553862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8403496610553862 | validation: 1.09836109571041]
	TIME [epoch: 1.88 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8898871483622184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8898871483622184 | validation: 0.9833740148840632]
	TIME [epoch: 1.87 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7831260511500395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7831260511500395 | validation: 0.8620272926187721]
	TIME [epoch: 1.88 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7840198362973402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7840198362973402 | validation: 0.951734702280621]
	TIME [epoch: 1.87 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7982379624109717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7982379624109717 | validation: 0.9092698084814187]
	TIME [epoch: 1.88 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7861899642224464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7861899642224464 | validation: 0.9275295346124217]
	TIME [epoch: 1.89 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.775710602453295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.775710602453295 | validation: 0.9904806971007357]
	TIME [epoch: 1.89 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7818457115975977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7818457115975977 | validation: 0.8729575438041466]
	TIME [epoch: 45.8 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7765023908919134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7765023908919134 | validation: 1.0682702629605827]
	TIME [epoch: 3.74 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7995843081977794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7995843081977794 | validation: 0.8590523564387003]
	TIME [epoch: 3.75 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7648236658539209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7648236658539209 | validation: 0.8890336117741688]
	TIME [epoch: 3.74 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7500709704022843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7500709704022843 | validation: 0.8956452343406478]
	TIME [epoch: 3.74 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7565095990855214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7565095990855214 | validation: 0.8622680529710905]
	TIME [epoch: 3.73 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7471084292843486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7471084292843486 | validation: 0.9374142723798538]
	TIME [epoch: 3.73 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7809337318377644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7809337318377644 | validation: 1.172725961715802]
	TIME [epoch: 3.74 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9782704044303734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9782704044303734 | validation: 1.0882362795327454]
	TIME [epoch: 3.74 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9060670265401729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9060670265401729 | validation: 0.9268123546690396]
	TIME [epoch: 3.74 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.765041350657298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.765041350657298 | validation: 0.9478447687679916]
	TIME [epoch: 3.73 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8025318836442858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8025318836442858 | validation: 1.0425590831123883]
	TIME [epoch: 3.74 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7937879819698115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7937879819698115 | validation: 0.8862647417923899]
	TIME [epoch: 3.73 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7720788776591758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7720788776591758 | validation: 1.0956167529822352]
	TIME [epoch: 3.73 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7902796667817583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7902796667817583 | validation: 0.8735738540024226]
	TIME [epoch: 3.73 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7614848428502882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7614848428502882 | validation: 0.9110050473356923]
	TIME [epoch: 3.76 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7543616420785341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7543616420785341 | validation: 0.9077603352148569]
	TIME [epoch: 3.73 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7567701731077375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7567701731077375 | validation: 0.8686723885585377]
	TIME [epoch: 3.74 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7898779292856452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7898779292856452 | validation: 1.1901241326534118]
	TIME [epoch: 3.74 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8306381491210141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8306381491210141 | validation: 0.8676691028217767]
	TIME [epoch: 3.74 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7858152564922719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7858152564922719 | validation: 0.8761996576756407]
	TIME [epoch: 3.73 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.766228895829947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.766228895829947 | validation: 1.016928367775219]
	TIME [epoch: 3.73 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7852667332542117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7852667332542117 | validation: 0.8346568468025034]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7877840549807836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7877840549807836 | validation: 1.0213851600200312]
	TIME [epoch: 3.71 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8140513100708195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8140513100708195 | validation: 0.9381126929646268]
	TIME [epoch: 3.72 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7952308356996978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7952308356996978 | validation: 0.9237360073470197]
	TIME [epoch: 3.72 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8172136781697122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8172136781697122 | validation: 1.1213034479245336]
	TIME [epoch: 3.74 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8160090181822512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8160090181822512 | validation: 0.8987575923083805]
	TIME [epoch: 3.73 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7537211696273661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7537211696273661 | validation: 0.8751331945157738]
	TIME [epoch: 3.71 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7585337145485282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7585337145485282 | validation: 0.9873766776082613]
	TIME [epoch: 3.71 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7638695370090866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7638695370090866 | validation: 0.8483323672903386]
	TIME [epoch: 3.72 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7414086786230075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7414086786230075 | validation: 0.864091771512649]
	TIME [epoch: 3.71 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7427925457958904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7427925457958904 | validation: 0.9123413983008011]
	TIME [epoch: 3.71 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7442007948788181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7442007948788181 | validation: 0.8730349791723331]
	TIME [epoch: 3.7 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7630680999790818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7630680999790818 | validation: 1.1526952769388688]
	TIME [epoch: 3.72 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.917236234559852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.917236234559852 | validation: 1.114186023901255]
	TIME [epoch: 3.71 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8776490960576766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8776490960576766 | validation: 0.8866572043565664]
	TIME [epoch: 3.72 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7588459178289637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7588459178289637 | validation: 1.0323433863033014]
	TIME [epoch: 3.73 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7726640437822943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7726640437822943 | validation: 0.8855685180170341]
	TIME [epoch: 3.74 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7504581232652114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7504581232652114 | validation: 0.8795721957183741]
	TIME [epoch: 3.74 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7349196345264332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7349196345264332 | validation: 0.8512410303567096]
	TIME [epoch: 3.74 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7357274319831568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7357274319831568 | validation: 0.8904263737237392]
	TIME [epoch: 3.73 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7485519826534852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7485519826534852 | validation: 0.8872583570081329]
	TIME [epoch: 3.73 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7953891535183877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7953891535183877 | validation: 1.0178852578480837]
	TIME [epoch: 3.72 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8962466433492641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8962466433492641 | validation: 1.2545107157690358]
	TIME [epoch: 3.72 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8540551397401359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8540551397401359 | validation: 0.8988129599726641]
	TIME [epoch: 3.72 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7629905724435577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7629905724435577 | validation: 1.0241626213033699]
	TIME [epoch: 3.73 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7996849556695321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7996849556695321 | validation: 0.8834243278336171]
	TIME [epoch: 3.73 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.759293495978972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.759293495978972 | validation: 1.0223246375599844]
	TIME [epoch: 3.73 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7554260800816225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7554260800816225 | validation: 0.8721129402565172]
	TIME [epoch: 3.75 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.735939408791422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.735939408791422 | validation: 0.8572716998958623]
	TIME [epoch: 3.74 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7345549722642574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7345549722642574 | validation: 0.8908079823569655]
	TIME [epoch: 3.73 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7318325942214903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7318325942214903 | validation: 0.8320163661378488]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_253.pth
	Model improved!!!
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7250537218131943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7250537218131943 | validation: 0.873575035701348]
	TIME [epoch: 3.71 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7224807478036853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7224807478036853 | validation: 0.8880193160323521]
	TIME [epoch: 3.71 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8368798473380453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8368798473380453 | validation: 1.495759027082137]
	TIME [epoch: 3.72 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.013055407468929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.013055407468929 | validation: 0.9988776822994345]
	TIME [epoch: 3.72 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7467375625572146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7467375625572146 | validation: 0.8727357720941428]
	TIME [epoch: 3.71 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7842791199622932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7842791199622932 | validation: 0.8572382152984113]
	TIME [epoch: 3.71 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7380084721637863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7380084721637863 | validation: 0.9292440983480637]
	TIME [epoch: 3.71 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7309660990176229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7309660990176229 | validation: 0.8299314728775663]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_261.pth
	Model improved!!!
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7173093692029161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7173093692029161 | validation: 0.8326421260191176]
	TIME [epoch: 3.71 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7234598499272736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7234598499272736 | validation: 0.8308502906345968]
	TIME [epoch: 3.7 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7754571616778441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7754571616778441 | validation: 1.0598551396631042]
	TIME [epoch: 3.7 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9590170145613693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9590170145613693 | validation: 1.2078513993444913]
	TIME [epoch: 3.7 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8320611514261527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8320611514261527 | validation: 1.0483287958783032]
	TIME [epoch: 3.7 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7529196659821628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7529196659821628 | validation: 0.9266933409376394]
	TIME [epoch: 3.71 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7529190420821132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7529190420821132 | validation: 0.8922704097564793]
	TIME [epoch: 3.7 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.725238758259096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.725238758259096 | validation: 0.8659183865697946]
	TIME [epoch: 3.7 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7270926035060541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7270926035060541 | validation: 0.8205993760284387]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_270.pth
	Model improved!!!
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7160874795561716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7160874795561716 | validation: 0.8058462473671768]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_271.pth
	Model improved!!!
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6974367457618952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6974367457618952 | validation: 0.8125531687555313]
	TIME [epoch: 3.74 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6978657453403554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6978657453403554 | validation: 0.7705402183836517]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_273.pth
	Model improved!!!
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.696025677843326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.696025677843326 | validation: 1.093796811382659]
	TIME [epoch: 3.72 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7739782486130216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7739782486130216 | validation: 0.7734807084415734]
	TIME [epoch: 3.73 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7270586318987435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7270586318987435 | validation: 0.9329755564746155]
	TIME [epoch: 3.72 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.819273875639808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.819273875639808 | validation: 1.2555520074907767]
	TIME [epoch: 3.72 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8875348297414177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8875348297414177 | validation: 0.9755404680377453]
	TIME [epoch: 3.72 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7088290243063554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7088290243063554 | validation: 0.7749769040217965]
	TIME [epoch: 3.73 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7333838749482229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7333838749482229 | validation: 0.8170383208537095]
	TIME [epoch: 3.73 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7521530862986527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7521530862986527 | validation: 1.2020592873754057]
	TIME [epoch: 3.73 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7776330397254801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7776330397254801 | validation: 0.9080520361511362]
	TIME [epoch: 3.72 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6877641831000881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6877641831000881 | validation: 0.7784675746111303]
	TIME [epoch: 3.73 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7198011065733141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7198011065733141 | validation: 0.9204074128949208]
	TIME [epoch: 3.75 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6928741520963569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6928741520963569 | validation: 1.0520415748910608]
	TIME [epoch: 3.74 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7008360745667355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7008360745667355 | validation: 0.6600519688494716]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_286.pth
	Model improved!!!
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6369858886951655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6369858886951655 | validation: 0.6815218611535839]
	TIME [epoch: 3.74 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.662395368944047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.662395368944047 | validation: 1.3653642151015462]
	TIME [epoch: 3.73 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9548735794771376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9548735794771376 | validation: 1.3112009290048503]
	TIME [epoch: 3.73 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7898930529064414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7898930529064414 | validation: 0.8549671330767767]
	TIME [epoch: 3.73 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7158866039265283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7158866039265283 | validation: 0.7201264622979423]
	TIME [epoch: 3.72 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6946406732164492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6946406732164492 | validation: 0.7603543843103977]
	TIME [epoch: 3.73 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6677934588405777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6677934588405777 | validation: 0.6744228385238387]
	TIME [epoch: 3.72 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6336502113591734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6336502113591734 | validation: 0.6644770781131829]
	TIME [epoch: 3.73 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6330841169986351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6330841169986351 | validation: 0.9201797517835808]
	TIME [epoch: 3.74 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8886907896102244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8886907896102244 | validation: 1.3548453619188918]
	TIME [epoch: 3.72 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9335267648731721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9335267648731721 | validation: 1.3939417581925875]
	TIME [epoch: 3.72 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9222066464059816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9222066464059816 | validation: 1.119162346371184]
	TIME [epoch: 3.72 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.793464206298103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.793464206298103 | validation: 0.9706359597246483]
	TIME [epoch: 3.72 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7799625065860741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7799625065860741 | validation: 1.0424264041745723]
	TIME [epoch: 3.73 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7384752121114496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7384752121114496 | validation: 1.0824681762098505]
	TIME [epoch: 3.72 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7291352079019991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7291352079019991 | validation: 0.9923549811800119]
	TIME [epoch: 3.72 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7197468897364698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7197468897364698 | validation: 0.8492498906675321]
	TIME [epoch: 3.73 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7046807809277449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7046807809277449 | validation: 0.7614325605281493]
	TIME [epoch: 3.72 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6787262363081217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6787262363081217 | validation: 0.7515531638966002]
	TIME [epoch: 3.73 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6618965596295716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6618965596295716 | validation: 0.7223877434646215]
	TIME [epoch: 3.74 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6482633741642485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6482633741642485 | validation: 0.7001964024626499]
	TIME [epoch: 3.73 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.635137376861835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.635137376861835 | validation: 0.6776645272578994]
	TIME [epoch: 3.72 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.642968698583045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.642968698583045 | validation: 0.900196984842895]
	TIME [epoch: 3.73 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8988046779546385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8988046779546385 | validation: 1.3383883446092995]
	TIME [epoch: 3.72 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9108788689535883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9108788689535883 | validation: 1.3236336914180078]
	TIME [epoch: 3.72 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7921385302221131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7921385302221131 | validation: 1.0742888010496103]
	TIME [epoch: 3.73 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.744933144602866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.744933144602866 | validation: 0.8480796869927897]
	TIME [epoch: 3.73 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.706443969639439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.706443969639439 | validation: 0.7320874961427782]
	TIME [epoch: 3.72 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.667948661068931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.667948661068931 | validation: 0.7282964448872129]
	TIME [epoch: 3.73 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6354853879548708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6354853879548708 | validation: 0.6336475890434671]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_316.pth
	Model improved!!!
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6089751055124168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6089751055124168 | validation: 0.6376828435356696]
	TIME [epoch: 3.74 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5997220122469322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5997220122469322 | validation: 0.6530903691306946]
	TIME [epoch: 3.73 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5823747050303577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5823747050303577 | validation: 0.6350385662920823]
	TIME [epoch: 3.71 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.566811684352461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.566811684352461 | validation: 0.6728311288365545]
	TIME [epoch: 3.71 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5662773650669987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5662773650669987 | validation: 0.7556305341632231]
	TIME [epoch: 3.72 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7119238773780947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7119238773780947 | validation: 1.2928762395975122]
	TIME [epoch: 3.72 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9522393446434829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9522393446434829 | validation: 1.3227856884425848]
	TIME [epoch: 3.73 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7752121010782176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7752121010782176 | validation: 0.8635968980175781]
	TIME [epoch: 3.72 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6838664800526074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6838664800526074 | validation: 0.6942506194363597]
	TIME [epoch: 3.71 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6483360053086762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6483360053086762 | validation: 0.6459604691613157]
	TIME [epoch: 3.72 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5657530866490267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5657530866490267 | validation: 0.6133487298274068]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_327.pth
	Model improved!!!
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5625044884190288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5625044884190288 | validation: 0.6258769945833762]
	TIME [epoch: 3.73 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5596806863214282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5596806863214282 | validation: 0.6757211924440568]
	TIME [epoch: 3.74 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6179928894606893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6179928894606893 | validation: 1.1639584999204222]
	TIME [epoch: 3.73 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7085732746785991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7085732746785991 | validation: 1.1627952712256244]
	TIME [epoch: 3.72 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6489075982097374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6489075982097374 | validation: 0.7451294828004456]
	TIME [epoch: 3.72 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5775798014016732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5775798014016732 | validation: 0.5659698037849641]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_333.pth
	Model improved!!!
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6013432593024026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6013432593024026 | validation: 0.9210407841584685]
	TIME [epoch: 3.72 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.592543488237639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.592543488237639 | validation: 0.8892129573866836]
	TIME [epoch: 3.72 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.592320978929798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.592320978929798 | validation: 0.524776207742123]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_336.pth
	Model improved!!!
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47584571393737696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47584571393737696 | validation: 0.5629599362986901]
	TIME [epoch: 3.73 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46090771811274456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46090771811274456 | validation: 0.6371831391645213]
	TIME [epoch: 3.72 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47404638092926077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47404638092926077 | validation: 0.8585925121626531]
	TIME [epoch: 3.73 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5759644857873732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5759644857873732 | validation: 0.5347201470622054]
	TIME [epoch: 3.75 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.452191014602959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.452191014602959 | validation: 0.9046274846044018]
	TIME [epoch: 3.73 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4982243226483042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4982243226483042 | validation: 0.5485495729364834]
	TIME [epoch: 3.72 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41727290819461327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41727290819461327 | validation: 0.9133113406494702]
	TIME [epoch: 3.73 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5609710645733139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5609710645733139 | validation: 0.6406926680850442]
	TIME [epoch: 3.72 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3975711822664739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3975711822664739 | validation: 0.8087596373200934]
	TIME [epoch: 3.72 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6944422647060827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6944422647060827 | validation: 1.2127244902708725]
	TIME [epoch: 3.72 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8530648325162287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8530648325162287 | validation: 0.9288115533514211]
	TIME [epoch: 3.72 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6864152337461016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6864152337461016 | validation: 0.7222797137420307]
	TIME [epoch: 3.72 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5817362438598395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5817362438598395 | validation: 0.6208394547341516]
	TIME [epoch: 3.72 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4712339734520167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4712339734520167 | validation: 0.6189597013328586]
	TIME [epoch: 3.72 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42950793549679217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42950793549679217 | validation: 0.5298659724544436]
	TIME [epoch: 3.76 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4119534521610618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4119534521610618 | validation: 0.5743285673984435]
	TIME [epoch: 3.74 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37981047200492785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37981047200492785 | validation: 0.45282794511698043]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_353.pth
	Model improved!!!
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3763451919678171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3763451919678171 | validation: 0.8280781564108843]
	TIME [epoch: 3.72 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45365857866467824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45365857866467824 | validation: 0.47329416722994877]
	TIME [epoch: 3.72 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5854494482276434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5854494482276434 | validation: 0.606530742045753]
	TIME [epoch: 3.72 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3376838762560097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3376838762560097 | validation: 0.5519253255156219]
	TIME [epoch: 3.72 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3149913833673297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3149913833673297 | validation: 0.44572493294237053]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_358.pth
	Model improved!!!
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33241270050755845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33241270050755845 | validation: 0.5948974717586418]
	TIME [epoch: 3.74 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3303391748260847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3303391748260847 | validation: 0.40276468177777236]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_360.pth
	Model improved!!!
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36076332477316775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36076332477316775 | validation: 0.7703960826170068]
	TIME [epoch: 3.75 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3801448641411724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3801448641411724 | validation: 0.8358788463792053]
	TIME [epoch: 3.76 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8152895415049111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8152895415049111 | validation: 0.930177435282534]
	TIME [epoch: 3.74 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6065332068668825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6065332068668825 | validation: 0.7534149292994553]
	TIME [epoch: 3.73 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43577516425522594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43577516425522594 | validation: 0.5458420355911134]
	TIME [epoch: 3.74 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32915943223429833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32915943223429833 | validation: 0.5203313257271561]
	TIME [epoch: 3.74 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3220712847959802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3220712847959802 | validation: 0.535660957802401]
	TIME [epoch: 3.73 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30387499894112396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30387499894112396 | validation: 0.48880520831589663]
	TIME [epoch: 3.73 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.275159440626302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.275159440626302 | validation: 0.45626792084948375]
	TIME [epoch: 3.73 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25983162963882367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25983162963882367 | validation: 0.621018843954248]
	TIME [epoch: 3.73 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.312038514847848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.312038514847848 | validation: 0.42082677515165057]
	TIME [epoch: 3.73 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5558654357804236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5558654357804236 | validation: 0.643968624136557]
	TIME [epoch: 3.73 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28310858510608217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28310858510608217 | validation: 0.4492319208279225]
	TIME [epoch: 3.74 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2822144963113304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2822144963113304 | validation: 0.6558140580747683]
	TIME [epoch: 3.76 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31377487705215595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31377487705215595 | validation: 0.4956831546257201]
	TIME [epoch: 3.74 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3040589957557064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3040589957557064 | validation: 0.5957635996818269]
	TIME [epoch: 3.73 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.362990376346547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.362990376346547 | validation: 0.44736486156408745]
	TIME [epoch: 3.73 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24677507404604804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24677507404604804 | validation: 0.5349853483157322]
	TIME [epoch: 3.74 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23431951257664746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23431951257664746 | validation: 0.388741529559012]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_379.pth
	Model improved!!!
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32427028078587805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32427028078587805 | validation: 0.9316045489798171]
	TIME [epoch: 3.74 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5610381418713634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5610381418713634 | validation: 0.5511532676330352]
	TIME [epoch: 3.73 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4511439554615957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4511439554615957 | validation: 0.69692551953644]
	TIME [epoch: 3.73 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4348674593203966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4348674593203966 | validation: 0.46346380333437676]
	TIME [epoch: 3.74 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2527897364193023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2527897364193023 | validation: 0.5670689845060592]
	TIME [epoch: 3.74 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36726495358916095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36726495358916095 | validation: 0.5495639324402329]
	TIME [epoch: 3.75 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29057794550932253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29057794550932253 | validation: 0.4402831689153713]
	TIME [epoch: 3.75 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2366780078831056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2366780078831056 | validation: 0.4898040063464773]
	TIME [epoch: 3.74 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27001600227864986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27001600227864986 | validation: 0.48304247849018833]
	TIME [epoch: 3.73 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2382571314433718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2382571314433718 | validation: 0.5084232283928162]
	TIME [epoch: 3.73 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22906050871125175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22906050871125175 | validation: 0.4080892036581878]
	TIME [epoch: 3.73 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2599123526580888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2599123526580888 | validation: 0.7902639136798926]
	TIME [epoch: 3.73 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37079427095376305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37079427095376305 | validation: 0.5359297052423954]
	TIME [epoch: 3.73 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5115715998564803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5115715998564803 | validation: 0.6511663038198138]
	TIME [epoch: 3.73 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2825046594046653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2825046594046653 | validation: 0.5451528614378663]
	TIME [epoch: 3.73 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28503898319885984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28503898319885984 | validation: 0.4265203276868089]
	TIME [epoch: 3.74 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2852599025158059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2852599025158059 | validation: 0.5104025307384462]
	TIME [epoch: 3.75 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22624807858398666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22624807858398666 | validation: 0.4457239371855821]
	TIME [epoch: 3.75 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21992391471838815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21992391471838815 | validation: 0.5080411620581606]
	TIME [epoch: 3.74 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21300095723227983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21300095723227983 | validation: 0.42253189434010896]
	TIME [epoch: 3.74 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20101646289816102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20101646289816102 | validation: 0.6806520685521935]
	TIME [epoch: 3.74 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2791205301291343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2791205301291343 | validation: 1.024907982700902]
	TIME [epoch: 3.73 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9790679777257745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9790679777257745 | validation: 0.45624476551420123]
	TIME [epoch: 3.73 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4751698362057307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4751698362057307 | validation: 0.8804630426707111]
	TIME [epoch: 3.74 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4373164183253397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4373164183253397 | validation: 0.522043695718693]
	TIME [epoch: 3.73 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2368244972456357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2368244972456357 | validation: 0.45020805581108614]
	TIME [epoch: 3.73 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30306327992409293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30306327992409293 | validation: 0.4545738949453109]
	TIME [epoch: 3.73 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2239218272217714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2239218272217714 | validation: 0.5622691102918459]
	TIME [epoch: 3.75 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23924998482904433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23924998482904433 | validation: 0.42572532998968476]
	TIME [epoch: 3.76 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21870353794355496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21870353794355496 | validation: 0.48550939071699384]
	TIME [epoch: 3.74 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23048063606406488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23048063606406488 | validation: 0.43522967318946276]
	TIME [epoch: 3.73 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26569457282130005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26569457282130005 | validation: 0.6538007854745183]
	TIME [epoch: 3.73 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3214251427523762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3214251427523762 | validation: 0.40995744455437716]
	TIME [epoch: 3.73 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4425482829144558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4425482829144558 | validation: 0.4645396160020392]
	TIME [epoch: 3.73 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27682805569487257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27682805569487257 | validation: 0.5730659919089472]
	TIME [epoch: 3.73 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3118425364397763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3118425364397763 | validation: 0.4404015658339683]
	TIME [epoch: 3.74 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1951230448436534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1951230448436534 | validation: 0.4225085542332103]
	TIME [epoch: 3.73 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2114099508367477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2114099508367477 | validation: 0.47674866967059315]
	TIME [epoch: 3.73 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19953370998868233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19953370998868233 | validation: 0.41140400610828576]
	TIME [epoch: 3.74 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24648421906388138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24648421906388138 | validation: 0.717651275307631]
	TIME [epoch: 3.76 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3224491098170219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3224491098170219 | validation: 0.9059311619189032]
	TIME [epoch: 3.74 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9552889703075964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9552889703075964 | validation: 0.4912278787634475]
	TIME [epoch: 3.73 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4182235983967448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4182235983967448 | validation: 0.7251891161752089]
	TIME [epoch: 3.73 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34144596208783057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34144596208783057 | validation: 0.6173341442354301]
	TIME [epoch: 3.73 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25205870364802097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25205870364802097 | validation: 0.39795826695610453]
	TIME [epoch: 3.73 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23004744783808354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23004744783808354 | validation: 0.4372998400337862]
	TIME [epoch: 3.74 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23060650883480618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23060650883480618 | validation: 0.44108913476763745]
	TIME [epoch: 3.73 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2023615096081283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2023615096081283 | validation: 0.480945864146144]
	TIME [epoch: 3.73 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19760279161061053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19760279161061053 | validation: 0.44675307800334246]
	TIME [epoch: 3.73 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19440519479240656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19440519479240656 | validation: 0.4576079889434029]
	TIME [epoch: 3.73 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20251900794330394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20251900794330394 | validation: 0.44867102789490415]
	TIME [epoch: 3.75 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18785651764140832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18785651764140832 | validation: 0.49338260838977527]
	TIME [epoch: 3.77 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21241007953298138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21241007953298138 | validation: 0.4313308549115391]
	TIME [epoch: 3.73 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35639706977244656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35639706977244656 | validation: 0.6209786839223854]
	TIME [epoch: 3.73 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30258563556060936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30258563556060936 | validation: 0.3795773153194988]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_434.pth
	Model improved!!!
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2357774792434554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2357774792434554 | validation: 0.48285577079766107]
	TIME [epoch: 3.71 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1810957530164121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1810957530164121 | validation: 0.46138099572415414]
	TIME [epoch: 3.7 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1757852381855877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1757852381855877 | validation: 0.38417218412886517]
	TIME [epoch: 3.7 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1927371966999965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1927371966999965 | validation: 0.5592773549901526]
	TIME [epoch: 3.7 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2075260703958833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2075260703958833 | validation: 0.36909549220998383]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_439.pth
	Model improved!!!
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1808754161564223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1808754161564223 | validation: 0.5381991256133558]
	TIME [epoch: 3.73 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19682019642969695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19682019642969695 | validation: 0.3932508267949779]
	TIME [epoch: 3.73 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23429794195578194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23429794195578194 | validation: 0.524775686142408]
	TIME [epoch: 3.74 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27416336893116156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27416336893116156 | validation: 0.6238836067196896]
	TIME [epoch: 3.73 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27935080048210376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27935080048210376 | validation: 0.4027386203967144]
	TIME [epoch: 3.72 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16937987719207687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16937987719207687 | validation: 0.43555850847673505]
	TIME [epoch: 3.72 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15770206863363392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15770206863363392 | validation: 0.38762208934669534]
	TIME [epoch: 3.71 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17242025836335764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17242025836335764 | validation: 0.5666724416113242]
	TIME [epoch: 3.72 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21506155413744085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21506155413744085 | validation: 0.45479750632576954]
	TIME [epoch: 3.72 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33651154223975577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33651154223975577 | validation: 0.7668689977473704]
	TIME [epoch: 3.73 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36395985775145234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36395985775145234 | validation: 0.46719100468079855]
	TIME [epoch: 3.71 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21769360439568908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21769360439568908 | validation: 0.5657322417145999]
	TIME [epoch: 3.72 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3485535843241632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3485535843241632 | validation: 0.48346675384865456]
	TIME [epoch: 3.72 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2718865740612748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2718865740612748 | validation: 0.5678735674012234]
	TIME [epoch: 3.75 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2107690015167492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2107690015167492 | validation: 0.37520176982597364]
	TIME [epoch: 3.7 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18846044450596824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18846044450596824 | validation: 0.4855155982123419]
	TIME [epoch: 3.71 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16758219286635892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16758219286635892 | validation: 0.3958834435174362]
	TIME [epoch: 3.72 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17056693371216658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17056693371216658 | validation: 0.4841161290031126]
	TIME [epoch: 3.73 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1974421604081066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1974421604081066 | validation: 0.3702292180899752]
	TIME [epoch: 3.73 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2727311155614855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2727311155614855 | validation: 0.6131515575377343]
	TIME [epoch: 3.73 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27423985397176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27423985397176 | validation: 0.38159204180577644]
	TIME [epoch: 3.71 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16900400721820982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16900400721820982 | validation: 0.521413570878131]
	TIME [epoch: 3.71 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.196390135964893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.196390135964893 | validation: 0.4134418954302865]
	TIME [epoch: 3.71 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17319317098500028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17319317098500028 | validation: 0.41499664899409017]
	TIME [epoch: 3.71 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1673896330240403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1673896330240403 | validation: 0.45237009961377317]
	TIME [epoch: 3.72 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17906479466981623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17906479466981623 | validation: 0.41103852263692037]
	TIME [epoch: 3.75 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17668716145606006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17668716145606006 | validation: 0.4384644331479831]
	TIME [epoch: 3.72 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18259958113729893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18259958113729893 | validation: 0.453923132468152]
	TIME [epoch: 3.72 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17252514296755428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17252514296755428 | validation: 0.407838302868889]
	TIME [epoch: 3.71 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21613771408469723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21613771408469723 | validation: 0.8293987919867013]
	TIME [epoch: 3.73 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4231915224225413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4231915224225413 | validation: 0.6477574930820702]
	TIME [epoch: 3.72 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2638037002050498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2638037002050498 | validation: 1.1325377498104148]
	TIME [epoch: 3.73 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3793079951216425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3793079951216425 | validation: 1.1571727480429281]
	TIME [epoch: 3.73 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3748922476757865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3748922476757865 | validation: 0.8972799131606423]
	TIME [epoch: 3.74 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0454416005776035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0454416005776035 | validation: 0.47651691968307414]
	TIME [epoch: 3.72 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3928916508928137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3928916508928137 | validation: 0.594161646976862]
	TIME [epoch: 3.73 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27091660437169696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27091660437169696 | validation: 0.6482555800271895]
	TIME [epoch: 3.74 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28120831180024186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28120831180024186 | validation: 0.5425461155064837]
	TIME [epoch: 3.73 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22390005954240785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22390005954240785 | validation: 0.45549517533164574]
	TIME [epoch: 3.72 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20602786170537468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20602786170537468 | validation: 0.44199774531623925]
	TIME [epoch: 3.73 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19688890714328197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19688890714328197 | validation: 0.4491775894488137]
	TIME [epoch: 3.73 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20800703731176787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20800703731176787 | validation: 0.5903326832720158]
	TIME [epoch: 3.73 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27130714151816937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27130714151816937 | validation: 0.4494054037986491]
	TIME [epoch: 3.72 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2170904442056499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2170904442056499 | validation: 0.4570896477709053]
	TIME [epoch: 3.73 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.175063503277807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.175063503277807 | validation: 0.41806675859754333]
	TIME [epoch: 3.73 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16171826934003997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16171826934003997 | validation: 0.43782381986218]
	TIME [epoch: 3.73 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1592427424595689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1592427424595689 | validation: 0.40291990877982714]
	TIME [epoch: 3.7 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15500842072121862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15500842072121862 | validation: 0.4588362993249481]
	TIME [epoch: 3.73 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16322351475535435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16322351475535435 | validation: 0.3842066537274018]
	TIME [epoch: 3.73 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20662534224871362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20662534224871362 | validation: 0.5896319279198935]
	TIME [epoch: 3.73 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2908307867401899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2908307867401899 | validation: 0.4134704363170368]
	TIME [epoch: 3.71 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1731152298747366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1731152298747366 | validation: 0.41431872658914026]
	TIME [epoch: 3.72 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15070655787695797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15070655787695797 | validation: 0.4450586161775809]
	TIME [epoch: 3.73 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15617053676995174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15617053676995174 | validation: 0.37214761570221716]
	TIME [epoch: 3.72 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15075133300905968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15075133300905968 | validation: 0.4447618444632593]
	TIME [epoch: 3.72 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1553735881191186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1553735881191186 | validation: 0.3971687400719652]
	TIME [epoch: 3.71 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1711807170332196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1711807170332196 | validation: 0.5231913963123062]
	TIME [epoch: 3.72 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2037511532304252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2037511532304252 | validation: 0.46317877928179957]
	TIME [epoch: 3.72 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21884892485735072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21884892485735072 | validation: 0.4215732936813288]
	TIME [epoch: 3.73 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2233233229542851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2233233229542851 | validation: 0.5950125363782391]
	TIME [epoch: 3.73 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24347475753369566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24347475753369566 | validation: 0.3332264103225631]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_500.pth
	Model improved!!!
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1966360495197533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1966360495197533 | validation: 0.4624345585015573]
	TIME [epoch: 50.9 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.174340584683474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.174340584683474 | validation: 0.4219241509686321]
	TIME [epoch: 8.12 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13544399112569483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13544399112569483 | validation: 0.3228012574033879]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_503.pth
	Model improved!!!
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1513932730162314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1513932730162314 | validation: 0.5285913447366831]
	TIME [epoch: 8.08 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16850881444757995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16850881444757995 | validation: 0.33789703242134234]
	TIME [epoch: 8.08 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14891213723173902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14891213723173902 | validation: 0.46921329215892416]
	TIME [epoch: 8.08 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1587686721895571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1587686721895571 | validation: 0.3899864372210613]
	TIME [epoch: 8.1 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1552293650194638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1552293650194638 | validation: 0.3979412336345092]
	TIME [epoch: 8.1 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17289148914276928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17289148914276928 | validation: 0.5289757464188194]
	TIME [epoch: 8.11 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2358425333322954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2358425333322954 | validation: 0.43998505136801525]
	TIME [epoch: 8.09 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21388953774422828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21388953774422828 | validation: 0.5244086479133235]
	TIME [epoch: 8.08 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22483859258813532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22483859258813532 | validation: 0.7607384629413518]
	TIME [epoch: 8.09 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7444975813964999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7444975813964999 | validation: 0.39181268006137226]
	TIME [epoch: 8.12 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22676113975228332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22676113975228332 | validation: 0.5868965605864448]
	TIME [epoch: 8.09 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20387795604201045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20387795604201045 | validation: 0.5828298140931668]
	TIME [epoch: 8.08 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20042394984512132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20042394984512132 | validation: 0.3815877147610936]
	TIME [epoch: 8.08 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1448475099740276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1448475099740276 | validation: 0.3448274579948035]
	TIME [epoch: 8.09 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15096644152569355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15096644152569355 | validation: 0.3602615294003177]
	TIME [epoch: 8.1 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.128715591462698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.128715591462698 | validation: 0.3882276071065348]
	TIME [epoch: 8.12 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12410482322959177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12410482322959177 | validation: 0.40467070287668716]
	TIME [epoch: 8.08 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1274374450808437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1274374450808437 | validation: 0.29868860948185]
	TIME [epoch: 8.08 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_521.pth
	Model improved!!!
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2799022203067372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2799022203067372 | validation: 0.7264953604471975]
	TIME [epoch: 8.09 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5290086478592481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5290086478592481 | validation: 0.6727937460404423]
	TIME [epoch: 8.08 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39626993336345295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39626993336345295 | validation: 0.406988524315348]
	TIME [epoch: 8.11 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1736505468466792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1736505468466792 | validation: 0.37360275648286967]
	TIME [epoch: 8.1 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20231523112840932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20231523112840932 | validation: 0.4105307833828022]
	TIME [epoch: 8.08 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16333714815483183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16333714815483183 | validation: 0.37361707138424916]
	TIME [epoch: 8.08 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14732523716284418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14732523716284418 | validation: 0.3612963059366075]
	TIME [epoch: 8.08 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13202900384608776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13202900384608776 | validation: 0.3578189916258083]
	TIME [epoch: 8.09 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1261348930110159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1261348930110159 | validation: 0.36602019858361934]
	TIME [epoch: 8.12 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12342687648584971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12342687648584971 | validation: 0.35000403836802735]
	TIME [epoch: 8.08 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12428634310078768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12428634310078768 | validation: 0.3954704801034934]
	TIME [epoch: 8.08 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1359561591316262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1359561591316262 | validation: 0.3665521280239019]
	TIME [epoch: 8.08 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18947832095226452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18947832095226452 | validation: 0.5006640935304681]
	TIME [epoch: 8.09 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27457193337938035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27457193337938035 | validation: 0.3691130004935896]
	TIME [epoch: 8.1 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21935170381968214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21935170381968214 | validation: 0.4090603169994476]
	TIME [epoch: 8.12 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1268359654335193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1268359654335193 | validation: 0.323634810717716]
	TIME [epoch: 8.09 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1232480249251816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1232480249251816 | validation: 0.42476340970835463]
	TIME [epoch: 8.09 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1343465974084876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1343465974084876 | validation: 0.3070550574548881]
	TIME [epoch: 8.08 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1425711667065407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1425711667065407 | validation: 0.6134915587564358]
	TIME [epoch: 8.09 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19073505731820597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19073505731820597 | validation: 0.4884688958037955]
	TIME [epoch: 8.1 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46215012597678323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46215012597678323 | validation: 0.34214491420697624]
	TIME [epoch: 8.11 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13117935680250206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13117935680250206 | validation: 0.5817859857979673]
	TIME [epoch: 8.1 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1892857048792524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1892857048792524 | validation: 0.3953535496040104]
	TIME [epoch: 8.09 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11678953541381343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11678953541381343 | validation: 0.3111859323813135]
	TIME [epoch: 8.09 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12674218563382378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12674218563382378 | validation: 0.36045574298659927]
	TIME [epoch: 8.09 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11252839850531458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11252839850531458 | validation: 0.35118750555038186]
	TIME [epoch: 8.11 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10701930821600467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10701930821600467 | validation: 0.32820562855059426]
	TIME [epoch: 8.1 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10667225956130892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10667225956130892 | validation: 0.3166364401817585]
	TIME [epoch: 8.09 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11981989570986029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11981989570986029 | validation: 0.5723576315592348]
	TIME [epoch: 8.08 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3340194608875274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3340194608875274 | validation: 0.33605083990702306]
	TIME [epoch: 8.1 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21241741541129205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21241741541129205 | validation: 0.4764102478157433]
	TIME [epoch: 8.1 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2236425588202512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2236425588202512 | validation: 0.3891831177935833]
	TIME [epoch: 8.13 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16761709785990925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16761709785990925 | validation: 0.2999395665541928]
	TIME [epoch: 8.11 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14186683408922698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14186683408922698 | validation: 0.3751326657497023]
	TIME [epoch: 8.09 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15369635185209482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15369635185209482 | validation: 0.3516513361622995]
	TIME [epoch: 8.09 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13501186511232002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13501186511232002 | validation: 0.30512275500968156]
	TIME [epoch: 8.09 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11027860105837398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11027860105837398 | validation: 0.3475383686494]
	TIME [epoch: 8.12 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09983931962401175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09983931962401175 | validation: 0.3006009717057101]
	TIME [epoch: 8.12 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09824909343047061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09824909343047061 | validation: 0.3456876933915955]
	TIME [epoch: 8.1 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10146850389985022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10146850389985022 | validation: 0.2551907855260251]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_561.pth
	Model improved!!!
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1422146018664422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1422146018664422 | validation: 0.7642691642481918]
	TIME [epoch: 8.09 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34338159727390843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34338159727390843 | validation: 0.2922105106591799]
	TIME [epoch: 8.09 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14734102176349143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14734102176349143 | validation: 0.3017954129467595]
	TIME [epoch: 8.11 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10637184281187544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10637184281187544 | validation: 0.4050037922386574]
	TIME [epoch: 8.1 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1275879885736906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1275879885736906 | validation: 0.2666589799568626]
	TIME [epoch: 8.1 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11842717341788817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11842717341788817 | validation: 0.3139655372201535]
	TIME [epoch: 8.09 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12683733024170235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12683733024170235 | validation: 0.48614467103166625]
	TIME [epoch: 8.09 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1652637213842051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1652637213842051 | validation: 0.35841372642770253]
	TIME [epoch: 8.1 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1875107286999208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1875107286999208 | validation: 0.6056318388398305]
	TIME [epoch: 8.13 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25876358829206736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25876358829206736 | validation: 0.3782288084435812]
	TIME [epoch: 8.09 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1540784827261378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1540784827261378 | validation: 0.2725910154282451]
	TIME [epoch: 8.09 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15489230475558624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15489230475558624 | validation: 0.43124932816543105]
	TIME [epoch: 8.1 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17422282241130585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17422282241130585 | validation: 0.29615835697303433]
	TIME [epoch: 8.1 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10536097396614427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10536097396614427 | validation: 0.2941239974580583]
	TIME [epoch: 8.1 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09850026644489107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09850026644489107 | validation: 0.35912474023950425]
	TIME [epoch: 8.12 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11123811860472102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11123811860472102 | validation: 0.2976980927678386]
	TIME [epoch: 8.09 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12906237284593128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12906237284593128 | validation: 0.35788363843318766]
	TIME [epoch: 8.08 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1743881511283109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1743881511283109 | validation: 0.3380371481148559]
	TIME [epoch: 8.08 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16034524146105056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16034524146105056 | validation: 0.31447173246926446]
	TIME [epoch: 8.08 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1306964263289795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1306964263289795 | validation: 0.35455034429810073]
	TIME [epoch: 8.1 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11775321778886234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11775321778886234 | validation: 0.2850419162384628]
	TIME [epoch: 8.11 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11226745502507704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11226745502507704 | validation: 0.4299501649469079]
	TIME [epoch: 8.09 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12299220208948201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12299220208948201 | validation: 0.2951994694894281]
	TIME [epoch: 8.08 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13922652496748225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13922652496748225 | validation: 0.4648252424771297]
	TIME [epoch: 8.09 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17245059556339462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17245059556339462 | validation: 0.3124880543643662]
	TIME [epoch: 8.09 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1380161925437197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1380161925437197 | validation: 0.33816246984999326]
	TIME [epoch: 8.11 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1179304895975799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1179304895975799 | validation: 0.35970318797642503]
	TIME [epoch: 8.1 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1264840653718205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1264840653718205 | validation: 0.28569987017175685]
	TIME [epoch: 8.08 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.183190154037591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.183190154037591 | validation: 0.635320023626718]
	TIME [epoch: 8.09 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22655584257958114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22655584257958114 | validation: 0.28695836895158794]
	TIME [epoch: 8.08 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10482119405417743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10482119405417743 | validation: 0.26211120784362246]
	TIME [epoch: 8.09 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11538728912096913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11538728912096913 | validation: 0.4296690501246908]
	TIME [epoch: 8.11 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11705077115151127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11705077115151127 | validation: 0.3031645177817404]
	TIME [epoch: 8.09 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09204594022378892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09204594022378892 | validation: 0.27159205345948356]
	TIME [epoch: 8.09 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10337351765724902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10337351765724902 | validation: 0.3349753029627909]
	TIME [epoch: 8.1 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10480096853676088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10480096853676088 | validation: 0.32862643789011947]
	TIME [epoch: 8.09 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1313631981452012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1313631981452012 | validation: 0.34119538514586767]
	TIME [epoch: 8.11 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19206794128219012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19206794128219012 | validation: 0.5226416189419193]
	TIME [epoch: 8.1 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2525301498382814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2525301498382814 | validation: 0.3478416784805065]
	TIME [epoch: 8.09 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15268969290708925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15268969290708925 | validation: 0.2543398073486752]
	TIME [epoch: 8.07 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_601.pth
	Model improved!!!
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11798688911076392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11798688911076392 | validation: 0.45260415562132983]
	TIME [epoch: 8.06 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12068880361708816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12068880361708816 | validation: 0.26370774138672043]
	TIME [epoch: 8.06 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08836715683642848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08836715683642848 | validation: 0.2877761550410452]
	TIME [epoch: 8.09 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08471923571635963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08471923571635963 | validation: 0.2801583747748089]
	TIME [epoch: 8.07 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08582833825972142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08582833825972142 | validation: 0.23251269417902853]
	TIME [epoch: 8.05 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_606.pth
	Model improved!!!
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15148598478587616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15148598478587616 | validation: 0.605194554700256]
	TIME [epoch: 8.05 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3652439767830803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3652439767830803 | validation: 0.31936736562221735]
	TIME [epoch: 8.06 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11260970725571792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11260970725571792 | validation: 0.22707538895607599]
	TIME [epoch: 8.05 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_609.pth
	Model improved!!!
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13548989264408182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13548989264408182 | validation: 0.4836616203054012]
	TIME [epoch: 8.11 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14446184989821548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14446184989821548 | validation: 0.24653856891481474]
	TIME [epoch: 8.08 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10275984127143904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10275984127143904 | validation: 0.30961819837987875]
	TIME [epoch: 8.08 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11212535032037049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11212535032037049 | validation: 0.25949947561880954]
	TIME [epoch: 8.08 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11387219603901866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11387219603901866 | validation: 0.3914328877846199]
	TIME [epoch: 8.08 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12279799740510434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12279799740510434 | validation: 0.2722163512347723]
	TIME [epoch: 8.1 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11374778858345444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11374778858345444 | validation: 0.31705639432873184]
	TIME [epoch: 8.09 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10759248107556511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10759248107556511 | validation: 0.257821257051657]
	TIME [epoch: 8.08 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10833737981802376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10833737981802376 | validation: 0.36808937418156384]
	TIME [epoch: 8.08 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10139809315421525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10139809315421525 | validation: 0.24157998642449607]
	TIME [epoch: 8.08 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10064897153891052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10064897153891052 | validation: 0.4869732451411868]
	TIME [epoch: 8.07 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13815218488768488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13815218488768488 | validation: 0.3377304281610285]
	TIME [epoch: 8.13 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14788948679725508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14788948679725508 | validation: 0.24628957724599246]
	TIME [epoch: 8.07 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18341426815291492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18341426815291492 | validation: 0.5718277600115655]
	TIME [epoch: 8.1 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21722770704058866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21722770704058866 | validation: 0.3561148060086627]
	TIME [epoch: 8.08 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1293365166256613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1293365166256613 | validation: 0.22083278209180657]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_625.pth
	Model improved!!!
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11185356367772141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11185356367772141 | validation: 0.28440930580148277]
	TIME [epoch: 8.1 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07715756917794366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07715756917794366 | validation: 0.2706817819261808]
	TIME [epoch: 8.11 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08625199989173812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08625199989173812 | validation: 0.24639168860260555]
	TIME [epoch: 8.05 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08217755447054562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08217755447054562 | validation: 0.22988766020273244]
	TIME [epoch: 8.05 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07750298811299501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07750298811299501 | validation: 0.2899250008644411]
	TIME [epoch: 8.05 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08636975723279555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08636975723279555 | validation: 0.22384395107048605]
	TIME [epoch: 8.05 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1074767685050706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1074767685050706 | validation: 0.5940359402637213]
	TIME [epoch: 8.07 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1670167900342161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1670167900342161 | validation: 0.22963973446866062]
	TIME [epoch: 8.1 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10901010455237167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10901010455237167 | validation: 0.29162393403953873]
	TIME [epoch: 8.09 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2613194034825503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2613194034825503 | validation: 0.4142684725510245]
	TIME [epoch: 8.09 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28742284702172116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28742284702172116 | validation: 0.34346549937861887]
	TIME [epoch: 8.08 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13087399029395314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13087399029395314 | validation: 0.23074964053388342]
	TIME [epoch: 8.08 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11460874391645078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11460874391645078 | validation: 0.23761963399638822]
	TIME [epoch: 8.1 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08801125984418567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08801125984418567 | validation: 0.2558010895794842]
	TIME [epoch: 8.08 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07966446165396798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07966446165396798 | validation: 0.24545853792988753]
	TIME [epoch: 8.07 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07298241335952751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07298241335952751 | validation: 0.20521213277605238]
	TIME [epoch: 8.07 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_641.pth
	Model improved!!!
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07046839724735264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07046839724735264 | validation: 0.30773678892139644]
	TIME [epoch: 8.08 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07520474967795336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07520474967795336 | validation: 0.20900155005113846]
	TIME [epoch: 8.11 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06966876236574412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06966876236574412 | validation: 0.2768401060938409]
	TIME [epoch: 8.13 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08509160497469924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08509160497469924 | validation: 0.22820554237686982]
	TIME [epoch: 8.1 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12484275632056793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12484275632056793 | validation: 0.4336527392080208]
	TIME [epoch: 8.09 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17296676460031526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17296676460031526 | validation: 0.2749968347978995]
	TIME [epoch: 8.09 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.162333262196957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.162333262196957 | validation: 0.2670631861932209]
	TIME [epoch: 8.1 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15421467422087431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15421467422087431 | validation: 0.6266648959468282]
	TIME [epoch: 8.12 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22533276328016766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22533276328016766 | validation: 0.2504785376557231]
	TIME [epoch: 8.12 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0905045250724406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0905045250724406 | validation: 0.22027263668500782]
	TIME [epoch: 8.09 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10164427477706309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10164427477706309 | validation: 0.28088948444524736]
	TIME [epoch: 8.11 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08004563129120108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08004563129120108 | validation: 0.23928765550673786]
	TIME [epoch: 8.08 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07091498813054446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07091498813054446 | validation: 0.22415080229798107]
	TIME [epoch: 8.09 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07776167472276685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07776167472276685 | validation: 0.2172565235768607]
	TIME [epoch: 8.1 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0845858869495179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0845858869495179 | validation: 0.3318169084854889]
	TIME [epoch: 8.12 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10977687318892752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10977687318892752 | validation: 0.25268415555385904]
	TIME [epoch: 8.1 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13293457774411002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13293457774411002 | validation: 0.5081489537469229]
	TIME [epoch: 8.1 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14879638357963637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14879638357963637 | validation: 0.24564570337299527]
	TIME [epoch: 8.09 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08666562858736594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08666562858736594 | validation: 0.19052701482568923]
	TIME [epoch: 8.12 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_660.pth
	Model improved!!!
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.157623785673685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.157623785673685 | validation: 0.41594718325556307]
	TIME [epoch: 8.12 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20541710961276383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20541710961276383 | validation: 0.3138752088992603]
	TIME [epoch: 8.1 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11177244004480581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11177244004480581 | validation: 0.17700650143595398]
	TIME [epoch: 8.08 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_663.pth
	Model improved!!!
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09403224924490981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09403224924490981 | validation: 0.19727099147702726]
	TIME [epoch: 8.05 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07981392266205987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07981392266205987 | validation: 0.2907058065134526]
	TIME [epoch: 8.05 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07557135744685536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07557135744685536 | validation: 0.20697648869854446]
	TIME [epoch: 8.07 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06505158168593525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06505158168593525 | validation: 0.17220036485860254]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_667.pth
	Model improved!!!
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06577038285080829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06577038285080829 | validation: 0.21629924878441145]
	TIME [epoch: 8.06 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07359193697873033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07359193697873033 | validation: 0.21562162757179057]
	TIME [epoch: 8.05 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10035295686956852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10035295686956852 | validation: 0.2620008831779987]
	TIME [epoch: 8.06 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1606429423023903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1606429423023903 | validation: 0.40276882131366937]
	TIME [epoch: 8.06 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21179471144696294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21179471144696294 | validation: 0.21794706430445476]
	TIME [epoch: 8.09 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1183461662182416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1183461662182416 | validation: 0.2661426691439976]
	TIME [epoch: 8.07 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08031642619991827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08031642619991827 | validation: 0.18935181557048297]
	TIME [epoch: 8.05 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07983278052959461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07983278052959461 | validation: 0.22905415892401593]
	TIME [epoch: 8.04 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06282346263424875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06282346263424875 | validation: 0.21188409273035608]
	TIME [epoch: 8.06 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059220369199712034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059220369199712034 | validation: 0.2225474143513532]
	TIME [epoch: 8.05 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05451240870189054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05451240870189054 | validation: 0.19662238729353662]
	TIME [epoch: 8.08 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054925286839713226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054925286839713226 | validation: 0.21962706176035113]
	TIME [epoch: 8.06 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06271065642670826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06271065642670826 | validation: 0.4033214650767109]
	TIME [epoch: 8.05 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2889875350204198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2889875350204198 | validation: 0.7097319009087942]
	TIME [epoch: 8.06 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2914031491551853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2914031491551853 | validation: 0.5627443519924065]
	TIME [epoch: 8.08 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15702361889020539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15702361889020539 | validation: 0.23480409840758495]
	TIME [epoch: 8.07 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15056736101632834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15056736101632834 | validation: 0.20543277712575372]
	TIME [epoch: 8.07 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14260644986880175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14260644986880175 | validation: 0.27091752580121337]
	TIME [epoch: 8.05 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07753516718054439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07753516718054439 | validation: 0.27084735666065357]
	TIME [epoch: 8.06 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0983750124847161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0983750124847161 | validation: 0.25944798274567155]
	TIME [epoch: 8.08 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.135459734311518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.135459734311518 | validation: 0.3802411468283933]
	TIME [epoch: 8.11 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21539896516706195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21539896516706195 | validation: 0.24456998928580942]
	TIME [epoch: 8.12 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12978645036092834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12978645036092834 | validation: 0.2163867089667474]
	TIME [epoch: 8.09 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05861825908275885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05861825908275885 | validation: 0.19453057841589227]
	TIME [epoch: 8.05 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06659151902744963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06659151902744963 | validation: 0.17829583725533804]
	TIME [epoch: 8.08 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06552831173807933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06552831173807933 | validation: 0.2163569273331774]
	TIME [epoch: 8.08 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06539127601289894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06539127601289894 | validation: 0.17107948991787236]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_694.pth
	Model improved!!!
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06115436889262649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06115436889262649 | validation: 0.1851413294831844]
	TIME [epoch: 8.11 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05700061720725005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05700061720725005 | validation: 0.16463887083115536]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_696.pth
	Model improved!!!
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05976791806372157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05976791806372157 | validation: 0.20644502735916137]
	TIME [epoch: 8.04 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08070027564295566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08070027564295566 | validation: 0.2210876934449081]
	TIME [epoch: 8.05 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.154428215199764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.154428215199764 | validation: 0.24093601399157036]
	TIME [epoch: 8.05 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12126750464863742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12126750464863742 | validation: 0.16584405252110895]
	TIME [epoch: 8.06 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06884315831945044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06884315831945044 | validation: 0.18222457227592392]
	TIME [epoch: 8.11 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060561424429951834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060561424429951834 | validation: 0.17394966244659563]
	TIME [epoch: 8.09 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09075702362830657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09075702362830657 | validation: 0.2842890056385171]
	TIME [epoch: 8.1 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1153418695020545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1153418695020545 | validation: 0.14777750212956328]
	TIME [epoch: 8.07 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_704.pth
	Model improved!!!
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06948094246120527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06948094246120527 | validation: 0.19383240111896632]
	TIME [epoch: 8.08 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056201671974644096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056201671974644096 | validation: 0.14701451999530799]
	TIME [epoch: 8.04 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_706.pth
	Model improved!!!
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0646681915086358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0646681915086358 | validation: 0.20348990694968383]
	TIME [epoch: 8.04 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09868906857881037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09868906857881037 | validation: 0.18034167440131346]
	TIME [epoch: 8.06 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07534341244960324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07534341244960324 | validation: 0.15630333181524916]
	TIME [epoch: 8.04 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0867366588026899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0867366588026899 | validation: 0.3431148375366963]
	TIME [epoch: 8.1 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10654429469731783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10654429469731783 | validation: 0.18513987987532887]
	TIME [epoch: 8.05 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0978417680775051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0978417680775051 | validation: 0.37779529365721537]
	TIME [epoch: 8.06 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10487434676923861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10487434676923861 | validation: 0.15761114080915975]
	TIME [epoch: 8.06 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07498229739030231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07498229739030231 | validation: 0.14552000347432195]
	TIME [epoch: 8.06 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_714.pth
	Model improved!!!
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04613627628945492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04613627628945492 | validation: 0.17188282672551058]
	TIME [epoch: 8.09 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03885381522527811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03885381522527811 | validation: 0.1579822590293825]
	TIME [epoch: 8.08 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051031758732439626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051031758732439626 | validation: 0.15785647774425327]
	TIME [epoch: 8.07 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09740011533475938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09740011533475938 | validation: 0.3376338873878455]
	TIME [epoch: 8.07 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27603409658487754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27603409658487754 | validation: 0.24965098811644626]
	TIME [epoch: 8.07 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13995993567745085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13995993567745085 | validation: 0.14481037192402507]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_720.pth
	Model improved!!!
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10942759899233809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10942759899233809 | validation: 0.18272806985496262]
	TIME [epoch: 8.12 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1000492286050787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1000492286050787 | validation: 0.20894416160199594]
	TIME [epoch: 8.12 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07279289399666002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07279289399666002 | validation: 0.1685348895424114]
	TIME [epoch: 8.1 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06820966756943295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06820966756943295 | validation: 0.13281515059593166]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_724.pth
	Model improved!!!
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06133859803966769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06133859803966769 | validation: 0.25408512743759204]
	TIME [epoch: 8.09 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08678594942112546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08678594942112546 | validation: 0.2291861093386078]
	TIME [epoch: 8.1 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1339740595894395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1339740595894395 | validation: 0.430627754273744]
	TIME [epoch: 8.09 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1096760097316589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1096760097316589 | validation: 0.16164461659994445]
	TIME [epoch: 8.09 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04861464058183583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04861464058183583 | validation: 0.1658537315854267]
	TIME [epoch: 8.08 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04099925389663788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04099925389663788 | validation: 0.1904850393163399]
	TIME [epoch: 8.09 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04685426659258824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04685426659258824 | validation: 0.12393131828927739]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_731.pth
	Model improved!!!
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050905563710862085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050905563710862085 | validation: 0.2803895460931389]
	TIME [epoch: 8.1 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08058676604867009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08058676604867009 | validation: 0.21099034065550565]
	TIME [epoch: 8.09 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13050495810512044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13050495810512044 | validation: 0.38136724776684255]
	TIME [epoch: 8.08 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14858130390522312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14858130390522312 | validation: 0.273823741029395]
	TIME [epoch: 8.09 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07293271592739843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07293271592739843 | validation: 0.12318278566485225]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_736.pth
	Model improved!!!
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05209999322541499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05209999322541499 | validation: 0.14100904275459314]
	TIME [epoch: 8.07 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04356896298389109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04356896298389109 | validation: 0.24761742050772098]
	TIME [epoch: 8.03 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05658250902025966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05658250902025966 | validation: 0.16104687657110306]
	TIME [epoch: 8.04 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05676132742935826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05676132742935826 | validation: 0.15232855193584224]
	TIME [epoch: 8.05 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10132652207777816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10132652207777816 | validation: 0.19159802883327792]
	TIME [epoch: 8.04 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14152141159891193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14152141159891193 | validation: 0.19118448151648892]
	TIME [epoch: 8.07 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07814848084793236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07814848084793236 | validation: 0.1133434404633073]
	TIME [epoch: 8.06 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_743.pth
	Model improved!!!
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05912516910970533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05912516910970533 | validation: 0.14569203896742786]
	TIME [epoch: 8.09 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05611538338904306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05611538338904306 | validation: 0.28242074615548174]
	TIME [epoch: 8.08 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05839966795876086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05839966795876086 | validation: 0.12103483503236673]
	TIME [epoch: 8.08 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04668743689298598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04668743689298598 | validation: 0.20549966221070615]
	TIME [epoch: 8.09 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08615617249815827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08615617249815827 | validation: 0.29685608815986014]
	TIME [epoch: 8.09 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23273757806047435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23273757806047435 | validation: 0.3104796022830951]
	TIME [epoch: 8.09 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17352031576240862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17352031576240862 | validation: 0.17145526964536942]
	TIME [epoch: 8.08 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07730475753155798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07730475753155798 | validation: 0.14052855779051995]
	TIME [epoch: 8.09 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039362303313900886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039362303313900886 | validation: 0.1427925405462922]
	TIME [epoch: 8.09 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053043083961185164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053043083961185164 | validation: 0.1588132280867125]
	TIME [epoch: 8.12 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06439143165258517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06439143165258517 | validation: 0.29964025981542547]
	TIME [epoch: 8.09 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061580363157767375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061580363157767375 | validation: 0.11677707292639006]
	TIME [epoch: 8.09 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04565589467677234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04565589467677234 | validation: 0.14108204209204053]
	TIME [epoch: 8.09 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048799152775874544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048799152775874544 | validation: 0.19287534388172262]
	TIME [epoch: 8.09 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0596502179819805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0596502179819805 | validation: 0.14750430400599823]
	TIME [epoch: 8.12 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06919111611655089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06919111611655089 | validation: 0.10561502584142768]
	TIME [epoch: 8.07 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_759.pth
	Model improved!!!
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05622635952561206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05622635952561206 | validation: 0.17387078512437168]
	TIME [epoch: 8.06 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053320724292619505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053320724292619505 | validation: 0.11393219277476258]
	TIME [epoch: 8.08 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06868765226317597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06868765226317597 | validation: 0.24558266633116924]
	TIME [epoch: 8.07 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13414861440209092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13414861440209092 | validation: 0.20612252975683304]
	TIME [epoch: 8.1 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15259873651335817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15259873651335817 | validation: 0.10932454467457763]
	TIME [epoch: 8.09 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047021204777681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047021204777681 | validation: 0.17568418776635825]
	TIME [epoch: 8.08 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051256029340852494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051256029340852494 | validation: 0.19506229029282024]
	TIME [epoch: 8.08 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09809242725289004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09809242725289004 | validation: 0.42117110743482145]
	TIME [epoch: 8.08 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1100817895200326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1100817895200326 | validation: 0.14495473425935068]
	TIME [epoch: 8.07 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07201319551394716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07201319551394716 | validation: 0.11988727031496652]
	TIME [epoch: 8.09 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05392767093392324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05392767093392324 | validation: 0.16811479746900548]
	TIME [epoch: 8.05 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06793900492302585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06793900492302585 | validation: 0.1642465488257845]
	TIME [epoch: 8.06 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08490096247632019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08490096247632019 | validation: 0.18001241644101396]
	TIME [epoch: 8.05 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09349861071347755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09349861071347755 | validation: 0.1509211261737632]
	TIME [epoch: 8.05 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06697195287176032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06697195287176032 | validation: 0.14640333327210972]
	TIME [epoch: 8.09 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0712625008469948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0712625008469948 | validation: 0.15357185502886095]
	TIME [epoch: 8.09 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04999119048887542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04999119048887542 | validation: 0.11113552452101008]
	TIME [epoch: 8.05 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03627025439804293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03627025439804293 | validation: 0.11152358948205779]
	TIME [epoch: 8.08 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034922889290631386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034922889290631386 | validation: 0.10961818770581991]
	TIME [epoch: 8.06 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04222431866804266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04222431866804266 | validation: 0.19344645013690046]
	TIME [epoch: 8.08 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07624002547743668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07624002547743668 | validation: 0.1616911202657445]
	TIME [epoch: 8.1 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13481856993644825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13481856993644825 | validation: 0.1948707939434503]
	TIME [epoch: 8.07 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12724359880511077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12724359880511077 | validation: 0.18610516247818723]
	TIME [epoch: 8.06 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09475108635827927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09475108635827927 | validation: 0.13502747332503448]
	TIME [epoch: 8.07 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08492337432116492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08492337432116492 | validation: 0.13001266886159854]
	TIME [epoch: 8.07 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07200778395216835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07200778395216835 | validation: 0.13975845696166408]
	TIME [epoch: 8.1 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05768007441663465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05768007441663465 | validation: 0.21187920085092551]
	TIME [epoch: 8.07 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056812205448246715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056812205448246715 | validation: 0.1413783155933991]
	TIME [epoch: 8.07 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0676318730739753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0676318730739753 | validation: 0.23292748885460562]
	TIME [epoch: 8.07 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0747170874866111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0747170874866111 | validation: 0.14145246861011884]
	TIME [epoch: 8.08 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07045277543858217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07045277543858217 | validation: 0.14368746005645136]
	TIME [epoch: 8.1 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0693458462167733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0693458462167733 | validation: 0.14983913459522835]
	TIME [epoch: 8.11 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0707049338816578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0707049338816578 | validation: 0.13677321797613248]
	TIME [epoch: 8.07 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06862830238656549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06862830238656549 | validation: 0.10693192657747222]
	TIME [epoch: 8.07 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06118436507797274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06118436507797274 | validation: 0.17039298971291214]
	TIME [epoch: 8.06 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04066506322559233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04066506322559233 | validation: 0.10045898389612136]
	TIME [epoch: 8.07 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_795.pth
	Model improved!!!
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03649422748340004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03649422748340004 | validation: 0.11191003559752984]
	TIME [epoch: 8.08 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051713527763922205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051713527763922205 | validation: 0.13297014768331797]
	TIME [epoch: 8.05 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09285964307119246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09285964307119246 | validation: 0.15933308905519247]
	TIME [epoch: 8.06 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09864902330280194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09864902330280194 | validation: 0.11962956251205403]
	TIME [epoch: 8.06 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05124726148146878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05124726148146878 | validation: 0.09009110889047957]
	TIME [epoch: 8.06 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_800.pth
	Model improved!!!
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044935146128976235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044935146128976235 | validation: 0.13295779326598872]
	TIME [epoch: 8.14 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05066175996360596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05066175996360596 | validation: 0.12308872174379998]
	TIME [epoch: 8.08 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06856897612034527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06856897612034527 | validation: 0.12862944431829662]
	TIME [epoch: 8.08 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09611540056728571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09611540056728571 | validation: 0.19687206262291035]
	TIME [epoch: 8.08 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15656632070929863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15656632070929863 | validation: 0.2666779721195702]
	TIME [epoch: 8.08 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07754920824983987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07754920824983987 | validation: 0.13380045535061624]
	TIME [epoch: 8.12 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06381218792995483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06381218792995483 | validation: 0.2845280693845562]
	TIME [epoch: 8.1 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07226908889452698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07226908889452698 | validation: 0.12801219663412672]
	TIME [epoch: 8.1 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06530472168485467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06530472168485467 | validation: 0.09969374010235041]
	TIME [epoch: 8.09 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04466307578420437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04466307578420437 | validation: 0.10298502212341226]
	TIME [epoch: 8.09 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03537642565570809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03537642565570809 | validation: 0.08852960038469669]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_811.pth
	Model improved!!!
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03286730031736369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03286730031736369 | validation: 0.10599477821282646]
	TIME [epoch: 8.12 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032272296113468386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032272296113468386 | validation: 0.10892092711612632]
	TIME [epoch: 8.07 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04437466889138162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04437466889138162 | validation: 0.11842837006379442]
	TIME [epoch: 8.07 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06368599919189538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06368599919189538 | validation: 0.11207236899105136]
	TIME [epoch: 8.05 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06531174865222222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06531174865222222 | validation: 0.1821263215017045]
	TIME [epoch: 8.06 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059982082680858126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059982082680858126 | validation: 0.15873138891571614]
	TIME [epoch: 8.07 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07113891198350832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07113891198350832 | validation: 0.17876254526299182]
	TIME [epoch: 8.08 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03733891114246737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03733891114246737 | validation: 0.1257680311557116]
	TIME [epoch: 8.06 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040444779986014194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040444779986014194 | validation: 0.17470949309419065]
	TIME [epoch: 8.04 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11308334608264387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11308334608264387 | validation: 0.2437575013804407]
	TIME [epoch: 8.05 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1957981143740659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1957981143740659 | validation: 0.2417965089823893]
	TIME [epoch: 8.07 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09867784092172016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09867784092172016 | validation: 0.0975287783103092]
	TIME [epoch: 8.09 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04430781100050263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04430781100050263 | validation: 0.10969737394020065]
	TIME [epoch: 8.05 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051575103189120494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051575103189120494 | validation: 0.10174185575885111]
	TIME [epoch: 8.05 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05462056951671085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05462056951671085 | validation: 0.19663358826167904]
	TIME [epoch: 8.06 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06577533725539599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06577533725539599 | validation: 0.12575494801853018]
	TIME [epoch: 8.07 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08138643563081814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08138643563081814 | validation: 0.1511326794816662]
	TIME [epoch: 8.08 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08861981786783428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08861981786783428 | validation: 0.1329586242893734]
	TIME [epoch: 8.05 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07250132453876575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07250132453876575 | validation: 0.1536850873265374]
	TIME [epoch: 8.04 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046336139028575245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046336139028575245 | validation: 0.10424103661561386]
	TIME [epoch: 8.06 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040991604939646556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040991604939646556 | validation: 0.10595519453957576]
	TIME [epoch: 8.07 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0468539827653866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0468539827653866 | validation: 0.1246279372114362]
	TIME [epoch: 8.07 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06752233123208233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06752233123208233 | validation: 0.10497085698395861]
	TIME [epoch: 8.07 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05223096742820685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05223096742820685 | validation: 0.09995294905469891]
	TIME [epoch: 8.04 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041486102345835676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041486102345835676 | validation: 0.09391626228000736]
	TIME [epoch: 8.05 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046926958263149895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046926958263149895 | validation: 0.11757346359449393]
	TIME [epoch: 8.05 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062238109913608196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062238109913608196 | validation: 0.09536039613575106]
	TIME [epoch: 8.05 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05546632276982637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05546632276982637 | validation: 0.09765923877280158]
	TIME [epoch: 8.08 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04374000214370749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04374000214370749 | validation: 0.1093411823430402]
	TIME [epoch: 8.05 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057951737066877076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057951737066877076 | validation: 0.24025465991505782]
	TIME [epoch: 8.04 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0968546007231545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0968546007231545 | validation: 0.15973263022566095]
	TIME [epoch: 8.05 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09758504149514792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09758504149514792 | validation: 0.15824050647258192]
	TIME [epoch: 8.05 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06324743861167033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06324743861167033 | validation: 0.12506812688449095]
	TIME [epoch: 8.07 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06295357007189512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06295357007189512 | validation: 0.1404624760200753]
	TIME [epoch: 8.06 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08127595448721919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08127595448721919 | validation: 0.1619986158099388]
	TIME [epoch: 8.05 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08667390093395567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08667390093395567 | validation: 0.13135182800963066]
	TIME [epoch: 8.05 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03637269318402917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03637269318402917 | validation: 0.09749839310375104]
	TIME [epoch: 8.04 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03434058033011044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03434058033011044 | validation: 0.13809777986387445]
	TIME [epoch: 8.06 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03821655662567601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03821655662567601 | validation: 0.11113159874793183]
	TIME [epoch: 8.06 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05527613252786383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05527613252786383 | validation: 0.16389257038820315]
	TIME [epoch: 8.05 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07111934563238972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07111934563238972 | validation: 0.15286277624305755]
	TIME [epoch: 8.05 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10669688232904098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10669688232904098 | validation: 0.1561221240106574]
	TIME [epoch: 8.04 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08904730644561208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08904730644561208 | validation: 0.11693660926294958]
	TIME [epoch: 8.05 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03834065623895136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03834065623895136 | validation: 0.09621604269009391]
	TIME [epoch: 8.05 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02616338478179805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02616338478179805 | validation: 0.092232350460982]
	TIME [epoch: 8.05 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032601562600527816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032601562600527816 | validation: 0.08493327468167063]
	TIME [epoch: 8.06 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_857.pth
	Model improved!!!
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03030001771588842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03030001771588842 | validation: 0.08354754250374935]
	TIME [epoch: 8.04 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_858.pth
	Model improved!!!
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03985758485611711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03985758485611711 | validation: 0.13023457543199482]
	TIME [epoch: 8.04 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07154892154135845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07154892154135845 | validation: 0.14537316190351762]
	TIME [epoch: 8.05 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10979460628743647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10979460628743647 | validation: 0.29905733288568004]
	TIME [epoch: 8.07 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09906017894018852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09906017894018852 | validation: 0.09610725519536346]
	TIME [epoch: 8.05 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04497492741128364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04497492741128364 | validation: 0.08985922086744434]
	TIME [epoch: 8.05 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04819503058439839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04819503058439839 | validation: 0.11630304317732239]
	TIME [epoch: 8.04 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06476424610581162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06476424610581162 | validation: 0.12959326996196233]
	TIME [epoch: 8.08 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050387871370571595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050387871370571595 | validation: 0.09024968234493895]
	TIME [epoch: 8.07 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04697479666398051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04697479666398051 | validation: 0.11606337679042232]
	TIME [epoch: 8.06 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06347916582661915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06347916582661915 | validation: 0.202272755672121]
	TIME [epoch: 8.04 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09182396670064222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09182396670064222 | validation: 0.14655101072540952]
	TIME [epoch: 8.05 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09567880295743436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09567880295743436 | validation: 0.18766340355901462]
	TIME [epoch: 8.04 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059018214266983185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059018214266983185 | validation: 0.06805547451684019]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_871.pth
	Model improved!!!
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0313729257407075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0313729257407075 | validation: 0.11706237568540755]
	TIME [epoch: 8.1 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03037291838096751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03037291838096751 | validation: 0.10590852577588464]
	TIME [epoch: 8.08 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03556104635033351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03556104635033351 | validation: 0.09541614166262632]
	TIME [epoch: 8.08 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04288456266485464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04288456266485464 | validation: 0.09533455491231263]
	TIME [epoch: 8.07 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049813901715674395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049813901715674395 | validation: 0.10302788510495563]
	TIME [epoch: 8.1 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06331575805494029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06331575805494029 | validation: 0.1000636993387102]
	TIME [epoch: 8.12 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06342741824699603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06342741824699603 | validation: 0.10302499987296394]
	TIME [epoch: 8.06 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07905328282227565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07905328282227565 | validation: 0.1062358465944524]
	TIME [epoch: 8.07 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0563203740729468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0563203740729468 | validation: 0.07769890979030371]
	TIME [epoch: 8.06 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04202968189629591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04202968189629591 | validation: 0.09766315584802819]
	TIME [epoch: 8.15 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04919485079151976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04919485079151976 | validation: 0.1169936324453115]
	TIME [epoch: 8.1 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06698878833347374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06698878833347374 | validation: 0.09069495898979422]
	TIME [epoch: 8.08 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04876249743162818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04876249743162818 | validation: 0.08706992876188217]
	TIME [epoch: 8.07 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03672956211358715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03672956211358715 | validation: 0.07750309316778808]
	TIME [epoch: 8.06 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023050953033155474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023050953033155474 | validation: 0.08516776276129084]
	TIME [epoch: 8.07 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028660216001095966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028660216001095966 | validation: 0.25616385230778216]
	TIME [epoch: 8.1 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06634421805295243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06634421805295243 | validation: 0.19931908705319037]
	TIME [epoch: 8.08 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09464723524182465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09464723524182465 | validation: 0.460874770850149]
	TIME [epoch: 8.07 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12078601540699835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12078601540699835 | validation: 0.18523622097423204]
	TIME [epoch: 8.07 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09812350396472251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09812350396472251 | validation: 0.11870594241660344]
	TIME [epoch: 8.06 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037387326703251525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037387326703251525 | validation: 0.09506596517685068]
	TIME [epoch: 8.09 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03193582417900194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03193582417900194 | validation: 0.10583581180778695]
	TIME [epoch: 8.1 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04381392773194295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04381392773194295 | validation: 0.09393805876964328]
	TIME [epoch: 8.08 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04396422352320451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04396422352320451 | validation: 0.11083503427228059]
	TIME [epoch: 8.07 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060794304571950813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060794304571950813 | validation: 0.14244908541729173]
	TIME [epoch: 8.07 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12738687575625693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12738687575625693 | validation: 0.17774486948008508]
	TIME [epoch: 8.08 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057212417458104015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057212417458104015 | validation: 0.0905826745967419]
	TIME [epoch: 8.1 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06635794837392889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06635794837392889 | validation: 0.13797896465310686]
	TIME [epoch: 8.09 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07506062190801291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07506062190801291 | validation: 0.13664494095199609]
	TIME [epoch: 8.07 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08103340916280054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08103340916280054 | validation: 0.14046565192291222]
	TIME [epoch: 8.06 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040286004006575594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040286004006575594 | validation: 0.08587395261602553]
	TIME [epoch: 8.07 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03345394538704402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03345394538704402 | validation: 0.09948402722541445]
	TIME [epoch: 8.07 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0403794689639945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0403794689639945 | validation: 0.13343358037014316]
	TIME [epoch: 8.1 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042947500720358356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042947500720358356 | validation: 0.09565427846651761]
	TIME [epoch: 8.06 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04062747530501864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04062747530501864 | validation: 0.07936663190256095]
	TIME [epoch: 8.07 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038565831378370455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038565831378370455 | validation: 0.08906263244858047]
	TIME [epoch: 8.06 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03357719822618533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03357719822618533 | validation: 0.08131499608729692]
	TIME [epoch: 8.07 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03994710735356065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03994710735356065 | validation: 0.09526881946380406]
	TIME [epoch: 8.09 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03973426183157817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03973426183157817 | validation: 0.08545154061732874]
	TIME [epoch: 8.07 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04859755379791263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04859755379791263 | validation: 0.11400663973065087]
	TIME [epoch: 8.07 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04935057112652995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04935057112652995 | validation: 0.11763725794032297]
	TIME [epoch: 8.09 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07679533619538484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07679533619538484 | validation: 0.1959784583984423]
	TIME [epoch: 8.07 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10896090609445867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10896090609445867 | validation: 0.17659657513987556]
	TIME [epoch: 8.09 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0915469822306053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0915469822306053 | validation: 0.20243277020636807]
	TIME [epoch: 8.07 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05592961799045254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05592961799045254 | validation: 0.07640310247231895]
	TIME [epoch: 8.03 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029207951038089198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029207951038089198 | validation: 0.07373922188697461]
	TIME [epoch: 8.08 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026887968599598615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026887968599598615 | validation: 0.1036681321656706]
	TIME [epoch: 8.09 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028652212408601125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028652212408601125 | validation: 0.09525429276160957]
	TIME [epoch: 8.09 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04476251203310976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04476251203310976 | validation: 0.11294662818603328]
	TIME [epoch: 8.1 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046258319827736906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046258319827736906 | validation: 0.10406404892298256]
	TIME [epoch: 8.08 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07305189261239059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07305189261239059 | validation: 0.12351528049529617]
	TIME [epoch: 8.07 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0500695757463512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0500695757463512 | validation: 0.08604418409714126]
	TIME [epoch: 8.07 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045312854989505776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045312854989505776 | validation: 0.11162159587799723]
	TIME [epoch: 8.09 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03886280396649717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03886280396649717 | validation: 0.23911974208749626]
	TIME [epoch: 8.11 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07484726578463732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07484726578463732 | validation: 0.19674857681304744]
	TIME [epoch: 8.09 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1031164972213459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1031164972213459 | validation: 0.16170222005361223]
	TIME [epoch: 8.08 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0689214462908949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0689214462908949 | validation: 0.07144873355617998]
	TIME [epoch: 8.08 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026417528943001624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026417528943001624 | validation: 0.06756298320319387]
	TIME [epoch: 8.07 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_929.pth
	Model improved!!!
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01834085455171999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01834085455171999 | validation: 0.07393218007928988]
	TIME [epoch: 8.1 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024956162971192505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024956162971192505 | validation: 0.0846786034834028]
	TIME [epoch: 8.1 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036487587294235936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036487587294235936 | validation: 0.11697890301212932]
	TIME [epoch: 8.08 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06464128454997488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06464128454997488 | validation: 0.11871625772966224]
	TIME [epoch: 8.07 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07852497552961334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07852497552961334 | validation: 0.125424765759271]
	TIME [epoch: 8.07 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07728160492292858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07728160492292858 | validation: 0.2060922024456709]
	TIME [epoch: 8.08 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08526196316659182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08526196316659182 | validation: 0.11730222622459321]
	TIME [epoch: 8.11 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0774481795241081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0774481795241081 | validation: 0.10121121268599503]
	TIME [epoch: 8.08 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03718098696319925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03718098696319925 | validation: 0.07901802271057072]
	TIME [epoch: 8.07 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024022060742313757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024022060742313757 | validation: 0.07232602062240369]
	TIME [epoch: 8.06 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019705126033610867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019705126033610867 | validation: 0.10068186351321312]
	TIME [epoch: 8.07 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02219386339140581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02219386339140581 | validation: 0.10020300598303677]
	TIME [epoch: 8.09 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0342446809631913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0342446809631913 | validation: 0.18266311834133475]
	TIME [epoch: 8.09 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052613006602861695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052613006602861695 | validation: 0.11108369241781842]
	TIME [epoch: 8.07 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06425736445680345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06425736445680345 | validation: 0.10692465064356092]
	TIME [epoch: 8.06 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06661520917683246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06661520917683246 | validation: 0.10875482684219302]
	TIME [epoch: 8.06 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06195024707025557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06195024707025557 | validation: 0.10013393316595205]
	TIME [epoch: 8.08 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06675307444221774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06675307444221774 | validation: 0.14966905223830393]
	TIME [epoch: 8.1 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04956120803146752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04956120803146752 | validation: 0.08556737901367105]
	TIME [epoch: 8.08 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0351529941814847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0351529941814847 | validation: 0.09041160503280088]
	TIME [epoch: 8.07 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030815836026808992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030815836026808992 | validation: 0.08708407147042256]
	TIME [epoch: 8.06 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03081206024856715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03081206024856715 | validation: 0.13281889083916837]
	TIME [epoch: 8.07 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04079141805091413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04079141805091413 | validation: 0.10261979998693711]
	TIME [epoch: 8.1 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061274163082520545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061274163082520545 | validation: 0.15559547801004192]
	TIME [epoch: 8.09 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08710084743560911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08710084743560911 | validation: 0.10069943793848952]
	TIME [epoch: 8.07 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055356656704579524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055356656704579524 | validation: 0.0934556989831241]
	TIME [epoch: 8.07 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04751692836357149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04751692836357149 | validation: 0.12836738794170793]
	TIME [epoch: 8.07 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05794187085131547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05794187085131547 | validation: 0.0915457455858707]
	TIME [epoch: 8.08 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04858890770385281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04858890770385281 | validation: 0.08661827413471028]
	TIME [epoch: 8.1 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04482935117754721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04482935117754721 | validation: 0.08692599890464281]
	TIME [epoch: 8.08 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03309480182312543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03309480182312543 | validation: 0.06977702491376667]
	TIME [epoch: 8.06 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026573216839862246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026573216839862246 | validation: 0.09196188969785246]
	TIME [epoch: 8.07 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026933312576566948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026933312576566948 | validation: 0.16978193994729046]
	TIME [epoch: 8.08 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04061634224231844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04061634224231844 | validation: 0.11282896638410936]
	TIME [epoch: 8.07 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04109770611418697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04109770611418697 | validation: 0.15133609007164606]
	TIME [epoch: 8.04 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05754142657832323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05754142657832323 | validation: 0.15812777181236828]
	TIME [epoch: 8.06 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0787462309611182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0787462309611182 | validation: 0.34283332441063635]
	TIME [epoch: 8.06 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08628298301707502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08628298301707502 | validation: 0.12953333109236004]
	TIME [epoch: 8.07 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06304708310323977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06304708310323977 | validation: 0.12642075271931802]
	TIME [epoch: 8.09 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05209919731518317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05209919731518317 | validation: 0.0852278699954773]
	TIME [epoch: 8.12 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04770959857949758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04770959857949758 | validation: 0.08243449700928476]
	TIME [epoch: 8.09 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031250803082286505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031250803082286505 | validation: 0.09247315782662155]
	TIME [epoch: 8.08 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028799979252757506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028799979252757506 | validation: 0.08801267800303823]
	TIME [epoch: 8.08 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052332045427490516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052332045427490516 | validation: 0.13668589858689703]
	TIME [epoch: 8.1 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07117710019716271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07117710019716271 | validation: 0.11353773696913516]
	TIME [epoch: 8.08 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06752409291117066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06752409291117066 | validation: 0.07877619284317054]
	TIME [epoch: 8.09 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046686999250067265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046686999250067265 | validation: 0.10916366459066423]
	TIME [epoch: 8.08 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04870574111805669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04870574111805669 | validation: 0.1118342903331284]
	TIME [epoch: 8.07 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0654941672745236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0654941672745236 | validation: 0.14107391565079488]
	TIME [epoch: 8.07 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04146399316724411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04146399316724411 | validation: 0.08015990253870682]
	TIME [epoch: 8.1 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031748150443275655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031748150443275655 | validation: 0.11504306520195305]
	TIME [epoch: 8.07 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03190761612245105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03190761612245105 | validation: 0.09629147427095296]
	TIME [epoch: 8.07 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055248475429898955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055248475429898955 | validation: 0.16413114805829568]
	TIME [epoch: 8.07 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08390878964546576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08390878964546576 | validation: 0.11365896140002898]
	TIME [epoch: 8.08 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07340226578814049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07340226578814049 | validation: 0.09169237886867404]
	TIME [epoch: 8.08 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031409249813350346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031409249813350346 | validation: 0.07315970916785716]
	TIME [epoch: 8.09 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01860186777942211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01860186777942211 | validation: 0.08820610168261667]
	TIME [epoch: 8.04 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02692264243730678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02692264243730678 | validation: 0.1190881088223179]
	TIME [epoch: 8.06 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0393705565078146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0393705565078146 | validation: 0.10998305563405969]
	TIME [epoch: 8.05 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0654860245223645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0654860245223645 | validation: 0.1250526838352403]
	TIME [epoch: 8.05 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07904404154405112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07904404154405112 | validation: 0.10953114747025886]
	TIME [epoch: 8.07 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0635645434381614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0635645434381614 | validation: 0.08987887942522733]
	TIME [epoch: 8.05 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02994013727576353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02994013727576353 | validation: 0.07558219459360983]
	TIME [epoch: 8.06 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015872002771830592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015872002771830592 | validation: 0.12393038705948803]
	TIME [epoch: 8.04 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0209862170417271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0209862170417271 | validation: 0.1677520933960834]
	TIME [epoch: 8.04 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05260840155458845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05260840155458845 | validation: 0.2548906712737761]
	TIME [epoch: 8.07 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07145905711159684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07145905711159684 | validation: 0.12414564602567797]
	TIME [epoch: 8.07 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07891041502072825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07891041502072825 | validation: 0.11366025906536775]
	TIME [epoch: 8.05 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0538074506738587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0538074506738587 | validation: 0.11753345111437684]
	TIME [epoch: 8.06 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06525100815198791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06525100815198791 | validation: 0.11191379825097288]
	TIME [epoch: 8.05 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07638494195109827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07638494195109827 | validation: 0.15014146740466172]
	TIME [epoch: 8.06 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05243760443857428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05243760443857428 | validation: 0.06176521482220684]
	TIME [epoch: 59.5 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_1001.pth
	Model improved!!!
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032636419246802266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032636419246802266 | validation: 0.07748796658473901]
	TIME [epoch: 17.3 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03023279939563218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03023279939563218 | validation: 0.11483646735593674]
	TIME [epoch: 17.3 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03797811558553567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03797811558553567 | validation: 0.09280928487981716]
	TIME [epoch: 17.3 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051260009033399216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051260009033399216 | validation: 0.08903639054939093]
	TIME [epoch: 17.3 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05264991584107831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05264991584107831 | validation: 0.0784160534926519]
	TIME [epoch: 17.3 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036483256300609286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036483256300609286 | validation: 0.07868329114382336]
	TIME [epoch: 17.3 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018859397748121693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018859397748121693 | validation: 0.07026112111451359]
	TIME [epoch: 17.3 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016311684324669985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016311684324669985 | validation: 0.08567014477879246]
	TIME [epoch: 17.3 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03235835019201109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03235835019201109 | validation: 0.1283502147244399]
	TIME [epoch: 17.3 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09650692852538852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09650692852538852 | validation: 0.17508682851384416]
	TIME [epoch: 17.3 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10370102445398045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10370102445398045 | validation: 0.08316332833629324]
	TIME [epoch: 17.3 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04974327381546454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04974327381546454 | validation: 0.09710229055697456]
	TIME [epoch: 17.3 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020645226413909788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020645226413909788 | validation: 0.0891093897170884]
	TIME [epoch: 17.2 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01884759730526024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01884759730526024 | validation: 0.09115804785062916]
	TIME [epoch: 17.3 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023465846527438848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023465846527438848 | validation: 0.0816671956499835]
	TIME [epoch: 17.3 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03520061460707757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03520061460707757 | validation: 0.10865072932427688]
	TIME [epoch: 17.3 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05813970568840992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05813970568840992 | validation: 0.1101902952705864]
	TIME [epoch: 17.3 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06013255843566668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06013255843566668 | validation: 0.1341527244399874]
	TIME [epoch: 17.3 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05912679402763369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05912679402763369 | validation: 0.09353627015734071]
	TIME [epoch: 17.3 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05352878793128746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05352878793128746 | validation: 0.1074514842701996]
	TIME [epoch: 17.3 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037175004955237234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037175004955237234 | validation: 0.1377564340823208]
	TIME [epoch: 17.3 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05597631679928485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05597631679928485 | validation: 0.1999350647080494]
	TIME [epoch: 17.3 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05592819434652732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05592819434652732 | validation: 0.14764593804966183]
	TIME [epoch: 17.3 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055741597955672885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055741597955672885 | validation: 0.12881052942281465]
	TIME [epoch: 17.3 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0606299663001276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0606299663001276 | validation: 0.10739787624016853]
	TIME [epoch: 17.3 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046403753173060995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046403753173060995 | validation: 0.08336447422271986]
	TIME [epoch: 17.3 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03208267482719883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03208267482719883 | validation: 0.0734475843944874]
	TIME [epoch: 17.3 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02483214099227983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02483214099227983 | validation: 0.10019129327092852]
	TIME [epoch: 17.3 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04739682011985501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04739682011985501 | validation: 0.12001768232890356]
	TIME [epoch: 17.3 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08212967991842156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08212967991842156 | validation: 0.11083397657194415]
	TIME [epoch: 17.3 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06759846257905162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06759846257905162 | validation: 0.07281771910059279]
	TIME [epoch: 17.3 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030815828622379225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030815828622379225 | validation: 0.06177572275130158]
	TIME [epoch: 17.3 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01819421740518781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01819421740518781 | validation: 0.10762736625951508]
	TIME [epoch: 17.3 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024792685873718377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024792685873718377 | validation: 0.08651735735981186]
	TIME [epoch: 17.3 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03600870356743619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03600870356743619 | validation: 0.19798654589039102]
	TIME [epoch: 17.3 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05265465532697012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05265465532697012 | validation: 0.07854938451799733]
	TIME [epoch: 17.3 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03480331708936423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03480331708936423 | validation: 0.06352581582352054]
	TIME [epoch: 17.3 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02612532547253697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02612532547253697 | validation: 0.08907933128232276]
	TIME [epoch: 17.3 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03663819309365834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03663819309365834 | validation: 0.11356931394881303]
	TIME [epoch: 17.3 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08405904115469451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08405904115469451 | validation: 0.12670301902701456]
	TIME [epoch: 17.3 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09211908486739846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09211908486739846 | validation: 0.07473889345007886]
	TIME [epoch: 17.3 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030067429572603884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030067429572603884 | validation: 0.05645069430770455]
	TIME [epoch: 17.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_1043.pth
	Model improved!!!
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013024710742451205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013024710742451205 | validation: 0.09949259575423426]
	TIME [epoch: 17.3 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018540187536725178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018540187536725178 | validation: 0.08973538946981469]
	TIME [epoch: 17.3 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028195672942601495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028195672942601495 | validation: 0.1288420162398546]
	TIME [epoch: 17.3 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05177729005964098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05177729005964098 | validation: 0.12483645243588315]
	TIME [epoch: 17.3 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07598631893985544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07598631893985544 | validation: 0.09895823585317093]
	TIME [epoch: 17.3 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058541764785795466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058541764785795466 | validation: 0.0760192861788645]
	TIME [epoch: 17.3 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03235748170927154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03235748170927154 | validation: 0.08108757852692125]
	TIME [epoch: 17.3 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03757370616168723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03757370616168723 | validation: 0.12217736773465458]
	TIME [epoch: 17.3 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06535687976336566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06535687976336566 | validation: 0.12374871681379825]
	TIME [epoch: 17.3 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06015742866903283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06015742866903283 | validation: 0.08844920282631596]
	TIME [epoch: 17.3 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040701060054589205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040701060054589205 | validation: 0.07706312265096649]
	TIME [epoch: 17.3 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021984538595559534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021984538595559534 | validation: 0.08543855357349477]
	TIME [epoch: 17.3 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016413610393785975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016413610393785975 | validation: 0.0744157613811384]
	TIME [epoch: 17.3 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02049029320985317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02049029320985317 | validation: 0.07829664691744935]
	TIME [epoch: 17.3 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031120189536718607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031120189536718607 | validation: 0.10619291776580325]
	TIME [epoch: 17.3 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07085210805342212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07085210805342212 | validation: 0.23034774335629438]
	TIME [epoch: 17.3 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11247490705281772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11247490705281772 | validation: 0.12913451718770486]
	TIME [epoch: 17.3 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08079420854401681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08079420854401681 | validation: 0.0720357742332715]
	TIME [epoch: 17.3 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02036358184305268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02036358184305268 | validation: 0.08118034384921269]
	TIME [epoch: 17.3 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012737933180054926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012737933180054926 | validation: 0.07862964698600361]
	TIME [epoch: 17.3 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01954140448561598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01954140448561598 | validation: 0.10534430357768983]
	TIME [epoch: 17.3 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025410474545250236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025410474545250236 | validation: 0.06878522339184472]
	TIME [epoch: 17.3 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02311081223121742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02311081223121742 | validation: 0.09602699600464305]
	TIME [epoch: 17.3 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020683736152766464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020683736152766464 | validation: 0.09988037921267284]
	TIME [epoch: 17.3 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027944697475716217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027944697475716217 | validation: 0.10213312498115501]
	TIME [epoch: 17.3 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03588070225264104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03588070225264104 | validation: 0.07199753612532442]
	TIME [epoch: 17.3 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037107167762957756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037107167762957756 | validation: 0.11216263218148886]
	TIME [epoch: 17.3 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062279636898658834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062279636898658834 | validation: 0.15135380731808953]
	TIME [epoch: 17.3 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10275917377016161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10275917377016161 | validation: 0.1326105709507895]
	TIME [epoch: 17.3 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08639508320973288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08639508320973288 | validation: 0.11849637066141093]
	TIME [epoch: 17.3 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07702532375292868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07702532375292868 | validation: 0.09059339154443746]
	TIME [epoch: 17.3 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04278664146419991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04278664146419991 | validation: 0.11677312960262021]
	TIME [epoch: 17.3 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04046492460896068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04046492460896068 | validation: 0.0828658345626074]
	TIME [epoch: 17.3 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020126464684080983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020126464684080983 | validation: 0.07807258676192504]
	TIME [epoch: 17.3 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023239034371727484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023239034371727484 | validation: 0.1667766599054742]
	TIME [epoch: 17.3 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035682862025446606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035682862025446606 | validation: 0.09291619446872591]
	TIME [epoch: 17.2 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036945310291256805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036945310291256805 | validation: 0.08482061384591676]
	TIME [epoch: 17.3 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0327464117363358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0327464117363358 | validation: 0.08291255041815049]
	TIME [epoch: 17.3 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033480338300782266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033480338300782266 | validation: 0.10130254465073874]
	TIME [epoch: 17.3 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045032855669536485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045032855669536485 | validation: 0.12135990186708466]
	TIME [epoch: 17.3 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06141527420823147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06141527420823147 | validation: 0.08965077080279081]
	TIME [epoch: 17.3 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05606681557047347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05606681557047347 | validation: 0.07108781617321629]
	TIME [epoch: 17.3 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03631380315433342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03631380315433342 | validation: 0.09954893878246002]
	TIME [epoch: 17.3 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028291994521030345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028291994521030345 | validation: 0.08016705465031726]
	TIME [epoch: 17.3 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02189688251060403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02189688251060403 | validation: 0.11343910065035714]
	TIME [epoch: 17.3 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027417590229791584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027417590229791584 | validation: 0.08969164718135227]
	TIME [epoch: 17.3 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04127581950554643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04127581950554643 | validation: 0.1105633963271998]
	TIME [epoch: 17.3 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05163450035023818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05163450035023818 | validation: 0.08802452516105518]
	TIME [epoch: 17.3 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04953688291617289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04953688291617289 | validation: 0.08980950650064207]
	TIME [epoch: 17.3 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04049981291004799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04049981291004799 | validation: 0.1011955603436129]
	TIME [epoch: 17.3 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049330915474057595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049330915474057595 | validation: 0.17367168602457347]
	TIME [epoch: 17.3 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08541358542337672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08541358542337672 | validation: 0.25625654765416794]
	TIME [epoch: 17.3 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09803919529611715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09803919529611715 | validation: 0.08123311372810343]
	TIME [epoch: 17.3 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03939603496725943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03939603496725943 | validation: 0.06353264610024469]
	TIME [epoch: 17.3 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020231956221043998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020231956221043998 | validation: 0.06741901678466432]
	TIME [epoch: 17.3 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02992130546221206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02992130546221206 | validation: 0.07706622142158687]
	TIME [epoch: 17.3 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039003084546248755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039003084546248755 | validation: 0.09870843020520902]
	TIME [epoch: 17.3 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0425885921542287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0425885921542287 | validation: 0.08507508072002389]
	TIME [epoch: 17.3 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04335946248841691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04335946248841691 | validation: 0.08187159982484717]
	TIME [epoch: 17.3 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040394378710788735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040394378710788735 | validation: 0.08052563782958508]
	TIME [epoch: 17.3 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03045843726747164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03045843726747164 | validation: 0.07572102585739757]
	TIME [epoch: 17.3 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0262179549957175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0262179549957175 | validation: 0.06722281943573737]
	TIME [epoch: 17.3 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027319558921208912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027319558921208912 | validation: 0.0861214736690053]
	TIME [epoch: 17.3 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042391571636952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042391571636952 | validation: 0.11149254477785653]
	TIME [epoch: 17.3 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07365141189657512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07365141189657512 | validation: 0.10404209533125353]
	TIME [epoch: 17.3 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051415055421123367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051415055421123367 | validation: 0.07164464136334918]
	TIME [epoch: 17.2 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03324345789022297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03324345789022297 | validation: 0.0681119811099605]
	TIME [epoch: 17.3 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020284180931228527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020284180931228527 | validation: 0.06018523010397005]
	TIME [epoch: 17.3 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013374485738187265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013374485738187265 | validation: 0.052249096844706945]
	TIME [epoch: 17.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_1112.pth
	Model improved!!!
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012155120428404317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012155120428404317 | validation: 0.06055024720857748]
	TIME [epoch: 17.4 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013497219434553566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013497219434553566 | validation: 0.08584367755380215]
	TIME [epoch: 17.4 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03129859427158834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03129859427158834 | validation: 0.15653335449829564]
	TIME [epoch: 17.3 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07657859573005672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07657859573005672 | validation: 0.15922793774339297]
	TIME [epoch: 17.3 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07821717207265247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07821717207265247 | validation: 0.20816094868176394]
	TIME [epoch: 17.4 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04066669714842501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04066669714842501 | validation: 0.15002773323468355]
	TIME [epoch: 17.4 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0512510073722364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0512510073722364 | validation: 0.14882997029145803]
	TIME [epoch: 17.4 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08665917153354359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08665917153354359 | validation: 0.08742207619549498]
	TIME [epoch: 17.3 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054786676917196964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054786676917196964 | validation: 0.10232779708706646]
	TIME [epoch: 17.4 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03899182353660272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03899182353660272 | validation: 0.06994332222747547]
	TIME [epoch: 17.3 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015688168113367915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015688168113367915 | validation: 0.06972057019766964]
	TIME [epoch: 17.3 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026512609646930967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026512609646930967 | validation: 0.08504238225310753]
	TIME [epoch: 17.4 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03103872466303952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03103872466303952 | validation: 0.09556503844261363]
	TIME [epoch: 17.4 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04265010574309956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04265010574309956 | validation: 0.10059156655384323]
	TIME [epoch: 17.3 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056100019877670625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056100019877670625 | validation: 0.1536285988142173]
	TIME [epoch: 17.4 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06161988349974016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06161988349974016 | validation: 0.10039016287950696]
	TIME [epoch: 17.3 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043373194803307966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043373194803307966 | validation: 0.05430430879562392]
	TIME [epoch: 17.4 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035321976188631206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035321976188631206 | validation: 0.07364176098438316]
	TIME [epoch: 17.4 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03194658612073836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03194658612073836 | validation: 0.07091397576784735]
	TIME [epoch: 17.3 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03175114308370203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03175114308370203 | validation: 0.11883072504426401]
	TIME [epoch: 17.4 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0399506733157165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0399506733157165 | validation: 0.0910442741741215]
	TIME [epoch: 17.4 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03621550671881755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03621550671881755 | validation: 0.07009731492284413]
	TIME [epoch: 17.3 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023650171647239766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023650171647239766 | validation: 0.06021163725744271]
	TIME [epoch: 17.4 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019646031705431184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019646031705431184 | validation: 0.08862610617030608]
	TIME [epoch: 17.3 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01708434638313134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01708434638313134 | validation: 0.0693120561680271]
	TIME [epoch: 17.4 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024375812096509856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024375812096509856 | validation: 0.09502512748351546]
	TIME [epoch: 17.4 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05475051773171626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05475051773171626 | validation: 0.14231629618801897]
	TIME [epoch: 17.3 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10857314027255072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10857314027255072 | validation: 0.09604541450520448]
	TIME [epoch: 17.4 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06392510418306505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06392510418306505 | validation: 0.04997170758722616]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_1141.pth
	Model improved!!!
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01515019805096179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01515019805096179 | validation: 0.07282646195367216]
	TIME [epoch: 17.3 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009838005955480476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009838005955480476 | validation: 0.06746374182482537]
	TIME [epoch: 17.3 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015343744928667002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015343744928667002 | validation: 0.06489319580612532]
	TIME [epoch: 17.3 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023539675705834057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023539675705834057 | validation: 0.08371361935021958]
	TIME [epoch: 17.3 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048134069764939655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048134069764939655 | validation: 0.12145992273863185]
	TIME [epoch: 17.3 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08282761972741108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08282761972741108 | validation: 0.13404369124891594]
	TIME [epoch: 17.3 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09993117050005211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09993117050005211 | validation: 0.09664330051852771]
	TIME [epoch: 17.3 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05820380937375255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05820380937375255 | validation: 0.05201489820104328]
	TIME [epoch: 17.3 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021157270934454048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021157270934454048 | validation: 0.0770204825041193]
	TIME [epoch: 17.3 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03109720777357403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03109720777357403 | validation: 0.14049055400190338]
	TIME [epoch: 17.3 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04206946889575341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04206946889575341 | validation: 0.07429727051574538]
	TIME [epoch: 17.3 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03624072402092225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03624072402092225 | validation: 0.07753734127555817]
	TIME [epoch: 17.3 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030654659392784583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030654659392784583 | validation: 0.08320848950400862]
	TIME [epoch: 17.3 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032275212165373574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032275212165373574 | validation: 0.10566815462667095]
	TIME [epoch: 17.3 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030707934412496685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030707934412496685 | validation: 0.08635248763251256]
	TIME [epoch: 17.3 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029546944474328693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029546944474328693 | validation: 0.08892292378073892]
	TIME [epoch: 17.3 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03168561802687551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03168561802687551 | validation: 0.08831057110971015]
	TIME [epoch: 17.3 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04641268632307179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04641268632307179 | validation: 0.10286079974429481]
	TIME [epoch: 17.3 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06823863368331956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06823863368331956 | validation: 0.10529332595419426]
	TIME [epoch: 17.3 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06368189401403616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06368189401403616 | validation: 0.07716830997822755]
	TIME [epoch: 17.3 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03386895169648496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03386895169648496 | validation: 0.07488622148472979]
	TIME [epoch: 17.3 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030838160691830607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030838160691830607 | validation: 0.06595477160593753]
	TIME [epoch: 17.3 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030377425654016976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030377425654016976 | validation: 0.10944874949582949]
	TIME [epoch: 17.3 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02683643474071106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02683643474071106 | validation: 0.07186029238180884]
	TIME [epoch: 17.3 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025552662807469862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025552662807469862 | validation: 0.07580712974669462]
	TIME [epoch: 17.3 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024042913872029205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024042913872029205 | validation: 0.08531300497000177]
	TIME [epoch: 17.3 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027352459727454786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027352459727454786 | validation: 0.10344023817806242]
	TIME [epoch: 17.3 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05433696900927846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05433696900927846 | validation: 0.10948654947776154]
	TIME [epoch: 17.3 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0729129805622949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0729129805622949 | validation: 0.0684070303172938]
	TIME [epoch: 17.3 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036754805271663964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036754805271663964 | validation: 0.066291775262295]
	TIME [epoch: 17.2 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021175327362322145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021175327362322145 | validation: 0.098334664759733]
	TIME [epoch: 17.3 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026839796536704267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026839796536704267 | validation: 0.0811504084450656]
	TIME [epoch: 17.3 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048421987925199805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048421987925199805 | validation: 0.11397335440788003]
	TIME [epoch: 17.3 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06498863039458347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06498863039458347 | validation: 0.08280462281826176]
	TIME [epoch: 17.3 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05106580036647717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05106580036647717 | validation: 0.10348341891587945]
	TIME [epoch: 17.3 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03129235960342524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03129235960342524 | validation: 0.09816956148203693]
	TIME [epoch: 17.3 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02704986264773136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02704986264773136 | validation: 0.09911145583662168]
	TIME [epoch: 17.3 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02539517427958044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02539517427958044 | validation: 0.09047430682067606]
	TIME [epoch: 17.3 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042361860778342496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042361860778342496 | validation: 0.13756661625469682]
	TIME [epoch: 17.3 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055668957037670455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055668957037670455 | validation: 0.09266255676093359]
	TIME [epoch: 17.3 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03676557434367648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03676557434367648 | validation: 0.07278686752278328]
	TIME [epoch: 17.3 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017266897661312845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017266897661312845 | validation: 0.052256435931432914]
	TIME [epoch: 17.3 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012158241365186147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012158241365186147 | validation: 0.0910436851247627]
	TIME [epoch: 17.4 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02464661064560254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02464661064560254 | validation: 0.0996515936117131]
	TIME [epoch: 17.3 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04645265726204773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04645265726204773 | validation: 0.20289607989735414]
	TIME [epoch: 17.2 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07365675999529213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07365675999529213 | validation: 0.10927467237607501]
	TIME [epoch: 17.3 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05829554920855627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05829554920855627 | validation: 0.10440234305969387]
	TIME [epoch: 17.3 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032839730244010525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032839730244010525 | validation: 0.057889830110120014]
	TIME [epoch: 17.3 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019305316474634508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019305316474634508 | validation: 0.05943592035110948]
	TIME [epoch: 17.3 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019997650944143876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019997650944143876 | validation: 0.07933988287936544]
	TIME [epoch: 17.3 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028233823517974904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028233823517974904 | validation: 0.09133669947008041]
	TIME [epoch: 17.3 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04410730994648695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04410730994648695 | validation: 0.08158517272798346]
	TIME [epoch: 17.2 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037166174485025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037166174485025 | validation: 0.07058453119491362]
	TIME [epoch: 17.3 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02904426352985156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02904426352985156 | validation: 0.0835856656466384]
	TIME [epoch: 17.3 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040124104002107426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040124104002107426 | validation: 0.10722065837571726]
	TIME [epoch: 17.2 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0745008009913165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0745008009913165 | validation: 0.11807875020286424]
	TIME [epoch: 17.3 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07461612047086001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07461612047086001 | validation: 0.10664369025564202]
	TIME [epoch: 17.3 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05054760817708342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05054760817708342 | validation: 0.15952795130706546]
	TIME [epoch: 17.3 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02902276089557497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02902276089557497 | validation: 0.11968745245433113]
	TIME [epoch: 17.3 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03403216823898357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03403216823898357 | validation: 0.07869837955808542]
	TIME [epoch: 17.2 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03204410742639538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03204410742639538 | validation: 0.07569155386726223]
	TIME [epoch: 17.1 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03045127682707132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03045127682707132 | validation: 0.10357186230077114]
	TIME [epoch: 17.2 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036071071840059483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036071071840059483 | validation: 0.07324653146225373]
	TIME [epoch: 17.2 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03216685241592611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03216685241592611 | validation: 0.08100497788900989]
	TIME [epoch: 17.2 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024715588247715296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024715588247715296 | validation: 0.05727312717956143]
	TIME [epoch: 17.2 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01919899204945358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01919899204945358 | validation: 0.045913109850781314]
	TIME [epoch: 17.2 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_1207.pth
	Model improved!!!
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01999234039042377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01999234039042377 | validation: 0.06512957498505417]
	TIME [epoch: 17.3 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03577571694233831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03577571694233831 | validation: 0.09363081089013935]
	TIME [epoch: 17.2 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05454428770672985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05454428770672985 | validation: 0.101353897667672]
	TIME [epoch: 17.2 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047920251545173605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047920251545173605 | validation: 0.07377143358189632]
	TIME [epoch: 17.2 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030162619531664818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030162619531664818 | validation: 0.07420230218579452]
	TIME [epoch: 17.1 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03179279579214505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03179279579214505 | validation: 0.1214085025090843]
	TIME [epoch: 17.2 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05213929286439952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05213929286439952 | validation: 0.12198438231537477]
	TIME [epoch: 17.2 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08425404787380252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08425404787380252 | validation: 0.10590871530205032]
	TIME [epoch: 17.2 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04606164264472671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04606164264472671 | validation: 0.06180358604806781]
	TIME [epoch: 17.2 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023213402558236222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023213402558236222 | validation: 0.06048553848595875]
	TIME [epoch: 17.2 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016997242790267826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016997242790267826 | validation: 0.0756701149444741]
	TIME [epoch: 17.2 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01951873826405051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01951873826405051 | validation: 0.08643826123539831]
	TIME [epoch: 17.2 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03067450318997029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03067450318997029 | validation: 0.10579075057141263]
	TIME [epoch: 17.2 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050286116060645016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050286116060645016 | validation: 0.1108797349192491]
	TIME [epoch: 17.2 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055016657965231745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055016657965231745 | validation: 0.06958832074549458]
	TIME [epoch: 17.2 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0333272490521742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0333272490521742 | validation: 0.061299404238154914]
	TIME [epoch: 17.2 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014203099866454467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014203099866454467 | validation: 0.06472214115874915]
	TIME [epoch: 17.2 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009589182121703524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009589182121703524 | validation: 0.06490989674569571]
	TIME [epoch: 17.3 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017178599859009155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017178599859009155 | validation: 0.08087525114560858]
	TIME [epoch: 17.3 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0439184011611534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0439184011611534 | validation: 0.11708514605179339]
	TIME [epoch: 17.2 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06063603230901546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06063603230901546 | validation: 0.14930555295806722]
	TIME [epoch: 17.2 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051745194206182114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051745194206182114 | validation: 0.06934594783515138]
	TIME [epoch: 17.2 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030701968079367814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030701968079367814 | validation: 0.08558210204450017]
	TIME [epoch: 17.2 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02988696292835294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02988696292835294 | validation: 0.2547493271031778]
	TIME [epoch: 17.2 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0569576135030235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0569576135030235 | validation: 0.09694929545654271]
	TIME [epoch: 17.3 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06599692635547204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06599692635547204 | validation: 0.0788157950444628]
	TIME [epoch: 17.3 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04681630635968299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04681630635968299 | validation: 0.07414902605036389]
	TIME [epoch: 17.3 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020745222745257502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020745222745257502 | validation: 0.08355793277056335]
	TIME [epoch: 17.3 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022115010853351658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022115010853351658 | validation: 0.07146306081269736]
	TIME [epoch: 17.3 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02688350110454071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02688350110454071 | validation: 0.0941452669748746]
	TIME [epoch: 17.3 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03973429229550738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03973429229550738 | validation: 0.07910721878037011]
	TIME [epoch: 17.3 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04541305684103633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04541305684103633 | validation: 0.0736199940529729]
	TIME [epoch: 17.3 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03807273506614343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03807273506614343 | validation: 0.07322743276723541]
	TIME [epoch: 17.3 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03604869254912022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03604869254912022 | validation: 0.0813191807708967]
	TIME [epoch: 17.3 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024909688350203348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024909688350203348 | validation: 0.07695337550514374]
	TIME [epoch: 17.3 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028604585830201916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028604585830201916 | validation: 0.0838212655822611]
	TIME [epoch: 17.3 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042346920247533344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042346920247533344 | validation: 0.08781135181332494]
	TIME [epoch: 17.3 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037701983930433926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037701983930433926 | validation: 0.07378897555277296]
	TIME [epoch: 17.2 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02341247668314228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02341247668314228 | validation: 0.05905944700667729]
	TIME [epoch: 17.2 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013116260279134335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013116260279134335 | validation: 0.08011115145533285]
	TIME [epoch: 17.2 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013600280187674688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013600280187674688 | validation: 0.05930128642101962]
	TIME [epoch: 17.2 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02649974404418785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02649974404418785 | validation: 0.07541385746768058]
	TIME [epoch: 17.2 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05845877675186183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05845877675186183 | validation: 0.11273971097214407]
	TIME [epoch: 17.2 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09022035123277089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09022035123277089 | validation: 0.11590684028734471]
	TIME [epoch: 17.1 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043176639010788824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043176639010788824 | validation: 0.08403511090481074]
	TIME [epoch: 17.2 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016321372554217246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016321372554217246 | validation: 0.07898625578520313]
	TIME [epoch: 17.2 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01510697328296346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01510697328296346 | validation: 0.0700065327262633]
	TIME [epoch: 17.2 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016674552088889537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016674552088889537 | validation: 0.0789081741509062]
	TIME [epoch: 17.3 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01724253840299171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01724253840299171 | validation: 0.11348702721458015]
	TIME [epoch: 17.2 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0400236930642967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0400236930642967 | validation: 0.2506807784726128]
	TIME [epoch: 17.2 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08793383448156825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08793383448156825 | validation: 0.09981967838516353]
	TIME [epoch: 17.3 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07435384531469627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07435384531469627 | validation: 0.09134787772063463]
	TIME [epoch: 17.2 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04544361779054828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04544361779054828 | validation: 0.14978053355296048]
	TIME [epoch: 17.3 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05183423753975011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05183423753975011 | validation: 0.10161163074826224]
	TIME [epoch: 17.2 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03047378672301555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03047378672301555 | validation: 0.06079226904477214]
	TIME [epoch: 17.3 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019202605725791324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019202605725791324 | validation: 0.057499609773469774]
	TIME [epoch: 17.3 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01720765835326891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01720765835326891 | validation: 0.07436860255524376]
	TIME [epoch: 17.3 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013599003479709208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013599003479709208 | validation: 0.07679641755625607]
	TIME [epoch: 17.2 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009562267769330935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009562267769330935 | validation: 0.05722531813456388]
	TIME [epoch: 17.3 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009476776213093818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009476776213093818 | validation: 0.04846083387456657]
	TIME [epoch: 17.2 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01086182357713695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01086182357713695 | validation: 0.06279181222181712]
	TIME [epoch: 17.3 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023182319909846923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023182319909846923 | validation: 0.11147126168351758]
	TIME [epoch: 17.3 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0674182099786603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0674182099786603 | validation: 0.13888225012348349]
	TIME [epoch: 17.2 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10902552971986416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10902552971986416 | validation: 0.09582872503358597]
	TIME [epoch: 17.3 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04491837615345937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04491837615345937 | validation: 0.0763527462915489]
	TIME [epoch: 17.3 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019693594531437604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019693594531437604 | validation: 0.08639227842841564]
	TIME [epoch: 17.2 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02536657232334757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02536657232334757 | validation: 0.07630197463611708]
	TIME [epoch: 17.3 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0332445413777812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0332445413777812 | validation: 0.0926607400887217]
	TIME [epoch: 17.3 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04980756730379883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04980756730379883 | validation: 0.09641567159878418]
	TIME [epoch: 17.3 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05097663216278472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05097663216278472 | validation: 0.06418506429948051]
	TIME [epoch: 17.3 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023452024645802937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023452024645802937 | validation: 0.0636667443609778]
	TIME [epoch: 17.2 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01944417284685842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01944417284685842 | validation: 0.12936735784090367]
	TIME [epoch: 17.3 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042332507207983525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042332507207983525 | validation: 0.09506131906404261]
	TIME [epoch: 17.2 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05092046090575735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05092046090575735 | validation: 0.09188800786766543]
	TIME [epoch: 17.2 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028628148780648625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028628148780648625 | validation: 0.07372056280342269]
	TIME [epoch: 17.2 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02388156417061315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02388156417061315 | validation: 0.08186950080178272]
	TIME [epoch: 17.3 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033924494081473126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033924494081473126 | validation: 0.08743550381228551]
	TIME [epoch: 17.2 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0456590867154255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0456590867154255 | validation: 0.10058844196367947]
	TIME [epoch: 17.3 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04539099515119371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04539099515119371 | validation: 0.08752514914739308]
	TIME [epoch: 17.2 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036091834682222636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036091834682222636 | validation: 0.12288193346961639]
	TIME [epoch: 17.3 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02356109888778695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02356109888778695 | validation: 0.058915469308378134]
	TIME [epoch: 17.2 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015522145889653251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015522145889653251 | validation: 0.05873634710135376]
	TIME [epoch: 17.2 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01434618373979063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01434618373979063 | validation: 0.05846731026130525]
	TIME [epoch: 17.3 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014904409935135324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014904409935135324 | validation: 0.08007667695033409]
	TIME [epoch: 17.2 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021684313558501082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021684313558501082 | validation: 0.08490419958561046]
	TIME [epoch: 17.1 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039796292094493986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039796292094493986 | validation: 0.09040699233940648]
	TIME [epoch: 17.2 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06323380559254545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06323380559254545 | validation: 0.07519908196022579]
	TIME [epoch: 17.2 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0526275848048175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0526275848048175 | validation: 0.0868092593027659]
	TIME [epoch: 17.1 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037350197474428255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037350197474428255 | validation: 0.08697780346335954]
	TIME [epoch: 17.2 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044931613462193855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044931613462193855 | validation: 0.1618008196781093]
	TIME [epoch: 17.2 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05261810412993924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05261810412993924 | validation: 0.10911253281547367]
	TIME [epoch: 17.2 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04556849588134635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04556849588134635 | validation: 0.09110655754354625]
	TIME [epoch: 17.1 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031222836098420484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031222836098420484 | validation: 0.18729889987367324]
	TIME [epoch: 17.1 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0320677370262538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0320677370262538 | validation: 0.10230401364293998]
	TIME [epoch: 17.3 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029579154127880404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029579154127880404 | validation: 0.07330031323725095]
	TIME [epoch: 17.3 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028831324475369065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028831324475369065 | validation: 0.08351514850894323]
	TIME [epoch: 17.3 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024228953890533783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024228953890533783 | validation: 0.09708633030393517]
	TIME [epoch: 17.3 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0252457749332597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0252457749332597 | validation: 0.07817406074264374]
	TIME [epoch: 17.3 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030123462505915608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030123462505915608 | validation: 0.06343110543204505]
	TIME [epoch: 17.3 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02876417375026974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02876417375026974 | validation: 0.06853014346594719]
	TIME [epoch: 17.3 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024929757016213822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024929757016213822 | validation: 0.09111782752573348]
	TIME [epoch: 17.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20240823_171535/states/model_phi1_4b_v_mmd1_1308.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 11152.667 seconds.
