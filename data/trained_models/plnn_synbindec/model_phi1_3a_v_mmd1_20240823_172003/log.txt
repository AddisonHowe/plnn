Args:
Namespace(name='model_phi1_3a_v_mmd1', outdir='out/model_training/model_phi1_3a_v_mmd1', training_data='data/training_data/data_phi1_3a/training', validation_data='data/training_data/data_phi1_3a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1058605246

Training model...

Saving initial model state to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.041081021678788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.041081021678788 | validation: 4.981312956252104]
	TIME [epoch: 24.8 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.8241238558780735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.8241238558780735 | validation: 4.685581332354603]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.454688268413709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.454688268413709 | validation: 5.283158872980649]
	TIME [epoch: 0.908 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.6109299727083695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6109299727083695 | validation: 6.572970101289804]
	TIME [epoch: 0.909 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.741546098321833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.741546098321833 | validation: 5.563255012207517]
	TIME [epoch: 0.909 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.705152118455284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.705152118455284 | validation: 4.387059850526774]
	TIME [epoch: 0.909 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.219267548919467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.219267548919467 | validation: 4.9491960033066995]
	TIME [epoch: 0.917 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.364862787881507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.364862787881507 | validation: 4.393883111682527]
	TIME [epoch: 0.915 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.914028048844565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.914028048844565 | validation: 4.516396476284668]
	TIME [epoch: 0.913 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.73732371319471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.73732371319471 | validation: 4.767931399552326]
	TIME [epoch: 0.912 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8178833715719502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8178833715719502 | validation: 4.359187395682606]
	TIME [epoch: 0.913 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7223932904344927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7223932904344927 | validation: 4.507671441931118]
	TIME [epoch: 0.914 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.636668886679048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.636668886679048 | validation: 4.257580308790608]
	TIME [epoch: 0.913 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5730750305606755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5730750305606755 | validation: 4.472194389045424]
	TIME [epoch: 0.908 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.530333134779115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.530333134779115 | validation: 4.281840331750302]
	TIME [epoch: 0.908 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5740885990792406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5740885990792406 | validation: 4.464000779173659]
	TIME [epoch: 0.91 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5643633414464397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5643633414464397 | validation: 4.201427465064152]
	TIME [epoch: 0.908 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.561098206636071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.561098206636071 | validation: 4.297764737299117]
	TIME [epoch: 0.911 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3947386123595975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3947386123595975 | validation: 4.211520983990169]
	TIME [epoch: 0.91 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3572604187981687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3572604187981687 | validation: 4.255465025901613]
	TIME [epoch: 0.911 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.344078304500755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.344078304500755 | validation: 4.16183714390519]
	TIME [epoch: 0.913 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.364081800284497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.364081800284497 | validation: 4.267514276863965]
	TIME [epoch: 0.913 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3915837958291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3915837958291 | validation: 4.168955753573329]
	TIME [epoch: 0.91 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4464766486174723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4464766486174723 | validation: 4.172303993429222]
	TIME [epoch: 0.909 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.305251158256869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.305251158256869 | validation: 4.098261996973256]
	TIME [epoch: 0.912 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2727809269661434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2727809269661434 | validation: 4.121728150247641]
	TIME [epoch: 0.913 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2504924388706304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2504924388706304 | validation: 4.058992115678821]
	TIME [epoch: 0.914 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2460516360253098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2460516360253098 | validation: 4.090954655589011]
	TIME [epoch: 0.912 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2409676073196194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2409676073196194 | validation: 4.043072948827024]
	TIME [epoch: 0.915 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2721291223260125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2721291223260125 | validation: 4.064680902374]
	TIME [epoch: 0.913 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.244374095016269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.244374095016269 | validation: 4.000288763583629]
	TIME [epoch: 0.914 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2440764400397373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2440764400397373 | validation: 4.011076866073203]
	TIME [epoch: 0.91 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1824916674293076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1824916674293076 | validation: 3.9554863062659935]
	TIME [epoch: 0.913 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.161319965952874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.161319965952874 | validation: 3.959972301356686]
	TIME [epoch: 0.912 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.135224124416194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.135224124416194 | validation: 3.913380982715122]
	TIME [epoch: 0.91 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.13103880264605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.13103880264605 | validation: 3.926655856959211]
	TIME [epoch: 0.91 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1198213737029175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1198213737029175 | validation: 3.8913020083303596]
	TIME [epoch: 0.907 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1204978948950406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1204978948950406 | validation: 3.8942607086629764]
	TIME [epoch: 0.908 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.103953096415965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.103953096415965 | validation: 3.8529592508080657]
	TIME [epoch: 0.908 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0898824271085186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0898824271085186 | validation: 3.851038534902295]
	TIME [epoch: 0.907 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.068432387046046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.068432387046046 | validation: 3.842427788162512]
	TIME [epoch: 0.912 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0586782494271763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0586782494271763 | validation: 3.821560186350478]
	TIME [epoch: 0.909 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0341711532050426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0341711532050426 | validation: 3.793365143519124]
	TIME [epoch: 0.912 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.014725036428706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.014725036428706 | validation: 3.7674131599161513]
	TIME [epoch: 0.91 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0059469834785677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0059469834785677 | validation: 3.761841986477022]
	TIME [epoch: 0.915 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9965160561885127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9965160561885127 | validation: 3.709787829522236]
	TIME [epoch: 0.914 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9853068571891987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9853068571891987 | validation: 3.757160578401805]
	TIME [epoch: 0.911 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9881575304525327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9881575304525327 | validation: 3.692009875031406]
	TIME [epoch: 0.917 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9605604450072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9605604450072 | validation: 3.721206688715592]
	TIME [epoch: 0.934 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.956476473933343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.956476473933343 | validation: 3.6537710412184694]
	TIME [epoch: 0.915 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9301846134397045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9301846134397045 | validation: 3.6966298652853697]
	TIME [epoch: 0.914 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.91544415810332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.91544415810332 | validation: 3.618911615868214]
	TIME [epoch: 0.908 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.901690269351381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.901690269351381 | validation: 3.6493898262824227]
	TIME [epoch: 0.911 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.891502889884114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.891502889884114 | validation: 3.5929476987417672]
	TIME [epoch: 0.91 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.880955964557299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.880955964557299 | validation: 3.6187289000811576]
	TIME [epoch: 0.916 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.876831261990251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.876831261990251 | validation: 3.5653854198124466]
	TIME [epoch: 0.911 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8559216569756134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8559216569756134 | validation: 3.614519965835053]
	TIME [epoch: 0.91 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8643135968348616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8643135968348616 | validation: 3.520326836101971]
	TIME [epoch: 0.909 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8537139376527696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8537139376527696 | validation: 3.5983801302778704]
	TIME [epoch: 0.913 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8514651696038085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8514651696038085 | validation: 3.4797109671525086]
	TIME [epoch: 0.908 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.821413439112041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.821413439112041 | validation: 3.529008342237452]
	TIME [epoch: 0.916 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.803745147765353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.803745147765353 | validation: 3.4657110618179026]
	TIME [epoch: 0.916 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.787701820233528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.787701820233528 | validation: 3.4868476047915338]
	TIME [epoch: 0.915 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.770678732178633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.770678732178633 | validation: 3.4369516939903395]
	TIME [epoch: 0.916 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.765246100503347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.765246100503347 | validation: 3.450772082438581]
	TIME [epoch: 0.911 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7563963351972394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7563963351972394 | validation: 3.391126118250364]
	TIME [epoch: 0.909 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7487571607033137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7487571607033137 | validation: 3.4197424816175457]
	TIME [epoch: 0.908 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.744593461259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.744593461259 | validation: 3.283244882044466]
	TIME [epoch: 0.912 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6655695552216225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6655695552216225 | validation: 3.054553226953561]
	TIME [epoch: 0.911 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4685594379998497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4685594379998497 | validation: 2.8487372794729344]
	TIME [epoch: 0.911 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.597706846260112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.597706846260112 | validation: 3.0222673710234234]
	TIME [epoch: 0.908 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.610230247580586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.610230247580586 | validation: 2.5691816114488817]
	TIME [epoch: 0.906 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.230315341525916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.230315341525916 | validation: 2.4287870785036665]
	TIME [epoch: 0.908 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9709952474745445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9709952474745445 | validation: 2.5177837847066558]
	TIME [epoch: 0.905 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0305388562868347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0305388562868347 | validation: 2.221589203601448]
	TIME [epoch: 0.905 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9548558158590994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9548558158590994 | validation: 1.9394453408261]
	TIME [epoch: 0.915 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6103502651335044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6103502651335044 | validation: 1.6357691434465544]
	TIME [epoch: 0.911 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4432711613568128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4432711613568128 | validation: 1.2637015695082412]
	TIME [epoch: 0.911 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.695022083824942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.695022083824942 | validation: 1.3863223613694426]
	TIME [epoch: 0.91 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.331417271819313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.331417271819313 | validation: 0.9183609865493292]
	TIME [epoch: 0.913 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1503537988069403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1503537988069403 | validation: 1.092324246090041]
	TIME [epoch: 0.91 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0431257407909231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0431257407909231 | validation: 0.9098288708098559]
	TIME [epoch: 0.911 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9673032420021247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9673032420021247 | validation: 1.0099803765212043]
	TIME [epoch: 0.91 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9559283733399502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9559283733399502 | validation: 0.8949054152116702]
	TIME [epoch: 0.91 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0439452972143155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0439452972143155 | validation: 1.5048642229142724]
	TIME [epoch: 0.913 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2904447624482545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2904447624482545 | validation: 0.887494043129964]
	TIME [epoch: 0.911 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9661651006316574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9661651006316574 | validation: 0.8724815207485191]
	TIME [epoch: 0.922 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9026707050698711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9026707050698711 | validation: 1.0209940813333227]
	TIME [epoch: 0.924 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9529500279810756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9529500279810756 | validation: 0.8796015771614274]
	TIME [epoch: 0.913 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9725455036949572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9725455036949572 | validation: 1.1277331706510882]
	TIME [epoch: 0.911 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0045117117102624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0045117117102624 | validation: 0.8567835462228338]
	TIME [epoch: 0.916 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9198332150035395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9198332150035395 | validation: 0.9938562407429432]
	TIME [epoch: 0.911 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9068797844450035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9068797844450035 | validation: 0.8730611430775714]
	TIME [epoch: 0.911 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8992208552502351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8992208552502351 | validation: 0.9641369861530156]
	TIME [epoch: 0.916 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9034416605228972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9034416605228972 | validation: 0.8723902910272842]
	TIME [epoch: 0.911 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9050295587081246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9050295587081246 | validation: 1.0205599736082134]
	TIME [epoch: 0.912 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9334770334586275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9334770334586275 | validation: 0.8827103602286294]
	TIME [epoch: 0.911 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9068239553037819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9068239553037819 | validation: 1.0357650118731818]
	TIME [epoch: 0.912 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9415751065014124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9415751065014124 | validation: 0.869563044175322]
	TIME [epoch: 0.911 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.90337993184418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.90337993184418 | validation: 0.9770738113914859]
	TIME [epoch: 0.912 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9145025053151739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9145025053151739 | validation: 0.886681532837849]
	TIME [epoch: 0.913 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8945409104074244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8945409104074244 | validation: 0.9692850721139599]
	TIME [epoch: 0.979 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.902141268982696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.902141268982696 | validation: 0.891917606705804]
	TIME [epoch: 0.923 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9008640498697693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9008640498697693 | validation: 0.9492303833631395]
	TIME [epoch: 0.913 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8854517405041348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8854517405041348 | validation: 0.896627095841568]
	TIME [epoch: 0.91 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8791771329738254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8791771329738254 | validation: 1.0105912029236028]
	TIME [epoch: 0.913 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.917606968404734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.917606968404734 | validation: 0.8847327033507857]
	TIME [epoch: 0.91 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8932813076023288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8932813076023288 | validation: 1.004612775253355]
	TIME [epoch: 0.914 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9118468233631117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9118468233631117 | validation: 0.8696338445789658]
	TIME [epoch: 0.911 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8507361648677555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8507361648677555 | validation: 0.8834392771134468]
	TIME [epoch: 0.912 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8452452420369849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8452452420369849 | validation: 0.8749200315396136]
	TIME [epoch: 0.913 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8414797479555953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8414797479555953 | validation: 0.8853769693961784]
	TIME [epoch: 0.913 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8513023178952988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8513023178952988 | validation: 0.8919382342298946]
	TIME [epoch: 0.912 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8884374507960445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8884374507960445 | validation: 1.012392954616391]
	TIME [epoch: 0.911 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0025063438025004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0025063438025004 | validation: 0.9002761979553147]
	TIME [epoch: 0.91 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8555711266458091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8555711266458091 | validation: 0.9469393698206346]
	TIME [epoch: 0.912 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9056839983313952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9056839983313952 | validation: 1.2916634118765584]
	TIME [epoch: 0.911 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1008661298191553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1008661298191553 | validation: 0.9083563831048418]
	TIME [epoch: 0.914 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8564379631077605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8564379631077605 | validation: 0.8929176100843379]
	TIME [epoch: 0.912 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8775310340677905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8775310340677905 | validation: 0.9805471314894704]
	TIME [epoch: 0.914 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8986911810486582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8986911810486582 | validation: 0.8675066825724134]
	TIME [epoch: 0.913 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8441959353772854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8441959353772854 | validation: 0.8822581541828914]
	TIME [epoch: 0.915 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8387026395193415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8387026395193415 | validation: 0.9005877097481829]
	TIME [epoch: 0.912 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8495108216682615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8495108216682615 | validation: 0.8900872884374476]
	TIME [epoch: 0.917 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8882132196909374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8882132196909374 | validation: 1.016201773518538]
	TIME [epoch: 0.913 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0140305093031108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0140305093031108 | validation: 1.1511331706103312]
	TIME [epoch: 0.915 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0125514363649528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0125514363649528 | validation: 0.8798412936770847]
	TIME [epoch: 0.911 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8433544762841497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8433544762841497 | validation: 0.8875768675334025]
	TIME [epoch: 0.914 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8262051064394282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8262051064394282 | validation: 0.8716001369284903]
	TIME [epoch: 0.912 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8287661137727625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8287661137727625 | validation: 0.8711603502751608]
	TIME [epoch: 0.912 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8310500638274932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8310500638274932 | validation: 0.8957997789684408]
	TIME [epoch: 0.913 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8283776553856711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8283776553856711 | validation: 0.9029373448496302]
	TIME [epoch: 0.911 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.858030968671141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.858030968671141 | validation: 1.000925446408046]
	TIME [epoch: 0.91 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9472898090481434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9472898090481434 | validation: 1.020304800410854]
	TIME [epoch: 0.908 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0144248052874705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0144248052874705 | validation: 1.056219858031347]
	TIME [epoch: 0.91 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9499997133715197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9499997133715197 | validation: 0.8705122463099779]
	TIME [epoch: 0.911 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8525269837907673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8525269837907673 | validation: 0.9626229659776749]
	TIME [epoch: 0.911 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8870014466228584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8870014466228584 | validation: 0.8693914039198589]
	TIME [epoch: 0.925 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8427653029872593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8427653029872593 | validation: 0.8807090042924882]
	TIME [epoch: 0.907 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8208189929469751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8208189929469751 | validation: 0.8769795368638118]
	TIME [epoch: 0.911 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8169528428126513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8169528428126513 | validation: 0.8741085556736682]
	TIME [epoch: 0.909 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8199917485872577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8199917485872577 | validation: 0.876476168620486]
	TIME [epoch: 0.911 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8212354118229849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8212354118229849 | validation: 0.9177428872189339]
	TIME [epoch: 0.909 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8461313703157509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8461313703157509 | validation: 0.9482398682161707]
	TIME [epoch: 0.911 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9453775459476376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9453775459476376 | validation: 1.1397669063122684]
	TIME [epoch: 0.906 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.016186989879243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.016186989879243 | validation: 0.8675251774133019]
	TIME [epoch: 0.913 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8219949167114635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8219949167114635 | validation: 0.9056502376546908]
	TIME [epoch: 0.908 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9084814308874084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9084814308874084 | validation: 1.053532205941139]
	TIME [epoch: 0.911 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9755000920259579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9755000920259579 | validation: 0.9458053289398367]
	TIME [epoch: 0.907 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9054667124672386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9054667124672386 | validation: 0.8787013863796048]
	TIME [epoch: 0.911 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8891357722697072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8891357722697072 | validation: 0.9360799813042618]
	TIME [epoch: 0.913 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8598353557308199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8598353557308199 | validation: 0.8561647339116907]
	TIME [epoch: 0.908 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_152.pth
	Model improved!!!
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.816972989961176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.816972989961176 | validation: 0.8656445641731791]
	TIME [epoch: 0.907 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8219935886326362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8219935886326362 | validation: 0.8911440878108003]
	TIME [epoch: 0.908 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8346862505411815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8346862505411815 | validation: 0.8753476331631793]
	TIME [epoch: 0.908 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8279418482727681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8279418482727681 | validation: 0.8916508609467931]
	TIME [epoch: 0.912 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8443634314730838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8443634314730838 | validation: 0.9070245900052996]
	TIME [epoch: 0.908 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8772593059104955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8772593059104955 | validation: 0.9856531175162104]
	TIME [epoch: 0.908 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9444108582213396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9444108582213396 | validation: 0.9684172271227697]
	TIME [epoch: 0.908 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.972701111112881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.972701111112881 | validation: 0.85107680682573]
	TIME [epoch: 0.904 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8110855707438127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8110855707438127 | validation: 0.8688997956919077]
	TIME [epoch: 0.91 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8502272324422754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8502272324422754 | validation: 0.9077181769729665]
	TIME [epoch: 0.908 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8658654211224992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8658654211224992 | validation: 0.8517818138611664]
	TIME [epoch: 0.909 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8169708056402641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8169708056402641 | validation: 0.8449534961837387]
	TIME [epoch: 0.907 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8026487686707641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8026487686707641 | validation: 0.8584182500280563]
	TIME [epoch: 0.905 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8101076532995866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8101076532995866 | validation: 0.8321863011686542]
	TIME [epoch: 0.906 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_166.pth
	Model improved!!!
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8192105292069255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8192105292069255 | validation: 0.9762874616003865]
	TIME [epoch: 0.918 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8904208763739752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8904208763739752 | validation: 0.8986263220954154]
	TIME [epoch: 0.913 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9142565630170427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9142565630170427 | validation: 1.0435173515691039]
	TIME [epoch: 0.914 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9249107863463223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9249107863463223 | validation: 0.8277238512594973]
	TIME [epoch: 0.916 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8026515194557934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8026515194557934 | validation: 0.8363416306355106]
	TIME [epoch: 0.911 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8149420579578156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8149420579578156 | validation: 0.9334510926832609]
	TIME [epoch: 0.91 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.846940002410841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.846940002410841 | validation: 0.8340466039929745]
	TIME [epoch: 0.912 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8052287828232274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8052287828232274 | validation: 0.8486247526451705]
	TIME [epoch: 0.912 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8031261785770564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8031261785770564 | validation: 0.8215025008650612]
	TIME [epoch: 0.911 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_175.pth
	Model improved!!!
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7855758423560221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7855758423560221 | validation: 0.8375276583008502]
	TIME [epoch: 0.912 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7967353483502093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7967353483502093 | validation: 0.7921154120192654]
	TIME [epoch: 0.909 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_177.pth
	Model improved!!!
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8232812362631089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8232812362631089 | validation: 1.0141221726809626]
	TIME [epoch: 0.91 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9930476090647921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9930476090647921 | validation: 0.8283083001987386]
	TIME [epoch: 0.913 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8426603052513929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8426603052513929 | validation: 0.8429407105447672]
	TIME [epoch: 0.909 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8112456063872605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8112456063872605 | validation: 0.8542932908175]
	TIME [epoch: 0.91 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8067870089349597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8067870089349597 | validation: 0.8606892103568029]
	TIME [epoch: 0.91 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8627883345878581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8627883345878581 | validation: 1.0860885530394764]
	TIME [epoch: 0.907 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9722644731166687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9722644731166687 | validation: 0.8485834664588175]
	TIME [epoch: 0.908 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8091809758275637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8091809758275637 | validation: 0.7935339479587165]
	TIME [epoch: 0.906 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8016912462123242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8016912462123242 | validation: 0.8848143216545395]
	TIME [epoch: 0.912 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8329197267035022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8329197267035022 | validation: 0.8040255375272892]
	TIME [epoch: 0.919 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8203622285584959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8203622285584959 | validation: 0.86218708887954]
	TIME [epoch: 0.911 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8412982070313632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8412982070313632 | validation: 0.7980234369030376]
	TIME [epoch: 0.906 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8130298042265772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8130298042265772 | validation: 0.8113648807131384]
	TIME [epoch: 0.908 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7870476862995663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7870476862995663 | validation: 0.7874591785382395]
	TIME [epoch: 0.909 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_191.pth
	Model improved!!!
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7677966147854549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7677966147854549 | validation: 0.7912273437563497]
	TIME [epoch: 0.909 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7669615962273353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7669615962273353 | validation: 0.794770656666168]
	TIME [epoch: 0.908 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7731358399907043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7731358399907043 | validation: 0.8167168422757666]
	TIME [epoch: 0.91 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8010363068251768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8010363068251768 | validation: 0.8930632324055856]
	TIME [epoch: 0.909 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8771632213087008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8771632213087008 | validation: 0.8839716511170214]
	TIME [epoch: 0.912 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9085152690346319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9085152690346319 | validation: 0.8160252882581407]
	TIME [epoch: 0.91 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7833004458855228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7833004458855228 | validation: 0.7582069327294103]
	TIME [epoch: 0.908 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7523027585013611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7523027585013611 | validation: 0.7471945925866679]
	TIME [epoch: 0.908 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_199.pth
	Model improved!!!
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7550190165099744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7550190165099744 | validation: 0.7887104156580853]
	TIME [epoch: 0.909 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7560616346535651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7560616346535651 | validation: 0.7449478781313162]
	TIME [epoch: 24 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7699395134249608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7699395134249608 | validation: 0.908134226247314]
	TIME [epoch: 1.79 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8728480618767911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8728480618767911 | validation: 0.7684484576755083]
	TIME [epoch: 1.79 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8473400347214306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8473400347214306 | validation: 0.7977462537753075]
	TIME [epoch: 1.79 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7792971257289716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7792971257289716 | validation: 0.7037629021030015]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_205.pth
	Model improved!!!
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7270022375361183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7270022375361183 | validation: 0.7132281004993015]
	TIME [epoch: 1.79 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7153403432635322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7153403432635322 | validation: 0.7249017138591541]
	TIME [epoch: 1.78 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7156237026034734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7156237026034734 | validation: 0.6831111616944773]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7244711344844157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7244711344844157 | validation: 0.7357115485190932]
	TIME [epoch: 1.8 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7474040708228229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7474040708228229 | validation: 0.7523676684706433]
	TIME [epoch: 1.79 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8088630390109549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8088630390109549 | validation: 0.8394432368041095]
	TIME [epoch: 1.8 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7892537333728515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7892537333728515 | validation: 0.695570369666069]
	TIME [epoch: 1.8 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7523050716050219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7523050716050219 | validation: 0.7430631013723444]
	TIME [epoch: 1.79 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7174975621997343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7174975621997343 | validation: 0.6314446562342416]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_214.pth
	Model improved!!!
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6931372350254285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6931372350254285 | validation: 0.6441218474703061]
	TIME [epoch: 1.78 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6581323071319901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6581323071319901 | validation: 0.6343816800030591]
	TIME [epoch: 1.78 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.682572878348939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.682572878348939 | validation: 0.761507332436175]
	TIME [epoch: 1.78 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7454541703858757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7454541703858757 | validation: 0.7344703396743725]
	TIME [epoch: 1.78 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7729903013089948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7729903013089948 | validation: 0.7729351454281672]
	TIME [epoch: 1.78 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7299817083212675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7299817083212675 | validation: 0.6277595211890539]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6394254701581362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6394254701581362 | validation: 0.668866443867781]
	TIME [epoch: 1.78 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.629099305640753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.629099305640753 | validation: 0.5910268868495749]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_222.pth
	Model improved!!!
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6161913370246436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6161913370246436 | validation: 0.6889025190508651]
	TIME [epoch: 1.78 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.662411481151153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.662411481151153 | validation: 0.6672447524406507]
	TIME [epoch: 1.78 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6800661152335604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6800661152335604 | validation: 0.6948069966660179]
	TIME [epoch: 1.78 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6426726395658179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6426726395658179 | validation: 0.5910831908580272]
	TIME [epoch: 1.78 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6086959026439115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6086959026439115 | validation: 0.5693839071802842]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_227.pth
	Model improved!!!
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.540117133349321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.540117133349321 | validation: 0.6305920842241073]
	TIME [epoch: 1.78 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.544322641941791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.544322641941791 | validation: 0.5222610090957301]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_229.pth
	Model improved!!!
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5571714396459824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5571714396459824 | validation: 0.6092758592187648]
	TIME [epoch: 1.78 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5332853679061642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5332853679061642 | validation: 0.6299738323039961]
	TIME [epoch: 1.79 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5817782890611544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5817782890611544 | validation: 0.6884790606914768]
	TIME [epoch: 1.78 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.691580094146689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.691580094146689 | validation: 0.569142972255972]
	TIME [epoch: 1.78 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5452441805653017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5452441805653017 | validation: 0.5749299491362051]
	TIME [epoch: 1.78 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47256165173557924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47256165173557924 | validation: 0.49635243669886275]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_235.pth
	Model improved!!!
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4733190440448366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4733190440448366 | validation: 0.6060166617347188]
	TIME [epoch: 1.8 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47778367397316784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47778367397316784 | validation: 0.49622294153225543]
	TIME [epoch: 1.8 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_237.pth
	Model improved!!!
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49085205301139306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49085205301139306 | validation: 0.5703106658862473]
	TIME [epoch: 1.8 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44214619509766406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44214619509766406 | validation: 0.4460590906239801]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41821945944658295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41821945944658295 | validation: 0.8515685098203631]
	TIME [epoch: 1.78 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7028878682358471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7028878682358471 | validation: 0.8120176419174543]
	TIME [epoch: 1.78 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8470145854249757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8470145854249757 | validation: 0.8637178233932247]
	TIME [epoch: 1.78 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8088731049551028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8088731049551028 | validation: 0.6359432847381132]
	TIME [epoch: 1.78 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6473050206025114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6473050206025114 | validation: 0.6020273706623748]
	TIME [epoch: 1.78 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5673571367375798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5673571367375798 | validation: 0.5283552636543529]
	TIME [epoch: 1.79 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.494274731169143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.494274731169143 | validation: 0.4900331007339935]
	TIME [epoch: 1.78 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48180541912192604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48180541912192604 | validation: 0.5293319341489002]
	TIME [epoch: 1.78 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4600123918416766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4600123918416766 | validation: 0.5221622685183726]
	TIME [epoch: 1.78 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4476309256173406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4476309256173406 | validation: 0.4996476681346498]
	TIME [epoch: 1.78 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4565472609056366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4565472609056366 | validation: 0.6536994901547559]
	TIME [epoch: 1.78 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5587561370129344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5587561370129344 | validation: 0.5979734245407734]
	TIME [epoch: 1.78 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5402542085857653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5402542085857653 | validation: 0.49651601278781676]
	TIME [epoch: 1.78 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4276282343625295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4276282343625295 | validation: 0.4913481973288886]
	TIME [epoch: 1.78 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39789854964772703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39789854964772703 | validation: 0.46471677980355425]
	TIME [epoch: 1.78 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4138657396548232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4138657396548232 | validation: 0.5416881338322485]
	TIME [epoch: 1.78 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46871823466877205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46871823466877205 | validation: 0.6113825753378805]
	TIME [epoch: 1.78 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5319436289791147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5319436289791147 | validation: 0.45925039002180257]
	TIME [epoch: 1.78 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4293638453787251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4293638453787251 | validation: 0.542901436963916]
	TIME [epoch: 1.78 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4073758376253233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4073758376253233 | validation: 0.4718294728098947]
	TIME [epoch: 1.78 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5336900485316844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5336900485316844 | validation: 0.7031466089139725]
	TIME [epoch: 1.79 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49270631879970317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49270631879970317 | validation: 0.479705996863612]
	TIME [epoch: 1.78 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4045705323506076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4045705323506076 | validation: 0.42915343204513245]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_262.pth
	Model improved!!!
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3850585827426088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3850585827426088 | validation: 0.5183730924767149]
	TIME [epoch: 1.78 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37413955593532405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37413955593532405 | validation: 0.43004442912852386]
	TIME [epoch: 1.77 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43336629878394695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43336629878394695 | validation: 0.5682055747491516]
	TIME [epoch: 1.78 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4334777314641544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4334777314641544 | validation: 0.5162919934708595]
	TIME [epoch: 1.77 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4393855133113253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4393855133113253 | validation: 0.5084929656184495]
	TIME [epoch: 1.78 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37644605196979725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37644605196979725 | validation: 0.37386927680097215]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_268.pth
	Model improved!!!
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3699992236025021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3699992236025021 | validation: 0.6451901038243281]
	TIME [epoch: 1.79 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4822119555754742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4822119555754742 | validation: 0.4543535296644047]
	TIME [epoch: 1.79 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4238520302022114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4238520302022114 | validation: 0.5503639029035002]
	TIME [epoch: 1.79 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42188421055395786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42188421055395786 | validation: 0.43426129099534916]
	TIME [epoch: 1.78 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3675226069416684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3675226069416684 | validation: 0.5153921456446441]
	TIME [epoch: 1.79 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45528891718414416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45528891718414416 | validation: 0.7237648912700706]
	TIME [epoch: 1.78 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6556906449870598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6556906449870598 | validation: 0.4844456463380848]
	TIME [epoch: 1.79 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43932651785461574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43932651785461574 | validation: 0.5010369829538066]
	TIME [epoch: 1.78 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42265563096428793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42265563096428793 | validation: 0.48270736350597376]
	TIME [epoch: 1.78 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3703119855055899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3703119855055899 | validation: 0.3992936685744175]
	TIME [epoch: 1.79 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35840654867185423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35840654867185423 | validation: 0.6295180015468995]
	TIME [epoch: 1.78 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41547417021214406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41547417021214406 | validation: 0.38377565615607934]
	TIME [epoch: 1.78 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4068005202598228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4068005202598228 | validation: 0.6101661890747506]
	TIME [epoch: 1.78 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4268910157151039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4268910157151039 | validation: 0.3907784018088403]
	TIME [epoch: 1.78 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33148215261563196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33148215261563196 | validation: 0.3474763252326134]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_283.pth
	Model improved!!!
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32563987883369755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32563987883369755 | validation: 0.48890686482422857]
	TIME [epoch: 1.78 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3528126164041903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3528126164041903 | validation: 0.4019606934824085]
	TIME [epoch: 1.78 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4047917846276019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4047917846276019 | validation: 0.6634472937657018]
	TIME [epoch: 1.79 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48306346060606475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48306346060606475 | validation: 0.48756563046529033]
	TIME [epoch: 1.78 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39117093267231967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39117093267231967 | validation: 0.4679635376250231]
	TIME [epoch: 1.79 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4934604968771047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4934604968771047 | validation: 0.5156328506946258]
	TIME [epoch: 1.78 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41621593680189783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41621593680189783 | validation: 0.6003730320600396]
	TIME [epoch: 1.78 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4055263628138127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4055263628138127 | validation: 0.49412976219896954]
	TIME [epoch: 1.78 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4712716553812556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4712716553812556 | validation: 0.5028649047240886]
	TIME [epoch: 1.78 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37266425871224373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37266425871224373 | validation: 0.41141704172561105]
	TIME [epoch: 1.79 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3388503673046765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3388503673046765 | validation: 0.4210823446551255]
	TIME [epoch: 1.79 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3305656974976843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3305656974976843 | validation: 0.3697407177497684]
	TIME [epoch: 1.79 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30381890709998677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30381890709998677 | validation: 0.36053355934978265]
	TIME [epoch: 1.78 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31990818893508555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31990818893508555 | validation: 0.8398553248021859]
	TIME [epoch: 1.79 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8876977818020515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8876977818020515 | validation: 0.6831553800561925]
	TIME [epoch: 1.79 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8568751325539538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8568751325539538 | validation: 0.6381717835415095]
	TIME [epoch: 1.79 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5217147705428171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5217147705428171 | validation: 0.5550590001073926]
	TIME [epoch: 1.79 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4404197929341884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4404197929341884 | validation: 0.47450951114756657]
	TIME [epoch: 1.79 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3737816405943004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3737816405943004 | validation: 0.4255299206885367]
	TIME [epoch: 1.78 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3361211770300422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3361211770300422 | validation: 0.36130846086188967]
	TIME [epoch: 1.78 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3303226514196584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3303226514196584 | validation: 0.39515612088372337]
	TIME [epoch: 1.78 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3076050391230936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3076050391230936 | validation: 0.3342419915236383]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3051402490462822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3051402490462822 | validation: 0.5462161274321373]
	TIME [epoch: 1.79 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4206900647966319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4206900647966319 | validation: 0.4194497693783416]
	TIME [epoch: 1.78 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47788698676231917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47788698676231917 | validation: 0.583432325552889]
	TIME [epoch: 1.78 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42668150045162123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42668150045162123 | validation: 0.4652937592037937]
	TIME [epoch: 1.78 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3400243861223462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3400243861223462 | validation: 0.3538321110953467]
	TIME [epoch: 1.79 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3782979848736476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3782979848736476 | validation: 0.6219188991193028]
	TIME [epoch: 1.79 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43368311927007214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43368311927007214 | validation: 0.4453836004423701]
	TIME [epoch: 1.78 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3521671650443014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3521671650443014 | validation: 0.3515431819442124]
	TIME [epoch: 1.78 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35378203360911714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35378203360911714 | validation: 0.5320548989930984]
	TIME [epoch: 1.78 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38600799211899045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38600799211899045 | validation: 0.41273270270764595]
	TIME [epoch: 1.78 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3056957987525386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3056957987525386 | validation: 0.3051453092715175]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_316.pth
	Model improved!!!
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32810751381289455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32810751381289455 | validation: 0.38512356253439534]
	TIME [epoch: 1.78 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2823639173213147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2823639173213147 | validation: 0.35887516642684975]
	TIME [epoch: 1.78 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31177657102561745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31177657102561745 | validation: 0.5158032308326856]
	TIME [epoch: 1.77 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4409466642159306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4409466642159306 | validation: 0.4055103375552519]
	TIME [epoch: 1.77 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33294800964762017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33294800964762017 | validation: 0.42133740422980587]
	TIME [epoch: 1.78 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35846643901774655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35846643901774655 | validation: 0.5600692195738111]
	TIME [epoch: 1.78 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5146083852469553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5146083852469553 | validation: 0.5731991209325603]
	TIME [epoch: 1.77 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.561099648944296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.561099648944296 | validation: 0.6945793204562928]
	TIME [epoch: 1.77 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5600190419853432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5600190419853432 | validation: 0.6436645410218138]
	TIME [epoch: 1.78 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45532541835147217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45532541835147217 | validation: 0.4529193329172678]
	TIME [epoch: 1.77 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32887816091241706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32887816091241706 | validation: 0.376790069628391]
	TIME [epoch: 1.78 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3638769431915846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3638769431915846 | validation: 0.6274600696426291]
	TIME [epoch: 1.77 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4414141145295931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4414141145295931 | validation: 0.3811435674490864]
	TIME [epoch: 1.8 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30686659944305333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30686659944305333 | validation: 0.31911366289445237]
	TIME [epoch: 1.79 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2923165041686304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2923165041686304 | validation: 0.47968798440355603]
	TIME [epoch: 1.78 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3590684743202769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3590684743202769 | validation: 0.3288875322541352]
	TIME [epoch: 1.78 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3471225419434073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3471225419434073 | validation: 0.3759893619395886]
	TIME [epoch: 1.78 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29547398812541154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29547398812541154 | validation: 0.34122098556833547]
	TIME [epoch: 1.79 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2757470476904496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2757470476904496 | validation: 0.5024401372441739]
	TIME [epoch: 1.78 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37609587472458267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37609587472458267 | validation: 0.33311373995850463]
	TIME [epoch: 1.79 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29155319808315716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29155319808315716 | validation: 0.3109669922035501]
	TIME [epoch: 1.78 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25760115925779514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25760115925779514 | validation: 0.3282828807546412]
	TIME [epoch: 1.78 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2975291211946545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2975291211946545 | validation: 0.6584652342771558]
	TIME [epoch: 1.78 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4836642673042231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4836642673042231 | validation: 0.3796663311149805]
	TIME [epoch: 1.79 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29704969584497753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29704969584497753 | validation: 0.3355425958193894]
	TIME [epoch: 1.78 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29550166863580285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29550166863580285 | validation: 0.4230877278758544]
	TIME [epoch: 1.78 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2907459048654502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2907459048654502 | validation: 0.3041955551172943]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_343.pth
	Model improved!!!
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25279444525878303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25279444525878303 | validation: 0.3462340824472199]
	TIME [epoch: 1.79 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32932796951576876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32932796951576876 | validation: 0.47744908331989155]
	TIME [epoch: 1.78 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4063894955901587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4063894955901587 | validation: 0.5595200994464355]
	TIME [epoch: 1.78 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4474729010259976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4474729010259976 | validation: 0.38686205531269713]
	TIME [epoch: 1.79 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2859946539655443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2859946539655443 | validation: 0.4408695484876948]
	TIME [epoch: 1.78 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42089125054363863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42089125054363863 | validation: 0.5214677236308092]
	TIME [epoch: 1.78 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3968173110188933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3968173110188933 | validation: 0.5137058646080044]
	TIME [epoch: 1.78 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40515317766740794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40515317766740794 | validation: 0.41223224705091505]
	TIME [epoch: 1.78 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32062359401169716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32062359401169716 | validation: 0.38249389020054475]
	TIME [epoch: 1.78 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3203758996324542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3203758996324542 | validation: 0.4658339426215625]
	TIME [epoch: 1.79 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3301291179550087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3301291179550087 | validation: 0.39808316267170374]
	TIME [epoch: 1.78 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2995566605264343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2995566605264343 | validation: 0.4998398469623145]
	TIME [epoch: 1.78 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5265011249405888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5265011249405888 | validation: 0.42233770333132664]
	TIME [epoch: 1.78 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41754625465121886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41754625465121886 | validation: 0.29456573327103414]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_357.pth
	Model improved!!!
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2644293145965101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2644293145965101 | validation: 0.4366124743082376]
	TIME [epoch: 1.78 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3034975671271547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3034975671271547 | validation: 0.26314754334147716]
	TIME [epoch: 1.79 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_359.pth
	Model improved!!!
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2172109592361686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2172109592361686 | validation: 0.3144135690827116]
	TIME [epoch: 1.8 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22887671802947718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22887671802947718 | validation: 0.29458097426382207]
	TIME [epoch: 1.78 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27743824400071504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27743824400071504 | validation: 0.6697227616712302]
	TIME [epoch: 1.78 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5316847427107899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5316847427107899 | validation: 0.6129948567716275]
	TIME [epoch: 1.78 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46264408731837287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46264408731837287 | validation: 0.4259713079732557]
	TIME [epoch: 1.78 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.457067536266631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.457067536266631 | validation: 0.38389792563675096]
	TIME [epoch: 1.78 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3530453026976562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3530453026976562 | validation: 0.34688376164092266]
	TIME [epoch: 1.78 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28130888084415956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28130888084415956 | validation: 0.31358625828503917]
	TIME [epoch: 1.78 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24671341337858926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24671341337858926 | validation: 0.2614596720394626]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_368.pth
	Model improved!!!
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20802629212798557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20802629212798557 | validation: 0.2826116412490253]
	TIME [epoch: 1.78 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.211807318104526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.211807318104526 | validation: 0.21731980102746734]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_370.pth
	Model improved!!!
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22146447710004202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22146447710004202 | validation: 0.40039870161836655]
	TIME [epoch: 1.79 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2969991914302749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2969991914302749 | validation: 0.2955659189228856]
	TIME [epoch: 1.78 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27478090413159884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27478090413159884 | validation: 0.27898498212201867]
	TIME [epoch: 1.78 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20153990620926301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20153990620926301 | validation: 0.2608374553711358]
	TIME [epoch: 1.78 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19954247481583615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19954247481583615 | validation: 0.3407581984480598]
	TIME [epoch: 1.78 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3292200629717757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3292200629717757 | validation: 0.5439244125159902]
	TIME [epoch: 1.78 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47887425955149837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47887425955149837 | validation: 0.47563117988759035]
	TIME [epoch: 1.78 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3787474216964855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3787474216964855 | validation: 0.2597857226691122]
	TIME [epoch: 1.78 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20098646417617272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20098646417617272 | validation: 0.39712997572229924]
	TIME [epoch: 1.78 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36171948418621486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36171948418621486 | validation: 0.2996822400390388]
	TIME [epoch: 1.78 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2793329797317536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2793329797317536 | validation: 0.3045528796769621]
	TIME [epoch: 1.78 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23626419222122905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23626419222122905 | validation: 0.27245946491438067]
	TIME [epoch: 1.78 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20643915687943654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20643915687943654 | validation: 0.22608219506369265]
	TIME [epoch: 1.78 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21341773051106894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21341773051106894 | validation: 0.5882189634694432]
	TIME [epoch: 1.78 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8244425866424072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8244425866424072 | validation: 0.48036362196129834]
	TIME [epoch: 1.79 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.715799523198816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.715799523198816 | validation: 0.5105117508748332]
	TIME [epoch: 1.78 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5747837579315312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5747837579315312 | validation: 0.35797299422629997]
	TIME [epoch: 1.78 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34056754808582995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34056754808582995 | validation: 0.33209747207226537]
	TIME [epoch: 1.78 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2842929515783771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2842929515783771 | validation: 0.33098381164704876]
	TIME [epoch: 1.78 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2219525385578792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2219525385578792 | validation: 0.2223211515584514]
	TIME [epoch: 1.78 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19417026009363575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19417026009363575 | validation: 0.24927177571640088]
	TIME [epoch: 1.78 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21311411466948837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21311411466948837 | validation: 0.25654083914604175]
	TIME [epoch: 1.78 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23161223069023518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23161223069023518 | validation: 0.35659509581336113]
	TIME [epoch: 1.78 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29937161907882065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29937161907882065 | validation: 0.3019224051617639]
	TIME [epoch: 1.78 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2533245936347669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2533245936347669 | validation: 0.2753715295355965]
	TIME [epoch: 1.78 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24895283913289146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24895283913289146 | validation: 0.3242040473590785]
	TIME [epoch: 1.78 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27327167611246056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27327167611246056 | validation: 0.338704165134942]
	TIME [epoch: 1.78 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2510204935379431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2510204935379431 | validation: 0.2185437335864978]
	TIME [epoch: 1.79 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1880356223024724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1880356223024724 | validation: 0.24075574441334094]
	TIME [epoch: 1.78 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1726951494407286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1726951494407286 | validation: 0.20291117396489003]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_400.pth
	Model improved!!!
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17139959210326614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17139959210326614 | validation: 0.2157752594795709]
	TIME [epoch: 1.78 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2112589079339908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2112589079339908 | validation: 0.3408869985726364]
	TIME [epoch: 1.78 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2924917828627247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2924917828627247 | validation: 0.36061170124764297]
	TIME [epoch: 1.77 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3676421605509592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3676421605509592 | validation: 0.23136147526441275]
	TIME [epoch: 1.78 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1775528946188321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1775528946188321 | validation: 0.14591834634622813]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_405.pth
	Model improved!!!
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13097216205355308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13097216205355308 | validation: 0.2316862964499654]
	TIME [epoch: 1.78 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18802371155937983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18802371155937983 | validation: 0.30227882214834817]
	TIME [epoch: 1.78 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2633024141413122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2633024141413122 | validation: 0.4812736114912588]
	TIME [epoch: 1.78 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4129979730211921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4129979730211921 | validation: 0.36228994951430094]
	TIME [epoch: 1.78 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2617553553289656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2617553553289656 | validation: 0.3172177643908263]
	TIME [epoch: 1.79 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28815723489731404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28815723489731404 | validation: 0.34329001725234004]
	TIME [epoch: 1.78 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29501219074242263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29501219074242263 | validation: 0.27477277210739876]
	TIME [epoch: 1.78 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1976101253949114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1976101253949114 | validation: 0.20607513279951292]
	TIME [epoch: 1.78 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2318605859570894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2318605859570894 | validation: 0.27673464415099785]
	TIME [epoch: 1.77 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19305539238104644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19305539238104644 | validation: 0.1596023019379093]
	TIME [epoch: 1.78 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15777900501398606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15777900501398606 | validation: 0.21881007300770353]
	TIME [epoch: 1.77 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1483949627235812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1483949627235812 | validation: 0.2038192708774975]
	TIME [epoch: 1.78 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17610121280107827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17610121280107827 | validation: 0.26129339284593134]
	TIME [epoch: 1.78 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21672653883436924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21672653883436924 | validation: 0.39553637046271684]
	TIME [epoch: 1.78 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3209307597450307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3209307597450307 | validation: 0.37122614449807695]
	TIME [epoch: 1.78 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4264052447998945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4264052447998945 | validation: 0.27174395341102126]
	TIME [epoch: 1.78 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21150658884648338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21150658884648338 | validation: 0.2047067526133941]
	TIME [epoch: 1.78 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.171161862149483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.171161862149483 | validation: 0.1887693925296838]
	TIME [epoch: 1.78 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15002600267520144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15002600267520144 | validation: 0.21090790181282415]
	TIME [epoch: 1.78 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1638597557625893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1638597557625893 | validation: 0.20122735327848892]
	TIME [epoch: 1.78 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16171200134203276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16171200134203276 | validation: 0.22681532195845253]
	TIME [epoch: 1.78 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21990641106661662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21990641106661662 | validation: 0.2759897314931375]
	TIME [epoch: 1.78 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2331843817605399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2331843817605399 | validation: 0.15642780585840343]
	TIME [epoch: 1.78 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17514838923859466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17514838923859466 | validation: 0.20705041868478827]
	TIME [epoch: 1.78 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14174337780991014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14174337780991014 | validation: 0.11897666985262262]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_430.pth
	Model improved!!!
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11949582675082576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11949582675082576 | validation: 0.16991309144689157]
	TIME [epoch: 1.78 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11731329153692041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11731329153692041 | validation: 0.1996852386895288]
	TIME [epoch: 1.78 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17160189903641823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17160189903641823 | validation: 0.41860427968314995]
	TIME [epoch: 1.78 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3280523778856116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3280523778856116 | validation: 0.1932349630227341]
	TIME [epoch: 1.79 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1459704933879987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1459704933879987 | validation: 0.2043303951976695]
	TIME [epoch: 1.79 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2271010963360436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2271010963360436 | validation: 0.30267196292047993]
	TIME [epoch: 1.78 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2425231779112091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2425231779112091 | validation: 0.43719082829414424]
	TIME [epoch: 1.78 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49658089829151125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49658089829151125 | validation: 0.5738420021487842]
	TIME [epoch: 1.78 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.421740425719093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.421740425719093 | validation: 0.3400283379399058]
	TIME [epoch: 1.78 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3352645460173065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3352645460173065 | validation: 0.40251855372489626]
	TIME [epoch: 1.78 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3283482927250782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3283482927250782 | validation: 0.17336133805406687]
	TIME [epoch: 1.78 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1262146507274169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1262146507274169 | validation: 0.15138844387254974]
	TIME [epoch: 1.78 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1482410509916787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1482410509916787 | validation: 0.21397504720600066]
	TIME [epoch: 1.78 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15085273330975665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15085273330975665 | validation: 0.13208344700325825]
	TIME [epoch: 1.78 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.147582356589812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.147582356589812 | validation: 0.2460481541478058]
	TIME [epoch: 1.78 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16032058251665438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16032058251665438 | validation: 0.1535689580064405]
	TIME [epoch: 1.78 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18232009577713662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18232009577713662 | validation: 0.2750871755896715]
	TIME [epoch: 1.78 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24481437110407833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24481437110407833 | validation: 0.27432709666065663]
	TIME [epoch: 1.78 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26014042814332483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26014042814332483 | validation: 0.2016566075059818]
	TIME [epoch: 1.78 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15707762414367474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15707762414367474 | validation: 0.14465600313509994]
	TIME [epoch: 1.78 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10243813798346496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10243813798346496 | validation: 0.11960837040882671]
	TIME [epoch: 1.78 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.103248315335379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.103248315335379 | validation: 0.19765096655694048]
	TIME [epoch: 1.78 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14191274060680253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14191274060680253 | validation: 0.2776448464608416]
	TIME [epoch: 1.78 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3188736192073501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3188736192073501 | validation: 0.33634499276770713]
	TIME [epoch: 1.77 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3099435103404494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3099435103404494 | validation: 0.12140154240960328]
	TIME [epoch: 1.78 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12615893124106736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12615893124106736 | validation: 0.11965347950027888]
	TIME [epoch: 1.78 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10073417628357088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10073417628357088 | validation: 0.19679119347600116]
	TIME [epoch: 1.78 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.155961070104701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.155961070104701 | validation: 0.25858576591144916]
	TIME [epoch: 1.78 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24667836052788888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24667836052788888 | validation: 0.3111965583634046]
	TIME [epoch: 1.78 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2572345023172337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2572345023172337 | validation: 0.23066906035588436]
	TIME [epoch: 1.79 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1972124953233746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1972124953233746 | validation: 0.13599728711126963]
	TIME [epoch: 1.78 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17869229468929745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17869229468929745 | validation: 0.24550433415862774]
	TIME [epoch: 1.78 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1671968625884612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1671968625884612 | validation: 0.22140499120558932]
	TIME [epoch: 1.78 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1956812316838334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1956812316838334 | validation: 0.24834323532708]
	TIME [epoch: 1.78 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20803387579959767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20803387579959767 | validation: 0.16490360216920497]
	TIME [epoch: 1.78 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1942528326665295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1942528326665295 | validation: 0.18603197285978434]
	TIME [epoch: 1.78 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14079358384512028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14079358384512028 | validation: 0.12715240459504834]
	TIME [epoch: 1.78 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1263397631535192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1263397631535192 | validation: 0.19450898889337198]
	TIME [epoch: 1.78 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13925600373111444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13925600373111444 | validation: 0.16796964735914607]
	TIME [epoch: 1.78 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20667275654192202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20667275654192202 | validation: 0.26196024717926286]
	TIME [epoch: 1.78 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21626587882525983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21626587882525983 | validation: 0.28063730465208225]
	TIME [epoch: 1.78 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24961570169059336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24961570169059336 | validation: 0.17058922185699874]
	TIME [epoch: 1.78 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17045123196688458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17045123196688458 | validation: 0.237500514716223]
	TIME [epoch: 1.78 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20216556062177282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20216556062177282 | validation: 0.3068159906826533]
	TIME [epoch: 1.78 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2906232428680432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2906232428680432 | validation: 0.24323037365113587]
	TIME [epoch: 1.78 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2130960333495218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2130960333495218 | validation: 0.12460688255776449]
	TIME [epoch: 1.78 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13205664828881003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13205664828881003 | validation: 0.19350695090138617]
	TIME [epoch: 1.78 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12204629385237592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12204629385237592 | validation: 0.12606715368420934]
	TIME [epoch: 1.78 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09716933313440862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09716933313440862 | validation: 0.11218952318233494]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_479.pth
	Model improved!!!
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08846976991730493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08846976991730493 | validation: 0.11437237655755456]
	TIME [epoch: 1.78 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09602735449359562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09602735449359562 | validation: 0.1323315170674669]
	TIME [epoch: 1.77 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11978249977961221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11978249977961221 | validation: 0.1981438163013229]
	TIME [epoch: 1.78 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1699398050858409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1699398050858409 | validation: 0.20882289637006496]
	TIME [epoch: 1.78 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18870420310025957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18870420310025957 | validation: 0.19449329251761052]
	TIME [epoch: 1.78 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1413400281297835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1413400281297835 | validation: 0.17420192950994498]
	TIME [epoch: 1.79 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20592022009382022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20592022009382022 | validation: 0.333226534326005]
	TIME [epoch: 1.78 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31530542891919583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31530542891919583 | validation: 0.28153059201944414]
	TIME [epoch: 1.78 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2593760415245984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2593760415245984 | validation: 0.14399267382043243]
	TIME [epoch: 1.78 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1019814376004163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1019814376004163 | validation: 0.11535058808600739]
	TIME [epoch: 1.78 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08160131509228478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08160131509228478 | validation: 0.08871731212451912]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_490.pth
	Model improved!!!
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07832270622526843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07832270622526843 | validation: 0.11837668963674382]
	TIME [epoch: 1.78 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08388182481950017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08388182481950017 | validation: 0.10076079373816932]
	TIME [epoch: 1.78 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10532800472954343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10532800472954343 | validation: 0.18462738316375601]
	TIME [epoch: 1.78 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15423581020471613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15423581020471613 | validation: 0.21933716432461714]
	TIME [epoch: 1.78 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22903556899544242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22903556899544242 | validation: 0.16791603565595173]
	TIME [epoch: 1.78 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13842618434408727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13842618434408727 | validation: 0.08912689263210592]
	TIME [epoch: 1.78 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11989721629639596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11989721629639596 | validation: 0.22059104856977277]
	TIME [epoch: 1.78 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1598933699629648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1598933699629648 | validation: 0.16052877075470007]
	TIME [epoch: 1.78 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17747108443785134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17747108443785134 | validation: 0.20552094128330092]
	TIME [epoch: 1.78 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16472532434944526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16472532434944526 | validation: 0.11188232024081901]
	TIME [epoch: 1.78 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13955408007285353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13955408007285353 | validation: 0.16096227186646192]
	TIME [epoch: 25.5 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1310803244846051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1310803244846051 | validation: 0.16957490021726068]
	TIME [epoch: 3.54 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15646890879284914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15646890879284914 | validation: 0.17312572892725056]
	TIME [epoch: 3.52 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12956224655147286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12956224655147286 | validation: 0.14742219878470234]
	TIME [epoch: 3.53 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11727007618549833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11727007618549833 | validation: 0.11061521779953176]
	TIME [epoch: 3.52 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08699629378560567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08699629378560567 | validation: 0.13275275032416003]
	TIME [epoch: 3.54 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11340423300271855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11340423300271855 | validation: 0.19982992300068053]
	TIME [epoch: 3.53 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20255929443549975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20255929443549975 | validation: 0.30558193106173337]
	TIME [epoch: 3.53 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32428662768685323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32428662768685323 | validation: 0.2651183016547446]
	TIME [epoch: 3.53 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22015753375557315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22015753375557315 | validation: 0.09500144545644573]
	TIME [epoch: 3.53 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10757294222387158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10757294222387158 | validation: 0.09144704634307654]
	TIME [epoch: 3.52 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06639358846130695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06639358846130695 | validation: 0.07959466972249434]
	TIME [epoch: 3.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_512.pth
	Model improved!!!
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0718157124754205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0718157124754205 | validation: 0.08223569265136471]
	TIME [epoch: 3.53 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08655496733141636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08655496733141636 | validation: 0.1345832757351842]
	TIME [epoch: 3.55 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1405248186758987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1405248186758987 | validation: 0.18017373554680038]
	TIME [epoch: 3.54 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1538119086177639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1538119086177639 | validation: 0.1754997059344786]
	TIME [epoch: 3.54 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15005600560394622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15005600560394622 | validation: 0.15924695068567604]
	TIME [epoch: 3.53 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1371437536953548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1371437536953548 | validation: 0.15310634772158926]
	TIME [epoch: 3.53 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13927531033392324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13927531033392324 | validation: 0.15863086025845963]
	TIME [epoch: 3.52 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1287433756054566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1287433756054566 | validation: 0.20644469074869015]
	TIME [epoch: 3.53 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24069536332140576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24069536332140576 | validation: 0.27456547591389135]
	TIME [epoch: 3.52 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25808152197515344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25808152197515344 | validation: 0.1135309636707798]
	TIME [epoch: 3.53 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13339007873574601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13339007873574601 | validation: 0.0973190543820816]
	TIME [epoch: 3.53 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06272539906379629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06272539906379629 | validation: 0.05965907717682779]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_524.pth
	Model improved!!!
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045708879333314474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045708879333314474 | validation: 0.053905982877233416]
	TIME [epoch: 3.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_525.pth
	Model improved!!!
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05229577954242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05229577954242 | validation: 0.09168047933990514]
	TIME [epoch: 3.54 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10846755942345858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10846755942345858 | validation: 0.21827057699708724]
	TIME [epoch: 3.55 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20989103774506415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20989103774506415 | validation: 0.09284789908288596]
	TIME [epoch: 3.54 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12281382323678784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12281382323678784 | validation: 0.1071185712719942]
	TIME [epoch: 3.54 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07792122498774633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07792122498774633 | validation: 0.10495031579716728]
	TIME [epoch: 3.53 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0655931904432222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0655931904432222 | validation: 0.11940300797973441]
	TIME [epoch: 3.53 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08863881353451365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08863881353451365 | validation: 0.20561551539377182]
	TIME [epoch: 3.52 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15612434469655043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15612434469655043 | validation: 0.18754681049652128]
	TIME [epoch: 3.53 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.207108328633533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.207108328633533 | validation: 0.24770241907967006]
	TIME [epoch: 3.54 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23733227120171485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23733227120171485 | validation: 0.11064624569435169]
	TIME [epoch: 3.52 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16142017161358993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16142017161358993 | validation: 0.21333183018191026]
	TIME [epoch: 3.53 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1701830963069009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1701830963069009 | validation: 0.26386337453886405]
	TIME [epoch: 3.53 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24225095632219432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24225095632219432 | validation: 0.17636261422590271]
	TIME [epoch: 3.53 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1427384856929091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1427384856929091 | validation: 0.0955763333069916]
	TIME [epoch: 3.54 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13891046419275474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13891046419275474 | validation: 0.1245774193485067]
	TIME [epoch: 3.54 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09760992125460106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09760992125460106 | validation: 0.06323436586626767]
	TIME [epoch: 3.53 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06672516396065473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06672516396065473 | validation: 0.09316348255148704]
	TIME [epoch: 3.52 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06524301269939321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06524301269939321 | validation: 0.08770465791063535]
	TIME [epoch: 3.52 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07449233383734602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07449233383734602 | validation: 0.12436669866638822]
	TIME [epoch: 3.53 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09205263398363586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09205263398363586 | validation: 0.12423192854768583]
	TIME [epoch: 3.53 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11798610025818071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11798610025818071 | validation: 0.1519688270201509]
	TIME [epoch: 3.53 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13381710741033823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13381710741033823 | validation: 0.1224613087433521]
	TIME [epoch: 3.53 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16911298460781476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16911298460781476 | validation: 0.1848757982894863]
	TIME [epoch: 3.53 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17881429124659654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17881429124659654 | validation: 0.10131387205644223]
	TIME [epoch: 3.52 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14078280645738825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14078280645738825 | validation: 0.16827033586778872]
	TIME [epoch: 3.52 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14388906562284717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14388906562284717 | validation: 0.20036026118145425]
	TIME [epoch: 3.52 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2441311890866081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2441311890866081 | validation: 0.15983883150972328]
	TIME [epoch: 3.53 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17637393097108015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17637393097108015 | validation: 0.08912310227168216]
	TIME [epoch: 3.54 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06688549124226223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06688549124226223 | validation: 0.14526639331217872]
	TIME [epoch: 3.55 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07494300693596694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07494300693596694 | validation: 0.05336262641188427]
	TIME [epoch: 3.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_555.pth
	Model improved!!!
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07351280162030024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07351280162030024 | validation: 0.08886218972132923]
	TIME [epoch: 3.53 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06852117563335512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06852117563335512 | validation: 0.05833302506675562]
	TIME [epoch: 3.53 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06956103072738454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06956103072738454 | validation: 0.1604311888094745]
	TIME [epoch: 3.53 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11932565425243154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11932565425243154 | validation: 0.228727072330015]
	TIME [epoch: 3.53 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2276761272843726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2276761272843726 | validation: 0.14515253759959498]
	TIME [epoch: 3.52 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1312249986316796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1312249986316796 | validation: 0.06193273497787817]
	TIME [epoch: 3.52 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0829606285141732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0829606285141732 | validation: 0.12543044790397606]
	TIME [epoch: 3.52 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0989640684232294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0989640684232294 | validation: 0.15709081041608253]
	TIME [epoch: 3.52 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1572553511029248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1572553511029248 | validation: 0.20282452636461423]
	TIME [epoch: 3.52 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19108112205298666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19108112205298666 | validation: 0.17675631716228946]
	TIME [epoch: 3.53 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22269989381972874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22269989381972874 | validation: 0.10324867117266111]
	TIME [epoch: 3.54 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10500862129740643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10500862129740643 | validation: 0.07470518073891508]
	TIME [epoch: 3.54 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062495350240742535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062495350240742535 | validation: 0.08592680621896205]
	TIME [epoch: 3.52 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053642244548870566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053642244548870566 | validation: 0.0712487431848137]
	TIME [epoch: 3.52 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0728576020905732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0728576020905732 | validation: 0.11233351487998067]
	TIME [epoch: 3.52 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09339790424023087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09339790424023087 | validation: 0.10168480236727091]
	TIME [epoch: 3.52 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12261282034035803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12261282034035803 | validation: 0.17155905752563322]
	TIME [epoch: 3.53 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15281504802187573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15281504802187573 | validation: 0.11192378036651052]
	TIME [epoch: 3.52 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1657349827818413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1657349827818413 | validation: 0.15789724015395118]
	TIME [epoch: 3.52 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14433726371103098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14433726371103098 | validation: 0.13490776189141948]
	TIME [epoch: 3.52 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14923769010206323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14923769010206323 | validation: 0.14402784628237522]
	TIME [epoch: 3.52 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13873172625230992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13873172625230992 | validation: 0.11613785492033056]
	TIME [epoch: 3.52 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10183633541537551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10183633541537551 | validation: 0.0806609945202189]
	TIME [epoch: 3.52 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06807572647862227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06807572647862227 | validation: 0.05704518906289108]
	TIME [epoch: 3.54 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057028339991862395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057028339991862395 | validation: 0.06963643223894576]
	TIME [epoch: 3.53 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05869477121844551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05869477121844551 | validation: 0.07375480389221703]
	TIME [epoch: 3.53 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06691552048209022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06691552048209022 | validation: 0.11582824802298744]
	TIME [epoch: 3.53 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09041316628783325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09041316628783325 | validation: 0.08774502592698452]
	TIME [epoch: 3.53 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10279138705592106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10279138705592106 | validation: 0.1514813591761873]
	TIME [epoch: 3.53 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13787574700671532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13787574700671532 | validation: 0.07244448106383028]
	TIME [epoch: 3.53 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13544497899488409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13544497899488409 | validation: 0.17487350030361243]
	TIME [epoch: 3.53 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14674255805783515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14674255805783515 | validation: 0.15707493235800118]
	TIME [epoch: 3.52 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18130508101285336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18130508101285336 | validation: 0.14393097413888126]
	TIME [epoch: 3.52 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13430756489800835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13430756489800835 | validation: 0.09211614052070588]
	TIME [epoch: 3.52 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10209346313046234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10209346313046234 | validation: 0.09111292168940129]
	TIME [epoch: 3.53 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0709013633808839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0709013633808839 | validation: 0.10607477667136639]
	TIME [epoch: 3.53 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09722397216864499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09722397216864499 | validation: 0.1408341673152504]
	TIME [epoch: 3.53 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11648709577385642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11648709577385642 | validation: 0.12051952433171165]
	TIME [epoch: 3.54 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13161807996433256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13161807996433256 | validation: 0.10944904938054241]
	TIME [epoch: 3.52 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12138864651384594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12138864651384594 | validation: 0.10408067962545996]
	TIME [epoch: 3.52 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09142054240942712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09142054240942712 | validation: 0.07793528188330356]
	TIME [epoch: 3.52 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06916503042068735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06916503042068735 | validation: 0.044349512874207124]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_597.pth
	Model improved!!!
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060999139944787645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060999139944787645 | validation: 0.08944927291743598]
	TIME [epoch: 3.53 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07223495024821071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07223495024821071 | validation: 0.0737874982541024]
	TIME [epoch: 3.53 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10471833508044892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10471833508044892 | validation: 0.172697629289086]
	TIME [epoch: 3.54 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15742413588257376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15742413588257376 | validation: 0.17891312148524519]
	TIME [epoch: 3.53 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2007989385538992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2007989385538992 | validation: 0.14364331319030801]
	TIME [epoch: 3.53 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13159722619331385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13159722619331385 | validation: 0.11140992657583869]
	TIME [epoch: 3.53 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1222499934464583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1222499934464583 | validation: 0.1373621426938068]
	TIME [epoch: 3.53 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12310972513911499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12310972513911499 | validation: 0.11103383493665187]
	TIME [epoch: 3.53 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10855186098750924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10855186098750924 | validation: 0.06938356952333398]
	TIME [epoch: 3.52 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08046388787806158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08046388787806158 | validation: 0.08631322976427638]
	TIME [epoch: 3.52 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.063238997718528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.063238997718528 | validation: 0.046807370086431266]
	TIME [epoch: 3.52 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05493660368664157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05493660368664157 | validation: 0.0812944162039757]
	TIME [epoch: 3.52 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05668126663774939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05668126663774939 | validation: 0.07072736502617831]
	TIME [epoch: 3.52 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07974272074548673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07974272074548673 | validation: 0.14305487151202897]
	TIME [epoch: 3.53 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13942229827743888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13942229827743888 | validation: 0.15051612117103008]
	TIME [epoch: 3.53 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17821433293315303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17821433293315303 | validation: 0.14940610906362917]
	TIME [epoch: 3.53 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08909432811897638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08909432811897638 | validation: 0.050601804079184645]
	TIME [epoch: 3.53 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051832257006250194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051832257006250194 | validation: 0.081889751005671]
	TIME [epoch: 3.52 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05767719167419065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05767719167419065 | validation: 0.11674327335668329]
	TIME [epoch: 3.52 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10623903847468741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10623903847468741 | validation: 0.14769836032149328]
	TIME [epoch: 3.53 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16596028756357684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16596028756357684 | validation: 0.1646901413904699]
	TIME [epoch: 3.53 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1670473348365083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1670473348365083 | validation: 0.09259059564546908]
	TIME [epoch: 3.53 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11946132803816799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11946132803816799 | validation: 0.14096138731305327]
	TIME [epoch: 3.53 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11638781502870052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11638781502870052 | validation: 0.13865100477660866]
	TIME [epoch: 3.53 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1712048473387934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1712048473387934 | validation: 0.14625075990989686]
	TIME [epoch: 3.52 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14345209802431214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14345209802431214 | validation: 0.05177035178364329]
	TIME [epoch: 3.53 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07586224620188985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07586224620188985 | validation: 0.062007471098959134]
	TIME [epoch: 3.52 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04808608881296749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04808608881296749 | validation: 0.047455806512978586]
	TIME [epoch: 3.53 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04536805526702277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04536805526702277 | validation: 0.050596113591491514]
	TIME [epoch: 3.52 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0518301925391231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0518301925391231 | validation: 0.07751964794143806]
	TIME [epoch: 3.52 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.068908971430554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.068908971430554 | validation: 0.07430401893332089]
	TIME [epoch: 3.52 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09811778788794871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09811778788794871 | validation: 0.12730523847451866]
	TIME [epoch: 3.52 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11835503694420563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11835503694420563 | validation: 0.09812122793037575]
	TIME [epoch: 3.53 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1181004789387814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1181004789387814 | validation: 0.09937870285605085]
	TIME [epoch: 3.54 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09629820274484319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09629820274484319 | validation: 0.08554247600477556]
	TIME [epoch: 3.53 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08936074889405907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08936074889405907 | validation: 0.0925269630556169]
	TIME [epoch: 3.52 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1047385023300259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1047385023300259 | validation: 0.1642380712808182]
	TIME [epoch: 3.52 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14425417207948327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14425417207948327 | validation: 0.161993136775198]
	TIME [epoch: 3.52 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16691418012352824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16691418012352824 | validation: 0.10084576563160066]
	TIME [epoch: 3.52 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08669616990580112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08669616990580112 | validation: 0.05193359984429259]
	TIME [epoch: 3.52 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061499978372456764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061499978372456764 | validation: 0.0866227217386723]
	TIME [epoch: 3.53 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06791968851367765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06791968851367765 | validation: 0.07152102071498177]
	TIME [epoch: 3.52 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08284723147095743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08284723147095743 | validation: 0.10146758796847372]
	TIME [epoch: 3.53 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08939908334766311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08939908334766311 | validation: 0.07431312745152098]
	TIME [epoch: 3.53 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08686639473183858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08686639473183858 | validation: 0.07573295751193618]
	TIME [epoch: 3.53 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08046585946539084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08046585946539084 | validation: 0.06971568390791043]
	TIME [epoch: 3.53 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07354517005978065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07354517005978065 | validation: 0.05878326565715302]
	TIME [epoch: 3.53 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06689091221729568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06689091221729568 | validation: 0.05758668938972027]
	TIME [epoch: 3.53 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057846730483971424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057846730483971424 | validation: 0.05599882610728007]
	TIME [epoch: 3.53 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045767180093136774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045767180093136774 | validation: 0.06896562649567738]
	TIME [epoch: 3.53 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050014458841919476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050014458841919476 | validation: 0.0819881492572849]
	TIME [epoch: 3.52 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07184819428381431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07184819428381431 | validation: 0.12564220632398926]
	TIME [epoch: 3.54 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1021112750111342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1021112750111342 | validation: 0.10151842301296764]
	TIME [epoch: 3.52 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10080345657172092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10080345657172092 | validation: 0.12174433214060652]
	TIME [epoch: 3.53 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09278068389511639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09278068389511639 | validation: 0.08772405187248565]
	TIME [epoch: 3.52 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11670689350671751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11670689350671751 | validation: 0.18413914067730575]
	TIME [epoch: 3.52 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19568604878805937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19568604878805937 | validation: 0.17765009989304825]
	TIME [epoch: 3.52 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2030958834065026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2030958834065026 | validation: 0.07494706875180121]
	TIME [epoch: 3.53 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07401801149650782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07401801149650782 | validation: 0.08088095040379073]
	TIME [epoch: 3.52 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09754017009085005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09754017009085005 | validation: 0.1393586481613491]
	TIME [epoch: 3.53 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11144016319622814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11144016319622814 | validation: 0.10824251732335277]
	TIME [epoch: 3.54 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10851658564832539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10851658564832539 | validation: 0.08708679866785263]
	TIME [epoch: 3.53 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09057093972095219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09057093972095219 | validation: 0.07139190424730874]
	TIME [epoch: 3.52 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057961861048146796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057961861048146796 | validation: 0.04195385337144052]
	TIME [epoch: 3.54 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_661.pth
	Model improved!!!
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041830948319735856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041830948319735856 | validation: 0.034945018099588734]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_662.pth
	Model improved!!!
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04068012608185269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04068012608185269 | validation: 0.06286388562353055]
	TIME [epoch: 3.53 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056604498551400415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056604498551400415 | validation: 0.06165222536195958]
	TIME [epoch: 3.52 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10029269469981426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10029269469981426 | validation: 0.11423430051161194]
	TIME [epoch: 3.53 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11740079996621665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11740079996621665 | validation: 0.10101564936237693]
	TIME [epoch: 3.52 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11567474339409571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11567474339409571 | validation: 0.13790128786389902]
	TIME [epoch: 3.53 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11601827415278698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11601827415278698 | validation: 0.13611883175801817]
	TIME [epoch: 3.53 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1596603681045116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1596603681045116 | validation: 0.13098934875202875]
	TIME [epoch: 3.53 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14875651991839747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14875651991839747 | validation: 0.13105965533418798]
	TIME [epoch: 3.54 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1075995495791701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1075995495791701 | validation: 0.10179422957994358]
	TIME [epoch: 3.52 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08403703352668325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08403703352668325 | validation: 0.06322423638704532]
	TIME [epoch: 3.54 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0560297706102941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0560297706102941 | validation: 0.04592117107635172]
	TIME [epoch: 3.53 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03652608030165374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03652608030165374 | validation: 0.02639433867479877]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_674.pth
	Model improved!!!
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027174802735914404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027174802735914404 | validation: 0.03592105050128993]
	TIME [epoch: 3.55 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028860728566445903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028860728566445903 | validation: 0.029260717653251785]
	TIME [epoch: 3.53 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04732011761689849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04732011761689849 | validation: 0.10502028938943467]
	TIME [epoch: 3.53 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10165215637023273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10165215637023273 | validation: 0.0969018269015105]
	TIME [epoch: 3.54 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15601762355783932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15601762355783932 | validation: 0.10261413518193457]
	TIME [epoch: 3.54 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08946223594049371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08946223594049371 | validation: 0.03837581607092513]
	TIME [epoch: 3.53 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04183067205329076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04183067205329076 | validation: 0.06469384929451256]
	TIME [epoch: 3.54 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05185658875856972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05185658875856972 | validation: 0.15937217665672665]
	TIME [epoch: 3.54 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15024376587544594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15024376587544594 | validation: 0.15317794711781235]
	TIME [epoch: 3.54 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15214452742133688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15214452742133688 | validation: 0.12413191258876166]
	TIME [epoch: 3.53 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08430906359875438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08430906359875438 | validation: 0.0921308198500509]
	TIME [epoch: 3.53 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08774987074622578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08774987074622578 | validation: 0.17252876756605462]
	TIME [epoch: 3.53 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1702074192418852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1702074192418852 | validation: 0.16862331829268135]
	TIME [epoch: 3.54 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15242117262329216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15242117262329216 | validation: 0.057879267278952706]
	TIME [epoch: 3.55 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08644607515132197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08644607515132197 | validation: 0.05568283121879526]
	TIME [epoch: 3.53 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06722971685909437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06722971685909437 | validation: 0.043296369949594044]
	TIME [epoch: 3.54 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041279533588182496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041279533588182496 | validation: 0.02524502381324873]
	TIME [epoch: 3.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_691.pth
	Model improved!!!
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031792720359491565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031792720359491565 | validation: 0.049382517306473854]
	TIME [epoch: 3.53 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038028849764123275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038028849764123275 | validation: 0.03495192512550654]
	TIME [epoch: 3.52 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057409781869129456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057409781869129456 | validation: 0.1152019253357791]
	TIME [epoch: 3.53 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10539199267443948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10539199267443948 | validation: 0.12055311430161636]
	TIME [epoch: 3.53 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15596306628261553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15596306628261553 | validation: 0.1104445099530142]
	TIME [epoch: 3.54 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0966899807295518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0966899807295518 | validation: 0.06222680340258349]
	TIME [epoch: 3.53 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05214460263989969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05214460263989969 | validation: 0.04464606594462092]
	TIME [epoch: 3.53 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041239577215950424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041239577215950424 | validation: 0.06536457904728128]
	TIME [epoch: 3.53 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05519889188159715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05519889188159715 | validation: 0.07404414394119684]
	TIME [epoch: 3.52 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07803245992656514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07803245992656514 | validation: 0.10383505377062328]
	TIME [epoch: 3.55 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09078915041983536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09078915041983536 | validation: 0.07776882178939398]
	TIME [epoch: 3.53 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08134866885940092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08134866885940092 | validation: 0.09271331658089627]
	TIME [epoch: 3.54 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08912606615119602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08912606615119602 | validation: 0.1535191912800265]
	TIME [epoch: 3.54 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20671676497878597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20671676497878597 | validation: 0.11167958439938022]
	TIME [epoch: 3.53 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1490369149783798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1490369149783798 | validation: 0.07383696328207288]
	TIME [epoch: 3.53 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062394410810455686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062394410810455686 | validation: 0.0754334679881331]
	TIME [epoch: 3.54 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043796200893542955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043796200893542955 | validation: 0.037990335652950104]
	TIME [epoch: 3.54 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02901056762056351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02901056762056351 | validation: 0.03862241444605081]
	TIME [epoch: 3.55 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046031870641684285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046031870641684285 | validation: 0.0927260712993661]
	TIME [epoch: 3.54 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08613388651246773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08613388651246773 | validation: 0.11310939409577725]
	TIME [epoch: 3.53 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15911254995473667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15911254995473667 | validation: 0.14679204023677345]
	TIME [epoch: 3.53 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15113790035751484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15113790035751484 | validation: 0.0852779890852755]
	TIME [epoch: 3.53 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08482451689152348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08482451689152348 | validation: 0.06708070816148104]
	TIME [epoch: 3.53 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05745060610114118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05745060610114118 | validation: 0.07185588079245697]
	TIME [epoch: 3.53 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06015586103586811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06015586103586811 | validation: 0.06505030527631682]
	TIME [epoch: 3.53 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07553969650682872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07553969650682872 | validation: 0.08877969657019813]
	TIME [epoch: 3.53 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08885855607629563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08885855607629563 | validation: 0.08541354689455259]
	TIME [epoch: 3.55 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10462880229944409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10462880229944409 | validation: 0.07570429003180479]
	TIME [epoch: 3.54 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10511486137611993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10511486137611993 | validation: 0.06238257768187405]
	TIME [epoch: 3.54 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05458115477199014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05458115477199014 | validation: 0.038982496275382875]
	TIME [epoch: 3.54 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035272996050514045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035272996050514045 | validation: 0.05525099154679594]
	TIME [epoch: 3.53 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04100726684061126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04100726684061126 | validation: 0.07309042534654432]
	TIME [epoch: 3.53 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.069200375317444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.069200375317444 | validation: 0.10824486443488089]
	TIME [epoch: 3.53 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09205882963589336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09205882963589336 | validation: 0.09149137390257699]
	TIME [epoch: 3.53 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10990518120545742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10990518120545742 | validation: 0.08373132840017637]
	TIME [epoch: 3.54 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09348224755231758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09348224755231758 | validation: 0.0666764255883674]
	TIME [epoch: 3.52 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06695207568694127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06695207568694127 | validation: 0.07949872275066511]
	TIME [epoch: 3.54 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08069488636858611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08069488636858611 | validation: 0.11004090611044846]
	TIME [epoch: 3.53 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12158882487933022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12158882487933022 | validation: 0.0985681559572108]
	TIME [epoch: 3.53 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1194965697724153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1194965697724153 | validation: 0.07431704516402526]
	TIME [epoch: 3.54 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0607214833681131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0607214833681131 | validation: 0.030244211930523054]
	TIME [epoch: 3.52 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03178311263965019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03178311263965019 | validation: 0.03043155939961786]
	TIME [epoch: 3.53 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024300218632257266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024300218632257266 | validation: 0.025382073656612755]
	TIME [epoch: 3.54 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02709767585716593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02709767585716593 | validation: 0.052519356665287203]
	TIME [epoch: 3.55 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04722781035358124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04722781035358124 | validation: 0.09862632489026024]
	TIME [epoch: 3.53 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11317989707783203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11317989707783203 | validation: 0.16979944692159485]
	TIME [epoch: 3.53 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17129781450649634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17129781450649634 | validation: 0.09794884731203549]
	TIME [epoch: 3.53 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11242024863593418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11242024863593418 | validation: 0.0559403566472542]
	TIME [epoch: 3.53 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043894006332035065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043894006332035065 | validation: 0.049503300371686335]
	TIME [epoch: 3.54 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05107231000101505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05107231000101505 | validation: 0.11058980241637573]
	TIME [epoch: 3.53 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08944953576747548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08944953576747548 | validation: 0.09800257967734165]
	TIME [epoch: 3.54 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1026100800964726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1026100800964726 | validation: 0.09632836751704733]
	TIME [epoch: 3.54 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06588991953063567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06588991953063567 | validation: 0.04102561180989017]
	TIME [epoch: 3.52 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052256995448135446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052256995448135446 | validation: 0.07405813073260575]
	TIME [epoch: 3.53 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06960534396234042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06960534396234042 | validation: 0.04509416049986259]
	TIME [epoch: 3.52 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07299837099384454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07299837099384454 | validation: 0.06700069730085012]
	TIME [epoch: 3.53 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060124409686998385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060124409686998385 | validation: 0.05282564099005693]
	TIME [epoch: 3.52 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07347329144445214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07347329144445214 | validation: 0.11139790592568782]
	TIME [epoch: 3.53 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10008537365757779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10008537365757779 | validation: 0.1142146477141432]
	TIME [epoch: 3.54 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14038574459790287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14038574459790287 | validation: 0.1484807499665505]
	TIME [epoch: 3.51 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15515347912879546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15515347912879546 | validation: 0.08475392745746008]
	TIME [epoch: 3.53 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08350354766609108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08350354766609108 | validation: 0.03192576212499102]
	TIME [epoch: 3.53 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0376839540695471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0376839540695471 | validation: 0.030268436897536413]
	TIME [epoch: 3.53 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026178526676598608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026178526676598608 | validation: 0.027240717683316915]
	TIME [epoch: 3.52 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02424211601182897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02424211601182897 | validation: 0.047761046102150685]
	TIME [epoch: 3.53 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03570880812146065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03570880812146065 | validation: 0.08966146277933851]
	TIME [epoch: 3.53 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0821043029593535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0821043029593535 | validation: 0.12018233088754307]
	TIME [epoch: 3.53 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11361297245985483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11361297245985483 | validation: 0.09050549871682007]
	TIME [epoch: 3.52 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07300662798556898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07300662798556898 | validation: 0.05768908247697435]
	TIME [epoch: 3.52 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06550062036973063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06550062036973063 | validation: 0.08424288248051409]
	TIME [epoch: 3.53 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0958291904617294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0958291904617294 | validation: 0.13599742384692057]
	TIME [epoch: 3.53 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12352273009192243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12352273009192243 | validation: 0.08725147399822784]
	TIME [epoch: 3.52 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09615856060755179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09615856060755179 | validation: 0.054085452894049896]
	TIME [epoch: 3.53 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04820154138890327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04820154138890327 | validation: 0.04538454014106651]
	TIME [epoch: 3.52 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04630718126218107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04630718126218107 | validation: 0.05415301654868028]
	TIME [epoch: 3.52 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10068191914819011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10068191914819011 | validation: 0.08723152616177371]
	TIME [epoch: 3.54 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0813323243608114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0813323243608114 | validation: 0.052508541673549715]
	TIME [epoch: 3.54 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06650207849954402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06650207849954402 | validation: 0.08943130068801569]
	TIME [epoch: 3.53 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07520704795350165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07520704795350165 | validation: 0.09932222868418278]
	TIME [epoch: 3.52 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09567713916343352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09567713916343352 | validation: 0.08543776503478906]
	TIME [epoch: 3.54 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06947997411935501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06947997411935501 | validation: 0.05552054972485448]
	TIME [epoch: 3.53 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052227566206821216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052227566206821216 | validation: 0.056780414310792164]
	TIME [epoch: 3.55 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06112387638362904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06112387638362904 | validation: 0.09044749315521637]
	TIME [epoch: 3.54 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08909303024722894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08909303024722894 | validation: 0.11116485161996292]
	TIME [epoch: 3.52 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11792731528791023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11792731528791023 | validation: 0.0947556704651585]
	TIME [epoch: 3.52 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09927249534015807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09927249534015807 | validation: 0.05109862674696269]
	TIME [epoch: 3.52 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052667682144971514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052667682144971514 | validation: 0.03491001486237885]
	TIME [epoch: 3.53 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026958814808336975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026958814808336975 | validation: 0.018725543308378566]
	TIME [epoch: 3.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_779.pth
	Model improved!!!
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02550977302961221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02550977302961221 | validation: 0.03929533630714013]
	TIME [epoch: 3.52 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03744215356704877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03744215356704877 | validation: 0.03842304729198303]
	TIME [epoch: 3.52 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07950312356620652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07950312356620652 | validation: 0.09005284586877073]
	TIME [epoch: 3.52 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09597295941734509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09597295941734509 | validation: 0.05987458560311448]
	TIME [epoch: 3.53 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08524969326952922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08524969326952922 | validation: 0.12357375859010428]
	TIME [epoch: 3.52 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10085631792422607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10085631792422607 | validation: 0.12516434502513715]
	TIME [epoch: 3.52 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13027109416357835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13027109416357835 | validation: 0.0586944499869384]
	TIME [epoch: 3.53 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07663330207029405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07663330207029405 | validation: 0.0659622739816032]
	TIME [epoch: 3.53 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03349443400888673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03349443400888673 | validation: 0.04376535433281835]
	TIME [epoch: 3.51 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028760011292669394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028760011292669394 | validation: 0.05335095364650303]
	TIME [epoch: 3.51 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04967687825381458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04967687825381458 | validation: 0.09863574059562963]
	TIME [epoch: 3.51 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08077753025655678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08077753025655678 | validation: 0.10685121090313024]
	TIME [epoch: 3.52 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10875658981635092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10875658981635092 | validation: 0.09899145167603224]
	TIME [epoch: 3.52 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10935637905875906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10935637905875906 | validation: 0.0913675694332195]
	TIME [epoch: 3.52 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12024807116727139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12024807116727139 | validation: 0.09802010818730147]
	TIME [epoch: 3.52 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1096516365146821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1096516365146821 | validation: 0.04382075224442647]
	TIME [epoch: 3.51 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050283209517398036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050283209517398036 | validation: 0.0625233914082347]
	TIME [epoch: 3.52 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04377801436263942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04377801436263942 | validation: 0.03412441417630087]
	TIME [epoch: 3.51 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056807331549930055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056807331549930055 | validation: 0.0704855042030627]
	TIME [epoch: 3.52 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07749368527861285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07749368527861285 | validation: 0.05241480893511419]
	TIME [epoch: 3.52 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08095776721827319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08095776721827319 | validation: 0.059443387091838676]
	TIME [epoch: 3.53 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05684402389612076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05684402389612076 | validation: 0.03508323631438184]
	TIME [epoch: 3.53 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041162704559462124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041162704559462124 | validation: 0.0467515208590681]
	TIME [epoch: 3.53 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033659721675541314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033659721675541314 | validation: 0.030822968264689957]
	TIME [epoch: 3.53 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03721873987255688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03721873987255688 | validation: 0.07035766569833184]
	TIME [epoch: 3.53 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061289996279274686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061289996279274686 | validation: 0.10335775211128646]
	TIME [epoch: 3.53 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11805530526298924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11805530526298924 | validation: 0.13770552465631375]
	TIME [epoch: 3.53 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15830044224292877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15830044224292877 | validation: 0.09753421657092518]
	TIME [epoch: 3.53 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1028550578705234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1028550578705234 | validation: 0.058731930196596976]
	TIME [epoch: 3.52 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06480921295914584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06480921295914584 | validation: 0.06858200741671007]
	TIME [epoch: 3.53 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05687737296158358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05687737296158358 | validation: 0.062313039279402994]
	TIME [epoch: 3.53 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06838679419599152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06838679419599152 | validation: 0.0758455209628176]
	TIME [epoch: 3.55 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06843021703619036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06843021703619036 | validation: 0.0336377986046013]
	TIME [epoch: 3.53 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06142126289566185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06142126289566185 | validation: 0.04271775578793502]
	TIME [epoch: 3.54 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03556409777722257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03556409777722257 | validation: 0.021230014127243048]
	TIME [epoch: 3.53 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029523736155015794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029523736155015794 | validation: 0.030710016191431856]
	TIME [epoch: 3.52 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02261565661242212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02261565661242212 | validation: 0.021710087616824598]
	TIME [epoch: 3.53 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02462064714177692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02462064714177692 | validation: 0.055683688076761495]
	TIME [epoch: 3.53 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045172369046721686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045172369046721686 | validation: 0.09576487509900695]
	TIME [epoch: 3.53 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1040799080301273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1040799080301273 | validation: 0.15853728315440063]
	TIME [epoch: 3.54 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14304068924682053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14304068924682053 | validation: 0.08922901457247855]
	TIME [epoch: 3.54 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10036846378907611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10036846378907611 | validation: 0.07519799275899142]
	TIME [epoch: 3.54 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07325855897555313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07325855897555313 | validation: 0.07873251250890476]
	TIME [epoch: 3.53 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10260440513578846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10260440513578846 | validation: 0.08736668904356122]
	TIME [epoch: 3.53 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10519263872254028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10519263872254028 | validation: 0.07868659053737903]
	TIME [epoch: 3.53 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055750825025639966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055750825025639966 | validation: 0.03634916835908289]
	TIME [epoch: 3.55 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028092002204922854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028092002204922854 | validation: 0.01649730194429663]
	TIME [epoch: 3.53 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_826.pth
	Model improved!!!
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01730037233786494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01730037233786494 | validation: 0.02033612743868999]
	TIME [epoch: 3.52 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016787667175874182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016787667175874182 | validation: 0.019999558214247246]
	TIME [epoch: 3.51 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02241148162175903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02241148162175903 | validation: 0.04733847725199637]
	TIME [epoch: 3.52 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04238457051568508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04238457051568508 | validation: 0.07568539804744999]
	TIME [epoch: 3.52 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09422701224644447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09422701224644447 | validation: 0.1288372714541017]
	TIME [epoch: 3.52 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.140404180434531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.140404180434531 | validation: 0.06010694769613259]
	TIME [epoch: 3.52 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09505050596021875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09505050596021875 | validation: 0.04798744966514104]
	TIME [epoch: 3.53 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030038313936339926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030038313936339926 | validation: 0.017138756121871]
	TIME [epoch: 3.53 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016017292435530114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016017292435530114 | validation: 0.022810275748908867]
	TIME [epoch: 3.53 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023963933573798234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023963933573798234 | validation: 0.0430996042488389]
	TIME [epoch: 3.53 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0470227398088298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0470227398088298 | validation: 0.10818862754059831]
	TIME [epoch: 3.53 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11294034945478754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11294034945478754 | validation: 0.18906335568638594]
	TIME [epoch: 3.54 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1893637666529558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1893637666529558 | validation: 0.11707342356019963]
	TIME [epoch: 3.54 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11315311251834402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11315311251834402 | validation: 0.0629212053163597]
	TIME [epoch: 3.53 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053634095199153416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053634095199153416 | validation: 0.03391722407137681]
	TIME [epoch: 3.54 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03345605968779945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03345605968779945 | validation: 0.03464104012641952]
	TIME [epoch: 3.53 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02535883362617022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02535883362617022 | validation: 0.04790888405187799]
	TIME [epoch: 3.53 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03793427713399546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03793427713399546 | validation: 0.06740797958851674]
	TIME [epoch: 3.54 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06446208142638686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06446208142638686 | validation: 0.1329296070537652]
	TIME [epoch: 3.53 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16539712768632478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16539712768632478 | validation: 0.11730182350039271]
	TIME [epoch: 3.52 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14877166412556295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14877166412556295 | validation: 0.06175310655252484]
	TIME [epoch: 3.52 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05261829081210252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05261829081210252 | validation: 0.04537736870471633]
	TIME [epoch: 3.52 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03757078204572974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03757078204572974 | validation: 0.031446308672140266]
	TIME [epoch: 3.53 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05392757981129981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05392757981129981 | validation: 0.06001081106048583]
	TIME [epoch: 3.54 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061395894269449816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061395894269449816 | validation: 0.037804242264155546]
	TIME [epoch: 3.53 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05964664034443967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05964664034443967 | validation: 0.05861933257124117]
	TIME [epoch: 3.54 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04676611827716011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04676611827716011 | validation: 0.03537151326351803]
	TIME [epoch: 3.52 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039174618730528854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039174618730528854 | validation: 0.044952271467265006]
	TIME [epoch: 3.52 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033117667965726676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033117667965726676 | validation: 0.03558955294719488]
	TIME [epoch: 3.52 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03399107295809938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03399107295809938 | validation: 0.05184505428379815]
	TIME [epoch: 3.53 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04845012825030315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04845012825030315 | validation: 0.08034231014721181]
	TIME [epoch: 3.53 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08249474967836448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08249474967836448 | validation: 0.07748310307970796]
	TIME [epoch: 3.53 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09187671918032245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09187671918032245 | validation: 0.07304217309302052]
	TIME [epoch: 3.52 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05688017466700251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05688017466700251 | validation: 0.06818398558644682]
	TIME [epoch: 3.54 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06795903767390796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06795903767390796 | validation: 0.1061336591027255]
	TIME [epoch: 3.53 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1199054395009566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1199054395009566 | validation: 0.1202629046154764]
	TIME [epoch: 3.53 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11674119531905738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11674119531905738 | validation: 0.08591553681168297]
	TIME [epoch: 3.52 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08869545390123619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08869545390123619 | validation: 0.05297916417803588]
	TIME [epoch: 3.55 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09587218707798471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09587218707798471 | validation: 0.05844230055889246]
	TIME [epoch: 3.54 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04737391619136744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04737391619136744 | validation: 0.030995660384353658]
	TIME [epoch: 3.53 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035733808291829386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035733808291829386 | validation: 0.03626338476549001]
	TIME [epoch: 3.52 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03193733123513148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03193733123513148 | validation: 0.02554128100305766]
	TIME [epoch: 3.54 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03141323470585402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03141323470585402 | validation: 0.057208278613809]
	TIME [epoch: 3.52 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045680053200783236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045680053200783236 | validation: 0.07865741448634847]
	TIME [epoch: 3.54 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0847816395147051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0847816395147051 | validation: 0.10462973057359443]
	TIME [epoch: 3.52 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10825306834108389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10825306834108389 | validation: 0.08538410974197459]
	TIME [epoch: 3.54 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06978483558284258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06978483558284258 | validation: 0.03694730552449379]
	TIME [epoch: 3.52 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03562988440446923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03562988440446923 | validation: 0.043026246706454815]
	TIME [epoch: 3.53 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04384838478016653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04384838478016653 | validation: 0.09089376789220104]
	TIME [epoch: 3.52 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08285659497691611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08285659497691611 | validation: 0.10857508753838388]
	TIME [epoch: 3.52 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1126840717194095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1126840717194095 | validation: 0.0625724762446432]
	TIME [epoch: 3.54 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08477341683550836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08477341683550836 | validation: 0.06119678795119104]
	TIME [epoch: 3.53 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05595362999928977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05595362999928977 | validation: 0.027900135237316062]
	TIME [epoch: 3.53 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040359737307863394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040359737307863394 | validation: 0.029598444337196536]
	TIME [epoch: 3.52 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028437625367443806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028437625367443806 | validation: 0.031199232922384086]
	TIME [epoch: 3.53 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041635252221532686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041635252221532686 | validation: 0.060512356193079264]
	TIME [epoch: 3.52 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06776292542063243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06776292542063243 | validation: 0.057427165253914905]
	TIME [epoch: 3.53 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0800471535962883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0800471535962883 | validation: 0.08096406139352419]
	TIME [epoch: 3.53 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06863503664220615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06863503664220615 | validation: 0.056819549372802494]
	TIME [epoch: 3.53 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062371631604474356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062371631604474356 | validation: 0.07292540257937699]
	TIME [epoch: 3.52 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061581471264324075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061581471264324075 | validation: 0.07784777698443646]
	TIME [epoch: 3.52 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06831537699131604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06831537699131604 | validation: 0.046945083855191576]
	TIME [epoch: 3.52 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058541455336336326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058541455336336326 | validation: 0.04010720240459878]
	TIME [epoch: 3.52 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026453019876729527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026453019876729527 | validation: 0.05525175799642227]
	TIME [epoch: 3.53 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04962953749040937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04962953749040937 | validation: 0.09339324553753148]
	TIME [epoch: 3.54 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11481361736222635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11481361736222635 | validation: 0.12450083301878281]
	TIME [epoch: 3.52 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12173642751716844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12173642751716844 | validation: 0.05859402565732315]
	TIME [epoch: 3.52 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05420557212126215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05420557212126215 | validation: 0.02788908152903033]
	TIME [epoch: 3.52 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025378054633687145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025378054633687145 | validation: 0.03347981763929774]
	TIME [epoch: 3.54 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03733119814194337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03733119814194337 | validation: 0.03938336125912613]
	TIME [epoch: 3.53 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06544426145155907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06544426145155907 | validation: 0.08400217825874423]
	TIME [epoch: 3.54 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08816248583115256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08816248583115256 | validation: 0.058179060085432145]
	TIME [epoch: 3.55 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07489326909248843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07489326909248843 | validation: 0.06167842689403691]
	TIME [epoch: 3.53 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04694409562608223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04694409562608223 | validation: 0.030197451881567472]
	TIME [epoch: 3.53 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029520983865927358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029520983865927358 | validation: 0.029720232760603222]
	TIME [epoch: 3.52 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033044922619425585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033044922619425585 | validation: 0.08219561714651352]
	TIME [epoch: 3.53 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0792351618684741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0792351618684741 | validation: 0.051482346340706746]
	TIME [epoch: 3.54 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09050674623178188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09050674623178188 | validation: 0.060301067779990884]
	TIME [epoch: 3.54 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033483603983232246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033483603983232246 | validation: 0.06832611859267183]
	TIME [epoch: 3.54 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038346164140737465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038346164140737465 | validation: 0.0545614697840747]
	TIME [epoch: 3.53 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.064806685587767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.064806685587767 | validation: 0.11762995205198781]
	TIME [epoch: 3.54 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11990552412584919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11990552412584919 | validation: 0.11892648016517177]
	TIME [epoch: 3.52 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12220765585687136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12220765585687136 | validation: 0.07783029831065787]
	TIME [epoch: 3.53 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07428656775301319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07428656775301319 | validation: 0.08476928649770979]
	TIME [epoch: 3.56 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07282209748013037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07282209748013037 | validation: 0.05669907084462205]
	TIME [epoch: 3.52 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0988621975119187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0988621975119187 | validation: 0.06965274522944319]
	TIME [epoch: 3.53 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06813723668296498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06813723668296498 | validation: 0.03641781602431398]
	TIME [epoch: 3.53 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03468708029832312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03468708029832312 | validation: 0.020709748551244786]
	TIME [epoch: 3.52 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01945885965110638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01945885965110638 | validation: 0.025653892406567864]
	TIME [epoch: 3.54 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02212274902643916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02212274902643916 | validation: 0.038768784081843685]
	TIME [epoch: 3.53 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042789815837694374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042789815837694374 | validation: 0.07190179489278836]
	TIME [epoch: 3.53 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07324021000171056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07324021000171056 | validation: 0.05783324524832381]
	TIME [epoch: 3.52 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08814582528965843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08814582528965843 | validation: 0.06131184220896579]
	TIME [epoch: 3.53 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06409483337600207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06409483337600207 | validation: 0.08241959887189175]
	TIME [epoch: 3.52 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08067249728008996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08067249728008996 | validation: 0.06671217261571326]
	TIME [epoch: 3.54 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07785171537994516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07785171537994516 | validation: 0.06488323818360356]
	TIME [epoch: 3.54 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05739596449246438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05739596449246438 | validation: 0.05432133994777641]
	TIME [epoch: 3.53 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040667315178151676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040667315178151676 | validation: 0.038830882791489524]
	TIME [epoch: 3.52 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03595618509772191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03595618509772191 | validation: 0.0596479065813986]
	TIME [epoch: 3.52 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04810413725985743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04810413725985743 | validation: 0.057303845690550526]
	TIME [epoch: 3.53 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07681002631979127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07681002631979127 | validation: 0.09027055494307515]
	TIME [epoch: 3.52 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172003/states/model_phi1_3a_v_mmd1_927.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 2372.469 seconds.
