Args:
Namespace(name='model_phi1_3a_v_mmd1', outdir='out/model_training/model_phi1_3a_v_mmd1', training_data='data/training_data/data_phi1_3a/training', validation_data='data/training_data/data_phi1_3a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3321953643

Training model...

Saving initial model state to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.450488033422593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.450488033422593 | validation: 5.391162277180425]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.349119105824132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.349119105824132 | validation: 5.024062384379126]
	TIME [epoch: 0.96 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.958282989905367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.958282989905367 | validation: 5.859392969992079]
	TIME [epoch: 0.935 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.496311976377749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.496311976377749 | validation: 5.124358082290968]
	TIME [epoch: 0.933 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.315575665393042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.315575665393042 | validation: 5.033596565325343]
	TIME [epoch: 0.931 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.343222401800424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.343222401800424 | validation: 4.668643521623437]
	TIME [epoch: 0.93 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.577483704981857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.577483704981857 | validation: 5.052425332188644]
	TIME [epoch: 0.937 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.010612858606683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.010612858606683 | validation: 4.807112477423917]
	TIME [epoch: 0.937 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9097813598054687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9097813598054687 | validation: 4.893957673931216]
	TIME [epoch: 0.931 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.948731733015396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.948731733015396 | validation: 4.596930287864893]
	TIME [epoch: 0.931 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.061282535571069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.061282535571069 | validation: 4.872912863275701]
	TIME [epoch: 0.939 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8339138611219865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8339138611219865 | validation: 4.588588678080504]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.782969061219028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.782969061219028 | validation: 4.747369341473815]
	TIME [epoch: 0.935 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.732803796328139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.732803796328139 | validation: 4.545525789560275]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7214529702494294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7214529702494294 | validation: 4.710254063757226]
	TIME [epoch: 0.934 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6994046433140046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6994046433140046 | validation: 4.459710179500309]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.707155740056422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.707155740056422 | validation: 4.6827768694692855]
	TIME [epoch: 0.933 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.661013223785833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.661013223785833 | validation: 4.39080859471166]
	TIME [epoch: 0.931 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.647776986860636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.647776986860636 | validation: 4.610688165165056]
	TIME [epoch: 0.935 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.583791627381902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.583791627381902 | validation: 4.348893010366767]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.559879808403741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.559879808403741 | validation: 4.5613559062142075]
	TIME [epoch: 0.936 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5339452449775712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5339452449775712 | validation: 4.27338553013408]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5366451658527427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5366451658527427 | validation: 4.522507954202928]
	TIME [epoch: 0.935 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5026771705730217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5026771705730217 | validation: 4.251063639832049]
	TIME [epoch: 0.933 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5162554964051287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5162554964051287 | validation: 4.4161189857547445]
	TIME [epoch: 0.936 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4920551362743697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4920551362743697 | validation: 4.38312922338875]
	TIME [epoch: 0.936 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5786960739240437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5786960739240437 | validation: 4.216693951225582]
	TIME [epoch: 0.933 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4737568507072734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4737568507072734 | validation: 4.448244396756205]
	TIME [epoch: 0.939 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.442526175367338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.442526175367338 | validation: 4.13001537346216]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4190030503504416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4190030503504416 | validation: 4.282721844945142]
	TIME [epoch: 0.936 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.344571056392919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.344571056392919 | validation: 4.218641370338943]
	TIME [epoch: 0.935 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3407365011226084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3407365011226084 | validation: 4.188204461327637]
	TIME [epoch: 0.936 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3687720743175156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3687720743175156 | validation: 4.1530441100542825]
	TIME [epoch: 0.938 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3377847690502667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3377847690502667 | validation: 4.245093477424277]
	TIME [epoch: 0.935 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3279377248733257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3279377248733257 | validation: 4.062215145724315]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3538324478353037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3538324478353037 | validation: 4.175216516002102]
	TIME [epoch: 0.939 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.257355275397208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.257355275397208 | validation: 4.031290893216294]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.223798392641214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.223798392641214 | validation: 4.085217923931944]
	TIME [epoch: 0.937 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1992376218586176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1992376218586176 | validation: 4.0131839597172885]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.193480179044717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.193480179044717 | validation: 4.043689282230204]
	TIME [epoch: 0.935 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1914387882321833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1914387882321833 | validation: 3.985751378307903]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.223955981114938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.223955981114938 | validation: 4.026027258646273]
	TIME [epoch: 0.939 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2060976656426567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2060976656426567 | validation: 3.9328760015904822]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.192581071245356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.192581071245356 | validation: 3.962166725192903]
	TIME [epoch: 0.935 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1225654785052206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1225654785052206 | validation: 3.896682152230344]
	TIME [epoch: 0.941 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0926806263241065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0926806263241065 | validation: 3.9189308497229614]
	TIME [epoch: 0.937 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.072249987218244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.072249987218244 | validation: 3.8387543759427403]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.067794835544401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.067794835544401 | validation: 3.891252606020625]
	TIME [epoch: 0.936 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0564852011619186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0564852011619186 | validation: 3.7969011690105385]
	TIME [epoch: 0.951 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.067184834203623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.067184834203623 | validation: 3.8409775423106645]
	TIME [epoch: 0.936 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0517857647108007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0517857647108007 | validation: 3.852695695721554]
	TIME [epoch: 0.936 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0854850197201493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0854850197201493 | validation: 3.73294672072727]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0939717084674854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0939717084674854 | validation: 3.788098569421715]
	TIME [epoch: 0.936 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9947057935898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9947057935898 | validation: 3.738966442308122]
	TIME [epoch: 0.937 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9502053947789943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9502053947789943 | validation: 3.66596614700049]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.94154739912702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.94154739912702 | validation: 3.7056613077727913]
	TIME [epoch: 0.938 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9222547135545835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9222547135545835 | validation: 3.6498951516327693]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.912509997004852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.912509997004852 | validation: 3.648987653406384]
	TIME [epoch: 0.939 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8965347841993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8965347841993 | validation: 3.6392269348127573]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9012831566492983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9012831566492983 | validation: 3.615566621991436]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9268088141220483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9268088141220483 | validation: 3.7071462122432095]
	TIME [epoch: 0.935 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0172873945029064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0172873945029064 | validation: 3.5558741985966957]
	TIME [epoch: 0.933 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8870100908395675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8870100908395675 | validation: 3.583893243409162]
	TIME [epoch: 0.938 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.844044524380057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.844044524380057 | validation: 3.5196155118011134]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.826058851361515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.826058851361515 | validation: 3.500030182048164]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8155003437671975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8155003437671975 | validation: 3.508994788488999]
	TIME [epoch: 0.936 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.812313877051605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.812313877051605 | validation: 3.48841578537352]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.802436155107565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.802436155107565 | validation: 3.4864560418585406]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8068914953814077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8068914953814077 | validation: 3.4475287701750883]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8334865747338966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8334865747338966 | validation: 3.569686918032668]
	TIME [epoch: 0.937 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.887255159356426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.887255159356426 | validation: 3.3735646699889252]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7871256864758234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7871256864758234 | validation: 3.422636272445075]
	TIME [epoch: 0.938 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.744687213164231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.744687213164231 | validation: 3.407559965967998]
	TIME [epoch: 0.936 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7491657509993717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7491657509993717 | validation: 3.3534205166644337]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7390538745130035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7390538745130035 | validation: 3.3814087072733723]
	TIME [epoch: 0.939 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.741104314208143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.741104314208143 | validation: 3.351611321365793]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7248601317996672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7248601317996672 | validation: 3.346132387854899]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7114515759429585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7114515759429585 | validation: 3.2828727582337445]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.704760421907088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.704760421907088 | validation: 3.341176975278888]
	TIME [epoch: 0.935 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7010927211182194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7010927211182194 | validation: 3.2555090395540986]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.690015925291364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.690015925291364 | validation: 3.3102357183857167]
	TIME [epoch: 0.937 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.699688946321742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.699688946321742 | validation: 3.234456275891421]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6791608393948825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6791608393948825 | validation: 3.2643682948876362]
	TIME [epoch: 0.954 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6616526756452004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6616526756452004 | validation: 3.190417171879198]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6354336692455314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6354336692455314 | validation: 3.1998795126391517]
	TIME [epoch: 0.935 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6163552039723794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6163552039723794 | validation: 3.1511247104801683]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.596635323827858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.596635323827858 | validation: 3.111655036954558]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5548865193462653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5548865193462653 | validation: 2.899408158121493]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4091278892808057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4091278892808057 | validation: 2.2559780429118237]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2690531964407343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2690531964407343 | validation: 1.7049035263219368]
	TIME [epoch: 0.933 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6148594551970334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6148594551970334 | validation: 2.433959691240503]
	TIME [epoch: 0.934 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.874301698859963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.874301698859963 | validation: 2.013641614666191]
	TIME [epoch: 0.932 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.232112871842659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.232112871842659 | validation: 1.2145774179671747]
	TIME [epoch: 0.932 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2630497281696693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2630497281696693 | validation: 1.1225905530008289]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.35692980719916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.35692980719916 | validation: 1.435391626876917]
	TIME [epoch: 0.994 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3410700367176167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3410700367176167 | validation: 1.025928637480601]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0880129590413372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0880129590413372 | validation: 0.9394512846569509]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0722996407899987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0722996407899987 | validation: 1.220202834572478]
	TIME [epoch: 0.937 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1567582128772085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1567582128772085 | validation: 1.053474574407748]
	TIME [epoch: 0.935 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2703836677978593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2703836677978593 | validation: 1.306558028682689]
	TIME [epoch: 0.936 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1545147931166715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1545147931166715 | validation: 0.9327682939968736]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9824340678301389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9824340678301389 | validation: 0.9206330920953594]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9449573057946685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9449573057946685 | validation: 0.9563252936218309]
	TIME [epoch: 0.937 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9505407649105792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9505407649105792 | validation: 0.8774465359161804]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9700676554643042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9700676554643042 | validation: 1.0416918476472137]
	TIME [epoch: 0.933 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9938089851006132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9938089851006132 | validation: 0.9131235047088359]
	TIME [epoch: 0.935 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.006798675237252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.006798675237252 | validation: 1.1090975539189716]
	TIME [epoch: 0.936 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0392309185165751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0392309185165751 | validation: 0.9119685075628576]
	TIME [epoch: 0.937 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0145430112350755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0145430112350755 | validation: 1.023993377844201]
	TIME [epoch: 0.935 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9677520682228169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9677520682228169 | validation: 0.8756268610174301]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9248719815466601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9248719815466601 | validation: 0.9351687243364926]
	TIME [epoch: 0.934 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9064319128879038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9064319128879038 | validation: 0.8398412421207399]
	TIME [epoch: 0.931 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8961991106662491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8961991106662491 | validation: 0.9446910823128164]
	TIME [epoch: 0.936 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.890824921067217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.890824921067217 | validation: 0.8724156978231159]
	TIME [epoch: 0.935 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9190021515981045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9190021515981045 | validation: 1.046959411468855]
	TIME [epoch: 0.932 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.972137912498184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.972137912498184 | validation: 0.910419243298295]
	TIME [epoch: 0.93 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9934044623710713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9934044623710713 | validation: 1.0488587110644656]
	TIME [epoch: 0.93 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.988628783710849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.988628783710849 | validation: 0.8647346487683318]
	TIME [epoch: 0.929 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9014294335623632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9014294335623632 | validation: 0.8802244588898912]
	TIME [epoch: 0.936 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8728858869433634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8728858869433634 | validation: 0.8461739344895003]
	TIME [epoch: 0.941 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8399466600767932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8399466600767932 | validation: 0.8768482180306425]
	TIME [epoch: 0.931 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.844136573224693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.844136573224693 | validation: 0.8641935162848742]
	TIME [epoch: 0.932 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8634531579114263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8634531579114263 | validation: 0.9821036201584953]
	TIME [epoch: 0.934 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9152240168079022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9152240168079022 | validation: 0.9313033883046729]
	TIME [epoch: 0.933 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0046893935707855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0046893935707855 | validation: 1.1399254495298947]
	TIME [epoch: 0.934 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0187493223518382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0187493223518382 | validation: 0.8659765095567771]
	TIME [epoch: 0.934 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.878265373804968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.878265373804968 | validation: 0.849204757289694]
	TIME [epoch: 0.934 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.822254938745739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.822254938745739 | validation: 0.8494167238300602]
	TIME [epoch: 0.934 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8169545156685076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8169545156685076 | validation: 0.8445950898964985]
	TIME [epoch: 0.932 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8288265381957903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8288265381957903 | validation: 0.8996519780943273]
	TIME [epoch: 0.934 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8522652054296523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8522652054296523 | validation: 0.8933577202122218]
	TIME [epoch: 0.934 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8937999055281353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8937999055281353 | validation: 1.0754662923345881]
	TIME [epoch: 0.94 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9727552936571779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9727552936571779 | validation: 0.932141741266133]
	TIME [epoch: 0.934 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9407121489183962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9407121489183962 | validation: 0.9034079691177518]
	TIME [epoch: 0.934 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8510188896159199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8510188896159199 | validation: 0.854303886029105]
	TIME [epoch: 0.932 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8107727931888125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8107727931888125 | validation: 0.8562402960678734]
	TIME [epoch: 0.934 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.802471633357705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.802471633357705 | validation: 0.8484127523069884]
	TIME [epoch: 0.93 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8117610586326884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8117610586326884 | validation: 0.8510800293284249]
	TIME [epoch: 0.934 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8204255402499849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8204255402499849 | validation: 0.9119225756665394]
	TIME [epoch: 0.931 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.84798051928068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.84798051928068 | validation: 0.8689572141188032]
	TIME [epoch: 0.935 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.834558827713851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.834558827713851 | validation: 0.9234384942131876]
	TIME [epoch: 0.931 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8555927977093285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8555927977093285 | validation: 0.9323967638725803]
	TIME [epoch: 0.934 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9463930243867354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9463930243867354 | validation: 1.1024158804814401]
	TIME [epoch: 0.931 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0490239162970187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0490239162970187 | validation: 0.9460992185682779]
	TIME [epoch: 0.934 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9489632375972642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9489632375972642 | validation: 0.8531534824030417]
	TIME [epoch: 0.933 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8018734284620181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8018734284620181 | validation: 0.8617277160353268]
	TIME [epoch: 0.935 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8336665733016599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8336665733016599 | validation: 0.8918234168150665]
	TIME [epoch: 0.933 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8327923685159331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8327923685159331 | validation: 0.8599426582843555]
	TIME [epoch: 0.935 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7985225575056794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7985225575056794 | validation: 0.837265286034869]
	TIME [epoch: 0.942 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_149.pth
	Model improved!!!
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.813373072919687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.813373072919687 | validation: 0.8904529237661735]
	TIME [epoch: 0.934 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8166205193233751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8166205193233751 | validation: 0.8408140172672365]
	TIME [epoch: 0.934 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8150193809641939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8150193809641939 | validation: 0.9084551403991693]
	TIME [epoch: 0.93 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8403064584945742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8403064584945742 | validation: 0.9366158901388458]
	TIME [epoch: 0.934 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9280822682190644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9280822682190644 | validation: 1.021881139771045]
	TIME [epoch: 0.934 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9486029112439374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9486029112439374 | validation: 0.8916384438927007]
	TIME [epoch: 0.934 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8535928392421732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8535928392421732 | validation: 0.8334980259535165]
	TIME [epoch: 0.931 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7888535450118543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7888535450118543 | validation: 0.8304479192077046]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7783330595141555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7783330595141555 | validation: 0.818269575431144]
	TIME [epoch: 0.932 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_158.pth
	Model improved!!!
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7741372132177662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7741372132177662 | validation: 0.8089755924951954]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_159.pth
	Model improved!!!
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7712527068421673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7712527068421673 | validation: 0.8169716343355437]
	TIME [epoch: 0.936 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.776407607590207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.776407607590207 | validation: 0.8110216627612712]
	TIME [epoch: 0.937 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7799371435799618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7799371435799618 | validation: 0.8564958872061692]
	TIME [epoch: 0.94 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7939824305767901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7939824305767901 | validation: 0.9016868727541806]
	TIME [epoch: 0.95 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8866821761485173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8866821761485173 | validation: 1.088106982407318]
	TIME [epoch: 0.936 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0027882445312866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0027882445312866 | validation: 0.9979858357407352]
	TIME [epoch: 0.935 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0413689142294902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0413689142294902 | validation: 0.8208821815916976]
	TIME [epoch: 0.935 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7693594082110362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7693594082110362 | validation: 0.8220144625738094]
	TIME [epoch: 0.937 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8044956505375095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8044956505375095 | validation: 0.8636773103550794]
	TIME [epoch: 0.936 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8536094248531494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8536094248531494 | validation: 0.8203045717542651]
	TIME [epoch: 0.936 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7812535503563433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7812535503563433 | validation: 0.8120599812716164]
	TIME [epoch: 0.935 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7721591489144988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7721591489144988 | validation: 0.8432745855842495]
	TIME [epoch: 0.937 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7855614082699793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7855614082699793 | validation: 0.8024042710835059]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7696380657786037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7696380657786037 | validation: 0.8087361549098073]
	TIME [epoch: 0.936 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7657109228546806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7657109228546806 | validation: 0.7701350452186763]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7602978870555438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7602978870555438 | validation: 0.8402673259120452]
	TIME [epoch: 0.937 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7908085121570056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7908085121570056 | validation: 0.8703867899227138]
	TIME [epoch: 0.937 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8869002257599999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8869002257599999 | validation: 0.920203683727134]
	TIME [epoch: 0.935 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8810536796370937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8810536796370937 | validation: 0.8159801525009629]
	TIME [epoch: 0.936 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8221072179813157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8221072179813157 | validation: 0.7762969755279565]
	TIME [epoch: 0.936 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7432751590111627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7432751590111627 | validation: 0.7506304973853868]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_180.pth
	Model improved!!!
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7278738595292876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7278738595292876 | validation: 0.7480386572191944]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7334089437841445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7334089437841445 | validation: 0.7305881748341116]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_182.pth
	Model improved!!!
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.72956319610984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.72956319610984 | validation: 0.7690334715488816]
	TIME [epoch: 0.937 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7478123400442498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7478123400442498 | validation: 0.7706978908988449]
	TIME [epoch: 0.937 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7694114900303253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7694114900303253 | validation: 0.7772319604042585]
	TIME [epoch: 0.936 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7838228745775436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7838228745775436 | validation: 0.7566950010756613]
	TIME [epoch: 0.934 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7213591206132509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7213591206132509 | validation: 0.7014391920942455]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_187.pth
	Model improved!!!
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6983356603442985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6983356603442985 | validation: 0.7915639312055629]
	TIME [epoch: 0.936 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7288505283119193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7288505283119193 | validation: 0.7331465026659317]
	TIME [epoch: 0.934 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7638522351869711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7638522351869711 | validation: 0.8298765480228362]
	TIME [epoch: 0.934 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7659540279022641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7659540279022641 | validation: 0.6905706886601675]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_191.pth
	Model improved!!!
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7058960164531309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7058960164531309 | validation: 0.7145025299570203]
	TIME [epoch: 0.934 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7492022694174607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7492022694174607 | validation: 0.7768071699068397]
	TIME [epoch: 0.933 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.871411078138668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.871411078138668 | validation: 0.7024243172133025]
	TIME [epoch: 0.932 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6832344016611398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6832344016611398 | validation: 0.6886579216902814]
	TIME [epoch: 0.933 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6691061196352819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6691061196352819 | validation: 0.660113821822478]
	TIME [epoch: 0.933 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6772689040833582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6772689040833582 | validation: 0.6724736396013468]
	TIME [epoch: 0.934 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6898482828493715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6898482828493715 | validation: 0.6711431383619053]
	TIME [epoch: 0.933 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.732315553142678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.732315553142678 | validation: 0.7540891812329877]
	TIME [epoch: 0.933 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7311994343582504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7311994343582504 | validation: 0.6848963470718357]
	TIME [epoch: 0.933 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7300009778461979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7300009778461979 | validation: 0.787870107067962]
	TIME [epoch: 24.6 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7358660812560893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7358660812560893 | validation: 0.6130903342302821]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6329324907276247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6329324907276247 | validation: 0.631699375331828]
	TIME [epoch: 1.83 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6364300034836772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6364300034836772 | validation: 0.647038975092284]
	TIME [epoch: 1.83 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6511894504501146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6511894504501146 | validation: 0.6660805425392611]
	TIME [epoch: 1.83 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.710858862713026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.710858862713026 | validation: 0.6987533683987541]
	TIME [epoch: 1.83 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7408817408081732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7408817408081732 | validation: 0.706507983619935]
	TIME [epoch: 1.83 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6628816535596329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6628816535596329 | validation: 0.5852521666122311]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6035700342671801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6035700342671801 | validation: 0.6481261216882537]
	TIME [epoch: 1.83 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6285688647086801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6285688647086801 | validation: 0.5836269457945626]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_210.pth
	Model improved!!!
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6241116026825065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6241116026825065 | validation: 0.6732491118487043]
	TIME [epoch: 1.84 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.625291210476036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.625291210476036 | validation: 0.5261849777226061]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5693235485212434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5693235485212434 | validation: 0.5857166335406947]
	TIME [epoch: 1.83 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5825304206339113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5825304206339113 | validation: 0.6576225923330425]
	TIME [epoch: 1.83 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6611209320491185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6611209320491185 | validation: 0.6469085511574542]
	TIME [epoch: 1.83 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6994152546109664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6994152546109664 | validation: 0.6526911870496565]
	TIME [epoch: 1.84 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5868676141068362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5868676141068362 | validation: 0.5280503087335341]
	TIME [epoch: 1.84 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5274038359338341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5274038359338341 | validation: 0.5256183521506852]
	TIME [epoch: 1.85 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_218.pth
	Model improved!!!
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5427028651952691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5427028651952691 | validation: 0.519766338243662]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.508801859888769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.508801859888769 | validation: 0.5239873816009081]
	TIME [epoch: 1.83 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.498797195760142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.498797195760142 | validation: 0.5081953298624943]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47538139032402416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47538139032402416 | validation: 0.5004735613875999]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_222.pth
	Model improved!!!
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4577177696808402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4577177696808402 | validation: 0.4677818548262924]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4828065606262075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4828065606262075 | validation: 0.6804452483035727]
	TIME [epoch: 1.84 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5836026389896486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5836026389896486 | validation: 0.5473654125275915]
	TIME [epoch: 1.84 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5954963058039671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5954963058039671 | validation: 0.7111989301026118]
	TIME [epoch: 1.84 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.57453694083155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.57453694083155 | validation: 0.6079743990807448]
	TIME [epoch: 1.83 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5272201288573729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5272201288573729 | validation: 0.49732026694813025]
	TIME [epoch: 1.84 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5486555841520204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5486555841520204 | validation: 0.49534908812915923]
	TIME [epoch: 1.83 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40774879861653934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40774879861653934 | validation: 0.5002828358393168]
	TIME [epoch: 1.83 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49125940438609167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49125940438609167 | validation: 0.5838779121323795]
	TIME [epoch: 1.84 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48890079351173965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48890079351173965 | validation: 0.45058691881146623]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_232.pth
	Model improved!!!
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42040584711051515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42040584711051515 | validation: 0.48447718686381]
	TIME [epoch: 1.84 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3785566904829679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3785566904829679 | validation: 0.3912888048422445]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_234.pth
	Model improved!!!
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3774647958120354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3774647958120354 | validation: 0.49739031740815065]
	TIME [epoch: 1.83 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38234697800890793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38234697800890793 | validation: 0.4075863682445542]
	TIME [epoch: 1.83 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40475067647788576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40475067647788576 | validation: 0.6163517117286688]
	TIME [epoch: 1.83 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4664571265971007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4664571265971007 | validation: 0.48637015606670747]
	TIME [epoch: 1.83 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4230005982644749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4230005982644749 | validation: 0.4960939577702993]
	TIME [epoch: 1.84 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5421894324783137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5421894324783137 | validation: 0.5256757459247586]
	TIME [epoch: 1.84 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40736702773343836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40736702773343836 | validation: 0.37630676905863064]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_241.pth
	Model improved!!!
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33406183934449646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33406183934449646 | validation: 0.3932227895094738]
	TIME [epoch: 1.83 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3461753319797965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3461753319797965 | validation: 0.3838886958712725]
	TIME [epoch: 1.83 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33522443062473883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33522443062473883 | validation: 0.47239843908964674]
	TIME [epoch: 1.83 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3606206486827196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3606206486827196 | validation: 0.3831683388945852]
	TIME [epoch: 1.83 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39253394287679555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39253394287679555 | validation: 0.6252081798634749]
	TIME [epoch: 1.83 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45539477641456577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45539477641456577 | validation: 0.4640865742767005]
	TIME [epoch: 1.83 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32855631686626074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32855631686626074 | validation: 0.4661613383991129]
	TIME [epoch: 1.83 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5078534624538323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5078534624538323 | validation: 0.5740927728512673]
	TIME [epoch: 1.83 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41603795060147547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41603795060147547 | validation: 0.47825497927034066]
	TIME [epoch: 1.83 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35524763671696447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35524763671696447 | validation: 0.4389348642536858]
	TIME [epoch: 1.83 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45786568802799665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45786568802799665 | validation: 0.39507896367198936]
	TIME [epoch: 1.83 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30387144624462076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30387144624462076 | validation: 0.3456451573649246]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_253.pth
	Model improved!!!
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2736573898808196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2736573898808196 | validation: 0.2945829078660937]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_254.pth
	Model improved!!!
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2774395386202587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2774395386202587 | validation: 0.467549793236078]
	TIME [epoch: 1.84 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.341602042490269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.341602042490269 | validation: 0.3051433658228122]
	TIME [epoch: 1.84 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.262637760274596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.262637760274596 | validation: 0.30854728832388034]
	TIME [epoch: 1.84 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2747429010687268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2747429010687268 | validation: 0.3751310903885704]
	TIME [epoch: 1.83 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3372062373417056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3372062373417056 | validation: 0.6472837515298863]
	TIME [epoch: 1.84 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5738174091256467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5738174091256467 | validation: 0.39233852289732796]
	TIME [epoch: 1.83 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3363978789934529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3363978789934529 | validation: 0.5863764379628383]
	TIME [epoch: 1.84 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5530066887446894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5530066887446894 | validation: 0.4723278739172174]
	TIME [epoch: 1.84 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35764760394662326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35764760394662326 | validation: 0.33269871562528364]
	TIME [epoch: 1.83 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24918684768845376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24918684768845376 | validation: 0.3573448555997931]
	TIME [epoch: 1.83 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2969310172370135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2969310172370135 | validation: 0.38871315868849055]
	TIME [epoch: 1.83 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3013157903003621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3013157903003621 | validation: 0.30104452677135063]
	TIME [epoch: 1.83 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25415816350498655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25415816350498655 | validation: 0.33139658175608]
	TIME [epoch: 1.83 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25652118259796985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25652118259796985 | validation: 0.28263008952977314]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_268.pth
	Model improved!!!
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22029780197929724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22029780197929724 | validation: 0.25878840810258485]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_269.pth
	Model improved!!!
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21301621456148753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21301621456148753 | validation: 0.274979242068571]
	TIME [epoch: 1.83 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20891173699341523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20891173699341523 | validation: 0.25727833753771173]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_271.pth
	Model improved!!!
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.220167775301506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.220167775301506 | validation: 0.4461601570500192]
	TIME [epoch: 1.83 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42837309434588833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42837309434588833 | validation: 0.6461744330261829]
	TIME [epoch: 1.83 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48964087500503606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48964087500503606 | validation: 0.5065568992803228]
	TIME [epoch: 1.83 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40833013963447257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40833013963447257 | validation: 0.3617009487361396]
	TIME [epoch: 1.83 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2630325325533532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2630325325533532 | validation: 0.2941739476764309]
	TIME [epoch: 1.83 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22291286353543993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22291286353543993 | validation: 0.2996143255595063]
	TIME [epoch: 1.83 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2193385743708781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2193385743708781 | validation: 0.3244036632650683]
	TIME [epoch: 1.83 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24986233731587362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24986233731587362 | validation: 0.25816836095187495]
	TIME [epoch: 1.83 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20524616759590877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20524616759590877 | validation: 0.2715453144795029]
	TIME [epoch: 1.82 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.201587556392112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.201587556392112 | validation: 0.3445949317658599]
	TIME [epoch: 1.83 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26947418971748227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26947418971748227 | validation: 0.43874394386707216]
	TIME [epoch: 1.83 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3587527972397552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3587527972397552 | validation: 0.34132474689566156]
	TIME [epoch: 1.84 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2387682919448515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2387682919448515 | validation: 0.3361555923225264]
	TIME [epoch: 1.83 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3808388397699006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3808388397699006 | validation: 0.38241818419443147]
	TIME [epoch: 1.83 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26528214895805674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26528214895805674 | validation: 0.262367773772454]
	TIME [epoch: 1.83 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17498704586930738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17498704586930738 | validation: 0.264922226404121]
	TIME [epoch: 1.83 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2271330890826185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2271330890826185 | validation: 0.3347588246929946]
	TIME [epoch: 1.83 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2911129591411875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2911129591411875 | validation: 0.6358436019594829]
	TIME [epoch: 1.83 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4430513289276335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4430513289276335 | validation: 0.5951291509756901]
	TIME [epoch: 1.83 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43524444903927867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43524444903927867 | validation: 0.33940097138739134]
	TIME [epoch: 1.83 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24648939121735594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24648939121735594 | validation: 0.30238044924209073]
	TIME [epoch: 1.83 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2508726317646678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2508726317646678 | validation: 0.3439487145488142]
	TIME [epoch: 1.83 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2203086007410246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2203086007410246 | validation: 0.25157497147436797]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_294.pth
	Model improved!!!
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16699840709572478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16699840709572478 | validation: 0.20411581878023044]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_295.pth
	Model improved!!!
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16842655081062652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16842655081062652 | validation: 0.2732782929000874]
	TIME [epoch: 1.83 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20643697698484254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20643697698484254 | validation: 0.30709406729334443]
	TIME [epoch: 1.83 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2874637288034042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2874637288034042 | validation: 0.35922368472192706]
	TIME [epoch: 1.83 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2922180851884899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2922180851884899 | validation: 0.45598656948163035]
	TIME [epoch: 1.83 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32837756033274673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32837756033274673 | validation: 0.43736272306678625]
	TIME [epoch: 1.83 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35795845921798347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35795845921798347 | validation: 0.34555175486397305]
	TIME [epoch: 1.83 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28864268157754885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28864268157754885 | validation: 0.34823052847300495]
	TIME [epoch: 1.83 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22292113700298732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22292113700298732 | validation: 0.2758637317770362]
	TIME [epoch: 1.83 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18090409522288475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18090409522288475 | validation: 0.21106299263336356]
	TIME [epoch: 1.84 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1606773947010053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1606773947010053 | validation: 0.6325745453155203]
	TIME [epoch: 1.84 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.502458447612716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.502458447612716 | validation: 0.8691654721936578]
	TIME [epoch: 1.83 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7982299839358121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7982299839358121 | validation: 0.8737209098764392]
	TIME [epoch: 1.83 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6829517269921341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6829517269921341 | validation: 0.5683877480520334]
	TIME [epoch: 1.83 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5233068535642188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5233068535642188 | validation: 0.33524709853032997]
	TIME [epoch: 1.83 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31341520978195603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31341520978195603 | validation: 0.3391474372479557]
	TIME [epoch: 1.83 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26676945906661403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26676945906661403 | validation: 0.37439282781828115]
	TIME [epoch: 1.84 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2485151741205141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2485151741205141 | validation: 0.2887633606931523]
	TIME [epoch: 1.83 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17698462391243752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17698462391243752 | validation: 0.19183478552550365]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_313.pth
	Model improved!!!
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1709392902483342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1709392902483342 | validation: 0.2828018201702827]
	TIME [epoch: 1.83 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18884122410230106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18884122410230106 | validation: 0.22374870310850892]
	TIME [epoch: 1.83 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16034460190275204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16034460190275204 | validation: 0.29244812416148064]
	TIME [epoch: 1.83 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22692503070784298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22692503070784298 | validation: 0.3464716425391099]
	TIME [epoch: 1.83 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25776290556968495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25776290556968495 | validation: 0.6271371493695934]
	TIME [epoch: 1.83 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5082002704655281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5082002704655281 | validation: 0.7261409386290217]
	TIME [epoch: 1.83 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5469177501811866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5469177501811866 | validation: 0.8953995875547976]
	TIME [epoch: 1.83 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.738806851359731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.738806851359731 | validation: 0.6890367006690387]
	TIME [epoch: 1.83 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5529214126622556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5529214126622556 | validation: 0.7409065886850301]
	TIME [epoch: 1.83 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5477999173835635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5477999173835635 | validation: 0.6674035415062245]
	TIME [epoch: 1.83 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5286789177267321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5286789177267321 | validation: 0.6748569541247416]
	TIME [epoch: 1.83 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48751896097878594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48751896097878594 | validation: 0.4901536568069429]
	TIME [epoch: 1.83 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3747899413389736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3747899413389736 | validation: 0.3831713958425771]
	TIME [epoch: 1.83 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25269055368759324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25269055368759324 | validation: 0.29346062391766936]
	TIME [epoch: 1.84 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26428608241639556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26428608241639556 | validation: 0.3379136517918704]
	TIME [epoch: 1.83 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22011014343785754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22011014343785754 | validation: 0.24741413883407276]
	TIME [epoch: 1.83 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16819628534893308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16819628534893308 | validation: 0.27171188022920795]
	TIME [epoch: 1.83 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16408513014108192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16408513014108192 | validation: 0.22545510097619703]
	TIME [epoch: 1.83 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18473929165899464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18473929165899464 | validation: 0.32610352273473414]
	TIME [epoch: 1.83 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22011186997549817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22011186997549817 | validation: 0.2605933143414279]
	TIME [epoch: 1.82 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22586906752402697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22586906752402697 | validation: 0.3572329122374522]
	TIME [epoch: 1.83 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24158570202260898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24158570202260898 | validation: 0.2633120075869327]
	TIME [epoch: 1.82 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18996427402634491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18996427402634491 | validation: 0.25397805387938227]
	TIME [epoch: 1.82 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2782857594246186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2782857594246186 | validation: 0.31535573402465733]
	TIME [epoch: 1.82 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19968535080260857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19968535080260857 | validation: 0.20770489434235834]
	TIME [epoch: 1.82 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12307465925152211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12307465925152211 | validation: 0.19520619336792652]
	TIME [epoch: 1.82 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16216605315392227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16216605315392227 | validation: 0.30576550715788503]
	TIME [epoch: 1.82 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22933328002396494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22933328002396494 | validation: 0.33238944368285267]
	TIME [epoch: 1.82 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2779106994497517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2779106994497517 | validation: 0.2739550543727934]
	TIME [epoch: 1.82 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17907926343223396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17907926343223396 | validation: 0.2728838927055655]
	TIME [epoch: 1.83 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18739248953413198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18739248953413198 | validation: 0.2222245468881796]
	TIME [epoch: 1.82 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14005462383132708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14005462383132708 | validation: 0.1943911691183351]
	TIME [epoch: 1.83 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15269538137280195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15269538137280195 | validation: 0.294902912275084]
	TIME [epoch: 1.83 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21858532058689203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21858532058689203 | validation: 0.33198676353996803]
	TIME [epoch: 1.83 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25430418665431814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25430418665431814 | validation: 0.24241886224090423]
	TIME [epoch: 1.83 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1527926634763365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1527926634763365 | validation: 0.19914362521681994]
	TIME [epoch: 1.85 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15949801106539468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15949801106539468 | validation: 0.2102349195986571]
	TIME [epoch: 1.83 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13097930000504568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13097930000504568 | validation: 0.17259831335628004]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_351.pth
	Model improved!!!
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11360393853367017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11360393853367017 | validation: 0.20814664766031432]
	TIME [epoch: 1.83 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13970858126979976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13970858126979976 | validation: 0.3005278386327056]
	TIME [epoch: 1.83 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19442518494900385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19442518494900385 | validation: 0.35122504893658674]
	TIME [epoch: 1.83 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25594233531728316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25594233531728316 | validation: 0.4380707640176077]
	TIME [epoch: 1.83 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3462844798992504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3462844798992504 | validation: 0.40950389876826576]
	TIME [epoch: 1.83 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30936757780545415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30936757780545415 | validation: 0.23123172224577188]
	TIME [epoch: 1.83 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16998215660224159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16998215660224159 | validation: 0.19773085475362195]
	TIME [epoch: 1.83 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12665404908863445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12665404908863445 | validation: 0.20799460070814152]
	TIME [epoch: 1.83 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12231299360878424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12231299360878424 | validation: 0.1597564412105666]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_360.pth
	Model improved!!!
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11173846087863767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11173846087863767 | validation: 0.17479363813536258]
	TIME [epoch: 1.83 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09470533292582342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09470533292582342 | validation: 0.15255411158127036]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_362.pth
	Model improved!!!
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09156933977899452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09156933977899452 | validation: 0.1761498869000837]
	TIME [epoch: 1.83 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10179979228068224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10179979228068224 | validation: 0.22739438173805226]
	TIME [epoch: 1.83 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1829747273031871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1829747273031871 | validation: 0.49486754578537345]
	TIME [epoch: 1.83 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41141877450688674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41141877450688674 | validation: 0.5303250279708397]
	TIME [epoch: 1.83 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4578842029100319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4578842029100319 | validation: 0.4993209950523243]
	TIME [epoch: 1.83 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3477216611382701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3477216611382701 | validation: 0.26937167489771624]
	TIME [epoch: 1.83 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1865469449942921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1865469449942921 | validation: 0.24278704505635407]
	TIME [epoch: 1.83 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2019312433584615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2019312433584615 | validation: 0.2506846971509284]
	TIME [epoch: 1.84 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14905056867092772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14905056867092772 | validation: 0.1899796707668614]
	TIME [epoch: 1.84 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1092744477579378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1092744477579378 | validation: 0.1399748799244593]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_372.pth
	Model improved!!!
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10038702763297781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10038702763297781 | validation: 0.1776664609147868]
	TIME [epoch: 1.83 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10168424350780612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10168424350780612 | validation: 0.1430740882031569]
	TIME [epoch: 1.83 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12067868387799567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12067868387799567 | validation: 0.24884055576398453]
	TIME [epoch: 1.83 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16317603276835854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16317603276835854 | validation: 0.26867946483965177]
	TIME [epoch: 1.83 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2429291786225223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2429291786225223 | validation: 0.25941231026840933]
	TIME [epoch: 1.82 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1807343473886157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1807343473886157 | validation: 0.1863778372766812]
	TIME [epoch: 1.83 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1448089321394246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1448089321394246 | validation: 0.2383322886949281]
	TIME [epoch: 1.83 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19382560019877373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19382560019877373 | validation: 0.28231127351665963]
	TIME [epoch: 1.83 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21824144882336702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21824144882336702 | validation: 0.16164953971635865]
	TIME [epoch: 1.83 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11789334072004164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11789334072004164 | validation: 0.23546935907876498]
	TIME [epoch: 1.83 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13186876860040692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13186876860040692 | validation: 0.23103530149102128]
	TIME [epoch: 1.83 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1824390450039088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1824390450039088 | validation: 0.284898997100223]
	TIME [epoch: 1.83 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19209568570966085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19209568570966085 | validation: 0.18324263262213716]
	TIME [epoch: 1.83 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14296470112571957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14296470112571957 | validation: 0.17262296742100963]
	TIME [epoch: 1.83 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10647104152163385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10647104152163385 | validation: 0.18447589768383976]
	TIME [epoch: 1.83 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12143730644960687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12143730644960687 | validation: 0.21534382683856032]
	TIME [epoch: 1.83 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13964659704259222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13964659704259222 | validation: 0.35055613729021423]
	TIME [epoch: 1.83 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2604135923960727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2604135923960727 | validation: 0.23503052984116923]
	TIME [epoch: 1.83 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1657672813178214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1657672813178214 | validation: 0.2523127199016764]
	TIME [epoch: 1.83 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22404715548679804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22404715548679804 | validation: 0.4266535824314687]
	TIME [epoch: 1.82 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3332958423135802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3332958423135802 | validation: 0.4013539783559069]
	TIME [epoch: 1.84 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3036483148634209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3036483148634209 | validation: 0.2256693795231962]
	TIME [epoch: 1.83 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14914869463020813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14914869463020813 | validation: 0.19793538450876466]
	TIME [epoch: 1.83 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15163221296197027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15163221296197027 | validation: 0.1826127240922032]
	TIME [epoch: 1.83 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11501566115226246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11501566115226246 | validation: 0.14677569858291512]
	TIME [epoch: 1.83 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0857849259898533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0857849259898533 | validation: 0.14720825511018007]
	TIME [epoch: 1.83 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08412773848482255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08412773848482255 | validation: 0.1364578682535748]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_399.pth
	Model improved!!!
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08682128205796769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08682128205796769 | validation: 0.16019642366699538]
	TIME [epoch: 1.83 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0839590576292122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0839590576292122 | validation: 0.14537715163917322]
	TIME [epoch: 1.83 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11633052492132356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11633052492132356 | validation: 0.2604514614544288]
	TIME [epoch: 1.83 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20653051481883883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20653051481883883 | validation: 0.36827886391696035]
	TIME [epoch: 1.83 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36299688867512175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36299688867512175 | validation: 0.31543550773384593]
	TIME [epoch: 1.83 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23655125134834168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23655125134834168 | validation: 0.15559970236568435]
	TIME [epoch: 1.83 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10486615682615817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10486615682615817 | validation: 0.1607988627911407]
	TIME [epoch: 1.83 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09445709621615396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09445709621615396 | validation: 0.15192975014794718]
	TIME [epoch: 1.83 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09937208404814162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09937208404814162 | validation: 0.1669817180078033]
	TIME [epoch: 1.83 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11231818340415607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11231818340415607 | validation: 0.14752227202139334]
	TIME [epoch: 1.83 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10832062782887159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10832062782887159 | validation: 0.20998151256463676]
	TIME [epoch: 1.83 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13674029427496634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13674029427496634 | validation: 0.20311359529581474]
	TIME [epoch: 1.83 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1770556363322041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1770556363322041 | validation: 0.2641557395714795]
	TIME [epoch: 1.83 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19762640838939036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19762640838939036 | validation: 0.31840874659852253]
	TIME [epoch: 1.82 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26605936841131533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26605936841131533 | validation: 0.22332900391627103]
	TIME [epoch: 1.83 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15857976874494548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15857976874494548 | validation: 0.19831505488086887]
	TIME [epoch: 1.84 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16171728450946013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16171728450946013 | validation: 0.4023312535554296]
	TIME [epoch: 1.83 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2661018733087753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2661018733087753 | validation: 0.3464091339529236]
	TIME [epoch: 1.83 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2203544078676829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2203544078676829 | validation: 0.22604061954160792]
	TIME [epoch: 1.83 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17303134537756598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17303134537756598 | validation: 0.19303113791558696]
	TIME [epoch: 1.83 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11225337905108955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11225337905108955 | validation: 0.15134576359939325]
	TIME [epoch: 1.83 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08932067062050711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08932067062050711 | validation: 0.14144718650395754]
	TIME [epoch: 1.83 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0810841524969913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0810841524969913 | validation: 0.12897161385009479]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_422.pth
	Model improved!!!
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06876553120019258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06876553120019258 | validation: 0.1274463662146073]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_423.pth
	Model improved!!!
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06569348904639337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06569348904639337 | validation: 0.12154436114781993]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_424.pth
	Model improved!!!
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07297397306085132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07297397306085132 | validation: 0.16671424238667534]
	TIME [epoch: 1.83 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1156379448972217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1156379448972217 | validation: 0.31189518389129106]
	TIME [epoch: 1.83 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2548877635289481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2548877635289481 | validation: 0.36998025744530083]
	TIME [epoch: 1.83 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3767954385397805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3767954385397805 | validation: 0.16778586819977925]
	TIME [epoch: 1.83 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11385255998165711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11385255998165711 | validation: 0.12228842297044296]
	TIME [epoch: 1.83 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07571874662905122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07571874662905122 | validation: 0.1323730961648046]
	TIME [epoch: 1.82 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09186088191385398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09186088191385398 | validation: 0.13029527263005572]
	TIME [epoch: 1.83 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08307004342757247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08307004342757247 | validation: 0.11206339194268147]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_432.pth
	Model improved!!!
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07395811244381408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07395811244381408 | validation: 0.15975339612663034]
	TIME [epoch: 1.84 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1094799213651218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1094799213651218 | validation: 0.1903226295647239]
	TIME [epoch: 1.84 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14569208844309572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14569208844309572 | validation: 0.3226704607388188]
	TIME [epoch: 1.84 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2656636414752095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2656636414752095 | validation: 0.18427163403302904]
	TIME [epoch: 1.85 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11785779941564312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11785779941564312 | validation: 0.22088917541739989]
	TIME [epoch: 1.84 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15844397273580704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15844397273580704 | validation: 0.1480371286569891]
	TIME [epoch: 1.84 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13119380559115293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13119380559115293 | validation: 0.26083689847886266]
	TIME [epoch: 1.84 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1785440389396389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1785440389396389 | validation: 0.18088337100630425]
	TIME [epoch: 1.84 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17238050822319487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17238050822319487 | validation: 0.19945502493689748]
	TIME [epoch: 1.84 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11224354286789774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11224354286789774 | validation: 0.18834822386301275]
	TIME [epoch: 1.83 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13590449506814367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13590449506814367 | validation: 0.14556138936488866]
	TIME [epoch: 1.84 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08525556622183665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08525556622183665 | validation: 0.12166287595036517]
	TIME [epoch: 1.83 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08084162996310394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08084162996310394 | validation: 0.1310648717904185]
	TIME [epoch: 1.84 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08757429556121664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08757429556121664 | validation: 0.19351398166292957]
	TIME [epoch: 1.83 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14184279275806522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14184279275806522 | validation: 0.20790673720754557]
	TIME [epoch: 1.84 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14359561698134674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14359561698134674 | validation: 0.1757696525860348]
	TIME [epoch: 1.84 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16446624264137302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16446624264137302 | validation: 0.16632791250807394]
	TIME [epoch: 1.84 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12419519836613857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12419519836613857 | validation: 0.1341310338372867]
	TIME [epoch: 1.84 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10538168110682163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10538168110682163 | validation: 0.16100864874212872]
	TIME [epoch: 1.84 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12846885505762518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12846885505762518 | validation: 0.22127172570983122]
	TIME [epoch: 1.83 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16318969278072007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16318969278072007 | validation: 0.16774893987113376]
	TIME [epoch: 1.84 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14560414876287553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14560414876287553 | validation: 0.15891758130747655]
	TIME [epoch: 1.83 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09997959819216569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09997959819216569 | validation: 0.08975898568232477]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_455.pth
	Model improved!!!
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07955263374947673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07955263374947673 | validation: 0.15401572490061854]
	TIME [epoch: 1.83 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09424770775077157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09424770775077157 | validation: 0.206689990536145]
	TIME [epoch: 1.83 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16085425877757054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16085425877757054 | validation: 0.2798213639401008]
	TIME [epoch: 1.84 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18980017997584803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18980017997584803 | validation: 0.1707581464088975]
	TIME [epoch: 1.83 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1498932151968344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1498932151968344 | validation: 0.127804012395876]
	TIME [epoch: 1.83 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09192508563292037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09192508563292037 | validation: 0.10719104701920852]
	TIME [epoch: 1.83 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07868770694050504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07868770694050504 | validation: 0.11988011177827991]
	TIME [epoch: 1.83 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07539665308310578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07539665308310578 | validation: 0.14983129916186896]
	TIME [epoch: 1.83 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12300090040034356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12300090040034356 | validation: 0.17358018244300763]
	TIME [epoch: 1.83 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.115188355637905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.115188355637905 | validation: 0.12354560640933215]
	TIME [epoch: 1.83 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09575464483312054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09575464483312054 | validation: 0.1781430263683836]
	TIME [epoch: 1.83 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12161833367926157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12161833367926157 | validation: 0.24029882314929107]
	TIME [epoch: 1.83 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17842297400309803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17842297400309803 | validation: 0.12091302800433774]
	TIME [epoch: 1.83 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07658275847693209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07658275847693209 | validation: 0.09993956746014254]
	TIME [epoch: 1.83 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06096028014313145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06096028014313145 | validation: 0.08665244078096707]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_470.pth
	Model improved!!!
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06548309062146919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06548309062146919 | validation: 0.12586002887507428]
	TIME [epoch: 1.83 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09480926150932385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09480926150932385 | validation: 0.1479411098457731]
	TIME [epoch: 1.82 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10558400189240118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10558400189240118 | validation: 0.15987518700741954]
	TIME [epoch: 1.83 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11678441791460775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11678441791460775 | validation: 0.31576982445696045]
	TIME [epoch: 1.83 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28459917087448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28459917087448 | validation: 0.3423420657810192]
	TIME [epoch: 1.83 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32262449925550246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32262449925550246 | validation: 0.15380299357199193]
	TIME [epoch: 1.83 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10859567326728381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10859567326728381 | validation: 0.11682657676841518]
	TIME [epoch: 1.82 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08643497030730217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08643497030730217 | validation: 0.1300681979619677]
	TIME [epoch: 1.83 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08716336282021421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08716336282021421 | validation: 0.11347537901277532]
	TIME [epoch: 1.84 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08616488074878997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08616488074878997 | validation: 0.1252802355650887]
	TIME [epoch: 1.84 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07538689847446556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07538689847446556 | validation: 0.13612576421676278]
	TIME [epoch: 1.83 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09869393800967056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09869393800967056 | validation: 0.13548155788187052]
	TIME [epoch: 1.83 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08610740419016215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08610740419016215 | validation: 0.16591886307165932]
	TIME [epoch: 1.83 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13191181459648849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13191181459648849 | validation: 0.1277876816980799]
	TIME [epoch: 1.83 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09283340818915954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09283340818915954 | validation: 0.12084036758188206]
	TIME [epoch: 1.83 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1055551752628367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1055551752628367 | validation: 0.11890116566850036]
	TIME [epoch: 1.83 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09325783567264408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09325783567264408 | validation: 0.1385036055121879]
	TIME [epoch: 1.83 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10734327557476928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10734327557476928 | validation: 0.24529853340422783]
	TIME [epoch: 1.83 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20643381190761534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20643381190761534 | validation: 0.18082432191935266]
	TIME [epoch: 1.83 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15249474717898281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15249474717898281 | validation: 0.11231449861978865]
	TIME [epoch: 1.83 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07592911825651685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07592911825651685 | validation: 0.09423328563388761]
	TIME [epoch: 1.83 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06807179317261303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06807179317261303 | validation: 0.12305515899518389]
	TIME [epoch: 1.83 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09967818894754413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09967818894754413 | validation: 0.13148929471604962]
	TIME [epoch: 1.83 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11408912040129991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11408912040129991 | validation: 0.1645656903429319]
	TIME [epoch: 1.88 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11819130027144396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11819130027144396 | validation: 0.12179437930284086]
	TIME [epoch: 1.83 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11728487920137212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11728487920137212 | validation: 0.1481005501677487]
	TIME [epoch: 1.83 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1034098540012243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1034098540012243 | validation: 0.15042013305806387]
	TIME [epoch: 1.83 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13455080704912017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13455080704912017 | validation: 0.20285130967484633]
	TIME [epoch: 1.83 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15573647841301835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15573647841301835 | validation: 0.16048528315533872]
	TIME [epoch: 1.83 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13223422101425608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13223422101425608 | validation: 0.09682061996586636]
	TIME [epoch: 1.83 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06507737176080212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06507737176080212 | validation: 0.07694096620019741]
	TIME [epoch: 26.8 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_501.pth
	Model improved!!!
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04735566678342742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04735566678342742 | validation: 0.08035482255767089]
	TIME [epoch: 3.64 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04628956375228071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04628956375228071 | validation: 0.07307381743397515]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_503.pth
	Model improved!!!
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0430734095903957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0430734095903957 | validation: 0.07642224899612376]
	TIME [epoch: 3.62 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048910941377950774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048910941377950774 | validation: 0.09524710406410183]
	TIME [epoch: 3.62 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0712283425769061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0712283425769061 | validation: 0.16007651748043153]
	TIME [epoch: 3.62 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13003969149367342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13003969149367342 | validation: 0.2555219428817918]
	TIME [epoch: 3.63 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23194886026730038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23194886026730038 | validation: 0.20724845366545905]
	TIME [epoch: 3.63 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22326212810525153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22326212810525153 | validation: 0.2757483029080967]
	TIME [epoch: 3.62 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2101012228365311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2101012228365311 | validation: 0.10043806090383872]
	TIME [epoch: 3.63 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09275564464519043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09275564464519043 | validation: 0.08926739159948316]
	TIME [epoch: 3.62 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06804178935766757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06804178935766757 | validation: 0.13730309019041606]
	TIME [epoch: 3.62 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07872690789731138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07872690789731138 | validation: 0.09204843595577317]
	TIME [epoch: 3.62 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06099875316430455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06099875316430455 | validation: 0.09077217546262767]
	TIME [epoch: 3.61 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04909853533793335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04909853533793335 | validation: 0.08517825638495079]
	TIME [epoch: 3.62 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062274600280909254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062274600280909254 | validation: 0.16096588262740608]
	TIME [epoch: 3.6 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11952402251549896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11952402251549896 | validation: 0.20147831714843925]
	TIME [epoch: 3.61 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15344371492010114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15344371492010114 | validation: 0.11150372936627223]
	TIME [epoch: 3.61 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08001857614989785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08001857614989785 | validation: 0.15644589920964871]
	TIME [epoch: 3.61 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1635994159966684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1635994159966684 | validation: 0.2048581008044609]
	TIME [epoch: 3.62 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1837022399437344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1837022399437344 | validation: 0.13014721671687002]
	TIME [epoch: 3.63 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10334426361403043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10334426361403043 | validation: 0.15567485948383264]
	TIME [epoch: 3.63 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09699601446927947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09699601446927947 | validation: 0.20036023126342634]
	TIME [epoch: 3.62 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14204284191900335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14204284191900335 | validation: 0.1722378084731484]
	TIME [epoch: 3.62 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13179654270532898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13179654270532898 | validation: 0.10936417478379874]
	TIME [epoch: 3.61 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10782484801404157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10782484801404157 | validation: 0.0948856655465463]
	TIME [epoch: 3.61 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06700619275647761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06700619275647761 | validation: 0.09789464495015082]
	TIME [epoch: 3.62 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05337444428309057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05337444428309057 | validation: 0.06989824428272333]
	TIME [epoch: 3.6 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_528.pth
	Model improved!!!
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04757737139548262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04757737139548262 | validation: 0.07197727622330179]
	TIME [epoch: 3.62 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04267580330066416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04267580330066416 | validation: 0.0646480968870511]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_530.pth
	Model improved!!!
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04336564960263047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04336564960263047 | validation: 0.09000443163262938]
	TIME [epoch: 3.62 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0623819520553249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0623819520553249 | validation: 0.1856183043799316]
	TIME [epoch: 3.62 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15116355711648471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15116355711648471 | validation: 0.20944063747294953]
	TIME [epoch: 3.63 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20142197444094753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20142197444094753 | validation: 0.11691437959739821]
	TIME [epoch: 3.6 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0877132442422597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0877132442422597 | validation: 0.0570794874542063]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_535.pth
	Model improved!!!
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03750869809820195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03750869809820195 | validation: 0.06923394082376874]
	TIME [epoch: 3.62 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04328622131314048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04328622131314048 | validation: 0.07127593308403808]
	TIME [epoch: 3.61 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0532121379718463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0532121379718463 | validation: 0.11325006669952371]
	TIME [epoch: 3.62 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09919189146289611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09919189146289611 | validation: 0.2598326079557333]
	TIME [epoch: 3.62 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24771521744436353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24771521744436353 | validation: 0.3321615757925612]
	TIME [epoch: 3.61 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3284109455578507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3284109455578507 | validation: 0.18358176086127065]
	TIME [epoch: 3.62 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13092111584860677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13092111584860677 | validation: 0.1041680491302579]
	TIME [epoch: 3.62 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0938674544800017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0938674544800017 | validation: 0.07060748924913829]
	TIME [epoch: 3.62 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058002348464559254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058002348464559254 | validation: 0.14715773276738614]
	TIME [epoch: 3.61 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09506803740764064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09506803740764064 | validation: 0.09730006453906817]
	TIME [epoch: 3.63 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0869431294606676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0869431294606676 | validation: 0.10725798630263979]
	TIME [epoch: 3.61 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0774478667367107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0774478667367107 | validation: 0.11689480098825132]
	TIME [epoch: 3.62 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09197440908558793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09197440908558793 | validation: 0.1259814065107436]
	TIME [epoch: 3.61 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08408291381930695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08408291381930695 | validation: 0.10936965949551883]
	TIME [epoch: 3.62 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10405499171358454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10405499171358454 | validation: 0.16138583272204457]
	TIME [epoch: 3.62 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11811567350587343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11811567350587343 | validation: 0.10486366411970083]
	TIME [epoch: 3.61 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10154884351542237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10154884351542237 | validation: 0.1569291521054086]
	TIME [epoch: 3.62 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09648128485622479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09648128485622479 | validation: 0.09896201618258298]
	TIME [epoch: 3.62 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10446160997576874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10446160997576874 | validation: 0.1105397272854649]
	TIME [epoch: 3.62 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08430818366244389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08430818366244389 | validation: 0.13032093485419619]
	TIME [epoch: 3.62 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07924281489304477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07924281489304477 | validation: 0.08704195833820205]
	TIME [epoch: 3.63 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06337746013848905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06337746013848905 | validation: 0.060855133540187616]
	TIME [epoch: 3.62 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04949686431878325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04949686431878325 | validation: 0.090553579296822]
	TIME [epoch: 3.61 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0566897142589181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0566897142589181 | validation: 0.09466822245856421]
	TIME [epoch: 3.61 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07528228129645242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07528228129645242 | validation: 0.15420355813806796]
	TIME [epoch: 3.61 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09704982976102916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09704982976102916 | validation: 0.0964070463572465]
	TIME [epoch: 3.62 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08295807733664777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08295807733664777 | validation: 0.0999552139544761]
	TIME [epoch: 3.62 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08997530015563913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08997530015563913 | validation: 0.15477310647097126]
	TIME [epoch: 3.63 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14812966360639893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14812966360639893 | validation: 0.1899405401800726]
	TIME [epoch: 3.61 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19187822161613935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19187822161613935 | validation: 0.2833668798326859]
	TIME [epoch: 3.61 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1784406602708657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1784406602708657 | validation: 0.20370939524062628]
	TIME [epoch: 3.61 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16278999602298733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16278999602298733 | validation: 0.08142989664012662]
	TIME [epoch: 3.63 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06368322079931533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06368322079931533 | validation: 0.16942370943269192]
	TIME [epoch: 3.61 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0972194586714117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0972194586714117 | validation: 0.09590824929511389]
	TIME [epoch: 3.61 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0720764413035999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0720764413035999 | validation: 0.06213285248227636]
	TIME [epoch: 3.62 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03866782762828479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03866782762828479 | validation: 0.07397913526328029]
	TIME [epoch: 3.62 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050554347040514765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050554347040514765 | validation: 0.08349871390393938]
	TIME [epoch: 3.62 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06285684561193838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06285684561193838 | validation: 0.10180154129678612]
	TIME [epoch: 3.62 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09308562965667143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09308562965667143 | validation: 0.17641739309545035]
	TIME [epoch: 3.62 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14406036533141775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14406036533141775 | validation: 0.15816450384967073]
	TIME [epoch: 3.62 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14827807695701922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14827807695701922 | validation: 0.1346191848429931]
	TIME [epoch: 3.62 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10466967545797565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10466967545797565 | validation: 0.0934596998230821]
	TIME [epoch: 3.62 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08767502822897674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08767502822897674 | validation: 0.100175974788698]
	TIME [epoch: 3.62 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07686415750251208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07686415750251208 | validation: 0.07948999712906751]
	TIME [epoch: 3.62 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07235649603096313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07235649603096313 | validation: 0.10072735008346989]
	TIME [epoch: 3.63 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07741106839179855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07741106839179855 | validation: 0.09991730350059186]
	TIME [epoch: 3.62 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07622294160423083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07622294160423083 | validation: 0.08134483327467595]
	TIME [epoch: 3.62 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062318813766103716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062318813766103716 | validation: 0.07317134410534235]
	TIME [epoch: 3.63 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041787223944289784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041787223944289784 | validation: 0.05161461128286198]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_584.pth
	Model improved!!!
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0384994109885235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0384994109885235 | validation: 0.08086343948495393]
	TIME [epoch: 3.64 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049860477703019955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049860477703019955 | validation: 0.12442124711650344]
	TIME [epoch: 3.64 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08771292394020101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08771292394020101 | validation: 0.23061411066866683]
	TIME [epoch: 3.64 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15443729656437605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15443729656437605 | validation: 0.1193246425894464]
	TIME [epoch: 3.66 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10176645498074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10176645498074 | validation: 0.16420547592842066]
	TIME [epoch: 3.64 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13242019274432384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13242019274432384 | validation: 0.23936784389968502]
	TIME [epoch: 3.64 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22758621239996676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22758621239996676 | validation: 0.13337736822171592]
	TIME [epoch: 3.65 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11683960126510867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11683960126510867 | validation: 0.08294385555751799]
	TIME [epoch: 3.63 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07172235513076501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07172235513076501 | validation: 0.08549370361382357]
	TIME [epoch: 3.62 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046731721009110945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046731721009110945 | validation: 0.061366654596545556]
	TIME [epoch: 3.62 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03933056381062608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03933056381062608 | validation: 0.05934759512708776]
	TIME [epoch: 3.63 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04914995881953127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04914995881953127 | validation: 0.13465152169390046]
	TIME [epoch: 3.63 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11193584847410573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11193584847410573 | validation: 0.09200351897679632]
	TIME [epoch: 3.64 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08426882707983127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08426882707983127 | validation: 0.08419358572996893]
	TIME [epoch: 3.63 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06770349683110777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06770349683110777 | validation: 0.07721940839758135]
	TIME [epoch: 3.63 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0596190042121941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0596190042121941 | validation: 0.07136448278842529]
	TIME [epoch: 3.64 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06680810021673478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06680810021673478 | validation: 0.08352190289851738]
	TIME [epoch: 3.64 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07711943536520123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07711943536520123 | validation: 0.10446154493918969]
	TIME [epoch: 3.64 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09359178876562223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09359178876562223 | validation: 0.12737862238351236]
	TIME [epoch: 3.65 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10862906511141919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10862906511141919 | validation: 0.15702808927519404]
	TIME [epoch: 3.64 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1293157326146501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1293157326146501 | validation: 0.16898093541721948]
	TIME [epoch: 3.64 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14306626923144036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14306626923144036 | validation: 0.2264502780925322]
	TIME [epoch: 3.63 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23306025773763403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23306025773763403 | validation: 0.17770058121376556]
	TIME [epoch: 3.63 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13049302470305021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13049302470305021 | validation: 0.07046332451604764]
	TIME [epoch: 3.62 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039493314974086896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039493314974086896 | validation: 0.06486548384639332]
	TIME [epoch: 3.64 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053484435331504966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053484435331504966 | validation: 0.06835380457271947]
	TIME [epoch: 3.64 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04131583789259057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04131583789259057 | validation: 0.05966588398818959]
	TIME [epoch: 3.64 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04163649350962841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04163649350962841 | validation: 0.0633329404318808]
	TIME [epoch: 3.64 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04591539172321097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04591539172321097 | validation: 0.08088739654153927]
	TIME [epoch: 3.64 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052152156464857415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052152156464857415 | validation: 0.07031672341029196]
	TIME [epoch: 3.65 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05514434235742023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05514434235742023 | validation: 0.09637403251410635]
	TIME [epoch: 3.63 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07187810883524134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07187810883524134 | validation: 0.10214961373146672]
	TIME [epoch: 3.64 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09040187245861737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09040187245861737 | validation: 0.09875254257382571]
	TIME [epoch: 3.64 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11261977042082039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11261977042082039 | validation: 0.14784625410546595]
	TIME [epoch: 3.63 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12547668894367328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12547668894367328 | validation: 0.1355236469376346]
	TIME [epoch: 3.63 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1337901352086828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1337901352086828 | validation: 0.25146360076339525]
	TIME [epoch: 3.64 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16074084849335446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16074084849335446 | validation: 0.17652051665472027]
	TIME [epoch: 3.63 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15361591554457016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15361591554457016 | validation: 0.07384178556004077]
	TIME [epoch: 3.63 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06253453260069058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06253453260069058 | validation: 0.1340222136869301]
	TIME [epoch: 3.63 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08252269207362314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08252269207362314 | validation: 0.07470639933179438]
	TIME [epoch: 3.64 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055202185718645505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055202185718645505 | validation: 0.05482274745614635]
	TIME [epoch: 3.64 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04147706913218068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04147706913218068 | validation: 0.07397593220934465]
	TIME [epoch: 3.65 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05732671841318613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05732671841318613 | validation: 0.0701627009982374]
	TIME [epoch: 3.63 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06414575005397412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06414575005397412 | validation: 0.07394982322892585]
	TIME [epoch: 3.64 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0745965632747805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0745965632747805 | validation: 0.13200841520653578]
	TIME [epoch: 3.63 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09590949356725698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09590949356725698 | validation: 0.10879477789204942]
	TIME [epoch: 3.64 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09621985631348112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09621985631348112 | validation: 0.1536007691021214]
	TIME [epoch: 3.62 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11914830060914078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11914830060914078 | validation: 0.20898097691805564]
	TIME [epoch: 3.64 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20665694751035893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20665694751035893 | validation: 0.14015488540748966]
	TIME [epoch: 3.63 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12959911927255802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12959911927255802 | validation: 0.06868050704704283]
	TIME [epoch: 3.64 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05446082985556256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05446082985556256 | validation: 0.06952889228285367]
	TIME [epoch: 3.64 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03938602889612737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03938602889612737 | validation: 0.0470964548356684]
	TIME [epoch: 3.64 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_636.pth
	Model improved!!!
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0358000859663549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0358000859663549 | validation: 0.04853885224569884]
	TIME [epoch: 3.63 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03404292677066638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03404292677066638 | validation: 0.05076357879050282]
	TIME [epoch: 3.62 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034797974003060846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034797974003060846 | validation: 0.0532100100218637]
	TIME [epoch: 3.61 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04778678730013592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04778678730013592 | validation: 0.13623716634073757]
	TIME [epoch: 3.62 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08997316883671885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08997316883671885 | validation: 0.20990970461284875]
	TIME [epoch: 3.62 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1939453768993191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1939453768993191 | validation: 0.24117818891307766]
	TIME [epoch: 3.61 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19491883897572904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19491883897572904 | validation: 0.0906622401879988]
	TIME [epoch: 3.62 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07182306180828135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07182306180828135 | validation: 0.08835623960399584]
	TIME [epoch: 3.62 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06623095479859384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06623095479859384 | validation: 0.08734415207491929]
	TIME [epoch: 3.62 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05804181930808289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05804181930808289 | validation: 0.08989428889561915]
	TIME [epoch: 3.61 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0759013689423463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0759013689423463 | validation: 0.08120927087185764]
	TIME [epoch: 3.62 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09172307721801143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09172307721801143 | validation: 0.11889398079798559]
	TIME [epoch: 3.62 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08367398224670378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08367398224670378 | validation: 0.07725071598614087]
	TIME [epoch: 3.62 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05905655595473569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05905655595473569 | validation: 0.04708921273620222]
	TIME [epoch: 3.61 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_650.pth
	Model improved!!!
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04906294908894211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04906294908894211 | validation: 0.08192772650541472]
	TIME [epoch: 3.63 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05171156537654995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05171156537654995 | validation: 0.07544268292019307]
	TIME [epoch: 3.63 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0672583363002469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0672583363002469 | validation: 0.15023598205939376]
	TIME [epoch: 3.63 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10406464574070448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10406464574070448 | validation: 0.18793314724393073]
	TIME [epoch: 3.62 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1682466752930526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1682466752930526 | validation: 0.1612691569167787]
	TIME [epoch: 3.62 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14830727240767616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14830727240767616 | validation: 0.0853994264182426]
	TIME [epoch: 3.61 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0822142301090083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0822142301090083 | validation: 0.05917308640169984]
	TIME [epoch: 3.62 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040820112344702876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040820112344702876 | validation: 0.05572443513111022]
	TIME [epoch: 3.61 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04179118212466123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04179118212466123 | validation: 0.06686979809048173]
	TIME [epoch: 3.64 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05096576944520773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05096576944520773 | validation: 0.0858455929514444]
	TIME [epoch: 3.63 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0724892822380905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0724892822380905 | validation: 0.09520931199590156]
	TIME [epoch: 3.63 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09119467959986301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09119467959986301 | validation: 0.09421668352882522]
	TIME [epoch: 3.63 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07785503841959203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07785503841959203 | validation: 0.060786582166115724]
	TIME [epoch: 3.62 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056047334477232924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056047334477232924 | validation: 0.0634070452204361]
	TIME [epoch: 3.63 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04218966563785537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04218966563785537 | validation: 0.06569900043543167]
	TIME [epoch: 3.63 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06491336009772469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06491336009772469 | validation: 0.20318759357533614]
	TIME [epoch: 3.63 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1820228006250498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1820228006250498 | validation: 0.19685853086435523]
	TIME [epoch: 3.63 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1847523171513447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1847523171513447 | validation: 0.11166299594617515]
	TIME [epoch: 3.62 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0807774100145148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0807774100145148 | validation: 0.0692969139982647]
	TIME [epoch: 3.62 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04413612164362805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04413612164362805 | validation: 0.056490904286061405]
	TIME [epoch: 3.63 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0437055380749636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0437055380749636 | validation: 0.06908698264555178]
	TIME [epoch: 3.62 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041077887611408084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041077887611408084 | validation: 0.07009381506181081]
	TIME [epoch: 3.65 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05177130669431355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05177130669431355 | validation: 0.08472228853745563]
	TIME [epoch: 3.64 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08010043904963862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08010043904963862 | validation: 0.0983049370388211]
	TIME [epoch: 3.63 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10387918657497157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10387918657497157 | validation: 0.10734522155837922]
	TIME [epoch: 3.62 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09137685386615951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09137685386615951 | validation: 0.10027015873179071]
	TIME [epoch: 3.62 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08426165083320722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08426165083320722 | validation: 0.1289666269538323]
	TIME [epoch: 3.62 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12324558053616994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12324558053616994 | validation: 0.1454238756635125]
	TIME [epoch: 3.62 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12746488653048538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12746488653048538 | validation: 0.08085374027801069]
	TIME [epoch: 3.62 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07601043138261597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07601043138261597 | validation: 0.05141373211834589]
	TIME [epoch: 3.62 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035058628137918185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035058628137918185 | validation: 0.05044132391154968]
	TIME [epoch: 3.62 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028291863250557627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028291863250557627 | validation: 0.04406098893415578]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_682.pth
	Model improved!!!
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035568465605158764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035568465605158764 | validation: 0.07105894669544327]
	TIME [epoch: 3.64 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03753924055386217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03753924055386217 | validation: 0.0803570206927897]
	TIME [epoch: 3.63 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05233176202626865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05233176202626865 | validation: 0.10219241987864931]
	TIME [epoch: 3.63 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10096524960171518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10096524960171518 | validation: 0.17677383593033225]
	TIME [epoch: 3.62 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12527014238857606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12527014238857606 | validation: 0.08021286064908663]
	TIME [epoch: 3.61 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0832877546084915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0832877546084915 | validation: 0.11849305057091866]
	TIME [epoch: 3.62 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07709706614572874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07709706614572874 | validation: 0.10527642194975169]
	TIME [epoch: 3.62 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0993042885819554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0993042885819554 | validation: 0.08131881613900066]
	TIME [epoch: 3.62 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10226501535449384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10226501535449384 | validation: 0.17990164237169395]
	TIME [epoch: 3.62 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1208453881558759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1208453881558759 | validation: 0.139697966749027]
	TIME [epoch: 3.63 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1256192730297744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1256192730297744 | validation: 0.08823052305713776]
	TIME [epoch: 3.62 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060225911823438845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060225911823438845 | validation: 0.07253681720057284]
	TIME [epoch: 3.63 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0521914586300216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0521914586300216 | validation: 0.07192178903634748]
	TIME [epoch: 3.63 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04965587641329473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04965587641329473 | validation: 0.06397254127968191]
	TIME [epoch: 3.62 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05611708296813886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05611708296813886 | validation: 0.06580462929755555]
	TIME [epoch: 3.61 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06069461214771727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06069461214771727 | validation: 0.08057749830057162]
	TIME [epoch: 3.62 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06690422816847968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06690422816847968 | validation: 0.10067028422128382]
	TIME [epoch: 3.62 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08106165282149373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08106165282149373 | validation: 0.0989242206556044]
	TIME [epoch: 3.63 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09988619577288364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09988619577288364 | validation: 0.10989234055377409]
	TIME [epoch: 3.64 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08403761161329365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08403761161329365 | validation: 0.11842937556384839]
	TIME [epoch: 3.64 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11653561471068162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11653561471068162 | validation: 0.1335412673579439]
	TIME [epoch: 3.63 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1221635147132495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1221635147132495 | validation: 0.08341722572049459]
	TIME [epoch: 3.63 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07441900762517131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07441900762517131 | validation: 0.04999524141919556]
	TIME [epoch: 3.63 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035106326962893855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035106326962893855 | validation: 0.04040710612948931]
	TIME [epoch: 3.64 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_706.pth
	Model improved!!!
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022389541244084148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022389541244084148 | validation: 0.03480319149894774]
	TIME [epoch: 3.64 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_707.pth
	Model improved!!!
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022572029184952637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022572029184952637 | validation: 0.04786986916629746]
	TIME [epoch: 3.63 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026669652397418094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026669652397418094 | validation: 0.04542885742782737]
	TIME [epoch: 3.63 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040052942467119115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040052942467119115 | validation: 0.11336490520945719]
	TIME [epoch: 3.62 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0819217957286992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0819217957286992 | validation: 0.16225967154537052]
	TIME [epoch: 3.63 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16223726195937802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16223726195937802 | validation: 0.21093340052904486]
	TIME [epoch: 3.62 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17433856917012583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17433856917012583 | validation: 0.163967715423441]
	TIME [epoch: 3.63 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1374600994316353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1374600994316353 | validation: 0.14827580543051347]
	TIME [epoch: 3.63 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1274613506017017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1274613506017017 | validation: 0.0893354910711576]
	TIME [epoch: 3.62 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06145011503832572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06145011503832572 | validation: 0.0653016280563613]
	TIME [epoch: 3.63 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03760483756121459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03760483756121459 | validation: 0.05707749970482449]
	TIME [epoch: 3.64 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044350802328767445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044350802328767445 | validation: 0.052535025045152596]
	TIME [epoch: 3.64 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04169296943034958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04169296943034958 | validation: 0.0696601803387881]
	TIME [epoch: 3.64 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047885682218422966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047885682218422966 | validation: 0.06611403191939703]
	TIME [epoch: 3.63 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07223081328716384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07223081328716384 | validation: 0.08290949404977187]
	TIME [epoch: 3.63 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07208789045697017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07208789045697017 | validation: 0.05688177853571608]
	TIME [epoch: 3.63 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05847089582257709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05847089582257709 | validation: 0.06293326260439057]
	TIME [epoch: 3.62 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04378198017152234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04378198017152234 | validation: 0.06899955071247506]
	TIME [epoch: 3.62 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042023708500476376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042023708500476376 | validation: 0.04708308834946909]
	TIME [epoch: 3.61 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04060525757461167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04060525757461167 | validation: 0.0655601793023693]
	TIME [epoch: 3.62 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05538677339929453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05538677339929453 | validation: 0.09775670956676026]
	TIME [epoch: 3.63 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10681165071790232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10681165071790232 | validation: 0.23078692451671978]
	TIME [epoch: 3.63 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2075684589449645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2075684589449645 | validation: 0.23070753496903187]
	TIME [epoch: 3.64 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19273535743229864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19273535743229864 | validation: 0.06588777115660373]
	TIME [epoch: 3.64 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0519036763759503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0519036763759503 | validation: 0.07214204062979386]
	TIME [epoch: 3.63 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07317245518886438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07317245518886438 | validation: 0.08196390463629326]
	TIME [epoch: 3.63 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07233573391720194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07233573391720194 | validation: 0.05752439135941498]
	TIME [epoch: 3.63 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05793995102083355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05793995102083355 | validation: 0.0629867060699687]
	TIME [epoch: 3.62 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03178096583389738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03178096583389738 | validation: 0.0455723705026731]
	TIME [epoch: 3.63 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030947823439494995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030947823439494995 | validation: 0.03995007203539784]
	TIME [epoch: 3.63 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036722633351991336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036722633351991336 | validation: 0.07094970269112182]
	TIME [epoch: 3.63 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03988025996751431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03988025996751431 | validation: 0.05642601776373152]
	TIME [epoch: 3.63 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051790305037527955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051790305037527955 | validation: 0.09674172131331105]
	TIME [epoch: 3.63 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05660795056784146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05660795056784146 | validation: 0.08766847170139647]
	TIME [epoch: 3.64 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08456504838015036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08456504838015036 | validation: 0.13197808466927077]
	TIME [epoch: 3.64 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14473230876492005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14473230876492005 | validation: 0.23914063474556754]
	TIME [epoch: 3.64 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19598825049366095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19598825049366095 | validation: 0.09950400764473794]
	TIME [epoch: 3.63 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0814854801429837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0814854801429837 | validation: 0.051399032934129944]
	TIME [epoch: 3.63 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029390327839214338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029390327839214338 | validation: 0.05432713822003435]
	TIME [epoch: 3.63 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02844754403000507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02844754403000507 | validation: 0.05261327486748987]
	TIME [epoch: 3.63 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04416449717318528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04416449717318528 | validation: 0.06877421337626174]
	TIME [epoch: 3.63 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06229508499329888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06229508499329888 | validation: 0.10101653387001792]
	TIME [epoch: 3.62 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09028594983817484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09028594983817484 | validation: 0.08428554385021345]
	TIME [epoch: 3.63 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08970563482366965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08970563482366965 | validation: 0.08765219277866984]
	TIME [epoch: 3.63 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06041956903832949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06041956903832949 | validation: 0.04212221423758196]
	TIME [epoch: 3.63 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029054254212725922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029054254212725922 | validation: 0.030003819344609273]
	TIME [epoch: 3.64 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_752.pth
	Model improved!!!
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02897934357917144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02897934357917144 | validation: 0.08740700390716874]
	TIME [epoch: 3.64 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04841633670141155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04841633670141155 | validation: 0.09598131691769102]
	TIME [epoch: 3.64 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08866942056586335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08866942056586335 | validation: 0.17030319610780267]
	TIME [epoch: 3.63 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11555616529794235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11555616529794235 | validation: 0.06798733236940242]
	TIME [epoch: 3.63 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05996284083773599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05996284083773599 | validation: 0.07988960188759966]
	TIME [epoch: 3.63 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09322709691791242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09322709691791242 | validation: 0.1767396108743775]
	TIME [epoch: 3.62 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15213427064973015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15213427064973015 | validation: 0.10300513592770888]
	TIME [epoch: 3.64 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10329604960715535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10329604960715535 | validation: 0.06240854698385079]
	TIME [epoch: 3.63 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044631321549203115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044631321549203115 | validation: 0.06307647170316254]
	TIME [epoch: 3.63 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03258888319592832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03258888319592832 | validation: 0.04467042805800692]
	TIME [epoch: 3.63 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03422633726182573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03422633726182573 | validation: 0.052681958892413006]
	TIME [epoch: 3.65 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03956956768409654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03956956768409654 | validation: 0.07283482728612703]
	TIME [epoch: 3.64 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06595880588731345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06595880588731345 | validation: 0.08335123546563283]
	TIME [epoch: 3.63 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09967046519271883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09967046519271883 | validation: 0.093179167660219]
	TIME [epoch: 3.63 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06406888520080248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06406888520080248 | validation: 0.06554369278426696]
	TIME [epoch: 3.63 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04301224681049707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04301224681049707 | validation: 0.048994805218832954]
	TIME [epoch: 3.63 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03933964741203159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03933964741203159 | validation: 0.06917657641333166]
	TIME [epoch: 3.63 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048972291641975065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048972291641975065 | validation: 0.11345469324064056]
	TIME [epoch: 3.63 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08832467113578646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08832467113578646 | validation: 0.18305271655803537]
	TIME [epoch: 3.63 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18330457019999372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18330457019999372 | validation: 0.18221252079798733]
	TIME [epoch: 3.63 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16967926177053752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16967926177053752 | validation: 0.08370385018251694]
	TIME [epoch: 3.63 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07783840625340031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07783840625340031 | validation: 0.07066191879665888]
	TIME [epoch: 3.63 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04778418636770926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04778418636770926 | validation: 0.07145388971742316]
	TIME [epoch: 3.64 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0570616936490794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0570616936490794 | validation: 0.06052897675004934]
	TIME [epoch: 3.64 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04632832955756148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04632832955756148 | validation: 0.0568092223683651]
	TIME [epoch: 3.64 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033862635995713326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033862635995713326 | validation: 0.040387507915000065]
	TIME [epoch: 3.63 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0286581656861894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0286581656861894 | validation: 0.043447331559100424]
	TIME [epoch: 3.63 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03414844932080146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03414844932080146 | validation: 0.05283831171214053]
	TIME [epoch: 3.63 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03934982379700268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03934982379700268 | validation: 0.06307473962390002]
	TIME [epoch: 3.63 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05405196961928013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05405196961928013 | validation: 0.11720247695474516]
	TIME [epoch: 3.64 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12184065592754867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12184065592754867 | validation: 0.20460341583516867]
	TIME [epoch: 3.63 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1936448192379028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1936448192379028 | validation: 0.1073982130054646]
	TIME [epoch: 3.63 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.115802646714095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.115802646714095 | validation: 0.08837789348800516]
	TIME [epoch: 3.63 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043749748045099235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043749748045099235 | validation: 0.04951464144187254]
	TIME [epoch: 3.63 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03768849558708006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03768849558708006 | validation: 0.06141964231234487]
	TIME [epoch: 3.63 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04380788061100426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04380788061100426 | validation: 0.05985997637265689]
	TIME [epoch: 3.65 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048603458120994265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048603458120994265 | validation: 0.051291011195964555]
	TIME [epoch: 3.64 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05772377311514929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05772377311514929 | validation: 0.06922977879564944]
	TIME [epoch: 3.63 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06011821657829909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06011821657829909 | validation: 0.0790617863797425]
	TIME [epoch: 3.63 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07329165867127584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07329165867127584 | validation: 0.08822624196218885]
	TIME [epoch: 3.63 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09401185445028212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09401185445028212 | validation: 0.1268197026065244]
	TIME [epoch: 3.63 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1030030441676614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1030030441676614 | validation: 0.12792785906434206]
	TIME [epoch: 3.63 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10364730406121629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10364730406121629 | validation: 0.07913863202959744]
	TIME [epoch: 3.63 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056513219619417986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056513219619417986 | validation: 0.0447860607337817]
	TIME [epoch: 3.63 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042321371490922584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042321371490922584 | validation: 0.0677543843014687]
	TIME [epoch: 3.63 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05107853444868134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05107853444868134 | validation: 0.04104722063396158]
	TIME [epoch: 3.64 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052800424968829925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052800424968829925 | validation: 0.06805232185773043]
	TIME [epoch: 3.64 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029839794438220837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029839794438220837 | validation: 0.030354675058523958]
	TIME [epoch: 3.63 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019146094325309882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019146094325309882 | validation: 0.03083828082088516]
	TIME [epoch: 3.63 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01927432037805493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01927432037805493 | validation: 0.035182944028267125]
	TIME [epoch: 3.62 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0227156140089527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0227156140089527 | validation: 0.04316915958401229]
	TIME [epoch: 3.64 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04316978645005252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04316978645005252 | validation: 0.12735858534791503]
	TIME [epoch: 3.62 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1069429536477383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1069429536477383 | validation: 0.1683601727233816]
	TIME [epoch: 3.63 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17754767188277554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17754767188277554 | validation: 0.1436758011455951]
	TIME [epoch: 3.62 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11034932164915319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11034932164915319 | validation: 0.15116984270640962]
	TIME [epoch: 3.63 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11281136459086487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11281136459086487 | validation: 0.12644212465306337]
	TIME [epoch: 3.62 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10396709033185651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10396709033185651 | validation: 0.07250014872788926]
	TIME [epoch: 3.63 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06351819148906772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06351819148906772 | validation: 0.05283128278263142]
	TIME [epoch: 3.63 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05206405505118151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05206405505118151 | validation: 0.06009908522329835]
	TIME [epoch: 3.64 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06392017249836653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06392017249836653 | validation: 0.10601374342753189]
	TIME [epoch: 3.62 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04504589087287386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04504589087287386 | validation: 0.03797372569016814]
	TIME [epoch: 3.63 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02764307302487902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02764307302487902 | validation: 0.03882141185109001]
	TIME [epoch: 3.63 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029993985562426762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029993985562426762 | validation: 0.05822281241983124]
	TIME [epoch: 3.63 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03712527283016914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03712527283016914 | validation: 0.04660624384671185]
	TIME [epoch: 3.62 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050636869658535186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050636869658535186 | validation: 0.11286554083198752]
	TIME [epoch: 3.63 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09509450385245828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09509450385245828 | validation: 0.16506328275843934]
	TIME [epoch: 3.62 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15117920269809026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15117920269809026 | validation: 0.12136260070757837]
	TIME [epoch: 3.62 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10876440045252729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10876440045252729 | validation: 0.06474592856763338]
	TIME [epoch: 3.63 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0511281701758133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0511281701758133 | validation: 0.05578897491541899]
	TIME [epoch: 3.63 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041653463338176516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041653463338176516 | validation: 0.06352926434907978]
	TIME [epoch: 3.63 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07064011290603919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07064011290603919 | validation: 0.10575387350910291]
	TIME [epoch: 3.63 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06980505312997895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06980505312997895 | validation: 0.05923115808551423]
	TIME [epoch: 3.62 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04931455165767806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04931455165767806 | validation: 0.040492552736437294]
	TIME [epoch: 3.63 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037932653924757455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037932653924757455 | validation: 0.05099655698347277]
	TIME [epoch: 3.62 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035506769986920976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035506769986920976 | validation: 0.043590807305691495]
	TIME [epoch: 3.63 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04427355137590063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04427355137590063 | validation: 0.10814116749449112]
	TIME [epoch: 3.62 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0834915416567104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0834915416567104 | validation: 0.1253824134356985]
	TIME [epoch: 3.63 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12810837761902102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12810837761902102 | validation: 0.15240933373106907]
	TIME [epoch: 3.62 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11613372304771176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11613372304771176 | validation: 0.10161732174265725]
	TIME [epoch: 3.62 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07331451081059684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07331451081059684 | validation: 0.04211033265977626]
	TIME [epoch: 3.62 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03649486834413707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03649486834413707 | validation: 0.06426485642744875]
	TIME [epoch: 3.64 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023873588872147383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023873588872147383 | validation: 0.026492776229881568]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_834.pth
	Model improved!!!
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01999894877579979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01999894877579979 | validation: 0.03390493504254694]
	TIME [epoch: 3.63 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02195892254722523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02195892254722523 | validation: 0.04442702015255398]
	TIME [epoch: 3.62 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028069929500088808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028069929500088808 | validation: 0.04267342322488968]
	TIME [epoch: 3.63 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03842789625115052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03842789625115052 | validation: 0.0700139884107915]
	TIME [epoch: 3.62 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05571208438851929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05571208438851929 | validation: 0.08078654912987022]
	TIME [epoch: 3.63 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07615373206244869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07615373206244869 | validation: 0.10526152154514908]
	TIME [epoch: 3.63 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13543834587772252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13543834587772252 | validation: 0.13708102946460013]
	TIME [epoch: 3.63 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07570995493412862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07570995493412862 | validation: 0.0719197775197184]
	TIME [epoch: 3.67 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05023601775786002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05023601775786002 | validation: 0.14792056056376507]
	TIME [epoch: 3.63 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12381068015331255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12381068015331255 | validation: 0.15450325861277509]
	TIME [epoch: 3.63 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16866192711646755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16866192711646755 | validation: 0.11128548813108613]
	TIME [epoch: 3.64 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1176133969988757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1176133969988757 | validation: 0.10869437848901049]
	TIME [epoch: 3.63 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06629388483866444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06629388483866444 | validation: 0.03400968088534822]
	TIME [epoch: 3.63 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025959447600777334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025959447600777334 | validation: 0.03192859636834641]
	TIME [epoch: 3.63 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02483381435147301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02483381435147301 | validation: 0.041438726328879794]
	TIME [epoch: 3.62 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03258494034370757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03258494034370757 | validation: 0.048170446834617686]
	TIME [epoch: 3.62 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049502723441346375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049502723441346375 | validation: 0.06476843734065872]
	TIME [epoch: 3.63 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06494407888683856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06494407888683856 | validation: 0.0719192479299559]
	TIME [epoch: 3.64 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04853081492539806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04853081492539806 | validation: 0.06281455582505084]
	TIME [epoch: 3.63 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050232888897457764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050232888897457764 | validation: 0.07049845771110873]
	TIME [epoch: 3.63 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059473850819740354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059473850819740354 | validation: 0.09089818664998178]
	TIME [epoch: 3.63 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0888731683166246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0888731683166246 | validation: 0.10938934057216035]
	TIME [epoch: 3.64 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12395740882657987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12395740882657987 | validation: 0.11389907359878804]
	TIME [epoch: 3.64 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09165110462534848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09165110462534848 | validation: 0.054755844556417646]
	TIME [epoch: 3.63 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044913064519150774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044913064519150774 | validation: 0.03398579894093947]
	TIME [epoch: 3.63 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025386892376606865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025386892376606865 | validation: 0.05368892969980941]
	TIME [epoch: 3.63 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026161576953291216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026161576953291216 | validation: 0.03527630085803953]
	TIME [epoch: 3.62 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028885130478469263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028885130478469263 | validation: 0.049138259888214415]
	TIME [epoch: 3.62 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026974002513500035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026974002513500035 | validation: 0.03319403735333841]
	TIME [epoch: 3.62 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03244854297057729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03244854297057729 | validation: 0.06548622180399132]
	TIME [epoch: 3.62 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0541711891557917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0541711891557917 | validation: 0.11012399725733496]
	TIME [epoch: 3.63 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11321563901494645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11321563901494645 | validation: 0.135017570283249]
	TIME [epoch: 3.63 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1362103954380504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1362103954380504 | validation: 0.12147159105839207]
	TIME [epoch: 3.63 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.097673212592004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.097673212592004 | validation: 0.06906316689434462]
	TIME [epoch: 3.64 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053976749919159406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053976749919159406 | validation: 0.06723642155296439]
	TIME [epoch: 3.64 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040995312189927455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040995312189927455 | validation: 0.03951425921680676]
	TIME [epoch: 3.63 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030448053590642763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030448053590642763 | validation: 0.054864732112313554]
	TIME [epoch: 3.63 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03132902971810884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03132902971810884 | validation: 0.04187641030301465]
	TIME [epoch: 3.62 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026413946742095674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026413946742095674 | validation: 0.03824776161862753]
	TIME [epoch: 3.63 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028431866703949104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028431866703949104 | validation: 0.0825337647389433]
	TIME [epoch: 3.65 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08479863576495568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08479863576495568 | validation: 0.15445930453707005]
	TIME [epoch: 3.62 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19469221517647461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19469221517647461 | validation: 0.20591780620514913]
	TIME [epoch: 3.63 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13327076412979738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13327076412979738 | validation: 0.15094690169954889]
	TIME [epoch: 3.63 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0943970482332302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0943970482332302 | validation: 0.06108495267759665]
	TIME [epoch: 3.63 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061177077238468804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061177077238468804 | validation: 0.061669340776758785]
	TIME [epoch: 3.64 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046872561714961895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046872561714961895 | validation: 0.04634944036736025]
	TIME [epoch: 3.64 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03472215937787874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03472215937787874 | validation: 0.03149472412010982]
	TIME [epoch: 3.64 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025391681868582774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025391681868582774 | validation: 0.031065329123326626]
	TIME [epoch: 3.62 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019587261184686296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019587261184686296 | validation: 0.03492731650367513]
	TIME [epoch: 3.63 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021659751975869907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021659751975869907 | validation: 0.03249019167720776]
	TIME [epoch: 3.62 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030052275459411425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030052275459411425 | validation: 0.0824998480880262]
	TIME [epoch: 3.6 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0668385123080073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0668385123080073 | validation: 0.11431681503349456]
	TIME [epoch: 3.62 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11160973614855489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11160973614855489 | validation: 0.1352431512031322]
	TIME [epoch: 3.63 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11251336039734453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11251336039734453 | validation: 0.08053545570398185]
	TIME [epoch: 3.63 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0617737658947334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0617737658947334 | validation: 0.04606747767274556]
	TIME [epoch: 3.63 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04442386521481593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04442386521481593 | validation: 0.08730118523286995]
	TIME [epoch: 3.63 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05268807438549877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05268807438549877 | validation: 0.050070872228306654]
	TIME [epoch: 3.64 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05601147224358338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05601147224358338 | validation: 0.07423544281208985]
	TIME [epoch: 3.64 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05102718802824541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05102718802824541 | validation: 0.05080586276398465]
	TIME [epoch: 3.64 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06446043524222579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06446043524222579 | validation: 0.08067487808412657]
	TIME [epoch: 3.63 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07437831231422379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07437831231422379 | validation: 0.10042427482599406]
	TIME [epoch: 3.63 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07183765584356246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07183765584356246 | validation: 0.06630718766950815]
	TIME [epoch: 3.63 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047184029743260636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047184029743260636 | validation: 0.055707634104674456]
	TIME [epoch: 3.63 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05010800554414386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05010800554414386 | validation: 0.09477391064267865]
	TIME [epoch: 3.62 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07005357603177466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07005357603177466 | validation: 0.08067647679712374]
	TIME [epoch: 3.64 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07765275909390087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07765275909390087 | validation: 0.06583419546776847]
	TIME [epoch: 3.62 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05885095797266329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05885095797266329 | validation: 0.04689747687322751]
	TIME [epoch: 3.63 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04149616661394708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04149616661394708 | validation: 0.03388472395678266]
	TIME [epoch: 3.63 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03262181013650466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03262181013650466 | validation: 0.04354120732164755]
	TIME [epoch: 3.63 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02892898971100635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02892898971100635 | validation: 0.03567692039988354]
	TIME [epoch: 3.62 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03945685654765332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03945685654765332 | validation: 0.06717629106656234]
	TIME [epoch: 3.63 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05239627283102115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05239627283102115 | validation: 0.051332188544972526]
	TIME [epoch: 3.61 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06532551874072988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06532551874072988 | validation: 0.06860089603701532]
	TIME [epoch: 3.62 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04221787635281498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04221787635281498 | validation: 0.1634582939564878]
	TIME [epoch: 3.62 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19119177717904465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19119177717904465 | validation: 0.11135704318672063]
	TIME [epoch: 3.63 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12079230312190417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12079230312190417 | validation: 0.14429956913141864]
	TIME [epoch: 3.61 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1479521120369973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1479521120369973 | validation: 0.273886227677389]
	TIME [epoch: 3.62 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23794848944190075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23794848944190075 | validation: 0.09324541402324392]
	TIME [epoch: 3.62 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09567840349376243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09567840349376243 | validation: 0.04965413905840738]
	TIME [epoch: 3.63 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052341996847685215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052341996847685215 | validation: 0.03868013698166705]
	TIME [epoch: 3.64 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029740042894431325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029740042894431325 | validation: 0.03162441904064015]
	TIME [epoch: 3.65 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026649243673325513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026649243673325513 | validation: 0.028211159710699735]
	TIME [epoch: 3.63 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02541269768271173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02541269768271173 | validation: 0.02727534557348867]
	TIME [epoch: 3.63 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021565875500334918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021565875500334918 | validation: 0.025528026281758428]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_918.pth
	Model improved!!!
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023650675377229282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023650675377229282 | validation: 0.03429674845753099]
	TIME [epoch: 3.62 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030954362093890974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030954362093890974 | validation: 0.05368232366609454]
	TIME [epoch: 3.61 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05151738778375933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05151738778375933 | validation: 0.07618068507469988]
	TIME [epoch: 3.62 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08148100095422894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08148100095422894 | validation: 0.09172726826412654]
	TIME [epoch: 3.62 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09341943665500405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09341943665500405 | validation: 0.05540473671555366]
	TIME [epoch: 3.62 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0560684945045825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0560684945045825 | validation: 0.04652380787586136]
	TIME [epoch: 3.62 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04041309051145664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04041309051145664 | validation: 0.051791192983684176]
	TIME [epoch: 3.62 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04413913922011097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04413913922011097 | validation: 0.07535831083137967]
	TIME [epoch: 3.63 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06493799202624148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06493799202624148 | validation: 0.12093632249213959]
	TIME [epoch: 3.63 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13547434948139406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13547434948139406 | validation: 0.06833336028685301]
	TIME [epoch: 3.64 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10690416261596129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10690416261596129 | validation: 0.07890512744903573]
	TIME [epoch: 3.63 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06095003496120294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06095003496120294 | validation: 0.07421230058320392]
	TIME [epoch: 3.63 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04921640470307091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04921640470307091 | validation: 0.03276950065763981]
	TIME [epoch: 3.63 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035995621800148196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035995621800148196 | validation: 0.02755885485849733]
	TIME [epoch: 3.63 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027792538625247793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027792538625247793 | validation: 0.03148704143246912]
	TIME [epoch: 3.62 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0267527409928893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0267527409928893 | validation: 0.03761157552858398]
	TIME [epoch: 3.63 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03380438615447982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03380438615447982 | validation: 0.04661818192573877]
	TIME [epoch: 3.63 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043296293097181186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043296293097181186 | validation: 0.06534141700964242]
	TIME [epoch: 3.62 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06653380012740312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06653380012740312 | validation: 0.07276143380380795]
	TIME [epoch: 3.63 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07970014149244879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07970014149244879 | validation: 0.08540205074157577]
	TIME [epoch: 3.63 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08108992252365986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08108992252365986 | validation: 0.10046585082209387]
	TIME [epoch: 3.63 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10279179628900949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10279179628900949 | validation: 0.10353485865065307]
	TIME [epoch: 3.62 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12196104461460643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12196104461460643 | validation: 0.06481724920532408]
	TIME [epoch: 3.63 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05230916334768662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05230916334768662 | validation: 0.03483959330058464]
	TIME [epoch: 3.62 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024063950143977247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024063950143977247 | validation: 0.020411963856296046]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_943.pth
	Model improved!!!
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02005111872402914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02005111872402914 | validation: 0.033406249782163305]
	TIME [epoch: 3.62 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02308072126122181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02308072126122181 | validation: 0.026583508294295136]
	TIME [epoch: 3.62 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029602687664373177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029602687664373177 | validation: 0.05183070384644561]
	TIME [epoch: 3.63 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04180908818778476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04180908818778476 | validation: 0.06881882590755964]
	TIME [epoch: 3.62 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060722743595459615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060722743595459615 | validation: 0.0788469944543213]
	TIME [epoch: 3.62 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0775637173746964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0775637173746964 | validation: 0.1045644089041177]
	TIME [epoch: 3.63 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09845767090126835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09845767090126835 | validation: 0.14744848787119225]
	TIME [epoch: 3.61 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11095572709349914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11095572709349914 | validation: 0.07148849659238161]
	TIME [epoch: 3.6 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057965324127079096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057965324127079096 | validation: 0.026359704524657974]
	TIME [epoch: 3.62 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02080371163201447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02080371163201447 | validation: 0.03505067345434393]
	TIME [epoch: 3.62 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022360214111260292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022360214111260292 | validation: 0.02721198436767344]
	TIME [epoch: 3.63 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021298079917016623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021298079917016623 | validation: 0.031213447808824624]
	TIME [epoch: 3.62 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020378749342028525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020378749342028525 | validation: 0.03088239923790241]
	TIME [epoch: 3.62 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028704706761780652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028704706761780652 | validation: 0.07242859306860365]
	TIME [epoch: 3.61 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06556978890878488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06556978890878488 | validation: 0.09843160081611482]
	TIME [epoch: 3.63 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1021145593885241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1021145593885241 | validation: 0.08297456542818474]
	TIME [epoch: 3.61 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1027521450187458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1027521450187458 | validation: 0.09372229343251313]
	TIME [epoch: 3.64 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05662309094756496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05662309094756496 | validation: 0.02894530729583172]
	TIME [epoch: 3.63 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025048393295528354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025048393295528354 | validation: 0.048038589967024196]
	TIME [epoch: 3.61 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04798902987682933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04798902987682933 | validation: 0.13875905025510732]
	TIME [epoch: 3.61 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13106661817445356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13106661817445356 | validation: 0.16325633450438676]
	TIME [epoch: 3.63 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1785908079267723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1785908079267723 | validation: 0.0687890942616171]
	TIME [epoch: 3.62 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05841363993193356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05841363993193356 | validation: 0.03605117572032525]
	TIME [epoch: 3.63 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026888650372428536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026888650372428536 | validation: 0.03442797426727691]
	TIME [epoch: 3.62 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025681838965091748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025681838965091748 | validation: 0.031042870125930763]
	TIME [epoch: 3.62 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025171954334929494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025171954334929494 | validation: 0.03157652640173796]
	TIME [epoch: 3.61 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03113129362878867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03113129362878867 | validation: 0.04674073003451909]
	TIME [epoch: 3.62 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033610327408911636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033610327408911636 | validation: 0.03472040946033598]
	TIME [epoch: 3.62 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040966701531622574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040966701531622574 | validation: 0.05782816431348421]
	TIME [epoch: 3.64 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05272025891697994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05272025891697994 | validation: 0.04190866950397327]
	TIME [epoch: 3.61 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05096632505861662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05096632505861662 | validation: 0.05683742013883635]
	TIME [epoch: 3.61 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04747966978458594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04747966978458594 | validation: 0.07583832766021001]
	TIME [epoch: 3.62 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06835373374626673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06835373374626673 | validation: 0.1297192474963623]
	TIME [epoch: 3.62 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11347544496766324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11347544496766324 | validation: 0.14646314698967572]
	TIME [epoch: 3.62 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14586009609268868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14586009609268868 | validation: 0.07166176504755377]
	TIME [epoch: 3.62 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061199009083074005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061199009083074005 | validation: 0.03051995385085231]
	TIME [epoch: 3.62 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02643078868320969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02643078868320969 | validation: 0.03485520879394753]
	TIME [epoch: 3.62 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022452712335524084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022452712335524084 | validation: 0.03329174589327496]
	TIME [epoch: 3.63 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02808710564236123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02808710564236123 | validation: 0.03965499352710023]
	TIME [epoch: 3.63 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035838213938861295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035838213938861295 | validation: 0.06949079457481225]
	TIME [epoch: 3.64 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060566100091693394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060566100091693394 | validation: 0.06298176071702088]
	TIME [epoch: 3.63 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08043621858495835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08043621858495835 | validation: 0.08304561699953514]
	TIME [epoch: 3.64 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07649153549775999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07649153549775999 | validation: 0.054815404902775346]
	TIME [epoch: 3.63 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05396924243154841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05396924243154841 | validation: 0.0504996259243382]
	TIME [epoch: 3.63 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04372530237129924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04372530237129924 | validation: 0.054718642428863956]
	TIME [epoch: 3.63 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033394580296820714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033394580296820714 | validation: 0.02867108554649821]
	TIME [epoch: 3.63 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02219008512263233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02219008512263233 | validation: 0.020850478505354667]
	TIME [epoch: 3.64 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020155466585264844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020155466585264844 | validation: 0.03707075370898135]
	TIME [epoch: 3.63 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028829554939428167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028829554939428167 | validation: 0.049332291850523506]
	TIME [epoch: 3.63 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05703145225009834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05703145225009834 | validation: 0.10251689262769018]
	TIME [epoch: 3.63 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09571809913252687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09571809913252687 | validation: 0.08262304065198206]
	TIME [epoch: 3.64 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09076947401406471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09076947401406471 | validation: 0.07244751395007638]
	TIME [epoch: 3.63 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08177331422618934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08177331422618934 | validation: 0.09629906587626227]
	TIME [epoch: 3.63 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09479559930775533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09479559930775533 | validation: 0.08597830626106884]
	TIME [epoch: 3.63 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08343409080760188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08343409080760188 | validation: 0.08256708290413971]
	TIME [epoch: 3.63 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0563040968158798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0563040968158798 | validation: 0.04192398104234571]
	TIME [epoch: 3.63 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03251533706862095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03251533706862095 | validation: 0.029661153708647515]
	TIME [epoch: 3.63 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01971084117814854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01971084117814854 | validation: 0.033090226630492506]
	TIME [epoch: 30.8 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01928727429605299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01928727429605299 | validation: 0.03407534901004806]
	TIME [epoch: 7.89 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019287214706401594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019287214706401594 | validation: 0.02112989100762827]
	TIME [epoch: 7.88 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02108402071911943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02108402071911943 | validation: 0.035861125210946095]
	TIME [epoch: 7.88 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032521501127981474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032521501127981474 | validation: 0.04312308546548487]
	TIME [epoch: 7.88 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05139444907787187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05139444907787187 | validation: 0.060827847900028535]
	TIME [epoch: 7.89 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05143999359242891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05143999359242891 | validation: 0.08168500385031002]
	TIME [epoch: 7.9 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08975574733349696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08975574733349696 | validation: 0.17775282558418942]
	TIME [epoch: 7.88 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.152575842570973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.152575842570973 | validation: 0.13602300121674413]
	TIME [epoch: 7.88 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11622105712438785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11622105712438785 | validation: 0.048159209976600975]
	TIME [epoch: 7.87 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03323244624250845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03323244624250845 | validation: 0.05040696136295675]
	TIME [epoch: 7.87 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04052027021903463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04052027021903463 | validation: 0.05322096509142391]
	TIME [epoch: 7.87 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053558503910918674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053558503910918674 | validation: 0.05078051712674915]
	TIME [epoch: 7.88 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04662284192055363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04662284192055363 | validation: 0.029638168855166472]
	TIME [epoch: 7.87 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030376164408682026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030376164408682026 | validation: 0.03297882749565103]
	TIME [epoch: 7.87 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022269529480580568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022269529480580568 | validation: 0.040740708141087]
	TIME [epoch: 7.86 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021809242817012943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021809242817012943 | validation: 0.03401092385379155]
	TIME [epoch: 7.87 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03354906648722407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03354906648722407 | validation: 0.07231881385899874]
	TIME [epoch: 7.88 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0548421480273564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0548421480273564 | validation: 0.07867517969453951]
	TIME [epoch: 7.88 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08750490690365567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08750490690365567 | validation: 0.10330403782694841]
	TIME [epoch: 7.87 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09771851746039126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09771851746039126 | validation: 0.09435617169220045]
	TIME [epoch: 7.86 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0749474543734092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0749474543734092 | validation: 0.05106836725263502]
	TIME [epoch: 7.87 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041190060600500546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041190060600500546 | validation: 0.04275062193549778]
	TIME [epoch: 7.87 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03463918239846101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03463918239846101 | validation: 0.03706663189848564]
	TIME [epoch: 7.89 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04086979223453678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04086979223453678 | validation: 0.04323890138104589]
	TIME [epoch: 7.86 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05708581474236235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05708581474236235 | validation: 0.06790028097756291]
	TIME [epoch: 7.87 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058658350385951366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058658350385951366 | validation: 0.03387435432359047]
	TIME [epoch: 7.87 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04975095438416505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04975095438416505 | validation: 0.04437442404717516]
	TIME [epoch: 7.87 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02194795248024253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02194795248024253 | validation: 0.026956248761411446]
	TIME [epoch: 7.88 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02083261206539957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02083261206539957 | validation: 0.02818027877807612]
	TIME [epoch: 7.89 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019845491643373795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019845491643373795 | validation: 0.026336214634945922]
	TIME [epoch: 7.87 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025892516466030725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025892516466030725 | validation: 0.03815338433555431]
	TIME [epoch: 7.88 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03434756761129597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03434756761129597 | validation: 0.07126954354819848]
	TIME [epoch: 7.88 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04923295508646104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04923295508646104 | validation: 0.0513800831083307]
	TIME [epoch: 7.89 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062480711731009476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062480711731009476 | validation: 0.1017814040202844]
	TIME [epoch: 7.9 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07158540343845043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07158540343845043 | validation: 0.060919114512511335]
	TIME [epoch: 7.88 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07636974641269169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07636974641269169 | validation: 0.11586401657581774]
	TIME [epoch: 7.92 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09625008197839872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09625008197839872 | validation: 0.1040720174554314]
	TIME [epoch: 7.87 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09253964473639205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09253964473639205 | validation: 0.05957577230227984]
	TIME [epoch: 7.87 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07484017974554899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07484017974554899 | validation: 0.10328545011063946]
	TIME [epoch: 7.89 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049796207448322714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049796207448322714 | validation: 0.06399707778105986]
	TIME [epoch: 7.89 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061120101985336066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061120101985336066 | validation: 0.052029846828352426]
	TIME [epoch: 7.89 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037322423497967175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037322423497967175 | validation: 0.04414069906571485]
	TIME [epoch: 7.88 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03165359021152696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03165359021152696 | validation: 0.048663821271130975]
	TIME [epoch: 7.88 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20240823_172105/states/model_phi1_3a_v_mmd1_1044.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 3097.110 seconds.
