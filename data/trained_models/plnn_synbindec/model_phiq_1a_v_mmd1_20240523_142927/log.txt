Args:
Namespace(name='model_phiq_1a_v_mmd1', outdir='out/model_training/model_phiq_1a_v_mmd1', training_data='data/training_data/data_phiq_1a/training', validation_data='data/training_data/data_phiq_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=1000, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.01, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2885816398

Training model...

Saving initial model state to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_0.pth
EPOCH 1/1000:
	Training over batches...
		[batch 4/4] avg loss: 5.0686970658438355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.0686970658438355 | validation: 4.945615017646396]
	TIME [epoch: 112 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.927335723216215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.927335723216215 | validation: 5.005893746532012]
	TIME [epoch: 7.79 sec]
EPOCH 3/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.771757130144554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.771757130144554 | validation: 4.750562244127292]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.717072155658501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.717072155658501 | validation: 4.696577186195211]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.457401068544459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.457401068544459 | validation: 4.553492934581087]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.296675320677332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.296675320677332 | validation: 4.394960497633088]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.145576710422247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.145576710422247 | validation: 3.9389702777685356]
	TIME [epoch: 7.69 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.165498457799847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.165498457799847 | validation: 4.890778365855564]
	TIME [epoch: 7.63 sec]
EPOCH 9/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.360750211318111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.360750211318111 | validation: 4.011659526474702]
	TIME [epoch: 7.62 sec]
EPOCH 10/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.787034427886435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.787034427886435 | validation: 3.731008418224904]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.52822381255739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.52822381255739 | validation: 3.439519468216267]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.296390210848333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.296390210848333 | validation: 3.042140164000485]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.130324678876891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.130324678876891 | validation: 2.824457891546448]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.7803950640257717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7803950640257717 | validation: 2.6633196980653473]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.5963652276705336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5963652276705336 | validation: 2.6921511067900683]
	TIME [epoch: 7.62 sec]
EPOCH 16/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.641566848638758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.641566848638758 | validation: 2.4406102850635523]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.3589698571044626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3589698571044626 | validation: 2.2612380562171053]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1878993894174075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1878993894174075 | validation: 2.1330749798268975]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.102432698730816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.102432698730816 | validation: 2.16437846925533]
	TIME [epoch: 7.63 sec]
EPOCH 20/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0984102337702923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0984102337702923 | validation: 2.0131076937857424]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9490522173101583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9490522173101583 | validation: 2.0706421175635494]
	TIME [epoch: 7.68 sec]
EPOCH 22/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9387235757097108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9387235757097108 | validation: 1.8920520685975384]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.9264358472538807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9264358472538807 | validation: 2.033710339026309]
	TIME [epoch: 7.62 sec]
EPOCH 24/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8817877405637224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8817877405637224 | validation: 1.8089773454608091]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7688778527873938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7688778527873938 | validation: 1.7550626397967743]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7574485720821393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7574485720821393 | validation: 1.7986378059222248]
	TIME [epoch: 7.66 sec]
EPOCH 27/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.771360235808105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.771360235808105 | validation: 1.674421394926505]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6594594266517793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6594594266517793 | validation: 1.654841372982475]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6513087461251783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6513087461251783 | validation: 1.6542868262586516]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6273905529695225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6273905529695225 | validation: 1.9089203977383185]
	TIME [epoch: 7.68 sec]
EPOCH 31/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7482060582211894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7482060582211894 | validation: 1.6543835551107344]
	TIME [epoch: 7.63 sec]
EPOCH 32/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6019631702551438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6019631702551438 | validation: 1.5649420308322983]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5590386926099602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5590386926099602 | validation: 1.5482455162227677]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5205434450472204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5205434450472204 | validation: 1.511590047080793]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5028216456778856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5028216456778856 | validation: 1.490696770803382]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4847579811545302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4847579811545302 | validation: 1.5240596853434094]
	TIME [epoch: 7.62 sec]
EPOCH 37/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5249677819472947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5249677819472947 | validation: 1.4788793813854206]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.468700054403152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.468700054403152 | validation: 1.4376959372978968]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4331353511783704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4331353511783704 | validation: 1.4340009017507915]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4262340160535856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4262340160535856 | validation: 1.4426700094777685]
	TIME [epoch: 7.64 sec]
EPOCH 41/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.419453657445457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.419453657445457 | validation: 1.3999581642417747]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3903056498019688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3903056498019688 | validation: 1.3772452739503485]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.374899728846838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.374899728846838 | validation: 1.369215325405569]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3791163216821705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3791163216821705 | validation: 1.366794337431465]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.361445295938392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.361445295938392 | validation: 1.338458286864117]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3415335157109327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3415335157109327 | validation: 1.3380272319363078]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3275181527201942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3275181527201942 | validation: 1.3320498091064703]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3455074684703225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3455074684703225 | validation: 1.3546077725148287]
	TIME [epoch: 7.67 sec]
EPOCH 49/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3275804882056021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3275804882056021 | validation: 1.3159109035459937]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3180124339870978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3180124339870978 | validation: 1.3076081555647863]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.299391102744725		[learning rate: 0.0099456]
	Learning Rate: 0.00994561
	LOSS [training: 1.299391102744725 | validation: 1.2827479917500435]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3052064125944625		[learning rate: 0.0098736]
	Learning Rate: 0.00987356
	LOSS [training: 1.3052064125944625 | validation: 1.2839538205620766]
	TIME [epoch: 7.63 sec]
EPOCH 53/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2950953520509034		[learning rate: 0.009802]
	Learning Rate: 0.00980202
	LOSS [training: 1.2950953520509034 | validation: 1.2752898796910874]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2833737604864397		[learning rate: 0.009731]
	Learning Rate: 0.00973101
	LOSS [training: 1.2833737604864397 | validation: 1.2915519541878688]
	TIME [epoch: 7.63 sec]
EPOCH 55/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2972988438524617		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 1.2972988438524617 | validation: 1.2776278360405158]
	TIME [epoch: 7.62 sec]
EPOCH 56/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2729636816211605		[learning rate: 0.0095905]
	Learning Rate: 0.00959052
	LOSS [training: 1.2729636816211605 | validation: 1.2706185983634157]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2671013537593037		[learning rate: 0.009521]
	Learning Rate: 0.00952104
	LOSS [training: 1.2671013537593037 | validation: 1.2499289462096728]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.258267204263368		[learning rate: 0.0094521]
	Learning Rate: 0.00945206
	LOSS [training: 1.258267204263368 | validation: 1.3021218576941256]
	TIME [epoch: 7.68 sec]
EPOCH 59/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2718376051810492		[learning rate: 0.0093836]
	Learning Rate: 0.00938358
	LOSS [training: 1.2718376051810492 | validation: 1.2317918169843034]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2396898378779755		[learning rate: 0.0093156]
	Learning Rate: 0.00931559
	LOSS [training: 1.2396898378779755 | validation: 1.2048432663912334]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.246928903236744		[learning rate: 0.0092481]
	Learning Rate: 0.0092481
	LOSS [training: 1.246928903236744 | validation: 1.265073866721952]
	TIME [epoch: 7.64 sec]
EPOCH 62/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2299895368070404		[learning rate: 0.0091811]
	Learning Rate: 0.0091811
	LOSS [training: 1.2299895368070404 | validation: 1.2202503513010186]
	TIME [epoch: 7.67 sec]
EPOCH 63/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2366317458253189		[learning rate: 0.0091146]
	Learning Rate: 0.00911458
	LOSS [training: 1.2366317458253189 | validation: 1.1922664014213646]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2059920477323776		[learning rate: 0.0090485]
	Learning Rate: 0.00904855
	LOSS [training: 1.2059920477323776 | validation: 1.167578339601705]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2220272344280287		[learning rate: 0.008983]
	Learning Rate: 0.00898299
	LOSS [training: 1.2220272344280287 | validation: 1.189234102524326]
	TIME [epoch: 7.63 sec]
EPOCH 66/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.222619675733181		[learning rate: 0.0089179]
	Learning Rate: 0.00891791
	LOSS [training: 1.222619675733181 | validation: 1.2471688140021038]
	TIME [epoch: 7.62 sec]
EPOCH 67/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.214260579178962		[learning rate: 0.0088533]
	Learning Rate: 0.0088533
	LOSS [training: 1.214260579178962 | validation: 1.223026176978892]
	TIME [epoch: 7.68 sec]
EPOCH 68/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.206988372404074		[learning rate: 0.0087892]
	Learning Rate: 0.00878916
	LOSS [training: 1.206988372404074 | validation: 1.1898774127946403]
	TIME [epoch: 7.63 sec]
EPOCH 69/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2110546757862226		[learning rate: 0.0087255]
	Learning Rate: 0.00872548
	LOSS [training: 1.2110546757862226 | validation: 1.209297726454067]
	TIME [epoch: 7.62 sec]
EPOCH 70/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1761550026102403		[learning rate: 0.0086623]
	Learning Rate: 0.00866227
	LOSS [training: 1.1761550026102403 | validation: 1.1827684513816314]
	TIME [epoch: 7.62 sec]
EPOCH 71/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.172603272877713		[learning rate: 0.0085995]
	Learning Rate: 0.00859951
	LOSS [training: 1.172603272877713 | validation: 1.1255593295903743]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_71.pth
	Model improved!!!
EPOCH 72/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1602289568229915		[learning rate: 0.0085372]
	Learning Rate: 0.00853721
	LOSS [training: 1.1602289568229915 | validation: 1.2576051979710727]
	TIME [epoch: 7.67 sec]
EPOCH 73/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1750153031033126		[learning rate: 0.0084754]
	Learning Rate: 0.00847535
	LOSS [training: 1.1750153031033126 | validation: 1.1453274354126273]
	TIME [epoch: 7.62 sec]
EPOCH 74/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.133627372327504		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 1.133627372327504 | validation: 1.129558805729201]
	TIME [epoch: 7.62 sec]
EPOCH 75/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1472743599538373		[learning rate: 0.008353]
	Learning Rate: 0.00835299
	LOSS [training: 1.1472743599538373 | validation: 1.1377113337847402]
	TIME [epoch: 7.62 sec]
EPOCH 76/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1441220493946824		[learning rate: 0.0082925]
	Learning Rate: 0.00829248
	LOSS [training: 1.1441220493946824 | validation: 1.1231532154534558]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_76.pth
	Model improved!!!
EPOCH 77/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1661546330703587		[learning rate: 0.0082324]
	Learning Rate: 0.0082324
	LOSS [training: 1.1661546330703587 | validation: 1.1295578202945735]
	TIME [epoch: 7.67 sec]
EPOCH 78/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0998747347684508		[learning rate: 0.0081728]
	Learning Rate: 0.00817275
	LOSS [training: 1.0998747347684508 | validation: 1.0567533512430638]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1304487178922105		[learning rate: 0.0081135]
	Learning Rate: 0.00811354
	LOSS [training: 1.1304487178922105 | validation: 1.0804503232986318]
	TIME [epoch: 7.63 sec]
EPOCH 80/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.091459867979417		[learning rate: 0.0080548]
	Learning Rate: 0.00805476
	LOSS [training: 1.091459867979417 | validation: 1.099959590380612]
	TIME [epoch: 7.63 sec]
EPOCH 81/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1250511704101795		[learning rate: 0.0079964]
	Learning Rate: 0.0079964
	LOSS [training: 1.1250511704101795 | validation: 1.0443283727129669]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1056993006836504		[learning rate: 0.0079385]
	Learning Rate: 0.00793847
	LOSS [training: 1.1056993006836504 | validation: 1.1351442182714253]
	TIME [epoch: 7.66 sec]
EPOCH 83/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.097434887982455		[learning rate: 0.007881]
	Learning Rate: 0.00788096
	LOSS [training: 1.097434887982455 | validation: 1.0591480610324253]
	TIME [epoch: 7.62 sec]
EPOCH 84/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0697320558543966		[learning rate: 0.0078239]
	Learning Rate: 0.00782386
	LOSS [training: 1.0697320558543966 | validation: 1.136472292637854]
	TIME [epoch: 7.62 sec]
EPOCH 85/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0705802785788887		[learning rate: 0.0077672]
	Learning Rate: 0.00776718
	LOSS [training: 1.0705802785788887 | validation: 1.0521068701270848]
	TIME [epoch: 7.61 sec]
EPOCH 86/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1009961341457613		[learning rate: 0.0077109]
	Learning Rate: 0.0077109
	LOSS [training: 1.1009961341457613 | validation: 1.040431910810123]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.135522144080955		[learning rate: 0.007655]
	Learning Rate: 0.00765504
	LOSS [training: 1.135522144080955 | validation: 1.168963936445211]
	TIME [epoch: 7.65 sec]
EPOCH 88/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0785792661330071		[learning rate: 0.0075996]
	Learning Rate: 0.00759958
	LOSS [training: 1.0785792661330071 | validation: 0.9998170491630234]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0496209781639179		[learning rate: 0.0075445]
	Learning Rate: 0.00754452
	LOSS [training: 1.0496209781639179 | validation: 1.0954789223250487]
	TIME [epoch: 7.63 sec]
EPOCH 90/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.034883313808355		[learning rate: 0.0074899]
	Learning Rate: 0.00748986
	LOSS [training: 1.034883313808355 | validation: 1.1576215020020977]
	TIME [epoch: 7.63 sec]
EPOCH 91/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0845785412071185		[learning rate: 0.0074356]
	Learning Rate: 0.0074356
	LOSS [training: 1.0845785412071185 | validation: 0.9899130573986317]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_91.pth
	Model improved!!!
EPOCH 92/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0583982820584288		[learning rate: 0.0073817]
	Learning Rate: 0.00738173
	LOSS [training: 1.0583982820584288 | validation: 0.9944263069989205]
	TIME [epoch: 7.63 sec]
EPOCH 93/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0505977416297145		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 1.0505977416297145 | validation: 1.0808321518678947]
	TIME [epoch: 7.63 sec]
EPOCH 94/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0631981851256442		[learning rate: 0.0072752]
	Learning Rate: 0.00727515
	LOSS [training: 1.0631981851256442 | validation: 0.9694013199010472]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9787704605059484		[learning rate: 0.0072224]
	Learning Rate: 0.00722244
	LOSS [training: 0.9787704605059484 | validation: 1.1901799617496276]
	TIME [epoch: 7.64 sec]
EPOCH 96/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1596027562529074		[learning rate: 0.0071701]
	Learning Rate: 0.00717012
	LOSS [training: 1.1596027562529074 | validation: 1.0219737867828762]
	TIME [epoch: 7.66 sec]
EPOCH 97/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0154907881636814		[learning rate: 0.0071182]
	Learning Rate: 0.00711817
	LOSS [training: 1.0154907881636814 | validation: 1.0196174586952902]
	TIME [epoch: 7.63 sec]
EPOCH 98/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9885510271617367		[learning rate: 0.0070666]
	Learning Rate: 0.0070666
	LOSS [training: 0.9885510271617367 | validation: 0.9339411227579411]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0056877287578512		[learning rate: 0.0070154]
	Learning Rate: 0.0070154
	LOSS [training: 1.0056877287578512 | validation: 1.170906915607514]
	TIME [epoch: 7.62 sec]
EPOCH 100/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0571918814340286		[learning rate: 0.0069646]
	Learning Rate: 0.00696458
	LOSS [training: 1.0571918814340286 | validation: 0.9830926522889543]
	TIME [epoch: 7.63 sec]
EPOCH 101/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9690217628671528		[learning rate: 0.0069141]
	Learning Rate: 0.00691412
	LOSS [training: 0.9690217628671528 | validation: 0.9504689821711004]
	TIME [epoch: 7.66 sec]
EPOCH 102/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9663237649311271		[learning rate: 0.006864]
	Learning Rate: 0.00686403
	LOSS [training: 0.9663237649311271 | validation: 0.9118723981582173]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_102.pth
	Model improved!!!
EPOCH 103/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9674542538623683		[learning rate: 0.0068143]
	Learning Rate: 0.0068143
	LOSS [training: 0.9674542538623683 | validation: 0.9378156969782516]
	TIME [epoch: 7.63 sec]
EPOCH 104/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9829279710631464		[learning rate: 0.0067649]
	Learning Rate: 0.00676493
	LOSS [training: 0.9829279710631464 | validation: 1.0321386322230017]
	TIME [epoch: 7.61 sec]
EPOCH 105/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9891837091074208		[learning rate: 0.0067159]
	Learning Rate: 0.00671592
	LOSS [training: 0.9891837091074208 | validation: 0.9048307402721181]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_105.pth
	Model improved!!!
EPOCH 106/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9442267447723642		[learning rate: 0.0066673]
	Learning Rate: 0.00666726
	LOSS [training: 0.9442267447723642 | validation: 0.876370268104861]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_106.pth
	Model improved!!!
EPOCH 107/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9351738353421666		[learning rate: 0.006619]
	Learning Rate: 0.00661896
	LOSS [training: 0.9351738353421666 | validation: 1.0353726498370968]
	TIME [epoch: 7.63 sec]
EPOCH 108/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0026255693336485		[learning rate: 0.006571]
	Learning Rate: 0.006571
	LOSS [training: 1.0026255693336485 | validation: 0.8821892917976638]
	TIME [epoch: 7.63 sec]
EPOCH 109/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9346282667665775		[learning rate: 0.0065234]
	Learning Rate: 0.00652339
	LOSS [training: 0.9346282667665775 | validation: 1.114737145003096]
	TIME [epoch: 7.63 sec]
EPOCH 110/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9394275823717505		[learning rate: 0.0064761]
	Learning Rate: 0.00647613
	LOSS [training: 0.9394275823717505 | validation: 0.9305225183880355]
	TIME [epoch: 7.68 sec]
EPOCH 111/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9762813246999131		[learning rate: 0.0064292]
	Learning Rate: 0.00642921
	LOSS [training: 0.9762813246999131 | validation: 0.8923831065252691]
	TIME [epoch: 7.63 sec]
EPOCH 112/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9150080770513456		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.9150080770513456 | validation: 0.9285295822479575]
	TIME [epoch: 7.62 sec]
EPOCH 113/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9763195774234386		[learning rate: 0.0063364]
	Learning Rate: 0.00633639
	LOSS [training: 0.9763195774234386 | validation: 1.0023501657350513]
	TIME [epoch: 7.63 sec]
EPOCH 114/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9577657750387818		[learning rate: 0.0062905]
	Learning Rate: 0.00629049
	LOSS [training: 0.9577657750387818 | validation: 0.8556093529635613]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_114.pth
	Model improved!!!
EPOCH 115/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9679016265666976		[learning rate: 0.0062449]
	Learning Rate: 0.00624491
	LOSS [training: 0.9679016265666976 | validation: 0.9475064953401507]
	TIME [epoch: 7.68 sec]
EPOCH 116/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9659083569253555		[learning rate: 0.0061997]
	Learning Rate: 0.00619967
	LOSS [training: 0.9659083569253555 | validation: 1.0866190586250175]
	TIME [epoch: 7.63 sec]
EPOCH 117/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9804579627215058		[learning rate: 0.0061548]
	Learning Rate: 0.00615475
	LOSS [training: 0.9804579627215058 | validation: 0.8866780220344626]
	TIME [epoch: 7.62 sec]
EPOCH 118/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8802924216077606		[learning rate: 0.0061102]
	Learning Rate: 0.00611016
	LOSS [training: 0.8802924216077606 | validation: 0.8471226199386637]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_118.pth
	Model improved!!!
EPOCH 119/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9219228994843218		[learning rate: 0.0060659]
	Learning Rate: 0.00606589
	LOSS [training: 0.9219228994843218 | validation: 0.8456926238016192]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_119.pth
	Model improved!!!
EPOCH 120/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.864095801054731		[learning rate: 0.0060219]
	Learning Rate: 0.00602195
	LOSS [training: 0.864095801054731 | validation: 0.8163222769762148]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_120.pth
	Model improved!!!
EPOCH 121/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8420802602966259		[learning rate: 0.0059783]
	Learning Rate: 0.00597832
	LOSS [training: 0.8420802602966259 | validation: 0.9737618041829299]
	TIME [epoch: 7.62 sec]
EPOCH 122/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.938933270286306		[learning rate: 0.005935]
	Learning Rate: 0.005935
	LOSS [training: 0.938933270286306 | validation: 0.8366360472638285]
	TIME [epoch: 7.61 sec]
EPOCH 123/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9112735839814639		[learning rate: 0.005892]
	Learning Rate: 0.00589201
	LOSS [training: 0.9112735839814639 | validation: 1.112901751176718]
	TIME [epoch: 7.61 sec]
EPOCH 124/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8649734580888413		[learning rate: 0.0058493]
	Learning Rate: 0.00584932
	LOSS [training: 0.8649734580888413 | validation: 0.9718857013753015]
	TIME [epoch: 7.67 sec]
EPOCH 125/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9119466897170965		[learning rate: 0.0058069]
	Learning Rate: 0.00580694
	LOSS [training: 0.9119466897170965 | validation: 0.8259242764317676]
	TIME [epoch: 7.62 sec]
EPOCH 126/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8036770039799155		[learning rate: 0.0057649]
	Learning Rate: 0.00576487
	LOSS [training: 0.8036770039799155 | validation: 1.0818508666983444]
	TIME [epoch: 7.61 sec]
EPOCH 127/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9163958142370121		[learning rate: 0.0057231]
	Learning Rate: 0.0057231
	LOSS [training: 0.9163958142370121 | validation: 0.8608492037264388]
	TIME [epoch: 7.61 sec]
EPOCH 128/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.852520674249603		[learning rate: 0.0056816]
	Learning Rate: 0.00568164
	LOSS [training: 0.852520674249603 | validation: 0.7693047263877525]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_128.pth
	Model improved!!!
EPOCH 129/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8206212156488255		[learning rate: 0.0056405]
	Learning Rate: 0.00564048
	LOSS [training: 0.8206212156488255 | validation: 0.9722964738063304]
	TIME [epoch: 7.66 sec]
EPOCH 130/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9588017706279816		[learning rate: 0.0055996]
	Learning Rate: 0.00559961
	LOSS [training: 0.9588017706279816 | validation: 0.8632705996009629]
	TIME [epoch: 7.62 sec]
EPOCH 131/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8435407933435226		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.8435407933435226 | validation: 0.7707641131105647]
	TIME [epoch: 7.61 sec]
EPOCH 132/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9169327446975564		[learning rate: 0.0055188]
	Learning Rate: 0.00551877
	LOSS [training: 0.9169327446975564 | validation: 0.918448461869501]
	TIME [epoch: 7.61 sec]
EPOCH 133/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9161071222024448		[learning rate: 0.0054788]
	Learning Rate: 0.00547878
	LOSS [training: 0.9161071222024448 | validation: 0.7974911165279892]
	TIME [epoch: 7.62 sec]
EPOCH 134/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7668221805424198		[learning rate: 0.0054391]
	Learning Rate: 0.00543909
	LOSS [training: 0.7668221805424198 | validation: 0.7229565409023557]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_134.pth
	Model improved!!!
EPOCH 135/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9348929184790015		[learning rate: 0.0053997]
	Learning Rate: 0.00539968
	LOSS [training: 0.9348929184790015 | validation: 0.8797575032721352]
	TIME [epoch: 7.62 sec]
EPOCH 136/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9776910846172715		[learning rate: 0.0053606]
	Learning Rate: 0.00536056
	LOSS [training: 0.9776910846172715 | validation: 0.9429558964028766]
	TIME [epoch: 7.62 sec]
EPOCH 137/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.92505437069378		[learning rate: 0.0053217]
	Learning Rate: 0.00532173
	LOSS [training: 0.92505437069378 | validation: 0.8938823548973747]
	TIME [epoch: 7.62 sec]
EPOCH 138/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8281703175017218		[learning rate: 0.0052832]
	Learning Rate: 0.00528317
	LOSS [training: 0.8281703175017218 | validation: 1.0183880179065479]
	TIME [epoch: 7.61 sec]
EPOCH 139/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.931279360601358		[learning rate: 0.0052449]
	Learning Rate: 0.0052449
	LOSS [training: 0.931279360601358 | validation: 0.780911100115554]
	TIME [epoch: 7.67 sec]
EPOCH 140/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8322209153723673		[learning rate: 0.0052069]
	Learning Rate: 0.0052069
	LOSS [training: 0.8322209153723673 | validation: 0.7958282707391302]
	TIME [epoch: 7.62 sec]
EPOCH 141/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7987271929107456		[learning rate: 0.0051692]
	Learning Rate: 0.00516917
	LOSS [training: 0.7987271929107456 | validation: 0.8278072598640847]
	TIME [epoch: 7.62 sec]
EPOCH 142/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7866941346634156		[learning rate: 0.0051317]
	Learning Rate: 0.00513172
	LOSS [training: 0.7866941346634156 | validation: 0.8994596424157845]
	TIME [epoch: 7.61 sec]
EPOCH 143/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.851627322734156		[learning rate: 0.0050945]
	Learning Rate: 0.00509454
	LOSS [training: 0.851627322734156 | validation: 0.7348152562538057]
	TIME [epoch: 7.62 sec]
EPOCH 144/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.756848233533854		[learning rate: 0.0050576]
	Learning Rate: 0.00505763
	LOSS [training: 0.756848233533854 | validation: 0.8448447706295583]
	TIME [epoch: 7.67 sec]
EPOCH 145/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8878729722560622		[learning rate: 0.005021]
	Learning Rate: 0.00502099
	LOSS [training: 0.8878729722560622 | validation: 0.7761380773344795]
	TIME [epoch: 7.62 sec]
EPOCH 146/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7808454701356078		[learning rate: 0.0049846]
	Learning Rate: 0.00498461
	LOSS [training: 0.7808454701356078 | validation: 0.7771566472234994]
	TIME [epoch: 7.61 sec]
EPOCH 147/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7814670178704937		[learning rate: 0.0049485]
	Learning Rate: 0.0049485
	LOSS [training: 0.7814670178704937 | validation: 0.7970041975721507]
	TIME [epoch: 7.61 sec]
EPOCH 148/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7739402958296202		[learning rate: 0.0049126]
	Learning Rate: 0.00491265
	LOSS [training: 0.7739402958296202 | validation: 0.9728630809464816]
	TIME [epoch: 7.63 sec]
EPOCH 149/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8896636038313639		[learning rate: 0.0048771]
	Learning Rate: 0.00487706
	LOSS [training: 0.8896636038313639 | validation: 0.8770848014927468]
	TIME [epoch: 7.66 sec]
EPOCH 150/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7737072343807171		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.7737072343807171 | validation: 0.728966663454063]
	TIME [epoch: 7.62 sec]
EPOCH 151/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6771497920871408		[learning rate: 0.0048066]
	Learning Rate: 0.00480665
	LOSS [training: 0.6771497920871408 | validation: 1.0823822179987301]
	TIME [epoch: 7.62 sec]
EPOCH 152/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7939475579338671		[learning rate: 0.0047718]
	Learning Rate: 0.00477182
	LOSS [training: 0.7939475579338671 | validation: 0.7730469636611929]
	TIME [epoch: 7.62 sec]
EPOCH 153/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7652893100261063		[learning rate: 0.0047373]
	Learning Rate: 0.00473725
	LOSS [training: 0.7652893100261063 | validation: 0.7030652553315131]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_153.pth
	Model improved!!!
EPOCH 154/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7398182010392323		[learning rate: 0.0047029]
	Learning Rate: 0.00470293
	LOSS [training: 0.7398182010392323 | validation: 0.8348243616126934]
	TIME [epoch: 7.67 sec]
EPOCH 155/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7057799161772164		[learning rate: 0.0046689]
	Learning Rate: 0.00466886
	LOSS [training: 0.7057799161772164 | validation: 0.69837259011176]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_155.pth
	Model improved!!!
EPOCH 156/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.786796949239681		[learning rate: 0.004635]
	Learning Rate: 0.00463503
	LOSS [training: 0.786796949239681 | validation: 0.7452659192943325]
	TIME [epoch: 7.62 sec]
EPOCH 157/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7285272668937813		[learning rate: 0.0046015]
	Learning Rate: 0.00460145
	LOSS [training: 0.7285272668937813 | validation: 0.6920766959624555]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_157.pth
	Model improved!!!
EPOCH 158/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7142368595028252		[learning rate: 0.0045681]
	Learning Rate: 0.00456811
	LOSS [training: 0.7142368595028252 | validation: 0.9273338297278793]
	TIME [epoch: 7.66 sec]
EPOCH 159/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.818086539524589		[learning rate: 0.004535]
	Learning Rate: 0.00453502
	LOSS [training: 0.818086539524589 | validation: 0.8471409274694544]
	TIME [epoch: 7.63 sec]
EPOCH 160/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6997251659236383		[learning rate: 0.0045022]
	Learning Rate: 0.00450216
	LOSS [training: 0.6997251659236383 | validation: 0.784158019388657]
	TIME [epoch: 7.62 sec]
EPOCH 161/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6672732016506208		[learning rate: 0.0044695]
	Learning Rate: 0.00446954
	LOSS [training: 0.6672732016506208 | validation: 0.9258471862401751]
	TIME [epoch: 7.62 sec]
EPOCH 162/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7904488084141541		[learning rate: 0.0044372]
	Learning Rate: 0.00443716
	LOSS [training: 0.7904488084141541 | validation: 0.6951581574226126]
	TIME [epoch: 7.62 sec]
EPOCH 163/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7386659597789189		[learning rate: 0.004405]
	Learning Rate: 0.00440501
	LOSS [training: 0.7386659597789189 | validation: 0.6657496821169673]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_163.pth
	Model improved!!!
EPOCH 164/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7226550418116809		[learning rate: 0.0043731]
	Learning Rate: 0.0043731
	LOSS [training: 0.7226550418116809 | validation: 0.80512350232898]
	TIME [epoch: 7.62 sec]
EPOCH 165/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7446594953637171		[learning rate: 0.0043414]
	Learning Rate: 0.00434142
	LOSS [training: 0.7446594953637171 | validation: 0.7265626768268323]
	TIME [epoch: 7.62 sec]
EPOCH 166/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6994484863068954		[learning rate: 0.00431]
	Learning Rate: 0.00430996
	LOSS [training: 0.6994484863068954 | validation: 0.707383216158904]
	TIME [epoch: 7.61 sec]
EPOCH 167/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.694399275571989		[learning rate: 0.0042787]
	Learning Rate: 0.00427874
	LOSS [training: 0.694399275571989 | validation: 1.0396443609226913]
	TIME [epoch: 7.62 sec]
EPOCH 168/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7341539180643137		[learning rate: 0.0042477]
	Learning Rate: 0.00424774
	LOSS [training: 0.7341539180643137 | validation: 0.7049800246018141]
	TIME [epoch: 7.67 sec]
EPOCH 169/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7035586874485086		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.7035586874485086 | validation: 0.8032228012474953]
	TIME [epoch: 7.62 sec]
EPOCH 170/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7306224524375773		[learning rate: 0.0041864]
	Learning Rate: 0.00418641
	LOSS [training: 0.7306224524375773 | validation: 0.6445091923834474]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_170.pth
	Model improved!!!
EPOCH 171/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.670273758091742		[learning rate: 0.0041561]
	Learning Rate: 0.00415608
	LOSS [training: 0.670273758091742 | validation: 0.7986266628157195]
	TIME [epoch: 7.62 sec]
EPOCH 172/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7202792955942529		[learning rate: 0.004126]
	Learning Rate: 0.00412597
	LOSS [training: 0.7202792955942529 | validation: 0.7175100926107562]
	TIME [epoch: 7.62 sec]
EPOCH 173/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7644218858355458		[learning rate: 0.0040961]
	Learning Rate: 0.00409608
	LOSS [training: 0.7644218858355458 | validation: 0.8938434713569832]
	TIME [epoch: 7.66 sec]
EPOCH 174/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7962868306233563		[learning rate: 0.0040664]
	Learning Rate: 0.0040664
	LOSS [training: 0.7962868306233563 | validation: 0.6920770226274102]
	TIME [epoch: 7.62 sec]
EPOCH 175/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7035092196640053		[learning rate: 0.0040369]
	Learning Rate: 0.00403694
	LOSS [training: 0.7035092196640053 | validation: 0.7385411559078814]
	TIME [epoch: 7.62 sec]
EPOCH 176/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6729190542067398		[learning rate: 0.0040077]
	Learning Rate: 0.0040077
	LOSS [training: 0.6729190542067398 | validation: 0.6627522400445722]
	TIME [epoch: 7.62 sec]
EPOCH 177/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6753261194389057		[learning rate: 0.0039787]
	Learning Rate: 0.00397866
	LOSS [training: 0.6753261194389057 | validation: 0.7239850300184523]
	TIME [epoch: 7.62 sec]
EPOCH 178/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6892250714676224		[learning rate: 0.0039498]
	Learning Rate: 0.00394984
	LOSS [training: 0.6892250714676224 | validation: 0.63687240415564]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_178.pth
	Model improved!!!
EPOCH 179/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6613350702017249		[learning rate: 0.0039212]
	Learning Rate: 0.00392122
	LOSS [training: 0.6613350702017249 | validation: 0.6453726146747791]
	TIME [epoch: 7.62 sec]
EPOCH 180/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6355820103706002		[learning rate: 0.0038928]
	Learning Rate: 0.00389281
	LOSS [training: 0.6355820103706002 | validation: 0.9409173447454822]
	TIME [epoch: 7.62 sec]
EPOCH 181/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7208592095789463		[learning rate: 0.0038646]
	Learning Rate: 0.00386461
	LOSS [training: 0.7208592095789463 | validation: 0.8123050761956387]
	TIME [epoch: 7.62 sec]
EPOCH 182/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6289669989948942		[learning rate: 0.0038366]
	Learning Rate: 0.00383661
	LOSS [training: 0.6289669989948942 | validation: 0.7378841243760473]
	TIME [epoch: 7.65 sec]
EPOCH 183/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7000936121228468		[learning rate: 0.0038088]
	Learning Rate: 0.00380881
	LOSS [training: 0.7000936121228468 | validation: 0.6948281875786904]
	TIME [epoch: 7.64 sec]
EPOCH 184/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.692954474307302		[learning rate: 0.0037812]
	Learning Rate: 0.00378122
	LOSS [training: 0.692954474307302 | validation: 0.7017643331896977]
	TIME [epoch: 7.62 sec]
EPOCH 185/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6742080450302701		[learning rate: 0.0037538]
	Learning Rate: 0.00375382
	LOSS [training: 0.6742080450302701 | validation: 0.7629343173331473]
	TIME [epoch: 7.61 sec]
EPOCH 186/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6133901523463328		[learning rate: 0.0037266]
	Learning Rate: 0.00372663
	LOSS [training: 0.6133901523463328 | validation: 0.6968770309292034]
	TIME [epoch: 7.62 sec]
EPOCH 187/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6493912019921522		[learning rate: 0.0036996]
	Learning Rate: 0.00369963
	LOSS [training: 0.6493912019921522 | validation: 0.6563835239762437]
	TIME [epoch: 7.66 sec]
EPOCH 188/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.620983244619878		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.620983244619878 | validation: 0.7574010494655845]
	TIME [epoch: 7.64 sec]
EPOCH 189/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6560798178864571		[learning rate: 0.0036462]
	Learning Rate: 0.00364621
	LOSS [training: 0.6560798178864571 | validation: 0.6349081817290783]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_189.pth
	Model improved!!!
EPOCH 190/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6198235057763137		[learning rate: 0.0036198]
	Learning Rate: 0.0036198
	LOSS [training: 0.6198235057763137 | validation: 0.713803642040762]
	TIME [epoch: 7.62 sec]
EPOCH 191/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6314832427651674		[learning rate: 0.0035936]
	Learning Rate: 0.00359357
	LOSS [training: 0.6314832427651674 | validation: 0.6292921388949368]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_191.pth
	Model improved!!!
EPOCH 192/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5966017176270368		[learning rate: 0.0035675]
	Learning Rate: 0.00356754
	LOSS [training: 0.5966017176270368 | validation: 0.6737140521836089]
	TIME [epoch: 7.68 sec]
EPOCH 193/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6165162273958318		[learning rate: 0.0035417]
	Learning Rate: 0.00354169
	LOSS [training: 0.6165162273958318 | validation: 0.7336817996421012]
	TIME [epoch: 7.63 sec]
EPOCH 194/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6954793123071394		[learning rate: 0.003516]
	Learning Rate: 0.00351603
	LOSS [training: 0.6954793123071394 | validation: 0.6587937497685767]
	TIME [epoch: 7.62 sec]
EPOCH 195/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6197654092353407		[learning rate: 0.0034906]
	Learning Rate: 0.00349056
	LOSS [training: 0.6197654092353407 | validation: 0.6117958505229747]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_195.pth
	Model improved!!!
EPOCH 196/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6211811520763157		[learning rate: 0.0034653]
	Learning Rate: 0.00346527
	LOSS [training: 0.6211811520763157 | validation: 0.6453023970128443]
	TIME [epoch: 7.62 sec]
EPOCH 197/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6247584797257548		[learning rate: 0.0034402]
	Learning Rate: 0.00344016
	LOSS [training: 0.6247584797257548 | validation: 0.610282447925897]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_197.pth
	Model improved!!!
EPOCH 198/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7042710398624338		[learning rate: 0.0034152]
	Learning Rate: 0.00341524
	LOSS [training: 0.7042710398624338 | validation: 0.8042163455412242]
	TIME [epoch: 7.63 sec]
EPOCH 199/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5880885580435893		[learning rate: 0.0033905]
	Learning Rate: 0.0033905
	LOSS [training: 0.5880885580435893 | validation: 0.6661930069541312]
	TIME [epoch: 7.62 sec]
EPOCH 200/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.575741030599844		[learning rate: 0.0033659]
	Learning Rate: 0.00336593
	LOSS [training: 0.575741030599844 | validation: 0.6122127820342117]
	TIME [epoch: 7.62 sec]
EPOCH 201/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6191450611629737		[learning rate: 0.0033415]
	Learning Rate: 0.00334155
	LOSS [training: 0.6191450611629737 | validation: 0.8452290539264458]
	TIME [epoch: 7.64 sec]
EPOCH 202/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6042754116995586		[learning rate: 0.0033173]
	Learning Rate: 0.00331734
	LOSS [training: 0.6042754116995586 | validation: 0.7248438626307636]
	TIME [epoch: 7.66 sec]
EPOCH 203/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5786668538141579		[learning rate: 0.0032933]
	Learning Rate: 0.0032933
	LOSS [training: 0.5786668538141579 | validation: 0.8391271316385974]
	TIME [epoch: 7.62 sec]
EPOCH 204/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6135003233291474		[learning rate: 0.0032694]
	Learning Rate: 0.00326944
	LOSS [training: 0.6135003233291474 | validation: 0.7042031403793385]
	TIME [epoch: 7.61 sec]
EPOCH 205/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5787315276782038		[learning rate: 0.0032458]
	Learning Rate: 0.00324576
	LOSS [training: 0.5787315276782038 | validation: 0.9780076792422633]
	TIME [epoch: 7.62 sec]
EPOCH 206/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6690328853471084		[learning rate: 0.0032222]
	Learning Rate: 0.00322224
	LOSS [training: 0.6690328853471084 | validation: 0.6888273533786451]
	TIME [epoch: 7.63 sec]
EPOCH 207/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6255697027808519		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.6255697027808519 | validation: 0.7913331194911777]
	TIME [epoch: 7.66 sec]
EPOCH 208/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5677771805557067		[learning rate: 0.0031757]
	Learning Rate: 0.00317572
	LOSS [training: 0.5677771805557067 | validation: 0.5662956504107448]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_208.pth
	Model improved!!!
EPOCH 209/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5804825616374896		[learning rate: 0.0031527]
	Learning Rate: 0.00315271
	LOSS [training: 0.5804825616374896 | validation: 0.5682440520201429]
	TIME [epoch: 7.62 sec]
EPOCH 210/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5793001086644312		[learning rate: 0.0031299]
	Learning Rate: 0.00312987
	LOSS [training: 0.5793001086644312 | validation: 0.6699754958230639]
	TIME [epoch: 7.62 sec]
EPOCH 211/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5762868833724663		[learning rate: 0.0031072]
	Learning Rate: 0.00310719
	LOSS [training: 0.5762868833724663 | validation: 0.6300443592449311]
	TIME [epoch: 7.66 sec]
EPOCH 212/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6196324306287829		[learning rate: 0.0030847]
	Learning Rate: 0.00308468
	LOSS [training: 0.6196324306287829 | validation: 0.6039498765194963]
	TIME [epoch: 7.62 sec]
EPOCH 213/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5855876428737149		[learning rate: 0.0030623]
	Learning Rate: 0.00306233
	LOSS [training: 0.5855876428737149 | validation: 0.5914483328981639]
	TIME [epoch: 7.62 sec]
EPOCH 214/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.55032084842798		[learning rate: 0.0030401]
	Learning Rate: 0.00304015
	LOSS [training: 0.55032084842798 | validation: 0.6111917285946216]
	TIME [epoch: 7.61 sec]
EPOCH 215/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5388110296371409		[learning rate: 0.0030181]
	Learning Rate: 0.00301812
	LOSS [training: 0.5388110296371409 | validation: 0.6587479056772125]
	TIME [epoch: 7.61 sec]
EPOCH 216/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6132199299211947		[learning rate: 0.0029963]
	Learning Rate: 0.00299626
	LOSS [training: 0.6132199299211947 | validation: 0.6123544918584728]
	TIME [epoch: 7.65 sec]
EPOCH 217/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5641840117068275		[learning rate: 0.0029745]
	Learning Rate: 0.00297455
	LOSS [training: 0.5641840117068275 | validation: 0.6194206300714766]
	TIME [epoch: 7.62 sec]
EPOCH 218/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.529909953256964		[learning rate: 0.002953]
	Learning Rate: 0.002953
	LOSS [training: 0.529909953256964 | validation: 1.0728015318607262]
	TIME [epoch: 7.61 sec]
EPOCH 219/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6682701890538282		[learning rate: 0.0029316]
	Learning Rate: 0.0029316
	LOSS [training: 0.6682701890538282 | validation: 0.6858623846295142]
	TIME [epoch: 7.61 sec]
EPOCH 220/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5381594486789572		[learning rate: 0.0029104]
	Learning Rate: 0.00291036
	LOSS [training: 0.5381594486789572 | validation: 0.6165718184771998]
	TIME [epoch: 7.61 sec]
EPOCH 221/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5893142163911026		[learning rate: 0.0028893]
	Learning Rate: 0.00288928
	LOSS [training: 0.5893142163911026 | validation: 0.5727445943251939]
	TIME [epoch: 7.66 sec]
EPOCH 222/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5474435338370486		[learning rate: 0.0028683]
	Learning Rate: 0.00286835
	LOSS [training: 0.5474435338370486 | validation: 0.6530830108374766]
	TIME [epoch: 7.62 sec]
EPOCH 223/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.562558728308375		[learning rate: 0.0028476]
	Learning Rate: 0.00284757
	LOSS [training: 0.562558728308375 | validation: 0.6401424170311367]
	TIME [epoch: 7.61 sec]
EPOCH 224/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5188780312274777		[learning rate: 0.0028269]
	Learning Rate: 0.00282693
	LOSS [training: 0.5188780312274777 | validation: 0.5835612610131808]
	TIME [epoch: 7.61 sec]
EPOCH 225/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5467865423056266		[learning rate: 0.0028065]
	Learning Rate: 0.00280645
	LOSS [training: 0.5467865423056266 | validation: 0.7261482941585556]
	TIME [epoch: 7.61 sec]
EPOCH 226/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6094924865566206		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.6094924865566206 | validation: 0.5517656918691775]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_226.pth
	Model improved!!!
EPOCH 227/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5087401197510737		[learning rate: 0.0027659]
	Learning Rate: 0.00276594
	LOSS [training: 0.5087401197510737 | validation: 0.5899712461899216]
	TIME [epoch: 7.61 sec]
EPOCH 228/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5343732882273806		[learning rate: 0.0027459]
	Learning Rate: 0.0027459
	LOSS [training: 0.5343732882273806 | validation: 0.5875545007514679]
	TIME [epoch: 7.61 sec]
EPOCH 229/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5747052452073463		[learning rate: 0.002726]
	Learning Rate: 0.002726
	LOSS [training: 0.5747052452073463 | validation: 0.7834358090774627]
	TIME [epoch: 7.61 sec]
EPOCH 230/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5611535543933849		[learning rate: 0.0027063]
	Learning Rate: 0.00270625
	LOSS [training: 0.5611535543933849 | validation: 0.5627450751386502]
	TIME [epoch: 7.61 sec]
EPOCH 231/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5542046466541874		[learning rate: 0.0026866]
	Learning Rate: 0.00268665
	LOSS [training: 0.5542046466541874 | validation: 0.622121523556046]
	TIME [epoch: 7.66 sec]
EPOCH 232/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5355751810674074		[learning rate: 0.0026672]
	Learning Rate: 0.00266718
	LOSS [training: 0.5355751810674074 | validation: 0.6693923709083469]
	TIME [epoch: 7.61 sec]
EPOCH 233/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5271960619842462		[learning rate: 0.0026479]
	Learning Rate: 0.00264786
	LOSS [training: 0.5271960619842462 | validation: 0.5816117043245719]
	TIME [epoch: 7.61 sec]
EPOCH 234/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5414947653596007		[learning rate: 0.0026287]
	Learning Rate: 0.00262867
	LOSS [training: 0.5414947653596007 | validation: 0.5574526395129635]
	TIME [epoch: 7.61 sec]
EPOCH 235/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5149666004661455		[learning rate: 0.0026096]
	Learning Rate: 0.00260963
	LOSS [training: 0.5149666004661455 | validation: 0.6093436820388298]
	TIME [epoch: 7.61 sec]
EPOCH 236/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5345922305632892		[learning rate: 0.0025907]
	Learning Rate: 0.00259072
	LOSS [training: 0.5345922305632892 | validation: 0.6674041555370189]
	TIME [epoch: 7.66 sec]
EPOCH 237/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.558059001447164		[learning rate: 0.002572]
	Learning Rate: 0.00257195
	LOSS [training: 0.558059001447164 | validation: 0.6272706394561298]
	TIME [epoch: 7.62 sec]
EPOCH 238/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5409938138248608		[learning rate: 0.0025533]
	Learning Rate: 0.00255332
	LOSS [training: 0.5409938138248608 | validation: 0.56068575659785]
	TIME [epoch: 7.61 sec]
EPOCH 239/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5640469453816437		[learning rate: 0.0025348]
	Learning Rate: 0.00253482
	LOSS [training: 0.5640469453816437 | validation: 0.6913175208512345]
	TIME [epoch: 7.61 sec]
EPOCH 240/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5158602982084501		[learning rate: 0.0025165]
	Learning Rate: 0.00251646
	LOSS [training: 0.5158602982084501 | validation: 0.5555215175210819]
	TIME [epoch: 7.62 sec]
EPOCH 241/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.49764484104302464		[learning rate: 0.0024982]
	Learning Rate: 0.00249823
	LOSS [training: 0.49764484104302464 | validation: 0.5907050082459308]
	TIME [epoch: 7.66 sec]
EPOCH 242/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5631195827296684		[learning rate: 0.0024801]
	Learning Rate: 0.00248013
	LOSS [training: 0.5631195827296684 | validation: 0.7281285025889719]
	TIME [epoch: 7.62 sec]
EPOCH 243/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5051129190738155		[learning rate: 0.0024622]
	Learning Rate: 0.00246216
	LOSS [training: 0.5051129190738155 | validation: 0.6132987650369213]
	TIME [epoch: 7.61 sec]
EPOCH 244/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5398232649956453		[learning rate: 0.0024443]
	Learning Rate: 0.00244432
	LOSS [training: 0.5398232649956453 | validation: 0.6112668461477486]
	TIME [epoch: 7.61 sec]
EPOCH 245/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5059111464277245		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.5059111464277245 | validation: 0.7099233273097919]
	TIME [epoch: 7.62 sec]
EPOCH 246/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5334810609085512		[learning rate: 0.002409]
	Learning Rate: 0.00240903
	LOSS [training: 0.5334810609085512 | validation: 0.6272914431162113]
	TIME [epoch: 7.65 sec]
EPOCH 247/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5051362693688557		[learning rate: 0.0023916]
	Learning Rate: 0.00239158
	LOSS [training: 0.5051362693688557 | validation: 0.5895423676246341]
	TIME [epoch: 7.61 sec]
EPOCH 248/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5127032159446465		[learning rate: 0.0023742]
	Learning Rate: 0.00237425
	LOSS [training: 0.5127032159446465 | validation: 0.7494973839383974]
	TIME [epoch: 7.61 sec]
EPOCH 249/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5148258790488844		[learning rate: 0.002357]
	Learning Rate: 0.00235705
	LOSS [training: 0.5148258790488844 | validation: 0.5585255841269825]
	TIME [epoch: 7.61 sec]
EPOCH 250/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4886193654393311		[learning rate: 0.00234]
	Learning Rate: 0.00233997
	LOSS [training: 0.4886193654393311 | validation: 0.5640403826026705]
	TIME [epoch: 7.62 sec]
EPOCH 251/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5627838951318584		[learning rate: 0.002323]
	Learning Rate: 0.00232302
	LOSS [training: 0.5627838951318584 | validation: 0.7636626128308038]
	TIME [epoch: 7.65 sec]
EPOCH 252/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5280045411535621		[learning rate: 0.0023062]
	Learning Rate: 0.00230619
	LOSS [training: 0.5280045411535621 | validation: 0.5749169959701137]
	TIME [epoch: 7.61 sec]
EPOCH 253/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4860687223734874		[learning rate: 0.0022895]
	Learning Rate: 0.00228948
	LOSS [training: 0.4860687223734874 | validation: 0.567294410103774]
	TIME [epoch: 7.61 sec]
EPOCH 254/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.48878256553398647		[learning rate: 0.0022729]
	Learning Rate: 0.00227289
	LOSS [training: 0.48878256553398647 | validation: 0.5966828432979614]
	TIME [epoch: 7.61 sec]
EPOCH 255/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5409176959369666		[learning rate: 0.0022564]
	Learning Rate: 0.00225643
	LOSS [training: 0.5409176959369666 | validation: 0.5759152254726663]
	TIME [epoch: 7.64 sec]
EPOCH 256/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4759963041420715		[learning rate: 0.0022401]
	Learning Rate: 0.00224008
	LOSS [training: 0.4759963041420715 | validation: 0.5989677853550469]
	TIME [epoch: 7.64 sec]
EPOCH 257/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.48261510031713484		[learning rate: 0.0022238]
	Learning Rate: 0.00222385
	LOSS [training: 0.48261510031713484 | validation: 0.6304540551318877]
	TIME [epoch: 7.61 sec]
EPOCH 258/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5628581965807669		[learning rate: 0.0022077]
	Learning Rate: 0.00220774
	LOSS [training: 0.5628581965807669 | validation: 0.5769712987276073]
	TIME [epoch: 7.61 sec]
EPOCH 259/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5023455030138922		[learning rate: 0.0021917]
	Learning Rate: 0.00219174
	LOSS [training: 0.5023455030138922 | validation: 0.5314599836595216]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_259.pth
	Model improved!!!
EPOCH 260/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4978280258031369		[learning rate: 0.0021759]
	Learning Rate: 0.00217586
	LOSS [training: 0.4978280258031369 | validation: 0.6589758791701035]
	TIME [epoch: 7.67 sec]
EPOCH 261/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.55714918741195		[learning rate: 0.0021601]
	Learning Rate: 0.0021601
	LOSS [training: 0.55714918741195 | validation: 0.5201566954287005]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_261.pth
	Model improved!!!
EPOCH 262/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4963975360977113		[learning rate: 0.0021444]
	Learning Rate: 0.00214445
	LOSS [training: 0.4963975360977113 | validation: 0.5551777212271691]
	TIME [epoch: 7.62 sec]
EPOCH 263/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4639115544332454		[learning rate: 0.0021289]
	Learning Rate: 0.00212891
	LOSS [training: 0.4639115544332454 | validation: 0.584056133883891]
	TIME [epoch: 7.61 sec]
EPOCH 264/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5203119161008425		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.5203119161008425 | validation: 0.559785024477369]
	TIME [epoch: 7.61 sec]
EPOCH 265/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5006535049792469		[learning rate: 0.0020982]
	Learning Rate: 0.00209818
	LOSS [training: 0.5006535049792469 | validation: 0.5287369756432234]
	TIME [epoch: 7.66 sec]
EPOCH 266/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.43399898418576793		[learning rate: 0.002083]
	Learning Rate: 0.00208298
	LOSS [training: 0.43399898418576793 | validation: 0.6696520200489439]
	TIME [epoch: 7.62 sec]
EPOCH 267/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.488531510243729		[learning rate: 0.0020679]
	Learning Rate: 0.00206788
	LOSS [training: 0.488531510243729 | validation: 0.5679218537577816]
	TIME [epoch: 7.61 sec]
EPOCH 268/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.48830218174493484		[learning rate: 0.0020529]
	Learning Rate: 0.0020529
	LOSS [training: 0.48830218174493484 | validation: 0.5371354617159305]
	TIME [epoch: 7.61 sec]
EPOCH 269/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.48105199521810593		[learning rate: 0.002038]
	Learning Rate: 0.00203803
	LOSS [training: 0.48105199521810593 | validation: 0.5295821068153141]
	TIME [epoch: 7.61 sec]
EPOCH 270/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4935643313979163		[learning rate: 0.0020233]
	Learning Rate: 0.00202326
	LOSS [training: 0.4935643313979163 | validation: 0.6394733613732547]
	TIME [epoch: 7.66 sec]
EPOCH 271/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.47361004693834413		[learning rate: 0.0020086]
	Learning Rate: 0.00200861
	LOSS [training: 0.47361004693834413 | validation: 0.5483577308685893]
	TIME [epoch: 7.62 sec]
EPOCH 272/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4854712628391006		[learning rate: 0.0019941]
	Learning Rate: 0.00199405
	LOSS [training: 0.4854712628391006 | validation: 0.5558842979236599]
	TIME [epoch: 7.62 sec]
EPOCH 273/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4740900321378292		[learning rate: 0.0019796]
	Learning Rate: 0.00197961
	LOSS [training: 0.4740900321378292 | validation: 0.5838221705999007]
	TIME [epoch: 7.61 sec]
EPOCH 274/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4563692088115579		[learning rate: 0.0019653]
	Learning Rate: 0.00196527
	LOSS [training: 0.4563692088115579 | validation: 0.5713020307580039]
	TIME [epoch: 7.61 sec]
EPOCH 275/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.48001229384209265		[learning rate: 0.001951]
	Learning Rate: 0.00195103
	LOSS [training: 0.48001229384209265 | validation: 0.5446461473285787]
	TIME [epoch: 7.66 sec]
EPOCH 276/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.47311400256548736		[learning rate: 0.0019369]
	Learning Rate: 0.00193689
	LOSS [training: 0.47311400256548736 | validation: 0.5990778039321336]
	TIME [epoch: 7.62 sec]
EPOCH 277/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4632028913714693		[learning rate: 0.0019229]
	Learning Rate: 0.00192286
	LOSS [training: 0.4632028913714693 | validation: 0.5187508241043421]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_277.pth
	Model improved!!!
EPOCH 278/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4686334903030131		[learning rate: 0.0019089]
	Learning Rate: 0.00190893
	LOSS [training: 0.4686334903030131 | validation: 0.6247758812335648]
	TIME [epoch: 7.62 sec]
EPOCH 279/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4749449175075963		[learning rate: 0.0018951]
	Learning Rate: 0.0018951
	LOSS [training: 0.4749449175075963 | validation: 0.5300467200789578]
	TIME [epoch: 7.62 sec]
EPOCH 280/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.465019412166627		[learning rate: 0.0018814]
	Learning Rate: 0.00188137
	LOSS [training: 0.465019412166627 | validation: 0.5775792094667405]
	TIME [epoch: 7.67 sec]
EPOCH 281/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.49986076982968525		[learning rate: 0.0018677]
	Learning Rate: 0.00186774
	LOSS [training: 0.49986076982968525 | validation: 0.571353864967007]
	TIME [epoch: 7.62 sec]
EPOCH 282/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4700390376172411		[learning rate: 0.0018542]
	Learning Rate: 0.00185421
	LOSS [training: 0.4700390376172411 | validation: 0.5641189309632494]
	TIME [epoch: 7.62 sec]
EPOCH 283/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4638484280393021		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.4638484280393021 | validation: 0.5664546434786202]
	TIME [epoch: 7.62 sec]
EPOCH 284/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.44005988545858643		[learning rate: 0.0018274]
	Learning Rate: 0.00182744
	LOSS [training: 0.44005988545858643 | validation: 0.5074906530475095]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_284.pth
	Model improved!!!
EPOCH 285/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.45376136378672693		[learning rate: 0.0018142]
	Learning Rate: 0.0018142
	LOSS [training: 0.45376136378672693 | validation: 0.5710888122142564]
	TIME [epoch: 7.66 sec]
EPOCH 286/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4483517704940597		[learning rate: 0.0018011]
	Learning Rate: 0.00180105
	LOSS [training: 0.4483517704940597 | validation: 0.49708749340014274]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_286.pth
	Model improved!!!
EPOCH 287/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.41605288934941864		[learning rate: 0.001788]
	Learning Rate: 0.001788
	LOSS [training: 0.41605288934941864 | validation: 0.5586084899343778]
	TIME [epoch: 7.62 sec]
EPOCH 288/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4441897007266463		[learning rate: 0.001775]
	Learning Rate: 0.00177505
	LOSS [training: 0.4441897007266463 | validation: 0.522021865317958]
	TIME [epoch: 7.61 sec]
EPOCH 289/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.532510466194431		[learning rate: 0.0017622]
	Learning Rate: 0.00176219
	LOSS [training: 0.532510466194431 | validation: 0.5896070128547266]
	TIME [epoch: 7.65 sec]
EPOCH 290/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4587181527494925		[learning rate: 0.0017494]
	Learning Rate: 0.00174942
	LOSS [training: 0.4587181527494925 | validation: 0.5112122552570065]
	TIME [epoch: 7.63 sec]
EPOCH 291/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4675365039006769		[learning rate: 0.0017367]
	Learning Rate: 0.00173675
	LOSS [training: 0.4675365039006769 | validation: 0.5749304697259674]
	TIME [epoch: 7.61 sec]
EPOCH 292/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4562604583619136		[learning rate: 0.0017242]
	Learning Rate: 0.00172417
	LOSS [training: 0.4562604583619136 | validation: 0.5451237395350554]
	TIME [epoch: 7.62 sec]
EPOCH 293/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4293468978948587		[learning rate: 0.0017117]
	Learning Rate: 0.00171167
	LOSS [training: 0.4293468978948587 | validation: 0.5175927753750158]
	TIME [epoch: 7.61 sec]
EPOCH 294/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.41930258630724165		[learning rate: 0.0016993]
	Learning Rate: 0.00169927
	LOSS [training: 0.41930258630724165 | validation: 0.5472852251474147]
	TIME [epoch: 7.66 sec]
EPOCH 295/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4393267032356942		[learning rate: 0.001687]
	Learning Rate: 0.00168696
	LOSS [training: 0.4393267032356942 | validation: 0.56315089562794]
	TIME [epoch: 7.62 sec]
EPOCH 296/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.46470855613373463		[learning rate: 0.0016747]
	Learning Rate: 0.00167474
	LOSS [training: 0.46470855613373463 | validation: 0.5391629260460536]
	TIME [epoch: 7.61 sec]
EPOCH 297/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4325509442177845		[learning rate: 0.0016626]
	Learning Rate: 0.00166261
	LOSS [training: 0.4325509442177845 | validation: 0.520671859012664]
	TIME [epoch: 7.61 sec]
EPOCH 298/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.41478388175743397		[learning rate: 0.0016506]
	Learning Rate: 0.00165056
	LOSS [training: 0.41478388175743397 | validation: 0.5726227423206467]
	TIME [epoch: 7.61 sec]
EPOCH 299/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.46032847515356967		[learning rate: 0.0016386]
	Learning Rate: 0.0016386
	LOSS [training: 0.46032847515356967 | validation: 0.5526335034079162]
	TIME [epoch: 7.66 sec]
EPOCH 300/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4446384825514942		[learning rate: 0.0016267]
	Learning Rate: 0.00162673
	LOSS [training: 0.4446384825514942 | validation: 0.5002782704638479]
	TIME [epoch: 7.62 sec]
EPOCH 301/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.41483466964301385		[learning rate: 0.0016149]
	Learning Rate: 0.00161495
	LOSS [training: 0.41483466964301385 | validation: 0.5215073152649119]
	TIME [epoch: 7.61 sec]
EPOCH 302/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4829603978197488		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.4829603978197488 | validation: 0.5519008169417712]
	TIME [epoch: 7.61 sec]
EPOCH 303/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.43625301810653544		[learning rate: 0.0015916]
	Learning Rate: 0.00159163
	LOSS [training: 0.43625301810653544 | validation: 0.519306295912882]
	TIME [epoch: 7.62 sec]
EPOCH 304/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.43728753803284937		[learning rate: 0.0015801]
	Learning Rate: 0.0015801
	LOSS [training: 0.43728753803284937 | validation: 0.5139782048210579]
	TIME [epoch: 7.67 sec]
EPOCH 305/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4221895562319913		[learning rate: 0.0015687]
	Learning Rate: 0.00156865
	LOSS [training: 0.4221895562319913 | validation: 0.5938451634615971]
	TIME [epoch: 7.61 sec]
EPOCH 306/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4587423070126149		[learning rate: 0.0015573]
	Learning Rate: 0.00155729
	LOSS [training: 0.4587423070126149 | validation: 0.49173190902190617]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_306.pth
	Model improved!!!
EPOCH 307/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4437004688202455		[learning rate: 0.001546]
	Learning Rate: 0.001546
	LOSS [training: 0.4437004688202455 | validation: 0.573180182643892]
	TIME [epoch: 7.61 sec]
EPOCH 308/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.45546867741324326		[learning rate: 0.0015348]
	Learning Rate: 0.0015348
	LOSS [training: 0.45546867741324326 | validation: 0.5569174849521514]
	TIME [epoch: 7.61 sec]
EPOCH 309/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.43153091728120213		[learning rate: 0.0015237]
	Learning Rate: 0.00152368
	LOSS [training: 0.43153091728120213 | validation: 0.5367213876240664]
	TIME [epoch: 7.66 sec]
EPOCH 310/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.42232058293571484		[learning rate: 0.0015126]
	Learning Rate: 0.00151264
	LOSS [training: 0.42232058293571484 | validation: 0.4950287262791392]
	TIME [epoch: 7.62 sec]
EPOCH 311/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.459313623834827		[learning rate: 0.0015017]
	Learning Rate: 0.00150169
	LOSS [training: 0.459313623834827 | validation: 0.5127496351102696]
	TIME [epoch: 7.61 sec]
EPOCH 312/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4010629873915243		[learning rate: 0.0014908]
	Learning Rate: 0.00149081
	LOSS [training: 0.4010629873915243 | validation: 0.562547124205571]
	TIME [epoch: 7.62 sec]
EPOCH 313/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4482809506120156		[learning rate: 0.00148]
	Learning Rate: 0.00148001
	LOSS [training: 0.4482809506120156 | validation: 0.5051363044703427]
	TIME [epoch: 7.61 sec]
EPOCH 314/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4295235531687782		[learning rate: 0.0014693]
	Learning Rate: 0.00146928
	LOSS [training: 0.4295235531687782 | validation: 0.48897544462805964]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_314.pth
	Model improved!!!
EPOCH 315/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4000960187578886		[learning rate: 0.0014586]
	Learning Rate: 0.00145864
	LOSS [training: 0.4000960187578886 | validation: 0.5993260264562793]
	TIME [epoch: 7.62 sec]
EPOCH 316/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4513262943102049		[learning rate: 0.0014481]
	Learning Rate: 0.00144807
	LOSS [training: 0.4513262943102049 | validation: 0.5292202996046783]
	TIME [epoch: 7.62 sec]
EPOCH 317/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.43619544736709753		[learning rate: 0.0014376]
	Learning Rate: 0.00143758
	LOSS [training: 0.43619544736709753 | validation: 0.5251437997559348]
	TIME [epoch: 7.61 sec]
EPOCH 318/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.40703298555423106		[learning rate: 0.0014272]
	Learning Rate: 0.00142716
	LOSS [training: 0.40703298555423106 | validation: 0.5135745552547544]
	TIME [epoch: 7.63 sec]
EPOCH 319/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4402614672875158		[learning rate: 0.0014168]
	Learning Rate: 0.00141682
	LOSS [training: 0.4402614672875158 | validation: 0.5179091303255001]
	TIME [epoch: 7.66 sec]
EPOCH 320/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.42797846817202834		[learning rate: 0.0014066]
	Learning Rate: 0.00140656
	LOSS [training: 0.42797846817202834 | validation: 0.530924703413428]
	TIME [epoch: 7.61 sec]
EPOCH 321/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.393090914199132		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.393090914199132 | validation: 0.501363412364785]
	TIME [epoch: 7.61 sec]
EPOCH 322/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.424897361941528		[learning rate: 0.0013863]
	Learning Rate: 0.00138625
	LOSS [training: 0.424897361941528 | validation: 0.5467506191493011]
	TIME [epoch: 7.61 sec]
EPOCH 323/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4300807897552567		[learning rate: 0.0013762]
	Learning Rate: 0.00137621
	LOSS [training: 0.4300807897552567 | validation: 0.5018561790963162]
	TIME [epoch: 7.64 sec]
EPOCH 324/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3827294496542931		[learning rate: 0.0013662]
	Learning Rate: 0.00136624
	LOSS [training: 0.3827294496542931 | validation: 0.597722894347809]
	TIME [epoch: 7.64 sec]
EPOCH 325/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4415542268378124		[learning rate: 0.0013563]
	Learning Rate: 0.00135634
	LOSS [training: 0.4415542268378124 | validation: 0.5671323957710472]
	TIME [epoch: 7.62 sec]
EPOCH 326/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.42777365311056337		[learning rate: 0.0013465]
	Learning Rate: 0.00134651
	LOSS [training: 0.42777365311056337 | validation: 0.5194845196817874]
	TIME [epoch: 7.61 sec]
EPOCH 327/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4186285692440942		[learning rate: 0.0013368]
	Learning Rate: 0.00133676
	LOSS [training: 0.4186285692440942 | validation: 0.488258557154964]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_327.pth
	Model improved!!!
EPOCH 328/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.40369563363921923		[learning rate: 0.0013271]
	Learning Rate: 0.00132707
	LOSS [training: 0.40369563363921923 | validation: 0.52851740213544]
	TIME [epoch: 7.67 sec]
EPOCH 329/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4037391660979125		[learning rate: 0.0013175]
	Learning Rate: 0.00131746
	LOSS [training: 0.4037391660979125 | validation: 0.4912641410713281]
	TIME [epoch: 7.62 sec]
EPOCH 330/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.40367099774632376		[learning rate: 0.0013079]
	Learning Rate: 0.00130791
	LOSS [training: 0.40367099774632376 | validation: 0.5251262740946628]
	TIME [epoch: 7.62 sec]
EPOCH 331/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4151543641854286		[learning rate: 0.0012984]
	Learning Rate: 0.00129844
	LOSS [training: 0.4151543641854286 | validation: 0.4748009836828637]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_331.pth
	Model improved!!!
EPOCH 332/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.42901551615401234		[learning rate: 0.001289]
	Learning Rate: 0.00128903
	LOSS [training: 0.42901551615401234 | validation: 0.5095594894785969]
	TIME [epoch: 7.63 sec]
EPOCH 333/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4165954491496616		[learning rate: 0.0012797]
	Learning Rate: 0.00127969
	LOSS [training: 0.4165954491496616 | validation: 0.5044553210215934]
	TIME [epoch: 7.67 sec]
EPOCH 334/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.43401936319096385		[learning rate: 0.0012704]
	Learning Rate: 0.00127042
	LOSS [training: 0.43401936319096385 | validation: 0.5075839907139781]
	TIME [epoch: 7.62 sec]
EPOCH 335/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3950322458431298		[learning rate: 0.0012612]
	Learning Rate: 0.00126122
	LOSS [training: 0.3950322458431298 | validation: 0.4877909561342727]
	TIME [epoch: 7.62 sec]
EPOCH 336/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4156580426754126		[learning rate: 0.0012521]
	Learning Rate: 0.00125208
	LOSS [training: 0.4156580426754126 | validation: 0.5311159184946724]
	TIME [epoch: 7.62 sec]
EPOCH 337/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.41529635050180325		[learning rate: 0.001243]
	Learning Rate: 0.00124301
	LOSS [training: 0.41529635050180325 | validation: 0.48399366355217155]
	TIME [epoch: 7.62 sec]
EPOCH 338/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4016428933852785		[learning rate: 0.001234]
	Learning Rate: 0.001234
	LOSS [training: 0.4016428933852785 | validation: 0.4799523116723791]
	TIME [epoch: 7.67 sec]
EPOCH 339/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.38254416524636575		[learning rate: 0.0012251]
	Learning Rate: 0.00122506
	LOSS [training: 0.38254416524636575 | validation: 0.4962601224578632]
	TIME [epoch: 7.62 sec]
EPOCH 340/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.40382951478506024		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.40382951478506024 | validation: 0.48483681386623023]
	TIME [epoch: 7.62 sec]
EPOCH 341/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.41930472670037955		[learning rate: 0.0012074]
	Learning Rate: 0.00120737
	LOSS [training: 0.41930472670037955 | validation: 0.5164524489011039]
	TIME [epoch: 7.62 sec]
EPOCH 342/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3996239286657288		[learning rate: 0.0011986]
	Learning Rate: 0.00119863
	LOSS [training: 0.3996239286657288 | validation: 0.4642140709399046]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_342.pth
	Model improved!!!
EPOCH 343/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.41572488465975377		[learning rate: 0.0011899]
	Learning Rate: 0.00118994
	LOSS [training: 0.41572488465975377 | validation: 0.5216151760491443]
	TIME [epoch: 7.68 sec]
EPOCH 344/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.38794203419039486		[learning rate: 0.0011813]
	Learning Rate: 0.00118132
	LOSS [training: 0.38794203419039486 | validation: 0.5244192836915826]
	TIME [epoch: 7.62 sec]
EPOCH 345/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4246563094960737		[learning rate: 0.0011728]
	Learning Rate: 0.00117276
	LOSS [training: 0.4246563094960737 | validation: 0.5001115346123206]
	TIME [epoch: 7.62 sec]
EPOCH 346/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.41293830160980527		[learning rate: 0.0011643]
	Learning Rate: 0.00116427
	LOSS [training: 0.41293830160980527 | validation: 0.4858468351012511]
	TIME [epoch: 7.62 sec]
EPOCH 347/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3777775343551602		[learning rate: 0.0011558]
	Learning Rate: 0.00115583
	LOSS [training: 0.3777775343551602 | validation: 0.5078680724544462]
	TIME [epoch: 7.64 sec]
EPOCH 348/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.41095358808541993		[learning rate: 0.0011475]
	Learning Rate: 0.00114746
	LOSS [training: 0.41095358808541993 | validation: 0.5232889386029745]
	TIME [epoch: 7.66 sec]
EPOCH 349/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.38459038383419203		[learning rate: 0.0011391]
	Learning Rate: 0.00113914
	LOSS [training: 0.38459038383419203 | validation: 0.490931614218241]
	TIME [epoch: 7.62 sec]
EPOCH 350/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.37113066884149254		[learning rate: 0.0011309]
	Learning Rate: 0.00113089
	LOSS [training: 0.37113066884149254 | validation: 0.5019494254595827]
	TIME [epoch: 7.62 sec]
EPOCH 351/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.43714067881124974		[learning rate: 0.0011227]
	Learning Rate: 0.0011227
	LOSS [training: 0.43714067881124974 | validation: 0.4872687643149333]
	TIME [epoch: 7.62 sec]
EPOCH 352/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3958151109012287		[learning rate: 0.0011146]
	Learning Rate: 0.00111456
	LOSS [training: 0.3958151109012287 | validation: 0.5205288819357798]
	TIME [epoch: 7.65 sec]
EPOCH 353/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4046116416867271		[learning rate: 0.0011065]
	Learning Rate: 0.00110649
	LOSS [training: 0.4046116416867271 | validation: 0.49059747983032853]
	TIME [epoch: 7.66 sec]
EPOCH 354/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4018786046607576		[learning rate: 0.0010985]
	Learning Rate: 0.00109847
	LOSS [training: 0.4018786046607576 | validation: 0.5106687323424559]
	TIME [epoch: 7.63 sec]
EPOCH 355/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.38815565226775994		[learning rate: 0.0010905]
	Learning Rate: 0.00109051
	LOSS [training: 0.38815565226775994 | validation: 0.5013876547323393]
	TIME [epoch: 7.63 sec]
EPOCH 356/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3875512043197416		[learning rate: 0.0010826]
	Learning Rate: 0.00108261
	LOSS [training: 0.3875512043197416 | validation: 0.5419172243070016]
	TIME [epoch: 7.62 sec]
EPOCH 357/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.39916921385322635		[learning rate: 0.0010748]
	Learning Rate: 0.00107477
	LOSS [training: 0.39916921385322635 | validation: 0.47304336203552705]
	TIME [epoch: 7.64 sec]
EPOCH 358/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3768730076119729		[learning rate: 0.001067]
	Learning Rate: 0.00106698
	LOSS [training: 0.3768730076119729 | validation: 0.5159855787812989]
	TIME [epoch: 7.67 sec]
EPOCH 359/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3826695106586122		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.3826695106586122 | validation: 0.5082711112672227]
	TIME [epoch: 7.62 sec]
EPOCH 360/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4149702789215694		[learning rate: 0.0010516]
	Learning Rate: 0.00105158
	LOSS [training: 0.4149702789215694 | validation: 0.4811294163608201]
	TIME [epoch: 7.62 sec]
EPOCH 361/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3807753635342211		[learning rate: 0.001044]
	Learning Rate: 0.00104396
	LOSS [training: 0.3807753635342211 | validation: 0.49965512884037766]
	TIME [epoch: 7.62 sec]
EPOCH 362/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3995571520789414		[learning rate: 0.0010364]
	Learning Rate: 0.0010364
	LOSS [training: 0.3995571520789414 | validation: 0.515856772587023]
	TIME [epoch: 7.65 sec]
EPOCH 363/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.39280340227322086		[learning rate: 0.0010289]
	Learning Rate: 0.00102889
	LOSS [training: 0.39280340227322086 | validation: 0.4790748185057452]
	TIME [epoch: 7.65 sec]
EPOCH 364/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3747605888388046		[learning rate: 0.0010214]
	Learning Rate: 0.00102143
	LOSS [training: 0.3747605888388046 | validation: 0.4744379773547005]
	TIME [epoch: 7.62 sec]
EPOCH 365/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.37084911074848664		[learning rate: 0.001014]
	Learning Rate: 0.00101403
	LOSS [training: 0.37084911074848664 | validation: 0.47698607603701204]
	TIME [epoch: 7.62 sec]
EPOCH 366/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.37005887957582695		[learning rate: 0.0010067]
	Learning Rate: 0.00100669
	LOSS [training: 0.37005887957582695 | validation: 0.4850764730953576]
	TIME [epoch: 7.62 sec]
EPOCH 367/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.38779166413806127		[learning rate: 0.00099939]
	Learning Rate: 0.000999394
	LOSS [training: 0.38779166413806127 | validation: 0.48231263959681603]
	TIME [epoch: 7.67 sec]
EPOCH 368/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3615233201714094		[learning rate: 0.00099215]
	Learning Rate: 0.000992154
	LOSS [training: 0.3615233201714094 | validation: 0.49381147256408264]
	TIME [epoch: 7.63 sec]
EPOCH 369/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.39580046648718575		[learning rate: 0.00098497]
	Learning Rate: 0.000984966
	LOSS [training: 0.39580046648718575 | validation: 0.5008817052002938]
	TIME [epoch: 7.62 sec]
EPOCH 370/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.36867208172894167		[learning rate: 0.00097783]
	Learning Rate: 0.00097783
	LOSS [training: 0.36867208172894167 | validation: 0.46344173213704937]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_370.pth
	Model improved!!!
EPOCH 371/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3953307247448452		[learning rate: 0.00097075]
	Learning Rate: 0.000970745
	LOSS [training: 0.3953307247448452 | validation: 0.47968141408283915]
	TIME [epoch: 7.63 sec]
EPOCH 372/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.36130059545963344		[learning rate: 0.00096371]
	Learning Rate: 0.000963712
	LOSS [training: 0.36130059545963344 | validation: 0.48512914634819515]
	TIME [epoch: 7.67 sec]
EPOCH 373/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.37562966064196157		[learning rate: 0.00095673]
	Learning Rate: 0.00095673
	LOSS [training: 0.37562966064196157 | validation: 0.5251038254927972]
	TIME [epoch: 7.62 sec]
EPOCH 374/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.38738492154061754		[learning rate: 0.0009498]
	Learning Rate: 0.000949799
	LOSS [training: 0.38738492154061754 | validation: 0.5215110056697854]
	TIME [epoch: 7.62 sec]
EPOCH 375/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3788596337079214		[learning rate: 0.00094292]
	Learning Rate: 0.000942918
	LOSS [training: 0.3788596337079214 | validation: 0.4684700401937535]
	TIME [epoch: 7.62 sec]
EPOCH 376/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.37705765870982383		[learning rate: 0.00093609]
	Learning Rate: 0.000936086
	LOSS [training: 0.37705765870982383 | validation: 0.4754888810540494]
	TIME [epoch: 7.62 sec]
EPOCH 377/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3729601529853144		[learning rate: 0.0009293]
	Learning Rate: 0.000929304
	LOSS [training: 0.3729601529853144 | validation: 0.4854733274372245]
	TIME [epoch: 7.67 sec]
EPOCH 378/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3773212413331299		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.3773212413331299 | validation: 0.4745007082893902]
	TIME [epoch: 7.63 sec]
EPOCH 379/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.37227929165708445		[learning rate: 0.00091589]
	Learning Rate: 0.000915888
	LOSS [training: 0.37227929165708445 | validation: 0.46976014692003976]
	TIME [epoch: 7.62 sec]
EPOCH 380/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.38810167684721225		[learning rate: 0.00090925]
	Learning Rate: 0.000909252
	LOSS [training: 0.38810167684721225 | validation: 0.5064467957406726]
	TIME [epoch: 7.62 sec]
EPOCH 381/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.37248471082071605		[learning rate: 0.00090266]
	Learning Rate: 0.000902664
	LOSS [training: 0.37248471082071605 | validation: 0.4809694291150163]
	TIME [epoch: 7.61 sec]
EPOCH 382/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.37218961295015496		[learning rate: 0.00089612]
	Learning Rate: 0.000896125
	LOSS [training: 0.37218961295015496 | validation: 0.4669823265219369]
	TIME [epoch: 7.67 sec]
EPOCH 383/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.36882294845215857		[learning rate: 0.00088963]
	Learning Rate: 0.000889632
	LOSS [training: 0.36882294845215857 | validation: 0.47371766757408296]
	TIME [epoch: 7.61 sec]
EPOCH 384/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3727623217573462		[learning rate: 0.00088319]
	Learning Rate: 0.000883187
	LOSS [training: 0.3727623217573462 | validation: 0.49128400383027876]
	TIME [epoch: 7.62 sec]
EPOCH 385/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3691045632841825		[learning rate: 0.00087679]
	Learning Rate: 0.000876788
	LOSS [training: 0.3691045632841825 | validation: 0.47265390673250396]
	TIME [epoch: 7.61 sec]
EPOCH 386/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3600042522909531		[learning rate: 0.00087044]
	Learning Rate: 0.000870436
	LOSS [training: 0.3600042522909531 | validation: 0.49494345304168474]
	TIME [epoch: 7.62 sec]
EPOCH 387/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.37321424789376434		[learning rate: 0.00086413]
	Learning Rate: 0.00086413
	LOSS [training: 0.37321424789376434 | validation: 0.48329588953635216]
	TIME [epoch: 7.67 sec]
EPOCH 388/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3703948669613077		[learning rate: 0.00085787]
	Learning Rate: 0.000857869
	LOSS [training: 0.3703948669613077 | validation: 0.46328612793859725]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_388.pth
	Model improved!!!
EPOCH 389/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.37608738965657373		[learning rate: 0.00085165]
	Learning Rate: 0.000851654
	LOSS [training: 0.37608738965657373 | validation: 0.4878548453521796]
	TIME [epoch: 7.62 sec]
EPOCH 390/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3659051677600554		[learning rate: 0.00084548]
	Learning Rate: 0.000845484
	LOSS [training: 0.3659051677600554 | validation: 0.45620226679500653]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_390.pth
	Model improved!!!
EPOCH 391/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3541994095157599		[learning rate: 0.00083936]
	Learning Rate: 0.000839358
	LOSS [training: 0.3541994095157599 | validation: 0.5207601752307824]
	TIME [epoch: 7.63 sec]
EPOCH 392/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3939118159259576		[learning rate: 0.00083328]
	Learning Rate: 0.000833277
	LOSS [training: 0.3939118159259576 | validation: 0.5092199963383622]
	TIME [epoch: 7.66 sec]
EPOCH 393/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3487359231068532		[learning rate: 0.00082724]
	Learning Rate: 0.00082724
	LOSS [training: 0.3487359231068532 | validation: 0.4748611755645862]
	TIME [epoch: 7.61 sec]
EPOCH 394/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.37649887887999584		[learning rate: 0.00082125]
	Learning Rate: 0.000821247
	LOSS [training: 0.37649887887999584 | validation: 0.4831967314430681]
	TIME [epoch: 7.62 sec]
EPOCH 395/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.37600150526007625		[learning rate: 0.0008153]
	Learning Rate: 0.000815297
	LOSS [training: 0.37600150526007625 | validation: 0.4617815597289821]
	TIME [epoch: 7.61 sec]
EPOCH 396/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.357205939300696		[learning rate: 0.00080939]
	Learning Rate: 0.00080939
	LOSS [training: 0.357205939300696 | validation: 0.48387180308524524]
	TIME [epoch: 7.65 sec]
EPOCH 397/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.35399082780985985		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.35399082780985985 | validation: 0.524605741102261]
	TIME [epoch: 7.64 sec]
EPOCH 398/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.38343655997635717		[learning rate: 0.0007977]
	Learning Rate: 0.000797705
	LOSS [training: 0.38343655997635717 | validation: 0.46238685462695495]
	TIME [epoch: 7.62 sec]
EPOCH 399/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3696372923320248		[learning rate: 0.00079193]
	Learning Rate: 0.000791925
	LOSS [training: 0.3696372923320248 | validation: 0.48848047355165314]
	TIME [epoch: 7.61 sec]
EPOCH 400/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.35945089454371815		[learning rate: 0.00078619]
	Learning Rate: 0.000786188
	LOSS [training: 0.35945089454371815 | validation: 0.45339631777265765]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_400.pth
	Model improved!!!
EPOCH 401/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3568132394016991		[learning rate: 0.00078049]
	Learning Rate: 0.000780492
	LOSS [training: 0.3568132394016991 | validation: 0.4693553715839902]
	TIME [epoch: 7.66 sec]
EPOCH 402/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3549790664959201		[learning rate: 0.00077484]
	Learning Rate: 0.000774838
	LOSS [training: 0.3549790664959201 | validation: 0.4727731600869832]
	TIME [epoch: 7.62 sec]
EPOCH 403/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.36713586525489605		[learning rate: 0.00076922]
	Learning Rate: 0.000769224
	LOSS [training: 0.36713586525489605 | validation: 0.4464959107351565]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_403.pth
	Model improved!!!
EPOCH 404/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.35336828764806616		[learning rate: 0.00076365]
	Learning Rate: 0.000763651
	LOSS [training: 0.35336828764806616 | validation: 0.46595567759501516]
	TIME [epoch: 7.61 sec]
EPOCH 405/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.35600921061990864		[learning rate: 0.00075812]
	Learning Rate: 0.000758118
	LOSS [training: 0.35600921061990864 | validation: 0.4598405468081542]
	TIME [epoch: 7.61 sec]
EPOCH 406/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3722824384927514		[learning rate: 0.00075263]
	Learning Rate: 0.000752626
	LOSS [training: 0.3722824384927514 | validation: 0.4637907423307359]
	TIME [epoch: 7.66 sec]
EPOCH 407/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3599925375551598		[learning rate: 0.00074717]
	Learning Rate: 0.000747173
	LOSS [training: 0.3599925375551598 | validation: 0.4663060798907702]
	TIME [epoch: 7.61 sec]
EPOCH 408/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3507663125772693		[learning rate: 0.00074176]
	Learning Rate: 0.00074176
	LOSS [training: 0.3507663125772693 | validation: 0.48940912071252907]
	TIME [epoch: 7.62 sec]
EPOCH 409/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.34855750221564114		[learning rate: 0.00073639]
	Learning Rate: 0.000736386
	LOSS [training: 0.34855750221564114 | validation: 0.47781805991659254]
	TIME [epoch: 7.61 sec]
EPOCH 410/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.36366492120579996		[learning rate: 0.00073105]
	Learning Rate: 0.000731051
	LOSS [training: 0.36366492120579996 | validation: 0.4629325585758545]
	TIME [epoch: 7.61 sec]
EPOCH 411/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.34732892539101107		[learning rate: 0.00072575]
	Learning Rate: 0.000725754
	LOSS [training: 0.34732892539101107 | validation: 0.4541116523214754]
	TIME [epoch: 7.66 sec]
EPOCH 412/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3419000067000004		[learning rate: 0.0007205]
	Learning Rate: 0.000720496
	LOSS [training: 0.3419000067000004 | validation: 0.459012890088453]
	TIME [epoch: 7.61 sec]
EPOCH 413/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3429627234157268		[learning rate: 0.00071528]
	Learning Rate: 0.000715276
	LOSS [training: 0.3429627234157268 | validation: 0.45391239912278086]
	TIME [epoch: 7.61 sec]
EPOCH 414/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.36669188796784347		[learning rate: 0.00071009]
	Learning Rate: 0.000710094
	LOSS [training: 0.36669188796784347 | validation: 0.46459167284594566]
	TIME [epoch: 7.61 sec]
EPOCH 415/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3488635738608482		[learning rate: 0.00070495]
	Learning Rate: 0.000704949
	LOSS [training: 0.3488635738608482 | validation: 0.4691122618773074]
	TIME [epoch: 7.61 sec]
EPOCH 416/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.36187303959507355		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.36187303959507355 | validation: 0.46439016927204835]
	TIME [epoch: 7.68 sec]
EPOCH 417/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3420405119030379		[learning rate: 0.00069477]
	Learning Rate: 0.000694772
	LOSS [training: 0.3420405119030379 | validation: 0.4728542940025145]
	TIME [epoch: 7.62 sec]
EPOCH 418/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3549212602097984		[learning rate: 0.00068974]
	Learning Rate: 0.000689738
	LOSS [training: 0.3549212602097984 | validation: 0.47913526646395105]
	TIME [epoch: 7.61 sec]
EPOCH 419/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.35824812603856715		[learning rate: 0.00068474]
	Learning Rate: 0.000684741
	LOSS [training: 0.35824812603856715 | validation: 0.45815974715635044]
	TIME [epoch: 7.61 sec]
EPOCH 420/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.34913697761970097		[learning rate: 0.00067978]
	Learning Rate: 0.00067978
	LOSS [training: 0.34913697761970097 | validation: 0.45621778894826176]
	TIME [epoch: 7.62 sec]
EPOCH 421/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.346431600284159		[learning rate: 0.00067486]
	Learning Rate: 0.000674855
	LOSS [training: 0.346431600284159 | validation: 0.47434795609671176]
	TIME [epoch: 7.67 sec]
EPOCH 422/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.35469729711741116		[learning rate: 0.00066997]
	Learning Rate: 0.000669966
	LOSS [training: 0.35469729711741116 | validation: 0.4617286943361605]
	TIME [epoch: 7.62 sec]
EPOCH 423/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3466900433779607		[learning rate: 0.00066511]
	Learning Rate: 0.000665112
	LOSS [training: 0.3466900433779607 | validation: 0.45712850606453026]
	TIME [epoch: 7.62 sec]
EPOCH 424/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3377044599279557		[learning rate: 0.00066029]
	Learning Rate: 0.000660293
	LOSS [training: 0.3377044599279557 | validation: 0.4571544672938642]
	TIME [epoch: 7.61 sec]
EPOCH 425/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3322057771314605		[learning rate: 0.00065551]
	Learning Rate: 0.00065551
	LOSS [training: 0.3322057771314605 | validation: 0.4532686237183109]
	TIME [epoch: 7.62 sec]
EPOCH 426/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.34400234970356935		[learning rate: 0.00065076]
	Learning Rate: 0.00065076
	LOSS [training: 0.34400234970356935 | validation: 0.45191097790150603]
	TIME [epoch: 7.67 sec]
EPOCH 427/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3392163758573857		[learning rate: 0.00064605]
	Learning Rate: 0.000646046
	LOSS [training: 0.3392163758573857 | validation: 0.4880644254534889]
	TIME [epoch: 7.62 sec]
EPOCH 428/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3567899999700812		[learning rate: 0.00064137]
	Learning Rate: 0.000641365
	LOSS [training: 0.3567899999700812 | validation: 0.45103395016774295]
	TIME [epoch: 7.62 sec]
EPOCH 429/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3406999078387401		[learning rate: 0.00063672]
	Learning Rate: 0.000636718
	LOSS [training: 0.3406999078387401 | validation: 0.4581320986971503]
	TIME [epoch: 7.61 sec]
EPOCH 430/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3469517303417828		[learning rate: 0.00063211]
	Learning Rate: 0.000632105
	LOSS [training: 0.3469517303417828 | validation: 0.46716800033401457]
	TIME [epoch: 7.63 sec]
EPOCH 431/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.33590897823787225		[learning rate: 0.00062753]
	Learning Rate: 0.000627526
	LOSS [training: 0.33590897823787225 | validation: 0.4582533480340205]
	TIME [epoch: 7.66 sec]
EPOCH 432/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.34545730595589924		[learning rate: 0.00062298]
	Learning Rate: 0.000622979
	LOSS [training: 0.34545730595589924 | validation: 0.47471622372408656]
	TIME [epoch: 7.62 sec]
EPOCH 433/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.34545353347648583		[learning rate: 0.00061847]
	Learning Rate: 0.000618466
	LOSS [training: 0.34545353347648583 | validation: 0.46395395412984447]
	TIME [epoch: 7.62 sec]
EPOCH 434/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.33279894703875884		[learning rate: 0.00061399]
	Learning Rate: 0.000613985
	LOSS [training: 0.33279894703875884 | validation: 0.48791099759360723]
	TIME [epoch: 7.62 sec]
EPOCH 435/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.36140551535365184		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.36140551535365184 | validation: 0.4489181551382226]
	TIME [epoch: 7.63 sec]
EPOCH 436/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3280301531198743		[learning rate: 0.00060512]
	Learning Rate: 0.000605121
	LOSS [training: 0.3280301531198743 | validation: 0.4517098757119829]
	TIME [epoch: 7.66 sec]
EPOCH 437/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.33590908388987945		[learning rate: 0.00060074]
	Learning Rate: 0.000600737
	LOSS [training: 0.33590908388987945 | validation: 0.44977143345268444]
	TIME [epoch: 7.62 sec]
EPOCH 438/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3232041851817433		[learning rate: 0.00059638]
	Learning Rate: 0.000596384
	LOSS [training: 0.3232041851817433 | validation: 0.44951842738249637]
	TIME [epoch: 7.61 sec]
EPOCH 439/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3431862298871993		[learning rate: 0.00059206]
	Learning Rate: 0.000592064
	LOSS [training: 0.3431862298871993 | validation: 0.4457779060852348]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_439.pth
	Model improved!!!
EPOCH 440/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3352577967338093		[learning rate: 0.00058777]
	Learning Rate: 0.000587774
	LOSS [training: 0.3352577967338093 | validation: 0.4829807973767675]
	TIME [epoch: 7.67 sec]
EPOCH 441/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.33414697079542205		[learning rate: 0.00058352]
	Learning Rate: 0.000583516
	LOSS [training: 0.33414697079542205 | validation: 0.44414879784545624]
	TIME [epoch: 7.64 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_441.pth
	Model improved!!!
EPOCH 442/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.34541266014087546		[learning rate: 0.00057929]
	Learning Rate: 0.000579288
	LOSS [training: 0.34541266014087546 | validation: 0.45959277458505093]
	TIME [epoch: 7.62 sec]
EPOCH 443/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.33177729876402007		[learning rate: 0.00057509]
	Learning Rate: 0.000575091
	LOSS [training: 0.33177729876402007 | validation: 0.4464455967960911]
	TIME [epoch: 7.62 sec]
EPOCH 444/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.32599135599698004		[learning rate: 0.00057093]
	Learning Rate: 0.000570925
	LOSS [training: 0.32599135599698004 | validation: 0.4519657201250741]
	TIME [epoch: 7.62 sec]
EPOCH 445/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3535061830859241		[learning rate: 0.00056679]
	Learning Rate: 0.000566789
	LOSS [training: 0.3535061830859241 | validation: 0.44841416570645243]
	TIME [epoch: 7.67 sec]
EPOCH 446/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.33670712413940823		[learning rate: 0.00056268]
	Learning Rate: 0.000562682
	LOSS [training: 0.33670712413940823 | validation: 0.4365653936594454]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_446.pth
	Model improved!!!
EPOCH 447/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3336408593647704		[learning rate: 0.00055861]
	Learning Rate: 0.000558606
	LOSS [training: 0.3336408593647704 | validation: 0.4591852540365534]
	TIME [epoch: 7.62 sec]
EPOCH 448/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3400219247599189		[learning rate: 0.00055456]
	Learning Rate: 0.000554559
	LOSS [training: 0.3400219247599189 | validation: 0.4367097548313381]
	TIME [epoch: 7.62 sec]
EPOCH 449/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3252422933692914		[learning rate: 0.00055054]
	Learning Rate: 0.000550541
	LOSS [training: 0.3252422933692914 | validation: 0.4457968063639195]
	TIME [epoch: 7.62 sec]
EPOCH 450/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.32974123079987633		[learning rate: 0.00054655]
	Learning Rate: 0.000546552
	LOSS [training: 0.32974123079987633 | validation: 0.4470305197240795]
	TIME [epoch: 7.68 sec]
EPOCH 451/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.32590232486455806		[learning rate: 0.00054259]
	Learning Rate: 0.000542592
	LOSS [training: 0.32590232486455806 | validation: 0.45067627089330126]
	TIME [epoch: 7.62 sec]
EPOCH 452/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3201647091523042		[learning rate: 0.00053866]
	Learning Rate: 0.000538661
	LOSS [training: 0.3201647091523042 | validation: 0.4754930842923064]
	TIME [epoch: 7.62 sec]
EPOCH 453/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.352928325302976		[learning rate: 0.00053476]
	Learning Rate: 0.000534759
	LOSS [training: 0.352928325302976 | validation: 0.445903754282847]
	TIME [epoch: 7.62 sec]
EPOCH 454/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.32848760408257294		[learning rate: 0.00053088]
	Learning Rate: 0.000530884
	LOSS [training: 0.32848760408257294 | validation: 0.4442421500290371]
	TIME [epoch: 7.62 sec]
EPOCH 455/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3281009945002985		[learning rate: 0.00052704]
	Learning Rate: 0.000527038
	LOSS [training: 0.3281009945002985 | validation: 0.4336739573791217]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_455.pth
	Model improved!!!
EPOCH 456/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.33473631637841367		[learning rate: 0.00052322]
	Learning Rate: 0.00052322
	LOSS [training: 0.33473631637841367 | validation: 0.4448471148771773]
	TIME [epoch: 7.62 sec]
EPOCH 457/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.32003607132498935		[learning rate: 0.00051943]
	Learning Rate: 0.000519429
	LOSS [training: 0.32003607132498935 | validation: 0.4379083265873447]
	TIME [epoch: 7.61 sec]
EPOCH 458/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3359422418219301		[learning rate: 0.00051567]
	Learning Rate: 0.000515666
	LOSS [training: 0.3359422418219301 | validation: 0.43822735553036807]
	TIME [epoch: 7.61 sec]
EPOCH 459/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.32438829402820224		[learning rate: 0.00051193]
	Learning Rate: 0.00051193
	LOSS [training: 0.32438829402820224 | validation: 0.43601167937423146]
	TIME [epoch: 7.63 sec]
EPOCH 460/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3250370095223499		[learning rate: 0.00050822]
	Learning Rate: 0.000508221
	LOSS [training: 0.3250370095223499 | validation: 0.43802084444942657]
	TIME [epoch: 7.65 sec]
EPOCH 461/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.32717196852702946		[learning rate: 0.00050454]
	Learning Rate: 0.000504539
	LOSS [training: 0.32717196852702946 | validation: 0.4349667855421787]
	TIME [epoch: 7.62 sec]
EPOCH 462/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.32586909228178307		[learning rate: 0.00050088]
	Learning Rate: 0.000500884
	LOSS [training: 0.32586909228178307 | validation: 0.44538223315088615]
	TIME [epoch: 7.62 sec]
EPOCH 463/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.31997291187626864		[learning rate: 0.00049725]
	Learning Rate: 0.000497255
	LOSS [training: 0.31997291187626864 | validation: 0.43216392151063704]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_463.pth
	Model improved!!!
EPOCH 464/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3172669537222345		[learning rate: 0.00049365]
	Learning Rate: 0.000493652
	LOSS [training: 0.3172669537222345 | validation: 0.44427928066489264]
	TIME [epoch: 7.63 sec]
EPOCH 465/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3231287810804807		[learning rate: 0.00049008]
	Learning Rate: 0.000490076
	LOSS [training: 0.3231287810804807 | validation: 0.4416878632240079]
	TIME [epoch: 7.65 sec]
EPOCH 466/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.32181245096322186		[learning rate: 0.00048653]
	Learning Rate: 0.000486525
	LOSS [training: 0.32181245096322186 | validation: 0.46117331641979975]
	TIME [epoch: 7.61 sec]
EPOCH 467/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.33400919844782967		[learning rate: 0.000483]
	Learning Rate: 0.000483
	LOSS [training: 0.33400919844782967 | validation: 0.44337382372878864]
	TIME [epoch: 7.61 sec]
EPOCH 468/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.32031964210566777		[learning rate: 0.0004795]
	Learning Rate: 0.000479501
	LOSS [training: 0.32031964210566777 | validation: 0.4434239900334835]
	TIME [epoch: 7.61 sec]
EPOCH 469/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.31906480187081904		[learning rate: 0.00047603]
	Learning Rate: 0.000476027
	LOSS [training: 0.31906480187081904 | validation: 0.43850451886329345]
	TIME [epoch: 7.64 sec]
EPOCH 470/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3258215428516256		[learning rate: 0.00047258]
	Learning Rate: 0.000472578
	LOSS [training: 0.3258215428516256 | validation: 0.43251494908911187]
	TIME [epoch: 7.64 sec]
EPOCH 471/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3109229376508648		[learning rate: 0.00046915]
	Learning Rate: 0.000469154
	LOSS [training: 0.3109229376508648 | validation: 0.4361689010945565]
	TIME [epoch: 7.62 sec]
EPOCH 472/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.32455856747984746		[learning rate: 0.00046576]
	Learning Rate: 0.000465755
	LOSS [training: 0.32455856747984746 | validation: 0.4367119710311822]
	TIME [epoch: 7.62 sec]
EPOCH 473/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3240775715619043		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.3240775715619043 | validation: 0.45154484545658063]
	TIME [epoch: 7.61 sec]
EPOCH 474/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.32447257477155433		[learning rate: 0.00045903]
	Learning Rate: 0.000459031
	LOSS [training: 0.32447257477155433 | validation: 0.4296604782577866]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_474.pth
	Model improved!!!
EPOCH 475/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.31524572834559084		[learning rate: 0.00045571]
	Learning Rate: 0.000455706
	LOSS [training: 0.31524572834559084 | validation: 0.4488658413838902]
	TIME [epoch: 7.62 sec]
EPOCH 476/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.317399498252254		[learning rate: 0.0004524]
	Learning Rate: 0.000452404
	LOSS [training: 0.317399498252254 | validation: 0.4270076421378044]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_476.pth
	Model improved!!!
EPOCH 477/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.31944833133655126		[learning rate: 0.00044913]
	Learning Rate: 0.000449126
	LOSS [training: 0.31944833133655126 | validation: 0.4299122634890111]
	TIME [epoch: 7.62 sec]
EPOCH 478/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3158873585612595		[learning rate: 0.00044587]
	Learning Rate: 0.000445872
	LOSS [training: 0.3158873585612595 | validation: 0.4327488986753041]
	TIME [epoch: 7.62 sec]
EPOCH 479/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3110017506554011		[learning rate: 0.00044264]
	Learning Rate: 0.000442642
	LOSS [training: 0.3110017506554011 | validation: 0.46115714223094806]
	TIME [epoch: 7.68 sec]
EPOCH 480/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.32133472210160186		[learning rate: 0.00043944]
	Learning Rate: 0.000439435
	LOSS [training: 0.32133472210160186 | validation: 0.46245468222543756]
	TIME [epoch: 7.62 sec]
EPOCH 481/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3394123342202634		[learning rate: 0.00043625]
	Learning Rate: 0.000436251
	LOSS [training: 0.3394123342202634 | validation: 0.4372075985552142]
	TIME [epoch: 7.61 sec]
EPOCH 482/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3100240306230404		[learning rate: 0.00043309]
	Learning Rate: 0.000433091
	LOSS [training: 0.3100240306230404 | validation: 0.4416328084332549]
	TIME [epoch: 7.62 sec]
EPOCH 483/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3217984217676344		[learning rate: 0.00042995]
	Learning Rate: 0.000429953
	LOSS [training: 0.3217984217676344 | validation: 0.44497524009279704]
	TIME [epoch: 7.61 sec]
EPOCH 484/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.32470990992289217		[learning rate: 0.00042684]
	Learning Rate: 0.000426838
	LOSS [training: 0.32470990992289217 | validation: 0.44586995306461275]
	TIME [epoch: 7.68 sec]
EPOCH 485/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3241778722962834		[learning rate: 0.00042375]
	Learning Rate: 0.000423746
	LOSS [training: 0.3241778722962834 | validation: 0.43987085107049484]
	TIME [epoch: 7.62 sec]
EPOCH 486/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.31560371915517205		[learning rate: 0.00042068]
	Learning Rate: 0.000420676
	LOSS [training: 0.31560371915517205 | validation: 0.4316697620357889]
	TIME [epoch: 7.62 sec]
EPOCH 487/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3104499053030034		[learning rate: 0.00041763]
	Learning Rate: 0.000417628
	LOSS [training: 0.3104499053030034 | validation: 0.4353857775218726]
	TIME [epoch: 7.62 sec]
EPOCH 488/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.31103619842273966		[learning rate: 0.0004146]
	Learning Rate: 0.000414602
	LOSS [training: 0.31103619842273966 | validation: 0.43415102178591325]
	TIME [epoch: 7.62 sec]
EPOCH 489/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3167699344902309		[learning rate: 0.0004116]
	Learning Rate: 0.000411598
	LOSS [training: 0.3167699344902309 | validation: 0.4465410032470781]
	TIME [epoch: 7.68 sec]
EPOCH 490/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3158976171369853		[learning rate: 0.00040862]
	Learning Rate: 0.000408616
	LOSS [training: 0.3158976171369853 | validation: 0.43517278589983377]
	TIME [epoch: 7.62 sec]
EPOCH 491/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3126065824288598		[learning rate: 0.00040566]
	Learning Rate: 0.000405656
	LOSS [training: 0.3126065824288598 | validation: 0.42810586786465044]
	TIME [epoch: 7.62 sec]
EPOCH 492/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3160692422270555		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.3160692422270555 | validation: 0.4507432213254996]
	TIME [epoch: 7.62 sec]
EPOCH 493/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3153069097088998		[learning rate: 0.0003998]
	Learning Rate: 0.000399799
	LOSS [training: 0.3153069097088998 | validation: 0.44140963978580827]
	TIME [epoch: 7.63 sec]
EPOCH 494/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3181307713826619		[learning rate: 0.0003969]
	Learning Rate: 0.000396903
	LOSS [training: 0.3181307713826619 | validation: 0.4268190622170639]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_494.pth
	Model improved!!!
EPOCH 495/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3100483644379336		[learning rate: 0.00039403]
	Learning Rate: 0.000394027
	LOSS [training: 0.3100483644379336 | validation: 0.4352373701174507]
	TIME [epoch: 7.62 sec]
EPOCH 496/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.31251268384902503		[learning rate: 0.00039117]
	Learning Rate: 0.000391173
	LOSS [training: 0.31251268384902503 | validation: 0.4301640176422612]
	TIME [epoch: 7.62 sec]
EPOCH 497/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30975583898570946		[learning rate: 0.00038834]
	Learning Rate: 0.000388339
	LOSS [training: 0.30975583898570946 | validation: 0.42297061711179107]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_497.pth
	Model improved!!!
EPOCH 498/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3057727601856879		[learning rate: 0.00038553]
	Learning Rate: 0.000385525
	LOSS [training: 0.3057727601856879 | validation: 0.4253786685505297]
	TIME [epoch: 7.65 sec]
EPOCH 499/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3133901390356451		[learning rate: 0.00038273]
	Learning Rate: 0.000382732
	LOSS [training: 0.3133901390356451 | validation: 0.4355461016161385]
	TIME [epoch: 7.66 sec]
EPOCH 500/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3086932721677884		[learning rate: 0.00037996]
	Learning Rate: 0.000379959
	LOSS [training: 0.3086932721677884 | validation: 0.44055960167378183]
	TIME [epoch: 7.63 sec]
EPOCH 501/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3101628543520407		[learning rate: 0.00037721]
	Learning Rate: 0.000377206
	LOSS [training: 0.3101628543520407 | validation: 0.4422535155208739]
	TIME [epoch: 7.63 sec]
EPOCH 502/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3112135980862898		[learning rate: 0.00037447]
	Learning Rate: 0.000374474
	LOSS [training: 0.3112135980862898 | validation: 0.4290558494973327]
	TIME [epoch: 7.63 sec]
EPOCH 503/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30943976718380767		[learning rate: 0.00037176]
	Learning Rate: 0.00037176
	LOSS [training: 0.30943976718380767 | validation: 0.42722892727126943]
	TIME [epoch: 7.67 sec]
EPOCH 504/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30814276715235667		[learning rate: 0.00036907]
	Learning Rate: 0.000369067
	LOSS [training: 0.30814276715235667 | validation: 0.430932670598555]
	TIME [epoch: 7.64 sec]
EPOCH 505/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30150846807814813		[learning rate: 0.00036639]
	Learning Rate: 0.000366393
	LOSS [training: 0.30150846807814813 | validation: 0.420778877934622]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_505.pth
	Model improved!!!
EPOCH 506/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3141675233620411		[learning rate: 0.00036374]
	Learning Rate: 0.000363739
	LOSS [training: 0.3141675233620411 | validation: 0.42878560774991537]
	TIME [epoch: 7.63 sec]
EPOCH 507/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3037641900060464		[learning rate: 0.0003611]
	Learning Rate: 0.000361103
	LOSS [training: 0.3037641900060464 | validation: 0.4317459314413582]
	TIME [epoch: 7.62 sec]
EPOCH 508/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3065626718503387		[learning rate: 0.00035849]
	Learning Rate: 0.000358487
	LOSS [training: 0.3065626718503387 | validation: 0.4343567361461159]
	TIME [epoch: 7.67 sec]
EPOCH 509/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3053851665540822		[learning rate: 0.00035589]
	Learning Rate: 0.00035589
	LOSS [training: 0.3053851665540822 | validation: 0.4395332608780269]
	TIME [epoch: 7.63 sec]
EPOCH 510/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3143359562460333		[learning rate: 0.00035331]
	Learning Rate: 0.000353312
	LOSS [training: 0.3143359562460333 | validation: 0.44496288589111005]
	TIME [epoch: 7.62 sec]
EPOCH 511/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.303653968370056		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.303653968370056 | validation: 0.42951526515523064]
	TIME [epoch: 7.62 sec]
EPOCH 512/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30487218454342196		[learning rate: 0.00034821]
	Learning Rate: 0.000348211
	LOSS [training: 0.30487218454342196 | validation: 0.4252392953842722]
	TIME [epoch: 7.62 sec]
EPOCH 513/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.31151434978084913		[learning rate: 0.00034569]
	Learning Rate: 0.000345688
	LOSS [training: 0.31151434978084913 | validation: 0.4309978535089951]
	TIME [epoch: 7.68 sec]
EPOCH 514/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30542794966570036		[learning rate: 0.00034318]
	Learning Rate: 0.000343183
	LOSS [training: 0.30542794966570036 | validation: 0.4236453099812584]
	TIME [epoch: 7.63 sec]
EPOCH 515/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29902814609028616		[learning rate: 0.0003407]
	Learning Rate: 0.000340697
	LOSS [training: 0.29902814609028616 | validation: 0.42170972539561136]
	TIME [epoch: 7.62 sec]
EPOCH 516/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30523477510963243		[learning rate: 0.00033823]
	Learning Rate: 0.000338229
	LOSS [training: 0.30523477510963243 | validation: 0.43154087732206403]
	TIME [epoch: 7.62 sec]
EPOCH 517/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.31257533297187734		[learning rate: 0.00033578]
	Learning Rate: 0.000335778
	LOSS [training: 0.31257533297187734 | validation: 0.429476283996546]
	TIME [epoch: 7.62 sec]
EPOCH 518/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29852817893318057		[learning rate: 0.00033335]
	Learning Rate: 0.000333346
	LOSS [training: 0.29852817893318057 | validation: 0.42265778672175924]
	TIME [epoch: 7.68 sec]
EPOCH 519/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3013454504241873		[learning rate: 0.00033093]
	Learning Rate: 0.000330931
	LOSS [training: 0.3013454504241873 | validation: 0.4408006504182502]
	TIME [epoch: 7.62 sec]
EPOCH 520/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30644592050974945		[learning rate: 0.00032853]
	Learning Rate: 0.000328533
	LOSS [training: 0.30644592050974945 | validation: 0.42749726045167336]
	TIME [epoch: 7.62 sec]
EPOCH 521/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3048618982046018		[learning rate: 0.00032615]
	Learning Rate: 0.000326153
	LOSS [training: 0.3048618982046018 | validation: 0.4228447923640193]
	TIME [epoch: 7.62 sec]
EPOCH 522/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3008331954681691		[learning rate: 0.00032379]
	Learning Rate: 0.00032379
	LOSS [training: 0.3008331954681691 | validation: 0.4402454779493724]
	TIME [epoch: 7.63 sec]
EPOCH 523/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3050964659758075		[learning rate: 0.00032144]
	Learning Rate: 0.000321444
	LOSS [training: 0.3050964659758075 | validation: 0.4203273831719707]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_523.pth
	Model improved!!!
EPOCH 524/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2953462112263452		[learning rate: 0.00031912]
	Learning Rate: 0.000319115
	LOSS [training: 0.2953462112263452 | validation: 0.42923712195582586]
	TIME [epoch: 7.63 sec]
EPOCH 525/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30295548837695363		[learning rate: 0.0003168]
	Learning Rate: 0.000316803
	LOSS [training: 0.30295548837695363 | validation: 0.43133272984919424]
	TIME [epoch: 7.62 sec]
EPOCH 526/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3024692635027193		[learning rate: 0.00031451]
	Learning Rate: 0.000314508
	LOSS [training: 0.3024692635027193 | validation: 0.4236625002655975]
	TIME [epoch: 7.62 sec]
EPOCH 527/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30110157225314416		[learning rate: 0.00031223]
	Learning Rate: 0.000312229
	LOSS [training: 0.30110157225314416 | validation: 0.4187593444454162]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_527.pth
	Model improved!!!
EPOCH 528/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2982886081864638		[learning rate: 0.00030997]
	Learning Rate: 0.000309967
	LOSS [training: 0.2982886081864638 | validation: 0.4235423016219676]
	TIME [epoch: 7.66 sec]
EPOCH 529/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30035383295369966		[learning rate: 0.00030772]
	Learning Rate: 0.000307722
	LOSS [training: 0.30035383295369966 | validation: 0.4274576817752044]
	TIME [epoch: 7.62 sec]
EPOCH 530/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30265097625006215		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.30265097625006215 | validation: 0.4168664166761226]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_530.pth
	Model improved!!!
EPOCH 531/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30245251335468015		[learning rate: 0.00030328]
	Learning Rate: 0.000303279
	LOSS [training: 0.30245251335468015 | validation: 0.42579356428027537]
	TIME [epoch: 7.62 sec]
EPOCH 532/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3071803892609104		[learning rate: 0.00030108]
	Learning Rate: 0.000301082
	LOSS [training: 0.3071803892609104 | validation: 0.419430638756385]
	TIME [epoch: 7.67 sec]
EPOCH 533/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2942514042089821		[learning rate: 0.0002989]
	Learning Rate: 0.0002989
	LOSS [training: 0.2942514042089821 | validation: 0.4233408500928516]
	TIME [epoch: 7.63 sec]
EPOCH 534/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30214013655719413		[learning rate: 0.00029673]
	Learning Rate: 0.000296735
	LOSS [training: 0.30214013655719413 | validation: 0.419087991520066]
	TIME [epoch: 7.62 sec]
EPOCH 535/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3071196709894501		[learning rate: 0.00029458]
	Learning Rate: 0.000294585
	LOSS [training: 0.3071196709894501 | validation: 0.4301026072729893]
	TIME [epoch: 7.62 sec]
EPOCH 536/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2991611125189039		[learning rate: 0.00029245]
	Learning Rate: 0.000292451
	LOSS [training: 0.2991611125189039 | validation: 0.422786702438568]
	TIME [epoch: 7.62 sec]
EPOCH 537/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3025353986771661		[learning rate: 0.00029033]
	Learning Rate: 0.000290332
	LOSS [training: 0.3025353986771661 | validation: 0.41813449382091494]
	TIME [epoch: 7.67 sec]
EPOCH 538/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2930893073016138		[learning rate: 0.00028823]
	Learning Rate: 0.000288228
	LOSS [training: 0.2930893073016138 | validation: 0.4183806340339218]
	TIME [epoch: 7.63 sec]
EPOCH 539/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2952992060894266		[learning rate: 0.00028614]
	Learning Rate: 0.00028614
	LOSS [training: 0.2952992060894266 | validation: 0.41599020675672527]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_539.pth
	Model improved!!!
EPOCH 540/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2988017950006554		[learning rate: 0.00028407]
	Learning Rate: 0.000284067
	LOSS [training: 0.2988017950006554 | validation: 0.41521561038156085]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_540.pth
	Model improved!!!
EPOCH 541/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29949691709756204		[learning rate: 0.00028201]
	Learning Rate: 0.000282009
	LOSS [training: 0.29949691709756204 | validation: 0.42470052572172895]
	TIME [epoch: 7.63 sec]
EPOCH 542/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29839553470432145		[learning rate: 0.00027997]
	Learning Rate: 0.000279966
	LOSS [training: 0.29839553470432145 | validation: 0.4150365122638108]
	TIME [epoch: 7.68 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_542.pth
	Model improved!!!
EPOCH 543/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2953698029224774		[learning rate: 0.00027794]
	Learning Rate: 0.000277938
	LOSS [training: 0.2953698029224774 | validation: 0.41200167722814945]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_543.pth
	Model improved!!!
EPOCH 544/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29655078907309224		[learning rate: 0.00027592]
	Learning Rate: 0.000275924
	LOSS [training: 0.29655078907309224 | validation: 0.42480912761967343]
	TIME [epoch: 7.63 sec]
EPOCH 545/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2997039006685457		[learning rate: 0.00027392]
	Learning Rate: 0.000273925
	LOSS [training: 0.2997039006685457 | validation: 0.4191130975411781]
	TIME [epoch: 7.62 sec]
EPOCH 546/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29582528070512354		[learning rate: 0.00027194]
	Learning Rate: 0.00027194
	LOSS [training: 0.29582528070512354 | validation: 0.42019162249808567]
	TIME [epoch: 7.63 sec]
EPOCH 547/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29587999267324827		[learning rate: 0.00026997]
	Learning Rate: 0.00026997
	LOSS [training: 0.29587999267324827 | validation: 0.42511603089869954]
	TIME [epoch: 7.66 sec]
EPOCH 548/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29209279642619207		[learning rate: 0.00026801]
	Learning Rate: 0.000268014
	LOSS [training: 0.29209279642619207 | validation: 0.4208462946556495]
	TIME [epoch: 7.62 sec]
EPOCH 549/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30205455227771855		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.30205455227771855 | validation: 0.41559006586701697]
	TIME [epoch: 7.62 sec]
EPOCH 550/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2993172787952397		[learning rate: 0.00026414]
	Learning Rate: 0.000264145
	LOSS [training: 0.2993172787952397 | validation: 0.41438105246914136]
	TIME [epoch: 7.62 sec]
EPOCH 551/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2930325627104259		[learning rate: 0.00026223]
	Learning Rate: 0.000262231
	LOSS [training: 0.2930325627104259 | validation: 0.41339975948208063]
	TIME [epoch: 7.63 sec]
EPOCH 552/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29421394462918704		[learning rate: 0.00026033]
	Learning Rate: 0.000260331
	LOSS [training: 0.29421394462918704 | validation: 0.4229234493907849]
	TIME [epoch: 7.65 sec]
EPOCH 553/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2989833449003448		[learning rate: 0.00025845]
	Learning Rate: 0.000258445
	LOSS [training: 0.2989833449003448 | validation: 0.4161047089194052]
	TIME [epoch: 7.62 sec]
EPOCH 554/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.295101304605416		[learning rate: 0.00025657]
	Learning Rate: 0.000256573
	LOSS [training: 0.295101304605416 | validation: 0.41032572071312934]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_554.pth
	Model improved!!!
EPOCH 555/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2938204480148434		[learning rate: 0.00025471]
	Learning Rate: 0.000254714
	LOSS [training: 0.2938204480148434 | validation: 0.42500378623600454]
	TIME [epoch: 7.62 sec]
EPOCH 556/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2922879112913458		[learning rate: 0.00025287]
	Learning Rate: 0.000252868
	LOSS [training: 0.2922879112913458 | validation: 0.41703163145775823]
	TIME [epoch: 7.66 sec]
EPOCH 557/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2957692069569908		[learning rate: 0.00025104]
	Learning Rate: 0.000251037
	LOSS [training: 0.2957692069569908 | validation: 0.41439032684989974]
	TIME [epoch: 7.63 sec]
EPOCH 558/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2948168950767471		[learning rate: 0.00024922]
	Learning Rate: 0.000249218
	LOSS [training: 0.2948168950767471 | validation: 0.42026655543185487]
	TIME [epoch: 7.62 sec]
EPOCH 559/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29450862328475647		[learning rate: 0.00024741]
	Learning Rate: 0.000247412
	LOSS [training: 0.29450862328475647 | validation: 0.42302681370382483]
	TIME [epoch: 7.62 sec]
EPOCH 560/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29300441413272005		[learning rate: 0.00024562]
	Learning Rate: 0.00024562
	LOSS [training: 0.29300441413272005 | validation: 0.4185739268369054]
	TIME [epoch: 7.62 sec]
EPOCH 561/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2971714246508455		[learning rate: 0.00024384]
	Learning Rate: 0.00024384
	LOSS [training: 0.2971714246508455 | validation: 0.41533610152802775]
	TIME [epoch: 7.66 sec]
EPOCH 562/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29463836559142337		[learning rate: 0.00024207]
	Learning Rate: 0.000242074
	LOSS [training: 0.29463836559142337 | validation: 0.43023643671027056]
	TIME [epoch: 7.63 sec]
EPOCH 563/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29838827705406545		[learning rate: 0.00024032]
	Learning Rate: 0.00024032
	LOSS [training: 0.29838827705406545 | validation: 0.415854757230187]
	TIME [epoch: 7.62 sec]
EPOCH 564/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2925919278799189		[learning rate: 0.00023858]
	Learning Rate: 0.000238579
	LOSS [training: 0.2925919278799189 | validation: 0.41278422427476213]
	TIME [epoch: 7.62 sec]
EPOCH 565/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2897379327961068		[learning rate: 0.00023685]
	Learning Rate: 0.00023685
	LOSS [training: 0.2897379327961068 | validation: 0.40745497508493955]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_565.pth
	Model improved!!!
EPOCH 566/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2916655402730318		[learning rate: 0.00023513]
	Learning Rate: 0.000235134
	LOSS [training: 0.2916655402730318 | validation: 0.41210080180006203]
	TIME [epoch: 7.66 sec]
EPOCH 567/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29071034284469466		[learning rate: 0.00023343]
	Learning Rate: 0.000233431
	LOSS [training: 0.29071034284469466 | validation: 0.4223634055767735]
	TIME [epoch: 7.62 sec]
EPOCH 568/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30149204831083176		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.30149204831083176 | validation: 0.41387126481683856]
	TIME [epoch: 7.62 sec]
EPOCH 569/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2909578331566505		[learning rate: 0.00023006]
	Learning Rate: 0.000230061
	LOSS [training: 0.2909578331566505 | validation: 0.4139040498948799]
	TIME [epoch: 7.62 sec]
EPOCH 570/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29255900337086915		[learning rate: 0.00022839]
	Learning Rate: 0.000228394
	LOSS [training: 0.29255900337086915 | validation: 0.4097875795402687]
	TIME [epoch: 7.62 sec]
EPOCH 571/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2891415347188652		[learning rate: 0.00022674]
	Learning Rate: 0.000226739
	LOSS [training: 0.2891415347188652 | validation: 0.4080825270452082]
	TIME [epoch: 7.67 sec]
EPOCH 572/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29223864465117666		[learning rate: 0.0002251]
	Learning Rate: 0.000225096
	LOSS [training: 0.29223864465117666 | validation: 0.4141601685224447]
	TIME [epoch: 7.62 sec]
EPOCH 573/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2894674533919208		[learning rate: 0.00022347]
	Learning Rate: 0.000223466
	LOSS [training: 0.2894674533919208 | validation: 0.4103660118912573]
	TIME [epoch: 7.61 sec]
EPOCH 574/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29204663897225563		[learning rate: 0.00022185]
	Learning Rate: 0.000221847
	LOSS [training: 0.29204663897225563 | validation: 0.41685835882837297]
	TIME [epoch: 7.61 sec]
EPOCH 575/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2876895832871236		[learning rate: 0.00022024]
	Learning Rate: 0.000220239
	LOSS [training: 0.2876895832871236 | validation: 0.4121890364055686]
	TIME [epoch: 7.62 sec]
EPOCH 576/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28772664100601164		[learning rate: 0.00021864]
	Learning Rate: 0.000218644
	LOSS [training: 0.28772664100601164 | validation: 0.4115762957156944]
	TIME [epoch: 7.67 sec]
EPOCH 577/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29158037416208743		[learning rate: 0.00021706]
	Learning Rate: 0.00021706
	LOSS [training: 0.29158037416208743 | validation: 0.41696230672925955]
	TIME [epoch: 7.62 sec]
EPOCH 578/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2882226059988682		[learning rate: 0.00021549]
	Learning Rate: 0.000215487
	LOSS [training: 0.2882226059988682 | validation: 0.41047839343629017]
	TIME [epoch: 7.62 sec]
EPOCH 579/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28939503056496707		[learning rate: 0.00021393]
	Learning Rate: 0.000213926
	LOSS [training: 0.28939503056496707 | validation: 0.4098536053311974]
	TIME [epoch: 7.62 sec]
EPOCH 580/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.288808752277108		[learning rate: 0.00021238]
	Learning Rate: 0.000212376
	LOSS [training: 0.288808752277108 | validation: 0.40775188379995675]
	TIME [epoch: 7.62 sec]
EPOCH 581/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2878812353978976		[learning rate: 0.00021084]
	Learning Rate: 0.000210837
	LOSS [training: 0.2878812353978976 | validation: 0.4174634421380551]
	TIME [epoch: 7.66 sec]
EPOCH 582/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29087502541877924		[learning rate: 0.00020931]
	Learning Rate: 0.00020931
	LOSS [training: 0.29087502541877924 | validation: 0.412104379171674]
	TIME [epoch: 7.62 sec]
EPOCH 583/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28662818968234316		[learning rate: 0.00020779]
	Learning Rate: 0.000207793
	LOSS [training: 0.28662818968234316 | validation: 0.42349444141202774]
	TIME [epoch: 7.62 sec]
EPOCH 584/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28923112819443203		[learning rate: 0.00020629]
	Learning Rate: 0.000206288
	LOSS [training: 0.28923112819443203 | validation: 0.40837937258106627]
	TIME [epoch: 7.62 sec]
EPOCH 585/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28676132509291913		[learning rate: 0.00020479]
	Learning Rate: 0.000204793
	LOSS [training: 0.28676132509291913 | validation: 0.4152796296901088]
	TIME [epoch: 7.63 sec]
EPOCH 586/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2924546893807637		[learning rate: 0.00020331]
	Learning Rate: 0.00020331
	LOSS [training: 0.2924546893807637 | validation: 0.4148569826917525]
	TIME [epoch: 7.66 sec]
EPOCH 587/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28708060565749804		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 0.28708060565749804 | validation: 0.4154088296411661]
	TIME [epoch: 7.62 sec]
EPOCH 588/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2883177752835966		[learning rate: 0.00020037]
	Learning Rate: 0.000200374
	LOSS [training: 0.2883177752835966 | validation: 0.40824984554260324]
	TIME [epoch: 7.62 sec]
EPOCH 589/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2878500772016835		[learning rate: 0.00019892]
	Learning Rate: 0.000198923
	LOSS [training: 0.2878500772016835 | validation: 0.40916109697642067]
	TIME [epoch: 7.62 sec]
EPOCH 590/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2887416083228048		[learning rate: 0.00019748]
	Learning Rate: 0.000197482
	LOSS [training: 0.2887416083228048 | validation: 0.41245049368673425]
	TIME [epoch: 7.63 sec]
EPOCH 591/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28841966138775643		[learning rate: 0.00019605]
	Learning Rate: 0.000196051
	LOSS [training: 0.28841966138775643 | validation: 0.41150695530489867]
	TIME [epoch: 7.65 sec]
EPOCH 592/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28947666966468205		[learning rate: 0.00019463]
	Learning Rate: 0.00019463
	LOSS [training: 0.28947666966468205 | validation: 0.4134273428749058]
	TIME [epoch: 7.62 sec]
EPOCH 593/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28573135639591507		[learning rate: 0.00019322]
	Learning Rate: 0.00019322
	LOSS [training: 0.28573135639591507 | validation: 0.4096220058107756]
	TIME [epoch: 7.61 sec]
EPOCH 594/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2907722460821448		[learning rate: 0.00019182]
	Learning Rate: 0.00019182
	LOSS [training: 0.2907722460821448 | validation: 0.4110110303906972]
	TIME [epoch: 7.62 sec]
EPOCH 595/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2869214435919567		[learning rate: 0.00019043]
	Learning Rate: 0.000190431
	LOSS [training: 0.2869214435919567 | validation: 0.41024855304836305]
	TIME [epoch: 7.64 sec]
EPOCH 596/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28297845277797723		[learning rate: 0.00018905]
	Learning Rate: 0.000189051
	LOSS [training: 0.28297845277797723 | validation: 0.41050484358678685]
	TIME [epoch: 7.64 sec]
EPOCH 597/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2861908016746739		[learning rate: 0.00018768]
	Learning Rate: 0.000187681
	LOSS [training: 0.2861908016746739 | validation: 0.40879054400082904]
	TIME [epoch: 7.62 sec]
EPOCH 598/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.290013564382075		[learning rate: 0.00018632]
	Learning Rate: 0.000186322
	LOSS [training: 0.290013564382075 | validation: 0.4101908610425347]
	TIME [epoch: 7.62 sec]
EPOCH 599/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28603263450710825		[learning rate: 0.00018497]
	Learning Rate: 0.000184972
	LOSS [training: 0.28603263450710825 | validation: 0.40343393895250346]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_599.pth
	Model improved!!!
EPOCH 600/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2869676535823956		[learning rate: 0.00018363]
	Learning Rate: 0.000183632
	LOSS [training: 0.2869676535823956 | validation: 0.404324889184723]
	TIME [epoch: 7.67 sec]
EPOCH 601/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28569789652958544		[learning rate: 0.0001823]
	Learning Rate: 0.000182301
	LOSS [training: 0.28569789652958544 | validation: 0.405541558972122]
	TIME [epoch: 7.63 sec]
EPOCH 602/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2876389397111629		[learning rate: 0.00018098]
	Learning Rate: 0.00018098
	LOSS [training: 0.2876389397111629 | validation: 0.4188899367807581]
	TIME [epoch: 7.62 sec]
EPOCH 603/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.287202989675173		[learning rate: 0.00017967]
	Learning Rate: 0.000179669
	LOSS [training: 0.287202989675173 | validation: 0.413166725287118]
	TIME [epoch: 7.62 sec]
EPOCH 604/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28351691874684626		[learning rate: 0.00017837]
	Learning Rate: 0.000178368
	LOSS [training: 0.28351691874684626 | validation: 0.4105524201258699]
	TIME [epoch: 7.62 sec]
EPOCH 605/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2820769201525311		[learning rate: 0.00017708]
	Learning Rate: 0.000177075
	LOSS [training: 0.2820769201525311 | validation: 0.4058228875909653]
	TIME [epoch: 7.67 sec]
EPOCH 606/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2855325186817479		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 0.2855325186817479 | validation: 0.4096789547880154]
	TIME [epoch: 7.62 sec]
EPOCH 607/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28826870502723484		[learning rate: 0.00017452]
	Learning Rate: 0.000174519
	LOSS [training: 0.28826870502723484 | validation: 0.40902718800883187]
	TIME [epoch: 7.63 sec]
EPOCH 608/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28538844160875215		[learning rate: 0.00017325]
	Learning Rate: 0.000173254
	LOSS [training: 0.28538844160875215 | validation: 0.4097467828731149]
	TIME [epoch: 7.61 sec]
EPOCH 609/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2853639667008744		[learning rate: 0.000172]
	Learning Rate: 0.000171999
	LOSS [training: 0.2853639667008744 | validation: 0.40491811648993853]
	TIME [epoch: 7.62 sec]
EPOCH 610/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2886673394343462		[learning rate: 0.00017075]
	Learning Rate: 0.000170753
	LOSS [training: 0.2886673394343462 | validation: 0.40405284646519285]
	TIME [epoch: 7.67 sec]
EPOCH 611/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2838831318953998		[learning rate: 0.00016952]
	Learning Rate: 0.000169516
	LOSS [training: 0.2838831318953998 | validation: 0.40309188937793905]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_611.pth
	Model improved!!!
EPOCH 612/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28175221501369047		[learning rate: 0.00016829]
	Learning Rate: 0.000168288
	LOSS [training: 0.28175221501369047 | validation: 0.402313994711901]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_612.pth
	Model improved!!!
EPOCH 613/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28320537633630766		[learning rate: 0.00016707]
	Learning Rate: 0.000167069
	LOSS [training: 0.28320537633630766 | validation: 0.40360177273265296]
	TIME [epoch: 7.63 sec]
EPOCH 614/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2824548717596942		[learning rate: 0.00016586]
	Learning Rate: 0.000165858
	LOSS [training: 0.2824548717596942 | validation: 0.41678014318133927]
	TIME [epoch: 7.64 sec]
EPOCH 615/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2857790201915123		[learning rate: 0.00016466]
	Learning Rate: 0.000164657
	LOSS [training: 0.2857790201915123 | validation: 0.4133913160152374]
	TIME [epoch: 7.67 sec]
EPOCH 616/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2830839565853193		[learning rate: 0.00016346]
	Learning Rate: 0.000163464
	LOSS [training: 0.2830839565853193 | validation: 0.4118926284036704]
	TIME [epoch: 7.62 sec]
EPOCH 617/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2814382845756638		[learning rate: 0.00016228]
	Learning Rate: 0.000162279
	LOSS [training: 0.2814382845756638 | validation: 0.4034504986068076]
	TIME [epoch: 7.62 sec]
EPOCH 618/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28271101283543454		[learning rate: 0.0001611]
	Learning Rate: 0.000161104
	LOSS [training: 0.28271101283543454 | validation: 0.4041326158401115]
	TIME [epoch: 7.62 sec]
EPOCH 619/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28405234261573403		[learning rate: 0.00015994]
	Learning Rate: 0.000159936
	LOSS [training: 0.28405234261573403 | validation: 0.40720940919902293]
	TIME [epoch: 7.64 sec]
EPOCH 620/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28201597557469105		[learning rate: 0.00015878]
	Learning Rate: 0.000158778
	LOSS [training: 0.28201597557469105 | validation: 0.40885732697672944]
	TIME [epoch: 7.66 sec]
EPOCH 621/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28414598264434077		[learning rate: 0.00015763]
	Learning Rate: 0.000157627
	LOSS [training: 0.28414598264434077 | validation: 0.4134644681235672]
	TIME [epoch: 7.62 sec]
EPOCH 622/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2831762146209317		[learning rate: 0.00015649]
	Learning Rate: 0.000156485
	LOSS [training: 0.2831762146209317 | validation: 0.4107673588205493]
	TIME [epoch: 7.62 sec]
EPOCH 623/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2820215994951084		[learning rate: 0.00015535]
	Learning Rate: 0.000155352
	LOSS [training: 0.2820215994951084 | validation: 0.4074714763457558]
	TIME [epoch: 7.62 sec]
EPOCH 624/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2818446219011064		[learning rate: 0.00015423]
	Learning Rate: 0.000154226
	LOSS [training: 0.2818446219011064 | validation: 0.4003687224894147]
	TIME [epoch: 7.65 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_624.pth
	Model improved!!!
EPOCH 625/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2818862393239573		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 0.2818862393239573 | validation: 0.4126019031983852]
	TIME [epoch: 7.65 sec]
EPOCH 626/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28254473810585556		[learning rate: 0.000152]
	Learning Rate: 0.000152
	LOSS [training: 0.28254473810585556 | validation: 0.4026936107265907]
	TIME [epoch: 7.62 sec]
EPOCH 627/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2859004099050195		[learning rate: 0.0001509]
	Learning Rate: 0.000150898
	LOSS [training: 0.2859004099050195 | validation: 0.40552540533481296]
	TIME [epoch: 7.62 sec]
EPOCH 628/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28094203362071407		[learning rate: 0.00014981]
	Learning Rate: 0.000149805
	LOSS [training: 0.28094203362071407 | validation: 0.4044040871110192]
	TIME [epoch: 7.62 sec]
EPOCH 629/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2806575584716401		[learning rate: 0.00014872]
	Learning Rate: 0.00014872
	LOSS [training: 0.2806575584716401 | validation: 0.4021331638767007]
	TIME [epoch: 7.66 sec]
EPOCH 630/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28511286795582647		[learning rate: 0.00014764]
	Learning Rate: 0.000147642
	LOSS [training: 0.28511286795582647 | validation: 0.403421690542242]
	TIME [epoch: 7.63 sec]
EPOCH 631/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28156050733182103		[learning rate: 0.00014657]
	Learning Rate: 0.000146573
	LOSS [training: 0.28156050733182103 | validation: 0.4007372413934858]
	TIME [epoch: 7.62 sec]
EPOCH 632/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28029857995622953		[learning rate: 0.00014551]
	Learning Rate: 0.000145511
	LOSS [training: 0.28029857995622953 | validation: 0.4079107225783694]
	TIME [epoch: 7.62 sec]
EPOCH 633/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.279779103514322		[learning rate: 0.00014446]
	Learning Rate: 0.000144456
	LOSS [training: 0.279779103514322 | validation: 0.40487494447116507]
	TIME [epoch: 7.61 sec]
EPOCH 634/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2801391538370909		[learning rate: 0.00014341]
	Learning Rate: 0.00014341
	LOSS [training: 0.2801391538370909 | validation: 0.398469420911926]
	TIME [epoch: 7.67 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_634.pth
	Model improved!!!
EPOCH 635/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.283760630985682		[learning rate: 0.00014237]
	Learning Rate: 0.000142371
	LOSS [training: 0.283760630985682 | validation: 0.4068569795898047]
	TIME [epoch: 7.62 sec]
EPOCH 636/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2832527975499464		[learning rate: 0.00014134]
	Learning Rate: 0.000141339
	LOSS [training: 0.2832527975499464 | validation: 0.4034954289463642]
	TIME [epoch: 7.62 sec]
EPOCH 637/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2780001071452476		[learning rate: 0.00014032]
	Learning Rate: 0.000140315
	LOSS [training: 0.2780001071452476 | validation: 0.40256581827655347]
	TIME [epoch: 7.62 sec]
EPOCH 638/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27762244193033475		[learning rate: 0.0001393]
	Learning Rate: 0.000139299
	LOSS [training: 0.27762244193033475 | validation: 0.4032616335945541]
	TIME [epoch: 7.62 sec]
EPOCH 639/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2782145032545488		[learning rate: 0.00013829]
	Learning Rate: 0.00013829
	LOSS [training: 0.2782145032545488 | validation: 0.40464413942576516]
	TIME [epoch: 7.67 sec]
EPOCH 640/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2818306348758258		[learning rate: 0.00013729]
	Learning Rate: 0.000137288
	LOSS [training: 0.2818306348758258 | validation: 0.40591490272066094]
	TIME [epoch: 7.62 sec]
EPOCH 641/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.279142075269347		[learning rate: 0.00013629]
	Learning Rate: 0.000136293
	LOSS [training: 0.279142075269347 | validation: 0.40964145455296064]
	TIME [epoch: 7.61 sec]
EPOCH 642/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28088384265202254		[learning rate: 0.00013531]
	Learning Rate: 0.000135306
	LOSS [training: 0.28088384265202254 | validation: 0.4064870319248376]
	TIME [epoch: 7.62 sec]
EPOCH 643/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27846865094641005		[learning rate: 0.00013433]
	Learning Rate: 0.000134325
	LOSS [training: 0.27846865094641005 | validation: 0.40332343538890675]
	TIME [epoch: 7.61 sec]
EPOCH 644/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2803830925588935		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 0.2803830925588935 | validation: 0.3992184783055589]
	TIME [epoch: 7.67 sec]
EPOCH 645/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27893631200821895		[learning rate: 0.00013239]
	Learning Rate: 0.000132386
	LOSS [training: 0.27893631200821895 | validation: 0.3994600400488403]
	TIME [epoch: 7.61 sec]
EPOCH 646/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2800394507689152		[learning rate: 0.00013143]
	Learning Rate: 0.000131427
	LOSS [training: 0.2800394507689152 | validation: 0.407717628049533]
	TIME [epoch: 7.62 sec]
EPOCH 647/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2808286714681615		[learning rate: 0.00013047]
	Learning Rate: 0.000130475
	LOSS [training: 0.2808286714681615 | validation: 0.4035415787144016]
	TIME [epoch: 7.61 sec]
EPOCH 648/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27928255691405673		[learning rate: 0.00012953]
	Learning Rate: 0.000129529
	LOSS [training: 0.27928255691405673 | validation: 0.40269850251069417]
	TIME [epoch: 7.62 sec]
EPOCH 649/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2797553281101778		[learning rate: 0.00012859]
	Learning Rate: 0.000128591
	LOSS [training: 0.2797553281101778 | validation: 0.4081475192165447]
	TIME [epoch: 7.67 sec]
EPOCH 650/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2778601678930531		[learning rate: 0.00012766]
	Learning Rate: 0.000127659
	LOSS [training: 0.2778601678930531 | validation: 0.40411032143259346]
	TIME [epoch: 7.62 sec]
EPOCH 651/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27971607160026374		[learning rate: 0.00012673]
	Learning Rate: 0.000126735
	LOSS [training: 0.27971607160026374 | validation: 0.40260115322213325]
	TIME [epoch: 7.61 sec]
EPOCH 652/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27798569513650156		[learning rate: 0.00012582]
	Learning Rate: 0.000125816
	LOSS [training: 0.27798569513650156 | validation: 0.39989824945658636]
	TIME [epoch: 7.62 sec]
EPOCH 653/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.277604984225723		[learning rate: 0.0001249]
	Learning Rate: 0.000124905
	LOSS [training: 0.277604984225723 | validation: 0.4041801089055148]
	TIME [epoch: 7.63 sec]
EPOCH 654/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27843477931360466		[learning rate: 0.000124]
	Learning Rate: 0.000124
	LOSS [training: 0.27843477931360466 | validation: 0.3998374954230751]
	TIME [epoch: 7.66 sec]
EPOCH 655/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2786178103274349		[learning rate: 0.0001231]
	Learning Rate: 0.000123101
	LOSS [training: 0.2786178103274349 | validation: 0.40571644468532087]
	TIME [epoch: 7.62 sec]
EPOCH 656/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27717128920218204		[learning rate: 0.00012221]
	Learning Rate: 0.00012221
	LOSS [training: 0.27717128920218204 | validation: 0.40339515811607024]
	TIME [epoch: 7.61 sec]
EPOCH 657/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27891296648623887		[learning rate: 0.00012132]
	Learning Rate: 0.000121324
	LOSS [training: 0.27891296648623887 | validation: 0.39528960318703]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_657.pth
	Model improved!!!
EPOCH 658/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2788005926863739		[learning rate: 0.00012045]
	Learning Rate: 0.000120445
	LOSS [training: 0.2788005926863739 | validation: 0.3968160256905516]
	TIME [epoch: 7.64 sec]
EPOCH 659/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27676993101269726		[learning rate: 0.00011957]
	Learning Rate: 0.000119573
	LOSS [training: 0.27676993101269726 | validation: 0.4157707599554353]
	TIME [epoch: 7.65 sec]
EPOCH 660/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27968962912842965		[learning rate: 0.00011871]
	Learning Rate: 0.000118706
	LOSS [training: 0.27968962912842965 | validation: 0.4090508183699978]
	TIME [epoch: 7.61 sec]
EPOCH 661/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2787849898651682		[learning rate: 0.00011785]
	Learning Rate: 0.000117846
	LOSS [training: 0.2787849898651682 | validation: 0.4008242389022412]
	TIME [epoch: 7.62 sec]
EPOCH 662/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27948495045265237		[learning rate: 0.00011699]
	Learning Rate: 0.000116992
	LOSS [training: 0.27948495045265237 | validation: 0.3982327526353372]
	TIME [epoch: 7.61 sec]
EPOCH 663/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27827153225996903		[learning rate: 0.00011614]
	Learning Rate: 0.000116145
	LOSS [training: 0.27827153225996903 | validation: 0.3995932361673014]
	TIME [epoch: 7.64 sec]
EPOCH 664/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28060517823515574		[learning rate: 0.0001153]
	Learning Rate: 0.000115303
	LOSS [training: 0.28060517823515574 | validation: 0.40047838058065066]
	TIME [epoch: 7.63 sec]
EPOCH 665/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27625432523901666		[learning rate: 0.00011447]
	Learning Rate: 0.000114468
	LOSS [training: 0.27625432523901666 | validation: 0.40321120497328405]
	TIME [epoch: 7.62 sec]
EPOCH 666/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2787947387781291		[learning rate: 0.00011364]
	Learning Rate: 0.000113639
	LOSS [training: 0.2787947387781291 | validation: 0.4040469782011351]
	TIME [epoch: 7.61 sec]
EPOCH 667/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2771772603536781		[learning rate: 0.00011282]
	Learning Rate: 0.000112815
	LOSS [training: 0.2771772603536781 | validation: 0.4026556931791417]
	TIME [epoch: 7.62 sec]
EPOCH 668/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27856065034353283		[learning rate: 0.000112]
	Learning Rate: 0.000111998
	LOSS [training: 0.27856065034353283 | validation: 0.39452628253341343]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_668.pth
	Model improved!!!
EPOCH 669/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27618369328242454		[learning rate: 0.00011119]
	Learning Rate: 0.000111187
	LOSS [training: 0.27618369328242454 | validation: 0.4055771781496247]
	TIME [epoch: 7.63 sec]
EPOCH 670/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.276379441203894		[learning rate: 0.00011038]
	Learning Rate: 0.000110381
	LOSS [training: 0.276379441203894 | validation: 0.4129622507599064]
	TIME [epoch: 7.61 sec]
EPOCH 671/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2828499134887302		[learning rate: 0.00010958]
	Learning Rate: 0.000109581
	LOSS [training: 0.2828499134887302 | validation: 0.40436107177088887]
	TIME [epoch: 7.61 sec]
EPOCH 672/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2748001349918645		[learning rate: 0.00010879]
	Learning Rate: 0.000108787
	LOSS [training: 0.2748001349918645 | validation: 0.3994537778318165]
	TIME [epoch: 7.61 sec]
EPOCH 673/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27583746075014476		[learning rate: 0.000108]
	Learning Rate: 0.000107999
	LOSS [training: 0.27583746075014476 | validation: 0.40099435971789005]
	TIME [epoch: 7.66 sec]
EPOCH 674/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27347424046928104		[learning rate: 0.00010722]
	Learning Rate: 0.000107217
	LOSS [training: 0.27347424046928104 | validation: 0.40481217252111656]
	TIME [epoch: 7.62 sec]
EPOCH 675/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2752217551233081		[learning rate: 0.00010644]
	Learning Rate: 0.00010644
	LOSS [training: 0.2752217551233081 | validation: 0.4032600869962685]
	TIME [epoch: 7.6 sec]
EPOCH 676/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2741890510128163		[learning rate: 0.00010567]
	Learning Rate: 0.000105669
	LOSS [training: 0.2741890510128163 | validation: 0.400194825923013]
	TIME [epoch: 7.61 sec]
EPOCH 677/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27791183858339685		[learning rate: 0.0001049]
	Learning Rate: 0.000104903
	LOSS [training: 0.27791183858339685 | validation: 0.4011724072829064]
	TIME [epoch: 7.61 sec]
EPOCH 678/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2773280103964053		[learning rate: 0.00010414]
	Learning Rate: 0.000104143
	LOSS [training: 0.2773280103964053 | validation: 0.4018751036549276]
	TIME [epoch: 7.66 sec]
EPOCH 679/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27731268248646995		[learning rate: 0.00010339]
	Learning Rate: 0.000103389
	LOSS [training: 0.27731268248646995 | validation: 0.39908977596347633]
	TIME [epoch: 7.61 sec]
EPOCH 680/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2769922441706004		[learning rate: 0.00010264]
	Learning Rate: 0.00010264
	LOSS [training: 0.2769922441706004 | validation: 0.3914245968910961]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_680.pth
	Model improved!!!
EPOCH 681/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27635485466046994		[learning rate: 0.0001019]
	Learning Rate: 0.000101896
	LOSS [training: 0.27635485466046994 | validation: 0.4015934619456952]
	TIME [epoch: 7.62 sec]
EPOCH 682/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2734184273381559		[learning rate: 0.00010116]
	Learning Rate: 0.000101158
	LOSS [training: 0.2734184273381559 | validation: 0.4026471623817314]
	TIME [epoch: 7.61 sec]
EPOCH 683/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.277160393500528		[learning rate: 0.00010043]
	Learning Rate: 0.000100425
	LOSS [training: 0.277160393500528 | validation: 0.39727001476281365]
	TIME [epoch: 7.66 sec]
EPOCH 684/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.276557341573381		[learning rate: 9.9697e-05]
	Learning Rate: 9.96975e-05
	LOSS [training: 0.276557341573381 | validation: 0.39767312781178915]
	TIME [epoch: 7.61 sec]
EPOCH 685/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27464740678320876		[learning rate: 9.8975e-05]
	Learning Rate: 9.89752e-05
	LOSS [training: 0.27464740678320876 | validation: 0.39677114130088176]
	TIME [epoch: 7.61 sec]
EPOCH 686/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.275436469766299		[learning rate: 9.8258e-05]
	Learning Rate: 9.82581e-05
	LOSS [training: 0.275436469766299 | validation: 0.40297770041550024]
	TIME [epoch: 7.6 sec]
EPOCH 687/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27401649991349775		[learning rate: 9.7546e-05]
	Learning Rate: 9.75463e-05
	LOSS [training: 0.27401649991349775 | validation: 0.39802113849566434]
	TIME [epoch: 7.61 sec]
EPOCH 688/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27509892233138916		[learning rate: 9.684e-05]
	Learning Rate: 9.68396e-05
	LOSS [training: 0.27509892233138916 | validation: 0.3969455026708881]
	TIME [epoch: 7.66 sec]
EPOCH 689/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2750003328956454		[learning rate: 9.6138e-05]
	Learning Rate: 9.61379e-05
	LOSS [training: 0.2750003328956454 | validation: 0.3958035475231638]
	TIME [epoch: 7.61 sec]
EPOCH 690/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2765891819476673		[learning rate: 9.5441e-05]
	Learning Rate: 9.54414e-05
	LOSS [training: 0.2765891819476673 | validation: 0.39566208267298536]
	TIME [epoch: 7.61 sec]
EPOCH 691/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2779430518334798		[learning rate: 9.475e-05]
	Learning Rate: 9.475e-05
	LOSS [training: 0.2779430518334798 | validation: 0.3966760041186955]
	TIME [epoch: 7.61 sec]
EPOCH 692/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27411408551198146		[learning rate: 9.4064e-05]
	Learning Rate: 9.40635e-05
	LOSS [training: 0.27411408551198146 | validation: 0.39808447752567716]
	TIME [epoch: 7.63 sec]
EPOCH 693/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2767051655249976		[learning rate: 9.3382e-05]
	Learning Rate: 9.3382e-05
	LOSS [training: 0.2767051655249976 | validation: 0.40127104369139727]
	TIME [epoch: 7.66 sec]
EPOCH 694/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27527826397458366		[learning rate: 9.2705e-05]
	Learning Rate: 9.27055e-05
	LOSS [training: 0.27527826397458366 | validation: 0.39789494466415537]
	TIME [epoch: 7.61 sec]
EPOCH 695/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27298650034844235		[learning rate: 9.2034e-05]
	Learning Rate: 9.20338e-05
	LOSS [training: 0.27298650034844235 | validation: 0.39715864251743527]
	TIME [epoch: 7.61 sec]
EPOCH 696/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27455809827249383		[learning rate: 9.1367e-05]
	Learning Rate: 9.13671e-05
	LOSS [training: 0.27455809827249383 | validation: 0.39555410656428835]
	TIME [epoch: 7.6 sec]
EPOCH 697/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2749287202036297		[learning rate: 9.0705e-05]
	Learning Rate: 9.07051e-05
	LOSS [training: 0.2749287202036297 | validation: 0.39835095311520197]
	TIME [epoch: 7.63 sec]
EPOCH 698/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2741284600164084		[learning rate: 9.0048e-05]
	Learning Rate: 9.00479e-05
	LOSS [training: 0.2741284600164084 | validation: 0.40058831106073267]
	TIME [epoch: 7.65 sec]
EPOCH 699/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27201611808890586		[learning rate: 8.9396e-05]
	Learning Rate: 8.93955e-05
	LOSS [training: 0.27201611808890586 | validation: 0.3931791499251854]
	TIME [epoch: 7.61 sec]
EPOCH 700/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2746666019948692		[learning rate: 8.8748e-05]
	Learning Rate: 8.87479e-05
	LOSS [training: 0.2746666019948692 | validation: 0.39700985837352715]
	TIME [epoch: 7.6 sec]
EPOCH 701/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.274848409472913		[learning rate: 8.8105e-05]
	Learning Rate: 8.81049e-05
	LOSS [training: 0.274848409472913 | validation: 0.39990483024626355]
	TIME [epoch: 7.61 sec]
EPOCH 702/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27439332957010076		[learning rate: 8.7467e-05]
	Learning Rate: 8.74666e-05
	LOSS [training: 0.27439332957010076 | validation: 0.40620963722852965]
	TIME [epoch: 7.64 sec]
EPOCH 703/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2726137973563391		[learning rate: 8.6833e-05]
	Learning Rate: 8.68329e-05
	LOSS [training: 0.2726137973563391 | validation: 0.39795717932869923]
	TIME [epoch: 7.64 sec]
EPOCH 704/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2742113184802298		[learning rate: 8.6204e-05]
	Learning Rate: 8.62038e-05
	LOSS [training: 0.2742113184802298 | validation: 0.3979834257014401]
	TIME [epoch: 7.62 sec]
EPOCH 705/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27287352097015494		[learning rate: 8.5579e-05]
	Learning Rate: 8.55793e-05
	LOSS [training: 0.27287352097015494 | validation: 0.39977708247271015]
	TIME [epoch: 7.61 sec]
EPOCH 706/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2726825922757108		[learning rate: 8.4959e-05]
	Learning Rate: 8.49592e-05
	LOSS [training: 0.2726825922757108 | validation: 0.3955976836117111]
	TIME [epoch: 7.61 sec]
EPOCH 707/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2714688151955196		[learning rate: 8.4344e-05]
	Learning Rate: 8.43437e-05
	LOSS [training: 0.2714688151955196 | validation: 0.4019841405194696]
	TIME [epoch: 7.65 sec]
EPOCH 708/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27381954232783345		[learning rate: 8.3733e-05]
	Learning Rate: 8.37327e-05
	LOSS [training: 0.27381954232783345 | validation: 0.3961004347678662]
	TIME [epoch: 7.61 sec]
EPOCH 709/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27362951241217803		[learning rate: 8.3126e-05]
	Learning Rate: 8.3126e-05
	LOSS [training: 0.27362951241217803 | validation: 0.397847009944757]
	TIME [epoch: 7.62 sec]
EPOCH 710/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2757947744723017		[learning rate: 8.2524e-05]
	Learning Rate: 8.25238e-05
	LOSS [training: 0.2757947744723017 | validation: 0.3951049638462304]
	TIME [epoch: 7.61 sec]
EPOCH 711/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27124921386980105		[learning rate: 8.1926e-05]
	Learning Rate: 8.19259e-05
	LOSS [training: 0.27124921386980105 | validation: 0.3947388670305694]
	TIME [epoch: 7.6 sec]
EPOCH 712/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2732443341348869		[learning rate: 8.1332e-05]
	Learning Rate: 8.13323e-05
	LOSS [training: 0.2732443341348869 | validation: 0.39670304860856653]
	TIME [epoch: 7.66 sec]
EPOCH 713/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2754257221844069		[learning rate: 8.0743e-05]
	Learning Rate: 8.07431e-05
	LOSS [training: 0.2754257221844069 | validation: 0.3959566007704102]
	TIME [epoch: 7.61 sec]
EPOCH 714/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2753565439921541		[learning rate: 8.0158e-05]
	Learning Rate: 8.01581e-05
	LOSS [training: 0.2753565439921541 | validation: 0.3956747772380199]
	TIME [epoch: 7.61 sec]
EPOCH 715/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2730487009545289		[learning rate: 7.9577e-05]
	Learning Rate: 7.95774e-05
	LOSS [training: 0.2730487009545289 | validation: 0.3983507517166335]
	TIME [epoch: 7.6 sec]
EPOCH 716/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27270398513613525		[learning rate: 7.9001e-05]
	Learning Rate: 7.90008e-05
	LOSS [training: 0.27270398513613525 | validation: 0.39706363010604123]
	TIME [epoch: 7.6 sec]
EPOCH 717/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27411221766133087		[learning rate: 7.8428e-05]
	Learning Rate: 7.84285e-05
	LOSS [training: 0.27411221766133087 | validation: 0.39840192473126435]
	TIME [epoch: 7.65 sec]
EPOCH 718/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27313854430404416		[learning rate: 7.786e-05]
	Learning Rate: 7.78603e-05
	LOSS [training: 0.27313854430404416 | validation: 0.4030264396616166]
	TIME [epoch: 7.62 sec]
EPOCH 719/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27136327167314506		[learning rate: 7.7296e-05]
	Learning Rate: 7.72962e-05
	LOSS [training: 0.27136327167314506 | validation: 0.3996788484117769]
	TIME [epoch: 7.61 sec]
EPOCH 720/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2730135744699142		[learning rate: 7.6736e-05]
	Learning Rate: 7.67362e-05
	LOSS [training: 0.2730135744699142 | validation: 0.40132716052930684]
	TIME [epoch: 7.61 sec]
EPOCH 721/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2708225218777418		[learning rate: 7.618e-05]
	Learning Rate: 7.61802e-05
	LOSS [training: 0.2708225218777418 | validation: 0.39793350925477494]
	TIME [epoch: 7.61 sec]
EPOCH 722/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27402528744934107		[learning rate: 7.5628e-05]
	Learning Rate: 7.56283e-05
	LOSS [training: 0.27402528744934107 | validation: 0.3911288402402637]
	TIME [epoch: 7.66 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_722.pth
	Model improved!!!
EPOCH 723/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2710771607998922		[learning rate: 7.508e-05]
	Learning Rate: 7.50804e-05
	LOSS [training: 0.2710771607998922 | validation: 0.3931029429701361]
	TIME [epoch: 7.62 sec]
EPOCH 724/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2729208231064249		[learning rate: 7.4536e-05]
	Learning Rate: 7.45364e-05
	LOSS [training: 0.2729208231064249 | validation: 0.3964970778155974]
	TIME [epoch: 7.63 sec]
EPOCH 725/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27305186568938283		[learning rate: 7.3996e-05]
	Learning Rate: 7.39964e-05
	LOSS [training: 0.27305186568938283 | validation: 0.40366275959372233]
	TIME [epoch: 7.62 sec]
EPOCH 726/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27203618203660973		[learning rate: 7.346e-05]
	Learning Rate: 7.34603e-05
	LOSS [training: 0.27203618203660973 | validation: 0.4011604900103426]
	TIME [epoch: 7.62 sec]
EPOCH 727/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27180593406414244		[learning rate: 7.2928e-05]
	Learning Rate: 7.29281e-05
	LOSS [training: 0.27180593406414244 | validation: 0.4021020643287203]
	TIME [epoch: 7.68 sec]
EPOCH 728/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27466362045156484		[learning rate: 7.24e-05]
	Learning Rate: 7.23997e-05
	LOSS [training: 0.27466362045156484 | validation: 0.39614842063007555]
	TIME [epoch: 7.63 sec]
EPOCH 729/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27267338885192677		[learning rate: 7.1875e-05]
	Learning Rate: 7.18752e-05
	LOSS [training: 0.27267338885192677 | validation: 0.395177555278806]
	TIME [epoch: 7.62 sec]
EPOCH 730/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27468555731980376		[learning rate: 7.1354e-05]
	Learning Rate: 7.13545e-05
	LOSS [training: 0.27468555731980376 | validation: 0.3954872551646488]
	TIME [epoch: 7.63 sec]
EPOCH 731/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2719384670622911		[learning rate: 7.0838e-05]
	Learning Rate: 7.08375e-05
	LOSS [training: 0.2719384670622911 | validation: 0.39639048700128277]
	TIME [epoch: 7.63 sec]
EPOCH 732/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2726816180620665		[learning rate: 7.0324e-05]
	Learning Rate: 7.03243e-05
	LOSS [training: 0.2726816180620665 | validation: 0.3965379245129397]
	TIME [epoch: 7.68 sec]
EPOCH 733/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2717083912900981		[learning rate: 6.9815e-05]
	Learning Rate: 6.98148e-05
	LOSS [training: 0.2717083912900981 | validation: 0.39281135089141006]
	TIME [epoch: 7.62 sec]
EPOCH 734/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27358288864484714		[learning rate: 6.9309e-05]
	Learning Rate: 6.9309e-05
	LOSS [training: 0.27358288864484714 | validation: 0.3979750668498544]
	TIME [epoch: 7.63 sec]
EPOCH 735/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27201465926791424		[learning rate: 6.8807e-05]
	Learning Rate: 6.88069e-05
	LOSS [training: 0.27201465926791424 | validation: 0.38968946732177523]
	TIME [epoch: 7.63 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_735.pth
	Model improved!!!
EPOCH 736/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27102542290658693		[learning rate: 6.8308e-05]
	Learning Rate: 6.83084e-05
	LOSS [training: 0.27102542290658693 | validation: 0.39468654108086515]
	TIME [epoch: 7.64 sec]
EPOCH 737/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2711999988182591		[learning rate: 6.7813e-05]
	Learning Rate: 6.78134e-05
	LOSS [training: 0.2711999988182591 | validation: 0.40562851782536247]
	TIME [epoch: 7.66 sec]
EPOCH 738/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27355063924926537		[learning rate: 6.7322e-05]
	Learning Rate: 6.73221e-05
	LOSS [training: 0.27355063924926537 | validation: 0.398240272518362]
	TIME [epoch: 7.62 sec]
EPOCH 739/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2713370624472944		[learning rate: 6.6834e-05]
	Learning Rate: 6.68344e-05
	LOSS [training: 0.2713370624472944 | validation: 0.3902708948676078]
	TIME [epoch: 7.62 sec]
EPOCH 740/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2713164446481023		[learning rate: 6.635e-05]
	Learning Rate: 6.63502e-05
	LOSS [training: 0.2713164446481023 | validation: 0.39561958580339335]
	TIME [epoch: 7.62 sec]
EPOCH 741/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2718848876668276		[learning rate: 6.587e-05]
	Learning Rate: 6.58695e-05
	LOSS [training: 0.2718848876668276 | validation: 0.39338090980806395]
	TIME [epoch: 7.65 sec]
EPOCH 742/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27115964489475874		[learning rate: 6.5392e-05]
	Learning Rate: 6.53923e-05
	LOSS [training: 0.27115964489475874 | validation: 0.3941287401639979]
	TIME [epoch: 7.65 sec]
EPOCH 743/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2709636101168387		[learning rate: 6.4919e-05]
	Learning Rate: 6.49185e-05
	LOSS [training: 0.2709636101168387 | validation: 0.40084929765551425]
	TIME [epoch: 7.62 sec]
EPOCH 744/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27044220374439		[learning rate: 6.4448e-05]
	Learning Rate: 6.44482e-05
	LOSS [training: 0.27044220374439 | validation: 0.39395539571898]
	TIME [epoch: 7.62 sec]
EPOCH 745/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2715224739588339		[learning rate: 6.3981e-05]
	Learning Rate: 6.39813e-05
	LOSS [training: 0.2715224739588339 | validation: 0.39232083875947954]
	TIME [epoch: 7.62 sec]
EPOCH 746/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27028099642327663		[learning rate: 6.3518e-05]
	Learning Rate: 6.35177e-05
	LOSS [training: 0.27028099642327663 | validation: 0.3997991944170341]
	TIME [epoch: 7.65 sec]
EPOCH 747/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2712729836338863		[learning rate: 6.3058e-05]
	Learning Rate: 6.30575e-05
	LOSS [training: 0.2712729836338863 | validation: 0.39329058592963934]
	TIME [epoch: 7.64 sec]
EPOCH 748/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27062313938341376		[learning rate: 6.2601e-05]
	Learning Rate: 6.26007e-05
	LOSS [training: 0.27062313938341376 | validation: 0.3981351521282648]
	TIME [epoch: 7.62 sec]
EPOCH 749/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27098626159698813		[learning rate: 6.2147e-05]
	Learning Rate: 6.21471e-05
	LOSS [training: 0.27098626159698813 | validation: 0.39463758565007623]
	TIME [epoch: 7.62 sec]
EPOCH 750/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2688552392773124		[learning rate: 6.1697e-05]
	Learning Rate: 6.16969e-05
	LOSS [training: 0.2688552392773124 | validation: 0.394197628412877]
	TIME [epoch: 7.62 sec]
EPOCH 751/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2691534619204377		[learning rate: 6.125e-05]
	Learning Rate: 6.12499e-05
	LOSS [training: 0.2691534619204377 | validation: 0.39254940865453375]
	TIME [epoch: 7.67 sec]
EPOCH 752/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26980039745869566		[learning rate: 6.0806e-05]
	Learning Rate: 6.08061e-05
	LOSS [training: 0.26980039745869566 | validation: 0.39622322373789054]
	TIME [epoch: 7.63 sec]
EPOCH 753/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27098105047896914		[learning rate: 6.0366e-05]
	Learning Rate: 6.03656e-05
	LOSS [training: 0.27098105047896914 | validation: 0.39454182722097947]
	TIME [epoch: 7.62 sec]
EPOCH 754/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27002577056552945		[learning rate: 5.9928e-05]
	Learning Rate: 5.99283e-05
	LOSS [training: 0.27002577056552945 | validation: 0.39259134179003696]
	TIME [epoch: 7.62 sec]
EPOCH 755/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2705126109123076		[learning rate: 5.9494e-05]
	Learning Rate: 5.94941e-05
	LOSS [training: 0.2705126109123076 | validation: 0.39511391721814937]
	TIME [epoch: 7.62 sec]
EPOCH 756/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26855628681536875		[learning rate: 5.9063e-05]
	Learning Rate: 5.9063e-05
	LOSS [training: 0.26855628681536875 | validation: 0.3926543849211443]
	TIME [epoch: 7.67 sec]
EPOCH 757/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.271952921697017		[learning rate: 5.8635e-05]
	Learning Rate: 5.86351e-05
	LOSS [training: 0.271952921697017 | validation: 0.39325345636846715]
	TIME [epoch: 7.63 sec]
EPOCH 758/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2689975578969609		[learning rate: 5.821e-05]
	Learning Rate: 5.82103e-05
	LOSS [training: 0.2689975578969609 | validation: 0.39594243655093564]
	TIME [epoch: 7.62 sec]
EPOCH 759/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26945125076082044		[learning rate: 5.7789e-05]
	Learning Rate: 5.77886e-05
	LOSS [training: 0.26945125076082044 | validation: 0.38978218577511015]
	TIME [epoch: 7.62 sec]
EPOCH 760/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27092547520254234		[learning rate: 5.737e-05]
	Learning Rate: 5.73699e-05
	LOSS [training: 0.27092547520254234 | validation: 0.3933489773197444]
	TIME [epoch: 7.62 sec]
EPOCH 761/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2704184692101438		[learning rate: 5.6954e-05]
	Learning Rate: 5.69543e-05
	LOSS [training: 0.2704184692101438 | validation: 0.39842820343096164]
	TIME [epoch: 7.67 sec]
EPOCH 762/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2701396630982209		[learning rate: 5.6542e-05]
	Learning Rate: 5.65417e-05
	LOSS [training: 0.2701396630982209 | validation: 0.39427484070087027]
	TIME [epoch: 7.62 sec]
EPOCH 763/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2693976165621244		[learning rate: 5.6132e-05]
	Learning Rate: 5.6132e-05
	LOSS [training: 0.2693976165621244 | validation: 0.3926712983127377]
	TIME [epoch: 7.62 sec]
EPOCH 764/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2695435171937604		[learning rate: 5.5725e-05]
	Learning Rate: 5.57253e-05
	LOSS [training: 0.2695435171937604 | validation: 0.39483991135751506]
	TIME [epoch: 7.62 sec]
EPOCH 765/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26851134673646876		[learning rate: 5.5322e-05]
	Learning Rate: 5.53216e-05
	LOSS [training: 0.26851134673646876 | validation: 0.3921136362925625]
	TIME [epoch: 7.62 sec]
EPOCH 766/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27008047207448405		[learning rate: 5.4921e-05]
	Learning Rate: 5.49208e-05
	LOSS [training: 0.27008047207448405 | validation: 0.39244357966853705]
	TIME [epoch: 7.67 sec]
EPOCH 767/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27075297276994115		[learning rate: 5.4523e-05]
	Learning Rate: 5.45229e-05
	LOSS [training: 0.27075297276994115 | validation: 0.3983546942463305]
	TIME [epoch: 7.62 sec]
EPOCH 768/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2681325460577211		[learning rate: 5.4128e-05]
	Learning Rate: 5.41279e-05
	LOSS [training: 0.2681325460577211 | validation: 0.3907121880670022]
	TIME [epoch: 7.62 sec]
EPOCH 769/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2691251327466236		[learning rate: 5.3736e-05]
	Learning Rate: 5.37357e-05
	LOSS [training: 0.2691251327466236 | validation: 0.39189465209159147]
	TIME [epoch: 7.62 sec]
EPOCH 770/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26784495184610374		[learning rate: 5.3346e-05]
	Learning Rate: 5.33464e-05
	LOSS [training: 0.26784495184610374 | validation: 0.39539513532014786]
	TIME [epoch: 7.62 sec]
EPOCH 771/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26926869704255235		[learning rate: 5.296e-05]
	Learning Rate: 5.29599e-05
	LOSS [training: 0.26926869704255235 | validation: 0.394834950107788]
	TIME [epoch: 7.68 sec]
EPOCH 772/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2709192714469961		[learning rate: 5.2576e-05]
	Learning Rate: 5.25762e-05
	LOSS [training: 0.2709192714469961 | validation: 0.3946287470328675]
	TIME [epoch: 7.62 sec]
EPOCH 773/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2708245444316422		[learning rate: 5.2195e-05]
	Learning Rate: 5.21953e-05
	LOSS [training: 0.2708245444316422 | validation: 0.3927976319304106]
	TIME [epoch: 7.61 sec]
EPOCH 774/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2696446572590453		[learning rate: 5.1817e-05]
	Learning Rate: 5.18172e-05
	LOSS [training: 0.2696446572590453 | validation: 0.3944341520808474]
	TIME [epoch: 7.62 sec]
EPOCH 775/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2680582428024418		[learning rate: 5.1442e-05]
	Learning Rate: 5.14418e-05
	LOSS [training: 0.2680582428024418 | validation: 0.39284275321202594]
	TIME [epoch: 7.63 sec]
EPOCH 776/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26885661966873864		[learning rate: 5.1069e-05]
	Learning Rate: 5.10691e-05
	LOSS [training: 0.26885661966873864 | validation: 0.3905358040964907]
	TIME [epoch: 7.66 sec]
EPOCH 777/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27068693015045475		[learning rate: 5.0699e-05]
	Learning Rate: 5.06991e-05
	LOSS [training: 0.27068693015045475 | validation: 0.3909595668588318]
	TIME [epoch: 7.62 sec]
EPOCH 778/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2686214440469521		[learning rate: 5.0332e-05]
	Learning Rate: 5.03318e-05
	LOSS [training: 0.2686214440469521 | validation: 0.39305105543739427]
	TIME [epoch: 7.62 sec]
EPOCH 779/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2690216595337869		[learning rate: 4.9967e-05]
	Learning Rate: 4.99671e-05
	LOSS [training: 0.2690216595337869 | validation: 0.38824098618592867]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_779.pth
	Model improved!!!
EPOCH 780/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2676341912611827		[learning rate: 4.9605e-05]
	Learning Rate: 4.96051e-05
	LOSS [training: 0.2676341912611827 | validation: 0.3969407235554857]
	TIME [epoch: 7.63 sec]
EPOCH 781/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2691141110851629		[learning rate: 4.9246e-05]
	Learning Rate: 4.92457e-05
	LOSS [training: 0.2691141110851629 | validation: 0.3901287825037123]
	TIME [epoch: 7.65 sec]
EPOCH 782/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2692578097596069		[learning rate: 4.8889e-05]
	Learning Rate: 4.88889e-05
	LOSS [training: 0.2692578097596069 | validation: 0.3942616413086765]
	TIME [epoch: 7.61 sec]
EPOCH 783/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.267991931923698		[learning rate: 4.8535e-05]
	Learning Rate: 4.85347e-05
	LOSS [training: 0.267991931923698 | validation: 0.3885247729534884]
	TIME [epoch: 7.61 sec]
EPOCH 784/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2684036608448049		[learning rate: 4.8183e-05]
	Learning Rate: 4.81831e-05
	LOSS [training: 0.2684036608448049 | validation: 0.3965885862215957]
	TIME [epoch: 7.62 sec]
EPOCH 785/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.267125833836265		[learning rate: 4.7834e-05]
	Learning Rate: 4.7834e-05
	LOSS [training: 0.267125833836265 | validation: 0.3952199418109289]
	TIME [epoch: 7.64 sec]
EPOCH 786/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26739514724458946		[learning rate: 4.7487e-05]
	Learning Rate: 4.74875e-05
	LOSS [training: 0.26739514724458946 | validation: 0.3948761491736458]
	TIME [epoch: 7.65 sec]
EPOCH 787/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2687877354934969		[learning rate: 4.7143e-05]
	Learning Rate: 4.71434e-05
	LOSS [training: 0.2687877354934969 | validation: 0.3951746744403878]
	TIME [epoch: 7.61 sec]
EPOCH 788/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26924679049286004		[learning rate: 4.6802e-05]
	Learning Rate: 4.68019e-05
	LOSS [training: 0.26924679049286004 | validation: 0.3998163825371641]
	TIME [epoch: 7.62 sec]
EPOCH 789/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26768191397456415		[learning rate: 4.6463e-05]
	Learning Rate: 4.64628e-05
	LOSS [training: 0.26768191397456415 | validation: 0.39350742190223975]
	TIME [epoch: 7.61 sec]
EPOCH 790/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2669436252531208		[learning rate: 4.6126e-05]
	Learning Rate: 4.61262e-05
	LOSS [training: 0.2669436252531208 | validation: 0.39182989084491315]
	TIME [epoch: 7.65 sec]
EPOCH 791/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26909289801631253		[learning rate: 4.5792e-05]
	Learning Rate: 4.5792e-05
	LOSS [training: 0.26909289801631253 | validation: 0.39055680422085165]
	TIME [epoch: 7.63 sec]
EPOCH 792/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26622235323241017		[learning rate: 4.546e-05]
	Learning Rate: 4.54602e-05
	LOSS [training: 0.26622235323241017 | validation: 0.3925287145057006]
	TIME [epoch: 7.61 sec]
EPOCH 793/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2677772359365157		[learning rate: 4.5131e-05]
	Learning Rate: 4.51309e-05
	LOSS [training: 0.2677772359365157 | validation: 0.3945932694440712]
	TIME [epoch: 7.62 sec]
EPOCH 794/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2677073241523694		[learning rate: 4.4804e-05]
	Learning Rate: 4.48039e-05
	LOSS [training: 0.2677073241523694 | validation: 0.38725161557627846]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_794.pth
	Model improved!!!
EPOCH 795/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2680139960727182		[learning rate: 4.4479e-05]
	Learning Rate: 4.44793e-05
	LOSS [training: 0.2680139960727182 | validation: 0.3935577542081663]
	TIME [epoch: 7.66 sec]
EPOCH 796/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26767953136656597		[learning rate: 4.4157e-05]
	Learning Rate: 4.41571e-05
	LOSS [training: 0.26767953136656597 | validation: 0.39114527739690147]
	TIME [epoch: 7.62 sec]
EPOCH 797/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2679014009689041		[learning rate: 4.3837e-05]
	Learning Rate: 4.38371e-05
	LOSS [training: 0.2679014009689041 | validation: 0.38892110967543164]
	TIME [epoch: 7.62 sec]
EPOCH 798/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26829904620142553		[learning rate: 4.352e-05]
	Learning Rate: 4.35195e-05
	LOSS [training: 0.26829904620142553 | validation: 0.3882112331393599]
	TIME [epoch: 7.62 sec]
EPOCH 799/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2677027311895682		[learning rate: 4.3204e-05]
	Learning Rate: 4.32042e-05
	LOSS [training: 0.2677027311895682 | validation: 0.39277534555381416]
	TIME [epoch: 7.61 sec]
EPOCH 800/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2678892705155429		[learning rate: 4.2891e-05]
	Learning Rate: 4.28912e-05
	LOSS [training: 0.2678892705155429 | validation: 0.39575525769798914]
	TIME [epoch: 7.66 sec]
EPOCH 801/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2678967064862545		[learning rate: 4.258e-05]
	Learning Rate: 4.25805e-05
	LOSS [training: 0.2678967064862545 | validation: 0.39734733528439864]
	TIME [epoch: 7.62 sec]
EPOCH 802/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26795007809807836		[learning rate: 4.2272e-05]
	Learning Rate: 4.2272e-05
	LOSS [training: 0.26795007809807836 | validation: 0.3886491023079755]
	TIME [epoch: 7.61 sec]
EPOCH 803/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2680579492815032		[learning rate: 4.1966e-05]
	Learning Rate: 4.19657e-05
	LOSS [training: 0.2680579492815032 | validation: 0.39135865119448615]
	TIME [epoch: 7.61 sec]
EPOCH 804/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.268545324361211		[learning rate: 4.1662e-05]
	Learning Rate: 4.16617e-05
	LOSS [training: 0.268545324361211 | validation: 0.39416571715520443]
	TIME [epoch: 7.61 sec]
EPOCH 805/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2674364176100722		[learning rate: 4.136e-05]
	Learning Rate: 4.13599e-05
	LOSS [training: 0.2674364176100722 | validation: 0.39014053531810355]
	TIME [epoch: 7.67 sec]
EPOCH 806/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2683927095222532		[learning rate: 4.106e-05]
	Learning Rate: 4.10602e-05
	LOSS [training: 0.2683927095222532 | validation: 0.39267063611387487]
	TIME [epoch: 7.61 sec]
EPOCH 807/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2673594740735762		[learning rate: 4.0763e-05]
	Learning Rate: 4.07627e-05
	LOSS [training: 0.2673594740735762 | validation: 0.39491379778625574]
	TIME [epoch: 7.61 sec]
EPOCH 808/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26774996869219675		[learning rate: 4.0467e-05]
	Learning Rate: 4.04674e-05
	LOSS [training: 0.26774996869219675 | validation: 0.39050822528049023]
	TIME [epoch: 7.61 sec]
EPOCH 809/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2683678331967604		[learning rate: 4.0174e-05]
	Learning Rate: 4.01742e-05
	LOSS [training: 0.2683678331967604 | validation: 0.39199612329706385]
	TIME [epoch: 7.62 sec]
EPOCH 810/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2684082946569411		[learning rate: 3.9883e-05]
	Learning Rate: 3.98832e-05
	LOSS [training: 0.2684082946569411 | validation: 0.39225961541783694]
	TIME [epoch: 7.67 sec]
EPOCH 811/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2655028664988245		[learning rate: 3.9594e-05]
	Learning Rate: 3.95942e-05
	LOSS [training: 0.2655028664988245 | validation: 0.3888803795275697]
	TIME [epoch: 7.63 sec]
EPOCH 812/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.267587564665119		[learning rate: 3.9307e-05]
	Learning Rate: 3.93074e-05
	LOSS [training: 0.267587564665119 | validation: 0.39119554897632625]
	TIME [epoch: 7.62 sec]
EPOCH 813/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.265576803002853		[learning rate: 3.9023e-05]
	Learning Rate: 3.90226e-05
	LOSS [training: 0.265576803002853 | validation: 0.3904683517823079]
	TIME [epoch: 7.61 sec]
EPOCH 814/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2677201632199422		[learning rate: 3.874e-05]
	Learning Rate: 3.87399e-05
	LOSS [training: 0.2677201632199422 | validation: 0.38981421498016217]
	TIME [epoch: 7.63 sec]
EPOCH 815/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26820611065764965		[learning rate: 3.8459e-05]
	Learning Rate: 3.84592e-05
	LOSS [training: 0.26820611065764965 | validation: 0.39074510924644856]
	TIME [epoch: 7.65 sec]
EPOCH 816/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26615417005676706		[learning rate: 3.8181e-05]
	Learning Rate: 3.81806e-05
	LOSS [training: 0.26615417005676706 | validation: 0.38968923468245414]
	TIME [epoch: 7.61 sec]
EPOCH 817/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2680811808481821		[learning rate: 3.7904e-05]
	Learning Rate: 3.79039e-05
	LOSS [training: 0.2680811808481821 | validation: 0.3943952652754085]
	TIME [epoch: 7.61 sec]
EPOCH 818/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2662446379255737		[learning rate: 3.7629e-05]
	Learning Rate: 3.76293e-05
	LOSS [training: 0.2662446379255737 | validation: 0.39518616408817053]
	TIME [epoch: 7.61 sec]
EPOCH 819/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2680964811925649		[learning rate: 3.7357e-05]
	Learning Rate: 3.73567e-05
	LOSS [training: 0.2680964811925649 | validation: 0.3959796191429095]
	TIME [epoch: 7.64 sec]
EPOCH 820/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2663976816865083		[learning rate: 3.7086e-05]
	Learning Rate: 3.70861e-05
	LOSS [training: 0.2663976816865083 | validation: 0.3922969698414971]
	TIME [epoch: 7.65 sec]
EPOCH 821/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.268178906786344		[learning rate: 3.6817e-05]
	Learning Rate: 3.68174e-05
	LOSS [training: 0.268178906786344 | validation: 0.39138420893151715]
	TIME [epoch: 7.61 sec]
EPOCH 822/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2650837766969368		[learning rate: 3.6551e-05]
	Learning Rate: 3.65506e-05
	LOSS [training: 0.2650837766969368 | validation: 0.3921339558053851]
	TIME [epoch: 7.61 sec]
EPOCH 823/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26561974951426764		[learning rate: 3.6286e-05]
	Learning Rate: 3.62858e-05
	LOSS [training: 0.26561974951426764 | validation: 0.3904659527551361]
	TIME [epoch: 7.61 sec]
EPOCH 824/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2668524868979793		[learning rate: 3.6023e-05]
	Learning Rate: 3.60229e-05
	LOSS [training: 0.2668524868979793 | validation: 0.39014493815356033]
	TIME [epoch: 7.63 sec]
EPOCH 825/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26725143544345575		[learning rate: 3.5762e-05]
	Learning Rate: 3.57619e-05
	LOSS [training: 0.26725143544345575 | validation: 0.38912165357055484]
	TIME [epoch: 7.65 sec]
EPOCH 826/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2679830787783556		[learning rate: 3.5503e-05]
	Learning Rate: 3.55029e-05
	LOSS [training: 0.2679830787783556 | validation: 0.39452246426238385]
	TIME [epoch: 7.61 sec]
EPOCH 827/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2690349806391416		[learning rate: 3.5246e-05]
	Learning Rate: 3.52456e-05
	LOSS [training: 0.2690349806391416 | validation: 0.38895218147306715]
	TIME [epoch: 7.61 sec]
EPOCH 828/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2639674243494373		[learning rate: 3.499e-05]
	Learning Rate: 3.49903e-05
	LOSS [training: 0.2639674243494373 | validation: 0.39753540737368775]
	TIME [epoch: 7.61 sec]
EPOCH 829/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26750425626210966		[learning rate: 3.4737e-05]
	Learning Rate: 3.47368e-05
	LOSS [training: 0.26750425626210966 | validation: 0.38954661495101806]
	TIME [epoch: 7.64 sec]
EPOCH 830/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2674029495405853		[learning rate: 3.4485e-05]
	Learning Rate: 3.44851e-05
	LOSS [training: 0.2674029495405853 | validation: 0.39255536108280165]
	TIME [epoch: 7.63 sec]
EPOCH 831/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26577306126443534		[learning rate: 3.4235e-05]
	Learning Rate: 3.42353e-05
	LOSS [training: 0.26577306126443534 | validation: 0.3935909893407504]
	TIME [epoch: 7.61 sec]
EPOCH 832/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2667320425580917		[learning rate: 3.3987e-05]
	Learning Rate: 3.39872e-05
	LOSS [training: 0.2667320425580917 | validation: 0.39004071177897115]
	TIME [epoch: 7.61 sec]
EPOCH 833/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2674805209843802		[learning rate: 3.3741e-05]
	Learning Rate: 3.3741e-05
	LOSS [training: 0.2674805209843802 | validation: 0.387519241480753]
	TIME [epoch: 7.61 sec]
EPOCH 834/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2671280444597785		[learning rate: 3.3497e-05]
	Learning Rate: 3.34965e-05
	LOSS [training: 0.2671280444597785 | validation: 0.3881808816009593]
	TIME [epoch: 7.66 sec]
EPOCH 835/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26717994261944994		[learning rate: 3.3254e-05]
	Learning Rate: 3.32539e-05
	LOSS [training: 0.26717994261944994 | validation: 0.3888522623654135]
	TIME [epoch: 7.62 sec]
EPOCH 836/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26663405914883265		[learning rate: 3.3013e-05]
	Learning Rate: 3.3013e-05
	LOSS [training: 0.26663405914883265 | validation: 0.38793900912108975]
	TIME [epoch: 7.61 sec]
EPOCH 837/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26569969690038464		[learning rate: 3.2774e-05]
	Learning Rate: 3.27738e-05
	LOSS [training: 0.26569969690038464 | validation: 0.39074819938144467]
	TIME [epoch: 7.61 sec]
EPOCH 838/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2665620105711689		[learning rate: 3.2536e-05]
	Learning Rate: 3.25363e-05
	LOSS [training: 0.2665620105711689 | validation: 0.3920744972254855]
	TIME [epoch: 7.61 sec]
EPOCH 839/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2656610927004118		[learning rate: 3.2301e-05]
	Learning Rate: 3.23006e-05
	LOSS [training: 0.2656610927004118 | validation: 0.39399991749614094]
	TIME [epoch: 7.66 sec]
EPOCH 840/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2658712184971531		[learning rate: 3.2067e-05]
	Learning Rate: 3.20666e-05
	LOSS [training: 0.2658712184971531 | validation: 0.38527866017843354]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_840.pth
	Model improved!!!
EPOCH 841/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26497346442469366		[learning rate: 3.1834e-05]
	Learning Rate: 3.18343e-05
	LOSS [training: 0.26497346442469366 | validation: 0.3900681711645733]
	TIME [epoch: 7.62 sec]
EPOCH 842/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26674741802034074		[learning rate: 3.1604e-05]
	Learning Rate: 3.16036e-05
	LOSS [training: 0.26674741802034074 | validation: 0.3949313466682428]
	TIME [epoch: 7.62 sec]
EPOCH 843/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26827356648070816		[learning rate: 3.1375e-05]
	Learning Rate: 3.13747e-05
	LOSS [training: 0.26827356648070816 | validation: 0.3912326751586314]
	TIME [epoch: 7.61 sec]
EPOCH 844/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2664837973173999		[learning rate: 3.1147e-05]
	Learning Rate: 3.11474e-05
	LOSS [training: 0.2664837973173999 | validation: 0.39198661764581366]
	TIME [epoch: 7.66 sec]
EPOCH 845/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2666643759899933		[learning rate: 3.0922e-05]
	Learning Rate: 3.09217e-05
	LOSS [training: 0.2666643759899933 | validation: 0.3908187684860317]
	TIME [epoch: 7.62 sec]
EPOCH 846/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26589509749000506		[learning rate: 3.0698e-05]
	Learning Rate: 3.06977e-05
	LOSS [training: 0.26589509749000506 | validation: 0.39108557990265397]
	TIME [epoch: 7.61 sec]
EPOCH 847/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2664398383416224		[learning rate: 3.0475e-05]
	Learning Rate: 3.04753e-05
	LOSS [training: 0.2664398383416224 | validation: 0.38909721042698403]
	TIME [epoch: 7.61 sec]
EPOCH 848/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26621681766946276		[learning rate: 3.0254e-05]
	Learning Rate: 3.02545e-05
	LOSS [training: 0.26621681766946276 | validation: 0.3897825542672047]
	TIME [epoch: 7.61 sec]
EPOCH 849/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26594678782026115		[learning rate: 3.0035e-05]
	Learning Rate: 3.00353e-05
	LOSS [training: 0.26594678782026115 | validation: 0.39011608576867407]
	TIME [epoch: 7.67 sec]
EPOCH 850/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2653549531198021		[learning rate: 2.9818e-05]
	Learning Rate: 2.98177e-05
	LOSS [training: 0.2653549531198021 | validation: 0.38410841022007747]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_850.pth
	Model improved!!!
EPOCH 851/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2654353433902133		[learning rate: 2.9602e-05]
	Learning Rate: 2.96017e-05
	LOSS [training: 0.2654353433902133 | validation: 0.3869719042738386]
	TIME [epoch: 7.62 sec]
EPOCH 852/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2656303460003973		[learning rate: 2.9387e-05]
	Learning Rate: 2.93872e-05
	LOSS [training: 0.2656303460003973 | validation: 0.3923960381210945]
	TIME [epoch: 7.6 sec]
EPOCH 853/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2656829295261511		[learning rate: 2.9174e-05]
	Learning Rate: 2.91743e-05
	LOSS [training: 0.2656829295261511 | validation: 0.3883235126672705]
	TIME [epoch: 7.62 sec]
EPOCH 854/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26467334213799154		[learning rate: 2.8963e-05]
	Learning Rate: 2.89629e-05
	LOSS [training: 0.26467334213799154 | validation: 0.3913006793371631]
	TIME [epoch: 7.65 sec]
EPOCH 855/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26581671826843123		[learning rate: 2.8753e-05]
	Learning Rate: 2.87531e-05
	LOSS [training: 0.26581671826843123 | validation: 0.3929657623291406]
	TIME [epoch: 7.61 sec]
EPOCH 856/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2655432289937206		[learning rate: 2.8545e-05]
	Learning Rate: 2.85448e-05
	LOSS [training: 0.2655432289937206 | validation: 0.38881554844956556]
	TIME [epoch: 7.61 sec]
EPOCH 857/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2641140657519237		[learning rate: 2.8338e-05]
	Learning Rate: 2.8338e-05
	LOSS [training: 0.2641140657519237 | validation: 0.3907869726452705]
	TIME [epoch: 7.6 sec]
EPOCH 858/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2667541299812965		[learning rate: 2.8133e-05]
	Learning Rate: 2.81327e-05
	LOSS [training: 0.2667541299812965 | validation: 0.3916127490410011]
	TIME [epoch: 7.62 sec]
EPOCH 859/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2654756427048871		[learning rate: 2.7929e-05]
	Learning Rate: 2.79288e-05
	LOSS [training: 0.2654756427048871 | validation: 0.3892953850783565]
	TIME [epoch: 7.65 sec]
EPOCH 860/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2643666336547956		[learning rate: 2.7726e-05]
	Learning Rate: 2.77265e-05
	LOSS [training: 0.2643666336547956 | validation: 0.39542458644569517]
	TIME [epoch: 7.62 sec]
EPOCH 861/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26465586656145434		[learning rate: 2.7526e-05]
	Learning Rate: 2.75256e-05
	LOSS [training: 0.26465586656145434 | validation: 0.38761091091028543]
	TIME [epoch: 7.61 sec]
EPOCH 862/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2659247421073588		[learning rate: 2.7326e-05]
	Learning Rate: 2.73262e-05
	LOSS [training: 0.2659247421073588 | validation: 0.3925599272780333]
	TIME [epoch: 7.61 sec]
EPOCH 863/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26325995995717966		[learning rate: 2.7128e-05]
	Learning Rate: 2.71282e-05
	LOSS [training: 0.26325995995717966 | validation: 0.3848324891752435]
	TIME [epoch: 7.63 sec]
EPOCH 864/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.265585475398332		[learning rate: 2.6932e-05]
	Learning Rate: 2.69317e-05
	LOSS [training: 0.265585475398332 | validation: 0.3944447272153941]
	TIME [epoch: 7.66 sec]
EPOCH 865/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2642740441952802		[learning rate: 2.6737e-05]
	Learning Rate: 2.67365e-05
	LOSS [training: 0.2642740441952802 | validation: 0.392792190783803]
	TIME [epoch: 7.61 sec]
EPOCH 866/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26639086298848763		[learning rate: 2.6543e-05]
	Learning Rate: 2.65428e-05
	LOSS [training: 0.26639086298848763 | validation: 0.3917301841272724]
	TIME [epoch: 7.61 sec]
EPOCH 867/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26718488843976135		[learning rate: 2.6351e-05]
	Learning Rate: 2.63505e-05
	LOSS [training: 0.26718488843976135 | validation: 0.3914004954192711]
	TIME [epoch: 7.61 sec]
EPOCH 868/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2662567823108912		[learning rate: 2.616e-05]
	Learning Rate: 2.61596e-05
	LOSS [training: 0.2662567823108912 | validation: 0.39561567743829895]
	TIME [epoch: 7.65 sec]
EPOCH 869/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2657190899505565		[learning rate: 2.597e-05]
	Learning Rate: 2.59701e-05
	LOSS [training: 0.2657190899505565 | validation: 0.3940622349400813]
	TIME [epoch: 7.64 sec]
EPOCH 870/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2650676848045603		[learning rate: 2.5782e-05]
	Learning Rate: 2.5782e-05
	LOSS [training: 0.2650676848045603 | validation: 0.39213563320885947]
	TIME [epoch: 7.61 sec]
EPOCH 871/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26399298956400374		[learning rate: 2.5595e-05]
	Learning Rate: 2.55952e-05
	LOSS [training: 0.26399298956400374 | validation: 0.3941439363974156]
	TIME [epoch: 7.6 sec]
EPOCH 872/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2644790283154836		[learning rate: 2.541e-05]
	Learning Rate: 2.54097e-05
	LOSS [training: 0.2644790283154836 | validation: 0.3958320728974932]
	TIME [epoch: 7.61 sec]
EPOCH 873/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26507057876906753		[learning rate: 2.5226e-05]
	Learning Rate: 2.52256e-05
	LOSS [training: 0.26507057876906753 | validation: 0.3955736323060033]
	TIME [epoch: 7.65 sec]
EPOCH 874/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26423527599504304		[learning rate: 2.5043e-05]
	Learning Rate: 2.50429e-05
	LOSS [training: 0.26423527599504304 | validation: 0.39070124749861157]
	TIME [epoch: 7.63 sec]
EPOCH 875/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2628305285961521		[learning rate: 2.4861e-05]
	Learning Rate: 2.48614e-05
	LOSS [training: 0.2628305285961521 | validation: 0.3940898531577771]
	TIME [epoch: 7.62 sec]
EPOCH 876/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26455883580976075		[learning rate: 2.4681e-05]
	Learning Rate: 2.46813e-05
	LOSS [training: 0.26455883580976075 | validation: 0.3924921635865244]
	TIME [epoch: 7.62 sec]
EPOCH 877/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26552739961611505		[learning rate: 2.4503e-05]
	Learning Rate: 2.45025e-05
	LOSS [training: 0.26552739961611505 | validation: 0.39229610900041934]
	TIME [epoch: 7.61 sec]
EPOCH 878/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26445629352426453		[learning rate: 2.4325e-05]
	Learning Rate: 2.4325e-05
	LOSS [training: 0.26445629352426453 | validation: 0.39211899824529883]
	TIME [epoch: 7.66 sec]
EPOCH 879/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2633117949292763		[learning rate: 2.4149e-05]
	Learning Rate: 2.41488e-05
	LOSS [training: 0.2633117949292763 | validation: 0.38897515062399424]
	TIME [epoch: 7.62 sec]
EPOCH 880/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2642869169213079		[learning rate: 2.3974e-05]
	Learning Rate: 2.39738e-05
	LOSS [training: 0.2642869169213079 | validation: 0.3881189646595814]
	TIME [epoch: 7.62 sec]
EPOCH 881/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26394691576589757		[learning rate: 2.38e-05]
	Learning Rate: 2.38001e-05
	LOSS [training: 0.26394691576589757 | validation: 0.3874205180164395]
	TIME [epoch: 7.62 sec]
EPOCH 882/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26431963267587827		[learning rate: 2.3628e-05]
	Learning Rate: 2.36277e-05
	LOSS [training: 0.26431963267587827 | validation: 0.3920453138653104]
	TIME [epoch: 7.61 sec]
EPOCH 883/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26511815884440076		[learning rate: 2.3457e-05]
	Learning Rate: 2.34565e-05
	LOSS [training: 0.26511815884440076 | validation: 0.39087152529024666]
	TIME [epoch: 7.66 sec]
EPOCH 884/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2661670969303081		[learning rate: 2.3287e-05]
	Learning Rate: 2.32866e-05
	LOSS [training: 0.2661670969303081 | validation: 0.38699782494729634]
	TIME [epoch: 7.62 sec]
EPOCH 885/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2665039665123811		[learning rate: 2.3118e-05]
	Learning Rate: 2.31179e-05
	LOSS [training: 0.2665039665123811 | validation: 0.3963649278406409]
	TIME [epoch: 7.61 sec]
EPOCH 886/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2628633487142108		[learning rate: 2.295e-05]
	Learning Rate: 2.29504e-05
	LOSS [training: 0.2628633487142108 | validation: 0.38357791915391193]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_886.pth
	Model improved!!!
EPOCH 887/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26497505384080416		[learning rate: 2.2784e-05]
	Learning Rate: 2.27841e-05
	LOSS [training: 0.26497505384080416 | validation: 0.3924953256574656]
	TIME [epoch: 7.62 sec]
EPOCH 888/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2651658666255411		[learning rate: 2.2619e-05]
	Learning Rate: 2.2619e-05
	LOSS [training: 0.2651658666255411 | validation: 0.3907702009372826]
	TIME [epoch: 7.68 sec]
EPOCH 889/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.264170932068329		[learning rate: 2.2455e-05]
	Learning Rate: 2.24551e-05
	LOSS [training: 0.264170932068329 | validation: 0.39089738872912094]
	TIME [epoch: 7.62 sec]
EPOCH 890/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2647021631280941		[learning rate: 2.2292e-05]
	Learning Rate: 2.22925e-05
	LOSS [training: 0.2647021631280941 | validation: 0.3878545990263371]
	TIME [epoch: 7.62 sec]
EPOCH 891/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2636203362405255		[learning rate: 2.2131e-05]
	Learning Rate: 2.2131e-05
	LOSS [training: 0.2636203362405255 | validation: 0.3903420655762581]
	TIME [epoch: 7.62 sec]
EPOCH 892/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2643119069473919		[learning rate: 2.1971e-05]
	Learning Rate: 2.19706e-05
	LOSS [training: 0.2643119069473919 | validation: 0.3911783677305908]
	TIME [epoch: 7.62 sec]
EPOCH 893/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2658596159340387		[learning rate: 2.1811e-05]
	Learning Rate: 2.18114e-05
	LOSS [training: 0.2658596159340387 | validation: 0.3905478767448135]
	TIME [epoch: 7.68 sec]
EPOCH 894/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2642251410571392		[learning rate: 2.1653e-05]
	Learning Rate: 2.16534e-05
	LOSS [training: 0.2642251410571392 | validation: 0.389746014769512]
	TIME [epoch: 7.62 sec]
EPOCH 895/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26382058151720755		[learning rate: 2.1497e-05]
	Learning Rate: 2.14965e-05
	LOSS [training: 0.26382058151720755 | validation: 0.3902444730273254]
	TIME [epoch: 7.62 sec]
EPOCH 896/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26456794746833756		[learning rate: 2.1341e-05]
	Learning Rate: 2.13408e-05
	LOSS [training: 0.26456794746833756 | validation: 0.3850554823578728]
	TIME [epoch: 7.62 sec]
EPOCH 897/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2640730550171252		[learning rate: 2.1186e-05]
	Learning Rate: 2.11862e-05
	LOSS [training: 0.2640730550171252 | validation: 0.3912953797815154]
	TIME [epoch: 7.63 sec]
EPOCH 898/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26514440063687217		[learning rate: 2.1033e-05]
	Learning Rate: 2.10327e-05
	LOSS [training: 0.26514440063687217 | validation: 0.39131953891541027]
	TIME [epoch: 7.67 sec]
EPOCH 899/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26449932848272584		[learning rate: 2.088e-05]
	Learning Rate: 2.08803e-05
	LOSS [training: 0.26449932848272584 | validation: 0.3877487578029315]
	TIME [epoch: 7.62 sec]
EPOCH 900/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26320673031330116		[learning rate: 2.0729e-05]
	Learning Rate: 2.0729e-05
	LOSS [training: 0.26320673031330116 | validation: 0.3885020195952498]
	TIME [epoch: 7.62 sec]
EPOCH 901/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2647878431454952		[learning rate: 2.0579e-05]
	Learning Rate: 2.05789e-05
	LOSS [training: 0.2647878431454952 | validation: 0.38895609949388715]
	TIME [epoch: 7.62 sec]
EPOCH 902/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2645994623171043		[learning rate: 2.043e-05]
	Learning Rate: 2.04298e-05
	LOSS [training: 0.2645994623171043 | validation: 0.38750011152377273]
	TIME [epoch: 7.64 sec]
EPOCH 903/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26318060331821436		[learning rate: 2.0282e-05]
	Learning Rate: 2.02818e-05
	LOSS [training: 0.26318060331821436 | validation: 0.3884794289421606]
	TIME [epoch: 7.66 sec]
EPOCH 904/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26531656574804197		[learning rate: 2.0135e-05]
	Learning Rate: 2.01348e-05
	LOSS [training: 0.26531656574804197 | validation: 0.39186991694604223]
	TIME [epoch: 7.62 sec]
EPOCH 905/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2645724884426933		[learning rate: 1.9989e-05]
	Learning Rate: 1.99889e-05
	LOSS [training: 0.2645724884426933 | validation: 0.3878561623534168]
	TIME [epoch: 7.62 sec]
EPOCH 906/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2634514600418385		[learning rate: 1.9844e-05]
	Learning Rate: 1.98441e-05
	LOSS [training: 0.2634514600418385 | validation: 0.39257345120018233]
	TIME [epoch: 7.62 sec]
EPOCH 907/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2627682359623772		[learning rate: 1.97e-05]
	Learning Rate: 1.97003e-05
	LOSS [training: 0.2627682359623772 | validation: 0.38894120272177013]
	TIME [epoch: 7.65 sec]
EPOCH 908/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2631382686087104		[learning rate: 1.9558e-05]
	Learning Rate: 1.95576e-05
	LOSS [training: 0.2631382686087104 | validation: 0.39103440631537484]
	TIME [epoch: 7.65 sec]
EPOCH 909/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2639727866548248		[learning rate: 1.9416e-05]
	Learning Rate: 1.94159e-05
	LOSS [training: 0.2639727866548248 | validation: 0.38891824119065593]
	TIME [epoch: 7.62 sec]
EPOCH 910/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2641274844629534		[learning rate: 1.9275e-05]
	Learning Rate: 1.92753e-05
	LOSS [training: 0.2641274844629534 | validation: 0.3888118397100637]
	TIME [epoch: 7.61 sec]
EPOCH 911/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2618014744927214		[learning rate: 1.9136e-05]
	Learning Rate: 1.91356e-05
	LOSS [training: 0.2618014744927214 | validation: 0.38394504853518757]
	TIME [epoch: 7.61 sec]
EPOCH 912/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26478928912215993		[learning rate: 1.8997e-05]
	Learning Rate: 1.8997e-05
	LOSS [training: 0.26478928912215993 | validation: 0.39077350266245603]
	TIME [epoch: 7.65 sec]
EPOCH 913/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26348124731219114		[learning rate: 1.8859e-05]
	Learning Rate: 1.88593e-05
	LOSS [training: 0.26348124731219114 | validation: 0.3913347136644926]
	TIME [epoch: 7.63 sec]
EPOCH 914/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2622343990449625		[learning rate: 1.8723e-05]
	Learning Rate: 1.87227e-05
	LOSS [training: 0.2622343990449625 | validation: 0.3864630107084848]
	TIME [epoch: 7.61 sec]
EPOCH 915/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2631450034958218		[learning rate: 1.8587e-05]
	Learning Rate: 1.85871e-05
	LOSS [training: 0.2631450034958218 | validation: 0.3870806293191363]
	TIME [epoch: 7.62 sec]
EPOCH 916/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2639014646966893		[learning rate: 1.8452e-05]
	Learning Rate: 1.84524e-05
	LOSS [training: 0.2639014646966893 | validation: 0.3887033654612331]
	TIME [epoch: 7.62 sec]
EPOCH 917/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26465887562254575		[learning rate: 1.8319e-05]
	Learning Rate: 1.83187e-05
	LOSS [training: 0.26465887562254575 | validation: 0.39015744270960095]
	TIME [epoch: 7.66 sec]
EPOCH 918/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26508464705762647		[learning rate: 1.8186e-05]
	Learning Rate: 1.8186e-05
	LOSS [training: 0.26508464705762647 | validation: 0.38605698572380825]
	TIME [epoch: 7.62 sec]
EPOCH 919/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26601680192264304		[learning rate: 1.8054e-05]
	Learning Rate: 1.80542e-05
	LOSS [training: 0.26601680192264304 | validation: 0.3904251366725835]
	TIME [epoch: 7.61 sec]
EPOCH 920/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26247337518286434		[learning rate: 1.7923e-05]
	Learning Rate: 1.79234e-05
	LOSS [training: 0.26247337518286434 | validation: 0.38802208013426176]
	TIME [epoch: 7.61 sec]
EPOCH 921/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2640495344046892		[learning rate: 1.7794e-05]
	Learning Rate: 1.77936e-05
	LOSS [training: 0.2640495344046892 | validation: 0.39327953111369335]
	TIME [epoch: 7.61 sec]
EPOCH 922/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2640774643315448		[learning rate: 1.7665e-05]
	Learning Rate: 1.76647e-05
	LOSS [training: 0.2640774643315448 | validation: 0.3890313862025173]
	TIME [epoch: 7.66 sec]
EPOCH 923/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2618763287506195		[learning rate: 1.7537e-05]
	Learning Rate: 1.75367e-05
	LOSS [training: 0.2618763287506195 | validation: 0.39144671802411024]
	TIME [epoch: 7.62 sec]
EPOCH 924/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2640767918838959		[learning rate: 1.741e-05]
	Learning Rate: 1.74096e-05
	LOSS [training: 0.2640767918838959 | validation: 0.3860752281185028]
	TIME [epoch: 7.61 sec]
EPOCH 925/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26464836357326804		[learning rate: 1.7284e-05]
	Learning Rate: 1.72835e-05
	LOSS [training: 0.26464836357326804 | validation: 0.3907304126444652]
	TIME [epoch: 7.61 sec]
EPOCH 926/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2628489047051721		[learning rate: 1.7158e-05]
	Learning Rate: 1.71583e-05
	LOSS [training: 0.2628489047051721 | validation: 0.3905843664061949]
	TIME [epoch: 7.61 sec]
EPOCH 927/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26431393770541384		[learning rate: 1.7034e-05]
	Learning Rate: 1.7034e-05
	LOSS [training: 0.26431393770541384 | validation: 0.3872763501668746]
	TIME [epoch: 7.66 sec]
EPOCH 928/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2643790942895911		[learning rate: 1.6911e-05]
	Learning Rate: 1.69106e-05
	LOSS [training: 0.2643790942895911 | validation: 0.38762617682967476]
	TIME [epoch: 7.61 sec]
EPOCH 929/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26469663957352535		[learning rate: 1.6788e-05]
	Learning Rate: 1.6788e-05
	LOSS [training: 0.26469663957352535 | validation: 0.39598818505535915]
	TIME [epoch: 7.61 sec]
EPOCH 930/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2639375343388732		[learning rate: 1.6666e-05]
	Learning Rate: 1.66664e-05
	LOSS [training: 0.2639375343388732 | validation: 0.386997745327746]
	TIME [epoch: 7.61 sec]
EPOCH 931/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2636934203389493		[learning rate: 1.6546e-05]
	Learning Rate: 1.65457e-05
	LOSS [training: 0.2636934203389493 | validation: 0.3891417281298516]
	TIME [epoch: 7.61 sec]
EPOCH 932/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26415832665821043		[learning rate: 1.6426e-05]
	Learning Rate: 1.64258e-05
	LOSS [training: 0.26415832665821043 | validation: 0.38755427435240286]
	TIME [epoch: 7.66 sec]
EPOCH 933/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26394481884832977		[learning rate: 1.6307e-05]
	Learning Rate: 1.63068e-05
	LOSS [training: 0.26394481884832977 | validation: 0.38588600730984857]
	TIME [epoch: 7.61 sec]
EPOCH 934/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2659746206895509		[learning rate: 1.6189e-05]
	Learning Rate: 1.61887e-05
	LOSS [training: 0.2659746206895509 | validation: 0.390764238329659]
	TIME [epoch: 7.61 sec]
EPOCH 935/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26318683244517166		[learning rate: 1.6071e-05]
	Learning Rate: 1.60714e-05
	LOSS [training: 0.26318683244517166 | validation: 0.39032067387588365]
	TIME [epoch: 7.62 sec]
EPOCH 936/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26372737948759906		[learning rate: 1.5955e-05]
	Learning Rate: 1.59549e-05
	LOSS [training: 0.26372737948759906 | validation: 0.38926058697416166]
	TIME [epoch: 7.61 sec]
EPOCH 937/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2641271765580089		[learning rate: 1.5839e-05]
	Learning Rate: 1.58393e-05
	LOSS [training: 0.2641271765580089 | validation: 0.3868268370068822]
	TIME [epoch: 7.67 sec]
EPOCH 938/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.262427792410542		[learning rate: 1.5725e-05]
	Learning Rate: 1.57246e-05
	LOSS [training: 0.262427792410542 | validation: 0.38935994670708385]
	TIME [epoch: 7.61 sec]
EPOCH 939/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26201250514299473		[learning rate: 1.5611e-05]
	Learning Rate: 1.56107e-05
	LOSS [training: 0.26201250514299473 | validation: 0.38891913714152115]
	TIME [epoch: 7.61 sec]
EPOCH 940/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26372514723540996		[learning rate: 1.5498e-05]
	Learning Rate: 1.54976e-05
	LOSS [training: 0.26372514723540996 | validation: 0.3867929898334571]
	TIME [epoch: 7.61 sec]
EPOCH 941/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2633190119132385		[learning rate: 1.5385e-05]
	Learning Rate: 1.53853e-05
	LOSS [training: 0.2633190119132385 | validation: 0.39432853724189143]
	TIME [epoch: 7.62 sec]
EPOCH 942/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2640478433143011		[learning rate: 1.5274e-05]
	Learning Rate: 1.52738e-05
	LOSS [training: 0.2640478433143011 | validation: 0.38837592989812375]
	TIME [epoch: 7.66 sec]
EPOCH 943/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.263529486236545		[learning rate: 1.5163e-05]
	Learning Rate: 1.51632e-05
	LOSS [training: 0.263529486236545 | validation: 0.3879461227873148]
	TIME [epoch: 7.62 sec]
EPOCH 944/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26443830582017236		[learning rate: 1.5053e-05]
	Learning Rate: 1.50533e-05
	LOSS [training: 0.26443830582017236 | validation: 0.3906971086823473]
	TIME [epoch: 7.61 sec]
EPOCH 945/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26303565760105824		[learning rate: 1.4944e-05]
	Learning Rate: 1.49442e-05
	LOSS [training: 0.26303565760105824 | validation: 0.3882848199933761]
	TIME [epoch: 7.62 sec]
EPOCH 946/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26335421895533095		[learning rate: 1.4836e-05]
	Learning Rate: 1.4836e-05
	LOSS [training: 0.26335421895533095 | validation: 0.388566579499934]
	TIME [epoch: 7.63 sec]
EPOCH 947/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2631846463952105		[learning rate: 1.4728e-05]
	Learning Rate: 1.47285e-05
	LOSS [training: 0.2631846463952105 | validation: 0.3921128374240534]
	TIME [epoch: 7.66 sec]
EPOCH 948/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2633441132269528		[learning rate: 1.4622e-05]
	Learning Rate: 1.46218e-05
	LOSS [training: 0.2633441132269528 | validation: 0.39085314307594893]
	TIME [epoch: 7.62 sec]
EPOCH 949/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26262277262418765		[learning rate: 1.4516e-05]
	Learning Rate: 1.45158e-05
	LOSS [training: 0.26262277262418765 | validation: 0.38883228818461]
	TIME [epoch: 7.62 sec]
EPOCH 950/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2629612466449005		[learning rate: 1.4411e-05]
	Learning Rate: 1.44107e-05
	LOSS [training: 0.2629612466449005 | validation: 0.38487495799390037]
	TIME [epoch: 7.62 sec]
EPOCH 951/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26250378540228125		[learning rate: 1.4306e-05]
	Learning Rate: 1.43063e-05
	LOSS [training: 0.26250378540228125 | validation: 0.3857349374105207]
	TIME [epoch: 7.63 sec]
EPOCH 952/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26246741239723953		[learning rate: 1.4203e-05]
	Learning Rate: 1.42026e-05
	LOSS [training: 0.26246741239723953 | validation: 0.39018598821070477]
	TIME [epoch: 7.66 sec]
EPOCH 953/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26163416508089726		[learning rate: 1.41e-05]
	Learning Rate: 1.40997e-05
	LOSS [training: 0.26163416508089726 | validation: 0.3887437912924021]
	TIME [epoch: 7.61 sec]
EPOCH 954/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2634304265456036		[learning rate: 1.3998e-05]
	Learning Rate: 1.39976e-05
	LOSS [training: 0.2634304265456036 | validation: 0.3916958371043252]
	TIME [epoch: 7.62 sec]
EPOCH 955/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2621733924256299		[learning rate: 1.3896e-05]
	Learning Rate: 1.38962e-05
	LOSS [training: 0.2621733924256299 | validation: 0.3870018442800307]
	TIME [epoch: 7.61 sec]
EPOCH 956/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2620259663190277		[learning rate: 1.3795e-05]
	Learning Rate: 1.37955e-05
	LOSS [training: 0.2620259663190277 | validation: 0.38903371779284013]
	TIME [epoch: 7.65 sec]
EPOCH 957/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2643420615269527		[learning rate: 1.3696e-05]
	Learning Rate: 1.36955e-05
	LOSS [training: 0.2643420615269527 | validation: 0.386263390826576]
	TIME [epoch: 7.64 sec]
EPOCH 958/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26213660337047956		[learning rate: 1.3596e-05]
	Learning Rate: 1.35963e-05
	LOSS [training: 0.26213660337047956 | validation: 0.38487431376003245]
	TIME [epoch: 7.62 sec]
EPOCH 959/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26313333803119854		[learning rate: 1.3498e-05]
	Learning Rate: 1.34978e-05
	LOSS [training: 0.26313333803119854 | validation: 0.3821944925344195]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_959.pth
	Model improved!!!
EPOCH 960/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2632874426872093		[learning rate: 1.34e-05]
	Learning Rate: 1.34e-05
	LOSS [training: 0.2632874426872093 | validation: 0.38793131665402303]
	TIME [epoch: 7.62 sec]
EPOCH 961/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2630780941527001		[learning rate: 1.3303e-05]
	Learning Rate: 1.33029e-05
	LOSS [training: 0.2630780941527001 | validation: 0.3858461098380974]
	TIME [epoch: 7.66 sec]
EPOCH 962/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26341312992275845		[learning rate: 1.3207e-05]
	Learning Rate: 1.32066e-05
	LOSS [training: 0.26341312992275845 | validation: 0.3847554175464658]
	TIME [epoch: 7.62 sec]
EPOCH 963/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2626060114119742		[learning rate: 1.3111e-05]
	Learning Rate: 1.31109e-05
	LOSS [training: 0.2626060114119742 | validation: 0.3889992431200082]
	TIME [epoch: 7.61 sec]
EPOCH 964/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26338795647815605		[learning rate: 1.3016e-05]
	Learning Rate: 1.30159e-05
	LOSS [training: 0.26338795647815605 | validation: 0.39107555141371214]
	TIME [epoch: 7.61 sec]
EPOCH 965/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2637348325082884		[learning rate: 1.2922e-05]
	Learning Rate: 1.29216e-05
	LOSS [training: 0.2637348325082884 | validation: 0.3816606039136722]
	TIME [epoch: 7.61 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_965.pth
	Model improved!!!
EPOCH 966/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2610644234224851		[learning rate: 1.2828e-05]
	Learning Rate: 1.2828e-05
	LOSS [training: 0.2610644234224851 | validation: 0.38583970400591944]
	TIME [epoch: 7.67 sec]
EPOCH 967/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2617807831184228		[learning rate: 1.2735e-05]
	Learning Rate: 1.2735e-05
	LOSS [training: 0.2617807831184228 | validation: 0.38617902274143845]
	TIME [epoch: 7.61 sec]
EPOCH 968/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2631522346538508		[learning rate: 1.2643e-05]
	Learning Rate: 1.26428e-05
	LOSS [training: 0.2631522346538508 | validation: 0.38598077454215785]
	TIME [epoch: 7.61 sec]
EPOCH 969/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2621752550092008		[learning rate: 1.2551e-05]
	Learning Rate: 1.25512e-05
	LOSS [training: 0.2621752550092008 | validation: 0.38825540577136275]
	TIME [epoch: 7.61 sec]
EPOCH 970/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2631206849475072		[learning rate: 1.246e-05]
	Learning Rate: 1.24602e-05
	LOSS [training: 0.2631206849475072 | validation: 0.3799015678079968]
	TIME [epoch: 7.62 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240523_142927/states/model_phiq_1a_v_mmd1_970.pth
	Model improved!!!
EPOCH 971/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26269267621320175		[learning rate: 1.237e-05]
	Learning Rate: 1.237e-05
	LOSS [training: 0.26269267621320175 | validation: 0.389472982269341]
	TIME [epoch: 7.67 sec]
EPOCH 972/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26550696314429983		[learning rate: 1.228e-05]
	Learning Rate: 1.22803e-05
	LOSS [training: 0.26550696314429983 | validation: 0.3831066082224878]
	TIME [epoch: 7.62 sec]
EPOCH 973/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2627933210215042		[learning rate: 1.2191e-05]
	Learning Rate: 1.21914e-05
	LOSS [training: 0.2627933210215042 | validation: 0.39268229373124497]
	TIME [epoch: 7.61 sec]
EPOCH 974/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26416329123457505		[learning rate: 1.2103e-05]
	Learning Rate: 1.21031e-05
	LOSS [training: 0.26416329123457505 | validation: 0.3853816501496019]
	TIME [epoch: 7.61 sec]
EPOCH 975/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2613577796387633		[learning rate: 1.2015e-05]
	Learning Rate: 1.20154e-05
	LOSS [training: 0.2613577796387633 | validation: 0.3846239616652195]
	TIME [epoch: 7.62 sec]
EPOCH 976/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2654192354100048		[learning rate: 1.1928e-05]
	Learning Rate: 1.19283e-05
	LOSS [training: 0.2654192354100048 | validation: 0.39117451386031094]
	TIME [epoch: 7.66 sec]
EPOCH 977/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26299749066882766		[learning rate: 1.1842e-05]
	Learning Rate: 1.18419e-05
	LOSS [training: 0.26299749066882766 | validation: 0.39002949105156715]
	TIME [epoch: 7.62 sec]
EPOCH 978/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2610924346235849		[learning rate: 1.1756e-05]
	Learning Rate: 1.17561e-05
	LOSS [training: 0.2610924346235849 | validation: 0.3859005720405234]
	TIME [epoch: 7.61 sec]
EPOCH 979/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2620589035976757		[learning rate: 1.1671e-05]
	Learning Rate: 1.16709e-05
	LOSS [training: 0.2620589035976757 | validation: 0.3906801235381287]
	TIME [epoch: 7.62 sec]
EPOCH 980/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26227394609806864		[learning rate: 1.1586e-05]
	Learning Rate: 1.15864e-05
	LOSS [training: 0.26227394609806864 | validation: 0.3855878640741979]
	TIME [epoch: 7.62 sec]
EPOCH 981/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2623262508301805		[learning rate: 1.1502e-05]
	Learning Rate: 1.15024e-05
	LOSS [training: 0.2623262508301805 | validation: 0.39007055215297026]
	TIME [epoch: 7.65 sec]
EPOCH 982/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2623654134658676		[learning rate: 1.1419e-05]
	Learning Rate: 1.14191e-05
	LOSS [training: 0.2623654134658676 | validation: 0.3892520781551735]
	TIME [epoch: 7.62 sec]
EPOCH 983/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2624488987971779		[learning rate: 1.1336e-05]
	Learning Rate: 1.13364e-05
	LOSS [training: 0.2624488987971779 | validation: 0.383736699172581]
	TIME [epoch: 7.61 sec]
EPOCH 984/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2618896924364048		[learning rate: 1.1254e-05]
	Learning Rate: 1.12542e-05
	LOSS [training: 0.2618896924364048 | validation: 0.3875794902085131]
	TIME [epoch: 7.61 sec]
EPOCH 985/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2620825794873915		[learning rate: 1.1173e-05]
	Learning Rate: 1.11727e-05
	LOSS [training: 0.2620825794873915 | validation: 0.38183899141065186]
	TIME [epoch: 7.63 sec]
EPOCH 986/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2626541919678336		[learning rate: 1.1092e-05]
	Learning Rate: 1.10918e-05
	LOSS [training: 0.2626541919678336 | validation: 0.3902533502914899]
	TIME [epoch: 7.65 sec]
EPOCH 987/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26155043785094595		[learning rate: 1.1011e-05]
	Learning Rate: 1.10114e-05
	LOSS [training: 0.26155043785094595 | validation: 0.39158298606256314]
	TIME [epoch: 7.61 sec]
EPOCH 988/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2621413897837891		[learning rate: 1.0932e-05]
	Learning Rate: 1.09316e-05
	LOSS [training: 0.2621413897837891 | validation: 0.38687502853250305]
	TIME [epoch: 7.62 sec]
EPOCH 989/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26121283150460384		[learning rate: 1.0852e-05]
	Learning Rate: 1.08524e-05
	LOSS [training: 0.26121283150460384 | validation: 0.3859028912446561]
	TIME [epoch: 7.62 sec]
EPOCH 990/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2623092492771712		[learning rate: 1.0774e-05]
	Learning Rate: 1.07738e-05
	LOSS [training: 0.2623092492771712 | validation: 0.388180934911483]
	TIME [epoch: 7.63 sec]
EPOCH 991/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26341149266497726		[learning rate: 1.0696e-05]
	Learning Rate: 1.06957e-05
	LOSS [training: 0.26341149266497726 | validation: 0.3901359115503793]
	TIME [epoch: 7.65 sec]
EPOCH 992/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26301538627392174		[learning rate: 1.0618e-05]
	Learning Rate: 1.06182e-05
	LOSS [training: 0.26301538627392174 | validation: 0.3866343246068049]
	TIME [epoch: 7.61 sec]
EPOCH 993/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2633281686104802		[learning rate: 1.0541e-05]
	Learning Rate: 1.05413e-05
	LOSS [training: 0.2633281686104802 | validation: 0.39514859924007467]
	TIME [epoch: 7.62 sec]
EPOCH 994/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2636736765184582		[learning rate: 1.0465e-05]
	Learning Rate: 1.04649e-05
	LOSS [training: 0.2636736765184582 | validation: 0.38699674133993744]
	TIME [epoch: 7.61 sec]
EPOCH 995/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2614878727375599		[learning rate: 1.0389e-05]
	Learning Rate: 1.03891e-05
	LOSS [training: 0.2614878727375599 | validation: 0.38975031896222645]
	TIME [epoch: 7.65 sec]
EPOCH 996/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26374307946130354		[learning rate: 1.0314e-05]
	Learning Rate: 1.03139e-05
	LOSS [training: 0.26374307946130354 | validation: 0.3838139099800737]
	TIME [epoch: 7.63 sec]
EPOCH 997/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2618158649244266		[learning rate: 1.0239e-05]
	Learning Rate: 1.02391e-05
	LOSS [training: 0.2618158649244266 | validation: 0.3885823983504997]
	TIME [epoch: 7.61 sec]
EPOCH 998/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.263408276832174		[learning rate: 1.0165e-05]
	Learning Rate: 1.0165e-05
	LOSS [training: 0.263408276832174 | validation: 0.38437000803972904]
	TIME [epoch: 7.61 sec]
EPOCH 999/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2625470479154257		[learning rate: 1.0091e-05]
	Learning Rate: 1.00913e-05
	LOSS [training: 0.2625470479154257 | validation: 0.38695329374911674]
	TIME [epoch: 7.61 sec]
EPOCH 1000/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2623602492147036		[learning rate: 1.0018e-05]
	Learning Rate: 1.00182e-05
	LOSS [training: 0.2623602492147036 | validation: 0.38878958884609477]
	TIME [epoch: 7.66 sec]
Finished training in 7860.116 seconds.
