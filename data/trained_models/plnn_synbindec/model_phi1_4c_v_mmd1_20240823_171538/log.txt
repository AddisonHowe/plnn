Args:
Namespace(name='model_phi1_4c_v_mmd1', outdir='out/model_training/model_phi1_4c_v_mmd1', training_data='data/training_data/data_phi1_4c/training', validation_data='data/training_data/data_phi1_4c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1667048106

Training model...

Saving initial model state to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.270195488503357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.270195488503357 | validation: 5.2490873342157665]
	TIME [epoch: 47.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.957077990328136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.957077990328136 | validation: 6.093345338178345]
	TIME [epoch: 3.75 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.126671384412401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.126671384412401 | validation: 6.754556132129588]
	TIME [epoch: 3.73 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.711632075285906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.711632075285906 | validation: 5.145789214197046]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.153118369981602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.153118369981602 | validation: 4.769145668311765]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.541304158343944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.541304158343944 | validation: 4.44047977504889]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.287807393221575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.287807393221575 | validation: 4.260398275604556]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.197944905469688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.197944905469688 | validation: 4.398162579193193]
	TIME [epoch: 3.75 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.106279125209319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.106279125209319 | validation: 4.154234057142662]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.142164990872139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.142164990872139 | validation: 4.188819555178788]
	TIME [epoch: 3.72 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.98146444728749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.98146444728749 | validation: 4.163287802470138]
	TIME [epoch: 3.72 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9615579887610135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9615579887610135 | validation: 4.041635615927598]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.926438904092781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.926438904092781 | validation: 4.042357606106793]
	TIME [epoch: 3.73 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8724182399546527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8724182399546527 | validation: 4.00688152161804]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8442861859733775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8442861859733775 | validation: 3.958410793603706]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.819301388460512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.819301388460512 | validation: 3.961155069329165]
	TIME [epoch: 3.74 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.802627930959431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.802627930959431 | validation: 3.9838478681129064]
	TIME [epoch: 3.72 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.833382677210256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.833382677210256 | validation: 3.9798591271303545]
	TIME [epoch: 3.72 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.911327821373344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.911327821373344 | validation: 4.232550618172712]
	TIME [epoch: 3.74 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9723036260289923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9723036260289923 | validation: 3.8324456416543597]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.739477465993871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.739477465993871 | validation: 3.923189925223116]
	TIME [epoch: 3.76 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7954297364943446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7954297364943446 | validation: 3.923550937606625]
	TIME [epoch: 3.74 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7366869229261104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7366869229261104 | validation: 3.7613624482985566]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.645291306005962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.645291306005962 | validation: 3.8432731357078893]
	TIME [epoch: 3.74 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.659348449973645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.659348449973645 | validation: 3.7435370087155793]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6549782984509176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6549782984509176 | validation: 3.778641340736379]
	TIME [epoch: 3.74 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.591035250842799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.591035250842799 | validation: 3.6758715251423917]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.588821295632355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.588821295632355 | validation: 3.850718760067922]
	TIME [epoch: 3.72 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.635001226951317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.635001226951317 | validation: 3.6393302021698064]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.567005451210739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.567005451210739 | validation: 3.7510491718971255]
	TIME [epoch: 3.72 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.556687750166217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.556687750166217 | validation: 3.5914584086639154]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4987519235291806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4987519235291806 | validation: 3.6429444375927478]
	TIME [epoch: 3.74 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.47987981516342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.47987981516342 | validation: 3.5583098060569185]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4533504365460477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4533504365460477 | validation: 3.6083083355690015]
	TIME [epoch: 3.71 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4457092269085012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4457092269085012 | validation: 3.524050022490569]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.425257843688693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.425257843688693 | validation: 3.581670843445617]
	TIME [epoch: 3.71 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4198273353171365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4198273353171365 | validation: 3.4714930347736543]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.368128978626548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.368128978626548 | validation: 3.547686967420214]
	TIME [epoch: 3.72 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.35748733254424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.35748733254424 | validation: 3.437749766185606]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4020645661657443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4020645661657443 | validation: 3.6784532375379784]
	TIME [epoch: 3.74 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.447815327147783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.447815327147783 | validation: 3.4040352681247024]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.37623909831361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.37623909831361 | validation: 3.3661583014551115]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2522756075620296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2522756075620296 | validation: 3.449499766578123]
	TIME [epoch: 3.74 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2758686150424547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2758686150424547 | validation: 3.310212169344002]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2516710918568696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2516710918568696 | validation: 3.347043849847805]
	TIME [epoch: 3.71 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2100223236792496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2100223236792496 | validation: 3.322850177112819]
	TIME [epoch: 3.74 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.190020152199792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.190020152199792 | validation: 3.2690280306583563]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.149759828231801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.149759828231801 | validation: 3.2318947223369143]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1355434702377454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1355434702377454 | validation: 3.310134888952022]
	TIME [epoch: 3.75 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1513475132135245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1513475132135245 | validation: 3.330974050564875]
	TIME [epoch: 3.74 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.317514448289785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.317514448289785 | validation: 3.449891926423321]
	TIME [epoch: 3.73 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.280090573739556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.280090573739556 | validation: 3.1808244854445533]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.092153985735534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.092153985735534 | validation: 3.190463368236875]
	TIME [epoch: 3.73 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.12752635734489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.12752635734489 | validation: 3.463015108684525]
	TIME [epoch: 3.73 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2299485736133704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2299485736133704 | validation: 3.1817694043022926]
	TIME [epoch: 3.75 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.158162195920094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.158162195920094 | validation: 3.1249678401641177]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0792203200619217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0792203200619217 | validation: 3.1714063334322073]
	TIME [epoch: 3.73 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0305618902899516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0305618902899516 | validation: 3.0839032919533733]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0136661657361685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0136661657361685 | validation: 3.0718158581407575]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9745834482361877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9745834482361877 | validation: 3.039083808904711]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.948964741777436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.948964741777436 | validation: 3.022305770003063]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.927325995780148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.927325995780148 | validation: 2.9927329430168736]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9039332221342677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9039332221342677 | validation: 3.003219265105133]
	TIME [epoch: 3.73 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8795676844939546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8795676844939546 | validation: 3.14259279944231]
	TIME [epoch: 3.73 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1221988999234664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1221988999234664 | validation: 3.9439259300929943]
	TIME [epoch: 3.72 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.66954680352604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.66954680352604 | validation: 3.1170215053308636]
	TIME [epoch: 3.72 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.029649998554768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.029649998554768 | validation: 3.006221983168417]
	TIME [epoch: 3.72 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.985244472242316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.985244472242316 | validation: 2.9269482533575606]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8873686456437055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8873686456437055 | validation: 2.8563318904309436]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.76167939024323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.76167939024323 | validation: 2.7444412054939917]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.629816474989683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.629816474989683 | validation: 2.453326682597373]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3229063686274647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3229063686274647 | validation: 1.7743006083544242]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7139393286767453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7139393286767453 | validation: 2.188178620828668]
	TIME [epoch: 3.76 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.328541508689789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.328541508689789 | validation: 3.640829738938271]
	TIME [epoch: 3.75 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.043816937849107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.043816937849107 | validation: 1.3468882700835454]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3138157030607698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3138157030607698 | validation: 1.8288551060425542]
	TIME [epoch: 3.74 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.060901695960216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.060901695960216 | validation: 1.583220508116872]
	TIME [epoch: 3.74 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6678896531040195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6678896531040195 | validation: 1.2345500919887433]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3217749750764507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3217749750764507 | validation: 1.071617895639079]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.102849999280582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.102849999280582 | validation: 1.0384263862043017]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0977755687764512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0977755687764512 | validation: 0.8937647511442031]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9993465052992627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9993465052992627 | validation: 0.9214414105670152]
	TIME [epoch: 3.74 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9872313136805653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9872313136805653 | validation: 0.8919340292150209]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9569579889972277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9569579889972277 | validation: 0.874243531988153]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9584153854303057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9584153854303057 | validation: 0.9247304648317534]
	TIME [epoch: 3.73 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9830658565216988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9830658565216988 | validation: 0.9134799310228581]
	TIME [epoch: 3.72 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0227383895294215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0227383895294215 | validation: 0.8174810257532267]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9287677235376305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9287677235376305 | validation: 0.828314524808468]
	TIME [epoch: 3.73 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.945611420501743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.945611420501743 | validation: 1.0307910382135392]
	TIME [epoch: 3.73 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.243487028510034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.243487028510034 | validation: 0.8645465984018763]
	TIME [epoch: 3.72 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.977995983173919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.977995983173919 | validation: 0.8310640052424134]
	TIME [epoch: 3.73 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9529876721682417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9529876721682417 | validation: 0.7872523718885387]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.924806320986865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.924806320986865 | validation: 0.7796017072120609]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9042116892357425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9042116892357425 | validation: 0.7714938503354214]
	TIME [epoch: 3.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9025315912973898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9025315912973898 | validation: 0.8433359786792582]
	TIME [epoch: 3.74 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9948201857221753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9948201857221753 | validation: 0.8491723370769574]
	TIME [epoch: 3.74 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9765525487936421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9765525487936421 | validation: 0.7954135445396583]
	TIME [epoch: 3.73 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9080872800317839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9080872800317839 | validation: 0.773755652019583]
	TIME [epoch: 3.72 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.890526424146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.890526424146 | validation: 0.7596666962416283]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8954510659758598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8954510659758598 | validation: 0.7554717821627925]
	TIME [epoch: 3.71 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8994252251728633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8994252251728633 | validation: 0.7709420894702937]
	TIME [epoch: 3.73 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8851565699165859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8851565699165859 | validation: 0.7659095377558005]
	TIME [epoch: 3.72 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8901251574531471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8901251574531471 | validation: 0.7738425256106698]
	TIME [epoch: 3.74 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8791739736032714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8791739736032714 | validation: 0.7743822880273454]
	TIME [epoch: 3.75 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8852863697431405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8852863697431405 | validation: 0.7800778466417232]
	TIME [epoch: 3.74 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8913327696947468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8913327696947468 | validation: 0.8077971790644484]
	TIME [epoch: 3.74 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9784382494526243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9784382494526243 | validation: 0.8954886253669703]
	TIME [epoch: 3.74 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0357750352240058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0357750352240058 | validation: 0.7836172707838126]
	TIME [epoch: 3.74 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8815713991940084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8815713991940084 | validation: 0.7612751182032531]
	TIME [epoch: 3.73 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8714532809550178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8714532809550178 | validation: 0.7489664874324585]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8826920729075711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8826920729075711 | validation: 0.7638870128003844]
	TIME [epoch: 3.74 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9053797226347495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9053797226347495 | validation: 0.8300849610673109]
	TIME [epoch: 3.73 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9843532063811553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9843532063811553 | validation: 0.8690180386438415]
	TIME [epoch: 3.74 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0071986563489697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0071986563489697 | validation: 0.7549834041901716]
	TIME [epoch: 3.75 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8871072235117052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8871072235117052 | validation: 0.8096159612481323]
	TIME [epoch: 3.74 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9227540170441936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9227540170441936 | validation: 0.8358043198218431]
	TIME [epoch: 3.74 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9555223800141684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9555223800141684 | validation: 0.8244313887159834]
	TIME [epoch: 3.74 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9585130981102806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9585130981102806 | validation: 0.8648129443412599]
	TIME [epoch: 3.74 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0093003405937648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0093003405937648 | validation: 0.742263269246799]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8714859927370685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8714859927370685 | validation: 0.8871048224739986]
	TIME [epoch: 3.74 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0326953458833956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0326953458833956 | validation: 0.764112139742905]
	TIME [epoch: 3.74 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.911933153401427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.911933153401427 | validation: 0.8016508183711917]
	TIME [epoch: 3.74 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9117602670556056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9117602670556056 | validation: 0.7775163757261718]
	TIME [epoch: 3.74 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9084195964493771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9084195964493771 | validation: 0.7559481679701006]
	TIME [epoch: 3.74 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8755343431260932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8755343431260932 | validation: 0.7349506043504741]
	TIME [epoch: 3.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8645622320636992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8645622320636992 | validation: 0.7467633660247577]
	TIME [epoch: 3.75 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.874184378448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.874184378448 | validation: 0.7346703259390832]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8683126960120697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8683126960120697 | validation: 0.7375336267295515]
	TIME [epoch: 3.74 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.873899034445986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.873899034445986 | validation: 0.7530200803814452]
	TIME [epoch: 3.74 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8913085151399176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8913085151399176 | validation: 0.776180585385616]
	TIME [epoch: 3.74 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9147878390048436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9147878390048436 | validation: 0.8782940831825912]
	TIME [epoch: 3.74 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0359423267824424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0359423267824424 | validation: 0.8225565424571077]
	TIME [epoch: 3.74 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9308815998720579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9308815998720579 | validation: 0.9275840474398929]
	TIME [epoch: 3.74 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.044620190277245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.044620190277245 | validation: 0.8003933616878826]
	TIME [epoch: 3.74 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9396912011728534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9396912011728534 | validation: 0.7401355328700415]
	TIME [epoch: 3.74 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.881711066232989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.881711066232989 | validation: 0.7506792399327326]
	TIME [epoch: 3.75 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8805452897088089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8805452897088089 | validation: 0.7744629707867446]
	TIME [epoch: 3.74 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8879634552356095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8879634552356095 | validation: 0.8976430416114485]
	TIME [epoch: 3.73 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.024524682217842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.024524682217842 | validation: 0.7634083962070997]
	TIME [epoch: 3.74 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8977334864351394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8977334864351394 | validation: 0.8200925988262523]
	TIME [epoch: 3.73 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9182426976340428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9182426976340428 | validation: 0.8175733534932969]
	TIME [epoch: 3.73 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.908803549881371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.908803549881371 | validation: 0.7672676465556841]
	TIME [epoch: 3.73 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.863933572189579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.863933572189579 | validation: 0.7591865776009699]
	TIME [epoch: 3.74 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8726177841186705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8726177841186705 | validation: 0.755052851459213]
	TIME [epoch: 3.74 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8598597237939919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8598597237939919 | validation: 0.7359878416785276]
	TIME [epoch: 3.74 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8622062432192732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8622062432192732 | validation: 0.7589913620764477]
	TIME [epoch: 3.73 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8794877314894682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8794877314894682 | validation: 0.822522588762255]
	TIME [epoch: 3.75 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9665028095552711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9665028095552711 | validation: 0.7977883272049211]
	TIME [epoch: 3.73 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9075158406657327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9075158406657327 | validation: 0.8611527653603578]
	TIME [epoch: 3.73 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9616390630924179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9616390630924179 | validation: 0.8332443173591376]
	TIME [epoch: 3.74 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9201527837758964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9201527837758964 | validation: 0.9055767214078139]
	TIME [epoch: 3.73 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.996023975454729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.996023975454729 | validation: 0.7375301739428131]
	TIME [epoch: 3.74 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8708073695204052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8708073695204052 | validation: 0.8311788083524466]
	TIME [epoch: 3.73 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9172769507691132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9172769507691132 | validation: 0.8310839649838372]
	TIME [epoch: 3.73 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9130019163112829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9130019163112829 | validation: 0.7488760621020163]
	TIME [epoch: 3.74 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8462247045948453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8462247045948453 | validation: 0.7394254537801812]
	TIME [epoch: 3.74 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8415659039792328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8415659039792328 | validation: 0.7488881038517107]
	TIME [epoch: 3.74 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8428564671337747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8428564671337747 | validation: 0.7499096394185283]
	TIME [epoch: 3.75 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8358889011651515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8358889011651515 | validation: 0.7798371114877435]
	TIME [epoch: 3.74 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8957208032707726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8957208032707726 | validation: 0.954952745959454]
	TIME [epoch: 3.74 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0644893022981048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0644893022981048 | validation: 0.8671672231257695]
	TIME [epoch: 3.73 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9928793289269249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9928793289269249 | validation: 0.8041755521933388]
	TIME [epoch: 3.74 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9278486270669953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9278486270669953 | validation: 0.9129873645937305]
	TIME [epoch: 3.73 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9872766344815963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9872766344815963 | validation: 0.6982082023404207]
	TIME [epoch: 3.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8318102673014732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8318102673014732 | validation: 0.7696729798047534]
	TIME [epoch: 3.76 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8836639061650422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8836639061650422 | validation: 0.79540094935903]
	TIME [epoch: 3.75 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.850628293774258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.850628293774258 | validation: 0.7628834107921995]
	TIME [epoch: 3.74 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.826321799492285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.826321799492285 | validation: 0.7589590205946676]
	TIME [epoch: 3.76 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8728135221240948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8728135221240948 | validation: 0.8116962909599144]
	TIME [epoch: 3.75 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8741968866098785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8741968866098785 | validation: 0.8284056102665887]
	TIME [epoch: 3.74 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8739643612062623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8739643612062623 | validation: 0.8620126589099532]
	TIME [epoch: 3.73 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9574010679264328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9574010679264328 | validation: 0.7204914582250815]
	TIME [epoch: 3.74 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8038322748520307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8038322748520307 | validation: 0.7015172799125415]
	TIME [epoch: 3.73 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7696948689190171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7696948689190171 | validation: 0.6878951595362268]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7516264161995296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7516264161995296 | validation: 0.7714713199885493]
	TIME [epoch: 3.73 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.803505875694114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.803505875694114 | validation: 1.1844331231131233]
	TIME [epoch: 3.73 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2392528302605066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2392528302605066 | validation: 0.8444554921215339]
	TIME [epoch: 3.73 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9892757764927381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9892757764927381 | validation: 0.8593539928058725]
	TIME [epoch: 3.73 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0026444368221306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0026444368221306 | validation: 0.8248661354105811]
	TIME [epoch: 3.75 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8636586745567988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8636586745567988 | validation: 0.7414215675362757]
	TIME [epoch: 3.73 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7934679988153701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7934679988153701 | validation: 0.6818006237161086]
	TIME [epoch: 3.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7824654887594598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7824654887594598 | validation: 0.6800942928330971]
	TIME [epoch: 3.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_182.pth
	Model improved!!!
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7356767144754122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7356767144754122 | validation: 0.7069231831049906]
	TIME [epoch: 3.74 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7409082548572777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7409082548572777 | validation: 0.8581245345249631]
	TIME [epoch: 3.74 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.878649460915577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.878649460915577 | validation: 1.124660794155294]
	TIME [epoch: 3.74 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0925008836446262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0925008836446262 | validation: 0.7698532754112716]
	TIME [epoch: 3.73 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8431330701772887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8431330701772887 | validation: 0.6864764841445249]
	TIME [epoch: 3.74 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8363958805010248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8363958805010248 | validation: 0.6825860330622506]
	TIME [epoch: 3.73 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7538319245300156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7538319245300156 | validation: 0.7533715776636931]
	TIME [epoch: 3.75 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7791965036498588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7791965036498588 | validation: 0.7054000961687024]
	TIME [epoch: 3.75 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7979196886248281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7979196886248281 | validation: 0.7325182316201826]
	TIME [epoch: 3.75 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7580133030311369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7580133030311369 | validation: 0.8325080174635394]
	TIME [epoch: 3.74 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.804519735727884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.804519735727884 | validation: 0.7802182367054883]
	TIME [epoch: 3.74 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7941105006384452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7941105006384452 | validation: 0.7034240062869357]
	TIME [epoch: 3.74 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7573186141892627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7573186141892627 | validation: 0.7498578365865852]
	TIME [epoch: 3.74 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.760743191594701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.760743191594701 | validation: 0.6722837276522761]
	TIME [epoch: 3.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7315641993873462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7315641993873462 | validation: 0.7255153250658892]
	TIME [epoch: 3.74 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7345213264094739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7345213264094739 | validation: 0.7676182921501103]
	TIME [epoch: 3.75 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7684257172633782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7684257172633782 | validation: 0.8947585733701839]
	TIME [epoch: 3.75 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9104578874953748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9104578874953748 | validation: 0.7198083951920211]
	TIME [epoch: 3.75 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7509483494548203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7509483494548203 | validation: 0.6514468323598435]
	TIME [epoch: 48.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6758661197031733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6758661197031733 | validation: 0.66945382273607]
	TIME [epoch: 8.05 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6617563669065041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6617563669065041 | validation: 0.7398949444989295]
	TIME [epoch: 8.12 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7405433253272528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7405433253272528 | validation: 1.0103356651735351]
	TIME [epoch: 8.09 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0068081502897754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0068081502897754 | validation: 0.7742077915371072]
	TIME [epoch: 8.1 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8519725581197704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8519725581197704 | validation: 0.6376654198149915]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_206.pth
	Model improved!!!
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6867132598215783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6867132598215783 | validation: 0.7062344708910038]
	TIME [epoch: 8.13 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7001571811521072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7001571811521072 | validation: 0.7234335948898382]
	TIME [epoch: 8.15 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7597217953890513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7597217953890513 | validation: 0.8559294618947034]
	TIME [epoch: 8.14 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8537892244131774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8537892244131774 | validation: 0.6534021191057895]
	TIME [epoch: 8.12 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7016206436469885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7016206436469885 | validation: 0.6370392111179427]
	TIME [epoch: 8.14 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6580452420500893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6580452420500893 | validation: 0.6014059806577662]
	TIME [epoch: 8.15 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6626580604984852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6626580604984852 | validation: 0.7590525785138285]
	TIME [epoch: 8.17 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.712713866963779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.712713866963779 | validation: 0.6989901373541824]
	TIME [epoch: 8.13 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8126533077022506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8126533077022506 | validation: 0.6955770522402137]
	TIME [epoch: 8.14 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6848060180223167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6848060180223167 | validation: 0.6888220304484277]
	TIME [epoch: 8.14 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.680833687737904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.680833687737904 | validation: 0.7741521716095688]
	TIME [epoch: 8.14 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8252038142689136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8252038142689136 | validation: 0.9314798653115972]
	TIME [epoch: 8.13 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.928794929314069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.928794929314069 | validation: 0.6304084176097611]
	TIME [epoch: 8.15 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6987085289402956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6987085289402956 | validation: 0.6337632237216789]
	TIME [epoch: 8.12 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6686069262711984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6686069262711984 | validation: 0.6825653486489497]
	TIME [epoch: 8.13 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6651333457493124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6651333457493124 | validation: 0.7087874176779632]
	TIME [epoch: 8.13 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7227365486053238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7227365486053238 | validation: 0.8400868130285464]
	TIME [epoch: 8.13 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8626894459862365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8626894459862365 | validation: 0.7955647776066147]
	TIME [epoch: 8.15 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7958832998250344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7958832998250344 | validation: 0.6104991744300574]
	TIME [epoch: 8.15 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6774694705687311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6774694705687311 | validation: 0.6204470709787157]
	TIME [epoch: 8.12 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.621771749497212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.621771749497212 | validation: 0.7269656840261358]
	TIME [epoch: 8.11 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6950862273512104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6950862273512104 | validation: 0.818472669222513]
	TIME [epoch: 8.13 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.791303530943217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.791303530943217 | validation: 0.6944952660273928]
	TIME [epoch: 8.15 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7811770028742836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7811770028742836 | validation: 0.6873879492525651]
	TIME [epoch: 8.13 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6815427128086375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6815427128086375 | validation: 0.6012414708657655]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_231.pth
	Model improved!!!
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6348453643163756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6348453643163756 | validation: 0.5930428646709418]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_232.pth
	Model improved!!!
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.607330336662715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.607330336662715 | validation: 0.6475029615317621]
	TIME [epoch: 8.11 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6361447900889303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6361447900889303 | validation: 0.7842522844714801]
	TIME [epoch: 8.15 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7516524966053902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7516524966053902 | validation: 0.8846710933443931]
	TIME [epoch: 8.12 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9106666037446187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9106666037446187 | validation: 0.6259195000494995]
	TIME [epoch: 8.09 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6815359688799916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6815359688799916 | validation: 0.6277993501419703]
	TIME [epoch: 8.13 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.634640516632453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.634640516632453 | validation: 0.6359847444909942]
	TIME [epoch: 8.08 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6530334831457879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6530334831457879 | validation: 0.8403014436750063]
	TIME [epoch: 8.13 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7688984371732205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7688984371732205 | validation: 0.8267697946609325]
	TIME [epoch: 8.14 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8478587410657926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8478587410657926 | validation: 0.6740295861248818]
	TIME [epoch: 8.12 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8067172058236386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8067172058236386 | validation: 0.6280207559679726]
	TIME [epoch: 8.11 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6231043064527603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6231043064527603 | validation: 0.6098937909773459]
	TIME [epoch: 8.11 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6108913010012712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6108913010012712 | validation: 0.6107752059834085]
	TIME [epoch: 8.14 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6827528255060282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6827528255060282 | validation: 0.7037204765804326]
	TIME [epoch: 8.14 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6927390985556822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6927390985556822 | validation: 0.6252700117472654]
	TIME [epoch: 8.12 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6861313235931091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6861313235931091 | validation: 0.5795006799775916]
	TIME [epoch: 8.14 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6166697100506405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6166697100506405 | validation: 0.7054769552212488]
	TIME [epoch: 8.11 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6588192966793328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6588192966793328 | validation: 0.6955680519660157]
	TIME [epoch: 8.12 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7433909850746011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7433909850746011 | validation: 0.6851201593280916]
	TIME [epoch: 8.12 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6813741989034438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6813741989034438 | validation: 0.6442559107684969]
	TIME [epoch: 8.11 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6351680430286312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6351680430286312 | validation: 0.5870931825271474]
	TIME [epoch: 8.1 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6064702207515521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6064702207515521 | validation: 0.6328694310244041]
	TIME [epoch: 8.1 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6148480080442197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6148480080442197 | validation: 0.6702443875748062]
	TIME [epoch: 8.1 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6546186218413587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6546186218413587 | validation: 0.748253156885831]
	TIME [epoch: 8.11 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7486816412041705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7486816412041705 | validation: 0.7353982391145539]
	TIME [epoch: 8.08 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7210661053826732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7210661053826732 | validation: 0.5521618600650974]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_257.pth
	Model improved!!!
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5631395018138372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5631395018138372 | validation: 0.5523625402287424]
	TIME [epoch: 8.12 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5614172829755427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5614172829755427 | validation: 0.5739443565145532]
	TIME [epoch: 8.12 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.55765714804223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.55765714804223 | validation: 0.5633594135232735]
	TIME [epoch: 8.16 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.575972468663559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.575972468663559 | validation: 0.6166881610053822]
	TIME [epoch: 8.14 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6263271526006389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6263271526006389 | validation: 0.7759683373207349]
	TIME [epoch: 8.13 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7841539753298219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7841539753298219 | validation: 0.6341887818073538]
	TIME [epoch: 8.13 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7363117789365924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7363117789365924 | validation: 0.5635796913595726]
	TIME [epoch: 8.12 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5624280089447696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5624280089447696 | validation: 0.569729552271117]
	TIME [epoch: 8.14 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5324391806550475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5324391806550475 | validation: 0.5369252760206255]
	TIME [epoch: 8.12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_266.pth
	Model improved!!!
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5867843714782371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5867843714782371 | validation: 0.6420334903180429]
	TIME [epoch: 8.09 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5994208020190256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5994208020190256 | validation: 0.5851834954659354]
	TIME [epoch: 8.06 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6537123884823516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6537123884823516 | validation: 0.8958275236326547]
	TIME [epoch: 8.07 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8052234415826814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8052234415826814 | validation: 0.8664720026485194]
	TIME [epoch: 8.1 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9056121422199785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9056121422199785 | validation: 0.5905879152661901]
	TIME [epoch: 8.09 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6531891071951571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6531891071951571 | validation: 0.6094267684391297]
	TIME [epoch: 8.09 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6006472329647177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6006472329647177 | validation: 0.6412087834540506]
	TIME [epoch: 8.09 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6180047958723038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6180047958723038 | validation: 0.6092430105541002]
	TIME [epoch: 8.08 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6224049508025766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6224049508025766 | validation: 0.6409704331703167]
	TIME [epoch: 8.09 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6269192432151638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6269192432151638 | validation: 0.5373491524073375]
	TIME [epoch: 8.09 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5677724559483117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5677724559483117 | validation: 0.48499490163565095]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5157394604728278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5157394604728278 | validation: 0.6169853056076466]
	TIME [epoch: 8.14 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.566738788143155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.566738788143155 | validation: 0.6168787567809159]
	TIME [epoch: 8.13 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6860257672619402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6860257672619402 | validation: 0.7714145858266752]
	TIME [epoch: 8.18 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7403301940593524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7403301940593524 | validation: 0.5324555195994171]
	TIME [epoch: 8.13 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5392728316024383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5392728316024383 | validation: 0.5296430953351435]
	TIME [epoch: 8.09 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5887482514242506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5887482514242506 | validation: 0.6147954900334507]
	TIME [epoch: 8.13 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5707401209128276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5707401209128276 | validation: 0.5199678115444308]
	TIME [epoch: 8.07 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5966280751275911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5966280751275911 | validation: 0.5834633673077243]
	TIME [epoch: 8.09 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5704050380101942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5704050380101942 | validation: 0.5697226868294746]
	TIME [epoch: 8.12 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.578521992679468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.578521992679468 | validation: 0.6545497517966121]
	TIME [epoch: 8.06 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6744142009509386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6744142009509386 | validation: 0.7712428715620808]
	TIME [epoch: 8.07 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7188592041598457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7188592041598457 | validation: 0.5508119298832425]
	TIME [epoch: 8.06 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5260542776864566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5260542776864566 | validation: 0.4788374268353808]
	TIME [epoch: 8.07 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_290.pth
	Model improved!!!
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4922091548815513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4922091548815513 | validation: 0.5308695417896515]
	TIME [epoch: 8.16 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4789706706442183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4789706706442183 | validation: 0.48113958744670704]
	TIME [epoch: 8.13 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46784783268771224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46784783268771224 | validation: 0.4606780772304367]
	TIME [epoch: 8.14 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_293.pth
	Model improved!!!
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46403900868915093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46403900868915093 | validation: 0.4655412123337058]
	TIME [epoch: 8.11 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45481105209627554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45481105209627554 | validation: 0.4664640126595685]
	TIME [epoch: 8.12 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47106068609531826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47106068609531826 | validation: 0.7735018052585354]
	TIME [epoch: 8.13 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7289075949850725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7289075949850725 | validation: 1.0382411441407455]
	TIME [epoch: 8.13 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.151459510893173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.151459510893173 | validation: 0.6507820388842707]
	TIME [epoch: 8.12 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7477838131657351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7477838131657351 | validation: 0.6679571836698159]
	TIME [epoch: 8.13 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6983077167557185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6983077167557185 | validation: 0.47739582217299664]
	TIME [epoch: 8.11 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4707107562175588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4707107562175588 | validation: 0.5929277410828115]
	TIME [epoch: 8.14 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5336333461161503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5336333461161503 | validation: 0.5893176553967705]
	TIME [epoch: 8.17 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5871868315701828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5871868315701828 | validation: 0.6323082900050285]
	TIME [epoch: 8.15 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6594429184792184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6594429184792184 | validation: 0.49309042737690395]
	TIME [epoch: 8.15 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49929241710064526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49929241710064526 | validation: 0.6294568599521727]
	TIME [epoch: 8.14 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5636606719106417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5636606719106417 | validation: 0.5421423798872633]
	TIME [epoch: 8.15 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6606615575850862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6606615575850862 | validation: 0.7544062945344424]
	TIME [epoch: 8.16 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6829214839418932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6829214839418932 | validation: 0.47289596816283325]
	TIME [epoch: 8.15 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4503598899774531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4503598899774531 | validation: 0.4989658839963349]
	TIME [epoch: 8.15 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.570665134071801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.570665134071801 | validation: 0.6258668420943629]
	TIME [epoch: 8.16 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5519976252554565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5519976252554565 | validation: 0.4675904642066906]
	TIME [epoch: 8.16 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45998553417967214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45998553417967214 | validation: 0.5538031990276775]
	TIME [epoch: 8.17 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5932971866030564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5932971866030564 | validation: 0.7604792252707329]
	TIME [epoch: 8.15 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7066654387924485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7066654387924485 | validation: 0.5826787660306628]
	TIME [epoch: 8.14 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5639593906015048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5639593906015048 | validation: 0.4757747359183091]
	TIME [epoch: 8.16 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.530427043482965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.530427043482965 | validation: 0.44223871661078634]
	TIME [epoch: 8.15 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_316.pth
	Model improved!!!
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4274633936344162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4274633936344162 | validation: 0.5136248356256069]
	TIME [epoch: 8.11 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4572269871698821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4572269871698821 | validation: 0.4225259059539941]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_318.pth
	Model improved!!!
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44453072574638447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44453072574638447 | validation: 0.45845152241703496]
	TIME [epoch: 8.12 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44274383489804736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44274383489804736 | validation: 0.5212422611817756]
	TIME [epoch: 8.14 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4775362153106659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4775362153106659 | validation: 0.5887577943136704]
	TIME [epoch: 8.14 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6632884221225266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6632884221225266 | validation: 0.592624919610347]
	TIME [epoch: 8.15 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.549636761430124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.549636761430124 | validation: 0.47743113989412045]
	TIME [epoch: 8.14 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4480336098063438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4480336098063438 | validation: 0.44846627832487446]
	TIME [epoch: 8.13 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4784759862229707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4784759862229707 | validation: 0.5610506869094284]
	TIME [epoch: 8.14 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.51273574145035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.51273574145035 | validation: 0.516883055729464]
	TIME [epoch: 8.14 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5898486316371784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5898486316371784 | validation: 0.4250842525703645]
	TIME [epoch: 8.14 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42625431317908496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42625431317908496 | validation: 0.5117229000737761]
	TIME [epoch: 8.15 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44328320747912686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44328320747912686 | validation: 0.44383116684360524]
	TIME [epoch: 8.14 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5283172736169872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5283172736169872 | validation: 0.8292019959429624]
	TIME [epoch: 8.12 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7329286056055015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7329286056055015 | validation: 0.5141680833037278]
	TIME [epoch: 8.13 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5063999673902004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5063999673902004 | validation: 0.4928261795256066]
	TIME [epoch: 8.13 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5883683564406114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5883683564406114 | validation: 0.5274392691661212]
	TIME [epoch: 8.14 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4798013868278448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4798013868278448 | validation: 0.5442125220467112]
	TIME [epoch: 8.13 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5154375056946036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5154375056946036 | validation: 0.45994588497281425]
	TIME [epoch: 8.13 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4563912772101301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4563912772101301 | validation: 0.4024475253226714]
	TIME [epoch: 8.12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_336.pth
	Model improved!!!
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4009845127011307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4009845127011307 | validation: 0.5298742692665257]
	TIME [epoch: 8.13 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46015723597057145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46015723597057145 | validation: 0.44183149829740764]
	TIME [epoch: 8.16 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49513088508698316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49513088508698316 | validation: 0.637695056179587]
	TIME [epoch: 8.14 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5597580303175602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5597580303175602 | validation: 0.41867532456584045]
	TIME [epoch: 8.14 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38814957254747373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38814957254747373 | validation: 0.39009477731234354]
	TIME [epoch: 8.13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_341.pth
	Model improved!!!
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43882507424110545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43882507424110545 | validation: 0.5767993191291774]
	TIME [epoch: 8.13 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47781361708824416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47781361708824416 | validation: 0.35641152052135]
	TIME [epoch: 8.15 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_343.pth
	Model improved!!!
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3526307370079868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3526307370079868 | validation: 0.35590559667078225]
	TIME [epoch: 8.14 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_344.pth
	Model improved!!!
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3379893206354073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3379893206354073 | validation: 0.3726077242624686]
	TIME [epoch: 8.13 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3310601531293269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3310601531293269 | validation: 0.33207774814271285]
	TIME [epoch: 8.12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_346.pth
	Model improved!!!
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3485578558361837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3485578558361837 | validation: 0.6155128080264781]
	TIME [epoch: 8.12 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5088606932017906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5088606932017906 | validation: 0.5394564236002161]
	TIME [epoch: 8.13 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6739058499068308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6739058499068308 | validation: 0.9024531700606682]
	TIME [epoch: 8.12 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8352457505769764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8352457505769764 | validation: 0.47261947852724984]
	TIME [epoch: 8.11 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4066078587805828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4066078587805828 | validation: 0.5128841221409645]
	TIME [epoch: 8.11 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6272514341192647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6272514341192647 | validation: 0.5101818412921418]
	TIME [epoch: 8.09 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4618375598945862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4618375598945862 | validation: 0.5692443623961753]
	TIME [epoch: 8.15 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5303451422046044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5303451422046044 | validation: 0.38264974323929096]
	TIME [epoch: 8.1 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4241519064139384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4241519064139384 | validation: 0.3438399503038611]
	TIME [epoch: 8.1 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3537165443289502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3537165443289502 | validation: 0.45182905780138505]
	TIME [epoch: 8.09 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40065010306112103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40065010306112103 | validation: 0.41177687339869584]
	TIME [epoch: 8.12 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40980678264656517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40980678264656517 | validation: 0.4259725252594988]
	TIME [epoch: 8.11 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4049552523770409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4049552523770409 | validation: 0.4343454015895229]
	TIME [epoch: 8.12 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4330471983296468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4330471983296468 | validation: 0.4185102578283937]
	TIME [epoch: 8.11 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.393114428021007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.393114428021007 | validation: 0.39185764873282164]
	TIME [epoch: 8.11 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40728620030162693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40728620030162693 | validation: 0.5854254463676]
	TIME [epoch: 8.11 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4952286020702758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4952286020702758 | validation: 0.44021704768488334]
	TIME [epoch: 8.11 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5017780118702706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5017780118702706 | validation: 0.44108400765620726]
	TIME [epoch: 8.11 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3902453386777057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3902453386777057 | validation: 0.3294914506609858]
	TIME [epoch: 8.16 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_365.pth
	Model improved!!!
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.333641785504381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.333641785504381 | validation: 0.3366803564997649]
	TIME [epoch: 8.14 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3162165810420751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3162165810420751 | validation: 0.35424551394923287]
	TIME [epoch: 8.15 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35627688157369436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35627688157369436 | validation: 0.435084039281605]
	TIME [epoch: 8.15 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.420597880048184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.420597880048184 | validation: 0.5573795932192269]
	TIME [epoch: 8.22 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5376732170704369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5376732170704369 | validation: 0.4535120100752133]
	TIME [epoch: 8.15 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5225759202762701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5225759202762701 | validation: 0.7401279384479236]
	TIME [epoch: 8.15 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6885644737289504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6885644737289504 | validation: 0.3351884092621299]
	TIME [epoch: 8.14 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36882609707460207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36882609707460207 | validation: 0.35219267274402316]
	TIME [epoch: 8.15 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32315181414742894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32315181414742894 | validation: 0.31639583800053367]
	TIME [epoch: 8.16 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_374.pth
	Model improved!!!
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29948572307205823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29948572307205823 | validation: 0.30833016622230436]
	TIME [epoch: 8.14 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_375.pth
	Model improved!!!
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.284295240978143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.284295240978143 | validation: 0.30700779082253704]
	TIME [epoch: 8.15 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_376.pth
	Model improved!!!
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2950215070251373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2950215070251373 | validation: 0.4997357480886098]
	TIME [epoch: 8.13 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42800784830182687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42800784830182687 | validation: 0.74682497974899]
	TIME [epoch: 8.14 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8834132184104461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8834132184104461 | validation: 0.4485557454651803]
	TIME [epoch: 8.15 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4717097902663865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4717097902663865 | validation: 0.3246923953366554]
	TIME [epoch: 8.14 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32607165830625845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32607165830625845 | validation: 0.4161177460837566]
	TIME [epoch: 8.13 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43106543386557333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43106543386557333 | validation: 0.3506652958290192]
	TIME [epoch: 8.13 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31361702326422375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31361702326422375 | validation: 0.3933350757953636]
	TIME [epoch: 8.13 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46541423154184314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46541423154184314 | validation: 0.46037885400598466]
	TIME [epoch: 8.14 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40626040282456155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40626040282456155 | validation: 0.36491450839258016]
	TIME [epoch: 8.14 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37623091084153837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37623091084153837 | validation: 0.625562066636044]
	TIME [epoch: 8.13 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5365542952607435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5365542952607435 | validation: 0.4504372264148331]
	TIME [epoch: 8.09 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4777835333160951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4777835333160951 | validation: 0.404579068834096]
	TIME [epoch: 8.11 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3604343880194319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3604343880194319 | validation: 0.28403241213813074]
	TIME [epoch: 8.13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_389.pth
	Model improved!!!
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2697682098976268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2697682098976268 | validation: 0.2647834541821797]
	TIME [epoch: 8.15 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_390.pth
	Model improved!!!
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25392477558606397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25392477558606397 | validation: 0.29206269104044574]
	TIME [epoch: 8.14 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25609043690921274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25609043690921274 | validation: 0.312448503361721]
	TIME [epoch: 8.14 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31710396979019423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31710396979019423 | validation: 0.8188642617950941]
	TIME [epoch: 8.13 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6843296193432873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6843296193432873 | validation: 0.4724755108928504]
	TIME [epoch: 8.14 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5366635521301563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5366635521301563 | validation: 0.4250957502913936]
	TIME [epoch: 8.15 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40722831475290533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40722831475290533 | validation: 0.32867558581948236]
	TIME [epoch: 8.13 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36427230068566263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36427230068566263 | validation: 0.6536567547881793]
	TIME [epoch: 8.13 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.545219526824417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.545219526824417 | validation: 0.2711760465207695]
	TIME [epoch: 8.14 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3176032558618996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3176032558618996 | validation: 0.3114287943102332]
	TIME [epoch: 8.14 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2701782905590005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2701782905590005 | validation: 0.31438500267966524]
	TIME [epoch: 8.15 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3134582451532313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3134582451532313 | validation: 0.46660812676921504]
	TIME [epoch: 8.14 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3943990389470704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3943990389470704 | validation: 0.47081588542981995]
	TIME [epoch: 8.13 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5432530727344654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5432530727344654 | validation: 0.7299539426702155]
	TIME [epoch: 8.14 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.617953345236853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.617953345236853 | validation: 0.2601763892112075]
	TIME [epoch: 8.14 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_404.pth
	Model improved!!!
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25972506841525217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25972506841525217 | validation: 0.28736918557706687]
	TIME [epoch: 8.15 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32371764657383295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32371764657383295 | validation: 0.6599414475295917]
	TIME [epoch: 8.12 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5490535098219185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5490535098219185 | validation: 0.32143709628681233]
	TIME [epoch: 8.12 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3227353121326605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3227353121326605 | validation: 0.2878786953630006]
	TIME [epoch: 8.12 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2644444158629571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2644444158629571 | validation: 0.27285551766136423]
	TIME [epoch: 8.13 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2526202592751242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2526202592751242 | validation: 0.3284755932807209]
	TIME [epoch: 8.14 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2736089449134488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2736089449134488 | validation: 0.36772515419205054]
	TIME [epoch: 8.13 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40844247360671676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40844247360671676 | validation: 0.5540008148099007]
	TIME [epoch: 8.13 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42184710059417085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42184710059417085 | validation: 0.3734051032617098]
	TIME [epoch: 8.13 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4361140702434014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4361140702434014 | validation: 0.5087535914336939]
	TIME [epoch: 8.13 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39626328207841943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39626328207841943 | validation: 0.2586872903067473]
	TIME [epoch: 8.15 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_415.pth
	Model improved!!!
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2691487618324579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2691487618324579 | validation: 0.3052867055487375]
	TIME [epoch: 8.13 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30105836947788567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30105836947788567 | validation: 0.43566518727434034]
	TIME [epoch: 8.13 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3695483941656485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3695483941656485 | validation: 0.3335932501058354]
	TIME [epoch: 8.13 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3545860383403702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3545860383403702 | validation: 0.4171587988038843]
	TIME [epoch: 8.13 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31898663269662686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31898663269662686 | validation: 0.28978383735934127]
	TIME [epoch: 8.14 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34101607262334244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34101607262334244 | validation: 0.45842506960479795]
	TIME [epoch: 8.14 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35421504802315745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35421504802315745 | validation: 0.27959890344127497]
	TIME [epoch: 8.12 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29757537684133706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29757537684133706 | validation: 0.3624525577928011]
	TIME [epoch: 8.12 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3143544494392307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3143544494392307 | validation: 0.32219328819204684]
	TIME [epoch: 8.12 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32818639152792417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32818639152792417 | validation: 0.3468983067890914]
	TIME [epoch: 8.13 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.297382161360346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.297382161360346 | validation: 0.29688340194547486]
	TIME [epoch: 8.15 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3074724177734591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3074724177734591 | validation: 0.36687081642905994]
	TIME [epoch: 8.12 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.277647357410859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.277647357410859 | validation: 0.34563091963940007]
	TIME [epoch: 8.13 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44139849146887405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44139849146887405 | validation: 0.5638097818340505]
	TIME [epoch: 8.1 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44547997279405677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44547997279405677 | validation: 0.27664631902725256]
	TIME [epoch: 8.14 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2817678234428988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2817678234428988 | validation: 0.24946273647547879]
	TIME [epoch: 8.14 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_431.pth
	Model improved!!!
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23206452126405072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23206452126405072 | validation: 0.2701427176690385]
	TIME [epoch: 8.12 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2544257752465973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2544257752465973 | validation: 0.2765714491491674]
	TIME [epoch: 8.12 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24354278218270922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24354278218270922 | validation: 0.25964694029665236]
	TIME [epoch: 8.12 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27019405711044847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27019405711044847 | validation: 0.6385870073277133]
	TIME [epoch: 8.13 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.506425437333952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.506425437333952 | validation: 0.44024159113415706]
	TIME [epoch: 8.13 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5557507248333358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5557507248333358 | validation: 0.59871281780686]
	TIME [epoch: 8.13 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.522351219538479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.522351219538479 | validation: 0.40186102885810976]
	TIME [epoch: 8.1 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3549081238396351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3549081238396351 | validation: 0.39895814976733124]
	TIME [epoch: 8.12 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4413951863438763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4413951863438763 | validation: 0.29929984424190614]
	TIME [epoch: 8.1 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26364882571873005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26364882571873005 | validation: 0.18546276445961163]
	TIME [epoch: 8.14 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_441.pth
	Model improved!!!
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21244344085921696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21244344085921696 | validation: 0.2389784940147809]
	TIME [epoch: 8.11 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21127489530688612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21127489530688612 | validation: 0.19345000344668753]
	TIME [epoch: 8.11 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22373752519351875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22373752519351875 | validation: 0.45062267754781005]
	TIME [epoch: 8.12 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3332406300728323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3332406300728323 | validation: 0.4501936573656773]
	TIME [epoch: 8.1 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5445245227418741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5445245227418741 | validation: 0.3406583905128343]
	TIME [epoch: 8.13 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2641138764992237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2641138764992237 | validation: 0.20222715496373223]
	TIME [epoch: 8.13 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2037518657635894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2037518657635894 | validation: 0.2259219997759499]
	TIME [epoch: 8.13 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1932682591199276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1932682591199276 | validation: 0.20565141149191501]
	TIME [epoch: 8.11 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22350603792771268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22350603792771268 | validation: 0.5129725799769871]
	TIME [epoch: 8.13 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40115276406364403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40115276406364403 | validation: 0.38473203472678874]
	TIME [epoch: 8.14 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4722077671394635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4722077671394635 | validation: 0.3305933708594474]
	TIME [epoch: 8.15 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24321027868799924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24321027868799924 | validation: 0.267091478522394]
	TIME [epoch: 8.13 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29959777947580507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29959777947580507 | validation: 0.5646976159747669]
	TIME [epoch: 8.13 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46653660922953794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46653660922953794 | validation: 0.3506976565015894]
	TIME [epoch: 8.12 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4039908186950227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4039908186950227 | validation: 0.4298269394845681]
	TIME [epoch: 8.1 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33323824693589843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33323824693589843 | validation: 0.24832892290635566]
	TIME [epoch: 8.13 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27245930567795057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27245930567795057 | validation: 0.21903189755227262]
	TIME [epoch: 8.13 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24592483980481958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24592483980481958 | validation: 0.4692344176324456]
	TIME [epoch: 8.12 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3757233448957971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3757233448957971 | validation: 0.2973060906758243]
	TIME [epoch: 8.13 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3599512997566846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3599512997566846 | validation: 0.2854854957912659]
	TIME [epoch: 8.12 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.234034157542415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.234034157542415 | validation: 0.24797926042273222]
	TIME [epoch: 8.15 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22861463888176134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22861463888176134 | validation: 0.294778776244261]
	TIME [epoch: 8.1 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24812717674338833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24812717674338833 | validation: 0.2781349390089611]
	TIME [epoch: 8.13 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31549855104793517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31549855104793517 | validation: 0.47242977513444306]
	TIME [epoch: 8.11 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34345157686696753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34345157686696753 | validation: 0.3287835384393897]
	TIME [epoch: 8.13 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4160098582227573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4160098582227573 | validation: 0.3268813956439259]
	TIME [epoch: 8.09 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24688994745981618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24688994745981618 | validation: 0.19238429543864843]
	TIME [epoch: 8.12 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20660161396514412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20660161396514412 | validation: 0.21192921059686864]
	TIME [epoch: 8.12 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.191163420236428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.191163420236428 | validation: 0.22589506210681784]
	TIME [epoch: 8.13 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18603177053201386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18603177053201386 | validation: 0.23088996180401733]
	TIME [epoch: 8.12 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22988256457556241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22988256457556241 | validation: 0.4804956003933931]
	TIME [epoch: 8.12 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39182403189259674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39182403189259674 | validation: 0.37267219593872875]
	TIME [epoch: 8.14 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4332172878029195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4332172878029195 | validation: 0.6332669574473544]
	TIME [epoch: 8.12 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.470697734919465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.470697734919465 | validation: 0.5314443161206278]
	TIME [epoch: 8.12 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5679927896812452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5679927896812452 | validation: 0.4894312261616179]
	TIME [epoch: 8.12 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.423086540120428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.423086540120428 | validation: 0.22457741342683102]
	TIME [epoch: 8.13 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2286680217281328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2286680217281328 | validation: 0.2442990662034571]
	TIME [epoch: 8.12 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26783417881181676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26783417881181676 | validation: 0.36907877444964216]
	TIME [epoch: 8.08 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3055502891042042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3055502891042042 | validation: 0.31858500138457374]
	TIME [epoch: 8.11 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40516527339873754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40516527339873754 | validation: 0.3493404538852425]
	TIME [epoch: 8.08 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29178682132934486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29178682132934486 | validation: 0.21101185545783407]
	TIME [epoch: 8.11 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21936089736295308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21936089736295308 | validation: 0.2413979683147255]
	TIME [epoch: 8.11 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19734560318785466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19734560318785466 | validation: 0.22822503672750516]
	TIME [epoch: 8.11 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2725863395741443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2725863395741443 | validation: 0.5213442286094345]
	TIME [epoch: 8.08 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3908177096868124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3908177096868124 | validation: 0.2886852263792377]
	TIME [epoch: 8.11 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33176393467569093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33176393467569093 | validation: 0.41303966000228753]
	TIME [epoch: 8.09 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31597083998198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31597083998198 | validation: 0.22801504459501692]
	TIME [epoch: 8.13 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.250463317817642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.250463317817642 | validation: 0.2116265238923144]
	TIME [epoch: 8.08 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20544572032126862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20544572032126862 | validation: 0.2353647128290149]
	TIME [epoch: 8.11 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20836650667990333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20836650667990333 | validation: 0.20182006813768316]
	TIME [epoch: 8.09 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1957232160015263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1957232160015263 | validation: 0.2780107214718378]
	TIME [epoch: 8.12 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21715314005185019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21715314005185019 | validation: 0.20906255835557253]
	TIME [epoch: 8.14 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2248155266891096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2248155266891096 | validation: 0.313729192584985]
	TIME [epoch: 8.15 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22636766326388355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22636766326388355 | validation: 0.26174088432292403]
	TIME [epoch: 8.13 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3338021008928476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3338021008928476 | validation: 0.8541424661668942]
	TIME [epoch: 8.13 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6750615961608506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6750615961608506 | validation: 0.31983771440374]
	TIME [epoch: 8.13 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31550294785673666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31550294785673666 | validation: 0.2046191665473685]
	TIME [epoch: 8.13 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23216222051117943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23216222051117943 | validation: 0.4555157406591104]
	TIME [epoch: 8.14 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3702580706235844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3702580706235844 | validation: 0.33553396050537276]
	TIME [epoch: 8.13 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3857333103558448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3857333103558448 | validation: 0.2990751012363508]
	TIME [epoch: 57.2 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25321875939216076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25321875939216076 | validation: 0.19417638249753366]
	TIME [epoch: 17.4 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21809588951893136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21809588951893136 | validation: 0.2785882506401664]
	TIME [epoch: 17.4 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1984703318203555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1984703318203555 | validation: 0.20448695586479348]
	TIME [epoch: 17.5 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26568632412886595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26568632412886595 | validation: 0.5035815454840689]
	TIME [epoch: 17.5 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35546024211936206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35546024211936206 | validation: 0.31495927803665746]
	TIME [epoch: 17.4 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3503592277748972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3503592277748972 | validation: 0.34254941865540717]
	TIME [epoch: 17.4 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24295232062337765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24295232062337765 | validation: 0.1832718499242867]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_508.pth
	Model improved!!!
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18154624235311026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18154624235311026 | validation: 0.20600746442753737]
	TIME [epoch: 17.5 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16194434427641471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16194434427641471 | validation: 0.20080975087920674]
	TIME [epoch: 17.5 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18264620564842596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18264620564842596 | validation: 0.29907538353427715]
	TIME [epoch: 17.4 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25032755167405735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25032755167405735 | validation: 0.2981124116799897]
	TIME [epoch: 17.5 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34852330672932086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34852330672932086 | validation: 0.43273629529917335]
	TIME [epoch: 17.4 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.308380667804707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.308380667804707 | validation: 0.36423736029780723]
	TIME [epoch: 17.5 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4149247727836699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4149247727836699 | validation: 0.4169634285962218]
	TIME [epoch: 17.4 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3229958451887135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3229958451887135 | validation: 0.2738903590607323]
	TIME [epoch: 17.8 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27921084167583676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27921084167583676 | validation: 0.3077351640375345]
	TIME [epoch: 17.4 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2518665440998519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2518665440998519 | validation: 0.1804054515169693]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_518.pth
	Model improved!!!
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20442620027574585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20442620027574585 | validation: 0.26324695313075663]
	TIME [epoch: 17.5 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20345786385413078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20345786385413078 | validation: 0.2533607600949827]
	TIME [epoch: 17.5 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2999124462622391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2999124462622391 | validation: 0.5365853844721203]
	TIME [epoch: 17.4 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39471311784275304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39471311784275304 | validation: 0.21475360819326783]
	TIME [epoch: 17.5 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2484310757871833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2484310757871833 | validation: 0.23570811236249176]
	TIME [epoch: 17.4 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18375229476270658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18375229476270658 | validation: 0.1850144107766421]
	TIME [epoch: 17.4 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1867368647586953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1867368647586953 | validation: 0.2041836306186068]
	TIME [epoch: 17.5 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17519165562634967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17519165562634967 | validation: 0.19534088747555586]
	TIME [epoch: 17.5 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21068684474136207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21068684474136207 | validation: 0.42287902859790416]
	TIME [epoch: 17.5 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31135669007142924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31135669007142924 | validation: 0.37095705405539264]
	TIME [epoch: 17.4 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4792737264532265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4792737264532265 | validation: 0.377573757269862]
	TIME [epoch: 17.4 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3007674178131687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3007674178131687 | validation: 0.23577746934340757]
	TIME [epoch: 17.5 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21438912522389245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21438912522389245 | validation: 0.17284716502495218]
	TIME [epoch: 17.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_531.pth
	Model improved!!!
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19566866756875168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19566866756875168 | validation: 0.3109312442055993]
	TIME [epoch: 17.4 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23074694372696422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23074694372696422 | validation: 0.3021165641988116]
	TIME [epoch: 17.4 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3790332399525768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3790332399525768 | validation: 0.5162726671360928]
	TIME [epoch: 17.4 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3624025400254361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3624025400254361 | validation: 0.19289868474267793]
	TIME [epoch: 17.4 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2276051624591948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2276051624591948 | validation: 0.4628195409683472]
	TIME [epoch: 17.4 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34993517593199885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34993517593199885 | validation: 0.22186675708670062]
	TIME [epoch: 17.4 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23335678310650312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23335678310650312 | validation: 0.2060451088201709]
	TIME [epoch: 17.5 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1925970805506738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1925970805506738 | validation: 0.20759230400269157]
	TIME [epoch: 17.4 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17653671572906116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17653671572906116 | validation: 0.1652345076832957]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_540.pth
	Model improved!!!
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15111488095185252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15111488095185252 | validation: 0.13509994841176806]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_541.pth
	Model improved!!!
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1594517335812369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1594517335812369 | validation: 0.37465677920521756]
	TIME [epoch: 17.4 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24353614624740189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24353614624740189 | validation: 0.3734012635987379]
	TIME [epoch: 17.4 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4688339114707564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4688339114707564 | validation: 0.5224252988151862]
	TIME [epoch: 17.4 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38308571871379615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38308571871379615 | validation: 0.1816820442380398]
	TIME [epoch: 17.4 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17202877922954393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17202877922954393 | validation: 0.1813315173624584]
	TIME [epoch: 17.4 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20386927515259642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20386927515259642 | validation: 0.32264167611391265]
	TIME [epoch: 17.4 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23905045094756344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23905045094756344 | validation: 0.2577559790449093]
	TIME [epoch: 17.4 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27809505643190213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27809505643190213 | validation: 0.31885557509967516]
	TIME [epoch: 17.4 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2619559216236625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2619559216236625 | validation: 0.2344388476266609]
	TIME [epoch: 17.4 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24023273242544094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24023273242544094 | validation: 0.21772653486793248]
	TIME [epoch: 17.4 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16555665581997978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16555665581997978 | validation: 0.15992441273727873]
	TIME [epoch: 17.4 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15579436099938362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15579436099938362 | validation: 0.1728031490016488]
	TIME [epoch: 17.4 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1464941584132814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1464941584132814 | validation: 0.13807366686426734]
	TIME [epoch: 17.4 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14299040658704815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14299040658704815 | validation: 0.23067394055794654]
	TIME [epoch: 17.3 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15887991429351764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15887991429351764 | validation: 0.22796834178039274]
	TIME [epoch: 17.4 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2657150518879575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2657150518879575 | validation: 0.9283813299552384]
	TIME [epoch: 17.4 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7238033136973349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7238033136973349 | validation: 0.27290833013695626]
	TIME [epoch: 17.4 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2575282770112183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2575282770112183 | validation: 0.29367970351540107]
	TIME [epoch: 17.4 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29815121854906906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29815121854906906 | validation: 0.2733581235782495]
	TIME [epoch: 17.4 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23112670752442235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23112670752442235 | validation: 0.16922221399116544]
	TIME [epoch: 17.4 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1502181640854493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1502181640854493 | validation: 0.15744247246870263]
	TIME [epoch: 17.4 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14616891068396023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14616891068396023 | validation: 0.1996097459321749]
	TIME [epoch: 17.4 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15934948531727336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15934948531727336 | validation: 0.15895950165657957]
	TIME [epoch: 17.4 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1703578834020113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1703578834020113 | validation: 0.3175957167742105]
	TIME [epoch: 17.4 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2235528256702817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2235528256702817 | validation: 0.32221178552918456]
	TIME [epoch: 17.4 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35918076812890914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35918076812890914 | validation: 0.5747284062234511]
	TIME [epoch: 17.4 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4478389416939336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4478389416939336 | validation: 0.3817303166680257]
	TIME [epoch: 17.4 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4117513220184876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4117513220184876 | validation: 0.2455572445123323]
	TIME [epoch: 17.4 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2293615150714262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2293615150714262 | validation: 0.23097473478297104]
	TIME [epoch: 17.4 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2278714307551758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2278714307551758 | validation: 0.2232960316869976]
	TIME [epoch: 17.4 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24307504713104647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24307504713104647 | validation: 0.34073883878176536]
	TIME [epoch: 17.4 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25242121508372894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25242121508372894 | validation: 0.2295621642294755]
	TIME [epoch: 17.4 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2553113333058751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2553113333058751 | validation: 0.5716199569669403]
	TIME [epoch: 17.4 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39498231446392157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39498231446392157 | validation: 0.21784710526972156]
	TIME [epoch: 17.4 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23195562391726682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23195562391726682 | validation: 0.19482666220617026]
	TIME [epoch: 17.4 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1759685001149718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1759685001149718 | validation: 0.18570742567190257]
	TIME [epoch: 17.4 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15938518287456424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15938518287456424 | validation: 0.13097367204809748]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_578.pth
	Model improved!!!
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14305769012740294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14305769012740294 | validation: 0.23358147823017755]
	TIME [epoch: 17.4 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1549973300366128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1549973300366128 | validation: 0.20192727836677277]
	TIME [epoch: 17.4 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2778746740472811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2778746740472811 | validation: 0.6551290866934167]
	TIME [epoch: 17.4 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4895733420894872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4895733420894872 | validation: 0.166462048675589]
	TIME [epoch: 17.4 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1761790805798699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1761790805798699 | validation: 0.1850984235854472]
	TIME [epoch: 17.4 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2170779960671116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2170779960671116 | validation: 0.22473181939219786]
	TIME [epoch: 17.4 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16552000915493573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16552000915493573 | validation: 0.1393143250298814]
	TIME [epoch: 17.5 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14628542186547513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14628542186547513 | validation: 0.21416587291459335]
	TIME [epoch: 17.4 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15160969208820188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15160969208820188 | validation: 0.1761938311738875]
	TIME [epoch: 17.4 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22722957579205677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22722957579205677 | validation: 0.9612814849371291]
	TIME [epoch: 17.4 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7096326923295484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7096326923295484 | validation: 0.2401779289821013]
	TIME [epoch: 17.4 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23393229438069807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23393229438069807 | validation: 0.23971072486221826]
	TIME [epoch: 17.4 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24509584171256188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24509584171256188 | validation: 0.2051315733393466]
	TIME [epoch: 17.4 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16553917490646586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16553917490646586 | validation: 0.17583812447694713]
	TIME [epoch: 17.4 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14756044913033392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14756044913033392 | validation: 0.1739471076216586]
	TIME [epoch: 17.4 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1821952782399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1821952782399 | validation: 0.2965891692554065]
	TIME [epoch: 17.4 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20138784277143462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20138784277143462 | validation: 0.17301727061418812]
	TIME [epoch: 17.4 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24492252594169578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24492252594169578 | validation: 0.6057703248601847]
	TIME [epoch: 17.5 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43750260374673233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43750260374673233 | validation: 0.3030764872580105]
	TIME [epoch: 17.4 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3402849149949356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3402849149949356 | validation: 0.16999594111662072]
	TIME [epoch: 17.4 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16128094978187876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16128094978187876 | validation: 0.330397065956566]
	TIME [epoch: 17.4 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23589267209713716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23589267209713716 | validation: 0.24302307532248807]
	TIME [epoch: 17.4 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2910787671127809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2910787671127809 | validation: 0.27829671355108027]
	TIME [epoch: 17.3 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2025428458606152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2025428458606152 | validation: 0.14279789704379983]
	TIME [epoch: 17.3 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16683849037688994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16683849037688994 | validation: 0.2842595871271016]
	TIME [epoch: 17.4 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1804506763817368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1804506763817368 | validation: 0.18849493546722493]
	TIME [epoch: 17.3 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24558871613910818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24558871613910818 | validation: 0.4502186137602336]
	TIME [epoch: 17.3 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3031364593185257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3031364593185257 | validation: 0.19953525384500498]
	TIME [epoch: 17.4 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2266159222745108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2266159222745108 | validation: 0.2697304635650271]
	TIME [epoch: 17.4 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21444046632526245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21444046632526245 | validation: 0.20512672029982926]
	TIME [epoch: 17.3 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20150361829779026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20150361829779026 | validation: 0.15856035207013244]
	TIME [epoch: 17.4 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1844142594244366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1844142594244366 | validation: 0.5494324920382456]
	TIME [epoch: 17.3 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37766201286649675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37766201286649675 | validation: 0.1443587032302038]
	TIME [epoch: 17.3 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18314476114335854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18314476114335854 | validation: 0.18204859410021892]
	TIME [epoch: 17.4 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15980030178069893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15980030178069893 | validation: 0.15717635417595144]
	TIME [epoch: 17.3 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13832203466628237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13832203466628237 | validation: 0.14502044590049498]
	TIME [epoch: 17.3 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12629730071283202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12629730071283202 | validation: 0.11717920735190912]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_615.pth
	Model improved!!!
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12844379842662812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12844379842662812 | validation: 0.28885876516098863]
	TIME [epoch: 17.4 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17549458966722994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17549458966722994 | validation: 0.3057986393331155]
	TIME [epoch: 17.4 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40136171088740286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40136171088740286 | validation: 0.38833878935486155]
	TIME [epoch: 17.4 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26256347078747483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26256347078747483 | validation: 0.18001467002163207]
	TIME [epoch: 17.4 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19113525073508125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19113525073508125 | validation: 0.27159475210491624]
	TIME [epoch: 17.4 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17756716308253487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17756716308253487 | validation: 0.17673783059727666]
	TIME [epoch: 17.4 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23056413755626312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23056413755626312 | validation: 0.3762760212130605]
	TIME [epoch: 17.4 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2255843663154672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2255843663154672 | validation: 0.18499279176705705]
	TIME [epoch: 17.4 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23212224583556737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23212224583556737 | validation: 0.2931560715371654]
	TIME [epoch: 17.4 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18801479498855123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18801479498855123 | validation: 0.13854423914586597]
	TIME [epoch: 17.4 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15742445209753808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15742445209753808 | validation: 0.20163228623882523]
	TIME [epoch: 17.4 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13832559460074786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13832559460074786 | validation: 0.1142141862635743]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_627.pth
	Model improved!!!
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1581822399048217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1581822399048217 | validation: 0.2377029764076724]
	TIME [epoch: 17.4 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1896629489655839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1896629489655839 | validation: 0.19913023724838408]
	TIME [epoch: 17.4 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1784845840730922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1784845840730922 | validation: 0.21793923664742662]
	TIME [epoch: 17.4 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1662857745107858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1662857745107858 | validation: 0.22383851134441432]
	TIME [epoch: 17.4 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28066333603685684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28066333603685684 | validation: 0.5285143318858969]
	TIME [epoch: 17.4 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3770822674894791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3770822674894791 | validation: 0.17159757238249376]
	TIME [epoch: 17.4 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20102887846335687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20102887846335687 | validation: 0.7165421894657609]
	TIME [epoch: 17.4 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5346598189529378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5346598189529378 | validation: 0.18284347910388]
	TIME [epoch: 17.4 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15266843718593573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15266843718593573 | validation: 0.21438310507510275]
	TIME [epoch: 17.4 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2614360025408533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2614360025408533 | validation: 0.43763623252391337]
	TIME [epoch: 17.4 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3078408730552187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3078408730552187 | validation: 0.19689267213184675]
	TIME [epoch: 17.5 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2201977033052898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2201977033052898 | validation: 0.13294753344111873]
	TIME [epoch: 17.4 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16281887341547058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16281887341547058 | validation: 0.21296747096380553]
	TIME [epoch: 17.4 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1689653233452056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1689653233452056 | validation: 0.14660146665330795]
	TIME [epoch: 17.4 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15071363677669994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15071363677669994 | validation: 0.14696671180654872]
	TIME [epoch: 17.4 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13151078251337261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13151078251337261 | validation: 0.15355533405956406]
	TIME [epoch: 17.4 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11848560328961526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11848560328961526 | validation: 0.13072281739706113]
	TIME [epoch: 17.4 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11830354102390434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11830354102390434 | validation: 0.20662952712978894]
	TIME [epoch: 17.4 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13601725254604055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13601725254604055 | validation: 0.17531521693966298]
	TIME [epoch: 17.4 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22901164668073468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22901164668073468 | validation: 0.7011449468556045]
	TIME [epoch: 17.4 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.533976402300939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.533976402300939 | validation: 0.1671900836759133]
	TIME [epoch: 17.4 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15783274499526492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15783274499526492 | validation: 0.22023054721185537]
	TIME [epoch: 17.4 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2470659764775371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2470659764775371 | validation: 0.2625296779947298]
	TIME [epoch: 17.4 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1849678477930735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1849678477930735 | validation: 0.160443332447753]
	TIME [epoch: 17.4 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16082303782309595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16082303782309595 | validation: 0.2674386946791105]
	TIME [epoch: 17.4 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21136508032626353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21136508032626353 | validation: 0.16488432262228492]
	TIME [epoch: 17.4 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18015112488106405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18015112488106405 | validation: 0.24285803139293327]
	TIME [epoch: 17.4 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1467163987396983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1467163987396983 | validation: 0.12955025030511513]
	TIME [epoch: 17.4 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16903926338605213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16903926338605213 | validation: 0.3910606772055881]
	TIME [epoch: 17.4 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23980986539748692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23980986539748692 | validation: 0.22379588108863258]
	TIME [epoch: 17.4 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26074338007467707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26074338007467707 | validation: 0.3305406651866205]
	TIME [epoch: 17.4 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22682884121497973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22682884121497973 | validation: 0.15316042933211904]
	TIME [epoch: 17.4 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1419427077156323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1419427077156323 | validation: 0.14645855255625065]
	TIME [epoch: 17.5 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17265863855484065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17265863855484065 | validation: 0.2980896100682074]
	TIME [epoch: 17.4 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18119801892425413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18119801892425413 | validation: 0.1476457074770679]
	TIME [epoch: 17.4 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16566286112630274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16566286112630274 | validation: 0.30843764671804047]
	TIME [epoch: 17.4 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21044971871551166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21044971871551166 | validation: 0.15482938335694174]
	TIME [epoch: 17.4 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2004958463725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2004958463725 | validation: 0.21518149381504192]
	TIME [epoch: 17.5 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15252022014849215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15252022014849215 | validation: 0.10880729451285731]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_666.pth
	Model improved!!!
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1261737205372972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1261737205372972 | validation: 0.1726118466692671]
	TIME [epoch: 17.4 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12216890349093797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12216890349093797 | validation: 0.11148349675064993]
	TIME [epoch: 17.5 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15608316435137634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15608316435137634 | validation: 0.43355257828389715]
	TIME [epoch: 17.4 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26810295214284674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26810295214284674 | validation: 0.28801886402865007]
	TIME [epoch: 17.4 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32417086331579237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32417086331579237 | validation: 0.19654414648508572]
	TIME [epoch: 17.4 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2014768306760569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2014768306760569 | validation: 0.2017412685623429]
	TIME [epoch: 17.4 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16391165577683736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16391165577683736 | validation: 0.11651260102978547]
	TIME [epoch: 17.4 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11871810982717787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11871810982717787 | validation: 0.18742967254103146]
	TIME [epoch: 17.4 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11959447111097266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11959447111097266 | validation: 0.10515040632403548]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_675.pth
	Model improved!!!
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1630058685139492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1630058685139492 | validation: 0.5543392743912571]
	TIME [epoch: 17.4 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37482168689185363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37482168689185363 | validation: 0.28621161025610425]
	TIME [epoch: 17.3 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3393175706934421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3393175706934421 | validation: 0.533325131050646]
	TIME [epoch: 17.4 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38913373907187393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38913373907187393 | validation: 0.3033021157001974]
	TIME [epoch: 17.4 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2497779049380229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2497779049380229 | validation: 0.1722398258031592]
	TIME [epoch: 17.4 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20166221203953577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20166221203953577 | validation: 0.14975581659491113]
	TIME [epoch: 17.4 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.175230073362881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.175230073362881 | validation: 0.1641192928403722]
	TIME [epoch: 17.4 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.127707229724143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.127707229724143 | validation: 0.13880574510474777]
	TIME [epoch: 17.4 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12254370211940238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12254370211940238 | validation: 0.20081404172779058]
	TIME [epoch: 17.3 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13810288273031918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13810288273031918 | validation: 0.15552825553041463]
	TIME [epoch: 17.4 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2194356212759931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2194356212759931 | validation: 0.3868779574681248]
	TIME [epoch: 17.4 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24075494583330576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24075494583330576 | validation: 0.2214633758207973]
	TIME [epoch: 17.4 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23900791854990075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23900791854990075 | validation: 0.17540474960359634]
	TIME [epoch: 17.4 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.157899304504977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.157899304504977 | validation: 0.15404584271164945]
	TIME [epoch: 17.4 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14145462420911606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14145462420911606 | validation: 0.12384573963256035]
	TIME [epoch: 17.4 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14116625229419238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14116625229419238 | validation: 0.26300558036656546]
	TIME [epoch: 17.4 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20968939747873386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20968939747873386 | validation: 0.21746573079574547]
	TIME [epoch: 17.4 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20252608460236546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20252608460236546 | validation: 0.12626565733156597]
	TIME [epoch: 17.4 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11812873447022612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11812873447022612 | validation: 0.10447092568284791]
	TIME [epoch: 17.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_694.pth
	Model improved!!!
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10739779002124769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10739779002124769 | validation: 0.16334677224612434]
	TIME [epoch: 17.5 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1128098635514408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1128098635514408 | validation: 0.12351714807618747]
	TIME [epoch: 17.5 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16284182064114242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16284182064114242 | validation: 0.5906005496763032]
	TIME [epoch: 17.5 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39439200886867365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39439200886867365 | validation: 0.3395686166557309]
	TIME [epoch: 17.4 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37388974144806225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37388974144806225 | validation: 0.21586756248984412]
	TIME [epoch: 17.4 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21311839276905023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21311839276905023 | validation: 0.16028992126067113]
	TIME [epoch: 17.4 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15870616966588716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15870616966588716 | validation: 0.1752505015105494]
	TIME [epoch: 17.3 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1711671584039393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1711671584039393 | validation: 0.341728005104712]
	TIME [epoch: 17.5 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23549584698177065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23549584698177065 | validation: 0.16300895083260103]
	TIME [epoch: 17.5 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20738631125384976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20738631125384976 | validation: 0.35643205121068544]
	TIME [epoch: 17.5 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23356939239560215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23356939239560215 | validation: 0.1600079673028207]
	TIME [epoch: 17.5 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18599943313636766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18599943313636766 | validation: 0.3243404096228516]
	TIME [epoch: 17.5 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18524480822087752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18524480822087752 | validation: 0.10925491273627486]
	TIME [epoch: 17.5 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12269681956108838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12269681956108838 | validation: 0.11912619898366748]
	TIME [epoch: 17.5 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12089726476906737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12089726476906737 | validation: 0.15967541628836845]
	TIME [epoch: 17.5 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14159454294549992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14159454294549992 | validation: 0.16480759078709942]
	TIME [epoch: 17.5 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1465135387693612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1465135387693612 | validation: 0.11915168359561701]
	TIME [epoch: 17.5 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1523512549474947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1523512549474947 | validation: 0.500030880293881]
	TIME [epoch: 17.5 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3455346202382904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3455346202382904 | validation: 0.16964968181628867]
	TIME [epoch: 17.5 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20349494519389857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20349494519389857 | validation: 0.17029042509973502]
	TIME [epoch: 17.5 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17629940730884275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17629940730884275 | validation: 0.17486797456652703]
	TIME [epoch: 17.5 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15518120881747968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15518120881747968 | validation: 0.09786210450033317]
	TIME [epoch: 17.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_716.pth
	Model improved!!!
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14215563251946772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14215563251946772 | validation: 0.30963543489582945]
	TIME [epoch: 17.5 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19842500291015772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19842500291015772 | validation: 0.21143650757400476]
	TIME [epoch: 17.5 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28664477859847576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28664477859847576 | validation: 0.42688823532555464]
	TIME [epoch: 17.5 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25292362526612827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25292362526612827 | validation: 0.11025202660690732]
	TIME [epoch: 17.5 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14583987553536587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14583987553536587 | validation: 0.12103542624646352]
	TIME [epoch: 17.5 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12605988994715714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12605988994715714 | validation: 0.16161953488276543]
	TIME [epoch: 17.5 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11852017428997069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11852017428997069 | validation: 0.12841724260739848]
	TIME [epoch: 17.5 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13825841214636853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13825841214636853 | validation: 0.14938028134182116]
	TIME [epoch: 17.5 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1098725553729038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1098725553729038 | validation: 0.25136968906506957]
	TIME [epoch: 17.5 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16603317708550258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16603317708550258 | validation: 0.23784602214098607]
	TIME [epoch: 17.5 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23868276422190016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23868276422190016 | validation: 0.09683170581340231]
	TIME [epoch: 17.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_727.pth
	Model improved!!!
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11120350220981547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11120350220981547 | validation: 0.32461587190249047]
	TIME [epoch: 17.5 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22424649327586926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22424649327586926 | validation: 0.09462264248340112]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_729.pth
	Model improved!!!
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14605569091705992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14605569091705992 | validation: 0.19999587649492265]
	TIME [epoch: 17.5 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1710205599461937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1710205599461937 | validation: 0.16838105530860492]
	TIME [epoch: 17.5 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22506531256944207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22506531256944207 | validation: 0.4210582119042551]
	TIME [epoch: 17.5 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24553930365313306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24553930365313306 | validation: 0.21077005167281102]
	TIME [epoch: 17.5 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28868327886847756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28868327886847756 | validation: 0.42788871047680826]
	TIME [epoch: 17.5 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2812291412367027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2812291412367027 | validation: 0.2027741700636165]
	TIME [epoch: 17.5 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15298582642314368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15298582642314368 | validation: 0.11449138833504052]
	TIME [epoch: 17.5 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1383031199598948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1383031199598948 | validation: 0.14411800086481338]
	TIME [epoch: 17.5 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11461939198128979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11461939198128979 | validation: 0.11673229043034915]
	TIME [epoch: 17.5 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11752237861461846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11752237861461846 | validation: 0.1984650247162875]
	TIME [epoch: 17.5 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1319131068870006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1319131068870006 | validation: 0.11257474454934935]
	TIME [epoch: 17.5 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1575787378856696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1575787378856696 | validation: 0.1701718119160797]
	TIME [epoch: 17.5 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16139974989789224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16139974989789224 | validation: 0.13431870064669724]
	TIME [epoch: 17.5 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1156093926714558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1156093926714558 | validation: 0.12750254969090374]
	TIME [epoch: 17.5 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09717990900124945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09717990900124945 | validation: 0.09340893858315054]
	TIME [epoch: 17.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_744.pth
	Model improved!!!
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10724504190236392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10724504190236392 | validation: 0.24486889219761254]
	TIME [epoch: 17.5 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1495004889779492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1495004889779492 | validation: 0.3135139478247353]
	TIME [epoch: 17.5 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37870141505670474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37870141505670474 | validation: 0.2776877897137931]
	TIME [epoch: 17.5 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2596877148649826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2596877148649826 | validation: 0.21634518933610894]
	TIME [epoch: 17.5 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21058343099386057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21058343099386057 | validation: 0.12589603258045035]
	TIME [epoch: 17.5 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14120068555153245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14120068555153245 | validation: 0.13068104732262134]
	TIME [epoch: 17.5 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11741117603511715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11741117603511715 | validation: 0.1278078697458706]
	TIME [epoch: 17.5 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13244636001371146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13244636001371146 | validation: 0.3579572273711113]
	TIME [epoch: 17.5 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19760627656332358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19760627656332358 | validation: 0.3059118265458481]
	TIME [epoch: 17.5 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41741098528082077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41741098528082077 | validation: 0.19171749300487131]
	TIME [epoch: 17.5 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16231224535633196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16231224535633196 | validation: 0.1682054135436286]
	TIME [epoch: 17.5 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1461211222216093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1461211222216093 | validation: 0.08854617220574107]
	TIME [epoch: 17.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_756.pth
	Model improved!!!
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12120996244388488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12120996244388488 | validation: 0.2035150878313715]
	TIME [epoch: 17.5 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13048654752760241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13048654752760241 | validation: 0.11194754275732431]
	TIME [epoch: 17.5 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1746482980230995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1746482980230995 | validation: 0.3918728529020328]
	TIME [epoch: 17.5 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21659325258557566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21659325258557566 | validation: 0.22292481657004606]
	TIME [epoch: 17.5 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3012297792771129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3012297792771129 | validation: 0.17555081898481845]
	TIME [epoch: 17.5 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1575530305784267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1575530305784267 | validation: 0.13993302491439363]
	TIME [epoch: 17.5 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12469021510109542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12469021510109542 | validation: 0.11978738879555256]
	TIME [epoch: 17.5 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10401164759457825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10401164759457825 | validation: 0.1213151404614963]
	TIME [epoch: 17.5 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09538978303916125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09538978303916125 | validation: 0.10702125291046789]
	TIME [epoch: 17.5 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11700342868468357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11700342868468357 | validation: 0.21799721430055674]
	TIME [epoch: 17.5 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14272683744900747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14272683744900747 | validation: 0.1486635804582421]
	TIME [epoch: 17.5 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18659473310368432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18659473310368432 | validation: 0.15980948076760268]
	TIME [epoch: 17.4 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11336210162741601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11336210162741601 | validation: 0.12380769857983687]
	TIME [epoch: 17.5 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14747104698130314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14747104698130314 | validation: 0.35845182663770525]
	TIME [epoch: 17.5 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20724983036361594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20724983036361594 | validation: 0.23319707549660446]
	TIME [epoch: 17.5 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29194826003699803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29194826003699803 | validation: 0.24471585315304933]
	TIME [epoch: 17.5 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16092018457377172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16092018457377172 | validation: 0.102693577335721]
	TIME [epoch: 17.4 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10635932168582937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10635932168582937 | validation: 0.0990997969568913]
	TIME [epoch: 17.5 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09466301036568181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09466301036568181 | validation: 0.09442408497234428]
	TIME [epoch: 17.5 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08828469351942726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08828469351942726 | validation: 0.113186654855646]
	TIME [epoch: 17.5 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09054557313946836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09054557313946836 | validation: 0.092694942937653]
	TIME [epoch: 17.5 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10382597244089542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10382597244089542 | validation: 0.34936540309963915]
	TIME [epoch: 17.5 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2047892977117793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2047892977117793 | validation: 0.5032872960627961]
	TIME [epoch: 17.5 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5586462489083575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5586462489083575 | validation: 0.46312594849282623]
	TIME [epoch: 17.5 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5038656446153008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5038656446153008 | validation: 0.27268568832521023]
	TIME [epoch: 17.5 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28441023772854773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28441023772854773 | validation: 0.28841198488659026]
	TIME [epoch: 17.5 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2783940549294449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2783940549294449 | validation: 0.34353113739181795]
	TIME [epoch: 17.5 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32487390486196377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32487390486196377 | validation: 0.35202204254649566]
	TIME [epoch: 17.5 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27844940960069986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27844940960069986 | validation: 0.17781856265266996]
	TIME [epoch: 17.5 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17188574728010356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17188574728010356 | validation: 0.16834622520521716]
	TIME [epoch: 17.5 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15552023748268842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15552023748268842 | validation: 0.14151383951943522]
	TIME [epoch: 17.5 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13084423337037543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13084423337037543 | validation: 0.163119852858984]
	TIME [epoch: 17.5 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12930589175875173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12930589175875173 | validation: 0.14508061549162546]
	TIME [epoch: 17.5 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1559493123515333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1559493123515333 | validation: 0.54805234341875]
	TIME [epoch: 17.5 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3811157829896571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3811157829896571 | validation: 0.242848362961292]
	TIME [epoch: 17.5 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27824078246641815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27824078246641815 | validation: 0.3864841236318879]
	TIME [epoch: 17.5 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29182818142764805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29182818142764805 | validation: 0.14292450434160284]
	TIME [epoch: 17.5 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18049113106848702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18049113106848702 | validation: 0.16301938804220684]
	TIME [epoch: 17.5 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17296836108682256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17296836108682256 | validation: 0.11924305043798006]
	TIME [epoch: 17.5 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12013534272823126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12013534272823126 | validation: 0.12046200930722116]
	TIME [epoch: 17.5 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12056530013733323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12056530013733323 | validation: 0.10636827680437524]
	TIME [epoch: 17.5 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14552689787697878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14552689787697878 | validation: 0.44383439543318554]
	TIME [epoch: 17.5 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2661391568001322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2661391568001322 | validation: 0.17977900047663334]
	TIME [epoch: 17.5 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23031291992234582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23031291992234582 | validation: 0.6063897820400419]
	TIME [epoch: 17.5 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48490886191483995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48490886191483995 | validation: 0.3378543018071729]
	TIME [epoch: 17.4 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.318064321312863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.318064321312863 | validation: 0.60074864395384]
	TIME [epoch: 17.4 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7172582192205793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7172582192205793 | validation: 0.626087505849561]
	TIME [epoch: 17.4 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8310417107638641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8310417107638641 | validation: 0.4581431328299017]
	TIME [epoch: 17.4 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5681818691541177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5681818691541177 | validation: 0.34853493498003485]
	TIME [epoch: 17.4 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.405918233418912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.405918233418912 | validation: 0.24444696074064298]
	TIME [epoch: 17.4 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32551265658094997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32551265658094997 | validation: 0.22435537962307608]
	TIME [epoch: 17.4 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25137218522944094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25137218522944094 | validation: 0.21540275297601666]
	TIME [epoch: 17.4 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22721732312078813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22721732312078813 | validation: 0.29392057914979414]
	TIME [epoch: 17.4 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21688592341411103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21688592341411103 | validation: 0.17153083708142655]
	TIME [epoch: 17.4 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.182439349331001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.182439349331001 | validation: 0.1445962587166496]
	TIME [epoch: 17.4 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1644579302078703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1644579302078703 | validation: 0.17334748322020008]
	TIME [epoch: 17.4 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1432211590524914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1432211590524914 | validation: 0.13893463842855625]
	TIME [epoch: 17.4 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14068091394792315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14068091394792315 | validation: 0.15516314059659192]
	TIME [epoch: 17.4 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13241059546565834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13241059546565834 | validation: 0.11449908880537825]
	TIME [epoch: 17.4 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13538500205510734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13538500205510734 | validation: 0.23847231961412396]
	TIME [epoch: 17.4 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15282268705299806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15282268705299806 | validation: 0.14755043159552286]
	TIME [epoch: 17.4 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23096583697009018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23096583697009018 | validation: 0.6145799119592478]
	TIME [epoch: 17.4 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4048546174638743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4048546174638743 | validation: 0.17744860113877353]
	TIME [epoch: 17.4 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15005865555472472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15005865555472472 | validation: 0.12563745407461455]
	TIME [epoch: 17.4 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16881150898591663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16881150898591663 | validation: 0.13885761838603888]
	TIME [epoch: 17.4 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1406787251357988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1406787251357988 | validation: 0.1721509803737123]
	TIME [epoch: 17.4 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1410308508275931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1410308508275931 | validation: 0.137198305932116]
	TIME [epoch: 17.4 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12021042050297588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12021042050297588 | validation: 0.10034251524687443]
	TIME [epoch: 17.4 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11994363911041296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11994363911041296 | validation: 0.1639128669971015]
	TIME [epoch: 17.4 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11327314125308145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11327314125308145 | validation: 0.09074878411724788]
	TIME [epoch: 17.4 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11928059433430925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11928059433430925 | validation: 0.22082823130887583]
	TIME [epoch: 17.4 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14113272504092447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14113272504092447 | validation: 0.13066130375541266]
	TIME [epoch: 17.4 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1812059209856556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1812059209856556 | validation: 0.32097771769067474]
	TIME [epoch: 17.4 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1930546439386442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1930546439386442 | validation: 0.12413019116876618]
	TIME [epoch: 17.4 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1805496886385529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1805496886385529 | validation: 0.1915157472881842]
	TIME [epoch: 17.4 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12394347067685266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12394347067685266 | validation: 0.09465379019497089]
	TIME [epoch: 17.4 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11194734128165269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11194734128165269 | validation: 0.15510448492620144]
	TIME [epoch: 17.4 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10625017029607374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10625017029607374 | validation: 0.09568016805815978]
	TIME [epoch: 17.4 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10901785802360037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10901785802360037 | validation: 0.1614707490264607]
	TIME [epoch: 17.4 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10479837029166927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10479837029166927 | validation: 0.08470683403176926]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_836.pth
	Model improved!!!
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12454039876740355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12454039876740355 | validation: 0.3428712972959253]
	TIME [epoch: 17.4 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18302116163097032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18302116163097032 | validation: 0.20384336459411007]
	TIME [epoch: 17.4 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25823685591423606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25823685591423606 | validation: 0.3152331096758491]
	TIME [epoch: 17.4 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2189899418360866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2189899418360866 | validation: 0.2028248730289307]
	TIME [epoch: 17.4 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16381584628013737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16381584628013737 | validation: 0.3575570055547458]
	TIME [epoch: 17.4 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3922282639348572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3922282639348572 | validation: 0.17865695861964415]
	TIME [epoch: 17.4 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21714733759606158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21714733759606158 | validation: 0.1456774807046279]
	TIME [epoch: 17.4 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12538345750635552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12538345750635552 | validation: 0.20233332391741554]
	TIME [epoch: 17.4 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.142484107477918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.142484107477918 | validation: 0.11507786455778134]
	TIME [epoch: 17.4 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10782190022524678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10782190022524678 | validation: 0.09598617932617569]
	TIME [epoch: 17.4 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11016548958125752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11016548958125752 | validation: 0.09744007791027032]
	TIME [epoch: 17.4 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09887006903920402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09887006903920402 | validation: 0.1374126744384892]
	TIME [epoch: 17.4 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09452413000520941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09452413000520941 | validation: 0.09660750094798236]
	TIME [epoch: 17.4 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09551381600083367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09551381600083367 | validation: 0.15937974746776717]
	TIME [epoch: 17.4 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14787407084455886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14787407084455886 | validation: 0.21877352483929313]
	TIME [epoch: 17.4 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21897557221705166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21897557221705166 | validation: 0.26743128059713567]
	TIME [epoch: 17.4 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1864806273823009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1864806273823009 | validation: 0.12669883741654187]
	TIME [epoch: 17.4 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16039644358052807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16039644358052807 | validation: 0.5175412336433581]
	TIME [epoch: 17.4 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31816410892985764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31816410892985764 | validation: 0.121945075412624]
	TIME [epoch: 17.4 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12272063886389874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12272063886389874 | validation: 0.08857759598017444]
	TIME [epoch: 17.4 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13280975038510318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13280975038510318 | validation: 0.10470794278753562]
	TIME [epoch: 17.4 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10355810526742812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10355810526742812 | validation: 0.29585087203818833]
	TIME [epoch: 17.4 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2432552200850129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2432552200850129 | validation: 0.19863310830984512]
	TIME [epoch: 17.4 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16697551097835148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16697551097835148 | validation: 0.11285941317084341]
	TIME [epoch: 17.4 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13480141550442187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13480141550442187 | validation: 0.1188104057615425]
	TIME [epoch: 17.4 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13050187439873284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13050187439873284 | validation: 0.208575758589619]
	TIME [epoch: 17.4 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26321132633769606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26321132633769606 | validation: 0.42137594962949604]
	TIME [epoch: 17.4 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2757799942327016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2757799942327016 | validation: 0.2047440856004309]
	TIME [epoch: 17.4 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14500392078701854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14500392078701854 | validation: 0.0905627282911553]
	TIME [epoch: 17.4 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10955858436897561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10955858436897561 | validation: 0.09061217856189424]
	TIME [epoch: 17.4 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09296485849960254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09296485849960254 | validation: 0.14905848528417254]
	TIME [epoch: 17.4 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10323582596787578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10323582596787578 | validation: 0.0886173475298942]
	TIME [epoch: 17.4 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09122514310722547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09122514310722547 | validation: 0.08195148637042012]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_869.pth
	Model improved!!!
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09077522561168748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09077522561168748 | validation: 0.21853757446901248]
	TIME [epoch: 17.4 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.117474255333889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.117474255333889 | validation: 0.17104640732207385]
	TIME [epoch: 17.4 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2136134293043319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2136134293043319 | validation: 0.22425630858907636]
	TIME [epoch: 17.5 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1339642554738257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1339642554738257 | validation: 0.14978756222205555]
	TIME [epoch: 17.4 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1765685056237977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1765685056237977 | validation: 0.17901079971242947]
	TIME [epoch: 17.5 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14673032830035643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14673032830035643 | validation: 0.08453617907915659]
	TIME [epoch: 17.4 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09817634767748558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09817634767748558 | validation: 0.13876093580832813]
	TIME [epoch: 17.4 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09100734830763227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09100734830763227 | validation: 0.07344598789706018]
	TIME [epoch: 17.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_877.pth
	Model improved!!!
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08940465763554573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08940465763554573 | validation: 0.1667350184184353]
	TIME [epoch: 17.4 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1070591441195052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1070591441195052 | validation: 0.08720135025507932]
	TIME [epoch: 17.4 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12514533159547642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12514533159547642 | validation: 0.21448816255969474]
	TIME [epoch: 17.4 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12412312513768374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12412312513768374 | validation: 0.15435331836799354]
	TIME [epoch: 17.4 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18512980787590405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18512980787590405 | validation: 0.29063243625285295]
	TIME [epoch: 17.4 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15742911601296963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15742911601296963 | validation: 0.07947556334559686]
	TIME [epoch: 17.4 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09566006971329793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09566006971329793 | validation: 1.0923330197788947]
	TIME [epoch: 17.4 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9073224118148856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9073224118148856 | validation: 0.6654337021397456]
	TIME [epoch: 17.4 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5344631352680463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5344631352680463 | validation: 0.2923458644814973]
	TIME [epoch: 17.4 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28310919759019176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28310919759019176 | validation: 0.17916914298211153]
	TIME [epoch: 17.4 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22442796828566403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22442796828566403 | validation: 0.15028795303584086]
	TIME [epoch: 17.4 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20254794113089772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20254794113089772 | validation: 0.12534488083399675]
	TIME [epoch: 17.4 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14854868769219684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14854868769219684 | validation: 0.16389641715028813]
	TIME [epoch: 17.4 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1345068363379572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1345068363379572 | validation: 0.11659646076890727]
	TIME [epoch: 17.4 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11346001945082267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11346001945082267 | validation: 0.1100595106505766]
	TIME [epoch: 17.4 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10587363494672974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10587363494672974 | validation: 0.11261886121541223]
	TIME [epoch: 17.4 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09511120563207903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09511120563207903 | validation: 0.08320966656273164]
	TIME [epoch: 17.4 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09682728058929171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09682728058929171 | validation: 0.12882525270928355]
	TIME [epoch: 17.4 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0900658420126176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0900658420126176 | validation: 0.07725651420919313]
	TIME [epoch: 17.4 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09704840024442382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09704840024442382 | validation: 0.17057377095292614]
	TIME [epoch: 17.4 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12502757951326945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12502757951326945 | validation: 0.1192268779466894]
	TIME [epoch: 17.4 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13918876399879998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13918876399879998 | validation: 0.19078705001169197]
	TIME [epoch: 17.5 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11805439113952126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11805439113952126 | validation: 0.07410736507973029]
	TIME [epoch: 17.4 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1028442245121733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1028442245121733 | validation: 0.14886119223730063]
	TIME [epoch: 17.4 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10192946528117611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10192946528117611 | validation: 0.0763930749337518]
	TIME [epoch: 17.4 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10097751043289155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10097751043289155 | validation: 0.1942905343502246]
	TIME [epoch: 17.4 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11410825909624503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11410825909624503 | validation: 0.07128404189621394]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_904.pth
	Model improved!!!
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0998200710769629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0998200710769629 | validation: 0.16614069465808093]
	TIME [epoch: 17.4 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09743875364389575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09743875364389575 | validation: 0.0754930396501739]
	TIME [epoch: 17.4 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10032399525357107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10032399525357107 | validation: 0.1382705228353343]
	TIME [epoch: 17.4 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10613274575646932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10613274575646932 | validation: 0.08919572132823024]
	TIME [epoch: 17.4 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10482222685958131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10482222685958131 | validation: 0.28033529299989707]
	TIME [epoch: 17.5 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16239150765672983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16239150765672983 | validation: 0.11057126520128083]
	TIME [epoch: 17.4 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1845286229527327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1845286229527327 | validation: 0.14065300306290604]
	TIME [epoch: 17.4 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11106747534344487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11106747534344487 | validation: 0.19402230828609537]
	TIME [epoch: 17.4 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1400360901701814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1400360901701814 | validation: 0.07153101012070971]
	TIME [epoch: 17.4 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09858687068399333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09858687068399333 | validation: 0.0887510394327948]
	TIME [epoch: 17.4 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10230876773957924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10230876773957924 | validation: 0.10640048143418751]
	TIME [epoch: 17.4 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08789022149532431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08789022149532431 | validation: 0.09175526853682946]
	TIME [epoch: 17.4 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10835966046827675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10835966046827675 | validation: 0.28255132018098944]
	TIME [epoch: 17.4 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1858440558918376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1858440558918376 | validation: 0.09806987163592645]
	TIME [epoch: 17.4 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1408230794329466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1408230794329466 | validation: 0.2574366814036598]
	TIME [epoch: 17.4 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13813656553864856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13813656553864856 | validation: 0.06521254873873589]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_920.pth
	Model improved!!!
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07575864175566366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07575864175566366 | validation: 0.07050957213261914]
	TIME [epoch: 17.4 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08889898350005333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08889898350005333 | validation: 0.21695290594232183]
	TIME [epoch: 17.5 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11410199356389183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11410199356389183 | validation: 0.07877504452682638]
	TIME [epoch: 17.4 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08983443089004191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08983443089004191 | validation: 0.09984277476151003]
	TIME [epoch: 17.4 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09754766195261567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09754766195261567 | validation: 0.19710978484989222]
	TIME [epoch: 17.4 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1462322164763773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1462322164763773 | validation: 0.1041582814140913]
	TIME [epoch: 17.4 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10222869498831294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10222869498831294 | validation: 0.08203168435423941]
	TIME [epoch: 17.4 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1035574930663331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1035574930663331 | validation: 0.22756493591692434]
	TIME [epoch: 17.4 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11754680875984269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11754680875984269 | validation: 0.0856446808524452]
	TIME [epoch: 17.4 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12352812800045207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12352812800045207 | validation: 0.12439225116621033]
	TIME [epoch: 17.4 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10807129413356105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10807129413356105 | validation: 0.43832832934379407]
	TIME [epoch: 17.4 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5556855776543089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5556855776543089 | validation: 0.3991831284535538]
	TIME [epoch: 17.4 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44336641313993297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44336641313993297 | validation: 0.2292619339228945]
	TIME [epoch: 17.4 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28058265777561747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28058265777561747 | validation: 0.2470454690253676]
	TIME [epoch: 17.5 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27984263975885415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27984263975885415 | validation: 0.1712808262125509]
	TIME [epoch: 17.4 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2058187483279084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2058187483279084 | validation: 0.18232116351370353]
	TIME [epoch: 17.4 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13659492928818717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13659492928818717 | validation: 0.11587688728766088]
	TIME [epoch: 17.4 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09863064145332943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09863064145332943 | validation: 0.08506516312413714]
	TIME [epoch: 17.4 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09189657219895322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09189657219895322 | validation: 0.09342328993441015]
	TIME [epoch: 17.5 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08261830135415624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08261830135415624 | validation: 0.11303704746839759]
	TIME [epoch: 17.4 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08314656088470807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08314656088470807 | validation: 0.11180956087725423]
	TIME [epoch: 17.4 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09204463540256405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09204463540256405 | validation: 0.1702179231047027]
	TIME [epoch: 17.4 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16216673730359973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16216673730359973 | validation: 0.14720925970211507]
	TIME [epoch: 17.4 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11102020351644414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11102020351644414 | validation: 0.14836207111709349]
	TIME [epoch: 17.5 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10925921518461305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10925921518461305 | validation: 0.10008211863535604]
	TIME [epoch: 17.4 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0893940554302669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0893940554302669 | validation: 0.08502886030096708]
	TIME [epoch: 17.4 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0699360313499907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0699360313499907 | validation: 0.11408059883495661]
	TIME [epoch: 17.4 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07349873533588126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07349873533588126 | validation: 0.06557091105340809]
	TIME [epoch: 17.4 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06847488205328173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06847488205328173 | validation: 0.10684814406250898]
	TIME [epoch: 17.4 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07470505141642923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07470505141642923 | validation: 0.08153530005801837]
	TIME [epoch: 17.4 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07707204433154793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07707204433154793 | validation: 0.10379140988794287]
	TIME [epoch: 17.4 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11557878503553656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11557878503553656 | validation: 0.40917369724394903]
	TIME [epoch: 17.4 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2454034532671334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2454034532671334 | validation: 0.11209060125355297]
	TIME [epoch: 17.4 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1698919459895974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1698919459895974 | validation: 0.1738906543211604]
	TIME [epoch: 17.4 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10851692340414915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10851692340414915 | validation: 0.07483295873612132]
	TIME [epoch: 17.4 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06760313706191147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06760313706191147 | validation: 0.05596490199930108]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_956.pth
	Model improved!!!
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07397374314119104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07397374314119104 | validation: 0.1574777921303722]
	TIME [epoch: 17.4 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10078794788439333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10078794788439333 | validation: 0.06425478327699884]
	TIME [epoch: 17.4 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07303209482666714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07303209482666714 | validation: 0.07883933548853128]
	TIME [epoch: 17.4 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07540189632930346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07540189632930346 | validation: 0.08199910080602782]
	TIME [epoch: 17.4 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09661577256750936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09661577256750936 | validation: 0.17945395657887336]
	TIME [epoch: 17.4 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12135439465656125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12135439465656125 | validation: 0.07562247084553445]
	TIME [epoch: 17.4 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1005431061871642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1005431061871642 | validation: 0.1283211512334772]
	TIME [epoch: 17.4 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0889283281815597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0889283281815597 | validation: 0.06359007409492255]
	TIME [epoch: 17.4 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062314794652275615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062314794652275615 | validation: 0.06158394745009624]
	TIME [epoch: 17.4 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06345651666141815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06345651666141815 | validation: 0.1898004595239901]
	TIME [epoch: 17.4 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0882059509086233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0882059509086233 | validation: 0.08365989921353624]
	TIME [epoch: 17.4 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13344184319896987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13344184319896987 | validation: 0.3540369555063338]
	TIME [epoch: 17.4 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15845352274186555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15845352274186555 | validation: 0.086082550604326]
	TIME [epoch: 17.4 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08607891998261728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08607891998261728 | validation: 0.13085727804980332]
	TIME [epoch: 17.4 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11698121000184859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11698121000184859 | validation: 0.09203696903973722]
	TIME [epoch: 17.4 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0758495007790975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0758495007790975 | validation: 0.07805545927931212]
	TIME [epoch: 17.4 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06667854264392961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06667854264392961 | validation: 0.05731896473483006]
	TIME [epoch: 17.4 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0655079642230832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0655079642230832 | validation: 0.10140996015880206]
	TIME [epoch: 17.4 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07550921216975173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07550921216975173 | validation: 0.06376773728515277]
	TIME [epoch: 17.4 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0932589277326479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0932589277326479 | validation: 1.0748314160907533]
	TIME [epoch: 17.4 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8515919747522888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8515919747522888 | validation: 0.6312014644446818]
	TIME [epoch: 17.4 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47104168745335967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47104168745335967 | validation: 0.23248237031798463]
	TIME [epoch: 17.4 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19658388024199583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19658388024199583 | validation: 0.14187637401550823]
	TIME [epoch: 17.4 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17896488817385525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17896488817385525 | validation: 0.10542168086464435]
	TIME [epoch: 17.4 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09295096225846745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09295096225846745 | validation: 0.15387788434881433]
	TIME [epoch: 17.4 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.104338453675896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.104338453675896 | validation: 0.13724614532654797]
	TIME [epoch: 17.4 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08624258737557237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08624258737557237 | validation: 0.06745549374956807]
	TIME [epoch: 17.4 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08771778522764383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08771778522764383 | validation: 0.1417794066036848]
	TIME [epoch: 17.4 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09677034981517163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09677034981517163 | validation: 0.0803091469739432]
	TIME [epoch: 17.4 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1011177773498472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1011177773498472 | validation: 0.20174787652251377]
	TIME [epoch: 17.4 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12168065380818506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12168065380818506 | validation: 0.06305931088831383]
	TIME [epoch: 17.4 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07416249273468037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07416249273468037 | validation: 0.06318175258553423]
	TIME [epoch: 17.4 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06750647168088786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06750647168088786 | validation: 0.11638647357204322]
	TIME [epoch: 17.4 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0648682435543681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0648682435543681 | validation: 0.056314167682808274]
	TIME [epoch: 17.4 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06379809300247972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06379809300247972 | validation: 0.07174368321536062]
	TIME [epoch: 17.4 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06897412340431791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06897412340431791 | validation: 0.0728073642934525]
	TIME [epoch: 17.4 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07503725373342135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07503725373342135 | validation: 0.12187643348462159]
	TIME [epoch: 17.4 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08687685409588713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08687685409588713 | validation: 0.0650564392683721]
	TIME [epoch: 17.4 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08591033526756257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08591033526756257 | validation: 0.15731256479056085]
	TIME [epoch: 17.4 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.093394963231597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.093394963231597 | validation: 0.053586544929019]
	TIME [epoch: 17.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_996.pth
	Model improved!!!
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07468886730942512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07468886730942512 | validation: 0.255018388888738]
	TIME [epoch: 17.4 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11713107128073663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11713107128073663 | validation: 0.0620673690587087]
	TIME [epoch: 17.4 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07729314018715988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07729314018715988 | validation: 0.060179460725119205]
	TIME [epoch: 17.4 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08142354017140627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08142354017140627 | validation: 0.06759414513179397]
	TIME [epoch: 17.4 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.063948937676977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.063948937676977 | validation: 0.08771576795152403]
	TIME [epoch: 74.5 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0732579278657766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0732579278657766 | validation: 0.07049100326287067]
	TIME [epoch: 35.9 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08645099346466154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08645099346466154 | validation: 0.11680967925339668]
	TIME [epoch: 35.9 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08936725027693625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08936725027693625 | validation: 0.11543562555673988]
	TIME [epoch: 35.9 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13788146184215191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13788146184215191 | validation: 0.13973358307921796]
	TIME [epoch: 35.9 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08821224084889626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08821224084889626 | validation: 0.05635226069046463]
	TIME [epoch: 35.9 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07187666692879535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07187666692879535 | validation: 0.15482353865855591]
	TIME [epoch: 36 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08882169405238145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08882169405238145 | validation: 0.05660291602444539]
	TIME [epoch: 35.9 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06326581392693123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06326581392693123 | validation: 0.04911257083248674]
	TIME [epoch: 35.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_1009.pth
	Model improved!!!
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06340092992705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06340092992705 | validation: 0.1853882546156176]
	TIME [epoch: 35.9 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08826210478811049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08826210478811049 | validation: 0.06530775100771818]
	TIME [epoch: 35.9 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09362190827280492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09362190827280492 | validation: 0.19886223862026017]
	TIME [epoch: 35.9 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11578853741877757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11578853741877757 | validation: 0.08893597360990495]
	TIME [epoch: 36 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08645497891437089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08645497891437089 | validation: 0.08712408974491473]
	TIME [epoch: 35.9 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10126031164257558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10126031164257558 | validation: 0.1123240270959499]
	TIME [epoch: 36 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07610782145240233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07610782145240233 | validation: 0.054594689305966095]
	TIME [epoch: 35.9 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06374682956938965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06374682956938965 | validation: 0.14235269662022693]
	TIME [epoch: 35.9 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08076525434033145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08076525434033145 | validation: 0.05317552043480747]
	TIME [epoch: 35.9 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06709734173772507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06709734173772507 | validation: 0.059377059568094304]
	TIME [epoch: 35.9 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05772968664634773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05772968664634773 | validation: 0.05538146233069391]
	TIME [epoch: 35.9 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05353511506332168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05353511506332168 | validation: 0.05770857250551414]
	TIME [epoch: 35.9 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.072951862364224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.072951862364224 | validation: 0.214872169371111]
	TIME [epoch: 35.9 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11614440804899624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11614440804899624 | validation: 0.06732675636565966]
	TIME [epoch: 36 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09058970636063719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09058970636063719 | validation: 0.38534511748564865]
	TIME [epoch: 35.9 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20550839278446134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20550839278446134 | validation: 0.10183719724676928]
	TIME [epoch: 35.9 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10915301703456828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10915301703456828 | validation: 0.06534283469550413]
	TIME [epoch: 35.9 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10015930757508552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10015930757508552 | validation: 0.16446544410355157]
	TIME [epoch: 36 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08134052215367586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08134052215367586 | validation: 0.0644743156659586]
	TIME [epoch: 35.9 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06794426962661831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06794426962661831 | validation: 0.06535452324555817]
	TIME [epoch: 36 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06735147038286235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06735147038286235 | validation: 0.11675197982243204]
	TIME [epoch: 35.9 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08083245847424923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08083245847424923 | validation: 0.10842389890007849]
	TIME [epoch: 36 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08774825878049537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08774825878049537 | validation: 0.08522707313009294]
	TIME [epoch: 36 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10900068988038757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10900068988038757 | validation: 0.13386677127147287]
	TIME [epoch: 35.9 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13054249642384147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13054249642384147 | validation: 0.11273477115736362]
	TIME [epoch: 35.9 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07302633851594624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07302633851594624 | validation: 0.07331842202829263]
	TIME [epoch: 35.9 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08324573315680682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08324573315680682 | validation: 0.08080864169470713]
	TIME [epoch: 35.9 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05945746171357011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05945746171357011 | validation: 0.06768027177691498]
	TIME [epoch: 35.9 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0652492346457764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0652492346457764 | validation: 0.099589942830229]
	TIME [epoch: 35.9 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06917387339058279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06917387339058279 | validation: 0.06769648554971011]
	TIME [epoch: 35.9 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07448202709275178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07448202709275178 | validation: 0.1262836123007584]
	TIME [epoch: 36 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08813506958556497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08813506958556497 | validation: 0.06855753586945494]
	TIME [epoch: 36 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10515384577975581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10515384577975581 | validation: 0.3365868127608317]
	TIME [epoch: 35.9 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14663331978117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14663331978117 | validation: 0.05213018379350229]
	TIME [epoch: 36 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06784156386899809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06784156386899809 | validation: 0.05465703819536545]
	TIME [epoch: 35.9 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05723922332244813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05723922332244813 | validation: 0.09545225088998917]
	TIME [epoch: 36 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06347447569083366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06347447569083366 | validation: 0.051250963279420296]
	TIME [epoch: 35.9 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061489505932666416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061489505932666416 | validation: 0.07169065730926155]
	TIME [epoch: 35.9 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06738010997476242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06738010997476242 | validation: 0.08320069511119926]
	TIME [epoch: 35.9 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08621066899127051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08621066899127051 | validation: 0.1841796724332055]
	TIME [epoch: 36 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19582137589343787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19582137589343787 | validation: 0.20473751839059276]
	TIME [epoch: 35.9 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10577755581149897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10577755581149897 | validation: 0.07628322383385915]
	TIME [epoch: 35.9 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1042531240715558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1042531240715558 | validation: 0.11628277659217322]
	TIME [epoch: 35.9 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09863084957407826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09863084957407826 | validation: 0.05557248750918392]
	TIME [epoch: 36 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07251398441215405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07251398441215405 | validation: 0.05837349768271575]
	TIME [epoch: 35.9 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05774269624343003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05774269624343003 | validation: 0.19752067078663865]
	TIME [epoch: 35.9 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0920896781239594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0920896781239594 | validation: 0.0677940198270156]
	TIME [epoch: 35.9 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08205050082899944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08205050082899944 | validation: 0.08077367962329003]
	TIME [epoch: 35.9 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07787114045351372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07787114045351372 | validation: 0.08364148962900601]
	TIME [epoch: 35.9 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06746816073124495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06746816073124495 | validation: 0.06351224260356732]
	TIME [epoch: 35.9 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06954120050190815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06954120050190815 | validation: 0.07111999865141327]
	TIME [epoch: 35.9 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06606319342379466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06606319342379466 | validation: 0.0789340803425084]
	TIME [epoch: 36 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05937193022722715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05937193022722715 | validation: 0.061300104988300665]
	TIME [epoch: 35.9 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0806993036093879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0806993036093879 | validation: 0.24532042293577472]
	TIME [epoch: 36 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13714506432082604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13714506432082604 | validation: 0.056194725060750864]
	TIME [epoch: 35.9 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07209120815585177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07209120815585177 | validation: 0.07521740224664936]
	TIME [epoch: 36 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05215088798600331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05215088798600331 | validation: 0.056973789469540737]
	TIME [epoch: 35.9 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05020447284167902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05020447284167902 | validation: 0.050997138249748025]
	TIME [epoch: 36 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06755548039636317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06755548039636317 | validation: 0.20930438135780474]
	TIME [epoch: 35.9 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10166510890656576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10166510890656576 | validation: 0.05361603982158271]
	TIME [epoch: 36 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07106815581555316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07106815581555316 | validation: 0.10182229552250154]
	TIME [epoch: 35.9 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07018722511002215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07018722511002215 | validation: 0.06841351031827875]
	TIME [epoch: 36 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07161413808214637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07161413808214637 | validation: 0.07523214797555404]
	TIME [epoch: 35.9 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0742548710835411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0742548710835411 | validation: 0.06452574765518752]
	TIME [epoch: 36 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06061338477009917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06061338477009917 | validation: 0.2110194158723293]
	TIME [epoch: 36 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0950087635704206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0950087635704206 | validation: 0.10052886750788402]
	TIME [epoch: 36 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.132691551574818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.132691551574818 | validation: 0.23089111000760704]
	TIME [epoch: 35.9 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13509185061325144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13509185061325144 | validation: 0.09598358748561198]
	TIME [epoch: 35.9 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1312632754678679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1312632754678679 | validation: 0.10210081857583586]
	TIME [epoch: 35.9 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08737828481935463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08737828481935463 | validation: 0.06941636731278505]
	TIME [epoch: 36 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06607870601384976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06607870601384976 | validation: 0.06363717178388842]
	TIME [epoch: 35.9 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06687953187457737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06687953187457737 | validation: 0.06460491624246632]
	TIME [epoch: 35.9 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050398120090681404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050398120090681404 | validation: 0.06075751020587016]
	TIME [epoch: 35.9 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04822432801257582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04822432801257582 | validation: 0.19447318264277902]
	TIME [epoch: 36 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11590909871762413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11590909871762413 | validation: 0.1348867495929846]
	TIME [epoch: 35.9 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09367048876800799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09367048876800799 | validation: 0.08030059927328193]
	TIME [epoch: 35.9 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12244738472499218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12244738472499218 | validation: 0.1347670721766636]
	TIME [epoch: 35.9 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11865842434168661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11865842434168661 | validation: 0.12379745987276128]
	TIME [epoch: 36 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10157908395122017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10157908395122017 | validation: 0.0629371900638449]
	TIME [epoch: 35.9 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07484823475706881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07484823475706881 | validation: 0.05637224677681631]
	TIME [epoch: 36 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052801041800597644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052801041800597644 | validation: 0.04465318233860363]
	TIME [epoch: 35.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_1090.pth
	Model improved!!!
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04708816158172056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04708816158172056 | validation: 0.043643852708124015]
	TIME [epoch: 36 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_1091.pth
	Model improved!!!
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04592539031144645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04592539031144645 | validation: 0.044783947908689094]
	TIME [epoch: 36 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049051005435651725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049051005435651725 | validation: 0.1451183439516522]
	TIME [epoch: 35.9 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11671383283274367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11671383283274367 | validation: 0.0939536313809328]
	TIME [epoch: 35.9 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07560786418304109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07560786418304109 | validation: 0.056206564141803916]
	TIME [epoch: 36 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07343171437194217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07343171437194217 | validation: 0.2353840412476695]
	TIME [epoch: 36 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12412076479856646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12412076479856646 | validation: 0.09634677606157373]
	TIME [epoch: 36 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1063615561389894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1063615561389894 | validation: 0.08587305870277445]
	TIME [epoch: 35.9 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08330367011120256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08330367011120256 | validation: 0.06447978692598393]
	TIME [epoch: 36 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05447301385941918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05447301385941918 | validation: 0.039838621448110234]
	TIME [epoch: 36 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_1100.pth
	Model improved!!!
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04473699289087243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04473699289087243 | validation: 0.0425867199860306]
	TIME [epoch: 36.1 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04449022225605891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04449022225605891 | validation: 0.040412518705340496]
	TIME [epoch: 35.9 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04622639170160525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04622639170160525 | validation: 0.048694658696592924]
	TIME [epoch: 35.9 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04610741817295115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04610741817295115 | validation: 0.04447377583834864]
	TIME [epoch: 36 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052416662435929046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052416662435929046 | validation: 0.08771565811408695]
	TIME [epoch: 36 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07845623480039635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07845623480039635 | validation: 0.09044904604813458]
	TIME [epoch: 36 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12706660823161056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12706660823161056 | validation: 0.25873371514503457]
	TIME [epoch: 35.9 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12978152099298512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12978152099298512 | validation: 0.04276762574925187]
	TIME [epoch: 35.9 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06837473366790177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06837473366790177 | validation: 0.13696425160697318]
	TIME [epoch: 36 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09199771916924811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09199771916924811 | validation: 0.05378572699641341]
	TIME [epoch: 36 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06298707751346337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06298707751346337 | validation: 0.04719401999792497]
	TIME [epoch: 35.9 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04909055762810507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04909055762810507 | validation: 0.06967254569776848]
	TIME [epoch: 35.9 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05185762300475243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05185762300475243 | validation: 0.045498136185255125]
	TIME [epoch: 36 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05487010155566156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05487010155566156 | validation: 0.08954454301799593]
	TIME [epoch: 36 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06087672321042507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06087672321042507 | validation: 0.06220786506831536]
	TIME [epoch: 35.9 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09515444473202883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09515444473202883 | validation: 0.1912387039186677]
	TIME [epoch: 35.9 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1103675683178194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1103675683178194 | validation: 0.06045943521885654]
	TIME [epoch: 36 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05729682034074842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05729682034074842 | validation: 0.04975237953990608]
	TIME [epoch: 36 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0767673336849976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0767673336849976 | validation: 0.11840317457789405]
	TIME [epoch: 35.9 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07448765416810287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07448765416810287 | validation: 0.0388283713740908]
	TIME [epoch: 35.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_1120.pth
	Model improved!!!
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04876530626496047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04876530626496047 | validation: 0.05047280100104204]
	TIME [epoch: 36 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05503404066102674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05503404066102674 | validation: 0.0716417224198996]
	TIME [epoch: 36 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06516798569257402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06516798569257402 | validation: 0.04246672334397481]
	TIME [epoch: 36 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04271720865007921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04271720865007921 | validation: 0.03951825709441568]
	TIME [epoch: 35.9 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04551145029963676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04551145029963676 | validation: 0.06810302163803354]
	TIME [epoch: 36 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05001863201111046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05001863201111046 | validation: 0.044933579416958774]
	TIME [epoch: 36.1 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051097751234709804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051097751234709804 | validation: 0.12374051964519764]
	TIME [epoch: 36 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09996729972827698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09996729972827698 | validation: 0.11329795711176667]
	TIME [epoch: 35.9 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12419068865363354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12419068865363354 | validation: 0.28352117943579136]
	TIME [epoch: 35.9 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13500190124045752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13500190124045752 | validation: 0.1874886593722544]
	TIME [epoch: 36 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17368955792583837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17368955792583837 | validation: 0.27082205440601487]
	TIME [epoch: 36 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14137284544104872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14137284544104872 | validation: 0.114388653648121]
	TIME [epoch: 35.9 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06876410479979045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06876410479979045 | validation: 0.049641342652712576]
	TIME [epoch: 35.9 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08450220873156841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08450220873156841 | validation: 0.07583686674558435]
	TIME [epoch: 36 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054416114414699825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054416114414699825 | validation: 0.10462298740794454]
	TIME [epoch: 36 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06065768422342179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06065768422342179 | validation: 0.062409777356301044]
	TIME [epoch: 36 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04911308676153758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04911308676153758 | validation: 0.05838576339244486]
	TIME [epoch: 35.9 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045253659763026914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045253659763026914 | validation: 0.041815732871646555]
	TIME [epoch: 35.9 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04388723786956332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04388723786956332 | validation: 0.056475172703658275]
	TIME [epoch: 36.1 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04478827795741568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04478827795741568 | validation: 0.08221801070476187]
	TIME [epoch: 36 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07321172994251204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07321172994251204 | validation: 0.07569973194292576]
	TIME [epoch: 36 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06546780452889206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06546780452889206 | validation: 0.05346386140926077]
	TIME [epoch: 36 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05645873670395642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05645873670395642 | validation: 0.0569947486963986]
	TIME [epoch: 36 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06373854179996657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06373854179996657 | validation: 0.0579714076233872]
	TIME [epoch: 36 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06764464890119604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06764464890119604 | validation: 0.13488941992087622]
	TIME [epoch: 35.9 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09099481607771605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09099481607771605 | validation: 0.039778005935187814]
	TIME [epoch: 35.9 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0535066568489477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0535066568489477 | validation: 0.040536966715776494]
	TIME [epoch: 36 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04115845669876173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04115845669876173 | validation: 0.09164896763377495]
	TIME [epoch: 36 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08955565944883562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08955565944883562 | validation: 0.08005825878935456]
	TIME [epoch: 35.9 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08124470149850492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08124470149850492 | validation: 0.09159774564945526]
	TIME [epoch: 35.9 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07705363646264951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07705363646264951 | validation: 0.05136846148524678]
	TIME [epoch: 36 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048305933587513845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048305933587513845 | validation: 0.04931855049177956]
	TIME [epoch: 36 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050117045874548655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050117045874548655 | validation: 0.04302812499324413]
	TIME [epoch: 36 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05458706121605363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05458706121605363 | validation: 0.05038944756406534]
	TIME [epoch: 35.9 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05777338110076888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05777338110076888 | validation: 0.04561945916638365]
	TIME [epoch: 36 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05573377058361394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05573377058361394 | validation: 0.1676067582828573]
	TIME [epoch: 36 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0822613806252551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0822613806252551 | validation: 0.058790144772589925]
	TIME [epoch: 36 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08147497995870946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08147497995870946 | validation: 0.12787429052066604]
	TIME [epoch: 35.9 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07648856868402179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07648856868402179 | validation: 0.055869801205016105]
	TIME [epoch: 35.9 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05746393181471386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05746393181471386 | validation: 0.04415487084715505]
	TIME [epoch: 35.9 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041228724993175966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041228724993175966 | validation: 0.04296918646352821]
	TIME [epoch: 36 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04063415465459921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04063415465459921 | validation: 0.03486387352823781]
	TIME [epoch: 36 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_1162.pth
	Model improved!!!
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04166171242568258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04166171242568258 | validation: 0.22293705195538083]
	TIME [epoch: 35.9 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.135233893359828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.135233893359828 | validation: 0.1846207557897972]
	TIME [epoch: 36 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13413967149165598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13413967149165598 | validation: 0.09927383362954267]
	TIME [epoch: 36 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09504556304406396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09504556304406396 | validation: 0.10542960210814312]
	TIME [epoch: 36 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08878983422455097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08878983422455097 | validation: 0.0502079925572418]
	TIME [epoch: 35.9 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06023681111709211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06023681111709211 | validation: 0.06784847854180824]
	TIME [epoch: 36 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046630929235299094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046630929235299094 | validation: 0.03189606853425654]
	TIME [epoch: 36 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_1169.pth
	Model improved!!!
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039287233754208666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039287233754208666 | validation: 0.05162939622763007]
	TIME [epoch: 36.1 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049809582037069346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049809582037069346 | validation: 0.04195308130452401]
	TIME [epoch: 35.9 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05404752965918076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05404752965918076 | validation: 0.09847074741264275]
	TIME [epoch: 35.9 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06319926820033979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06319926820033979 | validation: 0.0584297265349196]
	TIME [epoch: 36 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06277460297719474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06277460297719474 | validation: 0.0630728248147323]
	TIME [epoch: 36 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05713544960495668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05713544960495668 | validation: 0.19901366210686675]
	TIME [epoch: 35.9 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20882961147128715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20882961147128715 | validation: 0.18744841608144938]
	TIME [epoch: 35.9 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10248071116551553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10248071116551553 | validation: 0.06911672248194015]
	TIME [epoch: 36 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05937510250363182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05937510250363182 | validation: 0.06449049344118656]
	TIME [epoch: 36 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06803807928673536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06803807928673536 | validation: 0.06842694814720227]
	TIME [epoch: 36 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05565457669794241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05565457669794241 | validation: 0.07159147346075631]
	TIME [epoch: 35.9 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054808719542889614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054808719542889614 | validation: 0.06412195347212417]
	TIME [epoch: 36 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0647672544320924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0647672544320924 | validation: 0.06492140182540695]
	TIME [epoch: 36 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055268901944234124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055268901944234124 | validation: 0.060769417706883926]
	TIME [epoch: 36 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045340129191022795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045340129191022795 | validation: 0.036467101671900495]
	TIME [epoch: 35.9 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048658781314852975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048658781314852975 | validation: 0.05869472425680594]
	TIME [epoch: 35.9 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051958855340302174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051958855340302174 | validation: 0.05742363528678725]
	TIME [epoch: 36 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059128746032524986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059128746032524986 | validation: 0.06639018112570935]
	TIME [epoch: 36 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06713646077043028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06713646077043028 | validation: 0.05309370280080498]
	TIME [epoch: 36 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04674407068540916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04674407068540916 | validation: 0.03941918728998572]
	TIME [epoch: 35.9 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03741554869961799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03741554869961799 | validation: 0.028094157691300737]
	TIME [epoch: 36 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_1190.pth
	Model improved!!!
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037352263713514376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037352263713514376 | validation: 0.06375987168882413]
	TIME [epoch: 36 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057252438894639004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057252438894639004 | validation: 0.36168223183382076]
	TIME [epoch: 36 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3443071874288027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3443071874288027 | validation: 0.32454824412835004]
	TIME [epoch: 35.9 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35642374996132403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35642374996132403 | validation: 0.2739832357841014]
	TIME [epoch: 36 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3068013221766111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3068013221766111 | validation: 0.9939510155309267]
	TIME [epoch: 36.1 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.720632659657706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.720632659657706 | validation: 0.8361730520559558]
	TIME [epoch: 36.1 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7928320202363033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7928320202363033 | validation: 0.5291238943881822]
	TIME [epoch: 36 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5236207023493737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5236207023493737 | validation: 0.2763016318286275]
	TIME [epoch: 36 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22465408295602238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22465408295602238 | validation: 0.11387797465250832]
	TIME [epoch: 36 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13140522629888793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13140522629888793 | validation: 0.0942568247285387]
	TIME [epoch: 36.1 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11695656377680529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11695656377680529 | validation: 0.06203413677829685]
	TIME [epoch: 35.7 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0793013045833745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0793013045833745 | validation: 0.06012968565416419]
	TIME [epoch: 35.7 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07321623777246336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07321623777246336 | validation: 0.05561929106815742]
	TIME [epoch: 35.7 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05689612807260041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05689612807260041 | validation: 0.0520142127546464]
	TIME [epoch: 35.7 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05261811355122841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05261811355122841 | validation: 0.04698549045747759]
	TIME [epoch: 35.7 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046172085403139954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046172085403139954 | validation: 0.041268625074245995]
	TIME [epoch: 35.7 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05059780355130469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05059780355130469 | validation: 0.6291847665933856]
	TIME [epoch: 35.7 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7747829867518652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7747829867518652 | validation: 0.6315958015947201]
	TIME [epoch: 35.7 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7938354057776489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7938354057776489 | validation: 0.5735513474940787]
	TIME [epoch: 35.8 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.652817390709227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.652817390709227 | validation: 0.46888386011985467]
	TIME [epoch: 35.7 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5253636627257896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5253636627257896 | validation: 0.39878958640190265]
	TIME [epoch: 35.7 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45917246578004545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45917246578004545 | validation: 0.3618235435476175]
	TIME [epoch: 35.7 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.432162968193894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.432162968193894 | validation: 0.3401571826107491]
	TIME [epoch: 35.7 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40098200535814343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40098200535814343 | validation: 0.2327905306583484]
	TIME [epoch: 35.7 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30476382756067893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30476382756067893 | validation: 0.17853232664661361]
	TIME [epoch: 35.8 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22403999137614938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22403999137614938 | validation: 0.1484651678137243]
	TIME [epoch: 35.8 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1651442376583001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1651442376583001 | validation: 0.09507176340525987]
	TIME [epoch: 35.7 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11047374923629892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11047374923629892 | validation: 0.07535084314733927]
	TIME [epoch: 35.7 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08628140327339485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08628140327339485 | validation: 0.07670483446641213]
	TIME [epoch: 35.7 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07359261418298868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07359261418298868 | validation: 0.070738640415951]
	TIME [epoch: 35.7 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06655199290299951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06655199290299951 | validation: 0.06346528607412648]
	TIME [epoch: 35.7 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06359160629746724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06359160629746724 | validation: 0.059060510895872165]
	TIME [epoch: 35.7 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05740845121562542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05740845121562542 | validation: 0.05223264640184852]
	TIME [epoch: 35.7 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05205956130972507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05205956130972507 | validation: 0.04165295900930913]
	TIME [epoch: 35.7 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048753859885465024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048753859885465024 | validation: 0.0486897030223048]
	TIME [epoch: 35.7 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04691912100810024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04691912100810024 | validation: 0.04124782591396971]
	TIME [epoch: 35.7 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04433164143381891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04433164143381891 | validation: 0.03520399966434297]
	TIME [epoch: 35.7 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04444342449324332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04444342449324332 | validation: 0.0338997814837626]
	TIME [epoch: 35.7 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04037800220026675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04037800220026675 | validation: 0.034240239225795]
	TIME [epoch: 35.7 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04030330630264744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04030330630264744 | validation: 0.03568936821590279]
	TIME [epoch: 35.8 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04326383170159728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04326383170159728 | validation: 0.0433311503834539]
	TIME [epoch: 35.7 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043218339609820716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043218339609820716 | validation: 0.05207021268052708]
	TIME [epoch: 35.7 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05022322495505091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05022322495505091 | validation: 0.05587251350589001]
	TIME [epoch: 35.7 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06516966068572805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06516966068572805 | validation: 0.062216952134667595]
	TIME [epoch: 35.8 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06292212822938222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06292212822938222 | validation: 0.06114745403094673]
	TIME [epoch: 35.7 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06796185865745746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06796185865745746 | validation: 0.05151017467180407]
	TIME [epoch: 35.7 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055717516584439526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055717516584439526 | validation: 0.13051655396408576]
	TIME [epoch: 35.8 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08050174190782972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08050174190782972 | validation: 0.09340235938829786]
	TIME [epoch: 35.8 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10146590913215939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10146590913215939 | validation: 0.05675950531322867]
	TIME [epoch: 35.8 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05970911899288327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05970911899288327 | validation: 0.0381297564265724]
	TIME [epoch: 35.7 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04083152218154256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04083152218154256 | validation: 0.028949276637813073]
	TIME [epoch: 35.7 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033697133155589476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033697133155589476 | validation: 0.07540430056129521]
	TIME [epoch: 35.9 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053273175836612695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053273175836612695 | validation: 0.03914140914741829]
	TIME [epoch: 35.9 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04351222126051515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04351222126051515 | validation: 0.0325687612083317]
	TIME [epoch: 35.8 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04085582955692685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04085582955692685 | validation: 0.034347304180879334]
	TIME [epoch: 35.7 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03824993457579432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03824993457579432 | validation: 0.048250948925141306]
	TIME [epoch: 35.8 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04509372138992968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04509372138992968 | validation: 0.048133448229462965]
	TIME [epoch: 35.8 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05168649335093486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05168649335093486 | validation: 0.0713271310898366]
	TIME [epoch: 35.8 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06561727471477546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06561727471477546 | validation: 0.06289670012104687]
	TIME [epoch: 35.8 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0739201381642821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0739201381642821 | validation: 0.1014742660875677]
	TIME [epoch: 35.8 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0719085468941714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0719085468941714 | validation: 0.08318918665418573]
	TIME [epoch: 35.7 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09611139092442067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09611139092442067 | validation: 0.2116387612025492]
	TIME [epoch: 35.8 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11758537437184305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11758537437184305 | validation: 0.06718103879104176]
	TIME [epoch: 36 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055665340633829885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055665340633829885 | validation: 0.04940452141730681]
	TIME [epoch: 35.7 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059856109822273074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059856109822273074 | validation: 0.03870204432143446]
	TIME [epoch: 35.7 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042322385043047056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042322385043047056 | validation: 0.042835925703890804]
	TIME [epoch: 35.8 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04077005997368826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04077005997368826 | validation: 0.037270183443541736]
	TIME [epoch: 35.9 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03156206081325269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03156206081325269 | validation: 0.06198272270293029]
	TIME [epoch: 36 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0433063021586876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0433063021586876 | validation: 0.042562101805286016]
	TIME [epoch: 35.9 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056936464580183394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056936464580183394 | validation: 0.08437990789419253]
	TIME [epoch: 36 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07676960106984478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07676960106984478 | validation: 0.08378247862409118]
	TIME [epoch: 35.9 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07088969849921412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07088969849921412 | validation: 0.04521445373014502]
	TIME [epoch: 35.9 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05316143426150509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05316143426150509 | validation: 0.02660437305067065]
	TIME [epoch: 35.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_1263.pth
	Model improved!!!
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0318787983917112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0318787983917112 | validation: 0.030514278607417934]
	TIME [epoch: 36 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031370518047660355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031370518047660355 | validation: 0.02590534046428832]
	TIME [epoch: 35.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_1265.pth
	Model improved!!!
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03365035594744911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03365035594744911 | validation: 0.032954310465683256]
	TIME [epoch: 35.8 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03820074726953709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03820074726953709 | validation: 0.03482491018186508]
	TIME [epoch: 35.9 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0518493414560837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0518493414560837 | validation: 0.07509299666534254]
	TIME [epoch: 35.9 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0808930983527739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0808930983527739 | validation: 0.06312758345523001]
	TIME [epoch: 35.8 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07073548095939607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07073548095939607 | validation: 0.06257491320579954]
	TIME [epoch: 35.9 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06478344121868178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06478344121868178 | validation: 0.10288011112529999]
	TIME [epoch: 35.9 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06750709836022269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06750709836022269 | validation: 0.03337873311458275]
	TIME [epoch: 35.8 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04659370200981833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04659370200981833 | validation: 0.029288379132557687]
	TIME [epoch: 35.9 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03860960573800606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03860960573800606 | validation: 0.043552078893705964]
	TIME [epoch: 35.8 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04847405888989189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04847405888989189 | validation: 0.08129701500623523]
	TIME [epoch: 35.8 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06738588044751279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06738588044751279 | validation: 0.051072496020203355]
	TIME [epoch: 35.8 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058106124638358576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058106124638358576 | validation: 0.1168740176647717]
	TIME [epoch: 35.9 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06613868945291539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06613868945291539 | validation: 0.15256066056984088]
	TIME [epoch: 35.8 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15257009421268836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15257009421268836 | validation: 0.13757267750195773]
	TIME [epoch: 35.8 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13675309698705213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13675309698705213 | validation: 0.09843023850956373]
	TIME [epoch: 35.9 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06527059385982925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06527059385982925 | validation: 0.0667345682768522]
	TIME [epoch: 35.8 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060278417008384995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060278417008384995 | validation: 0.06922730197811895]
	TIME [epoch: 35.9 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051625515254906586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051625515254906586 | validation: 0.06474748557692772]
	TIME [epoch: 35.8 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06618450792035127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06618450792035127 | validation: 0.04505665832414929]
	TIME [epoch: 35.9 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05069840671081023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05069840671081023 | validation: 0.04969241981329362]
	TIME [epoch: 35.9 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03885445898192327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03885445898192327 | validation: 0.04433763164903122]
	TIME [epoch: 35.8 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038125951242105625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038125951242105625 | validation: 0.043361237665864444]
	TIME [epoch: 35.9 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03464707676506187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03464707676506187 | validation: 0.025584687541996987]
	TIME [epoch: 35.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_1288.pth
	Model improved!!!
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03202218686176037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03202218686176037 | validation: 0.03006442320292827]
	TIME [epoch: 35.9 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036669919048944004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036669919048944004 | validation: 0.04948269604208008]
	TIME [epoch: 35.9 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04783184053211056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04783184053211056 | validation: 0.058602422038054824]
	TIME [epoch: 35.8 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06136723242287301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06136723242287301 | validation: 0.056280162515145095]
	TIME [epoch: 35.9 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058847344249793626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058847344249793626 | validation: 0.5591316749582093]
	TIME [epoch: 35.9 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6224218010163866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6224218010163866 | validation: 0.4842304447803538]
	TIME [epoch: 35.9 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4982465904723503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4982465904723503 | validation: 0.3842830114081839]
	TIME [epoch: 35.8 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39318700780743354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39318700780743354 | validation: 0.4205521280354483]
	TIME [epoch: 35.8 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5030217912473678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5030217912473678 | validation: 0.43787819190428157]
	TIME [epoch: 35.8 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3762005096173047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3762005096173047 | validation: 0.2914729179951629]
	TIME [epoch: 35.8 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.252071697730491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.252071697730491 | validation: 0.6678151124784529]
	TIME [epoch: 35.8 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4540773372255557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4540773372255557 | validation: 0.1094340359608885]
	TIME [epoch: 35.8 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1177859550489611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1177859550489611 | validation: 0.06827051849168626]
	TIME [epoch: 36.2 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07964987160052805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07964987160052805 | validation: 0.036038975325972604]
	TIME [epoch: 35.9 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04500606991191776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04500606991191776 | validation: 0.02939002491682119]
	TIME [epoch: 35.9 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041116098918348706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041116098918348706 | validation: 0.030112677990776705]
	TIME [epoch: 35.8 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033580060535727575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033580060535727575 | validation: 0.031191104917811886]
	TIME [epoch: 35.9 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027234316561131646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027234316561131646 | validation: 0.022926140425544264]
	TIME [epoch: 35.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_1306.pth
	Model improved!!!
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02681987337456239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02681987337456239 | validation: 0.02243592885452236]
	TIME [epoch: 35.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_1307.pth
	Model improved!!!
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026666431064213746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026666431064213746 | validation: 0.13837690344274242]
	TIME [epoch: 35.8 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0842217630661164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0842217630661164 | validation: 0.12295356097416091]
	TIME [epoch: 35.8 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07711362230179407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07711362230179407 | validation: 0.05864558115619711]
	TIME [epoch: 35.8 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06564781433073688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06564781433073688 | validation: 0.04035283375803838]
	TIME [epoch: 35.9 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052555074263620195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052555074263620195 | validation: 0.02959397643411923]
	TIME [epoch: 35.8 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036269925497116474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036269925497116474 | validation: 0.02124484585497871]
	TIME [epoch: 35.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_1313.pth
	Model improved!!!
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029748586452932022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029748586452932022 | validation: 0.03184412663148548]
	TIME [epoch: 35.8 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03206557168636439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03206557168636439 | validation: 0.03182639953018165]
	TIME [epoch: 35.9 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030081816540043813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030081816540043813 | validation: 0.03640903795087791]
	TIME [epoch: 35.9 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03194338764246051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03194338764246051 | validation: 0.03954079088036434]
	TIME [epoch: 35.8 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037433885401477164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037433885401477164 | validation: 0.04274736660328341]
	TIME [epoch: 35.9 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04379155817775194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04379155817775194 | validation: 0.043234402238011116]
	TIME [epoch: 35.9 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03343896261236761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03343896261236761 | validation: 0.034947853434313274]
	TIME [epoch: 35.9 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03158048768720149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03158048768720149 | validation: 0.023292491551981587]
	TIME [epoch: 35.8 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032007145023541426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032007145023541426 | validation: 0.029210836434410525]
	TIME [epoch: 35.9 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03240312800763528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03240312800763528 | validation: 0.025951105650417108]
	TIME [epoch: 35.9 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029250524272641173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029250524272641173 | validation: 0.026534385123354057]
	TIME [epoch: 35.9 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03170640734046407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03170640734046407 | validation: 0.030582014625491385]
	TIME [epoch: 35.8 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03665178933554019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03665178933554019 | validation: 0.04108111649177916]
	TIME [epoch: 35.8 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03929962982739447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03929962982739447 | validation: 0.029306705845129677]
	TIME [epoch: 35.9 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0361540673073043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0361540673073043 | validation: 0.05127551892200491]
	TIME [epoch: 35.9 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04604991116019149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04604991116019149 | validation: 0.048295683066963725]
	TIME [epoch: 35.8 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04835788303219862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04835788303219862 | validation: 0.06558956873357584]
	TIME [epoch: 35.9 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06530397456313124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06530397456313124 | validation: 0.08609822618941547]
	TIME [epoch: 35.8 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054833999695043316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054833999695043316 | validation: 0.02246672895729166]
	TIME [epoch: 35.9 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029560834450464188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029560834450464188 | validation: 0.03281176649638481]
	TIME [epoch: 35.8 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038366523392284864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038366523392284864 | validation: 0.07506228507707807]
	TIME [epoch: 35.9 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07466943018740016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07466943018740016 | validation: 0.05992623411849412]
	TIME [epoch: 35.8 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06229988738531349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06229988738531349 | validation: 0.022308174474769505]
	TIME [epoch: 35.9 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024905984993258547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024905984993258547 | validation: 0.02324235829718383]
	TIME [epoch: 35.9 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02465947725190737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02465947725190737 | validation: 0.02440930834502636]
	TIME [epoch: 35.9 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02741926040577961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02741926040577961 | validation: 0.019662793334686124]
	TIME [epoch: 35.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_1339.pth
	Model improved!!!
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02485153467590533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02485153467590533 | validation: 0.01725966061212314]
	TIME [epoch: 35.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_1340.pth
	Model improved!!!
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021908812846278682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021908812846278682 | validation: 0.03476336914009046]
	TIME [epoch: 35.9 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031331686729944914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031331686729944914 | validation: 0.0697721177861809]
	TIME [epoch: 35.8 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050840580654421236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050840580654421236 | validation: 0.08133389067601791]
	TIME [epoch: 35.8 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059570947903329986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059570947903329986 | validation: 0.08768448573812747]
	TIME [epoch: 35.8 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07725744524718205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07725744524718205 | validation: 0.06416001658567155]
	TIME [epoch: 35.8 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06863038803531823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06863038803531823 | validation: 0.04778407504524377]
	TIME [epoch: 35.8 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050403724713100326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050403724713100326 | validation: 0.03698998480730591]
	TIME [epoch: 35.8 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04114736735583815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04114736735583815 | validation: 0.05140768013823839]
	TIME [epoch: 35.8 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05595350779609362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05595350779609362 | validation: 0.05391061515728]
	TIME [epoch: 35.8 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053518512588256816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053518512588256816 | validation: 0.029160477365953154]
	TIME [epoch: 35.8 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03456045254682107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03456045254682107 | validation: 0.015225543849633594]
	TIME [epoch: 35.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_1351.pth
	Model improved!!!
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021747482524061425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021747482524061425 | validation: 0.016598533186120847]
	TIME [epoch: 35.9 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02094197566159421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02094197566159421 | validation: 0.01493003520588896]
	TIME [epoch: 35.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_1353.pth
	Model improved!!!
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020840244541822272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020840244541822272 | validation: 0.022250152617876495]
	TIME [epoch: 36 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023194810264846776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023194810264846776 | validation: 0.08570286900403828]
	TIME [epoch: 35.9 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05684239110262336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05684239110262336 | validation: 0.06738965542154317]
	TIME [epoch: 35.9 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06990890958313496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06990890958313496 | validation: 0.07488514646205789]
	TIME [epoch: 36 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07150955055880855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07150955055880855 | validation: 0.10226706707123934]
	TIME [epoch: 35.9 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07098272436974544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07098272436974544 | validation: 0.07794899743535143]
	TIME [epoch: 35.9 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06850205100619176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06850205100619176 | validation: 0.06490479580524605]
	TIME [epoch: 35.9 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05531059632325686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05531059632325686 | validation: 0.09896122025733922]
	TIME [epoch: 35.9 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09401580483085024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09401580483085024 | validation: 0.08290248758786053]
	TIME [epoch: 35.9 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0777620096507359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0777620096507359 | validation: 0.07761270451534362]
	TIME [epoch: 36 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0450257691733497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0450257691733497 | validation: 0.03012981674279276]
	TIME [epoch: 35.9 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032627728674706445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032627728674706445 | validation: 0.056320450782226916]
	TIME [epoch: 35.9 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04998288479126823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04998288479126823 | validation: 0.0782578912082913]
	TIME [epoch: 35.9 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06933805904163952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06933805904163952 | validation: 0.06255816995654005]
	TIME [epoch: 35.9 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06640982699813146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06640982699813146 | validation: 0.05582404851717886]
	TIME [epoch: 35.9 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04397856192911306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04397856192911306 | validation: 0.03786455375632339]
	TIME [epoch: 35.9 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03615227141668956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03615227141668956 | validation: 0.02447279009979202]
	TIME [epoch: 35.9 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02514968138819894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02514968138819894 | validation: 0.019884801649483776]
	TIME [epoch: 35.9 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025470131901466672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025470131901466672 | validation: 0.02596966857672164]
	TIME [epoch: 35.9 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028138685895967023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028138685895967023 | validation: 0.04743438490288324]
	TIME [epoch: 35.9 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046141964817699783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046141964817699783 | validation: 0.05216749218390231]
	TIME [epoch: 35.9 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06197084546851666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06197084546851666 | validation: 0.07450647694773342]
	TIME [epoch: 36 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07007084087923254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07007084087923254 | validation: 0.08408645590570059]
	TIME [epoch: 35.9 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08192641936103812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08192641936103812 | validation: 0.039780984569600365]
	TIME [epoch: 35.9 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042456946603789965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042456946603789965 | validation: 0.036564325503160765]
	TIME [epoch: 35.9 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03973205998041726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03973205998041726 | validation: 0.028973430275607483]
	TIME [epoch: 36 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03528640181055806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03528640181055806 | validation: 0.02390979788808827]
	TIME [epoch: 35.9 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027313229024007776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027313229024007776 | validation: 0.02311654371902684]
	TIME [epoch: 35.9 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02180274108585012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02180274108585012 | validation: 0.013755278549717233]
	TIME [epoch: 35.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20240823_171538/states/model_phi1_4c_v_mmd1_1382.pth
	Model improved!!!
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022356608176650878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022356608176650878 | validation: 0.019999645151702806]
	TIME [epoch: 36 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024339679188422655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024339679188422655 | validation: 0.01706334124220188]
	TIME [epoch: 36 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024312477882183908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024312477882183908 | validation: 0.021170120725913212]
	TIME [epoch: 35.9 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023564391712737064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023564391712737064 | validation: 0.02235472145486589]
	TIME [epoch: 35.9 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02784458980582383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02784458980582383 | validation: 0.03229540863346706]
	TIME [epoch: 36.1 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03420096198991264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03420096198991264 | validation: 0.043720252448025756]
	TIME [epoch: 36 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04947816113513486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04947816113513486 | validation: 0.07323397169168687]
	TIME [epoch: 36 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07104167568744861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07104167568744861 | validation: 0.04196686835019147]
	TIME [epoch: 36 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0453570460408649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0453570460408649 | validation: 0.02882195665636932]
	TIME [epoch: 36 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03323095219409694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03323095219409694 | validation: 0.05839651089541428]
	TIME [epoch: 36 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03180765500655758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03180765500655758 | validation: 0.09854652644892423]
	TIME [epoch: 36.1 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08480162822182907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08480162822182907 | validation: 0.12684708661790833]
	TIME [epoch: 35.8 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12240998191412523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12240998191412523 | validation: 0.12711852354269731]
	TIME [epoch: 36 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11189522156203349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11189522156203349 | validation: 0.09774998371846322]
	TIME [epoch: 35.9 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0913251384221437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0913251384221437 | validation: 0.06189091573798605]
	TIME [epoch: 35.9 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06301181923140015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06301181923140015 | validation: 0.05388792127059913]
	TIME [epoch: 35.8 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045838231162122885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045838231162122885 | validation: 0.04659397095478465]
	TIME [epoch: 35.9 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04827110497550385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04827110497550385 | validation: 0.05954744385916372]
	TIME [epoch: 35.9 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05843572495792036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05843572495792036 | validation: 0.05670147427239439]
	TIME [epoch: 35.9 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05764610662354402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05764610662354402 | validation: 0.0314023688146864]
	TIME [epoch: 35.8 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036703551574127795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036703551574127795 | validation: 0.027851678308362528]
	TIME [epoch: 35.8 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031148841704745918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031148841704745918 | validation: 0.05433317647243461]
	TIME [epoch: 35.9 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05011504498726119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05011504498726119 | validation: 0.0344266694123835]
	TIME [epoch: 35.9 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04191140175607098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04191140175607098 | validation: 0.027109231591068574]
	TIME [epoch: 35.9 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036971476924531246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036971476924531246 | validation: 0.038782132983318854]
	TIME [epoch: 35.8 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02945109340566652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02945109340566652 | validation: 0.023148971527263584]
	TIME [epoch: 35.9 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02477036447919482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02477036447919482 | validation: 0.03219771320938617]
	TIME [epoch: 35.9 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03555469071624959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03555469071624959 | validation: 0.05478901985295727]
	TIME [epoch: 35.9 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050694898289332053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050694898289332053 | validation: 0.042620466936888535]
	TIME [epoch: 35.8 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04132007997072918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04132007997072918 | validation: 0.028271183084722484]
	TIME [epoch: 38.2 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029432909697902902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029432909697902902 | validation: 0.023935606334248752]
	TIME [epoch: 35.9 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028928522135408936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028928522135408936 | validation: 0.32331663841635394]
	TIME [epoch: 35.9 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1716949458963747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1716949458963747 | validation: 0.1818311736734609]
	TIME [epoch: 36 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12778587013591747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12778587013591747 | validation: 0.07246604326396199]
	TIME [epoch: 35.8 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09016365346621172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09016365346621172 | validation: 0.03997704620480558]
	TIME [epoch: 35.9 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06801268054261563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06801268054261563 | validation: 0.07898897734853406]
	TIME [epoch: 36 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045513867149064476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045513867149064476 | validation: 0.03315238433423513]
	TIME [epoch: 35.9 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03478063013911919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03478063013911919 | validation: 0.02826971189447347]
	TIME [epoch: 35.9 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03479976033870574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03479976033870574 | validation: 0.028075604140932988]
	TIME [epoch: 35.9 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02863868620562011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02863868620562011 | validation: 0.02389809442756551]
	TIME [epoch: 36 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020693082305598267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020693082305598267 | validation: 0.017411112450355582]
	TIME [epoch: 35.9 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019261497641189406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019261497641189406 | validation: 0.02031128630802459]
	TIME [epoch: 35.9 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020880059927075575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020880059927075575 | validation: 0.01937256480966311]
	TIME [epoch: 35.8 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023935267785570424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023935267785570424 | validation: 0.033362578479353366]
	TIME [epoch: 36 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03523356009058945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03523356009058945 | validation: 0.04893704061018625]
	TIME [epoch: 35.9 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05259315709605193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05259315709605193 | validation: 0.059091788425507996]
	TIME [epoch: 35.8 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06262302805479095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06262302805479095 | validation: 0.03682011810496836]
	TIME [epoch: 35.9 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04046633874660863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04046633874660863 | validation: 0.04361566142695011]
	TIME [epoch: 35.8 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035134561416596315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035134561416596315 | validation: 0.039863532992395236]
	TIME [epoch: 35.9 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03498274813086206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03498274813086206 | validation: 0.04035779159662554]
	TIME [epoch: 35.8 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04815355996508885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04815355996508885 | validation: 0.05431197638361617]
	TIME [epoch: 35.8 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04719401920269789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04719401920269789 | validation: 0.03911810692832393]
	TIME [epoch: 35.8 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038594012582513305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038594012582513305 | validation: 0.05119334466579291]
	TIME [epoch: 35.8 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04417778010660262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04417778010660262 | validation: 0.04241579789028517]
	TIME [epoch: 35.9 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038334106333235606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038334106333235606 | validation: 0.04007498246629067]
	TIME [epoch: 35.7 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029097116206891382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029097116206891382 | validation: 0.03557500265338466]
	TIME [epoch: 35.7 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03699860693022572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03699860693022572 | validation: 0.0215443761950595]
	TIME [epoch: 35.9 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02431692398112847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02431692398112847 | validation: 0.05675987806521213]
	TIME [epoch: 35.7 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033985766800254565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033985766800254565 | validation: 0.03388372497701775]
	TIME [epoch: 35.7 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038050919676473303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038050919676473303 | validation: 0.045260657262316886]
	TIME [epoch: 35.7 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05669988623507001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05669988623507001 | validation: 0.037275811643180244]
	TIME [epoch: 35.7 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04689613500999288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04689613500999288 | validation: 0.036274346756480426]
	TIME [epoch: 35.9 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03726999190374623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03726999190374623 | validation: 0.03515359782942928]
	TIME [epoch: 35.8 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034907372311664436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034907372311664436 | validation: 0.03930312633679999]
	TIME [epoch: 36.1 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03542203219948731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03542203219948731 | validation: 0.03293220159425025]
	TIME [epoch: 35.8 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03418939573115859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03418939573115859 | validation: 0.03571917840158539]
	TIME [epoch: 36.2 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039450036004005436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039450036004005436 | validation: 0.04465505004900318]
	TIME [epoch: 35.9 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04191758391099065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04191758391099065 | validation: 0.03140476577860862]
	TIME [epoch: 36 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03334735309829009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03334735309829009 | validation: 0.030856463493238687]
	TIME [epoch: 35.9 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031700577733213324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031700577733213324 | validation: 0.04045458449997322]
	TIME [epoch: 36.1 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042361807756964985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042361807756964985 | validation: 0.03595587326718426]
	TIME [epoch: 35.9 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04590994988705596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04590994988705596 | validation: 0.03780659879493331]
	TIME [epoch: 36 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04225188454482781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04225188454482781 | validation: 0.056106083812524646]
	TIME [epoch: 35.9 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04905848587316586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04905848587316586 | validation: 0.0269123671722601]
	TIME [epoch: 36.1 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02758994829919733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02758994829919733 | validation: 0.015764181110335573]
	TIME [epoch: 36.1 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020096068763838897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020096068763838897 | validation: 0.050319375015894674]
	TIME [epoch: 35.9 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04949759504860852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04949759504860852 | validation: 0.1421280183296166]
	TIME [epoch: 35.8 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07494047049847333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07494047049847333 | validation: 0.035018441179533503]
	TIME [epoch: 36.1 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03885660243268982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03885660243268982 | validation: 0.021839585152425636]
	TIME [epoch: 36.1 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026378211117572086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026378211117572086 | validation: 0.016397311673178972]
	TIME [epoch: 36.1 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024046759422067358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024046759422067358 | validation: 0.030440020148972482]
	TIME [epoch: 35.9 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03056654512593969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03056654512593969 | validation: 0.043645390845659486]
	TIME [epoch: 35.9 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045174533669574045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045174533669574045 | validation: 0.045619544768047304]
	TIME [epoch: 35.9 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048765278841111835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048765278841111835 | validation: 0.04191733793896614]
	TIME [epoch: 36 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046057410236522406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046057410236522406 | validation: 0.028740787481587204]
	TIME [epoch: 35.9 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02932580663907537		[learning rate: 0.01]
