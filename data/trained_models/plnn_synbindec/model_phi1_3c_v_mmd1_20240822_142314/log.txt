Args:
Namespace(name='model_phi1_3c_v_mmd1', outdir='out/model_training/model_phi1_3c_v_mmd1', training_data='data/training_data/data_phi1_3c/training', validation_data='data/training_data/data_phi1_3c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2844042479

Training model...

Saving initial model state to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.03605772825687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.03605772825687 | validation: 4.208903478234638]
	TIME [epoch: 28.6 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.343112028644063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.343112028644063 | validation: 4.456221728239601]
	TIME [epoch: 3.68 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.608499381284843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.608499381284843 | validation: 3.716515545289605]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.834844252430268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.834844252430268 | validation: 4.249381746509978]
	TIME [epoch: 3.68 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.3872532647842775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3872532647842775 | validation: 3.999513245301016]
	TIME [epoch: 3.68 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.192947223986392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.192947223986392 | validation: 3.678933909850279]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7963840955092683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7963840955092683 | validation: 3.3234477439662586]
	TIME [epoch: 3.66 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4931171662511766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4931171662511766 | validation: 3.3079287981036942]
	TIME [epoch: 3.66 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4777841141687347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4777841141687347 | validation: 3.2520600657334597]
	TIME [epoch: 3.66 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4737081065585973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4737081065585973 | validation: 3.284654632002147]
	TIME [epoch: 3.7 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4542722347403885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4542722347403885 | validation: 3.13370257354173]
	TIME [epoch: 3.69 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4006058598434095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4006058598434095 | validation: 3.141659498874164]
	TIME [epoch: 3.69 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3043100995239936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3043100995239936 | validation: 3.0133582529951024]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.258474512095396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.258474512095396 | validation: 3.120861883007633]
	TIME [epoch: 3.68 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.270528531552838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.270528531552838 | validation: 3.0599614419623418]
	TIME [epoch: 3.68 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3430416133092713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3430416133092713 | validation: 2.9267974403961765]
	TIME [epoch: 3.69 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1351343035773125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1351343035773125 | validation: 2.8644991937898228]
	TIME [epoch: 3.79 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1048055624861868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1048055624861868 | validation: 2.8647939525767767]
	TIME [epoch: 3.67 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.079728135326493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.079728135326493 | validation: 2.859881683898039]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9920923170274647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9920923170274647 | validation: 2.953002661005373]
	TIME [epoch: 3.67 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.202701929101993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.202701929101993 | validation: 2.791406570423417]
	TIME [epoch: 3.66 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0973160269029547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0973160269029547 | validation: 2.2724295545153845]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5941066140484397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5941066140484397 | validation: 2.288126753638185]
	TIME [epoch: 3.67 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.584159169875666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.584159169875666 | validation: 2.050189801697497]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.275574216027235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.275574216027235 | validation: 1.9509884332654082]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.175124822103383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.175124822103383 | validation: 1.6769548737425375]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9369632285983513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9369632285983513 | validation: 1.4650560122846834]
	TIME [epoch: 3.7 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7302145236330766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7302145236330766 | validation: 1.263561778622123]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4556532796037636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4556532796037636 | validation: 1.1513050897073656]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2610965550051416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2610965550051416 | validation: 2.4983477786990633]
	TIME [epoch: 3.66 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9777941594713857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9777941594713857 | validation: 1.1207921378530787]
	TIME [epoch: 3.66 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0982518316699745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0982518316699745 | validation: 1.2339585672099689]
	TIME [epoch: 3.68 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3124962841895604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3124962841895604 | validation: 0.8308801169656738]
	TIME [epoch: 3.69 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8547546318984585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8547546318984585 | validation: 0.9018874360636057]
	TIME [epoch: 3.68 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8827839080759304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8827839080759304 | validation: 0.7744869519688682]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7702364755343786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7702364755343786 | validation: 0.785451968564892]
	TIME [epoch: 3.67 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.760805153510515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.760805153510515 | validation: 0.8676846483758617]
	TIME [epoch: 3.68 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9180178078349077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9180178078349077 | validation: 0.7672171453176276]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7740971252209699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7740971252209699 | validation: 0.8061248779424863]
	TIME [epoch: 3.67 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.743555689707471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.743555689707471 | validation: 0.7533941226278177]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7171530736949242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7171530736949242 | validation: 0.7764770859297843]
	TIME [epoch: 3.67 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7055812094482132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7055812094482132 | validation: 0.7858078137118826]
	TIME [epoch: 3.68 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7066439921578523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7066439921578523 | validation: 0.8074682695076929]
	TIME [epoch: 3.67 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7904298936857154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7904298936857154 | validation: 0.9621802531471536]
	TIME [epoch: 3.68 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0065964813544475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0065964813544475 | validation: 0.7178271353547458]
	TIME [epoch: 3.69 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7357861712245014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7357861712245014 | validation: 0.7614873940800644]
	TIME [epoch: 3.69 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7580762560915764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7580762560915764 | validation: 0.7189315798741678]
	TIME [epoch: 3.67 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7646642161610351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7646642161610351 | validation: 0.7374486922416787]
	TIME [epoch: 3.68 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7083593692646389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7083593692646389 | validation: 0.7112321502173496]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6996779349441807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6996779349441807 | validation: 0.7361609453602567]
	TIME [epoch: 3.68 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6730303102246145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6730303102246145 | validation: 0.7338138231147296]
	TIME [epoch: 3.67 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.668178121277235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.668178121277235 | validation: 0.7303516805259136]
	TIME [epoch: 3.67 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6668124442953723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6668124442953723 | validation: 0.7326004484995109]
	TIME [epoch: 3.66 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6749528687221542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6749528687221542 | validation: 0.7635812640598542]
	TIME [epoch: 3.67 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7937978988711657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7937978988711657 | validation: 0.7660313457136702]
	TIME [epoch: 3.66 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8470309304561436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8470309304561436 | validation: 0.7490921832537527]
	TIME [epoch: 3.66 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7809448313264804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7809448313264804 | validation: 0.7683252277006309]
	TIME [epoch: 3.66 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7803278663818949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7803278663818949 | validation: 0.7307889062021006]
	TIME [epoch: 3.68 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7313782378811129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7313782378811129 | validation: 0.8167448138140838]
	TIME [epoch: 3.68 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8473896050421413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8473896050421413 | validation: 0.9607594421569687]
	TIME [epoch: 3.67 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8893509102500826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8893509102500826 | validation: 0.8896925604939222]
	TIME [epoch: 3.67 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8061071583460039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8061071583460039 | validation: 0.6900900934736716]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7024382607173015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7024382607173015 | validation: 0.7689463496191503]
	TIME [epoch: 3.67 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7747267719743911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7747267719743911 | validation: 0.6869795211690466]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.712438833474295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.712438833474295 | validation: 0.7601643861781372]
	TIME [epoch: 3.67 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7165533464568138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7165533464568138 | validation: 0.7265442154084554]
	TIME [epoch: 3.67 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6791683611057409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6791683611057409 | validation: 0.7485210316961843]
	TIME [epoch: 3.66 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7568228679061935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7568228679061935 | validation: 0.7074727331286284]
	TIME [epoch: 3.66 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7117207898289388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7117207898289388 | validation: 0.7045380422145888]
	TIME [epoch: 3.67 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.69876893149489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.69876893149489 | validation: 0.7196782091717884]
	TIME [epoch: 3.67 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6677007141258892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6677007141258892 | validation: 0.7270063965097098]
	TIME [epoch: 3.68 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6875863570124944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6875863570124944 | validation: 0.7052503262906533]
	TIME [epoch: 3.69 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6951108766257336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6951108766257336 | validation: 0.7740031375001458]
	TIME [epoch: 3.68 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7246797226837576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7246797226837576 | validation: 0.9030648076680023]
	TIME [epoch: 3.68 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9242819919298677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9242819919298677 | validation: 0.6728275141337621]
	TIME [epoch: 3.68 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6704886104059915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6704886104059915 | validation: 0.8225969395041224]
	TIME [epoch: 3.66 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7740021885295246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7740021885295246 | validation: 0.8034895671358708]
	TIME [epoch: 3.66 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.785548075759977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.785548075759977 | validation: 0.7203402600584221]
	TIME [epoch: 3.66 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6796163023398848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6796163023398848 | validation: 0.7588486640626846]
	TIME [epoch: 3.66 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7110287655404409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7110287655404409 | validation: 0.6986241800073585]
	TIME [epoch: 3.66 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6773106869634551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6773106869634551 | validation: 0.7333592415086241]
	TIME [epoch: 3.66 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7152501742424505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7152501742424505 | validation: 0.711918024509789]
	TIME [epoch: 3.66 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7149766992923242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7149766992923242 | validation: 0.7244611067630381]
	TIME [epoch: 3.67 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6785468103766908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6785468103766908 | validation: 0.7193773380857973]
	TIME [epoch: 3.67 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6792754079562425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6792754079562425 | validation: 0.700996315089713]
	TIME [epoch: 3.67 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6944099159322198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6944099159322198 | validation: 0.7595109069184307]
	TIME [epoch: 3.66 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7009585620181745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7009585620181745 | validation: 0.7262163568358544]
	TIME [epoch: 3.66 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6894112336304801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6894112336304801 | validation: 0.7177626776023889]
	TIME [epoch: 3.66 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6751023252697935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6751023252697935 | validation: 0.7585946751354538]
	TIME [epoch: 3.66 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7412169978098991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7412169978098991 | validation: 0.6796761684072004]
	TIME [epoch: 3.65 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6731191366474921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6731191366474921 | validation: 0.7469322031738266]
	TIME [epoch: 3.65 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7523885598945667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7523885598945667 | validation: 0.7964506755732427]
	TIME [epoch: 3.65 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.786604964248819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.786604964248819 | validation: 0.84539276580363]
	TIME [epoch: 3.66 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7859886085842235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7859886085842235 | validation: 0.7319386271958328]
	TIME [epoch: 3.67 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7137981941821189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7137981941821189 | validation: 0.7721540238002865]
	TIME [epoch: 3.67 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7798346873530727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7798346873530727 | validation: 0.6973387201133978]
	TIME [epoch: 3.67 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7169768146490644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7169768146490644 | validation: 0.7518575596592569]
	TIME [epoch: 3.67 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7132078500390584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7132078500390584 | validation: 0.7187251897782593]
	TIME [epoch: 3.69 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6894595743990624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6894595743990624 | validation: 0.7208778486098053]
	TIME [epoch: 3.67 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6636265246505957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6636265246505957 | validation: 0.7198634440317363]
	TIME [epoch: 3.68 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6752127476095352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6752127476095352 | validation: 0.6966432781510195]
	TIME [epoch: 3.66 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6697141998164889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6697141998164889 | validation: 0.6786714357516872]
	TIME [epoch: 3.66 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6588415965562809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6588415965562809 | validation: 0.7012843967061063]
	TIME [epoch: 3.72 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6503501670232469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6503501670232469 | validation: 0.7029886133968575]
	TIME [epoch: 3.67 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6594857577959685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6594857577959685 | validation: 0.7091405567023297]
	TIME [epoch: 3.66 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6680641333805588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6680641333805588 | validation: 0.7486539141594815]
	TIME [epoch: 3.67 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.755018733483279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.755018733483279 | validation: 0.8775998008694293]
	TIME [epoch: 3.66 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8303950300991918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8303950300991918 | validation: 0.8307733033109358]
	TIME [epoch: 3.66 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8519411278941967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8519411278941967 | validation: 0.7135273352563494]
	TIME [epoch: 3.67 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7234412612429918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7234412612429918 | validation: 0.8105219072394578]
	TIME [epoch: 3.68 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.754222414944882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.754222414944882 | validation: 0.6989074108343151]
	TIME [epoch: 3.67 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6550556604346325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6550556604346325 | validation: 0.7091536166388969]
	TIME [epoch: 3.66 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6695756940563787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6695756940563787 | validation: 0.7155239203060456]
	TIME [epoch: 3.68 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6599043210358332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6599043210358332 | validation: 0.7255665734900405]
	TIME [epoch: 3.67 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6644087087668935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6644087087668935 | validation: 0.6976813947189501]
	TIME [epoch: 3.68 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6664585227542534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6664585227542534 | validation: 0.6873065654361418]
	TIME [epoch: 3.68 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6464906471489343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6464906471489343 | validation: 0.7082354551673098]
	TIME [epoch: 3.68 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6699028849829434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6699028849829434 | validation: 0.6851647146630153]
	TIME [epoch: 3.68 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6495522246697903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6495522246697903 | validation: 0.6897999104418303]
	TIME [epoch: 3.68 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6539872794675172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6539872794675172 | validation: 0.6866502041073361]
	TIME [epoch: 3.68 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6881813180069637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6881813180069637 | validation: 0.7015846092423597]
	TIME [epoch: 3.68 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.698747565252149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.698747565252149 | validation: 0.7269936116663565]
	TIME [epoch: 3.68 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7582981339279156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7582981339279156 | validation: 0.8088707724359931]
	TIME [epoch: 3.68 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8039833359334769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8039833359334769 | validation: 0.7987468401250228]
	TIME [epoch: 3.67 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7945953690640726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7945953690640726 | validation: 0.7393465154585744]
	TIME [epoch: 3.67 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7430081387216771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7430081387216771 | validation: 0.6889464273946959]
	TIME [epoch: 3.67 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6750787039385046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6750787039385046 | validation: 0.739886441121321]
	TIME [epoch: 3.67 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.690570896671263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.690570896671263 | validation: 0.7090227383696961]
	TIME [epoch: 3.67 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6628792439255429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6628792439255429 | validation: 0.7239698965660657]
	TIME [epoch: 3.67 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6716078201988911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6716078201988911 | validation: 0.7026066476034523]
	TIME [epoch: 3.67 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6867752788384324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6867752788384324 | validation: 0.7281954554327869]
	TIME [epoch: 3.67 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6747668342985975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6747668342985975 | validation: 0.6884229407667996]
	TIME [epoch: 3.66 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6834113794313091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6834113794313091 | validation: 0.7399731169459534]
	TIME [epoch: 3.67 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6675671615800904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6675671615800904 | validation: 0.7011967669514585]
	TIME [epoch: 3.68 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6534425167788376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6534425167788376 | validation: 0.7104324757736897]
	TIME [epoch: 3.69 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7362186073208815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7362186073208815 | validation: 0.8020469922548407]
	TIME [epoch: 3.68 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8186347079211794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8186347079211794 | validation: 0.7997126183865244]
	TIME [epoch: 3.68 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7964224727458689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7964224727458689 | validation: 0.7201417358353386]
	TIME [epoch: 3.67 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7264180162949475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7264180162949475 | validation: 0.7661917564281703]
	TIME [epoch: 3.68 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7379626613701566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7379626613701566 | validation: 0.7055208955092988]
	TIME [epoch: 3.67 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6625270226411939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6625270226411939 | validation: 0.714601564438164]
	TIME [epoch: 3.68 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6547773886737751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6547773886737751 | validation: 0.7162919438926663]
	TIME [epoch: 3.68 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.65671659152767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.65671659152767 | validation: 0.7090460182064894]
	TIME [epoch: 3.68 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6542614048629323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6542614048629323 | validation: 0.7068312442080852]
	TIME [epoch: 3.68 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6476082256438289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6476082256438289 | validation: 0.6894402073676126]
	TIME [epoch: 3.68 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6458026097477523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6458026097477523 | validation: 0.6851087948572285]
	TIME [epoch: 3.68 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6468742653047437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6468742653047437 | validation: 0.6996086952157388]
	TIME [epoch: 3.7 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6588082477472043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6588082477472043 | validation: 0.7292680111768985]
	TIME [epoch: 3.67 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7012738668251061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7012738668251061 | validation: 0.7448044231027037]
	TIME [epoch: 3.67 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7339953357904616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7339953357904616 | validation: 0.7799564780025566]
	TIME [epoch: 3.67 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8071979889518213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8071979889518213 | validation: 0.7184873328148138]
	TIME [epoch: 3.67 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7574673393607421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7574673393607421 | validation: 0.6990711112859698]
	TIME [epoch: 3.68 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7102930549249565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7102930549249565 | validation: 0.6977654766718534]
	TIME [epoch: 3.67 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6575089018578438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6575089018578438 | validation: 0.7008314632476567]
	TIME [epoch: 3.68 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6582838722171228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6582838722171228 | validation: 0.6924294727034791]
	TIME [epoch: 3.67 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6466120459185511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6466120459185511 | validation: 0.7072480356139816]
	TIME [epoch: 3.67 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6560936184804393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6560936184804393 | validation: 0.7000650300366926]
	TIME [epoch: 3.68 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6643456741520362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6643456741520362 | validation: 0.6859991771541023]
	TIME [epoch: 3.72 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.643973446554985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.643973446554985 | validation: 0.6659135985808086]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_159.pth
	Model improved!!!
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6487479190705273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6487479190705273 | validation: 0.6850396563730299]
	TIME [epoch: 3.67 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6552390304430188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6552390304430188 | validation: 0.6882303398583776]
	TIME [epoch: 3.67 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7222426596031482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7222426596031482 | validation: 0.6890102199590054]
	TIME [epoch: 3.67 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6773471415662325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6773471415662325 | validation: 0.7851074565446551]
	TIME [epoch: 3.67 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7921690196637299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7921690196637299 | validation: 0.7128448042030607]
	TIME [epoch: 3.67 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7610104882598856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7610104882598856 | validation: 0.7529764867378592]
	TIME [epoch: 3.66 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.756326524405429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.756326524405429 | validation: 0.6827764305671279]
	TIME [epoch: 3.67 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6759071222792267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6759071222792267 | validation: 0.7475011955896858]
	TIME [epoch: 3.67 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.683750716840113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.683750716840113 | validation: 0.6971573618673784]
	TIME [epoch: 3.67 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6503755525173825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6503755525173825 | validation: 0.6796384670280835]
	TIME [epoch: 3.68 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6615509264654564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6615509264654564 | validation: 0.7081776517744791]
	TIME [epoch: 3.66 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6556598196321148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6556598196321148 | validation: 0.6829347990303785]
	TIME [epoch: 3.66 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6573748659013612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6573748659013612 | validation: 0.7082729353869663]
	TIME [epoch: 3.67 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6628220820980741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6628220820980741 | validation: 0.659215759803346]
	TIME [epoch: 3.67 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6674923484354417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6674923484354417 | validation: 0.6917808195138968]
	TIME [epoch: 3.67 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6472072023377068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6472072023377068 | validation: 0.7309223998344582]
	TIME [epoch: 3.68 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7495561019378681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7495561019378681 | validation: 0.6680074792567373]
	TIME [epoch: 3.67 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6628535482686906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6628535482686906 | validation: 0.733905118377471]
	TIME [epoch: 3.68 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7266591466735238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7266591466735238 | validation: 0.8094406676942362]
	TIME [epoch: 3.67 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.792376241446112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.792376241446112 | validation: 0.7231060048307922]
	TIME [epoch: 3.68 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7145145142011459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7145145142011459 | validation: 0.6770154700524378]
	TIME [epoch: 3.68 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6799409933698085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6799409933698085 | validation: 0.6799857209653523]
	TIME [epoch: 3.69 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6823294031339462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6823294031339462 | validation: 0.6923111688083013]
	TIME [epoch: 3.68 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6401466833987712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6401466833987712 | validation: 0.7008815453280453]
	TIME [epoch: 3.68 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6592464821748846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6592464821748846 | validation: 0.6755356778920643]
	TIME [epoch: 3.67 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6573794501077868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6573794501077868 | validation: 0.7188372450803788]
	TIME [epoch: 3.68 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6618040225917645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6618040225917645 | validation: 0.7176758648498629]
	TIME [epoch: 3.68 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7007748009959694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7007748009959694 | validation: 0.7114213404453475]
	TIME [epoch: 3.68 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6698762092517424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6698762092517424 | validation: 0.6687908653487491]
	TIME [epoch: 3.67 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6710484287583239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6710484287583239 | validation: 0.6987240843222424]
	TIME [epoch: 3.67 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.655284747169086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.655284747169086 | validation: 0.6894508525378117]
	TIME [epoch: 3.67 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6497113830887722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6497113830887722 | validation: 0.7181903905989429]
	TIME [epoch: 3.67 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.657352405236021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.657352405236021 | validation: 0.7327122356012234]
	TIME [epoch: 3.68 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.757612102653185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.757612102653185 | validation: 0.6664330871152324]
	TIME [epoch: 3.67 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6973026355506198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6973026355506198 | validation: 0.7444062630558703]
	TIME [epoch: 3.69 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6954105013948358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6954105013948358 | validation: 0.6851996307805198]
	TIME [epoch: 3.68 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6659910253305573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6659910253305573 | validation: 0.698894086666458]
	TIME [epoch: 3.67 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6331605110713213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6331605110713213 | validation: 0.6790144609181817]
	TIME [epoch: 3.68 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6279769852540591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6279769852540591 | validation: 0.6871267576663773]
	TIME [epoch: 3.68 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6491093811614846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6491093811614846 | validation: 0.7101177912639898]
	TIME [epoch: 3.68 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6537952014071005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6537952014071005 | validation: 0.7696870778772725]
	TIME [epoch: 3.67 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7586882126690812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7586882126690812 | validation: 0.6975900612233352]
	TIME [epoch: 30.3 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6466227933344465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6466227933344465 | validation: 0.6846401457259594]
	TIME [epoch: 7.97 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6585196801066932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6585196801066932 | validation: 0.700804753953951]
	TIME [epoch: 7.98 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6610211807818109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6610211807818109 | validation: 0.7164213233091025]
	TIME [epoch: 7.98 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6675844011316994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6675844011316994 | validation: 0.6711193981610092]
	TIME [epoch: 7.98 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6962528468183148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6962528468183148 | validation: 0.6518824552897033]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_206.pth
	Model improved!!!
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.639454208422574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.639454208422574 | validation: 0.6913813075325245]
	TIME [epoch: 7.96 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6961002452453963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6961002452453963 | validation: 0.6666496657021093]
	TIME [epoch: 7.95 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6706359009426498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6706359009426498 | validation: 0.7230236646547124]
	TIME [epoch: 7.94 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6875178849314929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6875178849314929 | validation: 0.728793258128897]
	TIME [epoch: 7.96 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7079241102808977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7079241102808977 | validation: 0.6888946821496321]
	TIME [epoch: 7.94 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.663065382462705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.663065382462705 | validation: 0.6600872482978557]
	TIME [epoch: 7.99 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6507726727712162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6507726727712162 | validation: 0.6631728786496388]
	TIME [epoch: 7.96 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6389891201362733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6389891201362733 | validation: 0.6985081392151453]
	TIME [epoch: 7.96 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6462370071000576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6462370071000576 | validation: 0.6738977702904165]
	TIME [epoch: 7.94 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6428608433914907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6428608433914907 | validation: 0.6873774780393029]
	TIME [epoch: 7.95 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6368941946346006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6368941946346006 | validation: 0.6672382692410003]
	TIME [epoch: 7.94 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.632955790244435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.632955790244435 | validation: 0.6936499349568143]
	TIME [epoch: 7.95 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6402552361739697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6402552361739697 | validation: 0.6488886926247638]
	TIME [epoch: 7.94 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6388339630734281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6388339630734281 | validation: 0.6994355597098307]
	TIME [epoch: 7.97 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6314471491230071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6314471491230071 | validation: 0.6640983619283184]
	TIME [epoch: 7.96 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6695885730882517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6695885730882517 | validation: 0.7519852114973059]
	TIME [epoch: 7.96 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.718955573398127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.718955573398127 | validation: 0.7811386851555451]
	TIME [epoch: 7.97 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7953961272977667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7953961272977667 | validation: 0.6877945211779217]
	TIME [epoch: 7.98 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6650203925591711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6650203925591711 | validation: 0.7325630554718521]
	TIME [epoch: 7.98 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7040729129304729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7040729129304729 | validation: 0.6750222214230339]
	TIME [epoch: 7.96 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.663933840131018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.663933840131018 | validation: 0.6885851107130483]
	TIME [epoch: 7.97 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6462997599266125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6462997599266125 | validation: 0.6654669711932357]
	TIME [epoch: 7.96 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6364965627495819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6364965627495819 | validation: 0.6653509917472137]
	TIME [epoch: 7.96 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.621702056185402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.621702056185402 | validation: 0.6757735093331526]
	TIME [epoch: 7.97 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6224863282517924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6224863282517924 | validation: 0.652749562828994]
	TIME [epoch: 7.96 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6340278506097345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6340278506097345 | validation: 0.6657990822343861]
	TIME [epoch: 7.96 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.629174261605523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.629174261605523 | validation: 0.6564755323834914]
	TIME [epoch: 7.96 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6300557232401645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6300557232401645 | validation: 0.6817858740853332]
	TIME [epoch: 7.97 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6346542125554597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6346542125554597 | validation: 0.668803255718851]
	TIME [epoch: 7.97 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6704483838266958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6704483838266958 | validation: 0.7274817148118444]
	TIME [epoch: 7.97 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6828645707475086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6828645707475086 | validation: 0.6788162710419257]
	TIME [epoch: 7.97 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7216017005383887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7216017005383887 | validation: 0.6590900491958078]
	TIME [epoch: 7.97 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6282478098655498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6282478098655498 | validation: 0.6820725073768538]
	TIME [epoch: 7.97 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6340098708217016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6340098708217016 | validation: 0.6820732003427912]
	TIME [epoch: 7.97 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6553195611692556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6553195611692556 | validation: 0.6793979160143834]
	TIME [epoch: 7.97 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6348686933509028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6348686933509028 | validation: 0.6440598783772992]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6259025762629303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6259025762629303 | validation: 0.6501138745193917]
	TIME [epoch: 8 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6130287636925535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6130287636925535 | validation: 0.6778753895241509]
	TIME [epoch: 7.96 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6255400474591312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6255400474591312 | validation: 0.6406810282366467]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_245.pth
	Model improved!!!
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6247435312719195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6247435312719195 | validation: 0.6645194164487535]
	TIME [epoch: 7.95 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6194319083756722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6194319083756722 | validation: 0.6338908188339358]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.630627606704094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.630627606704094 | validation: 0.6763674865503131]
	TIME [epoch: 7.95 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6617331146080331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6617331146080331 | validation: 0.6726697566827904]
	TIME [epoch: 7.95 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7080881416265613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7080881416265613 | validation: 0.7283102039063071]
	TIME [epoch: 7.95 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7153026541378243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7153026541378243 | validation: 0.7094528463692145]
	TIME [epoch: 7.96 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6940632784649481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6940632784649481 | validation: 0.6648222910447711]
	TIME [epoch: 7.98 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6375232834253645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6375232834253645 | validation: 0.6879874245327583]
	TIME [epoch: 8.03 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6327988367583596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6327988367583596 | validation: 0.6597842987622886]
	TIME [epoch: 7.97 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6476290962328536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6476290962328536 | validation: 0.6692193534953069]
	TIME [epoch: 7.96 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6169260261145205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6169260261145205 | validation: 0.6698023088546979]
	TIME [epoch: 7.95 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6284429600512095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6284429600512095 | validation: 0.6408938921193219]
	TIME [epoch: 7.95 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6209996555565923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6209996555565923 | validation: 0.6942631490921957]
	TIME [epoch: 7.97 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6310282102684669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6310282102684669 | validation: 0.6751160921798716]
	TIME [epoch: 7.93 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6516607709495118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6516607709495118 | validation: 0.6780941414582276]
	TIME [epoch: 7.95 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6266288469090822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6266288469090822 | validation: 0.6265514734036048]
	TIME [epoch: 7.96 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_261.pth
	Model improved!!!
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6437041563916981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6437041563916981 | validation: 0.6589968678352965]
	TIME [epoch: 7.99 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6130143701743357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6130143701743357 | validation: 0.6534347272144183]
	TIME [epoch: 7.99 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6187679650217097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6187679650217097 | validation: 0.6599775429128398]
	TIME [epoch: 8.02 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6102533664470434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6102533664470434 | validation: 0.6386808338923142]
	TIME [epoch: 7.98 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5988483844032798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5988483844032798 | validation: 0.6962447906594638]
	TIME [epoch: 8.01 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6417931162950342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6417931162950342 | validation: 0.7295984599535514]
	TIME [epoch: 7.99 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7846310526009947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7846310526009947 | validation: 0.6994053670067786]
	TIME [epoch: 7.99 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7205325046933878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7205325046933878 | validation: 0.6881809228162862]
	TIME [epoch: 7.99 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6822804113653328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6822804113653328 | validation: 0.6594881097564271]
	TIME [epoch: 8 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6253864477553467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6253864477553467 | validation: 0.6564759108734496]
	TIME [epoch: 7.97 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6078887755898518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6078887755898518 | validation: 0.6461302062139543]
	TIME [epoch: 8 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6018381114679922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6018381114679922 | validation: 0.6547973611853063]
	TIME [epoch: 8 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5960608182056116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5960608182056116 | validation: 0.6505025765918712]
	TIME [epoch: 8 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.596631873434125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.596631873434125 | validation: 0.6366279618544562]
	TIME [epoch: 8.02 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.587497197910063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.587497197910063 | validation: 0.6444780192109393]
	TIME [epoch: 8 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5870671813868052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5870671813868052 | validation: 0.6223757131276083]
	TIME [epoch: 8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5835405941912859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5835405941912859 | validation: 0.6663522491158085]
	TIME [epoch: 7.95 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.591145599604116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.591145599604116 | validation: 0.7548427785777374]
	TIME [epoch: 7.97 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8393440731895558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8393440731895558 | validation: 0.7781426363695435]
	TIME [epoch: 7.96 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.783055805821657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.783055805821657 | validation: 0.6108612982286472]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_281.pth
	Model improved!!!
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6014938148982841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6014938148982841 | validation: 0.6175233443416047]
	TIME [epoch: 7.96 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5900182133507517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5900182133507517 | validation: 0.6328520121445631]
	TIME [epoch: 7.96 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5965451821168785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5965451821168785 | validation: 0.5847480000012019]
	TIME [epoch: 7.94 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_284.pth
	Model improved!!!
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5943015487522151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5943015487522151 | validation: 0.5947063648812881]
	TIME [epoch: 7.97 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5662200268835204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5662200268835204 | validation: 0.6333677230775641]
	TIME [epoch: 7.97 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5658804322186025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5658804322186025 | validation: 0.5865689408463438]
	TIME [epoch: 7.97 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.555945698066021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.555945698066021 | validation: 0.6018971788979408]
	TIME [epoch: 7.97 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5461631421073154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5461631421073154 | validation: 0.5714014508738311]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_289.pth
	Model improved!!!
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5265640306170334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5265640306170334 | validation: 0.6195589518593994]
	TIME [epoch: 7.98 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5188006671291361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5188006671291361 | validation: 0.5305981354520161]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5175051670428819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5175051670428819 | validation: 0.5361693080999296]
	TIME [epoch: 7.98 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49281118710346994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49281118710346994 | validation: 0.8122033225760075]
	TIME [epoch: 7.96 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0980495381003257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0980495381003257 | validation: 0.7299153449866357]
	TIME [epoch: 7.96 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.804708399797602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.804708399797602 | validation: 0.6901913353955293]
	TIME [epoch: 7.97 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7195753448622714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7195753448622714 | validation: 0.5999788152780722]
	TIME [epoch: 7.96 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6506936778233269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6506936778233269 | validation: 0.6045448854235164]
	TIME [epoch: 7.97 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5794212016376515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5794212016376515 | validation: 0.5816609030760395]
	TIME [epoch: 7.98 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5710559187770082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5710559187770082 | validation: 0.5825139615078188]
	TIME [epoch: 7.96 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5441675945036224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5441675945036224 | validation: 0.5685190838888371]
	TIME [epoch: 7.96 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5325311263899015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5325311263899015 | validation: 0.5690999966963634]
	TIME [epoch: 8.01 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5187685434136263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5187685434136263 | validation: 0.5609753496937486]
	TIME [epoch: 7.99 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5026671756236425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5026671756236425 | validation: 0.5213835316782108]
	TIME [epoch: 7.99 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_303.pth
	Model improved!!!
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47326304162107635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47326304162107635 | validation: 0.5455111846810692]
	TIME [epoch: 7.99 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4573429730253298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4573429730253298 | validation: 0.8017399787003567]
	TIME [epoch: 7.97 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8550032574320784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8550032574320784 | validation: 0.9045849433035429]
	TIME [epoch: 7.97 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2078208323635113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2078208323635113 | validation: 0.6311579162061478]
	TIME [epoch: 7.96 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7444510247994253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7444510247994253 | validation: 0.7484042991295268]
	TIME [epoch: 7.99 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7815910474894245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7815910474894245 | validation: 0.6323611109956959]
	TIME [epoch: 7.98 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6530212360885694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6530212360885694 | validation: 0.6103128450848248]
	TIME [epoch: 7.98 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6322995004810134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6322995004810134 | validation: 0.5873559642762278]
	TIME [epoch: 7.98 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5642594854101721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5642594854101721 | validation: 0.5822297877637735]
	TIME [epoch: 7.98 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5597292490444131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5597292490444131 | validation: 0.5732312038982784]
	TIME [epoch: 7.96 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5491062103991167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5491062103991167 | validation: 0.5855976519492777]
	TIME [epoch: 7.99 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5384655473225206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5384655473225206 | validation: 0.5754389194196972]
	TIME [epoch: 7.98 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5293429257254565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5293429257254565 | validation: 0.5866110262567069]
	TIME [epoch: 7.99 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5244469375602069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5244469375602069 | validation: 0.5717056507469664]
	TIME [epoch: 7.97 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5186464460278362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5186464460278362 | validation: 0.5562745044539289]
	TIME [epoch: 7.99 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5084881485701456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5084881485701456 | validation: 0.5679476498022192]
	TIME [epoch: 7.97 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4989289334154849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4989289334154849 | validation: 0.534208439662127]
	TIME [epoch: 8 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48666385243385224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48666385243385224 | validation: 0.5692675392380958]
	TIME [epoch: 7.97 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48172241124907317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48172241124907317 | validation: 0.5401451423187058]
	TIME [epoch: 7.98 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.516585474521116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.516585474521116 | validation: 0.6663851312208303]
	TIME [epoch: 7.96 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6179702829726518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6179702829726518 | validation: 0.48824360373403963]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_324.pth
	Model improved!!!
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46994114073583154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46994114073583154 | validation: 0.4758253300305866]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_325.pth
	Model improved!!!
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40211697229959326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40211697229959326 | validation: 0.45382340563277845]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_326.pth
	Model improved!!!
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4001846077308018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4001846077308018 | validation: 0.5150256419775179]
	TIME [epoch: 7.97 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.547273803036206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.547273803036206 | validation: 0.470282659829806]
	TIME [epoch: 7.97 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4999190394088302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4999190394088302 | validation: 0.43664693156972556]
	TIME [epoch: 7.96 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_329.pth
	Model improved!!!
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38515201410009314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38515201410009314 | validation: 0.5057563896795095]
	TIME [epoch: 7.98 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5153837825284134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5153837825284134 | validation: 0.6498229423775798]
	TIME [epoch: 7.99 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8817353136716898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8817353136716898 | validation: 0.6351937599241949]
	TIME [epoch: 7.98 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7501242700102863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7501242700102863 | validation: 0.522290997342097]
	TIME [epoch: 7.95 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5938231698353811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5938231698353811 | validation: 0.48635267543512023]
	TIME [epoch: 7.99 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46507867264672315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46507867264672315 | validation: 0.4277231862747005]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_335.pth
	Model improved!!!
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3753745587806083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3753745587806083 | validation: 0.44197492588084464]
	TIME [epoch: 7.97 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39566912903831225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39566912903831225 | validation: 0.49766515586201676]
	TIME [epoch: 7.97 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45122284779729094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45122284779729094 | validation: 0.42738479251798467]
	TIME [epoch: 7.94 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_338.pth
	Model improved!!!
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4050587115077374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4050587115077374 | validation: 0.38373764779588676]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_339.pth
	Model improved!!!
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3356751944305648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3356751944305648 | validation: 0.3953320144153043]
	TIME [epoch: 8 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3457697227455904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3457697227455904 | validation: 0.35760524581075476]
	TIME [epoch: 8.01 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_341.pth
	Model improved!!!
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3204114638778898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3204114638778898 | validation: 0.3746506987345447]
	TIME [epoch: 7.98 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3572309757691649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3572309757691649 | validation: 0.5227177221069621]
	TIME [epoch: 7.97 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5683013845456766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5683013845456766 | validation: 0.5523817635014201]
	TIME [epoch: 7.95 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.835886375855738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.835886375855738 | validation: 0.48877529113691875]
	TIME [epoch: 7.96 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7325504311750174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7325504311750174 | validation: 0.7034939945415584]
	TIME [epoch: 7.97 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7265258184894835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7265258184894835 | validation: 0.486278524032153]
	TIME [epoch: 7.96 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.453441999266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.453441999266 | validation: 0.5545078565573237]
	TIME [epoch: 8.02 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4746887640472746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4746887640472746 | validation: 0.41630845722406556]
	TIME [epoch: 7.95 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39116286815410506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39116286815410506 | validation: 0.40223088676145835]
	TIME [epoch: 7.96 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36059336447921797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36059336447921797 | validation: 0.3460749464575297]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_351.pth
	Model improved!!!
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3259047522718283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3259047522718283 | validation: 0.33973269612796314]
	TIME [epoch: 7.96 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_352.pth
	Model improved!!!
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31001826708368724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31001826708368724 | validation: 0.3820438362118478]
	TIME [epoch: 7.98 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3340528381457598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3340528381457598 | validation: 0.34929431300809366]
	TIME [epoch: 7.99 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3323943401819419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3323943401819419 | validation: 0.38368503668986603]
	TIME [epoch: 7.96 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3866769359347642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3866769359347642 | validation: 0.3322652543215253]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_356.pth
	Model improved!!!
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3034024253709209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3034024253709209 | validation: 0.32409985643591266]
	TIME [epoch: 7.99 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_357.pth
	Model improved!!!
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.276002453623374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.276002453623374 | validation: 0.32319920497360194]
	TIME [epoch: 7.96 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_358.pth
	Model improved!!!
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32528371070003864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32528371070003864 | validation: 0.406287323322464]
	TIME [epoch: 7.98 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39276261896854125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39276261896854125 | validation: 0.40642592513157577]
	TIME [epoch: 7.98 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5261833205812243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5261833205812243 | validation: 0.4008398324002098]
	TIME [epoch: 7.98 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38671576269942565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38671576269942565 | validation: 0.35522528309786705]
	TIME [epoch: 7.99 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28789221653233943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28789221653233943 | validation: 0.3925821381294073]
	TIME [epoch: 7.99 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3692796990955457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3692796990955457 | validation: 0.41893126749368625]
	TIME [epoch: 7.99 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47563855172711333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47563855172711333 | validation: 0.4423648918091053]
	TIME [epoch: 7.99 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4436227988707438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4436227988707438 | validation: 0.3860361304373066]
	TIME [epoch: 7.97 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3442365298929394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3442365298929394 | validation: 0.3253860746803321]
	TIME [epoch: 7.98 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28120156896420817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28120156896420817 | validation: 0.4423606626224563]
	TIME [epoch: 7.98 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4270588440803305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4270588440803305 | validation: 0.5476201978025278]
	TIME [epoch: 7.97 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5448808629404057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5448808629404057 | validation: 0.4179631757999029]
	TIME [epoch: 7.99 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4012086826889054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4012086826889054 | validation: 0.4232497551552076]
	TIME [epoch: 7.98 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40354996904837964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40354996904837964 | validation: 0.3576301259853796]
	TIME [epoch: 7.97 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3116071122121028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3116071122121028 | validation: 0.28562086880555704]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_373.pth
	Model improved!!!
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24255952016528093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24255952016528093 | validation: 0.27777961324647266]
	TIME [epoch: 7.97 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_374.pth
	Model improved!!!
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24313923664276801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24313923664276801 | validation: 0.2774432314867407]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_375.pth
	Model improved!!!
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25876882058402245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25876882058402245 | validation: 0.27693405255562586]
	TIME [epoch: 8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_376.pth
	Model improved!!!
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24412328160800761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24412328160800761 | validation: 0.27863213764345013]
	TIME [epoch: 7.97 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23185689643048818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23185689643048818 | validation: 0.26831382956463834]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_378.pth
	Model improved!!!
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22608921927050216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22608921927050216 | validation: 0.2607825319815679]
	TIME [epoch: 7.99 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_379.pth
	Model improved!!!
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33718389584673786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33718389584673786 | validation: 0.2945257954700154]
	TIME [epoch: 7.97 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2649274801551827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2649274801551827 | validation: 0.3236558116889357]
	TIME [epoch: 7.98 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23603170396414072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23603170396414072 | validation: 0.2272386334169883]
	TIME [epoch: 7.99 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_382.pth
	Model improved!!!
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24332993281683743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24332993281683743 | validation: 0.26322595375239655]
	TIME [epoch: 7.99 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23090362295126393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23090362295126393 | validation: 0.33648717230372066]
	TIME [epoch: 8 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2962352984255836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2962352984255836 | validation: 0.29911719633965933]
	TIME [epoch: 7.99 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21553589243032076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21553589243032076 | validation: 0.4280738782115072]
	TIME [epoch: 8.01 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3871510010746515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3871510010746515 | validation: 0.48631718410160496]
	TIME [epoch: 8 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6945233397545618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6945233397545618 | validation: 0.4704207969991215]
	TIME [epoch: 7.99 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5188307600123354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5188307600123354 | validation: 0.4598423697650542]
	TIME [epoch: 8.01 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4112143006146359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4112143006146359 | validation: 0.3492174392009507]
	TIME [epoch: 7.99 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24644694914895457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24644694914895457 | validation: 0.36153460768392587]
	TIME [epoch: 7.99 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2708301123263167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2708301123263167 | validation: 0.3721082772788172]
	TIME [epoch: 7.99 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35019534402397723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35019534402397723 | validation: 0.2579166208673769]
	TIME [epoch: 8.02 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2308414441250496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2308414441250496 | validation: 0.23852848242653224]
	TIME [epoch: 8 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1801678062302046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1801678062302046 | validation: 0.259974857589959]
	TIME [epoch: 7.99 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20051757806949078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20051757806949078 | validation: 0.22711252524960088]
	TIME [epoch: 8.02 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_396.pth
	Model improved!!!
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18649924943137616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18649924943137616 | validation: 0.22232053368581373]
	TIME [epoch: 7.99 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_397.pth
	Model improved!!!
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1919399131199673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1919399131199673 | validation: 0.2178857960528774]
	TIME [epoch: 8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_398.pth
	Model improved!!!
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17530475436909526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17530475436909526 | validation: 0.1903392490719083]
	TIME [epoch: 7.98 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_399.pth
	Model improved!!!
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17439423641223464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17439423641223464 | validation: 0.20064482581221516]
	TIME [epoch: 8 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15444741336376752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15444741336376752 | validation: 0.20429232090831975]
	TIME [epoch: 7.95 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18162059199414116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18162059199414116 | validation: 0.18595470528334915]
	TIME [epoch: 7.93 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_402.pth
	Model improved!!!
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13863047867385483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13863047867385483 | validation: 0.18942091746819872]
	TIME [epoch: 7.98 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14676488830948312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14676488830948312 | validation: 0.2097833780483166]
	TIME [epoch: 7.98 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2354629346276379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2354629346276379 | validation: 0.21484780092214778]
	TIME [epoch: 7.96 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15588801528340265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15588801528340265 | validation: 0.5286561704400211]
	TIME [epoch: 7.97 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5631346540782743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5631346540782743 | validation: 0.6698828718392122]
	TIME [epoch: 7.96 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9163265725891332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9163265725891332 | validation: 1.1159976291712839]
	TIME [epoch: 7.98 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5146810883930746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5146810883930746 | validation: 0.9419295604429263]
	TIME [epoch: 7.97 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2191220223989163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2191220223989163 | validation: 0.6229153473380514]
	TIME [epoch: 7.97 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7987583635094913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7987583635094913 | validation: 0.6828655559875245]
	TIME [epoch: 7.97 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6283692257094294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6283692257094294 | validation: 0.5093466127376357]
	TIME [epoch: 7.97 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42964588699904765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42964588699904765 | validation: 0.4188762302119017]
	TIME [epoch: 7.97 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3139163154161655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3139163154161655 | validation: 0.33768747453935766]
	TIME [epoch: 7.97 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29369203059096644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29369203059096644 | validation: 0.2798673932768386]
	TIME [epoch: 7.98 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2598412875420685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2598412875420685 | validation: 0.25467814871823047]
	TIME [epoch: 7.98 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23063366557671053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23063366557671053 | validation: 0.2307134960039993]
	TIME [epoch: 7.97 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17471139238570907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17471139238570907 | validation: 0.23974630747991899]
	TIME [epoch: 7.96 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1776448686735769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1776448686735769 | validation: 0.20146981400959088]
	TIME [epoch: 7.98 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1552222432237665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1552222432237665 | validation: 0.2079326297937918]
	TIME [epoch: 7.98 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15708995640429774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15708995640429774 | validation: 0.1929469786134513]
	TIME [epoch: 7.98 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15260850253105507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15260850253105507 | validation: 0.1764871677197477]
	TIME [epoch: 8.01 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_422.pth
	Model improved!!!
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15633746026737677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15633746026737677 | validation: 0.24524745893624536]
	TIME [epoch: 8.01 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19858290882540147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19858290882540147 | validation: 0.258133953419187]
	TIME [epoch: 8 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2806133630292428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2806133630292428 | validation: 0.2793563008289112]
	TIME [epoch: 8.01 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21280775363611096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21280775363611096 | validation: 0.23217437141101543]
	TIME [epoch: 8.02 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15365936948481385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15365936948481385 | validation: 0.16672728381761628]
	TIME [epoch: 8.01 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_427.pth
	Model improved!!!
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1574737347090759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1574737347090759 | validation: 0.21686992281010986]
	TIME [epoch: 8.03 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16086729262970484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16086729262970484 | validation: 0.2348186546158451]
	TIME [epoch: 8.02 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21885248541564395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21885248541564395 | validation: 0.21291624859833239]
	TIME [epoch: 8.02 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15702537197748506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15702537197748506 | validation: 0.35640419568500736]
	TIME [epoch: 8.03 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34871224647348753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34871224647348753 | validation: 0.482685632303069]
	TIME [epoch: 8.03 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6882632765809663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6882632765809663 | validation: 0.43347431712553475]
	TIME [epoch: 8.01 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4732037562305982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4732037562305982 | validation: 0.3046825931743103]
	TIME [epoch: 8.02 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2564547873214055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2564547873214055 | validation: 0.26719713602639]
	TIME [epoch: 8.02 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20422029397825608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20422029397825608 | validation: 0.393664125280257]
	TIME [epoch: 8.02 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27071261401829877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27071261401829877 | validation: 0.2556667740294362]
	TIME [epoch: 8.03 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22528570587307542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22528570587307542 | validation: 0.22647948724866684]
	TIME [epoch: 8.02 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19077635694296802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19077635694296802 | validation: 0.17570745885102346]
	TIME [epoch: 8.01 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1474453454252166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1474453454252166 | validation: 0.17983167555787838]
	TIME [epoch: 8.02 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15689343397948943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15689343397948943 | validation: 0.1621826273049182]
	TIME [epoch: 8.02 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_441.pth
	Model improved!!!
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13608145480394693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13608145480394693 | validation: 0.17115187226108672]
	TIME [epoch: 8.02 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.132672043202168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.132672043202168 | validation: 0.1594402882341443]
	TIME [epoch: 8.04 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_443.pth
	Model improved!!!
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12773333248442398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12773333248442398 | validation: 0.16617487490456395]
	TIME [epoch: 8.02 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1364606803237405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1364606803237405 | validation: 0.20986767763831118]
	TIME [epoch: 8.02 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20151244789615458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20151244789615458 | validation: 0.18787052279808802]
	TIME [epoch: 8.01 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16995492757550232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16995492757550232 | validation: 0.23006304212979042]
	TIME [epoch: 8.01 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19544283050803046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19544283050803046 | validation: 0.22851533384587094]
	TIME [epoch: 8.03 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32183522713735413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32183522713735413 | validation: 0.32992621632543695]
	TIME [epoch: 8.01 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22894408215811182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22894408215811182 | validation: 0.34540210154636175]
	TIME [epoch: 8.02 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2658615559889041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2658615559889041 | validation: 0.3323888896902603]
	TIME [epoch: 8.01 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.346034643982356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.346034643982356 | validation: 0.36863131816847683]
	TIME [epoch: 8.01 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3701720178905205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3701720178905205 | validation: 0.2624237935064135]
	TIME [epoch: 8.01 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2551470976580609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2551470976580609 | validation: 0.23446227695817756]
	TIME [epoch: 8.03 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20754575363276206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20754575363276206 | validation: 0.3158795787583175]
	TIME [epoch: 8.01 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22441218478250144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22441218478250144 | validation: 0.25325344861479565]
	TIME [epoch: 8.02 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17554758741739435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17554758741739435 | validation: 0.22466298475942012]
	TIME [epoch: 8.01 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.159080356654127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.159080356654127 | validation: 0.1771409609275658]
	TIME [epoch: 8.02 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1904027509430555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1904027509430555 | validation: 0.15813950938963467]
	TIME [epoch: 8.02 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_459.pth
	Model improved!!!
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14903347459349522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14903347459349522 | validation: 0.21795254319099444]
	TIME [epoch: 8.01 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17508467021347174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17508467021347174 | validation: 0.20635392085018306]
	TIME [epoch: 8.01 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22646615348616134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22646615348616134 | validation: 0.19643547609357626]
	TIME [epoch: 8 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18684326414533187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18684326414533187 | validation: 0.23560436280709035]
	TIME [epoch: 8 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1797270502039517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1797270502039517 | validation: 0.24772238414731274]
	TIME [epoch: 8 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24997744134902622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24997744134902622 | validation: 0.2823193661769944]
	TIME [epoch: 8.01 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26817097373423937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26817097373423937 | validation: 0.1546586471118116]
	TIME [epoch: 8 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_466.pth
	Model improved!!!
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13902859514347163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13902859514347163 | validation: 0.2859374924557047]
	TIME [epoch: 8 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2359377776382919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2359377776382919 | validation: 0.36254934426815133]
	TIME [epoch: 8 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3708405607952673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3708405607952673 | validation: 0.39820186352240744]
	TIME [epoch: 8.01 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.514952144635446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.514952144635446 | validation: 0.29058196215709464]
	TIME [epoch: 8 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42826534726972826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42826534726972826 | validation: 0.31379749145581953]
	TIME [epoch: 8.02 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18788151252877214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18788151252877214 | validation: 0.35654680686694085]
	TIME [epoch: 8 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28969543902467615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28969543902467615 | validation: 0.27760063701918547]
	TIME [epoch: 8.01 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2889933171029836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2889933171029836 | validation: 0.26920988530285267]
	TIME [epoch: 8 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3022443873037724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3022443873037724 | validation: 0.18431300500251532]
	TIME [epoch: 8.01 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19230357100083262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19230357100083262 | validation: 0.18209442978269924]
	TIME [epoch: 8.01 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15928894344869832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15928894344869832 | validation: 0.3289935310231673]
	TIME [epoch: 8.02 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2554639645224308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2554639645224308 | validation: 0.3478190416183942]
	TIME [epoch: 8.01 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30559315385352454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30559315385352454 | validation: 0.31957932084015045]
	TIME [epoch: 8.03 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26146833106011547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26146833106011547 | validation: 0.21043375563947173]
	TIME [epoch: 8 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18183268174557085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18183268174557085 | validation: 0.20442659524169385]
	TIME [epoch: 8.01 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17283086021536145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17283086021536145 | validation: 0.20696013194081894]
	TIME [epoch: 8.02 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1742573645749492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1742573645749492 | validation: 0.21290035396838386]
	TIME [epoch: 8.01 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2016077467077267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2016077467077267 | validation: 0.2124810337367977]
	TIME [epoch: 8 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18926504909538433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18926504909538433 | validation: 0.15628857895429887]
	TIME [epoch: 8 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13117854333084622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13117854333084622 | validation: 0.17919530423081265]
	TIME [epoch: 7.97 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15337897357002667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15337897357002667 | validation: 0.19540210311195746]
	TIME [epoch: 7.98 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18913077636405506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18913077636405506 | validation: 0.20585697065662753]
	TIME [epoch: 7.96 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18407398616006185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18407398616006185 | validation: 0.19183584872228202]
	TIME [epoch: 7.97 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15411744856757476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15411744856757476 | validation: 0.17980688500064357]
	TIME [epoch: 7.97 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13953515916212514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13953515916212514 | validation: 0.19957841815647065]
	TIME [epoch: 8 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1488374619856032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1488374619856032 | validation: 0.1687838454917796]
	TIME [epoch: 7.98 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14567195528794577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14567195528794577 | validation: 0.2071707514505061]
	TIME [epoch: 8.01 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1745948076375676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1745948076375676 | validation: 0.22593651650026994]
	TIME [epoch: 8 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.328493235585254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.328493235585254 | validation: 0.2539134329399178]
	TIME [epoch: 8 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21228124086256842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21228124086256842 | validation: 0.22993022296059162]
	TIME [epoch: 8 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16294204359995718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16294204359995718 | validation: 0.17093051126374514]
	TIME [epoch: 8 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13468144008455024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13468144008455024 | validation: 0.1997970487443257]
	TIME [epoch: 8 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1499199666790561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1499199666790561 | validation: 0.19998777091700737]
	TIME [epoch: 8.04 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15150348668603417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15150348668603417 | validation: 0.19438733399851685]
	TIME [epoch: 8.01 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17038372459257758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17038372459257758 | validation: 0.2565445252080316]
	TIME [epoch: 40 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2991891992414518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2991891992414518 | validation: 0.28520215342043215]
	TIME [epoch: 17.2 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39252553525533457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39252553525533457 | validation: 0.2884976557704593]
	TIME [epoch: 17.1 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18033869083008766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18033869083008766 | validation: 0.288700971137693]
	TIME [epoch: 17.1 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26480295973927126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26480295973927126 | validation: 0.26531891097274346]
	TIME [epoch: 17.2 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29749188766094653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29749188766094653 | validation: 0.29871154450920834]
	TIME [epoch: 17.1 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32878129173148496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32878129173148496 | validation: 0.21789323500894642]
	TIME [epoch: 17.2 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22889651941620173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22889651941620173 | validation: 0.20846590853090544]
	TIME [epoch: 17.2 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18055300581000844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18055300581000844 | validation: 0.2946330696848575]
	TIME [epoch: 17.1 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20763056774529481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20763056774529481 | validation: 0.22147911711009594]
	TIME [epoch: 17.2 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1742049144977355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1742049144977355 | validation: 0.20220666430488948]
	TIME [epoch: 17.2 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13706179307510066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13706179307510066 | validation: 0.14599368857167014]
	TIME [epoch: 17.1 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_512.pth
	Model improved!!!
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1200467431667615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1200467431667615 | validation: 0.1364688131568864]
	TIME [epoch: 17.1 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_513.pth
	Model improved!!!
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11803036355360397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11803036355360397 | validation: 0.14923476707731884]
	TIME [epoch: 17.1 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12073003041082046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12073003041082046 | validation: 0.17507731731812737]
	TIME [epoch: 17.1 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14347579792461287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14347579792461287 | validation: 0.1889352871103784]
	TIME [epoch: 17.1 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17226774803916628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17226774803916628 | validation: 0.1486483460808012]
	TIME [epoch: 17.1 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11656553536714612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11656553536714612 | validation: 0.1564786706062051]
	TIME [epoch: 17.1 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11755241820167045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11755241820167045 | validation: 0.17011940626299368]
	TIME [epoch: 17 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17529264755094245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17529264755094245 | validation: 0.124409137685417]
	TIME [epoch: 17 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_520.pth
	Model improved!!!
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12154378763731645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12154378763731645 | validation: 0.34037625968829366]
	TIME [epoch: 17.1 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3122344938175827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3122344938175827 | validation: 0.6998200856371457]
	TIME [epoch: 17 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6911116639502864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6911116639502864 | validation: 1.139619087301393]
	TIME [epoch: 17 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9918436306208593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9918436306208593 | validation: 0.6124919331633456]
	TIME [epoch: 17.1 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6589059695514711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6589059695514711 | validation: 0.25839387662100094]
	TIME [epoch: 17 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18561359121830814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18561359121830814 | validation: 0.31482818823303393]
	TIME [epoch: 17.1 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27420027612367354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27420027612367354 | validation: 0.2060327035923063]
	TIME [epoch: 17 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17974930872589923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17974930872589923 | validation: 0.2062641794790923]
	TIME [epoch: 17 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16378588560585214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16378588560585214 | validation: 0.14861614355623884]
	TIME [epoch: 17.1 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11995635322753084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11995635322753084 | validation: 0.15708914288235398]
	TIME [epoch: 17 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12914479068122123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12914479068122123 | validation: 0.14044883254488694]
	TIME [epoch: 17.1 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11815210822996979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11815210822996979 | validation: 0.1305279038183087]
	TIME [epoch: 17 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11124793253728975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11124793253728975 | validation: 0.10983850655450966]
	TIME [epoch: 17.1 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_533.pth
	Model improved!!!
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09982995082203644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09982995082203644 | validation: 0.12431137973937939]
	TIME [epoch: 17 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10085097861195104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10085097861195104 | validation: 0.13595426456454382]
	TIME [epoch: 17 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11117250214391604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11117250214391604 | validation: 0.13185612664310228]
	TIME [epoch: 17 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10570678252303309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10570678252303309 | validation: 0.12643500705386407]
	TIME [epoch: 17.1 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11361707095465605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11361707095465605 | validation: 0.1587516273195413]
	TIME [epoch: 17.1 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13138603246964575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13138603246964575 | validation: 0.19168139020336847]
	TIME [epoch: 17.1 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20260438952083226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20260438952083226 | validation: 0.18571278171527683]
	TIME [epoch: 17.1 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16098231164623591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16098231164623591 | validation: 0.11939860318791112]
	TIME [epoch: 17.1 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10028772138631598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10028772138631598 | validation: 0.11351329645907629]
	TIME [epoch: 17.1 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09289087128643814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09289087128643814 | validation: 0.1430501510495654]
	TIME [epoch: 17.1 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11495913627266383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11495913627266383 | validation: 0.20287140575507553]
	TIME [epoch: 17.1 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21478794155151107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21478794155151107 | validation: 0.38220227620332803]
	TIME [epoch: 17.1 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43705962211369065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43705962211369065 | validation: 0.681974147655616]
	TIME [epoch: 17.1 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6774978139339305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6774978139339305 | validation: 0.5324134799363816]
	TIME [epoch: 17.1 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.589246277466214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.589246277466214 | validation: 1.9707148362347817]
	TIME [epoch: 17.1 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6575197502505525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6575197502505525 | validation: 2.2398484749599046]
	TIME [epoch: 17.1 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8079490718981055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8079490718981055 | validation: 2.2969192242724215]
	TIME [epoch: 17.1 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8171665691864893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8171665691864893 | validation: 2.2948534324091248]
	TIME [epoch: 17.1 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8268988104895527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8268988104895527 | validation: 2.451763308879683]
	TIME [epoch: 17.1 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9586831515034953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9586831515034953 | validation: 2.319648698290383]
	TIME [epoch: 17.1 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8604220479940894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8604220479940894 | validation: 2.297040565496899]
	TIME [epoch: 17.1 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.844388386662583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.844388386662583 | validation: 2.346348523544092]
	TIME [epoch: 17.1 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8804774015186263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8804774015186263 | validation: 2.2545862414304745]
	TIME [epoch: 17.1 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.823699745878897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.823699745878897 | validation: 2.24415828148625]
	TIME [epoch: 17.1 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.82574758088052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.82574758088052 | validation: 2.223713512691352]
	TIME [epoch: 17.1 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.805505373659181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.805505373659181 | validation: 2.2465216601369344]
	TIME [epoch: 17.1 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.811449241156108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.811449241156108 | validation: 2.2273302992336723]
	TIME [epoch: 17.1 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7770437497681364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7770437497681364 | validation: 2.1396089664525095]
	TIME [epoch: 17.1 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.775021897547363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.775021897547363 | validation: 2.1970483620901136]
	TIME [epoch: 17.1 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.760729777601057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.760729777601057 | validation: 2.136513694293459]
	TIME [epoch: 17.1 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7375098930331063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7375098930331063 | validation: 2.1218446000121625]
	TIME [epoch: 17.1 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.755532540120283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.755532540120283 | validation: 2.0496567643728283]
	TIME [epoch: 17.1 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6997387093492136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6997387093492136 | validation: 2.08843389468391]
	TIME [epoch: 17.1 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7536299282216157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7536299282216157 | validation: 2.014610833863513]
	TIME [epoch: 17.1 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6498136504521983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6498136504521983 | validation: 2.156834688656894]
	TIME [epoch: 17 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.785971539306255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.785971539306255 | validation: 1.9597927431102722]
	TIME [epoch: 17 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6290601828743796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6290601828743796 | validation: 2.076846839386864]
	TIME [epoch: 17.1 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.698743499910776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.698743499910776 | validation: 2.0042764289662287]
	TIME [epoch: 17.1 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.622409852142535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.622409852142535 | validation: 2.0375192186406377]
	TIME [epoch: 17.1 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.64601988516419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.64601988516419 | validation: 1.9800704387381332]
	TIME [epoch: 17.1 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5984919248053173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5984919248053173 | validation: 1.9846431570488159]
	TIME [epoch: 17.1 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6262771367554727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6262771367554727 | validation: 1.9288803528159748]
	TIME [epoch: 17.1 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.520843111190884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.520843111190884 | validation: 2.0529405139160626]
	TIME [epoch: 17.1 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.676231198487993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.676231198487993 | validation: 1.8926249954408025]
	TIME [epoch: 17.1 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.441938694218936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.441938694218936 | validation: 1.950888243139528]
	TIME [epoch: 17.1 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5715270725861537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5715270725861537 | validation: 1.7847655678055228]
	TIME [epoch: 17.1 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3341102354944545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3341102354944545 | validation: 1.8413735885689755]
	TIME [epoch: 17.1 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4057338792394933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4057338792394933 | validation: 1.5990720106478113]
	TIME [epoch: 17.1 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.172590949777443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.172590949777443 | validation: 0.8900745552987609]
	TIME [epoch: 17.1 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3112335051697661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3112335051697661 | validation: 0.8950636575456099]
	TIME [epoch: 17.1 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1660501133890384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1660501133890384 | validation: 0.5354272074665511]
	TIME [epoch: 17 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6562045296671906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6562045296671906 | validation: 0.8552848856900571]
	TIME [epoch: 17 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9516504503132498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9516504503132498 | validation: 0.8634539732411146]
	TIME [epoch: 17 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8398960741220305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8398960741220305 | validation: 0.45317850473416305]
	TIME [epoch: 17.1 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38710731437395385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38710731437395385 | validation: 0.7474269363656457]
	TIME [epoch: 17.1 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7374342511642277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7374342511642277 | validation: 0.3949515479182786]
	TIME [epoch: 17 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39829563602741114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39829563602741114 | validation: 0.4017146621405207]
	TIME [epoch: 17 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4180930772065614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4180930772065614 | validation: 0.41421332078524176]
	TIME [epoch: 17 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38091733883330137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38091733883330137 | validation: 0.36062331216818866]
	TIME [epoch: 17 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31140747752577175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31140747752577175 | validation: 0.5051978313025901]
	TIME [epoch: 17 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4827391924974542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4827391924974542 | validation: 0.45819056660569424]
	TIME [epoch: 17 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40499843883009157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40499843883009157 | validation: 0.34084628082224033]
	TIME [epoch: 17 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2906464044707057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2906464044707057 | validation: 0.3563930525239332]
	TIME [epoch: 17 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29631552796725036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29631552796725036 | validation: 0.380973688042242]
	TIME [epoch: 17 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2832564142856066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2832564142856066 | validation: 0.36523588153229236]
	TIME [epoch: 17 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26585792842167044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26585792842167044 | validation: 0.3610497984327139]
	TIME [epoch: 17 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2961342626149368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2961342626149368 | validation: 0.33836606807509517]
	TIME [epoch: 17 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28541122129599145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28541122129599145 | validation: 0.3182351654364024]
	TIME [epoch: 17.1 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25569263146424637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25569263146424637 | validation: 0.5626052414153112]
	TIME [epoch: 17.1 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5192118659336724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5192118659336724 | validation: 0.3940364519371578]
	TIME [epoch: 17 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3616008414763656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3616008414763656 | validation: 0.3411300458017297]
	TIME [epoch: 17.1 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33385804660215795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33385804660215795 | validation: 0.9373771231144506]
	TIME [epoch: 17.1 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8807549293736332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8807549293736332 | validation: 0.5081539141315337]
	TIME [epoch: 17 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4609486302005368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4609486302005368 | validation: 0.44623653700002]
	TIME [epoch: 17.1 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46927297921040234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46927297921040234 | validation: 0.5763346570229254]
	TIME [epoch: 17.1 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5855137584156523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5855137584156523 | validation: 0.43617291993567736]
	TIME [epoch: 17.1 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32304182274641774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32304182274641774 | validation: 0.3526353573245172]
	TIME [epoch: 17 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24620371476171277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24620371476171277 | validation: 0.32261066358756435]
	TIME [epoch: 17.1 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25061243550982887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25061243550982887 | validation: 0.3191502155433028]
	TIME [epoch: 17 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24499936034866523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24499936034866523 | validation: 0.3169593045672236]
	TIME [epoch: 17.1 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23745842775503279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23745842775503279 | validation: 0.3154649785838613]
	TIME [epoch: 17.1 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22568286091844555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22568286091844555 | validation: 0.30205101362908743]
	TIME [epoch: 17.1 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21652728805950663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21652728805950663 | validation: 0.2964479932634103]
	TIME [epoch: 17.1 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21482010051259573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21482010051259573 | validation: 0.29728756012212315]
	TIME [epoch: 17.1 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2136496880461175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2136496880461175 | validation: 0.29192274966078874]
	TIME [epoch: 17.1 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20782420519301284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20782420519301284 | validation: 0.29213162733538656]
	TIME [epoch: 17.1 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20012961923327732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20012961923327732 | validation: 0.2770481575958096]
	TIME [epoch: 17.1 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21029299402283477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21029299402283477 | validation: 0.28068023848835805]
	TIME [epoch: 17.1 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20521049595092358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20521049595092358 | validation: 0.2730326777642293]
	TIME [epoch: 17.1 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20584777305439544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20584777305439544 | validation: 0.2561508377724497]
	TIME [epoch: 17.1 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18777432002606492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18777432002606492 | validation: 0.24746112952004792]
	TIME [epoch: 17.1 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18768412274227814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18768412274227814 | validation: 0.2560641650656892]
	TIME [epoch: 17.1 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19106235029314547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19106235029314547 | validation: 0.26534781034780613]
	TIME [epoch: 17.1 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2069098295438429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2069098295438429 | validation: 0.2600967372473983]
	TIME [epoch: 17.1 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19274282474545706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19274282474545706 | validation: 0.23882264951482324]
	TIME [epoch: 17.1 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1898016773651172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1898016773651172 | validation: 0.23561668182911832]
	TIME [epoch: 17.1 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16208378017338756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16208378017338756 | validation: 0.24509743243875712]
	TIME [epoch: 17.1 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.251373330483875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.251373330483875 | validation: 0.20511347139195807]
	TIME [epoch: 17.1 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18044954386065803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18044954386065803 | validation: 0.22135573570486772]
	TIME [epoch: 17.1 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15864430903072652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15864430903072652 | validation: 0.23336695074186253]
	TIME [epoch: 17.1 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1683466066070316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1683466066070316 | validation: 0.30244663751598094]
	TIME [epoch: 17.1 sec]
	Saving model to: out/model_training/model_phi1_3c_v_mmd1_20240822_142314/states/model_phi1_3c_v_mmd1_634.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 5565.893 seconds.
