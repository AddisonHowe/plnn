Args:
Namespace(name='model_phi1_2a_v_mmd1', outdir='out/model_training/model_phi1_2a_v_mmd1', training_data='data/training_data/data_phi1_2a/training', validation_data='data/training_data/data_phi1_2a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2040297076

Training model...

Saving initial model state to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 2/2] avg loss: 5.585684601080144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.585684601080144 | validation: 5.993595783699032]
	TIME [epoch: 92.6 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 2/2] avg loss: 5.756466605042306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.756466605042306 | validation: 5.390894309088512]
	TIME [epoch: 1.85 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 2/2] avg loss: 5.158774653148612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.158774653148612 | validation: 5.172807081159147]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 2/2] avg loss: 5.036745670615053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.036745670615053 | validation: 4.962091485235591]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.874559601687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.874559601687 | validation: 4.660335134221042]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.593733632242609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.593733632242609 | validation: 4.501690908955571]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.516106825516349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.516106825516349 | validation: 4.181572354254825]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.243673734211061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.243673734211061 | validation: 4.311761571669858]
	TIME [epoch: 1.75 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.334218715932137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.334218715932137 | validation: 4.211766753954546]
	TIME [epoch: 1.74 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.9635305145031756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9635305145031756 | validation: 3.614500675800676]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.6298821450217424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6298821450217424 | validation: 3.5117920356395373]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.481243104258658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.481243104258658 | validation: 3.474243669519083]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.3804734074530702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3804734074530702 | validation: 3.1429522061066617]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.085512796852796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.085512796852796 | validation: 3.027637519045832]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.9714631122969486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9714631122969486 | validation: 3.2480427172368564]
	TIME [epoch: 1.75 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.9628641737241392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9628641737241392 | validation: 3.2304626178355207]
	TIME [epoch: 1.74 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.0676874881392715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0676874881392715 | validation: 3.170116165524916]
	TIME [epoch: 1.74 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.872207713886863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.872207713886863 | validation: 2.8349688639840065]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.715733895574906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.715733895574906 | validation: 2.7420516368773353]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.6129506463606726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6129506463606726 | validation: 2.675474830762844]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.520971606880484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.520971606880484 | validation: 2.613994921605548]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.5164295350929624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5164295350929624 | validation: 2.893400637675093]
	TIME [epoch: 1.75 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.640492290881742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.640492290881742 | validation: 2.582061490568802]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.427717906935146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.427717906935146 | validation: 2.4268710822320476]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.297039719963935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.297039719963935 | validation: 2.3323960033121596]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.1839162023462926		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 2.1839162023462926 | validation: 2.343902950792225]
	TIME [epoch: 1.76 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.2424841896039336		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 2.2424841896039336 | validation: 2.3073004042528957]
	TIME [epoch: 1.76 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.1287975538747075		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 2.1287975538747075 | validation: 2.110598479184613]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.9185939606706581		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.9185939606706581 | validation: 1.861791349395646]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.7470317568873996		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.7470317568873996 | validation: 2.000333543797573]
	TIME [epoch: 1.75 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.7968880849933964		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.7968880849933964 | validation: 2.4617448420158983]
	TIME [epoch: 1.74 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.084598299295197		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 2.084598299295197 | validation: 1.770875670597178]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.749591083694427		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.749591083694427 | validation: 1.6250656198258746]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5299307271380753		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.5299307271380753 | validation: 1.4843660859388583]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4890755525204706		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.4890755525204706 | validation: 1.3956983455097949]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4121634298339947		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.4121634298339947 | validation: 1.495269115248729]
	TIME [epoch: 1.75 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3996620761128014		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.3996620761128014 | validation: 1.318753795280326]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4502005478817757		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.4502005478817757 | validation: 1.5707531452890717]
	TIME [epoch: 1.75 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.53710210986366		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.53710210986366 | validation: 1.426477669648935]
	TIME [epoch: 1.74 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4658647629807877		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.4658647629807877 | validation: 1.3554547136216883]
	TIME [epoch: 1.74 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3943757349653036		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.3943757349653036 | validation: 1.2880322867362137]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4060807419388355		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.4060807419388355 | validation: 1.2985014869710352]
	TIME [epoch: 1.75 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3213658523894585		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.3213658523894585 | validation: 1.3335775854167655]
	TIME [epoch: 1.74 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3466177454260286		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.3466177454260286 | validation: 1.217066650057894]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.245465044747496		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.245465044747496 | validation: 1.1787903623725766]
	TIME [epoch: 1.76 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2272464316696907		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.2272464316696907 | validation: 1.233624388427882]
	TIME [epoch: 1.77 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2824748734356617		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.2824748734356617 | validation: 1.1917698490951065]
	TIME [epoch: 1.75 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.232992081224634		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.232992081224634 | validation: 1.2008187438923346]
	TIME [epoch: 1.74 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.228395466746564		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.228395466746564 | validation: 1.0989843626779827]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.193941862113202		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.193941862113202 | validation: 1.3014078701762122]
	TIME [epoch: 1.75 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3147206337076898		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.3147206337076898 | validation: 1.0279202317048066]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1257900391246007		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.1257900391246007 | validation: 1.149862876332843]
	TIME [epoch: 1.76 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1623220240968453		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.1623220240968453 | validation: 0.9590262649181513]
	TIME [epoch: 1.76 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0273778579556239		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.0273778579556239 | validation: 0.9429867393947169]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0569377544802476		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.0569377544802476 | validation: 1.2579151613588508]
	TIME [epoch: 1.75 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1469810478286067		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.1469810478286067 | validation: 0.8848218653879455]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9622421435546491		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.9622421435546491 | validation: 1.1129607959200043]
	TIME [epoch: 1.75 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2609120570675065		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.2609120570675065 | validation: 0.9464586924698248]
	TIME [epoch: 1.75 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1034696337425105		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.1034696337425105 | validation: 0.8887433941187903]
	TIME [epoch: 1.74 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0037053581278432		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.0037053581278432 | validation: 0.8348947804059272]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9189314754304672		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.9189314754304672 | validation: 0.8454172838814078]
	TIME [epoch: 1.75 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8920798686060535		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.8920798686060535 | validation: 0.8710827156796317]
	TIME [epoch: 1.74 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.043547607314749		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.043547607314749 | validation: 1.1270905082738143]
	TIME [epoch: 1.76 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.069534131583095		[learning rate: 0.0076127]
	Learning Rate: 0.00761269
	LOSS [training: 1.069534131583095 | validation: 1.036643703121516]
	TIME [epoch: 1.76 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.999966260812647		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.999966260812647 | validation: 0.9114899201936034]
	TIME [epoch: 1.76 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9893849557865642		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.9893849557865642 | validation: 0.7887558152271046]
	TIME [epoch: 1.76 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8261839047709907		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.8261839047709907 | validation: 0.8041202632061545]
	TIME [epoch: 1.76 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8832367194578093		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.8832367194578093 | validation: 0.8626926625204572]
	TIME [epoch: 1.76 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8516853695080127		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.8516853695080127 | validation: 0.7890966278314988]
	TIME [epoch: 1.74 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9445728274553619		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.9445728274553619 | validation: 1.1447156742542146]
	TIME [epoch: 1.74 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0643935470094403		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.0643935470094403 | validation: 0.7669783458921935]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8221901406826286		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.8221901406826286 | validation: 0.7231337528802945]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7428053171832372		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.7428053171832372 | validation: 0.7740177252390809]
	TIME [epoch: 1.74 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8565579966683923		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.8565579966683923 | validation: 0.700952990989512]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7840186938909246		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.7840186938909246 | validation: 0.7002036495122507]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7507025599263464		[learning rate: 0.0069922]
	Learning Rate: 0.00699222
	LOSS [training: 0.7507025599263464 | validation: 0.6303912857786584]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8504356260117272		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.8504356260117272 | validation: 0.9541278680226597]
	TIME [epoch: 1.75 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.819878474090697		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.819878474090697 | validation: 0.6810457320769917]
	TIME [epoch: 1.76 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8005865995639606		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.8005865995639606 | validation: 0.5878459498168035]
	TIME [epoch: 1.76 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7481995101805379		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.7481995101805379 | validation: 0.6472361089564123]
	TIME [epoch: 1.75 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6973387115689651		[learning rate: 0.0067489]
	Learning Rate: 0.00674887
	LOSS [training: 0.6973387115689651 | validation: 0.8029772195958871]
	TIME [epoch: 1.76 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7736130763600191		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.7736130763600191 | validation: 0.7041459765359188]
	TIME [epoch: 1.76 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6851124650868736		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.6851124650868736 | validation: 0.751531171951174]
	TIME [epoch: 1.75 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6856125289781068		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.6856125289781068 | validation: 0.7546033382483227]
	TIME [epoch: 1.75 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7995971230908124		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.7995971230908124 | validation: 0.7954384592410411]
	TIME [epoch: 1.74 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8429085010236247		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.8429085010236247 | validation: 0.5770674158921649]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6289545290063733		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.6289545290063733 | validation: 0.5954878503689502]
	TIME [epoch: 1.76 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6205605624883139		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.6205605624883139 | validation: 0.587025083446333]
	TIME [epoch: 1.76 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5724079755695521		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.5724079755695521 | validation: 0.4630930833960674]
	TIME [epoch: 1.76 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5142599477784158		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.5142599477784158 | validation: 0.44823447415537454]
	TIME [epoch: 1.76 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.50682749958039		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.50682749958039 | validation: 0.4878895956692326]
	TIME [epoch: 1.75 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7169208278270056		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.7169208278270056 | validation: 0.9046271497383978]
	TIME [epoch: 1.75 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.820260225399299		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.820260225399299 | validation: 0.694545412835389]
	TIME [epoch: 1.75 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7026604791800665		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.7026604791800665 | validation: 0.4419371872085018]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4987367668690562		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.4987367668690562 | validation: 0.4456706407617832]
	TIME [epoch: 1.75 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5028947516277635		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.5028947516277635 | validation: 0.42346730439175845]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5244530214820811		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.5244530214820811 | validation: 0.5466812898664729]
	TIME [epoch: 1.75 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6789403119917442		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.6789403119917442 | validation: 0.39551756500029117]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.43775610719832053		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.43775610719832053 | validation: 0.5105244415063804]
	TIME [epoch: 1.75 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5047551887267261		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.5047551887267261 | validation: 0.48736929871274415]
	TIME [epoch: 1.74 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5827924638518445		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.5827924638518445 | validation: 0.39582305506442195]
	TIME [epoch: 1.75 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.43040229207639		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.43040229207639 | validation: 0.43752793994933753]
	TIME [epoch: 1.75 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5309949580433024		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.5309949580433024 | validation: 0.7251713063895546]
	TIME [epoch: 1.75 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7867516171220954		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.7867516171220954 | validation: 0.4611756615752544]
	TIME [epoch: 1.75 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.536488999985478		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.536488999985478 | validation: 0.43974182467532846]
	TIME [epoch: 1.75 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4313480059260126		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.4313480059260126 | validation: 0.3559652631960176]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3996689868094247		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.3996689868094247 | validation: 0.351177071311409]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.46861288067843143		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.46861288067843143 | validation: 0.7076653016428369]
	TIME [epoch: 1.75 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6069403082101236		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.6069403082101236 | validation: 0.3673352412678808]
	TIME [epoch: 1.75 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4344408073654658		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.4344408073654658 | validation: 0.5809995016931975]
	TIME [epoch: 1.76 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5210325327204834		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.5210325327204834 | validation: 0.46465035066400767]
	TIME [epoch: 1.76 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.46872976739357264		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.46872976739357264 | validation: 0.37598860921812527]
	TIME [epoch: 1.75 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4865571446579813		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.4865571446579813 | validation: 0.37495567567756316]
	TIME [epoch: 1.75 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4554466294632141		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.4554466294632141 | validation: 0.32316099231725176]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.38198487404341674		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.38198487404341674 | validation: 0.4550585470382107]
	TIME [epoch: 1.75 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5057101728067013		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.5057101728067013 | validation: 0.3507699601954742]
	TIME [epoch: 1.74 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3596425373414931		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.3596425373414931 | validation: 0.34877552750709895]
	TIME [epoch: 1.75 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.35079144570761744		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.35079144570761744 | validation: 0.4114905941308351]
	TIME [epoch: 1.74 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.39426456100330076		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.39426456100330076 | validation: 0.42947056552100005]
	TIME [epoch: 1.74 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.47325866497314584		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.47325866497314584 | validation: 0.505681599970346]
	TIME [epoch: 1.74 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.48317219159331093		[learning rate: 0.0050834]
	Learning Rate: 0.00508339
	LOSS [training: 0.48317219159331093 | validation: 0.4498151060908855]
	TIME [epoch: 1.75 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.418394555062995		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.418394555062995 | validation: 0.34063682418027535]
	TIME [epoch: 1.74 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4270941598030633		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.4270941598030633 | validation: 0.5332060328567636]
	TIME [epoch: 1.74 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4242044209224799		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.4242044209224799 | validation: 0.3336103634651896]
	TIME [epoch: 1.74 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.37915978488051844		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.37915978488051844 | validation: 0.37402778116524876]
	TIME [epoch: 1.75 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.35056824575577195		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.35056824575577195 | validation: 0.37298749270460013]
	TIME [epoch: 1.75 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3640625402550718		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.3640625402550718 | validation: 0.29152450969643745]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.34886294864529166		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.34886294864529166 | validation: 0.3685832629295345]
	TIME [epoch: 1.75 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4006463260828569		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.4006463260828569 | validation: 0.3653505208407025]
	TIME [epoch: 1.75 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4046513413931784		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.4046513413931784 | validation: 0.3033204461054146]
	TIME [epoch: 1.75 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.340151350885904		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.340151350885904 | validation: 0.3176930065952538]
	TIME [epoch: 1.75 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.32386392149594473		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.32386392149594473 | validation: 0.3905471268760825]
	TIME [epoch: 1.76 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4252848244245377		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.4252848244245377 | validation: 0.2674619033304279]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2913214950331593		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.2913214950331593 | validation: 0.42220690135116085]
	TIME [epoch: 1.76 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4557332262219771		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.4557332262219771 | validation: 0.5228937790022558]
	TIME [epoch: 1.76 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.43348711862223077		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.43348711862223077 | validation: 0.4524958743585873]
	TIME [epoch: 1.76 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.41243023368269655		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.41243023368269655 | validation: 0.31578941055119164]
	TIME [epoch: 1.74 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.30984634059606		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.30984634059606 | validation: 0.2619943546341946]
	TIME [epoch: 1.76 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.27620383104426927		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.27620383104426927 | validation: 0.27351812299175265]
	TIME [epoch: 1.75 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.27571886698201464		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.27571886698201464 | validation: 0.2563587453976343]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2878122544769762		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.2878122544769762 | validation: 0.28186612822284857]
	TIME [epoch: 1.75 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3079719075979437		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.3079719075979437 | validation: 0.31917891646694924]
	TIME [epoch: 1.75 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.39687550349917		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.39687550349917 | validation: 0.25194740303994817]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_143.pth
	Model improved!!!
EPOCH 144/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.271530925677815		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.271530925677815 | validation: 0.3296302635798363]
	TIME [epoch: 1.75 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3654177658242289		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.3654177658242289 | validation: 0.3302879345253151]
	TIME [epoch: 1.75 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.34473270050165367		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.34473270050165367 | validation: 0.2559264308112826]
	TIME [epoch: 1.75 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2927443269174327		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.2927443269174327 | validation: 0.23586939864421597]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23580618470674508		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.23580618470674508 | validation: 0.2541999033918086]
	TIME [epoch: 1.75 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3264129479874848		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.3264129479874848 | validation: 0.5793173681089984]
	TIME [epoch: 1.75 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.39165603075714517		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.39165603075714517 | validation: 0.34560894795580643]
	TIME [epoch: 1.75 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4915590071161959		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.4915590071161959 | validation: 0.5699987360297492]
	TIME [epoch: 1.75 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5391429816635589		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.5391429816635589 | validation: 0.32826286207758576]
	TIME [epoch: 1.76 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3430115765599073		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.3430115765599073 | validation: 0.36217387976904547]
	TIME [epoch: 1.76 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2886791976986697		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.2886791976986697 | validation: 0.220315070747945]
	TIME [epoch: 1.76 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2363312949991191		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.2363312949991191 | validation: 0.23134524399520298]
	TIME [epoch: 1.76 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.25595697967048175		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.25595697967048175 | validation: 0.2250386575959099]
	TIME [epoch: 1.76 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2731596246567173		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.2731596246567173 | validation: 0.27418408092729507]
	TIME [epoch: 1.78 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.33152017376291676		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.33152017376291676 | validation: 0.2116730667647728]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_158.pth
	Model improved!!!
EPOCH 159/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2207033875478802		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.2207033875478802 | validation: 0.24983992871710714]
	TIME [epoch: 1.76 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23650088035062494		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.23650088035062494 | validation: 0.27189822166394934]
	TIME [epoch: 1.76 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23622202147325289		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.23622202147325289 | validation: 0.19792221376983896]
	TIME [epoch: 1.76 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_161.pth
	Model improved!!!
EPOCH 162/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19433456042172803		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.19433456042172803 | validation: 0.2369280991190209]
	TIME [epoch: 1.75 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.30569374803239535		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.30569374803239535 | validation: 0.34161150025201775]
	TIME [epoch: 1.75 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.30894789898915376		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.30894789898915376 | validation: 0.3516700176686634]
	TIME [epoch: 1.76 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3710207599038361		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.3710207599038361 | validation: 0.6054730755886583]
	TIME [epoch: 1.75 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4970413498382432		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.4970413498382432 | validation: 0.26352680218795327]
	TIME [epoch: 1.74 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2106704477465672		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.2106704477465672 | validation: 0.19177877676728117]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_167.pth
	Model improved!!!
EPOCH 168/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19798670861197415		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.19798670861197415 | validation: 0.2128955204112395]
	TIME [epoch: 1.75 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19774797616276332		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.19774797616276332 | validation: 0.18436969989662977]
	TIME [epoch: 1.76 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19159980895223278		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.19159980895223278 | validation: 0.23505383170962846]
	TIME [epoch: 1.75 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21410282198301203		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.21410282198301203 | validation: 0.22106093707858665]
	TIME [epoch: 1.76 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23838808233621428		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.23838808233621428 | validation: 0.2870405956666269]
	TIME [epoch: 1.76 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20455707341058715		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.20455707341058715 | validation: 0.19356004099983887]
	TIME [epoch: 1.75 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23445450249783661		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.23445450249783661 | validation: 0.24209585417851498]
	TIME [epoch: 1.75 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.25841273594869474		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.25841273594869474 | validation: 0.21324916619039413]
	TIME [epoch: 1.75 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23446961435057184		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.23446961435057184 | validation: 0.17767544278060396]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16476635191303224		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.16476635191303224 | validation: 0.15647141174305387]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_177.pth
	Model improved!!!
EPOCH 178/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16632787799569976		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.16632787799569976 | validation: 0.15293612831456194]
	TIME [epoch: 1.77 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_178.pth
	Model improved!!!
EPOCH 179/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18694127648963454		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.18694127648963454 | validation: 0.29842459172166613]
	TIME [epoch: 1.77 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.28793415581621573		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.28793415581621573 | validation: 0.19374878861376194]
	TIME [epoch: 1.75 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22926957064880055		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.22926957064880055 | validation: 0.2068299055177497]
	TIME [epoch: 1.76 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1925518112944673		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.1925518112944673 | validation: 0.18432593959939675]
	TIME [epoch: 1.75 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20092441390169502		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.20092441390169502 | validation: 0.1586125659193356]
	TIME [epoch: 1.76 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18149389690912654		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.18149389690912654 | validation: 0.15405481472719917]
	TIME [epoch: 1.75 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17391693074015174		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.17391693074015174 | validation: 0.5293705362981438]
	TIME [epoch: 1.74 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.578804252752176		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.578804252752176 | validation: 0.18816041826638577]
	TIME [epoch: 1.75 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2469147367681162		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.2469147367681162 | validation: 0.17111213148923918]
	TIME [epoch: 1.74 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17577755627602681		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.17577755627602681 | validation: 0.21052199847456166]
	TIME [epoch: 1.75 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1688935833572628		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.1688935833572628 | validation: 0.14779873191228443]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_189.pth
	Model improved!!!
EPOCH 190/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18086767009974788		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.18086767009974788 | validation: 0.15313324602749093]
	TIME [epoch: 1.75 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14868762223977705		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.14868762223977705 | validation: 0.17031096970721826]
	TIME [epoch: 1.75 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18405568364691455		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.18405568364691455 | validation: 0.20287281946225866]
	TIME [epoch: 1.76 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1732453194337452		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.1732453194337452 | validation: 0.12748467070936084]
	TIME [epoch: 1.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15324672339280415		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.15324672339280415 | validation: 0.27269301922301725]
	TIME [epoch: 1.75 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.26551838678234824		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.26551838678234824 | validation: 0.19649782518451464]
	TIME [epoch: 1.75 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23678948838637784		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.23678948838637784 | validation: 0.2092303727170219]
	TIME [epoch: 1.76 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16628211025984957		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.16628211025984957 | validation: 0.14088553892843061]
	TIME [epoch: 1.75 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14383023211370835		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.14383023211370835 | validation: 0.12131541601621575]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13838517651924742		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.13838517651924742 | validation: 0.13148653187095707]
	TIME [epoch: 1.75 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12943940790959357		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.12943940790959357 | validation: 0.17517385637670813]
	TIME [epoch: 1.76 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22294044400381002		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.22294044400381002 | validation: 0.13171700729756822]
	TIME [epoch: 91.3 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.154051621428075		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.154051621428075 | validation: 0.28649049470364024]
	TIME [epoch: 3.45 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18590402030086764		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.18590402030086764 | validation: 0.3071688565508768]
	TIME [epoch: 3.41 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2374504064031961		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.2374504064031961 | validation: 0.14835821126152252]
	TIME [epoch: 3.43 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18200994998777564		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.18200994998777564 | validation: 0.1172495018552021]
	TIME [epoch: 3.41 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_205.pth
	Model improved!!!
EPOCH 206/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1285193736062522		[learning rate: 0.0027837]
	Learning Rate: 0.00278366
	LOSS [training: 0.1285193736062522 | validation: 0.14532898259401741]
	TIME [epoch: 3.42 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13573761464987216		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.13573761464987216 | validation: 0.15985668506352468]
	TIME [epoch: 3.43 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1317670847469289		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.1317670847469289 | validation: 0.15757838353158649]
	TIME [epoch: 3.41 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15131800601967027		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.15131800601967027 | validation: 0.1594253691218133]
	TIME [epoch: 3.43 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1522265184679631		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.1522265184679631 | validation: 0.13295822801913001]
	TIME [epoch: 3.44 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14839567412103608		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.14839567412103608 | validation: 0.1177834775259205]
	TIME [epoch: 3.45 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12143624607102015		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.12143624607102015 | validation: 0.20670170556717699]
	TIME [epoch: 3.42 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1768820515391612		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.1768820515391612 | validation: 0.23777845370455586]
	TIME [epoch: 3.43 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16796665987909798		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.16796665987909798 | validation: 0.12181115785851271]
	TIME [epoch: 3.43 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10277248635080594		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.10277248635080594 | validation: 0.11252324439544614]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_215.pth
	Model improved!!!
EPOCH 216/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17408914304297557		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.17408914304297557 | validation: 0.10213176350411735]
	TIME [epoch: 3.41 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_216.pth
	Model improved!!!
EPOCH 217/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1411987252909928		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.1411987252909928 | validation: 0.2024223001217536]
	TIME [epoch: 3.41 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14304269805496728		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.14304269805496728 | validation: 0.15862066812530656]
	TIME [epoch: 3.42 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16145438850098232		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.16145438850098232 | validation: 0.12924482279569438]
	TIME [epoch: 3.41 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1355438015844273		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.1355438015844273 | validation: 0.10616793941225677]
	TIME [epoch: 3.43 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11353593396503511		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.11353593396503511 | validation: 0.12845483674373767]
	TIME [epoch: 3.42 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15252587579819743		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.15252587579819743 | validation: 0.1567303631229447]
	TIME [epoch: 3.44 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14441916637538355		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.14441916637538355 | validation: 0.1112869186834308]
	TIME [epoch: 3.45 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09655443603500644		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.09655443603500644 | validation: 0.09377662149750891]
	TIME [epoch: 3.45 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_224.pth
	Model improved!!!
EPOCH 225/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09207002150973949		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.09207002150973949 | validation: 0.12351054481858442]
	TIME [epoch: 3.43 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14162104083514154		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.14162104083514154 | validation: 0.10577499121829322]
	TIME [epoch: 3.43 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11366784056722282		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.11366784056722282 | validation: 0.09899003022662614]
	TIME [epoch: 3.44 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09481300361701211		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.09481300361701211 | validation: 0.10292120923997868]
	TIME [epoch: 3.42 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09433289892696746		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.09433289892696746 | validation: 0.09930715555344208]
	TIME [epoch: 3.42 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09581096769170193		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.09581096769170193 | validation: 0.2735220941363372]
	TIME [epoch: 3.43 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17101325734858341		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.17101325734858341 | validation: 0.10319097243099336]
	TIME [epoch: 3.43 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10085788080066488		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.10085788080066488 | validation: 0.1292464828188161]
	TIME [epoch: 3.43 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12454245431528013		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.12454245431528013 | validation: 0.08900470332902904]
	TIME [epoch: 3.41 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_233.pth
	Model improved!!!
EPOCH 234/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10291214585816212		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.10291214585816212 | validation: 0.16335047220509671]
	TIME [epoch: 3.42 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1296590250688704		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.1296590250688704 | validation: 0.08068480861330363]
	TIME [epoch: 3.41 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_235.pth
	Model improved!!!
EPOCH 236/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08015249599408358		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.08015249599408358 | validation: 0.08098639352854245]
	TIME [epoch: 3.44 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0828490258006225		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.0828490258006225 | validation: 0.07669519520751245]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_237.pth
	Model improved!!!
EPOCH 238/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08665607728335051		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.08665607728335051 | validation: 0.1885152172672409]
	TIME [epoch: 3.42 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1508249333518974		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.1508249333518974 | validation: 0.10815459793778222]
	TIME [epoch: 3.42 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10056382930628596		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.10056382930628596 | validation: 0.08969409766897625]
	TIME [epoch: 3.43 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07911832970167847		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.07911832970167847 | validation: 0.08418104482251892]
	TIME [epoch: 3.43 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10379605931093466		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.10379605931093466 | validation: 0.13352858030572926]
	TIME [epoch: 3.41 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12060625445965384		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.12060625445965384 | validation: 0.1568016324701028]
	TIME [epoch: 3.43 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13339089069617144		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.13339089069617144 | validation: 0.12851043682379087]
	TIME [epoch: 3.42 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13229485523153495		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.13229485523153495 | validation: 0.12614747952522967]
	TIME [epoch: 3.41 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09053266971892952		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.09053266971892952 | validation: 0.0772348958062785]
	TIME [epoch: 3.42 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07333491967260031		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.07333491967260031 | validation: 0.07596361618235195]
	TIME [epoch: 3.42 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07306304054189788		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.07306304054189788 | validation: 0.06990075869713167]
	TIME [epoch: 3.41 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07398244436422126		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.07398244436422126 | validation: 0.08830667919920406]
	TIME [epoch: 3.42 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09627807313643477		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.09627807313643477 | validation: 0.08240702153076096]
	TIME [epoch: 3.42 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08409453036476669		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.08409453036476669 | validation: 0.08545473160381434]
	TIME [epoch: 3.43 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14458213282379181		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.14458213282379181 | validation: 0.15543583525211438]
	TIME [epoch: 3.45 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10996469012915748		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.10996469012915748 | validation: 0.09009869238809133]
	TIME [epoch: 3.41 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09656062084652524		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.09656062084652524 | validation: 0.09570931917993841]
	TIME [epoch: 3.42 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09393217932301104		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.09393217932301104 | validation: 0.07674690573090044]
	TIME [epoch: 3.42 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07315599440880924		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.07315599440880924 | validation: 0.0730000976018065]
	TIME [epoch: 3.42 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07078961915616508		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.07078961915616508 | validation: 0.06435211467883016]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_257.pth
	Model improved!!!
EPOCH 258/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06587697947785653		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.06587697947785653 | validation: 0.07999199825013455]
	TIME [epoch: 3.42 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07879878848021694		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.07879878848021694 | validation: 0.07037067036341536]
	TIME [epoch: 3.41 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0682217797795502		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.0682217797795502 | validation: 0.06362661724072988]
	TIME [epoch: 3.42 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_260.pth
	Model improved!!!
EPOCH 261/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07798221937869816		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.07798221937869816 | validation: 0.16724827501519368]
	TIME [epoch: 3.41 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1262974846871741		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.1262974846871741 | validation: 0.10639532121914375]
	TIME [epoch: 3.41 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1242249190000933		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.1242249190000933 | validation: 0.15254959323590547]
	TIME [epoch: 3.42 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09174091750168972		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.09174091750168972 | validation: 0.0903179794463293]
	TIME [epoch: 3.42 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07804930680650095		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.07804930680650095 | validation: 0.07525560949074435]
	TIME [epoch: 3.44 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06686995966851786		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.06686995966851786 | validation: 0.06974332484291366]
	TIME [epoch: 3.43 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06464889424172537		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.06464889424172537 | validation: 0.06627392919810988]
	TIME [epoch: 3.41 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0688176266065707		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.0688176266065707 | validation: 0.06456934758677282]
	TIME [epoch: 3.41 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07132835415037411		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.07132835415037411 | validation: 0.1553510262027729]
	TIME [epoch: 3.41 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12934953488083367		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.12934953488083367 | validation: 0.07532622687283835]
	TIME [epoch: 3.42 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08550241378369355		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.08550241378369355 | validation: 0.16167643950910754]
	TIME [epoch: 3.41 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09906983486903055		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.09906983486903055 | validation: 0.10614236433364961]
	TIME [epoch: 3.41 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09408784324597502		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.09408784324597502 | validation: 0.06428799000528565]
	TIME [epoch: 3.41 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06387380140071003		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.06387380140071003 | validation: 0.09158213491556352]
	TIME [epoch: 3.43 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07138997748095416		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.07138997748095416 | validation: 0.0981864776060158]
	TIME [epoch: 3.43 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0843026406949051		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.0843026406949051 | validation: 0.08053224874376093]
	TIME [epoch: 3.43 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06680758207163101		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.06680758207163101 | validation: 0.0662828217045474]
	TIME [epoch: 3.41 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06223910219610679		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.06223910219610679 | validation: 0.06713207709139986]
	TIME [epoch: 3.42 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06824005811105924		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.06824005811105924 | validation: 0.07140011358739469]
	TIME [epoch: 3.42 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07791497846570433		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.07791497846570433 | validation: 0.1277236479790996]
	TIME [epoch: 3.44 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10028860483057533		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.10028860483057533 | validation: 0.07649483823802733]
	TIME [epoch: 3.43 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06947307155318214		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.06947307155318214 | validation: 0.06655392055828961]
	TIME [epoch: 3.44 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06784683066093801		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.06784683066093801 | validation: 0.2898726571873181]
	TIME [epoch: 3.41 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1894540535642973		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.1894540535642973 | validation: 0.13203494059983065]
	TIME [epoch: 3.42 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09724491315564751		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.09724491315564751 | validation: 0.08551284405807726]
	TIME [epoch: 3.41 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07084416322310547		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.07084416322310547 | validation: 0.06732775600475038]
	TIME [epoch: 3.42 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06545530760125272		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.06545530760125272 | validation: 0.08253014576370048]
	TIME [epoch: 3.43 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06891540383563975		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.06891540383563975 | validation: 0.06025549387361287]
	TIME [epoch: 3.41 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_288.pth
	Model improved!!!
EPOCH 289/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056425337505524795		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.056425337505524795 | validation: 0.060169132658081964]
	TIME [epoch: 3.41 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_289.pth
	Model improved!!!
EPOCH 290/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06785631225512001		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.06785631225512001 | validation: 0.09378043915983406]
	TIME [epoch: 3.41 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08901967320954837		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.08901967320954837 | validation: 0.05863423524838968]
	TIME [epoch: 3.4 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07257142595068054		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.07257142595068054 | validation: 0.07031634135021632]
	TIME [epoch: 3.41 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06037895855804999		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.06037895855804999 | validation: 0.06288809883331166]
	TIME [epoch: 3.44 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06384930230355987		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.06384930230355987 | validation: 0.0667638403793381]
	TIME [epoch: 3.42 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06890972428560854		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.06890972428560854 | validation: 0.06676919310994467]
	TIME [epoch: 3.44 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05768575321066419		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.05768575321066419 | validation: 0.0588829667737931]
	TIME [epoch: 3.41 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05433637129425277		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.05433637129425277 | validation: 0.07368513654875726]
	TIME [epoch: 3.42 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07529004879673426		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.07529004879673426 | validation: 0.07346902307199964]
	TIME [epoch: 3.42 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06802185728797662		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.06802185728797662 | validation: 0.05291249073327042]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_299.pth
	Model improved!!!
EPOCH 300/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.055015702532506205		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.055015702532506205 | validation: 0.06431447670570918]
	TIME [epoch: 3.41 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.061098454838384116		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.061098454838384116 | validation: 0.05744436859527483]
	TIME [epoch: 3.42 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.057706827395356104		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.057706827395356104 | validation: 0.06735243246030741]
	TIME [epoch: 3.42 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08264374772916164		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.08264374772916164 | validation: 0.07192579945778825]
	TIME [epoch: 3.43 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.059186628932337185		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.059186628932337185 | validation: 0.06249561622986484]
	TIME [epoch: 3.42 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05876247554989745		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.05876247554989745 | validation: 0.06939544781556342]
	TIME [epoch: 3.42 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06443365228546949		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.06443365228546949 | validation: 0.0634599444492279]
	TIME [epoch: 3.43 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06355134375417168		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.06355134375417168 | validation: 0.05447493202664118]
	TIME [epoch: 3.42 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050754316822165746		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.050754316822165746 | validation: 0.4314042258940298]
	TIME [epoch: 3.45 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4148238699691049		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.4148238699691049 | validation: 0.0993455802721549]
	TIME [epoch: 3.45 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10585687951929638		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.10585687951929638 | validation: 0.07705332941328846]
	TIME [epoch: 3.45 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07129300987022899		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.07129300987022899 | validation: 0.055941186133344936]
	TIME [epoch: 3.42 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053764212021003975		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.053764212021003975 | validation: 0.05980637863827068]
	TIME [epoch: 3.42 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05147844344750643		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.05147844344750643 | validation: 0.05332720964738283]
	TIME [epoch: 3.42 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04657472778176307		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.04657472778176307 | validation: 0.06072108757299066]
	TIME [epoch: 3.43 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051238772763501		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.051238772763501 | validation: 0.05463431795007151]
	TIME [epoch: 3.43 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05157605390289645		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.05157605390289645 | validation: 0.05189342283424854]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_316.pth
	Model improved!!!
EPOCH 317/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048429797795677215		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.048429797795677215 | validation: 0.05522031689056615]
	TIME [epoch: 3.42 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05094637751085346		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.05094637751085346 | validation: 0.058197502498332714]
	TIME [epoch: 3.42 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05145064186307575		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.05145064186307575 | validation: 0.05445534488764521]
	TIME [epoch: 3.41 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05060292327882612		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.05060292327882612 | validation: 0.05006827920797317]
	TIME [epoch: 3.42 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_320.pth
	Model improved!!!
EPOCH 321/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047179371645342305		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.047179371645342305 | validation: 0.05527171899579078]
	TIME [epoch: 3.42 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05323322335249208		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.05323322335249208 | validation: 0.05320287695239676]
	TIME [epoch: 3.44 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.061732123356799076		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.061732123356799076 | validation: 0.0820860614672703]
	TIME [epoch: 3.44 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09423284443738532		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.09423284443738532 | validation: 0.05587922771385685]
	TIME [epoch: 3.42 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0690736347623627		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.0690736347623627 | validation: 0.05512227057677932]
	TIME [epoch: 3.44 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056361598964727935		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.056361598964727935 | validation: 0.05955247000565041]
	TIME [epoch: 3.43 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05754011914679844		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.05754011914679844 | validation: 0.062105358042321646]
	TIME [epoch: 3.41 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.059684712224416264		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.059684712224416264 | validation: 0.05053341692690858]
	TIME [epoch: 3.42 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04771027045735424		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.04771027045735424 | validation: 0.05750050036605227]
	TIME [epoch: 3.42 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052074727598782414		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.052074727598782414 | validation: 0.05128355944119907]
	TIME [epoch: 3.42 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04495961121235873		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.04495961121235873 | validation: 0.05007357330276979]
	TIME [epoch: 3.43 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04449092908837493		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.04449092908837493 | validation: 0.06038879024752311]
	TIME [epoch: 3.42 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0578191839887402		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.0578191839887402 | validation: 0.07673775083871302]
	TIME [epoch: 3.42 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.057432203888671755		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.057432203888671755 | validation: 0.04909259183041991]
	TIME [epoch: 3.42 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_334.pth
	Model improved!!!
EPOCH 335/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045402261295979615		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.045402261295979615 | validation: 0.06511838997654375]
	TIME [epoch: 3.42 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06441865823556113		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.06441865823556113 | validation: 0.06059833041036794]
	TIME [epoch: 3.43 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05369614629678329		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.05369614629678329 | validation: 0.0515505208645366]
	TIME [epoch: 3.43 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05055084882084248		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.05055084882084248 | validation: 0.08759627211201089]
	TIME [epoch: 3.43 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.057973085509259226		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.057973085509259226 | validation: 0.06915049681593126]
	TIME [epoch: 3.42 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06643301287473724		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.06643301287473724 | validation: 0.05107014268124628]
	TIME [epoch: 3.42 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045389959908310656		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.045389959908310656 | validation: 0.05494991594181721]
	TIME [epoch: 3.41 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047488034988950587		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.047488034988950587 | validation: 0.05234248175297978]
	TIME [epoch: 3.43 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04693061728564399		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.04693061728564399 | validation: 0.05026714873974685]
	TIME [epoch: 3.41 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046962759246633214		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.046962759246633214 | validation: 0.05019487994484115]
	TIME [epoch: 3.42 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04413142470375113		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.04413142470375113 | validation: 0.04845879332418381]
	TIME [epoch: 3.42 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_345.pth
	Model improved!!!
EPOCH 346/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04290709915497701		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.04290709915497701 | validation: 0.04455686742241077]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_346.pth
	Model improved!!!
EPOCH 347/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0451719731042564		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.0451719731042564 | validation: 0.10969410711016517]
	TIME [epoch: 3.42 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08839571473676834		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.08839571473676834 | validation: 0.07040533379978602]
	TIME [epoch: 3.42 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05427815131758452		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.05427815131758452 | validation: 0.05116060540124219]
	TIME [epoch: 3.44 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05967217976557096		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.05967217976557096 | validation: 0.051113908289311144]
	TIME [epoch: 3.46 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04451196506995833		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.04451196506995833 | validation: 0.049950950878075065]
	TIME [epoch: 3.43 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04335064265448085		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.04335064265448085 | validation: 0.05103647281249726]
	TIME [epoch: 3.42 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04484231624020464		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.04484231624020464 | validation: 0.04810371350859121]
	TIME [epoch: 3.41 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04430058026120722		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.04430058026120722 | validation: 0.10030480300981276]
	TIME [epoch: 3.41 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06169985863840215		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.06169985863840215 | validation: 0.05496311933823883]
	TIME [epoch: 3.41 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0547628745727565		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.0547628745727565 | validation: 0.04620998644567839]
	TIME [epoch: 3.42 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0500736506227343		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.0500736506227343 | validation: 0.063059958378939]
	TIME [epoch: 3.41 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05414363741953065		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.05414363741953065 | validation: 0.04575186609367424]
	TIME [epoch: 3.42 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040171029102050844		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.040171029102050844 | validation: 0.04383191765023989]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_359.pth
	Model improved!!!
EPOCH 360/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042889364841488115		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.042889364841488115 | validation: 0.04745245174179725]
	TIME [epoch: 3.42 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04822196257002202		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.04822196257002202 | validation: 0.045581911958575465]
	TIME [epoch: 3.41 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04453631187892709		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.04453631187892709 | validation: 0.054918299892431056]
	TIME [epoch: 3.43 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044860669721924096		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.044860669721924096 | validation: 0.04676754776164613]
	TIME [epoch: 3.43 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04284451605381484		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.04284451605381484 | validation: 0.04361831443364767]
	TIME [epoch: 3.45 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_364.pth
	Model improved!!!
EPOCH 365/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03982569592403977		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.03982569592403977 | validation: 0.0453107273113767]
	TIME [epoch: 3.43 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04143454126451858		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.04143454126451858 | validation: 0.05035101120936532]
	TIME [epoch: 3.42 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05160123019452159		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.05160123019452159 | validation: 0.04740797426957912]
	TIME [epoch: 3.41 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04138505465901955		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.04138505465901955 | validation: 0.04314571286634373]
	TIME [epoch: 3.42 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_368.pth
	Model improved!!!
EPOCH 369/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039944270652708176		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.039944270652708176 | validation: 0.08708306016252898]
	TIME [epoch: 3.42 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05339713894979958		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.05339713894979958 | validation: 0.05972218696942488]
	TIME [epoch: 3.41 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05201580073364104		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.05201580073364104 | validation: 0.046244968049373364]
	TIME [epoch: 3.41 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04097458441242405		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.04097458441242405 | validation: 0.04504300456341746]
	TIME [epoch: 3.43 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041845343481497495		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.041845343481497495 | validation: 0.04494222278597234]
	TIME [epoch: 3.41 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039067074488501496		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.039067074488501496 | validation: 0.0480593444336082]
	TIME [epoch: 3.41 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04084547675131741		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.04084547675131741 | validation: 0.04323202405169075]
	TIME [epoch: 3.43 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04620876327285494		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.04620876327285494 | validation: 0.057177071267444646]
	TIME [epoch: 3.44 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04471004195669731		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.04471004195669731 | validation: 0.04369175436672827]
	TIME [epoch: 3.42 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04368713505097109		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.04368713505097109 | validation: 0.04062668516259007]
	TIME [epoch: 3.42 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_378.pth
	Model improved!!!
EPOCH 379/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039601595226108		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.039601595226108 | validation: 0.04315966525386685]
	TIME [epoch: 3.42 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03871319697499459		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.03871319697499459 | validation: 0.06283320576347946]
	TIME [epoch: 3.41 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04577177632095274		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.04577177632095274 | validation: 0.043562166812726684]
	TIME [epoch: 3.41 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039902324836287875		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.039902324836287875 | validation: 0.05514787031757326]
	TIME [epoch: 3.41 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05227824544098065		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.05227824544098065 | validation: 0.044622845101843736]
	TIME [epoch: 3.42 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0433502601194519		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.0433502601194519 | validation: 0.055420144464850385]
	TIME [epoch: 3.43 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04890900439625118		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.04890900439625118 | validation: 0.04874601807159888]
	TIME [epoch: 3.43 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04131265659204293		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.04131265659204293 | validation: 0.04111659427272585]
	TIME [epoch: 3.41 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03738578048705603		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.03738578048705603 | validation: 0.043743072500401436]
	TIME [epoch: 3.42 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038654523842740436		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.038654523842740436 | validation: 0.04715060476406572]
	TIME [epoch: 3.41 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03765700732035225		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.03765700732035225 | validation: 0.04612867425135403]
	TIME [epoch: 3.43 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043179118286191795		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.043179118286191795 | validation: 0.04052262362193568]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_390.pth
	Model improved!!!
EPOCH 391/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04549023246967816		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.04549023246967816 | validation: 0.04270537920951544]
	TIME [epoch: 3.46 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03923402259011456		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.03923402259011456 | validation: 0.04201983708429629]
	TIME [epoch: 3.41 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03768285530494042		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.03768285530494042 | validation: 0.06483318513617227]
	TIME [epoch: 3.41 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047433837762659234		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.047433837762659234 | validation: 0.04643989205910306]
	TIME [epoch: 3.41 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041462197390168015		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.041462197390168015 | validation: 0.040565091885843396]
	TIME [epoch: 3.41 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04296353162980447		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.04296353162980447 | validation: 0.04610295617500755]
	TIME [epoch: 3.41 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03833015521786679		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.03833015521786679 | validation: 0.0424959395235104]
	TIME [epoch: 3.42 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.034421524971883236		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.034421524971883236 | validation: 0.04276089182966976]
	TIME [epoch: 3.41 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038064241352682465		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.038064241352682465 | validation: 0.04160692625027512]
	TIME [epoch: 3.42 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03746246793683465		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.03746246793683465 | validation: 0.04169277889590581]
	TIME [epoch: 3.4 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03667429958156371		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.03667429958156371 | validation: 0.04092068327910077]
	TIME [epoch: 3.43 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0352017528943325		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.0352017528943325 | validation: 0.04569833013456595]
	TIME [epoch: 3.4 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.037402965257316914		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.037402965257316914 | validation: 0.05671692798657938]
	TIME [epoch: 3.42 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04838625943578108		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.04838625943578108 | validation: 0.04282383384298459]
	TIME [epoch: 3.42 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038372586330675834		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.038372586330675834 | validation: 0.05222857613320919]
	TIME [epoch: 3.44 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043842240030881906		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.043842240030881906 | validation: 0.04939425125424205]
	TIME [epoch: 3.41 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045052037165705355		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.045052037165705355 | validation: 0.03907191154214262]
	TIME [epoch: 3.41 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_407.pth
	Model improved!!!
EPOCH 408/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03605673684570766		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.03605673684570766 | validation: 0.043035237259523296]
	TIME [epoch: 3.41 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03789386128365488		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.03789386128365488 | validation: 0.04273438865743588]
	TIME [epoch: 3.41 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0351191895199497		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.0351191895199497 | validation: 0.04681298629696562]
	TIME [epoch: 3.41 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039477669500460354		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.039477669500460354 | validation: 0.03688545766700662]
	TIME [epoch: 3.41 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_411.pth
	Model improved!!!
EPOCH 412/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03284166605406092		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.03284166605406092 | validation: 0.0375564261056196]
	TIME [epoch: 3.42 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03514771822011137		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.03514771822011137 | validation: 0.04730472832752825]
	TIME [epoch: 3.42 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04021587264278663		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.04021587264278663 | validation: 0.03768496034497169]
	TIME [epoch: 3.42 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.035413434290226184		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.035413434290226184 | validation: 0.04284124943923198]
	TIME [epoch: 3.43 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040354930684997736		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.040354930684997736 | validation: 0.03671559438637544]
	TIME [epoch: 3.44 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_416.pth
	Model improved!!!
EPOCH 417/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03595055471557854		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.03595055471557854 | validation: 0.04246954854964738]
	TIME [epoch: 3.43 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03959335758308987		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.03959335758308987 | validation: 0.03797453151405651]
	TIME [epoch: 3.46 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03497467577353129		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.03497467577353129 | validation: 0.04032945237805896]
	TIME [epoch: 3.42 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03562074828857154		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.03562074828857154 | validation: 0.04268468949528762]
	TIME [epoch: 3.43 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03829984425240722		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.03829984425240722 | validation: 0.0471030920672039]
	TIME [epoch: 3.42 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.035785076361677576		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.035785076361677576 | validation: 0.04490631479823983]
	TIME [epoch: 3.42 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040536252067799775		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.040536252067799775 | validation: 0.05759121356299977]
	TIME [epoch: 3.42 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050280081484144576		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.050280081484144576 | validation: 0.03875689457545832]
	TIME [epoch: 3.43 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.032861928562425285		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.032861928562425285 | validation: 0.03793912663927707]
	TIME [epoch: 3.43 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03479320474486836		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.03479320474486836 | validation: 0.03833706225346526]
	TIME [epoch: 3.41 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03391522967961233		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.03391522967961233 | validation: 0.03955183114628271]
	TIME [epoch: 3.44 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03325349595036856		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.03325349595036856 | validation: 0.042987593640786956]
	TIME [epoch: 3.41 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.035205931311028685		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.035205931311028685 | validation: 0.034781723832746664]
	TIME [epoch: 3.42 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_429.pth
	Model improved!!!
EPOCH 430/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.033693355764023915		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.033693355764023915 | validation: 0.03463336210001698]
	TIME [epoch: 3.42 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_430.pth
	Model improved!!!
EPOCH 431/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.034315024086696816		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.034315024086696816 | validation: 0.045234411467863736]
	TIME [epoch: 3.44 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03685182236591495		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.03685182236591495 | validation: 0.0421749055184849]
	TIME [epoch: 3.42 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.034835232800219385		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.034835232800219385 | validation: 0.03690996794505387]
	TIME [epoch: 3.42 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03248945372499368		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.03248945372499368 | validation: 0.03940905785103554]
	TIME [epoch: 3.41 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.035307075508675526		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.035307075508675526 | validation: 0.03944313710680584]
	TIME [epoch: 3.44 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03492580770636716		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.03492580770636716 | validation: 0.03366609689657573]
	TIME [epoch: 3.41 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_436.pth
	Model improved!!!
EPOCH 437/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0334027721686886		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.0334027721686886 | validation: 0.04755830879550029]
	TIME [epoch: 3.42 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03788343700162543		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.03788343700162543 | validation: 0.03834470307109789]
	TIME [epoch: 3.41 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.032918529444926556		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.032918529444926556 | validation: 0.03689379466519573]
	TIME [epoch: 3.42 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03267097074630529		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.03267097074630529 | validation: 0.035862863190067276]
	TIME [epoch: 3.42 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03608982399115474		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.03608982399115474 | validation: 0.0423461708707536]
	TIME [epoch: 3.41 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.035115748564523		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.035115748564523 | validation: 0.03905229817436844]
	TIME [epoch: 3.43 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03412864650039423		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.03412864650039423 | validation: 0.0399985301334588]
	TIME [epoch: 3.42 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.032271381355939883		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.032271381355939883 | validation: 0.03289381359955059]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_444.pth
	Model improved!!!
EPOCH 445/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03325996745683017		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.03325996745683017 | validation: 0.040991340122334755]
	TIME [epoch: 3.44 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03192322824175178		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.03192322824175178 | validation: 0.04329031289712062]
	TIME [epoch: 3.42 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04016961123649368		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.04016961123649368 | validation: 0.039449569578639945]
	TIME [epoch: 3.42 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.032339667497794176		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.032339667497794176 | validation: 0.03734493189091792]
	TIME [epoch: 3.41 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03213675151067848		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.03213675151067848 | validation: 0.03212471455741803]
	TIME [epoch: 3.42 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_449.pth
	Model improved!!!
EPOCH 450/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03507839514891575		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.03507839514891575 | validation: 0.046448623551493144]
	TIME [epoch: 3.41 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03507131315745433		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.03507131315745433 | validation: 0.041646929562822815]
	TIME [epoch: 3.41 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03720128010702248		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.03720128010702248 | validation: 0.04255401980513981]
	TIME [epoch: 3.43 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.031011941178407092		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.031011941178407092 | validation: 0.03210752453526158]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_453.pth
	Model improved!!!
EPOCH 454/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.031081650409841444		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.031081650409841444 | validation: 0.035027846010843675]
	TIME [epoch: 3.43 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.031049550257756072		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.031049550257756072 | validation: 0.03764898902903291]
	TIME [epoch: 3.41 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03297351937711316		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.03297351937711316 | validation: 0.033054552866491234]
	TIME [epoch: 3.41 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03411958675902306		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.03411958675902306 | validation: 0.03529819458684809]
	TIME [epoch: 3.42 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030044818806921354		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.030044818806921354 | validation: 0.034242112124665484]
	TIME [epoch: 3.43 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030723637019441787		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.030723637019441787 | validation: 0.03896302926219316]
	TIME [epoch: 3.45 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03113754040958953		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.03113754040958953 | validation: 0.03617596933514185]
	TIME [epoch: 3.42 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03615638507996612		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.03615638507996612 | validation: 0.038476688727760076]
	TIME [epoch: 3.41 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03302171600609329		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.03302171600609329 | validation: 0.03608899110049355]
	TIME [epoch: 3.41 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03153133399838619		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.03153133399838619 | validation: 0.035889175935400905]
	TIME [epoch: 3.41 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03190735630438543		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.03190735630438543 | validation: 0.047839550367998473]
	TIME [epoch: 3.41 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03455343004849393		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.03455343004849393 | validation: 0.05224895359657553]
	TIME [epoch: 3.42 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.036599123750747836		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.036599123750747836 | validation: 0.034372297890213614]
	TIME [epoch: 3.42 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030693472544431773		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.030693472544431773 | validation: 0.03464810273972851]
	TIME [epoch: 3.43 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.034687425798096895		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.034687425798096895 | validation: 0.036844645632787935]
	TIME [epoch: 3.41 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.032632526360682305		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.032632526360682305 | validation: 0.031592483100158934]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_469.pth
	Model improved!!!
EPOCH 470/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03270520356955765		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.03270520356955765 | validation: 0.03225110495955474]
	TIME [epoch: 3.43 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02943159423519609		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.02943159423519609 | validation: 0.032289263133184774]
	TIME [epoch: 3.42 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028733369769282736		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.028733369769282736 | validation: 0.033078705265736565]
	TIME [epoch: 3.43 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030182706761745788		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.030182706761745788 | validation: 0.03151288918351821]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_473.pth
	Model improved!!!
EPOCH 474/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028541727934250822		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.028541727934250822 | validation: 0.034364510360221624]
	TIME [epoch: 3.42 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03158860375748989		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.03158860375748989 | validation: 0.03608002764660815]
	TIME [epoch: 3.42 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0300134141385395		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.0300134141385395 | validation: 0.045924750433046696]
	TIME [epoch: 3.41 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03604124035113222		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.03604124035113222 | validation: 0.0375870222258576]
	TIME [epoch: 3.41 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03228639275121517		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.03228639275121517 | validation: 0.034100886648875825]
	TIME [epoch: 3.41 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02992002177564064		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.02992002177564064 | validation: 0.03362994468308394]
	TIME [epoch: 3.42 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029688470032302662		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.029688470032302662 | validation: 0.0324797657098865]
	TIME [epoch: 3.42 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029343974209248447		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.029343974209248447 | validation: 0.0349408260141903]
	TIME [epoch: 3.42 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03046170014205957		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.03046170014205957 | validation: 0.03338987692695431]
	TIME [epoch: 3.42 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.031031764891150553		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.031031764891150553 | validation: 0.03361898429311037]
	TIME [epoch: 3.43 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03018518413012206		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.03018518413012206 | validation: 0.03078673214396471]
	TIME [epoch: 3.43 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_484.pth
	Model improved!!!
EPOCH 485/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.031208501276010207		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.031208501276010207 | validation: 0.04243956319927395]
	TIME [epoch: 3.43 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0325119189643362		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.0325119189643362 | validation: 0.03585280776756838]
	TIME [epoch: 3.44 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029836159167455516		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.029836159167455516 | validation: 0.04515517457098053]
	TIME [epoch: 3.43 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.034104281584609916		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.034104281584609916 | validation: 0.03475783875328553]
	TIME [epoch: 3.41 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030038779239815225		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.030038779239815225 | validation: 0.03352940547364105]
	TIME [epoch: 3.41 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02822426481249333		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.02822426481249333 | validation: 0.03431904010009006]
	TIME [epoch: 3.42 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029895474738367		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.029895474738367 | validation: 0.032289901948016855]
	TIME [epoch: 3.42 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02994587006640264		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.02994587006640264 | validation: 0.03167152020597784]
	TIME [epoch: 3.43 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027959333256045327		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.027959333256045327 | validation: 0.03096988374060163]
	TIME [epoch: 3.43 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029089874662953836		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.029089874662953836 | validation: 0.044074668075992446]
	TIME [epoch: 3.42 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03340029219116036		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.03340029219116036 | validation: 0.03472988455455063]
	TIME [epoch: 3.41 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029943854702101893		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.029943854702101893 | validation: 0.03535219604619618]
	TIME [epoch: 3.41 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030157940234122153		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.030157940234122153 | validation: 0.03133578601534603]
	TIME [epoch: 3.42 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03030663513172848		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.03030663513172848 | validation: 0.03275792475516497]
	TIME [epoch: 3.43 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0305726937988682		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.0305726937988682 | validation: 0.03553773153167799]
	TIME [epoch: 3.44 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030333611583860373		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.030333611583860373 | validation: 0.03294814709694379]
	TIME [epoch: 3.45 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02819226681118501		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.02819226681118501 | validation: 0.030997235176912077]
	TIME [epoch: 95.5 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027856011380493637		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.027856011380493637 | validation: 0.03086329019752006]
	TIME [epoch: 6.78 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027096886016081262		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.027096886016081262 | validation: 0.03233296304267446]
	TIME [epoch: 6.76 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028575595381383226		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.028575595381383226 | validation: 0.03266908742473048]
	TIME [epoch: 6.76 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029623193112633027		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.029623193112633027 | validation: 0.03568936438722911]
	TIME [epoch: 6.79 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028658675352959927		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.028658675352959927 | validation: 0.030864928642014556]
	TIME [epoch: 6.76 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028864694138889667		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.028864694138889667 | validation: 0.031854435698148637]
	TIME [epoch: 6.75 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02777249031876579		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.02777249031876579 | validation: 0.03190811139485751]
	TIME [epoch: 6.77 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030330411288591617		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.030330411288591617 | validation: 0.03885117086073159]
	TIME [epoch: 6.76 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030215276312510114		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.030215276312510114 | validation: 0.031202560139549276]
	TIME [epoch: 6.76 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027654947109173853		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.027654947109173853 | validation: 0.029438694582252806]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_511.pth
	Model improved!!!
EPOCH 512/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029295206678232488		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.029295206678232488 | validation: 0.03487023492843355]
	TIME [epoch: 6.75 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028012367118933894		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.028012367118933894 | validation: 0.03196300513834429]
	TIME [epoch: 6.75 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028602013568306155		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.028602013568306155 | validation: 0.033986718984963984]
	TIME [epoch: 6.75 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02730547855838835		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.02730547855838835 | validation: 0.032599176837327576]
	TIME [epoch: 6.75 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02998560591815219		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.02998560591815219 | validation: 0.042531797260851316]
	TIME [epoch: 6.76 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03296526348720548		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.03296526348720548 | validation: 0.0405879486082116]
	TIME [epoch: 6.75 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03123983482462734		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.03123983482462734 | validation: 0.03145154072595507]
	TIME [epoch: 6.76 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030168489420950052		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.030168489420950052 | validation: 0.03283802396802592]
	TIME [epoch: 6.79 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027845131726666764		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.027845131726666764 | validation: 0.03238114599927499]
	TIME [epoch: 6.77 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02649168119511821		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.02649168119511821 | validation: 0.0329639835786575]
	TIME [epoch: 6.74 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028009537033832486		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.028009537033832486 | validation: 0.030599172208973846]
	TIME [epoch: 6.76 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02693078385543067		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.02693078385543067 | validation: 0.034762893744368986]
	TIME [epoch: 6.74 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02745735824849704		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.02745735824849704 | validation: 0.028299600880641203]
	TIME [epoch: 6.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_524.pth
	Model improved!!!
EPOCH 525/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028117809223349714		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.028117809223349714 | validation: 0.03540586797954579]
	TIME [epoch: 6.75 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02802006514245732		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.02802006514245732 | validation: 0.03685171104220664]
	TIME [epoch: 6.78 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030797597006135425		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.030797597006135425 | validation: 0.03604823609043324]
	TIME [epoch: 6.77 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02878699607145885		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.02878699607145885 | validation: 0.031432386143392645]
	TIME [epoch: 6.76 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028300147870997952		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.028300147870997952 | validation: 0.028434064292055396]
	TIME [epoch: 6.75 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02720279303650089		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.02720279303650089 | validation: 0.0316621414378375]
	TIME [epoch: 6.73 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026090582800058878		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.026090582800058878 | validation: 0.03184778013743466]
	TIME [epoch: 6.75 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02723362369574303		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.02723362369574303 | validation: 0.030271757476556933]
	TIME [epoch: 6.76 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028569671919839536		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.028569671919839536 | validation: 0.03179472149850592]
	TIME [epoch: 6.79 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028957025155937392		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.028957025155937392 | validation: 0.03189183844897635]
	TIME [epoch: 6.78 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02573903394097199		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.02573903394097199 | validation: 0.032961621089709126]
	TIME [epoch: 6.75 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025566304949779812		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.025566304949779812 | validation: 0.03215390178345762]
	TIME [epoch: 6.76 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026692858343658685		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.026692858343658685 | validation: 0.029537030055551913]
	TIME [epoch: 6.75 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03126713769837112		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.03126713769837112 | validation: 0.030162562120735894]
	TIME [epoch: 6.75 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027732407255516414		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.027732407255516414 | validation: 0.03238035263780017]
	TIME [epoch: 6.74 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025620222082340126		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.025620222082340126 | validation: 0.028603942621559637]
	TIME [epoch: 6.76 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026385118990427954		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.026385118990427954 | validation: 0.032754191461523356]
	TIME [epoch: 6.77 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02483592514530309		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.02483592514530309 | validation: 0.030672942412621987]
	TIME [epoch: 6.75 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02622278090215364		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.02622278090215364 | validation: 0.030536483359478395]
	TIME [epoch: 6.77 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028746455582935787		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.028746455582935787 | validation: 0.03341339064962811]
	TIME [epoch: 6.77 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025787529729549032		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.025787529729549032 | validation: 0.029480754329568867]
	TIME [epoch: 6.75 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026509944672668834		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.026509944672668834 | validation: 0.030961419989733032]
	TIME [epoch: 6.76 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02712144074105889		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.02712144074105889 | validation: 0.03305555220000196]
	TIME [epoch: 6.76 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026358704528109705		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.026358704528109705 | validation: 0.03331813266289197]
	TIME [epoch: 6.77 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02560356445153604		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.02560356445153604 | validation: 0.027255698667072226]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_549.pth
	Model improved!!!
EPOCH 550/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026135501934246075		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.026135501934246075 | validation: 0.03105311428020527]
	TIME [epoch: 6.74 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026816232220364337		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.026816232220364337 | validation: 0.028240928532319493]
	TIME [epoch: 6.75 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02515918248744954		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.02515918248744954 | validation: 0.03207849708346685]
	TIME [epoch: 6.75 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027292715140146966		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.027292715140146966 | validation: 0.030456861073370853]
	TIME [epoch: 6.78 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027894949702784624		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.027894949702784624 | validation: 0.03164020344907569]
	TIME [epoch: 6.76 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0266373137042882		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.0266373137042882 | validation: 0.028489594143510134]
	TIME [epoch: 6.79 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025996766748038424		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.025996766748038424 | validation: 0.028369055628541556]
	TIME [epoch: 6.76 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025869353729614374		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.025869353729614374 | validation: 0.03264937389049629]
	TIME [epoch: 6.74 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02959591906950545		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.02959591906950545 | validation: 0.02960356464207597]
	TIME [epoch: 6.76 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025791774829261052		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.025791774829261052 | validation: 0.029031345297613576]
	TIME [epoch: 6.74 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027093064568213154		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.027093064568213154 | validation: 0.0309402258758721]
	TIME [epoch: 6.77 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02586707778706037		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.02586707778706037 | validation: 0.029002974368037095]
	TIME [epoch: 6.76 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026480551141631378		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.026480551141631378 | validation: 0.03341519345606708]
	TIME [epoch: 6.79 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027768912330585756		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.027768912330585756 | validation: 0.02629669354295719]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_563.pth
	Model improved!!!
EPOCH 564/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026065698624426804		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.026065698624426804 | validation: 0.0307454404079342]
	TIME [epoch: 6.74 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025651150189822534		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.025651150189822534 | validation: 0.03443225804663545]
	TIME [epoch: 6.75 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02465798283656256		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.02465798283656256 | validation: 0.028861699891137477]
	TIME [epoch: 6.76 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0260324730477087		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.0260324730477087 | validation: 0.030877520888269328]
	TIME [epoch: 6.76 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02677399282009211		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.02677399282009211 | validation: 0.03116597170471798]
	TIME [epoch: 6.73 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02512038300541519		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.02512038300541519 | validation: 0.028368360951284744]
	TIME [epoch: 6.78 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025113088464410276		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.025113088464410276 | validation: 0.032378141098585014]
	TIME [epoch: 6.77 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024238252134871314		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.024238252134871314 | validation: 0.029533391306747255]
	TIME [epoch: 6.74 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02490108880483579		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.02490108880483579 | validation: 0.029267068428346]
	TIME [epoch: 6.76 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02553830019412598		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.02553830019412598 | validation: 0.029228970224159656]
	TIME [epoch: 6.75 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024879942899274442		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.024879942899274442 | validation: 0.029598569361284806]
	TIME [epoch: 6.74 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026298037117250556		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.026298037117250556 | validation: 0.029781484506403347]
	TIME [epoch: 6.77 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024661206102947187		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.024661206102947187 | validation: 0.030236433983000716]
	TIME [epoch: 6.79 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0255978024843588		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.0255978024843588 | validation: 0.0293361128213227]
	TIME [epoch: 6.77 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024473884565303188		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.024473884565303188 | validation: 0.02672163328919948]
	TIME [epoch: 6.75 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02370329498218134		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.02370329498218134 | validation: 0.02803987496742118]
	TIME [epoch: 6.75 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02449991506848876		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.02449991506848876 | validation: 0.02779510003093846]
	TIME [epoch: 6.76 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024458881701968575		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.024458881701968575 | validation: 0.030655160996330866]
	TIME [epoch: 6.75 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02627306051775955		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.02627306051775955 | validation: 0.02871427909461324]
	TIME [epoch: 6.79 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02424143094080501		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.02424143094080501 | validation: 0.036138676210623465]
	TIME [epoch: 6.79 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028252238069672718		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.028252238069672718 | validation: 0.02956816346910604]
	TIME [epoch: 6.75 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025717552680620617		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.025717552680620617 | validation: 0.03012914058668692]
	TIME [epoch: 6.75 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02412375959465879		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.02412375959465879 | validation: 0.028175132506207536]
	TIME [epoch: 6.75 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02421838360414886		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.02421838360414886 | validation: 0.027839827484439368]
	TIME [epoch: 6.74 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02632559147374286		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.02632559147374286 | validation: 0.027248153847670792]
	TIME [epoch: 6.75 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023455164570945063		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.023455164570945063 | validation: 0.026265567092261067]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_589.pth
	Model improved!!!
EPOCH 590/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02455184380929322		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.02455184380929322 | validation: 0.026170498524260916]
	TIME [epoch: 6.78 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_590.pth
	Model improved!!!
EPOCH 591/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02278052388371287		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.02278052388371287 | validation: 0.027804336661409263]
	TIME [epoch: 6.76 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02559953950377445		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.02559953950377445 | validation: 0.027103269603000354]
	TIME [epoch: 6.77 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025021833576657142		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.025021833576657142 | validation: 0.0314156434038027]
	TIME [epoch: 6.77 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02645963359758413		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.02645963359758413 | validation: 0.028653410726206675]
	TIME [epoch: 6.74 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0239643976138217		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.0239643976138217 | validation: 0.02936268164806969]
	TIME [epoch: 6.74 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023126403750770935		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.023126403750770935 | validation: 0.027831914813886673]
	TIME [epoch: 6.74 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024822561003008812		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.024822561003008812 | validation: 0.025568804891264004]
	TIME [epoch: 6.78 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_597.pth
	Model improved!!!
EPOCH 598/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023564166205925684		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.023564166205925684 | validation: 0.029082683749287505]
	TIME [epoch: 6.76 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024646964977277017		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.024646964977277017 | validation: 0.028865192320398622]
	TIME [epoch: 6.74 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025371130291364993		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.025371130291364993 | validation: 0.030964625189032726]
	TIME [epoch: 6.75 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023627443591109097		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.023627443591109097 | validation: 0.028184776384747312]
	TIME [epoch: 6.74 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023988651087390858		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.023988651087390858 | validation: 0.028164190010691644]
	TIME [epoch: 6.77 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024322850516562574		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.024322850516562574 | validation: 0.028624079420590656]
	TIME [epoch: 6.75 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025502640966055805		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.025502640966055805 | validation: 0.026427134306005642]
	TIME [epoch: 6.82 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022559141992557742		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.022559141992557742 | validation: 0.025649365390538997]
	TIME [epoch: 6.76 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02475923311559485		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.02475923311559485 | validation: 0.0277171921507488]
	TIME [epoch: 6.75 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0245978782519152		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.0245978782519152 | validation: 0.02772775585320495]
	TIME [epoch: 6.76 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023674362614838525		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.023674362614838525 | validation: 0.026339954801384936]
	TIME [epoch: 6.76 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022927740802284435		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.022927740802284435 | validation: 0.028684884298987853]
	TIME [epoch: 6.73 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025530959215096283		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.025530959215096283 | validation: 0.029527388178385273]
	TIME [epoch: 6.74 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02495924104687374		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.02495924104687374 | validation: 0.029015447980640697]
	TIME [epoch: 6.76 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02339558639628797		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.02339558639628797 | validation: 0.027808512820155937]
	TIME [epoch: 6.78 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02265230483970483		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.02265230483970483 | validation: 0.027608012451220423]
	TIME [epoch: 6.75 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02516342929050008		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.02516342929050008 | validation: 0.027152688203939665]
	TIME [epoch: 6.73 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025309738126446234		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.025309738126446234 | validation: 0.02586500548841654]
	TIME [epoch: 6.75 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024799254278735847		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.024799254278735847 | validation: 0.026867559179874163]
	TIME [epoch: 6.73 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02389831785206958		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.02389831785206958 | validation: 0.027713272045932286]
	TIME [epoch: 6.74 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022748534818587025		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.022748534818587025 | validation: 0.026159031349455006]
	TIME [epoch: 6.76 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023844182947652844		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.023844182947652844 | validation: 0.025380520893942795]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_619.pth
	Model improved!!!
EPOCH 620/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022949451418497423		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.022949451418497423 | validation: 0.02501656971672854]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_620.pth
	Model improved!!!
EPOCH 621/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024096548215775275		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.024096548215775275 | validation: 0.02897831009907272]
	TIME [epoch: 6.74 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023547371830813565		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.023547371830813565 | validation: 0.028318142310123397]
	TIME [epoch: 6.75 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024107855196450804		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.024107855196450804 | validation: 0.02578997225726671]
	TIME [epoch: 6.73 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02314789260302256		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.02314789260302256 | validation: 0.028067487518720558]
	TIME [epoch: 6.74 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024659164034695542		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.024659164034695542 | validation: 0.025255581380596245]
	TIME [epoch: 6.77 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024201738143151665		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.024201738143151665 | validation: 0.025849481551711975]
	TIME [epoch: 6.78 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023395798645933583		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.023395798645933583 | validation: 0.024936858573151706]
	TIME [epoch: 6.74 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_627.pth
	Model improved!!!
EPOCH 628/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023316346054876667		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.023316346054876667 | validation: 0.026951895434063396]
	TIME [epoch: 6.74 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023955356952636675		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.023955356952636675 | validation: 0.02846354290592252]
	TIME [epoch: 6.75 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02297785787599921		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.02297785787599921 | validation: 0.024303830828233897]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_630.pth
	Model improved!!!
EPOCH 631/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022709377255625753		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.022709377255625753 | validation: 0.025848431848007353]
	TIME [epoch: 6.74 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023604102658774172		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.023604102658774172 | validation: 0.024837926711152165]
	TIME [epoch: 6.75 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022972888133145006		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.022972888133145006 | validation: 0.02820448743629996]
	TIME [epoch: 6.78 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02330212659274547		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.02330212659274547 | validation: 0.025903271245642946]
	TIME [epoch: 6.72 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02354977980373925		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.02354977980373925 | validation: 0.028037709999975808]
	TIME [epoch: 6.75 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023142435433376334		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.023142435433376334 | validation: 0.028241518060499834]
	TIME [epoch: 6.75 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025383114628283264		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.025383114628283264 | validation: 0.027807362044462127]
	TIME [epoch: 6.76 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02423564581949382		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.02423564581949382 | validation: 0.02840722659986751]
	TIME [epoch: 6.73 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022835267934300872		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.022835267934300872 | validation: 0.0249837565028544]
	TIME [epoch: 6.76 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02383866848664017		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.02383866848664017 | validation: 0.02640920319676676]
	TIME [epoch: 6.77 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023893664471734438		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.023893664471734438 | validation: 0.027091036960161477]
	TIME [epoch: 6.74 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021976533063850782		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.021976533063850782 | validation: 0.027892524289212364]
	TIME [epoch: 6.74 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023272948078545755		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.023272948078545755 | validation: 0.02713039237394015]
	TIME [epoch: 6.75 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023044403043718895		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.023044403043718895 | validation: 0.0283522593586471]
	TIME [epoch: 6.74 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02313254582672379		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.02313254582672379 | validation: 0.027617846544206983]
	TIME [epoch: 6.75 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02298697048788793		[learning rate: 0.00012324]
	Learning Rate: 0.000123245
	LOSS [training: 0.02298697048788793 | validation: 0.02655009789865298]
	TIME [epoch: 6.76 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022812679674791746		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.022812679674791746 | validation: 0.027082094972111927]
	TIME [epoch: 6.76 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022712029778955138		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.022712029778955138 | validation: 0.026812892798081214]
	TIME [epoch: 6.75 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022501637811487223		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.022501637811487223 | validation: 0.026518196893000435]
	TIME [epoch: 6.74 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022643116335843136		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.022643116335843136 | validation: 0.029250096508093085]
	TIME [epoch: 6.75 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023138322177071652		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.023138322177071652 | validation: 0.028052070193949986]
	TIME [epoch: 6.74 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023322020982978156		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.023322020982978156 | validation: 0.026058267472890962]
	TIME [epoch: 6.75 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023206787847355893		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.023206787847355893 | validation: 0.02636179280577269]
	TIME [epoch: 6.74 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023551697389548827		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.023551697389548827 | validation: 0.027088980296296774]
	TIME [epoch: 6.77 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023161417114710495		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.023161417114710495 | validation: 0.029727709993603482]
	TIME [epoch: 6.76 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023321055715883804		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.023321055715883804 | validation: 0.027256562345381315]
	TIME [epoch: 6.75 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0238618109904941		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.0238618109904941 | validation: 0.027703456391037325]
	TIME [epoch: 6.74 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021644684044825134		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.021644684044825134 | validation: 0.027844110902660976]
	TIME [epoch: 6.75 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02114718034174274		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.02114718034174274 | validation: 0.02905693404951359]
	TIME [epoch: 6.75 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022694268283341687		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.022694268283341687 | validation: 0.02758946866310516]
	TIME [epoch: 6.74 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022239434682687055		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.022239434682687055 | validation: 0.02580130694048718]
	TIME [epoch: 6.77 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023272227777309674		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.023272227777309674 | validation: 0.02677425609326142]
	TIME [epoch: 6.75 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02231404488264319		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.02231404488264319 | validation: 0.02774539746861218]
	TIME [epoch: 6.75 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023077353879864912		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.023077353879864912 | validation: 0.02775497172664483]
	TIME [epoch: 6.74 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022504667469043386		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.022504667469043386 | validation: 0.025530097952799508]
	TIME [epoch: 6.75 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022429918727450645		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.022429918727450645 | validation: 0.0267380506445907]
	TIME [epoch: 6.75 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022808238557291804		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.022808238557291804 | validation: 0.026128250275875772]
	TIME [epoch: 6.75 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022836727169515948		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.022836727169515948 | validation: 0.025960704329248283]
	TIME [epoch: 6.78 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022632461643640196		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.022632461643640196 | validation: 0.025843043687057345]
	TIME [epoch: 6.76 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022957743160899283		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.022957743160899283 | validation: 0.025295857641432464]
	TIME [epoch: 6.74 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021615778124636217		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.021615778124636217 | validation: 0.026174824165503204]
	TIME [epoch: 6.75 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022460492957104353		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.022460492957104353 | validation: 0.023352950982789184]
	TIME [epoch: 6.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_672.pth
	Model improved!!!
EPOCH 673/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022093228139579354		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.022093228139579354 | validation: 0.026054014689168816]
	TIME [epoch: 6.76 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022002598601812574		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.022002598601812574 | validation: 0.025185488893613696]
	TIME [epoch: 6.77 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022633337003333122		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.022633337003333122 | validation: 0.022233624535856058]
	TIME [epoch: 6.79 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_675.pth
	Model improved!!!
EPOCH 676/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022256671229494665		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.022256671229494665 | validation: 0.02809439750649865]
	TIME [epoch: 6.76 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022702409332733306		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.022702409332733306 | validation: 0.02576416689150784]
	TIME [epoch: 6.77 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02187056164846318		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.02187056164846318 | validation: 0.025366494796033534]
	TIME [epoch: 6.77 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02143138529426033		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.02143138529426033 | validation: 0.027381171853429975]
	TIME [epoch: 6.76 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02256723491060201		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.02256723491060201 | validation: 0.025872295995835973]
	TIME [epoch: 6.76 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022084592586231237		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.022084592586231237 | validation: 0.026210178022858535]
	TIME [epoch: 6.77 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022176611165307558		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.022176611165307558 | validation: 0.025841488948445214]
	TIME [epoch: 6.8 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022316106133450447		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.022316106133450447 | validation: 0.025919755412935005]
	TIME [epoch: 6.77 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022320029778150518		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.022320029778150518 | validation: 0.026485836871226572]
	TIME [epoch: 6.74 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022077853544559583		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.022077853544559583 | validation: 0.024827831624022724]
	TIME [epoch: 6.75 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02191465860616098		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.02191465860616098 | validation: 0.02603612139573569]
	TIME [epoch: 6.75 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022459334072966207		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.022459334072966207 | validation: 0.026028572764450066]
	TIME [epoch: 6.76 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022704475236506497		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.022704475236506497 | validation: 0.02679687643081792]
	TIME [epoch: 6.75 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023000021550367174		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.023000021550367174 | validation: 0.022854256939041747]
	TIME [epoch: 6.79 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022191443478597243		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.022191443478597243 | validation: 0.025157498752393495]
	TIME [epoch: 6.77 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022693598986340865		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.022693598986340865 | validation: 0.025581172111429448]
	TIME [epoch: 6.75 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0214708993262996		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.0214708993262996 | validation: 0.026100627587961014]
	TIME [epoch: 6.74 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021020859714110215		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.021020859714110215 | validation: 0.025135873872380178]
	TIME [epoch: 6.76 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021351462858314782		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.021351462858314782 | validation: 0.025657561562200004]
	TIME [epoch: 6.76 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022880965167148513		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.022880965167148513 | validation: 0.026662184838103967]
	TIME [epoch: 6.75 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022746038422163377		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.022746038422163377 | validation: 0.023873669744553135]
	TIME [epoch: 6.77 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022560291789768386		[learning rate: 8.5871e-05]
	Learning Rate: 8.58709e-05
	LOSS [training: 0.022560291789768386 | validation: 0.02419483503733638]
	TIME [epoch: 6.79 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02137990749837368		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.02137990749837368 | validation: 0.026643325392869777]
	TIME [epoch: 6.76 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022169131985707295		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.022169131985707295 | validation: 0.024279032389698953]
	TIME [epoch: 6.76 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02249076136422669		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.02249076136422669 | validation: 0.024754687629491047]
	TIME [epoch: 6.76 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022534760431526177		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.022534760431526177 | validation: 0.024580011022014233]
	TIME [epoch: 6.74 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021850767020712952		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.021850767020712952 | validation: 0.02432975708018234]
	TIME [epoch: 6.75 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021068102954860377		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.021068102954860377 | validation: 0.02590573508897791]
	TIME [epoch: 6.77 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02133156958208534		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.02133156958208534 | validation: 0.0279886223949888]
	TIME [epoch: 6.78 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020963559411974753		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.020963559411974753 | validation: 0.025129131541939582]
	TIME [epoch: 6.77 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022408392178005686		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.022408392178005686 | validation: 0.022207781332038073]
	TIME [epoch: 6.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_706.pth
	Model improved!!!
EPOCH 707/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02175209393876082		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.02175209393876082 | validation: 0.02521476090122935]
	TIME [epoch: 6.74 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021609323166591428		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.021609323166591428 | validation: 0.0250986634969213]
	TIME [epoch: 6.75 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02202588895404961		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.02202588895404961 | validation: 0.026691679707428573]
	TIME [epoch: 6.75 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021630437127503857		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.021630437127503857 | validation: 0.024256616893983975]
	TIME [epoch: 6.77 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022290392777759162		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.022290392777759162 | validation: 0.026362585618779105]
	TIME [epoch: 6.79 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021942900845061758		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.021942900845061758 | validation: 0.02819271369799563]
	TIME [epoch: 6.76 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022840163509757642		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.022840163509757642 | validation: 0.023576638218656378]
	TIME [epoch: 6.76 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021396601554981715		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.021396601554981715 | validation: 0.026862363211410386]
	TIME [epoch: 6.75 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021578937853142292		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.021578937853142292 | validation: 0.024762998878474926]
	TIME [epoch: 6.76 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02105119955885254		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.02105119955885254 | validation: 0.025756780859645945]
	TIME [epoch: 6.75 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021348058211694793		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.021348058211694793 | validation: 0.025233325912249374]
	TIME [epoch: 6.76 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02166908581264698		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.02166908581264698 | validation: 0.025101883649332937]
	TIME [epoch: 6.78 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021780101327478093		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.021780101327478093 | validation: 0.025133391482662665]
	TIME [epoch: 6.74 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0221251422147188		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.0221251422147188 | validation: 0.023726514895500067]
	TIME [epoch: 6.75 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021671180132690558		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.021671180132690558 | validation: 0.023954045864170964]
	TIME [epoch: 6.76 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022316380495175793		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.022316380495175793 | validation: 0.0234879031192828]
	TIME [epoch: 6.75 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022332339807484284		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.022332339807484284 | validation: 0.02457279757111311]
	TIME [epoch: 6.75 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020640598764154115		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.020640598764154115 | validation: 0.02374203455429697]
	TIME [epoch: 6.77 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02147104080151316		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.02147104080151316 | validation: 0.026258865092717]
	TIME [epoch: 6.77 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02151732611683043		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.02151732611683043 | validation: 0.024873599547400007]
	TIME [epoch: 6.74 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020752987645260585		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.020752987645260585 | validation: 0.02470803002757789]
	TIME [epoch: 6.75 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020926548822605653		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.020926548822605653 | validation: 0.024149162281863163]
	TIME [epoch: 6.75 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022049950444790377		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.022049950444790377 | validation: 0.025077371024483576]
	TIME [epoch: 6.74 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0213355556955478		[learning rate: 6.7968e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.0213355556955478 | validation: 0.021985356024175978]
	TIME [epoch: 6.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_730.pth
	Model improved!!!
EPOCH 731/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0209604758875891		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.0209604758875891 | validation: 0.025265096592228854]
	TIME [epoch: 6.75 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02123530689779708		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.02123530689779708 | validation: 0.025116322744559418]
	TIME [epoch: 6.79 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020851821702194205		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.020851821702194205 | validation: 0.022271522241989555]
	TIME [epoch: 6.74 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02148221137980936		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.02148221137980936 | validation: 0.022853810820423304]
	TIME [epoch: 6.75 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021773425926450315		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.021773425926450315 | validation: 0.02252441759332066]
	TIME [epoch: 6.76 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021178876336941037		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.021178876336941037 | validation: 0.023400435955052576]
	TIME [epoch: 6.76 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02229388274722643		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.02229388274722643 | validation: 0.023891484391288298]
	TIME [epoch: 6.75 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02150403550829532		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.02150403550829532 | validation: 0.02615444099121091]
	TIME [epoch: 6.76 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021591602709071872		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.021591602709071872 | validation: 0.025081023712141527]
	TIME [epoch: 6.78 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021658765318644066		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.021658765318644066 | validation: 0.022654262318479963]
	TIME [epoch: 6.76 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021247549189735843		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.021247549189735843 | validation: 0.024023871554136624]
	TIME [epoch: 6.75 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02131787567494929		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.02131787567494929 | validation: 0.021689032493450246]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_742.pth
	Model improved!!!
EPOCH 743/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020983741983993234		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.020983741983993234 | validation: 0.023787807630634945]
	TIME [epoch: 6.74 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020556347300698087		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.020556347300698087 | validation: 0.023342445071728397]
	TIME [epoch: 6.75 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02113484017777756		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.02113484017777756 | validation: 0.0242752707535974]
	TIME [epoch: 6.75 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02172354410479304		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.02172354410479304 | validation: 0.024737109529342695]
	TIME [epoch: 6.79 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021149119306832116		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.021149119306832116 | validation: 0.02361618769208045]
	TIME [epoch: 6.76 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021790307171184974		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.021790307171184974 | validation: 0.02496271213138884]
	TIME [epoch: 6.75 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02138508599954891		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.02138508599954891 | validation: 0.023070155899408723]
	TIME [epoch: 6.76 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02143883272582483		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.02143883272582483 | validation: 0.02580654742976688]
	TIME [epoch: 6.74 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0206783595527226		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.0206783595527226 | validation: 0.025288912564316254]
	TIME [epoch: 6.75 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021276680721625627		[learning rate: 5.8159e-05]
	Learning Rate: 5.81588e-05
	LOSS [training: 0.021276680721625627 | validation: 0.02318120668354709]
	TIME [epoch: 6.75 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021019584194982968		[learning rate: 5.7748e-05]
	Learning Rate: 5.77482e-05
	LOSS [training: 0.021019584194982968 | validation: 0.02102291438817023]
	TIME [epoch: 6.77 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_753.pth
	Model improved!!!
EPOCH 754/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021193953287075375		[learning rate: 5.7341e-05]
	Learning Rate: 5.73405e-05
	LOSS [training: 0.021193953287075375 | validation: 0.023341842487998993]
	TIME [epoch: 6.75 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020500303276230397		[learning rate: 5.6936e-05]
	Learning Rate: 5.69357e-05
	LOSS [training: 0.020500303276230397 | validation: 0.024535562446873813]
	TIME [epoch: 6.75 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021265766748602408		[learning rate: 5.6534e-05]
	Learning Rate: 5.65337e-05
	LOSS [training: 0.021265766748602408 | validation: 0.025007282000750877]
	TIME [epoch: 6.76 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02145755523485899		[learning rate: 5.6135e-05]
	Learning Rate: 5.61346e-05
	LOSS [training: 0.02145755523485899 | validation: 0.023100525561968982]
	TIME [epoch: 6.74 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022112512139100603		[learning rate: 5.5738e-05]
	Learning Rate: 5.57383e-05
	LOSS [training: 0.022112512139100603 | validation: 0.024420868147591663]
	TIME [epoch: 6.76 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02135215159279378		[learning rate: 5.5345e-05]
	Learning Rate: 5.53448e-05
	LOSS [training: 0.02135215159279378 | validation: 0.023621427044684257]
	TIME [epoch: 6.76 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021715088199654037		[learning rate: 5.4954e-05]
	Learning Rate: 5.49541e-05
	LOSS [training: 0.021715088199654037 | validation: 0.02299014552894776]
	TIME [epoch: 6.76 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01996286494389065		[learning rate: 5.4566e-05]
	Learning Rate: 5.45661e-05
	LOSS [training: 0.01996286494389065 | validation: 0.022681346051663977]
	TIME [epoch: 6.75 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02102752350647012		[learning rate: 5.4181e-05]
	Learning Rate: 5.41809e-05
	LOSS [training: 0.02102752350647012 | validation: 0.025493161555984828]
	TIME [epoch: 6.75 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020489217430559405		[learning rate: 5.3798e-05]
	Learning Rate: 5.37984e-05
	LOSS [training: 0.020489217430559405 | validation: 0.023795743310353158]
	TIME [epoch: 6.75 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020890194089539342		[learning rate: 5.3419e-05]
	Learning Rate: 5.34186e-05
	LOSS [training: 0.020890194089539342 | validation: 0.02423016378734479]
	TIME [epoch: 6.74 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020424952010467977		[learning rate: 5.3041e-05]
	Learning Rate: 5.30415e-05
	LOSS [training: 0.020424952010467977 | validation: 0.023922545656448243]
	TIME [epoch: 6.75 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021880062081759147		[learning rate: 5.2667e-05]
	Learning Rate: 5.2667e-05
	LOSS [training: 0.021880062081759147 | validation: 0.025000462635221855]
	TIME [epoch: 6.76 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020890473616823356		[learning rate: 5.2295e-05]
	Learning Rate: 5.22952e-05
	LOSS [training: 0.020890473616823356 | validation: 0.024787139402309206]
	TIME [epoch: 6.77 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021930681548104184		[learning rate: 5.1926e-05]
	Learning Rate: 5.1926e-05
	LOSS [training: 0.021930681548104184 | validation: 0.02285351725213579]
	TIME [epoch: 6.76 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02224987538747651		[learning rate: 5.1559e-05]
	Learning Rate: 5.15594e-05
	LOSS [training: 0.02224987538747651 | validation: 0.021234922763893622]
	TIME [epoch: 6.75 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02061383883206265		[learning rate: 5.1195e-05]
	Learning Rate: 5.11954e-05
	LOSS [training: 0.02061383883206265 | validation: 0.023019179220507897]
	TIME [epoch: 6.74 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02010667062565625		[learning rate: 5.0834e-05]
	Learning Rate: 5.0834e-05
	LOSS [training: 0.02010667062565625 | validation: 0.026498302517246733]
	TIME [epoch: 6.75 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020995758914551715		[learning rate: 5.0475e-05]
	Learning Rate: 5.04751e-05
	LOSS [training: 0.020995758914551715 | validation: 0.02418656814125436]
	TIME [epoch: 6.74 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020210439797784557		[learning rate: 5.0119e-05]
	Learning Rate: 5.01187e-05
	LOSS [training: 0.020210439797784557 | validation: 0.026005078042872793]
	TIME [epoch: 6.76 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02119162728334681		[learning rate: 4.9765e-05]
	Learning Rate: 4.97649e-05
	LOSS [training: 0.02119162728334681 | validation: 0.024252744253399752]
	TIME [epoch: 6.78 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020637308557180045		[learning rate: 4.9414e-05]
	Learning Rate: 4.94136e-05
	LOSS [training: 0.020637308557180045 | validation: 0.024395126593400582]
	TIME [epoch: 6.78 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02125076417930878		[learning rate: 4.9065e-05]
	Learning Rate: 4.90647e-05
	LOSS [training: 0.02125076417930878 | validation: 0.02513327995269145]
	TIME [epoch: 6.76 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020761933083185108		[learning rate: 4.8718e-05]
	Learning Rate: 4.87183e-05
	LOSS [training: 0.020761933083185108 | validation: 0.024130630907426433]
	TIME [epoch: 6.74 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020551046117724597		[learning rate: 4.8374e-05]
	Learning Rate: 4.83744e-05
	LOSS [training: 0.020551046117724597 | validation: 0.026871747299526995]
	TIME [epoch: 6.75 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021083226481515116		[learning rate: 4.8033e-05]
	Learning Rate: 4.80329e-05
	LOSS [training: 0.021083226481515116 | validation: 0.0242764963087335]
	TIME [epoch: 6.76 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021496502064118937		[learning rate: 4.7694e-05]
	Learning Rate: 4.76938e-05
	LOSS [training: 0.021496502064118937 | validation: 0.025170669482657296]
	TIME [epoch: 6.74 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02142728806626514		[learning rate: 4.7357e-05]
	Learning Rate: 4.73571e-05
	LOSS [training: 0.02142728806626514 | validation: 0.024403773054692914]
	TIME [epoch: 6.76 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020378823264074454		[learning rate: 4.7023e-05]
	Learning Rate: 4.70227e-05
	LOSS [training: 0.020378823264074454 | validation: 0.026195239084621288]
	TIME [epoch: 6.78 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020066655754588963		[learning rate: 4.6691e-05]
	Learning Rate: 4.66907e-05
	LOSS [training: 0.020066655754588963 | validation: 0.02538584613665812]
	TIME [epoch: 6.77 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01982896508441563		[learning rate: 4.6361e-05]
	Learning Rate: 4.63611e-05
	LOSS [training: 0.01982896508441563 | validation: 0.02391540984766822]
	TIME [epoch: 6.75 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019837154294857696		[learning rate: 4.6034e-05]
	Learning Rate: 4.60338e-05
	LOSS [training: 0.019837154294857696 | validation: 0.0244981696343428]
	TIME [epoch: 6.75 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02150012947303811		[learning rate: 4.5709e-05]
	Learning Rate: 4.57088e-05
	LOSS [training: 0.02150012947303811 | validation: 0.022624638861021475]
	TIME [epoch: 6.76 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02012891472630925		[learning rate: 4.5386e-05]
	Learning Rate: 4.53861e-05
	LOSS [training: 0.02012891472630925 | validation: 0.022135141082863685]
	TIME [epoch: 6.75 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020821907466330766		[learning rate: 4.5066e-05]
	Learning Rate: 4.50657e-05
	LOSS [training: 0.020821907466330766 | validation: 0.02336191549105064]
	TIME [epoch: 6.77 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021135632469945008		[learning rate: 4.4748e-05]
	Learning Rate: 4.47476e-05
	LOSS [training: 0.021135632469945008 | validation: 0.022511921068386273]
	TIME [epoch: 6.79 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02061807092316142		[learning rate: 4.4432e-05]
	Learning Rate: 4.44316e-05
	LOSS [training: 0.02061807092316142 | validation: 0.022304460150379086]
	TIME [epoch: 6.74 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021302653567072817		[learning rate: 4.4118e-05]
	Learning Rate: 4.4118e-05
	LOSS [training: 0.021302653567072817 | validation: 0.023677975251810465]
	TIME [epoch: 6.76 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02134010086418147		[learning rate: 4.3807e-05]
	Learning Rate: 4.38065e-05
	LOSS [training: 0.02134010086418147 | validation: 0.024805831618374824]
	TIME [epoch: 6.74 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020189558747648474		[learning rate: 4.3497e-05]
	Learning Rate: 4.34972e-05
	LOSS [training: 0.020189558747648474 | validation: 0.02487952520153175]
	TIME [epoch: 6.74 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021191912142882442		[learning rate: 4.319e-05]
	Learning Rate: 4.31902e-05
	LOSS [training: 0.021191912142882442 | validation: 0.024086709243462918]
	TIME [epoch: 6.77 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021232689782939622		[learning rate: 4.2885e-05]
	Learning Rate: 4.28852e-05
	LOSS [training: 0.021232689782939622 | validation: 0.023111405851318628]
	TIME [epoch: 6.76 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021889613730749213		[learning rate: 4.2582e-05]
	Learning Rate: 4.25825e-05
	LOSS [training: 0.021889613730749213 | validation: 0.02539274475577287]
	TIME [epoch: 6.78 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020396616560215013		[learning rate: 4.2282e-05]
	Learning Rate: 4.22819e-05
	LOSS [training: 0.020396616560215013 | validation: 0.023172906498268482]
	TIME [epoch: 6.75 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019397840395108286		[learning rate: 4.1983e-05]
	Learning Rate: 4.19833e-05
	LOSS [training: 0.019397840395108286 | validation: 0.02250537664804063]
	TIME [epoch: 6.75 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01979165774351862		[learning rate: 4.1687e-05]
	Learning Rate: 4.16869e-05
	LOSS [training: 0.01979165774351862 | validation: 0.02461002695496066]
	TIME [epoch: 6.75 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021207027196437828		[learning rate: 4.1393e-05]
	Learning Rate: 4.13926e-05
	LOSS [training: 0.021207027196437828 | validation: 0.022700370598203547]
	TIME [epoch: 6.74 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020854206976210342		[learning rate: 4.11e-05]
	Learning Rate: 4.11004e-05
	LOSS [training: 0.020854206976210342 | validation: 0.023101322306918283]
	TIME [epoch: 6.75 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0210498365148603		[learning rate: 4.081e-05]
	Learning Rate: 4.08103e-05
	LOSS [training: 0.0210498365148603 | validation: 0.02533144712512765]
	TIME [epoch: 6.76 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019461635468065647		[learning rate: 4.0522e-05]
	Learning Rate: 4.05221e-05
	LOSS [training: 0.019461635468065647 | validation: 0.023149382006068343]
	TIME [epoch: 6.79 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020327930465839153		[learning rate: 4.0236e-05]
	Learning Rate: 4.02361e-05
	LOSS [training: 0.020327930465839153 | validation: 0.02595580888797713]
	TIME [epoch: 6.74 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020297128055339002		[learning rate: 3.9952e-05]
	Learning Rate: 3.9952e-05
	LOSS [training: 0.020297128055339002 | validation: 0.02627371671041337]
	TIME [epoch: 6.74 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02040306232470742		[learning rate: 3.967e-05]
	Learning Rate: 3.967e-05
	LOSS [training: 0.02040306232470742 | validation: 0.02320004719388403]
	TIME [epoch: 6.74 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019962037298114518		[learning rate: 3.939e-05]
	Learning Rate: 3.93899e-05
	LOSS [training: 0.019962037298114518 | validation: 0.0230393873826852]
	TIME [epoch: 6.73 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020750522247344767		[learning rate: 3.9112e-05]
	Learning Rate: 3.91118e-05
	LOSS [training: 0.020750522247344767 | validation: 0.023697930850417825]
	TIME [epoch: 6.74 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020648742037789464		[learning rate: 3.8836e-05]
	Learning Rate: 3.88357e-05
	LOSS [training: 0.020648742037789464 | validation: 0.023671244055394232]
	TIME [epoch: 6.76 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02119588695665295		[learning rate: 3.8561e-05]
	Learning Rate: 3.85615e-05
	LOSS [training: 0.02119588695665295 | validation: 0.024592240872161754]
	TIME [epoch: 6.77 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018910505731095595		[learning rate: 3.8289e-05]
	Learning Rate: 3.82893e-05
	LOSS [training: 0.018910505731095595 | validation: 0.0257965598660147]
	TIME [epoch: 6.75 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02060402143987477		[learning rate: 3.8019e-05]
	Learning Rate: 3.8019e-05
	LOSS [training: 0.02060402143987477 | validation: 0.021578914619226293]
	TIME [epoch: 6.75 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019873098087000607		[learning rate: 3.7751e-05]
	Learning Rate: 3.77505e-05
	LOSS [training: 0.019873098087000607 | validation: 0.022755534184375405]
	TIME [epoch: 6.75 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020549448160063724		[learning rate: 3.7484e-05]
	Learning Rate: 3.7484e-05
	LOSS [training: 0.020549448160063724 | validation: 0.021927316946492217]
	TIME [epoch: 6.73 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020389884466469547		[learning rate: 3.7219e-05]
	Learning Rate: 3.72194e-05
	LOSS [training: 0.020389884466469547 | validation: 0.023403764232167508]
	TIME [epoch: 6.74 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020160413699089705		[learning rate: 3.6957e-05]
	Learning Rate: 3.69566e-05
	LOSS [training: 0.020160413699089705 | validation: 0.023287023572565565]
	TIME [epoch: 6.75 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01932372274448542		[learning rate: 3.6696e-05]
	Learning Rate: 3.66957e-05
	LOSS [training: 0.01932372274448542 | validation: 0.024379473438743818]
	TIME [epoch: 6.77 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019489258722559737		[learning rate: 3.6437e-05]
	Learning Rate: 3.64367e-05
	LOSS [training: 0.019489258722559737 | validation: 0.023675251773808515]
	TIME [epoch: 6.76 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020384745923985724		[learning rate: 3.6179e-05]
	Learning Rate: 3.61794e-05
	LOSS [training: 0.020384745923985724 | validation: 0.023234041423629632]
	TIME [epoch: 6.75 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019056352633179088		[learning rate: 3.5924e-05]
	Learning Rate: 3.5924e-05
	LOSS [training: 0.019056352633179088 | validation: 0.022971171183789715]
	TIME [epoch: 6.73 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0194463933730271		[learning rate: 3.567e-05]
	Learning Rate: 3.56704e-05
	LOSS [training: 0.0194463933730271 | validation: 0.02114858741523907]
	TIME [epoch: 6.75 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02055874809757012		[learning rate: 3.5419e-05]
	Learning Rate: 3.54186e-05
	LOSS [training: 0.02055874809757012 | validation: 0.024098743803652286]
	TIME [epoch: 6.74 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019511855530101388		[learning rate: 3.5169e-05]
	Learning Rate: 3.51685e-05
	LOSS [training: 0.019511855530101388 | validation: 0.023221504395152293]
	TIME [epoch: 6.76 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019976442642290912		[learning rate: 3.492e-05]
	Learning Rate: 3.49202e-05
	LOSS [training: 0.019976442642290912 | validation: 0.02444453846302226]
	TIME [epoch: 6.77 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020829430900775		[learning rate: 3.4674e-05]
	Learning Rate: 3.46737e-05
	LOSS [training: 0.020829430900775 | validation: 0.02295974577342314]
	TIME [epoch: 6.75 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020678631421976394		[learning rate: 3.4429e-05]
	Learning Rate: 3.44289e-05
	LOSS [training: 0.020678631421976394 | validation: 0.022717918811296714]
	TIME [epoch: 6.76 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020850390901948575		[learning rate: 3.4186e-05]
	Learning Rate: 3.41858e-05
	LOSS [training: 0.020850390901948575 | validation: 0.022702564536725357]
	TIME [epoch: 6.75 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020447942768813992		[learning rate: 3.3944e-05]
	Learning Rate: 3.39445e-05
	LOSS [training: 0.020447942768813992 | validation: 0.024720172896317186]
	TIME [epoch: 6.75 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020410731390755166		[learning rate: 3.3705e-05]
	Learning Rate: 3.37049e-05
	LOSS [training: 0.020410731390755166 | validation: 0.021962036427692663]
	TIME [epoch: 6.75 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02084067699279086		[learning rate: 3.3467e-05]
	Learning Rate: 3.34669e-05
	LOSS [training: 0.02084067699279086 | validation: 0.021251528025072765]
	TIME [epoch: 6.76 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02030585445578858		[learning rate: 3.3231e-05]
	Learning Rate: 3.32306e-05
	LOSS [training: 0.02030585445578858 | validation: 0.022708152776860575]
	TIME [epoch: 6.77 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020464086158000012		[learning rate: 3.2996e-05]
	Learning Rate: 3.2996e-05
	LOSS [training: 0.020464086158000012 | validation: 0.023477972283392365]
	TIME [epoch: 6.77 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020709639092383273		[learning rate: 3.2763e-05]
	Learning Rate: 3.27631e-05
	LOSS [training: 0.020709639092383273 | validation: 0.02361066578710956]
	TIME [epoch: 6.73 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020585842647145768		[learning rate: 3.2532e-05]
	Learning Rate: 3.25318e-05
	LOSS [training: 0.020585842647145768 | validation: 0.024385318366716313]
	TIME [epoch: 6.75 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02136021752213392		[learning rate: 3.2302e-05]
	Learning Rate: 3.23021e-05
	LOSS [training: 0.02136021752213392 | validation: 0.023440492073557008]
	TIME [epoch: 6.75 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020504145741294652		[learning rate: 3.2074e-05]
	Learning Rate: 3.20741e-05
	LOSS [training: 0.020504145741294652 | validation: 0.02143462955974067]
	TIME [epoch: 6.75 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021046575369755446		[learning rate: 3.1848e-05]
	Learning Rate: 3.18476e-05
	LOSS [training: 0.021046575369755446 | validation: 0.023639077598410493]
	TIME [epoch: 6.74 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020516337444638832		[learning rate: 3.1623e-05]
	Learning Rate: 3.16228e-05
	LOSS [training: 0.020516337444638832 | validation: 0.025085017258204168]
	TIME [epoch: 6.76 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019764507507440195		[learning rate: 3.14e-05]
	Learning Rate: 3.13995e-05
	LOSS [training: 0.019764507507440195 | validation: 0.02315401594488995]
	TIME [epoch: 6.78 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019995271177412975		[learning rate: 3.1178e-05]
	Learning Rate: 3.11778e-05
	LOSS [training: 0.019995271177412975 | validation: 0.02351587070860969]
	TIME [epoch: 6.74 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020118388553039785		[learning rate: 3.0958e-05]
	Learning Rate: 3.09577e-05
	LOSS [training: 0.020118388553039785 | validation: 0.022966332650292628]
	TIME [epoch: 6.75 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019557418309898662		[learning rate: 3.0739e-05]
	Learning Rate: 3.07392e-05
	LOSS [training: 0.019557418309898662 | validation: 0.022335860446077816]
	TIME [epoch: 6.75 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02111839560002588		[learning rate: 3.0522e-05]
	Learning Rate: 3.05222e-05
	LOSS [training: 0.02111839560002588 | validation: 0.02402985462357037]
	TIME [epoch: 6.73 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020398851926246596		[learning rate: 3.0307e-05]
	Learning Rate: 3.03067e-05
	LOSS [training: 0.020398851926246596 | validation: 0.022779983952799832]
	TIME [epoch: 6.74 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020864020353039525		[learning rate: 3.0093e-05]
	Learning Rate: 3.00927e-05
	LOSS [training: 0.020864020353039525 | validation: 0.023557755150961514]
	TIME [epoch: 6.77 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01978143086013073		[learning rate: 2.988e-05]
	Learning Rate: 2.98803e-05
	LOSS [training: 0.01978143086013073 | validation: 0.021251808004390634]
	TIME [epoch: 6.77 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020513010951465002		[learning rate: 2.9669e-05]
	Learning Rate: 2.96693e-05
	LOSS [training: 0.020513010951465002 | validation: 0.0227896983106431]
	TIME [epoch: 6.75 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019052781564162846		[learning rate: 2.946e-05]
	Learning Rate: 2.94599e-05
	LOSS [training: 0.019052781564162846 | validation: 0.02066895293220802]
	TIME [epoch: 6.75 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_848.pth
	Model improved!!!
EPOCH 849/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02002761886361249		[learning rate: 2.9252e-05]
	Learning Rate: 2.92519e-05
	LOSS [training: 0.02002761886361249 | validation: 0.023256018562020232]
	TIME [epoch: 6.74 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01973675540578078		[learning rate: 2.9045e-05]
	Learning Rate: 2.90454e-05
	LOSS [training: 0.01973675540578078 | validation: 0.023789548557818386]
	TIME [epoch: 6.74 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018370688503340237		[learning rate: 2.884e-05]
	Learning Rate: 2.88403e-05
	LOSS [training: 0.018370688503340237 | validation: 0.023054604204821878]
	TIME [epoch: 6.75 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019735965421811424		[learning rate: 2.8637e-05]
	Learning Rate: 2.86367e-05
	LOSS [training: 0.019735965421811424 | validation: 0.024379565932559313]
	TIME [epoch: 6.76 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019537946312075623		[learning rate: 2.8435e-05]
	Learning Rate: 2.84345e-05
	LOSS [training: 0.019537946312075623 | validation: 0.02150837932636492]
	TIME [epoch: 6.78 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01944676833264445		[learning rate: 2.8234e-05]
	Learning Rate: 2.82338e-05
	LOSS [training: 0.01944676833264445 | validation: 0.022501728335551797]
	TIME [epoch: 6.74 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01971442424468573		[learning rate: 2.8034e-05]
	Learning Rate: 2.80345e-05
	LOSS [training: 0.01971442424468573 | validation: 0.023751712535429948]
	TIME [epoch: 6.75 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01972294962241093		[learning rate: 2.7837e-05]
	Learning Rate: 2.78366e-05
	LOSS [training: 0.01972294962241093 | validation: 0.023198580632204365]
	TIME [epoch: 6.75 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02081236758062244		[learning rate: 2.764e-05]
	Learning Rate: 2.764e-05
	LOSS [training: 0.02081236758062244 | validation: 0.0230993288528497]
	TIME [epoch: 6.74 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02006803792719939		[learning rate: 2.7445e-05]
	Learning Rate: 2.74449e-05
	LOSS [training: 0.02006803792719939 | validation: 0.024537815535572056]
	TIME [epoch: 6.74 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020844850872697614		[learning rate: 2.7251e-05]
	Learning Rate: 2.72511e-05
	LOSS [training: 0.020844850872697614 | validation: 0.024140010941445467]
	TIME [epoch: 6.76 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02167675166619211		[learning rate: 2.7059e-05]
	Learning Rate: 2.70587e-05
	LOSS [training: 0.02167675166619211 | validation: 0.021051062693710845]
	TIME [epoch: 6.77 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019286081060282425		[learning rate: 2.6868e-05]
	Learning Rate: 2.68677e-05
	LOSS [training: 0.019286081060282425 | validation: 0.020921385722295866]
	TIME [epoch: 6.74 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019915285947611196		[learning rate: 2.6678e-05]
	Learning Rate: 2.6678e-05
	LOSS [training: 0.019915285947611196 | validation: 0.022954978742704985]
	TIME [epoch: 6.75 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017916071813944535		[learning rate: 2.649e-05]
	Learning Rate: 2.64897e-05
	LOSS [training: 0.017916071813944535 | validation: 0.021092684863354746]
	TIME [epoch: 6.74 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018725563709674337		[learning rate: 2.6303e-05]
	Learning Rate: 2.63027e-05
	LOSS [training: 0.018725563709674337 | validation: 0.022801653264486934]
	TIME [epoch: 6.74 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019203394719782597		[learning rate: 2.6117e-05]
	Learning Rate: 2.6117e-05
	LOSS [training: 0.019203394719782597 | validation: 0.023510116979325812]
	TIME [epoch: 6.75 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020379788557120443		[learning rate: 2.5933e-05]
	Learning Rate: 2.59326e-05
	LOSS [training: 0.020379788557120443 | validation: 0.022798267090899212]
	TIME [epoch: 6.76 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020647929626200972		[learning rate: 2.575e-05]
	Learning Rate: 2.57495e-05
	LOSS [training: 0.020647929626200972 | validation: 0.022598322057790728]
	TIME [epoch: 6.77 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01977545816512352		[learning rate: 2.5568e-05]
	Learning Rate: 2.55677e-05
	LOSS [training: 0.01977545816512352 | validation: 0.02343071730240034]
	TIME [epoch: 6.75 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019410541467512477		[learning rate: 2.5387e-05]
	Learning Rate: 2.53872e-05
	LOSS [training: 0.019410541467512477 | validation: 0.02140405969347355]
	TIME [epoch: 6.75 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019225598718981086		[learning rate: 2.5208e-05]
	Learning Rate: 2.5208e-05
	LOSS [training: 0.019225598718981086 | validation: 0.022719077891505648]
	TIME [epoch: 6.74 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02191614543711843		[learning rate: 2.503e-05]
	Learning Rate: 2.503e-05
	LOSS [training: 0.02191614543711843 | validation: 0.022545351342741538]
	TIME [epoch: 6.75 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0204949311259207		[learning rate: 2.4853e-05]
	Learning Rate: 2.48533e-05
	LOSS [training: 0.0204949311259207 | validation: 0.022656420160647006]
	TIME [epoch: 6.75 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020060935285293704		[learning rate: 2.4678e-05]
	Learning Rate: 2.46779e-05
	LOSS [training: 0.020060935285293704 | validation: 0.024145613738700552]
	TIME [epoch: 6.74 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019752536436841384		[learning rate: 2.4504e-05]
	Learning Rate: 2.45037e-05
	LOSS [training: 0.019752536436841384 | validation: 0.02410027424807995]
	TIME [epoch: 6.77 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019511600965495667		[learning rate: 2.4331e-05]
	Learning Rate: 2.43307e-05
	LOSS [training: 0.019511600965495667 | validation: 0.022957913254219333]
	TIME [epoch: 6.77 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01953580668869482		[learning rate: 2.4159e-05]
	Learning Rate: 2.41589e-05
	LOSS [training: 0.01953580668869482 | validation: 0.024319990630696592]
	TIME [epoch: 6.76 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019797762344569227		[learning rate: 2.3988e-05]
	Learning Rate: 2.39883e-05
	LOSS [training: 0.019797762344569227 | validation: 0.023237281004414224]
	TIME [epoch: 6.76 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019177481314131523		[learning rate: 2.3819e-05]
	Learning Rate: 2.3819e-05
	LOSS [training: 0.019177481314131523 | validation: 0.021553239174966116]
	TIME [epoch: 6.74 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019778514284358444		[learning rate: 2.3651e-05]
	Learning Rate: 2.36508e-05
	LOSS [training: 0.019778514284358444 | validation: 0.022111367172636304]
	TIME [epoch: 6.76 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020559803804771286		[learning rate: 2.3484e-05]
	Learning Rate: 2.34838e-05
	LOSS [training: 0.020559803804771286 | validation: 0.023293800820149847]
	TIME [epoch: 6.77 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018507517525349917		[learning rate: 2.3318e-05]
	Learning Rate: 2.33181e-05
	LOSS [training: 0.018507517525349917 | validation: 0.021391101897471138]
	TIME [epoch: 6.78 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020325172437697957		[learning rate: 2.3153e-05]
	Learning Rate: 2.31534e-05
	LOSS [training: 0.020325172437697957 | validation: 0.02404723717247272]
	TIME [epoch: 6.76 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020006666130998788		[learning rate: 2.299e-05]
	Learning Rate: 2.299e-05
	LOSS [training: 0.020006666130998788 | validation: 0.02332891917439896]
	TIME [epoch: 6.73 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019766697195097226		[learning rate: 2.2828e-05]
	Learning Rate: 2.28277e-05
	LOSS [training: 0.019766697195097226 | validation: 0.023560197567459026]
	TIME [epoch: 6.75 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020657218646017265		[learning rate: 2.2667e-05]
	Learning Rate: 2.26665e-05
	LOSS [training: 0.020657218646017265 | validation: 0.023233346412561053]
	TIME [epoch: 6.75 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019172341532042076		[learning rate: 2.2506e-05]
	Learning Rate: 2.25065e-05
	LOSS [training: 0.019172341532042076 | validation: 0.023125482345211307]
	TIME [epoch: 6.75 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02058305656310127		[learning rate: 2.2348e-05]
	Learning Rate: 2.23476e-05
	LOSS [training: 0.02058305656310127 | validation: 0.022733179439751824]
	TIME [epoch: 6.74 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02069937769171125		[learning rate: 2.219e-05]
	Learning Rate: 2.21898e-05
	LOSS [training: 0.02069937769171125 | validation: 0.024489025681166746]
	TIME [epoch: 6.78 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020105034169496418		[learning rate: 2.2033e-05]
	Learning Rate: 2.20332e-05
	LOSS [training: 0.020105034169496418 | validation: 0.022495398532422745]
	TIME [epoch: 6.76 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019740921181225345		[learning rate: 2.1878e-05]
	Learning Rate: 2.18776e-05
	LOSS [training: 0.019740921181225345 | validation: 0.02436490628697119]
	TIME [epoch: 6.74 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020053838189596378		[learning rate: 2.1723e-05]
	Learning Rate: 2.17232e-05
	LOSS [training: 0.020053838189596378 | validation: 0.023066627259061313]
	TIME [epoch: 6.75 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01934248035328326		[learning rate: 2.157e-05]
	Learning Rate: 2.15698e-05
	LOSS [training: 0.01934248035328326 | validation: 0.022484757176461253]
	TIME [epoch: 6.75 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01951265532984839		[learning rate: 2.1418e-05]
	Learning Rate: 2.14175e-05
	LOSS [training: 0.01951265532984839 | validation: 0.02130790712520661]
	TIME [epoch: 6.75 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020275075991566686		[learning rate: 2.1266e-05]
	Learning Rate: 2.12663e-05
	LOSS [training: 0.020275075991566686 | validation: 0.022029378674081675]
	TIME [epoch: 6.75 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020956368027775975		[learning rate: 2.1116e-05]
	Learning Rate: 2.11162e-05
	LOSS [training: 0.020956368027775975 | validation: 0.02440851916382126]
	TIME [epoch: 6.77 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019180502953241092		[learning rate: 2.0967e-05]
	Learning Rate: 2.09671e-05
	LOSS [training: 0.019180502953241092 | validation: 0.023284334773776366]
	TIME [epoch: 6.79 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019232775040066673		[learning rate: 2.0819e-05]
	Learning Rate: 2.08191e-05
	LOSS [training: 0.019232775040066673 | validation: 0.022331591934886843]
	TIME [epoch: 6.77 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0205545270710903		[learning rate: 2.0672e-05]
	Learning Rate: 2.06721e-05
	LOSS [training: 0.0205545270710903 | validation: 0.02284006596615551]
	TIME [epoch: 6.75 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01991557225523799		[learning rate: 2.0526e-05]
	Learning Rate: 2.05262e-05
	LOSS [training: 0.01991557225523799 | validation: 0.0225538932279169]
	TIME [epoch: 6.73 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019668226278538398		[learning rate: 2.0381e-05]
	Learning Rate: 2.03813e-05
	LOSS [training: 0.019668226278538398 | validation: 0.024159531910198807]
	TIME [epoch: 6.74 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019446368245837134		[learning rate: 2.0237e-05]
	Learning Rate: 2.02374e-05
	LOSS [training: 0.019446368245837134 | validation: 0.023458630353620953]
	TIME [epoch: 6.74 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019527635506244125		[learning rate: 2.0094e-05]
	Learning Rate: 2.00945e-05
	LOSS [training: 0.019527635506244125 | validation: 0.023864571475739605]
	TIME [epoch: 6.76 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018986121514203442		[learning rate: 1.9953e-05]
	Learning Rate: 1.99526e-05
	LOSS [training: 0.018986121514203442 | validation: 0.022222885644248735]
	TIME [epoch: 6.78 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02016754442979037		[learning rate: 1.9812e-05]
	Learning Rate: 1.98118e-05
	LOSS [training: 0.02016754442979037 | validation: 0.0235746926660961]
	TIME [epoch: 6.74 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019239147544227498		[learning rate: 1.9672e-05]
	Learning Rate: 1.96719e-05
	LOSS [training: 0.019239147544227498 | validation: 0.020907070923220545]
	TIME [epoch: 6.74 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0190654638983841		[learning rate: 1.9533e-05]
	Learning Rate: 1.9533e-05
	LOSS [training: 0.0190654638983841 | validation: 0.022686074926018047]
	TIME [epoch: 6.75 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019381053165265276		[learning rate: 1.9395e-05]
	Learning Rate: 1.93951e-05
	LOSS [training: 0.019381053165265276 | validation: 0.024441243473266774]
	TIME [epoch: 6.74 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01958720260012127		[learning rate: 1.9258e-05]
	Learning Rate: 1.92582e-05
	LOSS [training: 0.01958720260012127 | validation: 0.02297336887479426]
	TIME [epoch: 6.75 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020057785436701944		[learning rate: 1.9122e-05]
	Learning Rate: 1.91222e-05
	LOSS [training: 0.020057785436701944 | validation: 0.02270769273853305]
	TIME [epoch: 6.74 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01927108524624937		[learning rate: 1.8987e-05]
	Learning Rate: 1.89872e-05
	LOSS [training: 0.01927108524624937 | validation: 0.021883499053810953]
	TIME [epoch: 6.76 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02019319511771711		[learning rate: 1.8853e-05]
	Learning Rate: 1.88532e-05
	LOSS [training: 0.02019319511771711 | validation: 0.024402413753589904]
	TIME [epoch: 6.74 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019548246186878258		[learning rate: 1.872e-05]
	Learning Rate: 1.87201e-05
	LOSS [training: 0.019548246186878258 | validation: 0.02177772328837793]
	TIME [epoch: 6.74 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020261141529859097		[learning rate: 1.8588e-05]
	Learning Rate: 1.85879e-05
	LOSS [training: 0.020261141529859097 | validation: 0.023124899048874478]
	TIME [epoch: 6.75 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02059423293282913		[learning rate: 1.8457e-05]
	Learning Rate: 1.84567e-05
	LOSS [training: 0.02059423293282913 | validation: 0.024012048102931616]
	TIME [epoch: 6.74 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019563927381015367		[learning rate: 1.8326e-05]
	Learning Rate: 1.83264e-05
	LOSS [training: 0.019563927381015367 | validation: 0.02219658441077855]
	TIME [epoch: 6.76 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020097988738760793		[learning rate: 1.8197e-05]
	Learning Rate: 1.8197e-05
	LOSS [training: 0.020097988738760793 | validation: 0.025188048920591424]
	TIME [epoch: 6.77 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020120805432558335		[learning rate: 1.8069e-05]
	Learning Rate: 1.80685e-05
	LOSS [training: 0.020120805432558335 | validation: 0.02319387078524708]
	TIME [epoch: 6.79 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02044822861736282		[learning rate: 1.7941e-05]
	Learning Rate: 1.7941e-05
	LOSS [training: 0.02044822861736282 | validation: 0.023604016869610545]
	TIME [epoch: 6.74 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020849935652657477		[learning rate: 1.7814e-05]
	Learning Rate: 1.78143e-05
	LOSS [training: 0.020849935652657477 | validation: 0.026257749672906245]
	TIME [epoch: 6.75 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01973179379925536		[learning rate: 1.7689e-05]
	Learning Rate: 1.76886e-05
	LOSS [training: 0.01973179379925536 | validation: 0.02289435742870945]
	TIME [epoch: 6.73 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019947541892469445		[learning rate: 1.7564e-05]
	Learning Rate: 1.75637e-05
	LOSS [training: 0.019947541892469445 | validation: 0.02315605617998531]
	TIME [epoch: 6.73 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019268004110337987		[learning rate: 1.744e-05]
	Learning Rate: 1.74397e-05
	LOSS [training: 0.019268004110337987 | validation: 0.022466420242543163]
	TIME [epoch: 6.74 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021463772589352833		[learning rate: 1.7317e-05]
	Learning Rate: 1.73166e-05
	LOSS [training: 0.021463772589352833 | validation: 0.024493316552090534]
	TIME [epoch: 6.74 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018914545175032722		[learning rate: 1.7194e-05]
	Learning Rate: 1.71943e-05
	LOSS [training: 0.018914545175032722 | validation: 0.023218465990551808]
	TIME [epoch: 6.78 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019625423526919795		[learning rate: 1.7073e-05]
	Learning Rate: 1.70729e-05
	LOSS [training: 0.019625423526919795 | validation: 0.022108461569768003]
	TIME [epoch: 6.76 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020103372419869432		[learning rate: 1.6952e-05]
	Learning Rate: 1.69524e-05
	LOSS [training: 0.020103372419869432 | validation: 0.022339327065457923]
	TIME [epoch: 6.76 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021322921602654595		[learning rate: 1.6833e-05]
	Learning Rate: 1.68327e-05
	LOSS [training: 0.021322921602654595 | validation: 0.0229081882571983]
	TIME [epoch: 6.74 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01978614719703551		[learning rate: 1.6714e-05]
	Learning Rate: 1.67139e-05
	LOSS [training: 0.01978614719703551 | validation: 0.02529115562534657]
	TIME [epoch: 6.75 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019618081172697314		[learning rate: 1.6596e-05]
	Learning Rate: 1.65959e-05
	LOSS [training: 0.019618081172697314 | validation: 0.02731435107583163]
	TIME [epoch: 6.75 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019000856844441256		[learning rate: 1.6479e-05]
	Learning Rate: 1.64787e-05
	LOSS [training: 0.019000856844441256 | validation: 0.021950010001755085]
	TIME [epoch: 6.74 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019611001516604908		[learning rate: 1.6362e-05]
	Learning Rate: 1.63624e-05
	LOSS [training: 0.019611001516604908 | validation: 0.023265950155941736]
	TIME [epoch: 6.76 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02033679114849838		[learning rate: 1.6247e-05]
	Learning Rate: 1.62469e-05
	LOSS [training: 0.02033679114849838 | validation: 0.025212797182943204]
	TIME [epoch: 6.76 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020097731520011736		[learning rate: 1.6132e-05]
	Learning Rate: 1.61322e-05
	LOSS [training: 0.020097731520011736 | validation: 0.02426428437615416]
	TIME [epoch: 6.74 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019231393765873815		[learning rate: 1.6018e-05]
	Learning Rate: 1.60183e-05
	LOSS [training: 0.019231393765873815 | validation: 0.024368217345701933]
	TIME [epoch: 6.75 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019082609831658878		[learning rate: 1.5905e-05]
	Learning Rate: 1.59052e-05
	LOSS [training: 0.019082609831658878 | validation: 0.023985646656828986]
	TIME [epoch: 6.75 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020739943114529067		[learning rate: 1.5793e-05]
	Learning Rate: 1.57929e-05
	LOSS [training: 0.020739943114529067 | validation: 0.02284994785973156]
	TIME [epoch: 6.75 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019983163936322136		[learning rate: 1.5681e-05]
	Learning Rate: 1.56814e-05
	LOSS [training: 0.019983163936322136 | validation: 0.020642733321357055]
	TIME [epoch: 6.76 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_937.pth
	Model improved!!!
EPOCH 938/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020051378131581428		[learning rate: 1.5571e-05]
	Learning Rate: 1.55707e-05
	LOSS [training: 0.020051378131581428 | validation: 0.024448747071432395]
	TIME [epoch: 6.77 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019647438517363378		[learning rate: 1.5461e-05]
	Learning Rate: 1.54608e-05
	LOSS [training: 0.019647438517363378 | validation: 0.023506250344466864]
	TIME [epoch: 6.76 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02041190679900634		[learning rate: 1.5352e-05]
	Learning Rate: 1.53516e-05
	LOSS [training: 0.02041190679900634 | validation: 0.022539157078793726]
	TIME [epoch: 6.74 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02013236545212898		[learning rate: 1.5243e-05]
	Learning Rate: 1.52432e-05
	LOSS [training: 0.02013236545212898 | validation: 0.022359066008062402]
	TIME [epoch: 6.74 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021142310963239375		[learning rate: 1.5136e-05]
	Learning Rate: 1.51356e-05
	LOSS [training: 0.021142310963239375 | validation: 0.02275197455073575]
	TIME [epoch: 6.75 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019308358284538377		[learning rate: 1.5029e-05]
	Learning Rate: 1.50288e-05
	LOSS [training: 0.019308358284538377 | validation: 0.023902459164032875]
	TIME [epoch: 6.74 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01927178736312706		[learning rate: 1.4923e-05]
	Learning Rate: 1.49227e-05
	LOSS [training: 0.01927178736312706 | validation: 0.023639668293913903]
	TIME [epoch: 6.77 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01934101340303364		[learning rate: 1.4817e-05]
	Learning Rate: 1.48173e-05
	LOSS [training: 0.01934101340303364 | validation: 0.021465743967600738]
	TIME [epoch: 6.78 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020340844303949085		[learning rate: 1.4713e-05]
	Learning Rate: 1.47127e-05
	LOSS [training: 0.020340844303949085 | validation: 0.023835641243800238]
	TIME [epoch: 6.77 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020520895501932937		[learning rate: 1.4609e-05]
	Learning Rate: 1.46088e-05
	LOSS [training: 0.020520895501932937 | validation: 0.021601970068346094]
	TIME [epoch: 6.74 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019905235071891493		[learning rate: 1.4506e-05]
	Learning Rate: 1.45057e-05
	LOSS [training: 0.019905235071891493 | validation: 0.022970226442053956]
	TIME [epoch: 6.75 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019165861851211385		[learning rate: 1.4403e-05]
	Learning Rate: 1.44033e-05
	LOSS [training: 0.019165861851211385 | validation: 0.022878559770343462]
	TIME [epoch: 6.75 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020079219335149674		[learning rate: 1.4302e-05]
	Learning Rate: 1.43016e-05
	LOSS [training: 0.020079219335149674 | validation: 0.021886332550085735]
	TIME [epoch: 6.74 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02017651122824679		[learning rate: 1.4201e-05]
	Learning Rate: 1.42006e-05
	LOSS [training: 0.02017651122824679 | validation: 0.021783483573534063]
	TIME [epoch: 6.75 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019521294131306238		[learning rate: 1.41e-05]
	Learning Rate: 1.41004e-05
	LOSS [training: 0.019521294131306238 | validation: 0.02313523034938772]
	TIME [epoch: 6.78 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020040836773450373		[learning rate: 1.4001e-05]
	Learning Rate: 1.40008e-05
	LOSS [training: 0.020040836773450373 | validation: 0.024716811445874134]
	TIME [epoch: 6.75 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019834207633884746		[learning rate: 1.3902e-05]
	Learning Rate: 1.3902e-05
	LOSS [training: 0.019834207633884746 | validation: 0.02236625264849694]
	TIME [epoch: 6.75 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019666189240771507		[learning rate: 1.3804e-05]
	Learning Rate: 1.38038e-05
	LOSS [training: 0.019666189240771507 | validation: 0.02079119814007332]
	TIME [epoch: 6.75 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01922084415687456		[learning rate: 1.3706e-05]
	Learning Rate: 1.37064e-05
	LOSS [training: 0.01922084415687456 | validation: 0.021588807554762238]
	TIME [epoch: 6.75 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020497337443532014		[learning rate: 1.361e-05]
	Learning Rate: 1.36096e-05
	LOSS [training: 0.020497337443532014 | validation: 0.0245730510978674]
	TIME [epoch: 6.72 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01942096989780668		[learning rate: 1.3514e-05]
	Learning Rate: 1.35135e-05
	LOSS [training: 0.01942096989780668 | validation: 0.022548634554402192]
	TIME [epoch: 6.74 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018777505524136796		[learning rate: 1.3418e-05]
	Learning Rate: 1.34181e-05
	LOSS [training: 0.018777505524136796 | validation: 0.021337552252628508]
	TIME [epoch: 6.76 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019684916322549324		[learning rate: 1.3323e-05]
	Learning Rate: 1.33234e-05
	LOSS [training: 0.019684916322549324 | validation: 0.021960530424256566]
	TIME [epoch: 6.79 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019964703043528827		[learning rate: 1.3229e-05]
	Learning Rate: 1.32294e-05
	LOSS [training: 0.019964703043528827 | validation: 0.02108349371277207]
	TIME [epoch: 6.74 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019625892224271857		[learning rate: 1.3136e-05]
	Learning Rate: 1.3136e-05
	LOSS [training: 0.019625892224271857 | validation: 0.022906946384524868]
	TIME [epoch: 6.73 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01884880150826436		[learning rate: 1.3043e-05]
	Learning Rate: 1.30432e-05
	LOSS [training: 0.01884880150826436 | validation: 0.02378203207562716]
	TIME [epoch: 6.73 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020237846576036368		[learning rate: 1.2951e-05]
	Learning Rate: 1.29511e-05
	LOSS [training: 0.020237846576036368 | validation: 0.02372108969222976]
	TIME [epoch: 6.73 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020699786074850964		[learning rate: 1.286e-05]
	Learning Rate: 1.28597e-05
	LOSS [training: 0.020699786074850964 | validation: 0.02316239107409543]
	TIME [epoch: 6.74 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019066449032308086		[learning rate: 1.2769e-05]
	Learning Rate: 1.27689e-05
	LOSS [training: 0.019066449032308086 | validation: 0.024032623816925492]
	TIME [epoch: 6.76 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019406142624308953		[learning rate: 1.2679e-05]
	Learning Rate: 1.26788e-05
	LOSS [training: 0.019406142624308953 | validation: 0.022368071986822776]
	TIME [epoch: 6.75 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019231630460256704		[learning rate: 1.2589e-05]
	Learning Rate: 1.25893e-05
	LOSS [training: 0.019231630460256704 | validation: 0.022447670208789072]
	TIME [epoch: 6.75 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019441322496912793		[learning rate: 1.25e-05]
	Learning Rate: 1.25004e-05
	LOSS [training: 0.019441322496912793 | validation: 0.023600396264368162]
	TIME [epoch: 6.73 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01961994308313786		[learning rate: 1.2412e-05]
	Learning Rate: 1.24121e-05
	LOSS [training: 0.01961994308313786 | validation: 0.021853468553356983]
	TIME [epoch: 6.73 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01967734975068143		[learning rate: 1.2325e-05]
	Learning Rate: 1.23245e-05
	LOSS [training: 0.01967734975068143 | validation: 0.023132825329593307]
	TIME [epoch: 6.77 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02039268623718847		[learning rate: 1.2237e-05]
	Learning Rate: 1.22375e-05
	LOSS [training: 0.02039268623718847 | validation: 0.021656571461271826]
	TIME [epoch: 6.75 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01900842865587015		[learning rate: 1.2151e-05]
	Learning Rate: 1.21511e-05
	LOSS [training: 0.01900842865587015 | validation: 0.021350594884271393]
	TIME [epoch: 6.75 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018766828898104944		[learning rate: 1.2065e-05]
	Learning Rate: 1.20653e-05
	LOSS [training: 0.018766828898104944 | validation: 0.022327757583793035]
	TIME [epoch: 6.77 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020229116939107988		[learning rate: 1.198e-05]
	Learning Rate: 1.19801e-05
	LOSS [training: 0.020229116939107988 | validation: 0.023162751774735882]
	TIME [epoch: 6.73 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01922024300689787		[learning rate: 1.1896e-05]
	Learning Rate: 1.18956e-05
	LOSS [training: 0.01922024300689787 | validation: 0.024493392451365684]
	TIME [epoch: 6.73 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019568120963765395		[learning rate: 1.1812e-05]
	Learning Rate: 1.18116e-05
	LOSS [training: 0.019568120963765395 | validation: 0.02202092495716171]
	TIME [epoch: 6.74 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018638315883888902		[learning rate: 1.1728e-05]
	Learning Rate: 1.17282e-05
	LOSS [training: 0.018638315883888902 | validation: 0.023120039377392005]
	TIME [epoch: 6.73 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01950171860655321		[learning rate: 1.1645e-05]
	Learning Rate: 1.16454e-05
	LOSS [training: 0.01950171860655321 | validation: 0.021796048178343193]
	TIME [epoch: 6.75 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01875646376759125		[learning rate: 1.1563e-05]
	Learning Rate: 1.15632e-05
	LOSS [training: 0.01875646376759125 | validation: 0.024108809482185342]
	TIME [epoch: 6.76 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01951570286708762		[learning rate: 1.1482e-05]
	Learning Rate: 1.14815e-05
	LOSS [training: 0.01951570286708762 | validation: 0.024222736728500335]
	TIME [epoch: 6.78 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020050848996377126		[learning rate: 1.14e-05]
	Learning Rate: 1.14005e-05
	LOSS [training: 0.020050848996377126 | validation: 0.025101797412514306]
	TIME [epoch: 6.75 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019396206377630648		[learning rate: 1.132e-05]
	Learning Rate: 1.132e-05
	LOSS [training: 0.019396206377630648 | validation: 0.023033785340011936]
	TIME [epoch: 6.73 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018884472770152993		[learning rate: 1.124e-05]
	Learning Rate: 1.12401e-05
	LOSS [training: 0.018884472770152993 | validation: 0.02277935168560501]
	TIME [epoch: 6.74 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01841561973852298		[learning rate: 1.1161e-05]
	Learning Rate: 1.11607e-05
	LOSS [training: 0.01841561973852298 | validation: 0.021438972464122737]
	TIME [epoch: 6.75 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018231079456432393		[learning rate: 1.1082e-05]
	Learning Rate: 1.10819e-05
	LOSS [training: 0.018231079456432393 | validation: 0.022354154670927204]
	TIME [epoch: 6.73 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019806487790321767		[learning rate: 1.1004e-05]
	Learning Rate: 1.10037e-05
	LOSS [training: 0.019806487790321767 | validation: 0.02210205495753544]
	TIME [epoch: 6.75 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01920472273559829		[learning rate: 1.0926e-05]
	Learning Rate: 1.0926e-05
	LOSS [training: 0.01920472273559829 | validation: 0.023465776875550622]
	TIME [epoch: 6.78 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019511547467780544		[learning rate: 1.0849e-05]
	Learning Rate: 1.08489e-05
	LOSS [training: 0.019511547467780544 | validation: 0.021925831822975556]
	TIME [epoch: 6.76 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02027100169936391		[learning rate: 1.0772e-05]
	Learning Rate: 1.07723e-05
	LOSS [training: 0.02027100169936391 | validation: 0.02191179903121772]
	TIME [epoch: 6.74 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0188123505868363		[learning rate: 1.0696e-05]
	Learning Rate: 1.06962e-05
	LOSS [training: 0.0188123505868363 | validation: 0.02290377347353007]
	TIME [epoch: 6.75 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0196840889928422		[learning rate: 1.0621e-05]
	Learning Rate: 1.06207e-05
	LOSS [training: 0.0196840889928422 | validation: 0.021587579061678602]
	TIME [epoch: 6.75 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020106563519515855		[learning rate: 1.0546e-05]
	Learning Rate: 1.05457e-05
	LOSS [training: 0.020106563519515855 | validation: 0.02449298235898171]
	TIME [epoch: 6.73 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019457197438995905		[learning rate: 1.0471e-05]
	Learning Rate: 1.04713e-05
	LOSS [training: 0.019457197438995905 | validation: 0.025262917983568157]
	TIME [epoch: 6.75 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01937961327396042		[learning rate: 1.0397e-05]
	Learning Rate: 1.03974e-05
	LOSS [training: 0.01937961327396042 | validation: 0.02392124088630784]
	TIME [epoch: 6.75 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01985920736374904		[learning rate: 1.0324e-05]
	Learning Rate: 1.0324e-05
	LOSS [training: 0.01985920736374904 | validation: 0.0233502230414703]
	TIME [epoch: 6.74 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01945778727613498		[learning rate: 1.0251e-05]
	Learning Rate: 1.02511e-05
	LOSS [training: 0.01945778727613498 | validation: 0.023716240046331813]
	TIME [epoch: 6.74 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018800355984531378		[learning rate: 1.0179e-05]
	Learning Rate: 1.01787e-05
	LOSS [training: 0.018800355984531378 | validation: 0.022899837656857044]
	TIME [epoch: 6.76 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019468932384511854		[learning rate: 1.0107e-05]
	Learning Rate: 1.01068e-05
	LOSS [training: 0.019468932384511854 | validation: 0.025267824196607343]
	TIME [epoch: 6.74 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02060621488003533		[learning rate: 1.0035e-05]
	Learning Rate: 1.00355e-05
	LOSS [training: 0.02060621488003533 | validation: 0.024240299070074363]
	TIME [epoch: 6.74 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020919209532770495		[learning rate: 9.9646e-06]
	Learning Rate: 9.96464e-06
	LOSS [training: 0.020919209532770495 | validation: 0.023366720120196638]
	TIME [epoch: 103 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02033844036553961		[learning rate: 9.8943e-06]
	Learning Rate: 9.89429e-06
	LOSS [training: 0.02033844036553961 | validation: 0.02201665952258965]
	TIME [epoch: 14.7 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019989061014603356		[learning rate: 9.8244e-06]
	Learning Rate: 9.82444e-06
	LOSS [training: 0.019989061014603356 | validation: 0.0202744210815604]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_1003.pth
	Model improved!!!
EPOCH 1004/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018886853393607093		[learning rate: 9.7551e-06]
	Learning Rate: 9.75508e-06
	LOSS [training: 0.018886853393607093 | validation: 0.021152063838608372]
	TIME [epoch: 14.7 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018862546092807464		[learning rate: 9.6862e-06]
	Learning Rate: 9.68621e-06
	LOSS [training: 0.018862546092807464 | validation: 0.02245269219392201]
	TIME [epoch: 14.7 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019509638022146933		[learning rate: 9.6178e-06]
	Learning Rate: 9.61783e-06
	LOSS [training: 0.019509638022146933 | validation: 0.022882217786758887]
	TIME [epoch: 14.7 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020527943533883403		[learning rate: 9.5499e-06]
	Learning Rate: 9.54993e-06
	LOSS [training: 0.020527943533883403 | validation: 0.020310409343606367]
	TIME [epoch: 14.7 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01962895276105296		[learning rate: 9.4825e-06]
	Learning Rate: 9.48251e-06
	LOSS [training: 0.01962895276105296 | validation: 0.02144294357702577]
	TIME [epoch: 14.7 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020186753272110617		[learning rate: 9.4156e-06]
	Learning Rate: 9.41556e-06
	LOSS [training: 0.020186753272110617 | validation: 0.02210186288377072]
	TIME [epoch: 14.7 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019991676315036983		[learning rate: 9.3491e-06]
	Learning Rate: 9.34909e-06
	LOSS [training: 0.019991676315036983 | validation: 0.02111496664006836]
	TIME [epoch: 14.7 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01994504637316219		[learning rate: 9.2831e-06]
	Learning Rate: 9.28308e-06
	LOSS [training: 0.01994504637316219 | validation: 0.02476809664816302]
	TIME [epoch: 14.7 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019437406642473884		[learning rate: 9.2176e-06]
	Learning Rate: 9.21755e-06
	LOSS [training: 0.019437406642473884 | validation: 0.022062293391321715]
	TIME [epoch: 14.7 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01902551113339735		[learning rate: 9.1525e-06]
	Learning Rate: 9.15248e-06
	LOSS [training: 0.01902551113339735 | validation: 0.021306068631990922]
	TIME [epoch: 14.7 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01912315887710824		[learning rate: 9.0879e-06]
	Learning Rate: 9.08786e-06
	LOSS [training: 0.01912315887710824 | validation: 0.02223960383318607]
	TIME [epoch: 14.7 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018985680977623723		[learning rate: 9.0237e-06]
	Learning Rate: 9.0237e-06
	LOSS [training: 0.018985680977623723 | validation: 0.022869151316134553]
	TIME [epoch: 14.7 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02016994497020415		[learning rate: 8.96e-06]
	Learning Rate: 8.96e-06
	LOSS [training: 0.02016994497020415 | validation: 0.020538043819998603]
	TIME [epoch: 14.7 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02060772838096332		[learning rate: 8.8967e-06]
	Learning Rate: 8.89674e-06
	LOSS [training: 0.02060772838096332 | validation: 0.023681097191509484]
	TIME [epoch: 14.7 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019998621795338022		[learning rate: 8.8339e-06]
	Learning Rate: 8.83393e-06
	LOSS [training: 0.019998621795338022 | validation: 0.022010367185453652]
	TIME [epoch: 14.7 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019536225958551673		[learning rate: 8.7716e-06]
	Learning Rate: 8.77157e-06
	LOSS [training: 0.019536225958551673 | validation: 0.021612604752676935]
	TIME [epoch: 14.7 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019661573018612842		[learning rate: 8.7096e-06]
	Learning Rate: 8.70964e-06
	LOSS [training: 0.019661573018612842 | validation: 0.02393563526273364]
	TIME [epoch: 14.7 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019190979149070184		[learning rate: 8.6481e-06]
	Learning Rate: 8.64815e-06
	LOSS [training: 0.019190979149070184 | validation: 0.02194793248541653]
	TIME [epoch: 14.7 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01978824679606478		[learning rate: 8.5871e-06]
	Learning Rate: 8.5871e-06
	LOSS [training: 0.01978824679606478 | validation: 0.023943457296063887]
	TIME [epoch: 14.7 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01894827880114808		[learning rate: 8.5265e-06]
	Learning Rate: 8.52647e-06
	LOSS [training: 0.01894827880114808 | validation: 0.023094252687131757]
	TIME [epoch: 14.7 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018872653204966457		[learning rate: 8.4663e-06]
	Learning Rate: 8.46627e-06
	LOSS [training: 0.018872653204966457 | validation: 0.021873331642679284]
	TIME [epoch: 14.7 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02106166039848284		[learning rate: 8.4065e-06]
	Learning Rate: 8.40651e-06
	LOSS [training: 0.02106166039848284 | validation: 0.021585724808650897]
	TIME [epoch: 14.7 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019167568696953824		[learning rate: 8.3472e-06]
	Learning Rate: 8.34716e-06
	LOSS [training: 0.019167568696953824 | validation: 0.020960561102789484]
	TIME [epoch: 14.7 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018777576565650177		[learning rate: 8.2882e-06]
	Learning Rate: 8.28823e-06
	LOSS [training: 0.018777576565650177 | validation: 0.021475016818247258]
	TIME [epoch: 14.7 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019302275028254545		[learning rate: 8.2297e-06]
	Learning Rate: 8.22972e-06
	LOSS [training: 0.019302275028254545 | validation: 0.02290032019347213]
	TIME [epoch: 14.7 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01939730682506021		[learning rate: 8.1716e-06]
	Learning Rate: 8.17162e-06
	LOSS [training: 0.01939730682506021 | validation: 0.023898389671270726]
	TIME [epoch: 14.7 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01901686679084169		[learning rate: 8.1139e-06]
	Learning Rate: 8.11392e-06
	LOSS [training: 0.01901686679084169 | validation: 0.0234067567423246]
	TIME [epoch: 14.7 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018136325670144657		[learning rate: 8.0566e-06]
	Learning Rate: 8.05664e-06
	LOSS [training: 0.018136325670144657 | validation: 0.02225526256379076]
	TIME [epoch: 14.7 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019475349660765542		[learning rate: 7.9998e-06]
	Learning Rate: 7.99976e-06
	LOSS [training: 0.019475349660765542 | validation: 0.024114374026255127]
	TIME [epoch: 14.7 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019370837331146893		[learning rate: 7.9433e-06]
	Learning Rate: 7.94328e-06
	LOSS [training: 0.019370837331146893 | validation: 0.022428096376100156]
	TIME [epoch: 14.7 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018836296050269742		[learning rate: 7.8872e-06]
	Learning Rate: 7.8872e-06
	LOSS [training: 0.018836296050269742 | validation: 0.02375368035313102]
	TIME [epoch: 14.7 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01925494099479534		[learning rate: 7.8315e-06]
	Learning Rate: 7.83153e-06
	LOSS [training: 0.01925494099479534 | validation: 0.022720716129049412]
	TIME [epoch: 14.7 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019856675576415066		[learning rate: 7.7762e-06]
	Learning Rate: 7.77624e-06
	LOSS [training: 0.019856675576415066 | validation: 0.02431298323517693]
	TIME [epoch: 14.7 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01934010900690333		[learning rate: 7.7213e-06]
	Learning Rate: 7.72133e-06
	LOSS [training: 0.01934010900690333 | validation: 0.023245578572612932]
	TIME [epoch: 14.7 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01793866873250355		[learning rate: 7.6668e-06]
	Learning Rate: 7.66683e-06
	LOSS [training: 0.01793866873250355 | validation: 0.021010542970621146]
	TIME [epoch: 14.7 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018678250508602362		[learning rate: 7.6127e-06]
	Learning Rate: 7.6127e-06
	LOSS [training: 0.018678250508602362 | validation: 0.02247420140099913]
	TIME [epoch: 14.7 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019958274680506254		[learning rate: 7.559e-06]
	Learning Rate: 7.55895e-06
	LOSS [training: 0.019958274680506254 | validation: 0.02133495497561041]
	TIME [epoch: 14.7 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019657675825799217		[learning rate: 7.5056e-06]
	Learning Rate: 7.50559e-06
	LOSS [training: 0.019657675825799217 | validation: 0.020989205531539802]
	TIME [epoch: 14.7 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019441087000478217		[learning rate: 7.4526e-06]
	Learning Rate: 7.4526e-06
	LOSS [training: 0.019441087000478217 | validation: 0.023120618716841703]
	TIME [epoch: 14.7 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0184949295097944		[learning rate: 7.4e-06]
	Learning Rate: 7.39998e-06
	LOSS [training: 0.0184949295097944 | validation: 0.020791643650651354]
	TIME [epoch: 14.7 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018665208297486025		[learning rate: 7.3477e-06]
	Learning Rate: 7.34774e-06
	LOSS [training: 0.018665208297486025 | validation: 0.021245940836680147]
	TIME [epoch: 14.7 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019359419801153438		[learning rate: 7.2959e-06]
	Learning Rate: 7.29587e-06
	LOSS [training: 0.019359419801153438 | validation: 0.023475665487806847]
	TIME [epoch: 14.7 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018983997449980333		[learning rate: 7.2444e-06]
	Learning Rate: 7.24436e-06
	LOSS [training: 0.018983997449980333 | validation: 0.02366180044982318]
	TIME [epoch: 14.7 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01931532077033655		[learning rate: 7.1932e-06]
	Learning Rate: 7.19322e-06
	LOSS [training: 0.01931532077033655 | validation: 0.02241856302332324]
	TIME [epoch: 14.7 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020769516609132534		[learning rate: 7.1424e-06]
	Learning Rate: 7.14244e-06
	LOSS [training: 0.020769516609132534 | validation: 0.025307799076022708]
	TIME [epoch: 14.7 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019893563648650574		[learning rate: 7.092e-06]
	Learning Rate: 7.09201e-06
	LOSS [training: 0.019893563648650574 | validation: 0.021752453957946928]
	TIME [epoch: 14.7 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01922982003894194		[learning rate: 7.0419e-06]
	Learning Rate: 7.04194e-06
	LOSS [training: 0.01922982003894194 | validation: 0.022806077351868254]
	TIME [epoch: 14.7 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019087212745610135		[learning rate: 6.9922e-06]
	Learning Rate: 6.99222e-06
	LOSS [training: 0.019087212745610135 | validation: 0.021870267547333167]
	TIME [epoch: 14.7 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019988265520270006		[learning rate: 6.9429e-06]
	Learning Rate: 6.94286e-06
	LOSS [training: 0.019988265520270006 | validation: 0.023583396596130297]
	TIME [epoch: 14.7 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0189467157046298		[learning rate: 6.8938e-06]
	Learning Rate: 6.89385e-06
	LOSS [training: 0.0189467157046298 | validation: 0.02129692514079979]
	TIME [epoch: 14.7 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01935167049777676		[learning rate: 6.8452e-06]
	Learning Rate: 6.84518e-06
	LOSS [training: 0.01935167049777676 | validation: 0.019798238015388638]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_1054.pth
	Model improved!!!
EPOCH 1055/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019053682952192834		[learning rate: 6.7969e-06]
	Learning Rate: 6.79685e-06
	LOSS [training: 0.019053682952192834 | validation: 0.020343180276329215]
	TIME [epoch: 14.7 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019561015124147346		[learning rate: 6.7489e-06]
	Learning Rate: 6.74887e-06
	LOSS [training: 0.019561015124147346 | validation: 0.02232866256268089]
	TIME [epoch: 14.7 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019219710667633266		[learning rate: 6.7012e-06]
	Learning Rate: 6.70122e-06
	LOSS [training: 0.019219710667633266 | validation: 0.023415744862912598]
	TIME [epoch: 14.7 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01982035032902095		[learning rate: 6.6539e-06]
	Learning Rate: 6.65391e-06
	LOSS [training: 0.01982035032902095 | validation: 0.01875491908284448]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_1058.pth
	Model improved!!!
EPOCH 1059/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02012671840920719		[learning rate: 6.6069e-06]
	Learning Rate: 6.60694e-06
	LOSS [training: 0.02012671840920719 | validation: 0.019704471881417385]
	TIME [epoch: 14.7 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018939252393394376		[learning rate: 6.5603e-06]
	Learning Rate: 6.56029e-06
	LOSS [training: 0.018939252393394376 | validation: 0.021834818114373624]
	TIME [epoch: 14.7 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019178195574733273		[learning rate: 6.514e-06]
	Learning Rate: 6.51398e-06
	LOSS [training: 0.019178195574733273 | validation: 0.022506347065776495]
	TIME [epoch: 14.7 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019739212321765013		[learning rate: 6.468e-06]
	Learning Rate: 6.46799e-06
	LOSS [training: 0.019739212321765013 | validation: 0.022483522144633502]
	TIME [epoch: 14.7 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018842130849622468		[learning rate: 6.4223e-06]
	Learning Rate: 6.42233e-06
	LOSS [training: 0.018842130849622468 | validation: 0.022141789673025953]
	TIME [epoch: 14.7 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019190645185418406		[learning rate: 6.377e-06]
	Learning Rate: 6.37698e-06
	LOSS [training: 0.019190645185418406 | validation: 0.02175523949863852]
	TIME [epoch: 14.7 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019312034526979795		[learning rate: 6.332e-06]
	Learning Rate: 6.33197e-06
	LOSS [training: 0.019312034526979795 | validation: 0.02434060393971288]
	TIME [epoch: 14.7 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020772826331125774		[learning rate: 6.2873e-06]
	Learning Rate: 6.28726e-06
	LOSS [training: 0.020772826331125774 | validation: 0.023143608785626393]
	TIME [epoch: 14.7 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019987443582621403		[learning rate: 6.2429e-06]
	Learning Rate: 6.24288e-06
	LOSS [training: 0.019987443582621403 | validation: 0.0214474899756537]
	TIME [epoch: 14.7 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01986285474620758		[learning rate: 6.1988e-06]
	Learning Rate: 6.1988e-06
	LOSS [training: 0.01986285474620758 | validation: 0.021679808806465934]
	TIME [epoch: 14.7 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019095310616072808		[learning rate: 6.155e-06]
	Learning Rate: 6.15504e-06
	LOSS [training: 0.019095310616072808 | validation: 0.022229038521840362]
	TIME [epoch: 14.7 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018995885632864126		[learning rate: 6.1116e-06]
	Learning Rate: 6.11159e-06
	LOSS [training: 0.018995885632864126 | validation: 0.024643887302761704]
	TIME [epoch: 14.7 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019321567535556403		[learning rate: 6.0684e-06]
	Learning Rate: 6.06844e-06
	LOSS [training: 0.019321567535556403 | validation: 0.0211982829243597]
	TIME [epoch: 14.7 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019898093217023703		[learning rate: 6.0256e-06]
	Learning Rate: 6.0256e-06
	LOSS [training: 0.019898093217023703 | validation: 0.025023564068440248]
	TIME [epoch: 14.7 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018653390140031364		[learning rate: 5.9831e-06]
	Learning Rate: 5.98306e-06
	LOSS [training: 0.018653390140031364 | validation: 0.021846058602660975]
	TIME [epoch: 14.7 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019835523261988883		[learning rate: 5.9408e-06]
	Learning Rate: 5.94082e-06
	LOSS [training: 0.019835523261988883 | validation: 0.023587040033877422]
	TIME [epoch: 14.7 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018011034227101853		[learning rate: 5.8989e-06]
	Learning Rate: 5.89888e-06
	LOSS [training: 0.018011034227101853 | validation: 0.021036897384729566]
	TIME [epoch: 14.7 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018994503955643813		[learning rate: 5.8572e-06]
	Learning Rate: 5.85723e-06
	LOSS [training: 0.018994503955643813 | validation: 0.022914598019319263]
	TIME [epoch: 14.7 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01882010980836809		[learning rate: 5.8159e-06]
	Learning Rate: 5.81588e-06
	LOSS [training: 0.01882010980836809 | validation: 0.022921090326241917]
	TIME [epoch: 14.7 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01879077312403428		[learning rate: 5.7748e-06]
	Learning Rate: 5.77482e-06
	LOSS [training: 0.01879077312403428 | validation: 0.023565822740333187]
	TIME [epoch: 14.7 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01887690107431149		[learning rate: 5.7341e-06]
	Learning Rate: 5.73405e-06
	LOSS [training: 0.01887690107431149 | validation: 0.020651110857038513]
	TIME [epoch: 14.7 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018792995897936617		[learning rate: 5.6936e-06]
	Learning Rate: 5.69357e-06
	LOSS [training: 0.018792995897936617 | validation: 0.021799787828033212]
	TIME [epoch: 14.7 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0200254816088922		[learning rate: 5.6534e-06]
	Learning Rate: 5.65337e-06
	LOSS [training: 0.0200254816088922 | validation: 0.021743608862395243]
	TIME [epoch: 14.7 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019751981327337978		[learning rate: 5.6135e-06]
	Learning Rate: 5.61346e-06
	LOSS [training: 0.019751981327337978 | validation: 0.02416021140195614]
	TIME [epoch: 14.7 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019411338801439533		[learning rate: 5.5738e-06]
	Learning Rate: 5.57383e-06
	LOSS [training: 0.019411338801439533 | validation: 0.02143038926189286]
	TIME [epoch: 14.7 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019829488939104217		[learning rate: 5.5345e-06]
	Learning Rate: 5.53448e-06
	LOSS [training: 0.019829488939104217 | validation: 0.02394567014597579]
	TIME [epoch: 14.7 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019132266401632056		[learning rate: 5.4954e-06]
	Learning Rate: 5.49541e-06
	LOSS [training: 0.019132266401632056 | validation: 0.021234704217028222]
	TIME [epoch: 14.7 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018609848913293793		[learning rate: 5.4566e-06]
	Learning Rate: 5.45661e-06
	LOSS [training: 0.018609848913293793 | validation: 0.0230632741520436]
	TIME [epoch: 14.7 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020306401373637373		[learning rate: 5.4181e-06]
	Learning Rate: 5.41809e-06
	LOSS [training: 0.020306401373637373 | validation: 0.020904933649898275]
	TIME [epoch: 14.7 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018789819841741627		[learning rate: 5.3798e-06]
	Learning Rate: 5.37984e-06
	LOSS [training: 0.018789819841741627 | validation: 0.02216955028269075]
	TIME [epoch: 14.7 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01888711438282091		[learning rate: 5.3419e-06]
	Learning Rate: 5.34186e-06
	LOSS [training: 0.01888711438282091 | validation: 0.023545001551274772]
	TIME [epoch: 14.7 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01817277871201617		[learning rate: 5.3041e-06]
	Learning Rate: 5.30415e-06
	LOSS [training: 0.01817277871201617 | validation: 0.02205082608253138]
	TIME [epoch: 14.7 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018743733635122238		[learning rate: 5.2667e-06]
	Learning Rate: 5.2667e-06
	LOSS [training: 0.018743733635122238 | validation: 0.022368760225875424]
	TIME [epoch: 14.7 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019901339633814637		[learning rate: 5.2295e-06]
	Learning Rate: 5.22952e-06
	LOSS [training: 0.019901339633814637 | validation: 0.02053980828757699]
	TIME [epoch: 14.7 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018872296492442847		[learning rate: 5.1926e-06]
	Learning Rate: 5.1926e-06
	LOSS [training: 0.018872296492442847 | validation: 0.022769567776357982]
	TIME [epoch: 14.7 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019064701762391306		[learning rate: 5.1559e-06]
	Learning Rate: 5.15594e-06
	LOSS [training: 0.019064701762391306 | validation: 0.022274530882974927]
	TIME [epoch: 14.7 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019282709695140458		[learning rate: 5.1195e-06]
	Learning Rate: 5.11954e-06
	LOSS [training: 0.019282709695140458 | validation: 0.02387877956642165]
	TIME [epoch: 14.7 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01956592500056044		[learning rate: 5.0834e-06]
	Learning Rate: 5.0834e-06
	LOSS [training: 0.01956592500056044 | validation: 0.02214926295584443]
	TIME [epoch: 14.7 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019207080974824684		[learning rate: 5.0475e-06]
	Learning Rate: 5.04751e-06
	LOSS [training: 0.019207080974824684 | validation: 0.02223006415105659]
	TIME [epoch: 14.7 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018607174632613466		[learning rate: 5.0119e-06]
	Learning Rate: 5.01188e-06
	LOSS [training: 0.018607174632613466 | validation: 0.02332965797926508]
	TIME [epoch: 14.7 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019817670559007287		[learning rate: 4.9765e-06]
	Learning Rate: 4.97649e-06
	LOSS [training: 0.019817670559007287 | validation: 0.022133626558126247]
	TIME [epoch: 14.7 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01907721056225898		[learning rate: 4.9414e-06]
	Learning Rate: 4.94136e-06
	LOSS [training: 0.01907721056225898 | validation: 0.024743893781271155]
	TIME [epoch: 14.7 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01898794336518618		[learning rate: 4.9065e-06]
	Learning Rate: 4.90647e-06
	LOSS [training: 0.01898794336518618 | validation: 0.02278741879457647]
	TIME [epoch: 14.7 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019053406491319966		[learning rate: 4.8718e-06]
	Learning Rate: 4.87183e-06
	LOSS [training: 0.019053406491319966 | validation: 0.01996426527738577]
	TIME [epoch: 14.7 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01832345455571323		[learning rate: 4.8374e-06]
	Learning Rate: 4.83744e-06
	LOSS [training: 0.01832345455571323 | validation: 0.0221029638334376]
	TIME [epoch: 14.7 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019903993323926082		[learning rate: 4.8033e-06]
	Learning Rate: 4.80329e-06
	LOSS [training: 0.019903993323926082 | validation: 0.021868748627933584]
	TIME [epoch: 14.7 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01875321993158005		[learning rate: 4.7694e-06]
	Learning Rate: 4.76938e-06
	LOSS [training: 0.01875321993158005 | validation: 0.02137696917148948]
	TIME [epoch: 14.7 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019670426046008112		[learning rate: 4.7357e-06]
	Learning Rate: 4.73571e-06
	LOSS [training: 0.019670426046008112 | validation: 0.026415798817536806]
	TIME [epoch: 14.7 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019390253739866248		[learning rate: 4.7023e-06]
	Learning Rate: 4.70227e-06
	LOSS [training: 0.019390253739866248 | validation: 0.021668361184650382]
	TIME [epoch: 14.7 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01975081695831844		[learning rate: 4.6691e-06]
	Learning Rate: 4.66908e-06
	LOSS [training: 0.01975081695831844 | validation: 0.021559884248988626]
	TIME [epoch: 14.7 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0181574744449139		[learning rate: 4.6361e-06]
	Learning Rate: 4.63611e-06
	LOSS [training: 0.0181574744449139 | validation: 0.019258443312662323]
	TIME [epoch: 14.7 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020152045137258406		[learning rate: 4.6034e-06]
	Learning Rate: 4.60338e-06
	LOSS [training: 0.020152045137258406 | validation: 0.02006879037152759]
	TIME [epoch: 14.7 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019604551429055476		[learning rate: 4.5709e-06]
	Learning Rate: 4.57088e-06
	LOSS [training: 0.019604551429055476 | validation: 0.02283524161221273]
	TIME [epoch: 14.7 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018430093309972626		[learning rate: 4.5386e-06]
	Learning Rate: 4.53861e-06
	LOSS [training: 0.018430093309972626 | validation: 0.02160287778695441]
	TIME [epoch: 14.7 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018691319672765147		[learning rate: 4.5066e-06]
	Learning Rate: 4.50657e-06
	LOSS [training: 0.018691319672765147 | validation: 0.022162109753067052]
	TIME [epoch: 14.7 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01906058523580089		[learning rate: 4.4748e-06]
	Learning Rate: 4.47475e-06
	LOSS [training: 0.01906058523580089 | validation: 0.02328256650401328]
	TIME [epoch: 14.7 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018178246006618576		[learning rate: 4.4432e-06]
	Learning Rate: 4.44317e-06
	LOSS [training: 0.018178246006618576 | validation: 0.020774331946792686]
	TIME [epoch: 14.7 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019936561107802203		[learning rate: 4.4118e-06]
	Learning Rate: 4.4118e-06
	LOSS [training: 0.019936561107802203 | validation: 0.0201128492826133]
	TIME [epoch: 14.7 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019074137688037413		[learning rate: 4.3806e-06]
	Learning Rate: 4.38065e-06
	LOSS [training: 0.019074137688037413 | validation: 0.02334682834520013]
	TIME [epoch: 14.7 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018771957199893684		[learning rate: 4.3497e-06]
	Learning Rate: 4.34973e-06
	LOSS [training: 0.018771957199893684 | validation: 0.022298868242939408]
	TIME [epoch: 14.7 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01949637401151782		[learning rate: 4.319e-06]
	Learning Rate: 4.31902e-06
	LOSS [training: 0.01949637401151782 | validation: 0.022522547597749267]
	TIME [epoch: 14.7 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019116243585696226		[learning rate: 4.2885e-06]
	Learning Rate: 4.28852e-06
	LOSS [training: 0.019116243585696226 | validation: 0.023244735468970813]
	TIME [epoch: 14.7 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018460397806629655		[learning rate: 4.2582e-06]
	Learning Rate: 4.25825e-06
	LOSS [training: 0.018460397806629655 | validation: 0.022474662028770777]
	TIME [epoch: 14.7 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020071337310832785		[learning rate: 4.2282e-06]
	Learning Rate: 4.22819e-06
	LOSS [training: 0.020071337310832785 | validation: 0.021587716917495475]
	TIME [epoch: 14.7 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02044120377171981		[learning rate: 4.1983e-06]
	Learning Rate: 4.19833e-06
	LOSS [training: 0.02044120377171981 | validation: 0.02268353565148571]
	TIME [epoch: 14.7 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01947412329956543		[learning rate: 4.1687e-06]
	Learning Rate: 4.16869e-06
	LOSS [training: 0.01947412329956543 | validation: 0.022961943719725708]
	TIME [epoch: 14.7 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018958149168485025		[learning rate: 4.1393e-06]
	Learning Rate: 4.13927e-06
	LOSS [training: 0.018958149168485025 | validation: 0.023385967122227763]
	TIME [epoch: 14.7 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018891231092978605		[learning rate: 4.11e-06]
	Learning Rate: 4.11004e-06
	LOSS [training: 0.018891231092978605 | validation: 0.02172299155356317]
	TIME [epoch: 14.7 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018325669934165255		[learning rate: 4.081e-06]
	Learning Rate: 4.08103e-06
	LOSS [training: 0.018325669934165255 | validation: 0.021204810313862302]
	TIME [epoch: 14.7 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020091319560468793		[learning rate: 4.0522e-06]
	Learning Rate: 4.05222e-06
	LOSS [training: 0.020091319560468793 | validation: 0.02244021867104389]
	TIME [epoch: 14.7 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019526527223312055		[learning rate: 4.0236e-06]
	Learning Rate: 4.02361e-06
	LOSS [training: 0.019526527223312055 | validation: 0.022906113555611847]
	TIME [epoch: 14.7 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02050961623224471		[learning rate: 3.9952e-06]
	Learning Rate: 3.9952e-06
	LOSS [training: 0.02050961623224471 | validation: 0.0215441904317013]
	TIME [epoch: 14.7 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019423899295523087		[learning rate: 3.967e-06]
	Learning Rate: 3.96699e-06
	LOSS [training: 0.019423899295523087 | validation: 0.020839679197542944]
	TIME [epoch: 14.7 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01880677154921492		[learning rate: 3.939e-06]
	Learning Rate: 3.93899e-06
	LOSS [training: 0.01880677154921492 | validation: 0.020408981983467817]
	TIME [epoch: 14.7 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019580770747781476		[learning rate: 3.9112e-06]
	Learning Rate: 3.91118e-06
	LOSS [training: 0.019580770747781476 | validation: 0.02367442131552644]
	TIME [epoch: 14.7 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018897997505186826		[learning rate: 3.8836e-06]
	Learning Rate: 3.88357e-06
	LOSS [training: 0.018897997505186826 | validation: 0.022886911159520736]
	TIME [epoch: 14.7 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018907499398072666		[learning rate: 3.8562e-06]
	Learning Rate: 3.85615e-06
	LOSS [training: 0.018907499398072666 | validation: 0.020610295825042653]
	TIME [epoch: 14.7 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019713738932477624		[learning rate: 3.8289e-06]
	Learning Rate: 3.82893e-06
	LOSS [training: 0.019713738932477624 | validation: 0.023046531266451405]
	TIME [epoch: 14.7 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019958562047921687		[learning rate: 3.8019e-06]
	Learning Rate: 3.80189e-06
	LOSS [training: 0.019958562047921687 | validation: 0.023239562726027597]
	TIME [epoch: 14.7 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018564793147015063		[learning rate: 3.7751e-06]
	Learning Rate: 3.77506e-06
	LOSS [training: 0.018564793147015063 | validation: 0.021411516500946543]
	TIME [epoch: 14.7 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01898354630469805		[learning rate: 3.7484e-06]
	Learning Rate: 3.7484e-06
	LOSS [training: 0.01898354630469805 | validation: 0.02127621142782676]
	TIME [epoch: 14.7 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01884507496821608		[learning rate: 3.7219e-06]
	Learning Rate: 3.72194e-06
	LOSS [training: 0.01884507496821608 | validation: 0.022236023575051757]
	TIME [epoch: 14.7 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02002688328683886		[learning rate: 3.6957e-06]
	Learning Rate: 3.69566e-06
	LOSS [training: 0.02002688328683886 | validation: 0.023136375999289645]
	TIME [epoch: 14.7 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019013721064519297		[learning rate: 3.6696e-06]
	Learning Rate: 3.66957e-06
	LOSS [training: 0.019013721064519297 | validation: 0.021448202159549637]
	TIME [epoch: 14.7 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019076577592653066		[learning rate: 3.6437e-06]
	Learning Rate: 3.64367e-06
	LOSS [training: 0.019076577592653066 | validation: 0.02152666633148839]
	TIME [epoch: 14.7 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019773407209618732		[learning rate: 3.6179e-06]
	Learning Rate: 3.61794e-06
	LOSS [training: 0.019773407209618732 | validation: 0.021203483918473483]
	TIME [epoch: 14.7 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019419119780247435		[learning rate: 3.5924e-06]
	Learning Rate: 3.5924e-06
	LOSS [training: 0.019419119780247435 | validation: 0.022500051535335575]
	TIME [epoch: 14.7 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020180753959204212		[learning rate: 3.567e-06]
	Learning Rate: 3.56704e-06
	LOSS [training: 0.020180753959204212 | validation: 0.022137769850697615]
	TIME [epoch: 14.7 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01818086961991803		[learning rate: 3.5419e-06]
	Learning Rate: 3.54186e-06
	LOSS [training: 0.01818086961991803 | validation: 0.023968327873497775]
	TIME [epoch: 14.7 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01959992088053259		[learning rate: 3.5169e-06]
	Learning Rate: 3.51685e-06
	LOSS [training: 0.01959992088053259 | validation: 0.022508615223604086]
	TIME [epoch: 14.7 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02059061952715692		[learning rate: 3.492e-06]
	Learning Rate: 3.49202e-06
	LOSS [training: 0.02059061952715692 | validation: 0.022999911214721887]
	TIME [epoch: 14.7 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019282709589318568		[learning rate: 3.4674e-06]
	Learning Rate: 3.46737e-06
	LOSS [training: 0.019282709589318568 | validation: 0.021716332324095936]
	TIME [epoch: 14.7 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02053815571596805		[learning rate: 3.4429e-06]
	Learning Rate: 3.44289e-06
	LOSS [training: 0.02053815571596805 | validation: 0.02303428432284856]
	TIME [epoch: 14.7 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019943814670634545		[learning rate: 3.4186e-06]
	Learning Rate: 3.41859e-06
	LOSS [training: 0.019943814670634545 | validation: 0.02208054840478618]
	TIME [epoch: 14.7 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019750956397158334		[learning rate: 3.3944e-06]
	Learning Rate: 3.39445e-06
	LOSS [training: 0.019750956397158334 | validation: 0.021605117015798966]
	TIME [epoch: 14.7 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01880247124406541		[learning rate: 3.3705e-06]
	Learning Rate: 3.37048e-06
	LOSS [training: 0.01880247124406541 | validation: 0.02564954457861815]
	TIME [epoch: 14.7 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019545532844788147		[learning rate: 3.3467e-06]
	Learning Rate: 3.34669e-06
	LOSS [training: 0.019545532844788147 | validation: 0.023579324393653324]
	TIME [epoch: 14.7 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019883757916554078		[learning rate: 3.3231e-06]
	Learning Rate: 3.32306e-06
	LOSS [training: 0.019883757916554078 | validation: 0.02240948539647818]
	TIME [epoch: 14.7 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018542083778804765		[learning rate: 3.2996e-06]
	Learning Rate: 3.2996e-06
	LOSS [training: 0.018542083778804765 | validation: 0.024027003681688558]
	TIME [epoch: 14.7 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020376813208257235		[learning rate: 3.2763e-06]
	Learning Rate: 3.27631e-06
	LOSS [training: 0.020376813208257235 | validation: 0.02305096347162777]
	TIME [epoch: 14.7 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018736744286774815		[learning rate: 3.2532e-06]
	Learning Rate: 3.25318e-06
	LOSS [training: 0.018736744286774815 | validation: 0.02527075779414996]
	TIME [epoch: 14.7 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240807_171303/states/model_phi1_2a_v_mmd1_1159.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 7568.996 seconds.
