Args:
Namespace(name='model_phi1_1a_v_mmd1_fix_noise_small', outdir='out/model_training/model_phi1_1a_v_mmd1_fix_noise_small', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1614743537

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.878243998489193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.878243998489193 | validation: 3.8750160567740886]
	TIME [epoch: 159 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9734554231596766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9734554231596766 | validation: 4.07290180743415]
	TIME [epoch: 7.9 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7637798523703028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7637798523703028 | validation: 3.6149124465951417]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.664698288386563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.664698288386563 | validation: 3.6625529854431673]
	TIME [epoch: 7.77 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5516838634143726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5516838634143726 | validation: 3.7073239012851875]
	TIME [epoch: 7.77 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5533374890760956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5533374890760956 | validation: 3.710823387087284]
	TIME [epoch: 7.78 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.388001856271068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.388001856271068 | validation: 3.5177790951135854]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3611235409756945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3611235409756945 | validation: 4.2442792789274755]
	TIME [epoch: 7.82 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4464943041833482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4464943041833482 | validation: 3.467428491443102]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.863990197711308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.863990197711308 | validation: 3.79692831136054]
	TIME [epoch: 7.77 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.269106277234324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.269106277234324 | validation: 2.345458770451126]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.120900818542644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.120900818542644 | validation: 2.449202493135491]
	TIME [epoch: 7.78 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.380477028211471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.380477028211471 | validation: 2.1405350310717197]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.149741797696955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.149741797696955 | validation: 2.6432575764614246]
	TIME [epoch: 7.77 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2559793708078053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2559793708078053 | validation: 1.765368651205185]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8169630990013164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8169630990013164 | validation: 2.212922574062287]
	TIME [epoch: 7.77 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.484121241794636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.484121241794636 | validation: 2.0592975193743195]
	TIME [epoch: 7.79 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8116315367843223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8116315367843223 | validation: 1.6567199206457153]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5801783247328038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5801783247328038 | validation: 1.707181465034762]
	TIME [epoch: 7.77 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.7840212250960277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7840212250960277 | validation: 2.8726593900947783]
	TIME [epoch: 7.77 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.283469378358724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.283469378358724 | validation: 2.4185944463774716]
	TIME [epoch: 7.78 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1865849993254862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1865849993254862 | validation: 1.7407699431203318]
	TIME [epoch: 7.8 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4536885041315935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4536885041315935 | validation: 1.4540356544540347]
	TIME [epoch: 7.79 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4184424918785745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4184424918785745 | validation: 1.1575195530391467]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1940687601967876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1940687601967876 | validation: 1.447965400762603]
	TIME [epoch: 7.77 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4074026087334717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4074026087334717 | validation: 1.1569497494539307]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.410336405442742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.410336405442742 | validation: 1.838752014877967]
	TIME [epoch: 7.81 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8116788996322635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8116788996322635 | validation: 1.7948354358722611]
	TIME [epoch: 7.78 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5860606442337406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5860606442337406 | validation: 1.0318512812130551]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8951730767646894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8951730767646894 | validation: 1.0341528175521155]
	TIME [epoch: 7.78 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5619607482907276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5619607482907276 | validation: 2.339632280419739]
	TIME [epoch: 7.77 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5647542138980546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5647542138980546 | validation: 1.212680053902956]
	TIME [epoch: 7.83 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2162447361610198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2162447361610198 | validation: 1.1291417066637526]
	TIME [epoch: 7.78 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0886588351479958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0886588351479958 | validation: 0.8319979821829392]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9568825922590143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9568825922590143 | validation: 0.7740718647617535]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0901785353085138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0901785353085138 | validation: 1.588101223004216]
	TIME [epoch: 7.78 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1069210547053083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1069210547053083 | validation: 0.82526768957685]
	TIME [epoch: 7.82 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1741686596658725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1741686596658725 | validation: 1.7514815077320116]
	TIME [epoch: 7.77 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4517597353144027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4517597353144027 | validation: 1.0351167767371081]
	TIME [epoch: 7.77 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1685909672152408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1685909672152408 | validation: 0.794561956506482]
	TIME [epoch: 7.77 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7767681878841266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7767681878841266 | validation: 1.3749337141404359]
	TIME [epoch: 7.78 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1829188812414717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1829188812414717 | validation: 1.2516698298456908]
	TIME [epoch: 7.82 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2815814937209065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2815814937209065 | validation: 0.9175750542887781]
	TIME [epoch: 7.77 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9478272759179791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9478272759179791 | validation: 0.64647273216094]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5997299637865787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5997299637865787 | validation: 0.8852868206782579]
	TIME [epoch: 7.77 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.247801529434021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.247801529434021 | validation: 1.0728139889079147]
	TIME [epoch: 7.78 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1134239890747388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1134239890747388 | validation: 1.2360557840172135]
	TIME [epoch: 7.8 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0686560071459348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0686560071459348 | validation: 0.7732305807184211]
	TIME [epoch: 7.77 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8392079254593579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8392079254593579 | validation: 0.6665930393467956]
	TIME [epoch: 7.77 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8121151019415853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8121151019415853 | validation: 0.5931777512010378]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9039916946971174		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 0.9039916946971174 | validation: 0.7749423981359382]
	TIME [epoch: 7.79 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9858394358951084		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 0.9858394358951084 | validation: 0.4832855835047206]
	TIME [epoch: 7.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5454247040774028		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 0.5454247040774028 | validation: 0.8586044926859067]
	TIME [epoch: 7.77 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9137518776526925		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 0.9137518776526925 | validation: 0.706195730864104]
	TIME [epoch: 7.76 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7466116345617685		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 0.7466116345617685 | validation: 0.4550013778511781]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.519646879014764		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 0.519646879014764 | validation: 0.5963260776478017]
	TIME [epoch: 7.8 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7788652531874611		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 0.7788652531874611 | validation: 0.9902034020742114]
	TIME [epoch: 7.79 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.832943167829414		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 0.832943167829414 | validation: 0.4353044515215343]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6315621134790209		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 0.6315621134790209 | validation: 0.5148543926419169]
	TIME [epoch: 7.76 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44624846897473397		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 0.44624846897473397 | validation: 0.6683775842435358]
	TIME [epoch: 7.76 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6647706357442845		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 0.6647706357442845 | validation: 0.6155323671630544]
	TIME [epoch: 7.81 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.628812862529353		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 0.628812862529353 | validation: 0.42203992478736413]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45972536849816237		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 0.45972536849816237 | validation: 0.47936501590087816]
	TIME [epoch: 7.77 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5170463262074757		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 0.5170463262074757 | validation: 0.39144550077031137]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4954479559805466		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 0.4954479559805466 | validation: 0.38701860314907355]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5066053090473278		[learning rate: 0.0094573]
	Learning Rate: 0.00945734
	LOSS [training: 0.5066053090473278 | validation: 0.36484309295958217]
	TIME [epoch: 7.83 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6409278621684278		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 0.6409278621684278 | validation: 0.40831741162935886]
	TIME [epoch: 7.77 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43343109594633494		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 0.43343109594633494 | validation: 0.4235302611446008]
	TIME [epoch: 7.78 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4136008756999916		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 0.4136008756999916 | validation: 0.5391315307084932]
	TIME [epoch: 7.76 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5287159103066776		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.5287159103066776 | validation: 0.45366591027130976]
	TIME [epoch: 7.77 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46409343524087576		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 0.46409343524087576 | validation: 0.3454188858087349]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36799350815317367		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 0.36799350815317367 | validation: 0.46067270553314854]
	TIME [epoch: 7.77 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5365098619804333		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 0.5365098619804333 | validation: 0.35751246414090954]
	TIME [epoch: 7.76 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5847183545554777		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 0.5847183545554777 | validation: 0.3678853454911033]
	TIME [epoch: 7.76 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41609081901842304		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.41609081901842304 | validation: 0.3742689418097125]
	TIME [epoch: 7.79 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47812798186646743		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 0.47812798186646743 | validation: 0.38442864643707775]
	TIME [epoch: 7.8 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4010089415873138		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 0.4010089415873138 | validation: 0.40325961171850155]
	TIME [epoch: 7.76 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3522218638761585		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.3522218638761585 | validation: 0.427476952328527]
	TIME [epoch: 7.77 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4108164965952357		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 0.4108164965952357 | validation: 0.6079338287348217]
	TIME [epoch: 7.76 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42745325937098433		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.42745325937098433 | validation: 0.3350524108403765]
	TIME [epoch: 7.79 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3284542248884232		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.3284542248884232 | validation: 0.32452407849255616]
	TIME [epoch: 7.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5136416570417688		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.5136416570417688 | validation: 0.4708704101105813]
	TIME [epoch: 7.76 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6268054832563208		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.6268054832563208 | validation: 0.3635712161264082]
	TIME [epoch: 7.76 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.363292262910501		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.363292262910501 | validation: 0.4208886424013775]
	TIME [epoch: 7.75 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4154043754113229		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.4154043754113229 | validation: 0.2876752588552313]
	TIME [epoch: 7.79 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.301099888739669		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.301099888739669 | validation: 0.33589278402853]
	TIME [epoch: 7.8 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42430628653483354		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.42430628653483354 | validation: 0.3606743552655042]
	TIME [epoch: 7.77 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.415720499026772		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.415720499026772 | validation: 0.3250752146060792]
	TIME [epoch: 7.77 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3096014953498837		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.3096014953498837 | validation: 0.28651138454620917]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3328398257949893		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.3328398257949893 | validation: 0.30647210736579156]
	TIME [epoch: 7.83 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3438176942879014		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.3438176942879014 | validation: 0.335630159684328]
	TIME [epoch: 7.78 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35133348460882596		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.35133348460882596 | validation: 0.4030249123804851]
	TIME [epoch: 7.78 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3767581042231262		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.3767581042231262 | validation: 0.5551469870258265]
	TIME [epoch: 7.76 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45874003589776935		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.45874003589776935 | validation: 0.34033370388273443]
	TIME [epoch: 7.76 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.309573914883727		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.309573914883727 | validation: 0.4264111301562778]
	TIME [epoch: 7.81 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30629392636944425		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.30629392636944425 | validation: 0.2746129691977027]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3013698860586095		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.3013698860586095 | validation: 0.3410668981283301]
	TIME [epoch: 7.77 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46451504911711144		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.46451504911711144 | validation: 0.3866613758698228]
	TIME [epoch: 7.76 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41979910373498686		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.41979910373498686 | validation: 0.31121545175072196]
	TIME [epoch: 7.76 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3267214449560525		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.3267214449560525 | validation: 0.3472596720548422]
	TIME [epoch: 7.81 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3350617807009568		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.3350617807009568 | validation: 0.37199096540081433]
	TIME [epoch: 7.77 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32120821747647493		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.32120821747647493 | validation: 0.2822545612811795]
	TIME [epoch: 7.76 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2872297661289511		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.2872297661289511 | validation: 0.39323976245655357]
	TIME [epoch: 7.76 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30876795152104836		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.30876795152104836 | validation: 0.29956485325048193]
	TIME [epoch: 7.77 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4070012276381262		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.4070012276381262 | validation: 0.6542313857662185]
	TIME [epoch: 7.8 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5376817203035253		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.5376817203035253 | validation: 0.24260577637027975]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2760720657161365		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.2760720657161365 | validation: 0.27095020288873517]
	TIME [epoch: 7.76 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24762307817278761		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.24762307817278761 | validation: 0.3054795288823864]
	TIME [epoch: 7.76 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4242523814400593		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.4242523814400593 | validation: 0.2603697476501361]
	TIME [epoch: 7.77 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2790586508466972		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.2790586508466972 | validation: 0.28958042588259925]
	TIME [epoch: 7.8 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28054407796666136		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.28054407796666136 | validation: 0.31935568129088576]
	TIME [epoch: 7.76 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34723658334820595		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.34723658334820595 | validation: 0.3329549773930492]
	TIME [epoch: 7.76 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2764309878858787		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.2764309878858787 | validation: 0.2187215836927544]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2365070895828827		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.2365070895828827 | validation: 0.22177706392613453]
	TIME [epoch: 7.78 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2570484701248685		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.2570484701248685 | validation: 0.3808222442189404]
	TIME [epoch: 7.79 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3767448601672875		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.3767448601672875 | validation: 0.23143821124421654]
	TIME [epoch: 7.76 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24673658052987862		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.24673658052987862 | validation: 0.21654323181837834]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24696206219555733		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.24696206219555733 | validation: 0.29227176716019554]
	TIME [epoch: 7.76 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2942491522234548		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.2942491522234548 | validation: 0.2281232090923882]
	TIME [epoch: 7.79 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24118128598658867		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.24118128598658867 | validation: 0.2378424719872417]
	TIME [epoch: 7.78 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2615248165945659		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.2615248165945659 | validation: 0.22793172476818527]
	TIME [epoch: 7.75 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2929607902403884		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.2929607902403884 | validation: 0.317560227441574]
	TIME [epoch: 7.76 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2786085335267635		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.2786085335267635 | validation: 0.28630974792742336]
	TIME [epoch: 7.75 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2894995462988883		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.2894995462988883 | validation: 0.22711736419576162]
	TIME [epoch: 7.78 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33416655859666133		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.33416655859666133 | validation: 0.22211771384192686]
	TIME [epoch: 7.79 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2074333654918304		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.2074333654918304 | validation: 0.1945403715708171]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20472617674941557		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.20472617674941557 | validation: 0.25454982440748497]
	TIME [epoch: 7.76 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2717270436100746		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.2717270436100746 | validation: 0.1770230771580826]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2485892980287917		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.2485892980287917 | validation: 0.37307984502659625]
	TIME [epoch: 7.8 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2774633404972447		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.2774633404972447 | validation: 0.3008217954700084]
	TIME [epoch: 7.78 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24773037426849312		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.24773037426849312 | validation: 0.17814921646616017]
	TIME [epoch: 7.76 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23211034427834354		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.23211034427834354 | validation: 0.45826614339096333]
	TIME [epoch: 7.75 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3503254425107502		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.3503254425107502 | validation: 0.1971751716881253]
	TIME [epoch: 7.76 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24163093829277477		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.24163093829277477 | validation: 0.33333743673219673]
	TIME [epoch: 7.8 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24361265108139613		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.24361265108139613 | validation: 0.21626228256570926]
	TIME [epoch: 7.78 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2453341914004724		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.2453341914004724 | validation: 0.22089985757180278]
	TIME [epoch: 7.76 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28146802741887883		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.28146802741887883 | validation: 0.24277341402858643]
	TIME [epoch: 7.76 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23038913545718365		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 0.23038913545718365 | validation: 0.28924304965611014]
	TIME [epoch: 7.76 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23452946268993566		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.23452946268993566 | validation: 0.20808368811626035]
	TIME [epoch: 7.81 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2746880602942203		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.2746880602942203 | validation: 0.2140573000122411]
	TIME [epoch: 7.77 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26025851361688923		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.26025851361688923 | validation: 0.16868263594766236]
	TIME [epoch: 7.75 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_141.pth
	Model improved!!!
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2142535264145628		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.2142535264145628 | validation: 0.38797168236488044]
	TIME [epoch: 7.77 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28280004186474234		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.28280004186474234 | validation: 0.21690445695460647]
	TIME [epoch: 7.77 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17334778443108645		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.17334778443108645 | validation: 0.23859705903657713]
	TIME [epoch: 7.82 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2538519567131037		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.2538519567131037 | validation: 0.19432429665881346]
	TIME [epoch: 7.77 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20648290029171024		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.20648290029171024 | validation: 0.18745905602404184]
	TIME [epoch: 7.77 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22262692663655909		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.22262692663655909 | validation: 0.16531994906327369]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21364585026296162		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.21364585026296162 | validation: 0.24130828904159227]
	TIME [epoch: 7.77 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22267846890351828		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.22267846890351828 | validation: 0.21782082242241657]
	TIME [epoch: 7.81 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26082103899757936		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.26082103899757936 | validation: 0.15742074856756305]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_150.pth
	Model improved!!!
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2247008283433822		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.2247008283433822 | validation: 0.2588852074750162]
	TIME [epoch: 7.76 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1944023794235843		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.1944023794235843 | validation: 0.22898171013433116]
	TIME [epoch: 7.76 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23609993490593156		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.23609993490593156 | validation: 0.16900307421930755]
	TIME [epoch: 7.77 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17966379191487622		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.17966379191487622 | validation: 0.1575557981161364]
	TIME [epoch: 7.81 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1718160289202602		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.1718160289202602 | validation: 0.16534731254904766]
	TIME [epoch: 7.76 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15935188433150865		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.15935188433150865 | validation: 0.15532660199876305]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19416262821513638		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.19416262821513638 | validation: 0.3112123102265971]
	TIME [epoch: 7.77 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25398412695358696		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.25398412695358696 | validation: 0.15300705722833274]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_158.pth
	Model improved!!!
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16907989978348623		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.16907989978348623 | validation: 0.24809308082133524]
	TIME [epoch: 7.81 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24064599571489692		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.24064599571489692 | validation: 0.15135747618479325]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17563427974962809		[learning rate: 0.0067548]
	Learning Rate: 0.00675485
	LOSS [training: 0.17563427974962809 | validation: 0.16910357594283407]
	TIME [epoch: 7.76 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15545843523219005		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.15545843523219005 | validation: 0.15560879500345753]
	TIME [epoch: 7.77 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1506730070908317		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.1506730070908317 | validation: 0.2383208473335116]
	TIME [epoch: 7.81 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2180128348218274		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.2180128348218274 | validation: 0.28173955860205635]
	TIME [epoch: 7.79 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21690152273325009		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.21690152273325009 | validation: 0.20592140172903056]
	TIME [epoch: 7.77 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16573727102066255		[learning rate: 0.0066363]
	Learning Rate: 0.00663625
	LOSS [training: 0.16573727102066255 | validation: 0.1529668310162156]
	TIME [epoch: 7.76 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17276485046441878		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.17276485046441878 | validation: 0.1486830809179841]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_167.pth
	Model improved!!!
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14566080902918077		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.14566080902918077 | validation: 0.12067524470500589]
	TIME [epoch: 7.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_168.pth
	Model improved!!!
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13140460890331798		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.13140460890331798 | validation: 0.1607575673965594]
	TIME [epoch: 7.79 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1504607798495386		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.1504607798495386 | validation: 0.15090145118153347]
	TIME [epoch: 7.76 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1867727509509043		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.1867727509509043 | validation: 0.15480620008465848]
	TIME [epoch: 7.76 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17956922787263457		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.17956922787263457 | validation: 0.1805864036530643]
	TIME [epoch: 7.77 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20738993988355175		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.20738993988355175 | validation: 0.14748127805502945]
	TIME [epoch: 7.8 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14618512967287375		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.14618512967287375 | validation: 0.19413265975046906]
	TIME [epoch: 7.78 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14428876954160977		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 0.14428876954160977 | validation: 0.1648689639947279]
	TIME [epoch: 7.76 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14157639168059133		[learning rate: 0.0064053]
	Learning Rate: 0.00640529
	LOSS [training: 0.14157639168059133 | validation: 0.13320533651829725]
	TIME [epoch: 7.77 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12010231402897345		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.12010231402897345 | validation: 0.1916796778262974]
	TIME [epoch: 7.76 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15662148719694183		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.15662148719694183 | validation: 0.15001802209332182]
	TIME [epoch: 7.81 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14179636706468673		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.14179636706468673 | validation: 0.18814876028919486]
	TIME [epoch: 7.77 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17524426327217296		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.17524426327217296 | validation: 0.11724890914793991]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_180.pth
	Model improved!!!
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14857382049172466		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.14857382049172466 | validation: 0.1502822888151024]
	TIME [epoch: 7.78 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.238190240958981		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.238190240958981 | validation: 0.24172604731505554]
	TIME [epoch: 7.77 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19297694398019427		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.19297694398019427 | validation: 0.07931219781800752]
	TIME [epoch: 7.82 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_183.pth
	Model improved!!!
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12791320298142225		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.12791320298142225 | validation: 0.14337332949917694]
	TIME [epoch: 7.77 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13122358745473053		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.13122358745473053 | validation: 0.12214481978132603]
	TIME [epoch: 7.77 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13358595554672534		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.13358595554672534 | validation: 0.20688522846420515]
	TIME [epoch: 7.77 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1838329583620289		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.1838329583620289 | validation: 0.16003071962408505]
	TIME [epoch: 7.77 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.165583091101505		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.165583091101505 | validation: 0.1431733978194788]
	TIME [epoch: 7.81 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11096521294494549		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.11096521294494549 | validation: 0.09611293630571419]
	TIME [epoch: 7.76 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11672249999989667		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.11672249999989667 | validation: 0.10236027459871044]
	TIME [epoch: 7.76 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09432270625701754		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.09432270625701754 | validation: 0.14416047626696799]
	TIME [epoch: 7.76 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17038458987027338		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.17038458987027338 | validation: 0.09802725812653307]
	TIME [epoch: 7.78 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14657405893939662		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.14657405893939662 | validation: 0.15468434087027325]
	TIME [epoch: 7.81 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11041605032956864		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.11041605032956864 | validation: 0.0921276054954871]
	TIME [epoch: 7.77 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1353612383465891		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.1353612383465891 | validation: 0.11060234825556642]
	TIME [epoch: 7.77 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12097655524714628		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.12097655524714628 | validation: 0.08588403437016622]
	TIME [epoch: 7.76 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1047549088999461		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.1047549088999461 | validation: 0.08983665449696264]
	TIME [epoch: 7.77 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10374117746860544		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.10374117746860544 | validation: 0.07488630961891318]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07793247420656409		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.07793247420656409 | validation: 0.09670516639717223]
	TIME [epoch: 7.77 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16323547307958453		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.16323547307958453 | validation: 0.1342644205858556]
	TIME [epoch: 7.76 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09489972431142366		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.09489972431142366 | validation: 0.08311788535690091]
	TIME [epoch: 7.76 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1342413345631502		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.1342413345631502 | validation: 0.09676114709983039]
	TIME [epoch: 7.78 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08850453414687466		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.08850453414687466 | validation: 0.06841517696503066]
	TIME [epoch: 7.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06536833983230549		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.06536833983230549 | validation: 0.15719527389394916]
	TIME [epoch: 7.76 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18423147263822354		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.18423147263822354 | validation: 0.25162074291908415]
	TIME [epoch: 7.76 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1408449149115634		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.1408449149115634 | validation: 0.1059808363657885]
	TIME [epoch: 7.76 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19595129412623985		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.19595129412623985 | validation: 0.13513922961315658]
	TIME [epoch: 7.79 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1535982409915841		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.1535982409915841 | validation: 0.13602290428314773]
	TIME [epoch: 7.79 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1005175121830969		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.1005175121830969 | validation: 0.09619169913196621]
	TIME [epoch: 7.76 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12107710076760013		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.12107710076760013 | validation: 0.10155302934933175]
	TIME [epoch: 7.75 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09287603127348745		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.09287603127348745 | validation: 0.11578984975960058]
	TIME [epoch: 7.76 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08526289608028097		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.08526289608028097 | validation: 0.10911923099460316]
	TIME [epoch: 7.79 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07881070100715196		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.07881070100715196 | validation: 0.0902685804895409]
	TIME [epoch: 7.78 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1039554668433243		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.1039554668433243 | validation: 0.11126106737700314]
	TIME [epoch: 7.75 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10384336234965927		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.10384336234965927 | validation: 0.10014091686012802]
	TIME [epoch: 7.76 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10131098616290247		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.10131098616290247 | validation: 0.06732642080953295]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_216.pth
	Model improved!!!
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09559439650261298		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.09559439650261298 | validation: 0.1059946567420547]
	TIME [epoch: 7.81 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07598248748611547		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.07598248748611547 | validation: 0.09701828393387907]
	TIME [epoch: 7.77 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08358321007688185		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.08358321007688185 | validation: 0.06532360415109087]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11430187410611373		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.11430187410611373 | validation: 0.08037416184414718]
	TIME [epoch: 7.76 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060441892733277866		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.060441892733277866 | validation: 0.0509650533011584]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10027776876141042		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.10027776876141042 | validation: 0.10749706885389149]
	TIME [epoch: 7.82 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08334590495620058		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.08334590495620058 | validation: 0.10375290443344923]
	TIME [epoch: 7.77 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09132608330657449		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.09132608330657449 | validation: 0.10546159358139837]
	TIME [epoch: 7.77 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08147827821956047		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.08147827821956047 | validation: 0.08019552290931997]
	TIME [epoch: 7.77 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059693636332820835		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.059693636332820835 | validation: 0.0479050448816514]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0844446930916618		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.0844446930916618 | validation: 0.052836377429187384]
	TIME [epoch: 7.83 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0664846126871451		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.0664846126871451 | validation: 0.10451912795846614]
	TIME [epoch: 7.76 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0915837866846779		[learning rate: 0.0053088]
	Learning Rate: 0.00530885
	LOSS [training: 0.0915837866846779 | validation: 0.05286566221066137]
	TIME [epoch: 7.76 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06158148143793087		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.06158148143793087 | validation: 0.12872873356244252]
	TIME [epoch: 7.76 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08217937840038031		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.08217937840038031 | validation: 0.04422006727476599]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_231.pth
	Model improved!!!
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07936332633829232		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.07936332633829232 | validation: 0.1282157175887635]
	TIME [epoch: 7.81 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09903863884522779		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.09903863884522779 | validation: 0.05575577049576769]
	TIME [epoch: 7.76 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05853712327953689		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.05853712327953689 | validation: 0.10731012960372267]
	TIME [epoch: 7.76 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10401231890595743		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.10401231890595743 | validation: 0.12054544916246471]
	TIME [epoch: 7.76 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08021341508375034		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.08021341508375034 | validation: 0.07390485138339117]
	TIME [epoch: 7.77 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05217583866542478		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.05217583866542478 | validation: 0.05711401877852584]
	TIME [epoch: 7.81 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05538671099705236		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.05538671099705236 | validation: 0.06778897256434507]
	TIME [epoch: 7.76 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07575013139811156		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.07575013139811156 | validation: 0.08770886254073515]
	TIME [epoch: 7.76 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07791495550079727		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 0.07791495550079727 | validation: 0.07458726869179338]
	TIME [epoch: 7.76 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05807472425562065		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.05807472425562065 | validation: 0.0492247231973128]
	TIME [epoch: 7.77 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058755904522293234		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.058755904522293234 | validation: 0.15567068175485357]
	TIME [epoch: 7.81 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08760944842586918		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.08760944842586918 | validation: 0.09275795634814127]
	TIME [epoch: 7.76 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09572645677299144		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.09572645677299144 | validation: 0.09392443905872758]
	TIME [epoch: 7.76 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06822676119822658		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.06822676119822658 | validation: 0.12439569708085187]
	TIME [epoch: 7.76 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09021976769344807		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.09021976769344807 | validation: 0.08184759664106118]
	TIME [epoch: 7.77 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06335951698987546		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.06335951698987546 | validation: 0.07350973204891592]
	TIME [epoch: 7.81 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06569598133203777		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.06569598133203777 | validation: 0.06622576350884468]
	TIME [epoch: 7.76 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05863104441535166		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.05863104441535166 | validation: 0.08343656299290722]
	TIME [epoch: 7.76 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0682520056947998		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 0.0682520056947998 | validation: 0.03758394120795525]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_250.pth
	Model improved!!!
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03702080641531116		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.03702080641531116 | validation: 0.04245961976087584]
	TIME [epoch: 7.78 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.049290808198573864		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.049290808198573864 | validation: 0.07084256849575035]
	TIME [epoch: 7.8 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05821740271697009		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.05821740271697009 | validation: 0.04923108315507657]
	TIME [epoch: 7.77 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0500077334789337		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.0500077334789337 | validation: 0.0660507249475298]
	TIME [epoch: 7.76 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06526750403757443		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.06526750403757443 | validation: 0.05213652107960404]
	TIME [epoch: 7.76 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04286774531877748		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.04286774531877748 | validation: 0.26065003332465186]
	TIME [epoch: 7.79 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19699484833578612		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.19699484833578612 | validation: 0.09114517363190035]
	TIME [epoch: 7.79 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06722798191294004		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.06722798191294004 | validation: 0.032639350442761124]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07166011008550952		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.07166011008550952 | validation: 0.07330663127291934]
	TIME [epoch: 7.76 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04247080997308401		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.04247080997308401 | validation: 0.03953090541276343]
	TIME [epoch: 7.76 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08795983546577302		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.08795983546577302 | validation: 0.08848859842292495]
	TIME [epoch: 7.79 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047609903034183		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.047609903034183 | validation: 0.03775683631256827]
	TIME [epoch: 7.79 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03228289379061295		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.03228289379061295 | validation: 0.08988781106297061]
	TIME [epoch: 7.76 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059640589359441845		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.059640589359441845 | validation: 0.049174109932764304]
	TIME [epoch: 7.77 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07043302082617645		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.07043302082617645 | validation: 0.06488988677199366]
	TIME [epoch: 7.77 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04024430235110533		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.04024430235110533 | validation: 0.03141070714955278]
	TIME [epoch: 7.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_266.pth
	Model improved!!!
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027983232512486356		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.027983232512486356 | validation: 0.03116622181497727]
	TIME [epoch: 7.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03630334303634359		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.03630334303634359 | validation: 0.06399764848626345]
	TIME [epoch: 7.77 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08599211647421727		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.08599211647421727 | validation: 0.034459357108725275]
	TIME [epoch: 7.77 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042355725636671046		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.042355725636671046 | validation: 0.04848378451665002]
	TIME [epoch: 7.76 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04648447101495244		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.04648447101495244 | validation: 0.04881696497184648]
	TIME [epoch: 7.82 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034081508837205864		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.034081508837205864 | validation: 0.045426648705311025]
	TIME [epoch: 7.77 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04568865813606174		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.04568865813606174 | validation: 0.08654660537103727]
	TIME [epoch: 7.77 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05727855719481888		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.05727855719481888 | validation: 0.0300670351843532]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_274.pth
	Model improved!!!
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04349902816674258		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.04349902816674258 | validation: 0.08824840197862859]
	TIME [epoch: 7.77 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046994287928241694		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.046994287928241694 | validation: 0.04686500506198261]
	TIME [epoch: 7.82 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05029035701968259		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.05029035701968259 | validation: 0.04308537658810753]
	TIME [epoch: 7.76 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041858223338192245		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.041858223338192245 | validation: 0.05840927647086846]
	TIME [epoch: 7.77 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05138703754603926		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.05138703754603926 | validation: 0.03128763333708612]
	TIME [epoch: 7.77 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02991408313075741		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.02991408313075741 | validation: 0.03135788254080299]
	TIME [epoch: 7.77 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07455009921016097		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.07455009921016097 | validation: 0.0685121715892181]
	TIME [epoch: 7.82 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03861239124413552		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.03861239124413552 | validation: 0.026397218299777862]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_282.pth
	Model improved!!!
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023967537773889276		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.023967537773889276 | validation: 0.029892649361014236]
	TIME [epoch: 7.77 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0671463298261811		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.0671463298261811 | validation: 0.06462540469121664]
	TIME [epoch: 7.77 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07165048993769066		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.07165048993769066 | validation: 0.02456516576820607]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_285.pth
	Model improved!!!
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026183961808623167		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.026183961808623167 | validation: 0.028152815906610205]
	TIME [epoch: 7.81 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03489130796941228		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.03489130796941228 | validation: 0.07022995135060985]
	TIME [epoch: 7.76 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.049357167315137764		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.049357167315137764 | validation: 0.04363967822832088]
	TIME [epoch: 7.76 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03004524991163307		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.03004524991163307 | validation: 0.04963315567030953]
	TIME [epoch: 7.76 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0437009797324392		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.0437009797324392 | validation: 0.03929857048021537]
	TIME [epoch: 7.77 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041244737728615824		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.041244737728615824 | validation: 0.026980649655260217]
	TIME [epoch: 7.81 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02454171500963525		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.02454171500963525 | validation: 0.03207355568976052]
	TIME [epoch: 7.76 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06931056304767116		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.06931056304767116 | validation: 0.05258221419551891]
	TIME [epoch: 7.76 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03573406343922349		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.03573406343922349 | validation: 0.0338450546365009]
	TIME [epoch: 7.76 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04572048494498501		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.04572048494498501 | validation: 0.07297127469372197]
	TIME [epoch: 7.78 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042470129424759036		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.042470129424759036 | validation: 0.024754886780084626]
	TIME [epoch: 7.8 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034488833737958656		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.034488833737958656 | validation: 0.0317731834862262]
	TIME [epoch: 7.76 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02418056351080262		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.02418056351080262 | validation: 0.05299340001396538]
	TIME [epoch: 7.76 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05166605927068927		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.05166605927068927 | validation: 0.036524290268845266]
	TIME [epoch: 7.76 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02495897411476766		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.02495897411476766 | validation: 0.030973328944666555]
	TIME [epoch: 7.78 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05659004505812304		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.05659004505812304 | validation: 0.043160319837546264]
	TIME [epoch: 7.79 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03516346448752211		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.03516346448752211 | validation: 0.023894561012143087]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_302.pth
	Model improved!!!
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020178804822716088		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.020178804822716088 | validation: 0.023998797621795673]
	TIME [epoch: 7.76 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036453484123392436		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.036453484123392436 | validation: 0.10842875503720409]
	TIME [epoch: 7.76 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07376006607648364		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.07376006607648364 | validation: 0.05064386233158994]
	TIME [epoch: 7.79 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03067995883578658		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.03067995883578658 | validation: 0.021327617062426113]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_306.pth
	Model improved!!!
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01967882465816523		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.01967882465816523 | validation: 0.024008257573437365]
	TIME [epoch: 7.76 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024814145238513156		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.024814145238513156 | validation: 0.062130723417851655]
	TIME [epoch: 7.76 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.050144456921640455		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.050144456921640455 | validation: 0.03683728130086965]
	TIME [epoch: 7.76 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026547931222621328		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.026547931222621328 | validation: 0.06166588183989958]
	TIME [epoch: 7.81 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040465658113796585		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.040465658113796585 | validation: 0.03765662237623267]
	TIME [epoch: 7.77 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0330081698300272		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.0330081698300272 | validation: 0.0759788898513017]
	TIME [epoch: 7.76 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04642125989463719		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.04642125989463719 | validation: 0.02454514957310149]
	TIME [epoch: 7.77 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02896245580389761		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.02896245580389761 | validation: 0.027889551167826182]
	TIME [epoch: 7.76 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031190305674565405		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.031190305674565405 | validation: 0.04183753592068519]
	TIME [epoch: 7.81 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04214441268328367		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.04214441268328367 | validation: 0.02865101277081071]
	TIME [epoch: 7.77 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022992836704040256		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.022992836704040256 | validation: 0.021056270142854015]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_317.pth
	Model improved!!!
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029494375038647026		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.029494375038647026 | validation: 0.03200913860351715]
	TIME [epoch: 7.77 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04723424520086543		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.04723424520086543 | validation: 0.019810987456653044]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_319.pth
	Model improved!!!
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03142224228539259		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.03142224228539259 | validation: 0.06202414114936579]
	TIME [epoch: 7.82 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04001968396794692		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.04001968396794692 | validation: 0.023455139808788616]
	TIME [epoch: 7.78 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03666772633649428		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.03666772633649428 | validation: 0.02432374242306503]
	TIME [epoch: 7.77 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03267286197310833		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.03267286197310833 | validation: 0.0415794879807543]
	TIME [epoch: 7.77 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03404744815374043		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.03404744815374043 | validation: 0.03429748184744352]
	TIME [epoch: 7.77 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024553775265130643		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.024553775265130643 | validation: 0.022067944748683656]
	TIME [epoch: 7.82 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020353788449027986		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.020353788449027986 | validation: 0.02611497854760693]
	TIME [epoch: 7.77 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047459572284709775		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.047459572284709775 | validation: 0.037427433768408985]
	TIME [epoch: 7.77 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03051136341978552		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.03051136341978552 | validation: 0.021661588556732628]
	TIME [epoch: 7.77 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021247424683859033		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.021247424683859033 | validation: 0.033337186929052284]
	TIME [epoch: 7.77 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03826771610308131		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.03826771610308131 | validation: 0.019635015716023455]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_330.pth
	Model improved!!!
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02707305619703784		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.02707305619703784 | validation: 0.02945460346596975]
	TIME [epoch: 7.78 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034862598469159324		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.034862598469159324 | validation: 0.033925692791375875]
	TIME [epoch: 7.78 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025611520748033807		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.025611520748033807 | validation: 0.026531617786025634]
	TIME [epoch: 7.77 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03837129676230768		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.03837129676230768 | validation: 0.05944333802489132]
	TIME [epoch: 7.78 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03275769220038618		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.03275769220038618 | validation: 0.020552639906544184]
	TIME [epoch: 7.82 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023025086068041573		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.023025086068041573 | validation: 0.030487338107065127]
	TIME [epoch: 7.78 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022525907621605563		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.022525907621605563 | validation: 0.043329327849056715]
	TIME [epoch: 7.78 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04350340331783439		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.04350340331783439 | validation: 0.029537400705783715]
	TIME [epoch: 7.77 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027228225439159058		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.027228225439159058 | validation: 0.021972049964908962]
	TIME [epoch: 7.79 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019259961182746102		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.019259961182746102 | validation: 0.046544430260920984]
	TIME [epoch: 7.82 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04139244221368163		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.04139244221368163 | validation: 0.03212137689543343]
	TIME [epoch: 7.77 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019482160398792893		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.019482160398792893 | validation: 0.030891562145601993]
	TIME [epoch: 7.77 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03444627590929751		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.03444627590929751 | validation: 0.034791908663504845]
	TIME [epoch: 7.78 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02635075353940903		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.02635075353940903 | validation: 0.017813361311683932]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_344.pth
	Model improved!!!
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015166512097730237		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.015166512097730237 | validation: 0.032112661345480874]
	TIME [epoch: 7.82 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046832005568117285		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.046832005568117285 | validation: 0.03511607499698509]
	TIME [epoch: 7.77 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02651359075528587		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.02651359075528587 | validation: 0.02240220377143213]
	TIME [epoch: 7.77 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025506393000444477		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.025506393000444477 | validation: 0.02689465076390236]
	TIME [epoch: 7.77 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02036127054437492		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.02036127054437492 | validation: 0.021145985457700994]
	TIME [epoch: 7.79 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027544655924818574		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.027544655924818574 | validation: 0.05444486868009496]
	TIME [epoch: 7.81 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03806308952461847		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.03806308952461847 | validation: 0.023130170628499837]
	TIME [epoch: 7.77 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01868199498383065		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.01868199498383065 | validation: 0.03317795048320914]
	TIME [epoch: 7.77 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03862618095314006		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.03862618095314006 | validation: 0.030453300711099196]
	TIME [epoch: 7.77 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020025666638869778		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.020025666638869778 | validation: 0.020139242007116816]
	TIME [epoch: 7.81 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029916239900996754		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.029916239900996754 | validation: 0.02046719806111197]
	TIME [epoch: 7.8 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03118213780431573		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.03118213780431573 | validation: 0.02498711402929358]
	TIME [epoch: 7.77 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017140613422554445		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.017140613422554445 | validation: 0.019164857141943263]
	TIME [epoch: 7.77 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029627222505632718		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.029627222505632718 | validation: 0.06626361154514387]
	TIME [epoch: 7.77 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03495426429133971		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.03495426429133971 | validation: 0.04564728805855202]
	TIME [epoch: 7.8 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036635055187268714		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.036635055187268714 | validation: 0.019303708223724656]
	TIME [epoch: 7.8 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02138037962387546		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.02138037962387546 | validation: 0.026439763376864732]
	TIME [epoch: 7.77 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029231697566957815		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.029231697566957815 | validation: 0.03398736756940284]
	TIME [epoch: 7.77 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027335320300655403		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.027335320300655403 | validation: 0.017022694952976685]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_363.pth
	Model improved!!!
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01776740146437327		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.01776740146437327 | validation: 0.016759947180959106]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_364.pth
	Model improved!!!
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024487552959213888		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.024487552959213888 | validation: 0.02761226356616779]
	TIME [epoch: 7.79 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038021373615916565		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.038021373615916565 | validation: 0.04116970567030694]
	TIME [epoch: 7.77 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023080893765428753		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.023080893765428753 | validation: 0.025404105265944724]
	TIME [epoch: 7.77 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023707002453328567		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.023707002453328567 | validation: 0.02095601700452599]
	TIME [epoch: 7.77 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022396667122783445		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.022396667122783445 | validation: 0.028023586024541497]
	TIME [epoch: 7.81 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022773908045058897		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.022773908045058897 | validation: 0.01700339761095119]
	TIME [epoch: 7.78 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028328536503310603		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.028328536503310603 | validation: 0.04728920793272999]
	TIME [epoch: 7.77 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0357917448167515		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.0357917448167515 | validation: 0.020386366795726897]
	TIME [epoch: 7.77 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025121929513457996		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.025121929513457996 | validation: 0.027835008706756436]
	TIME [epoch: 7.77 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02324909262581256		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.02324909262581256 | validation: 0.01680656234053541]
	TIME [epoch: 7.82 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020050316752947377		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.020050316752947377 | validation: 0.03395530601346819]
	TIME [epoch: 7.77 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025878495067079586		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.025878495067079586 | validation: 0.03063443895992035]
	TIME [epoch: 7.77 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021843851679427012		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.021843851679427012 | validation: 0.018874543165270978]
	TIME [epoch: 7.77 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02335568626610071		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.02335568626610071 | validation: 0.04357992007835097]
	TIME [epoch: 7.77 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03526245227580529		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.03526245227580529 | validation: 0.017081697447405134]
	TIME [epoch: 7.82 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017616798881408432		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.017616798881408432 | validation: 0.023042594144922744]
	TIME [epoch: 7.77 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025499545384501963		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.025499545384501963 | validation: 0.025461088702114484]
	TIME [epoch: 7.77 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02846825180587247		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.02846825180587247 | validation: 0.022932006982519057]
	TIME [epoch: 7.77 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022596237188202088		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.022596237188202088 | validation: 0.016809372447152394]
	TIME [epoch: 7.77 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017913011500885832		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.017913011500885832 | validation: 0.033381039693529534]
	TIME [epoch: 7.81 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030388337099558377		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.030388337099558377 | validation: 0.03641586605063561]
	TIME [epoch: 7.77 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027087177162242355		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.027087177162242355 | validation: 0.01974166137064247]
	TIME [epoch: 7.77 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016136580918501273		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.016136580918501273 | validation: 0.023794097688600403]
	TIME [epoch: 7.77 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01708767729926574		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.01708767729926574 | validation: 0.017223970329510573]
	TIME [epoch: 7.77 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032971982987778675		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.032971982987778675 | validation: 0.03320650431858924]
	TIME [epoch: 7.81 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018880538699141507		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.018880538699141507 | validation: 0.018966686708158563]
	TIME [epoch: 7.77 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02280040519197251		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.02280040519197251 | validation: 0.02155744060167096]
	TIME [epoch: 7.77 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018889180741977443		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.018889180741977443 | validation: 0.015397385281029581]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_392.pth
	Model improved!!!
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019834371165294042		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.019834371165294042 | validation: 0.025747277112961264]
	TIME [epoch: 7.77 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019965087345224848		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.019965087345224848 | validation: 0.027703675006810306]
	TIME [epoch: 7.82 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027218815363776048		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.027218815363776048 | validation: 0.07078571690132923]
	TIME [epoch: 7.76 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034199951426729866		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.034199951426729866 | validation: 0.017403058121268036]
	TIME [epoch: 7.76 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018664685779481098		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.018664685779481098 | validation: 0.03578430366086421]
	TIME [epoch: 7.76 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024560197221577457		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.024560197221577457 | validation: 0.029479870196178246]
	TIME [epoch: 7.77 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03790970326090817		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.03790970326090817 | validation: 0.02552180553176241]
	TIME [epoch: 7.8 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021619648862464886		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.021619648862464886 | validation: 0.018221310860213766]
	TIME [epoch: 7.76 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017641801520531145		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.017641801520531145 | validation: 0.024420062192122838]
	TIME [epoch: 7.76 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024719558993749034		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.024719558993749034 | validation: 0.02006285841460214]
	TIME [epoch: 7.76 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019261076077541045		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.019261076077541045 | validation: 0.02294783674237927]
	TIME [epoch: 7.77 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020946787325603518		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.020946787325603518 | validation: 0.028100874197278025]
	TIME [epoch: 7.81 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021704729956392388		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.021704729956392388 | validation: 0.02122114295227004]
	TIME [epoch: 7.76 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018332136489322227		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.018332136489322227 | validation: 0.019241943641925816]
	TIME [epoch: 7.76 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015360287027719934		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.015360287027719934 | validation: 0.02943462613128489]
	TIME [epoch: 7.76 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035398713589797		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.035398713589797 | validation: 0.027180543904384086]
	TIME [epoch: 7.77 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020230608671238128		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.020230608671238128 | validation: 0.01859086375618228]
	TIME [epoch: 7.81 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016582884766032327		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.016582884766032327 | validation: 0.015761320162099746]
	TIME [epoch: 7.76 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019715781594901574		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.019715781594901574 | validation: 0.02785855186138258]
	TIME [epoch: 7.76 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029148859220089585		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.029148859220089585 | validation: 0.04006940041213791]
	TIME [epoch: 7.76 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024803202010052393		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.024803202010052393 | validation: 0.021590173291851433]
	TIME [epoch: 7.78 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02317128050002641		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.02317128050002641 | validation: 0.0207808011260341]
	TIME [epoch: 7.8 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01736332478277727		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.01736332478277727 | validation: 0.01474811814190732]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_415.pth
	Model improved!!!
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014562585348662407		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.014562585348662407 | validation: 0.018862814773374197]
	TIME [epoch: 7.77 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019188063970383494		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.019188063970383494 | validation: 0.04259021938457195]
	TIME [epoch: 7.76 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04850027687191254		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.04850027687191254 | validation: 0.021442882637869864]
	TIME [epoch: 7.79 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019021816750391093		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.019021816750391093 | validation: 0.016476925749938667]
	TIME [epoch: 7.79 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019851232524335922		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.019851232524335922 | validation: 0.020677692566678597]
	TIME [epoch: 7.76 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0174832469828452		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.0174832469828452 | validation: 0.022279361294808682]
	TIME [epoch: 7.76 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018143973240533618		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.018143973240533618 | validation: 0.014780744599835385]
	TIME [epoch: 7.76 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023334648129985522		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.023334648129985522 | validation: 0.027572853472815746]
	TIME [epoch: 7.79 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019581568380131413		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.019581568380131413 | validation: 0.0161980406278502]
	TIME [epoch: 7.79 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024302196114151544		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.024302196114151544 | validation: 0.03038622034660063]
	TIME [epoch: 7.76 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020201817158623903		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.020201817158623903 | validation: 0.017009248287637432]
	TIME [epoch: 7.76 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015315536835135438		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.015315536835135438 | validation: 0.02092418468688411]
	TIME [epoch: 7.76 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0209559286935355		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.0209559286935355 | validation: 0.02783623098623779]
	TIME [epoch: 7.78 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024008131753494196		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.024008131753494196 | validation: 0.0201925046124521]
	TIME [epoch: 7.79 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025173482860806187		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.025173482860806187 | validation: 0.03501595772159977]
	TIME [epoch: 7.76 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022295461342587274		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.022295461342587274 | validation: 0.015527411099703382]
	TIME [epoch: 7.77 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0162296276690338		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.0162296276690338 | validation: 0.014747937237598367]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_432.pth
	Model improved!!!
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018898710395478194		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.018898710395478194 | validation: 0.03780675101048929]
	TIME [epoch: 7.8 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03145272522021907		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.03145272522021907 | validation: 0.017636722710455896]
	TIME [epoch: 7.78 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015353855838699207		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.015353855838699207 | validation: 0.027992083187966633]
	TIME [epoch: 7.77 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022833467134909496		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.022833467134909496 | validation: 0.018269715828445752]
	TIME [epoch: 7.75 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016617496124988033		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.016617496124988033 | validation: 0.019835174069436932]
	TIME [epoch: 7.77 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01933272697270359		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.01933272697270359 | validation: 0.01611941745587677]
	TIME [epoch: 7.8 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0214598308953452		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.0214598308953452 | validation: 0.0247398911402626]
	TIME [epoch: 7.79 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020445186364560337		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.020445186364560337 | validation: 0.017725821248444205]
	TIME [epoch: 7.76 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01565202526891428		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.01565202526891428 | validation: 0.01878445209748185]
	TIME [epoch: 7.77 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019188476244531454		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.019188476244531454 | validation: 0.01622641936596008]
	TIME [epoch: 7.76 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016474350075339143		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.016474350075339143 | validation: 0.02563234316996233]
	TIME [epoch: 7.81 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024085346523058767		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.024085346523058767 | validation: 0.03236848974851386]
	TIME [epoch: 7.77 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018223686238818478		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.018223686238818478 | validation: 0.017133318648742894]
	TIME [epoch: 7.76 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015601656999141188		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.015601656999141188 | validation: 0.018875129291780597]
	TIME [epoch: 7.76 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021918844826727617		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.021918844826727617 | validation: 0.02064605101190787]
	TIME [epoch: 7.76 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01547122346487468		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.01547122346487468 | validation: 0.015656118691789535]
	TIME [epoch: 7.81 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01723485410630308		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.01723485410630308 | validation: 0.02705298320598077]
	TIME [epoch: 7.78 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018632282726087128		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.018632282726087128 | validation: 0.037335305388413495]
	TIME [epoch: 7.76 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025131551987636017		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.025131551987636017 | validation: 0.022362368426412906]
	TIME [epoch: 7.76 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01940597682016376		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.01940597682016376 | validation: 0.016685725362071063]
	TIME [epoch: 7.75 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01918988955909566		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.01918988955909566 | validation: 0.013439490457045174]
	TIME [epoch: 7.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_453.pth
	Model improved!!!
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013238492077170098		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.013238492077170098 | validation: 0.012905121004619527]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_454.pth
	Model improved!!!
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019028381262395068		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.019028381262395068 | validation: 0.027319459125032978]
	TIME [epoch: 7.76 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024820184828118687		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.024820184828118687 | validation: 0.024158969263805518]
	TIME [epoch: 7.76 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016111147372199677		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.016111147372199677 | validation: 0.03278425356198096]
	TIME [epoch: 7.77 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021653653651669637		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.021653653651669637 | validation: 0.028234173942653068]
	TIME [epoch: 7.81 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023839181674647082		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.023839181674647082 | validation: 0.01799711368064769]
	TIME [epoch: 7.76 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01715276264339353		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.01715276264339353 | validation: 0.020376753174806618]
	TIME [epoch: 7.77 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017018331803647316		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.017018331803647316 | validation: 0.016421123679910457]
	TIME [epoch: 7.76 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0228352816016642		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.0228352816016642 | validation: 0.018523562834457198]
	TIME [epoch: 7.77 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01457502958541344		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.01457502958541344 | validation: 0.01997934610409622]
	TIME [epoch: 7.81 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013871700650520475		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.013871700650520475 | validation: 0.015408318748752411]
	TIME [epoch: 7.76 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017111406140955892		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.017111406140955892 | validation: 0.03127999582741662]
	TIME [epoch: 7.76 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022167462930454896		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.022167462930454896 | validation: 0.015042112622742711]
	TIME [epoch: 7.76 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016074937325582912		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.016074937325582912 | validation: 0.014123461116563894]
	TIME [epoch: 7.77 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01697629103194064		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.01697629103194064 | validation: 0.019381139109658573]
	TIME [epoch: 7.81 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016345107545690435		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.016345107545690435 | validation: 0.017366897127709814]
	TIME [epoch: 7.76 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016549662371797672		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.016549662371797672 | validation: 0.019152972692229975]
	TIME [epoch: 7.76 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018044253043363538		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.018044253043363538 | validation: 0.0270399899490969]
	TIME [epoch: 7.76 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021531976923483694		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.021531976923483694 | validation: 0.01634340262805358]
	TIME [epoch: 7.77 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015101319381326937		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.015101319381326937 | validation: 0.017659526935238222]
	TIME [epoch: 7.81 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018042570770648598		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.018042570770648598 | validation: 0.023919551505791177]
	TIME [epoch: 7.77 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0163979403687737		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.0163979403687737 | validation: 0.019069105004032867]
	TIME [epoch: 7.77 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02092259799091782		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.02092259799091782 | validation: 0.01804802217976721]
	TIME [epoch: 7.76 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018333972674056745		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.018333972674056745 | validation: 0.021289892505429522]
	TIME [epoch: 7.77 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016201082416350175		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.016201082416350175 | validation: 0.020456355412223612]
	TIME [epoch: 7.81 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015472232381082097		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.015472232381082097 | validation: 0.014350836580804091]
	TIME [epoch: 7.76 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018270623730426604		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.018270623730426604 | validation: 0.03295149865992062]
	TIME [epoch: 7.77 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021382151049886798		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.021382151049886798 | validation: 0.0198995347783872]
	TIME [epoch: 7.76 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03402058745973016		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.03402058745973016 | validation: 0.17439034918970145]
	TIME [epoch: 7.77 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08379253247825547		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.08379253247825547 | validation: 0.07700599222998177]
	TIME [epoch: 7.81 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030521269362506175		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.030521269362506175 | validation: 0.028458573979300406]
	TIME [epoch: 7.77 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016946034712824614		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.016946034712824614 | validation: 0.01569930062793399]
	TIME [epoch: 7.76 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014683770968109886		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.014683770968109886 | validation: 0.01740847455714675]
	TIME [epoch: 7.76 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014040193099430682		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.014040193099430682 | validation: 0.014667235431734897]
	TIME [epoch: 7.77 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02203949657987995		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.02203949657987995 | validation: 0.022468681560230352]
	TIME [epoch: 7.8 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021107598729480037		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.021107598729480037 | validation: 0.01505620774019304]
	TIME [epoch: 7.76 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014373613052034231		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.014373613052034231 | validation: 0.013781688441214857]
	TIME [epoch: 7.76 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01424142532133767		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.01424142532133767 | validation: 0.02164938470878957]
	TIME [epoch: 7.76 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01575078559822515		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.01575078559822515 | validation: 0.013966815654141419]
	TIME [epoch: 7.77 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012527563083800547		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.012527563083800547 | validation: 0.014814524153646803]
	TIME [epoch: 7.81 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014658429982674465		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.014658429982674465 | validation: 0.019285117869384187]
	TIME [epoch: 7.76 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017956086597287237		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.017956086597287237 | validation: 0.013962136361806727]
	TIME [epoch: 7.76 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015525805810659439		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.015525805810659439 | validation: 0.018578347251722666]
	TIME [epoch: 7.76 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024868294393211247		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.024868294393211247 | validation: 0.015659063229740346]
	TIME [epoch: 7.78 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019814451805237752		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.019814451805237752 | validation: 0.014619962542012348]
	TIME [epoch: 7.8 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013846128257109888		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.013846128257109888 | validation: 0.013317381220150493]
	TIME [epoch: 7.77 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015891992859305766		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.015891992859305766 | validation: 0.026814664677689164]
	TIME [epoch: 7.76 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016972027420770246		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.016972027420770246 | validation: 0.014713991031569226]
	TIME [epoch: 7.76 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013473256547803713		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.013473256547803713 | validation: 0.015660571497224815]
	TIME [epoch: 7.79 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014821670441957133		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.014821670441957133 | validation: 0.0263748925662355]
	TIME [epoch: 7.79 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022181177703922514		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.022181177703922514 | validation: 0.0126412331897938]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_504.pth
	Model improved!!!
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013917861746652778		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.013917861746652778 | validation: 0.013667699220314317]
	TIME [epoch: 7.77 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014385173776677869		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.014385173776677869 | validation: 0.015077447113837891]
	TIME [epoch: 7.76 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022128171749192807		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.022128171749192807 | validation: 0.014057022336937208]
	TIME [epoch: 7.8 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013547026137123865		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.013547026137123865 | validation: 0.015966650930127776]
	TIME [epoch: 7.79 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016889501851579516		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.016889501851579516 | validation: 0.013302956243532088]
	TIME [epoch: 7.76 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02083297851741355		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.02083297851741355 | validation: 0.01481591293094297]
	TIME [epoch: 7.76 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018591414130748763		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.018591414130748763 | validation: 0.018317363753789784]
	TIME [epoch: 7.76 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015660172441234223		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.015660172441234223 | validation: 0.015959285854911392]
	TIME [epoch: 7.79 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014903463338623083		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.014903463338623083 | validation: 0.01410072129366599]
	TIME [epoch: 7.8 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015785704269660885		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.015785704269660885 | validation: 0.01891691362408325]
	TIME [epoch: 7.76 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021356056166315383		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.021356056166315383 | validation: 0.013740279854699151]
	TIME [epoch: 7.76 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015660009625162348		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.015660009625162348 | validation: 0.01698871298159014]
	TIME [epoch: 7.76 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013808196375973407		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.013808196375973407 | validation: 0.01853857257696792]
	TIME [epoch: 7.81 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019275330830590822		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.019275330830590822 | validation: 0.013702938646714503]
	TIME [epoch: 7.78 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013855671840123623		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.013855671840123623 | validation: 0.019292517553359097]
	TIME [epoch: 7.77 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02082604060604592		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.02082604060604592 | validation: 0.015512989759226013]
	TIME [epoch: 7.76 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01604606358221768		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.01604606358221768 | validation: 0.014099406692422597]
	TIME [epoch: 7.76 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025570898926937476		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.025570898926937476 | validation: 0.02507777710403291]
	TIME [epoch: 7.81 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01906251148349345		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.01906251148349345 | validation: 0.01354641794738809]
	TIME [epoch: 7.78 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014021968599486985		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.014021968599486985 | validation: 0.015106857589687134]
	TIME [epoch: 7.76 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014962866184099623		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.014962866184099623 | validation: 0.01898514666620675]
	TIME [epoch: 7.76 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01610373600708986		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.01610373600708986 | validation: 0.017497282909671428]
	TIME [epoch: 7.76 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01502058262322029		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.01502058262322029 | validation: 0.014825289775232404]
	TIME [epoch: 7.81 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01633145657319744		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.01633145657319744 | validation: 0.015926906268800615]
	TIME [epoch: 7.77 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021457732800097617		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.021457732800097617 | validation: 0.02543701155531897]
	TIME [epoch: 7.77 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016982200775082567		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.016982200775082567 | validation: 0.012683192764765025]
	TIME [epoch: 7.76 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013960596302310831		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.013960596302310831 | validation: 0.015907375991994727]
	TIME [epoch: 7.76 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014744054485240764		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.014744054485240764 | validation: 0.01816588191143423]
	TIME [epoch: 7.82 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01653880308956835		[learning rate: 0.0018085]
	Learning Rate: 0.00180845
	LOSS [training: 0.01653880308956835 | validation: 0.014156681870593246]
	TIME [epoch: 7.77 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014596203816750185		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.014596203816750185 | validation: 0.023088810126316466]
	TIME [epoch: 7.76 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017705910441848563		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.017705910441848563 | validation: 0.016570583103033512]
	TIME [epoch: 7.76 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016489444705509747		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.016489444705509747 | validation: 0.020262448516421366]
	TIME [epoch: 7.76 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014465279602389088		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.014465279602389088 | validation: 0.01612995667745043]
	TIME [epoch: 7.82 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015494695186520496		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.015494695186520496 | validation: 0.012475710769917537]
	TIME [epoch: 7.76 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_538.pth
	Model improved!!!
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015499302073683476		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.015499302073683476 | validation: 0.0215919262798991]
	TIME [epoch: 7.77 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016923920357922742		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.016923920357922742 | validation: 0.012920360289386583]
	TIME [epoch: 7.77 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013544207106905716		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.013544207106905716 | validation: 0.032518531475395865]
	TIME [epoch: 7.77 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02288061572027286		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.02288061572027286 | validation: 0.01423688446624622]
	TIME [epoch: 7.81 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0151672444719416		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.0151672444719416 | validation: 0.014784485101708636]
	TIME [epoch: 7.76 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013290790961586967		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.013290790961586967 | validation: 0.017297676410414704]
	TIME [epoch: 7.76 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014167267157128575		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.014167267157128575 | validation: 0.018114256555003764]
	TIME [epoch: 7.76 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017211209690566144		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.017211209690566144 | validation: 0.017277147162568972]
	TIME [epoch: 7.78 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015759060714930998		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.015759060714930998 | validation: 0.018712898671338987]
	TIME [epoch: 7.82 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016219426426219534		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.016219426426219534 | validation: 0.01871628106064909]
	TIME [epoch: 7.78 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01567181295996767		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.01567181295996767 | validation: 0.01895857033006268]
	TIME [epoch: 7.76 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017297300442719113		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.017297300442719113 | validation: 0.016344404252765957]
	TIME [epoch: 7.77 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01374992270483454		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.01374992270483454 | validation: 0.014996124561540298]
	TIME [epoch: 7.76 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014970134441401621		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.014970134441401621 | validation: 0.012822791079021532]
	TIME [epoch: 7.82 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012665535596448795		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.012665535596448795 | validation: 0.018945614035656685]
	TIME [epoch: 7.76 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016740108386110474		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.016740108386110474 | validation: 0.024609338924570033]
	TIME [epoch: 7.76 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017110530252331918		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.017110530252331918 | validation: 0.012940317951175654]
	TIME [epoch: 7.77 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02343399071428088		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.02343399071428088 | validation: 0.019033299441885202]
	TIME [epoch: 7.78 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0156712919613488		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.0156712919613488 | validation: 0.013800219330959072]
	TIME [epoch: 7.81 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02000589072065295		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.02000589072065295 | validation: 0.01682080492184742]
	TIME [epoch: 7.77 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019547543720586195		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.019547543720586195 | validation: 0.024269920212340187]
	TIME [epoch: 7.77 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016334304170564726		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.016334304170564726 | validation: 0.013571181309051675]
	TIME [epoch: 7.77 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014291701956423254		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.014291701956423254 | validation: 0.016815719757099]
	TIME [epoch: 7.78 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015735456761708552		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.015735456761708552 | validation: 0.014298394824050506]
	TIME [epoch: 7.8 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014612205046129229		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.014612205046129229 | validation: 0.013882074374255664]
	TIME [epoch: 7.76 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012620555000137701		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.012620555000137701 | validation: 0.0138046509292151]
	TIME [epoch: 7.77 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016492335253818633		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.016492335253818633 | validation: 0.017916737994737135]
	TIME [epoch: 7.77 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016319949542794306		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.016319949542794306 | validation: 0.0156593763984275]
	TIME [epoch: 7.78 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014017807961754147		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.014017807961754147 | validation: 0.014741621434316683]
	TIME [epoch: 7.8 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013816716154369406		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.013816716154369406 | validation: 0.016167307793891136]
	TIME [epoch: 7.76 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02162878360189762		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.02162878360189762 | validation: 0.013574482045025623]
	TIME [epoch: 7.77 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014684105105982429		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.014684105105982429 | validation: 0.017968758800796895]
	TIME [epoch: 7.76 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01508904998183289		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.01508904998183289 | validation: 0.01223045668458168]
	TIME [epoch: 7.78 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_571.pth
	Model improved!!!
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012986152440987376		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.012986152440987376 | validation: 0.014570737221629123]
	TIME [epoch: 7.81 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0171077722725622		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.0171077722725622 | validation: 0.019337647079685116]
	TIME [epoch: 7.77 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015868808116478977		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.015868808116478977 | validation: 0.021204726917984435]
	TIME [epoch: 7.77 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01319880856576844		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.01319880856576844 | validation: 0.012452221854725741]
	TIME [epoch: 7.77 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014659250131054194		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.014659250131054194 | validation: 0.01880758541844435]
	TIME [epoch: 7.79 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01707368883955797		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.01707368883955797 | validation: 0.017453132463983043]
	TIME [epoch: 7.81 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014787491275806183		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.014787491275806183 | validation: 0.015846226467934328]
	TIME [epoch: 7.77 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012777822071547828		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.012777822071547828 | validation: 0.014363231682056828]
	TIME [epoch: 7.76 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012564999034074997		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.012564999034074997 | validation: 0.014566991388340753]
	TIME [epoch: 7.76 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016756837043115112		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.016756837043115112 | validation: 0.01699062899178061]
	TIME [epoch: 7.79 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013525362489055155		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.013525362489055155 | validation: 0.013706349002092812]
	TIME [epoch: 7.79 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013647381663533138		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.013647381663533138 | validation: 0.013365149985232735]
	TIME [epoch: 7.77 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01656634250093799		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.01656634250093799 | validation: 0.017961374593743917]
	TIME [epoch: 7.76 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014523925772978835		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.014523925772978835 | validation: 0.013736416396195383]
	TIME [epoch: 7.77 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012670526341037077		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.012670526341037077 | validation: 0.018038835348930624]
	TIME [epoch: 7.8 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013313323483349512		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.013313323483349512 | validation: 0.011997542039650717]
	TIME [epoch: 7.8 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_587.pth
	Model improved!!!
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018384187050493155		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.018384187050493155 | validation: 0.01657052574241296]
	TIME [epoch: 7.76 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01250967736706221		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.01250967736706221 | validation: 0.013808704794515328]
	TIME [epoch: 7.77 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011928500445852091		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.011928500445852091 | validation: 0.015079582796571393]
	TIME [epoch: 7.76 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016989017261570122		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.016989017261570122 | validation: 0.013384882398493853]
	TIME [epoch: 7.8 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013098586722035467		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.013098586722035467 | validation: 0.01749191509456037]
	TIME [epoch: 7.78 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013306727852442465		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.013306727852442465 | validation: 0.013713178374954949]
	TIME [epoch: 7.76 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01428309940433252		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.01428309940433252 | validation: 0.013574195055105532]
	TIME [epoch: 7.76 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013889179689252868		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.013889179689252868 | validation: 0.013273719494092097]
	TIME [epoch: 7.76 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013768183330546253		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.013768183330546253 | validation: 0.016508779397102366]
	TIME [epoch: 7.81 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014843133480858394		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.014843133480858394 | validation: 0.011744761265928765]
	TIME [epoch: 7.79 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_597.pth
	Model improved!!!
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012941604244032531		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.012941604244032531 | validation: 0.01767579339322035]
	TIME [epoch: 7.77 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015185004052062913		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.015185004052062913 | validation: 0.015889815721238508]
	TIME [epoch: 7.78 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014084779807743352		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.014084779807743352 | validation: 0.01301282588036208]
	TIME [epoch: 7.78 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013529097868133999		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.013529097868133999 | validation: 0.014703820536144548]
	TIME [epoch: 7.83 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01586015086973045		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.01586015086973045 | validation: 0.01513191145382357]
	TIME [epoch: 7.78 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013839361350556975		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.013839361350556975 | validation: 0.014461571983176746]
	TIME [epoch: 7.77 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013354975321826399		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.013354975321826399 | validation: 0.015944401237753036]
	TIME [epoch: 7.78 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013124382316992471		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.013124382316992471 | validation: 0.014937166643914607]
	TIME [epoch: 7.78 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015176120168361553		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.015176120168361553 | validation: 0.013067358701211257]
	TIME [epoch: 7.83 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012917389380726806		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.012917389380726806 | validation: 0.016526210728400506]
	TIME [epoch: 7.78 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016207777505246185		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.016207777505246185 | validation: 0.015369684179253268]
	TIME [epoch: 7.78 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01572974071429912		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.01572974071429912 | validation: 0.01525077327753157]
	TIME [epoch: 7.77 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0144014432518522		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.0144014432518522 | validation: 0.014593863224712005]
	TIME [epoch: 7.78 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012813753249684385		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.012813753249684385 | validation: 0.014239710700973619]
	TIME [epoch: 7.83 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014381239400894082		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.014381239400894082 | validation: 0.014303076431202103]
	TIME [epoch: 7.78 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02312503773590939		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.02312503773590939 | validation: 0.021963565142530887]
	TIME [epoch: 7.77 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015442918527754685		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.015442918527754685 | validation: 0.015621998424662175]
	TIME [epoch: 7.77 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01257734439847832		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.01257734439847832 | validation: 0.012801213228206144]
	TIME [epoch: 7.78 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012496047095692308		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.012496047095692308 | validation: 0.01450105036523701]
	TIME [epoch: 7.83 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011891290788965352		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.011891290788965352 | validation: 0.015753393307048228]
	TIME [epoch: 7.78 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014711490500487187		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.014711490500487187 | validation: 0.014098585433780678]
	TIME [epoch: 7.78 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014890321608671618		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.014890321608671618 | validation: 0.012823626154632468]
	TIME [epoch: 7.78 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01223035604627706		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.01223035604627706 | validation: 0.013212899398433989]
	TIME [epoch: 7.79 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013490861985346447		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.013490861985346447 | validation: 0.015816549700187546]
	TIME [epoch: 7.81 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012974750943599487		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.012974750943599487 | validation: 0.014335336767620072]
	TIME [epoch: 7.77 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016146276521732505		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.016146276521732505 | validation: 0.020704570399646807]
	TIME [epoch: 7.77 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014514408982676075		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.014514408982676075 | validation: 0.012248476588038917]
	TIME [epoch: 7.78 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01255672842982717		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.01255672842982717 | validation: 0.013014706985024464]
	TIME [epoch: 7.79 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012528413281739405		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.012528413281739405 | validation: 0.012304087777817179]
	TIME [epoch: 7.81 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015073150807685658		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.015073150807685658 | validation: 0.013917805243033592]
	TIME [epoch: 7.78 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013452453999863764		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.013452453999863764 | validation: 0.014505233109771785]
	TIME [epoch: 7.77 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01410236265583021		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.01410236265583021 | validation: 0.02072495159333177]
	TIME [epoch: 7.78 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014690344188181898		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.014690344188181898 | validation: 0.012935065238915624]
	TIME [epoch: 7.79 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01369062314820629		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.01369062314820629 | validation: 0.01418038913804772]
	TIME [epoch: 7.82 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012021763078505594		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.012021763078505594 | validation: 0.014614692670425145]
	TIME [epoch: 7.77 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01469690644629433		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.01469690644629433 | validation: 0.014840268316294227]
	TIME [epoch: 7.77 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015853543399330493		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.015853543399330493 | validation: 0.017006021367284685]
	TIME [epoch: 7.76 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013600902541577198		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.013600902541577198 | validation: 0.017344626199133458]
	TIME [epoch: 7.79 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013322186043016328		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.013322186043016328 | validation: 0.011964532819634374]
	TIME [epoch: 7.82 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011877516227658374		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.011877516227658374 | validation: 0.013620783552420076]
	TIME [epoch: 7.78 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013898552484961923		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.013898552484961923 | validation: 0.015217078912150371]
	TIME [epoch: 7.78 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01573035689797024		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.01573035689797024 | validation: 0.014916051326615803]
	TIME [epoch: 7.76 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012648349302295043		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.012648349302295043 | validation: 0.012440958079729367]
	TIME [epoch: 7.79 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013149599690936327		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.013149599690936327 | validation: 0.011057365865547213]
	TIME [epoch: 7.81 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_641.pth
	Model improved!!!
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012565470834671853		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.012565470834671853 | validation: 0.01530985587937806]
	TIME [epoch: 7.78 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01415076409905746		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.01415076409905746 | validation: 0.012812021154263668]
	TIME [epoch: 7.76 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013805389709817012		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.013805389709817012 | validation: 0.015137982343678687]
	TIME [epoch: 7.77 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012919821550343307		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.012919821550343307 | validation: 0.013029801828129689]
	TIME [epoch: 7.77 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012654646144888218		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.012654646144888218 | validation: 0.015325400015179597]
	TIME [epoch: 7.81 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014609949721755047		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.014609949721755047 | validation: 0.014883681453443612]
	TIME [epoch: 7.77 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014645025297918127		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.014645025297918127 | validation: 0.01443551928396828]
	TIME [epoch: 7.76 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013418465833830835		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.013418465833830835 | validation: 0.012353563884707056]
	TIME [epoch: 7.77 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011943367172620626		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.011943367172620626 | validation: 0.014465304953597654]
	TIME [epoch: 7.8 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012480291278884833		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.012480291278884833 | validation: 0.014328924181912367]
	TIME [epoch: 7.79 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014318849471718537		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.014318849471718537 | validation: 0.015605509168640303]
	TIME [epoch: 7.76 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012872116434141871		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.012872116434141871 | validation: 0.012031318617559506]
	TIME [epoch: 7.76 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012533786784370893		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.012533786784370893 | validation: 0.013006092960938045]
	TIME [epoch: 7.76 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011577154555914472		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.011577154555914472 | validation: 0.016479827554158416]
	TIME [epoch: 7.78 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015198244340110106		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.015198244340110106 | validation: 0.016020721554535048]
	TIME [epoch: 7.79 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015120501318304785		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.015120501318304785 | validation: 0.012572149161450281]
	TIME [epoch: 7.76 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014733609846843392		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.014733609846843392 | validation: 0.015620965344691363]
	TIME [epoch: 7.77 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012444432826623464		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.012444432826623464 | validation: 0.015832091491719824]
	TIME [epoch: 7.77 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012043739914575587		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.012043739914575587 | validation: 0.011614544491018276]
	TIME [epoch: 7.79 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012056590012299032		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.012056590012299032 | validation: 0.011972029291982246]
	TIME [epoch: 7.8 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011595782171962535		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.011595782171962535 | validation: 0.012359351212407423]
	TIME [epoch: 7.76 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013133478205401663		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.013133478205401663 | validation: 0.012662280247433131]
	TIME [epoch: 7.77 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013096846049777091		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.013096846049777091 | validation: 0.01838647864805746]
	TIME [epoch: 7.76 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013679775135046259		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.013679775135046259 | validation: 0.012730028304882467]
	TIME [epoch: 7.79 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011843370450743677		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.011843370450743677 | validation: 0.012780207811087057]
	TIME [epoch: 7.78 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012267537985453013		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.012267537985453013 | validation: 0.014223352431948208]
	TIME [epoch: 7.76 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01422404791179207		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.01422404791179207 | validation: 0.01357164565699739]
	TIME [epoch: 7.76 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012905236570884694		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.012905236570884694 | validation: 0.012440061353735408]
	TIME [epoch: 7.77 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011806636359313285		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.011806636359313285 | validation: 0.012486573120156788]
	TIME [epoch: 7.8 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013203352768092133		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.013203352768092133 | validation: 0.012631198238746907]
	TIME [epoch: 7.78 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012610062492699463		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.012610062492699463 | validation: 0.012360796454111608]
	TIME [epoch: 7.76 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01369881383341413		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.01369881383341413 | validation: 0.012290538161626983]
	TIME [epoch: 7.75 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01230181630152576		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.01230181630152576 | validation: 0.017682148572739725]
	TIME [epoch: 7.76 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013028959269876723		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.013028959269876723 | validation: 0.012753484956817964]
	TIME [epoch: 7.78 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011877277415624483		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.011877277415624483 | validation: 0.01374446291605477]
	TIME [epoch: 7.78 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013446082589490399		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.013446082589490399 | validation: 0.015135239998527124]
	TIME [epoch: 7.75 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0132442979192839		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.0132442979192839 | validation: 0.012354171898178154]
	TIME [epoch: 7.76 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013381091338597155		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.013381091338597155 | validation: 0.012950859814427356]
	TIME [epoch: 7.77 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01363377643250437		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.01363377643250437 | validation: 0.023862821042885816]
	TIME [epoch: 7.81 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0215187656286803		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.0215187656286803 | validation: 0.013887554154333116]
	TIME [epoch: 7.78 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01339247636948478		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.01339247636948478 | validation: 0.012711950776863656]
	TIME [epoch: 7.76 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011741746432767196		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.011741746432767196 | validation: 0.014123503585157238]
	TIME [epoch: 7.76 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012577395647673212		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.012577395647673212 | validation: 0.013887249418319564]
	TIME [epoch: 7.76 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012653732148800655		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.012653732148800655 | validation: 0.013319183223705788]
	TIME [epoch: 7.81 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0117865584592652		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.0117865584592652 | validation: 0.011923957019973525]
	TIME [epoch: 7.78 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013221401017948513		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.013221401017948513 | validation: 0.01359913961474956]
	TIME [epoch: 7.76 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01359415856192557		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.01359415856192557 | validation: 0.014532262775222893]
	TIME [epoch: 7.76 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013625625545637465		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.013625625545637465 | validation: 0.013333084847974654]
	TIME [epoch: 7.76 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012631000208303197		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.012631000208303197 | validation: 0.012874633724082474]
	TIME [epoch: 7.81 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012076742491890786		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.012076742491890786 | validation: 0.012877229696110713]
	TIME [epoch: 7.77 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012040753813894772		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.012040753813894772 | validation: 0.015607011766623366]
	TIME [epoch: 7.76 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013986000188449962		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.013986000188449962 | validation: 0.013361155414456023]
	TIME [epoch: 7.76 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013663880546664288		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.013663880546664288 | validation: 0.012346796788254101]
	TIME [epoch: 7.77 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011941295128634167		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.011941295128634167 | validation: 0.012771633736310805]
	TIME [epoch: 7.81 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012129617941345724		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.012129617941345724 | validation: 0.012417713803293191]
	TIME [epoch: 7.78 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012004863164566199		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.012004863164566199 | validation: 0.016567486976249417]
	TIME [epoch: 7.76 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017321489722543354		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.017321489722543354 | validation: 0.02157854366373479]
	TIME [epoch: 7.76 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01365876402105506		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.01365876402105506 | validation: 0.01431679990122844]
	TIME [epoch: 7.76 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012947752173105901		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.012947752173105901 | validation: 0.013246607277173285]
	TIME [epoch: 7.83 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01215072578708442		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.01215072578708442 | validation: 0.011794958997693138]
	TIME [epoch: 7.77 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012718296209666755		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.012718296209666755 | validation: 0.014495569552169163]
	TIME [epoch: 7.76 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011824503894003933		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.011824503894003933 | validation: 0.014366355177701198]
	TIME [epoch: 7.76 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02373086146339979		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.02373086146339979 | validation: 0.0896090183378565]
	TIME [epoch: 7.77 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03604771114686309		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.03604771114686309 | validation: 0.04031651037890028]
	TIME [epoch: 7.81 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017671662270399575		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.017671662270399575 | validation: 0.01230885544893795]
	TIME [epoch: 7.76 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012137791994054424		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.012137791994054424 | validation: 0.01172424647411052]
	TIME [epoch: 7.75 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011671398549390024		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.011671398549390024 | validation: 0.011302374828982246]
	TIME [epoch: 7.76 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012079316775437873		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.012079316775437873 | validation: 0.013266053963833567]
	TIME [epoch: 7.76 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011385186559915488		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.011385186559915488 | validation: 0.01204243669922613]
	TIME [epoch: 7.81 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011062487409125785		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.011062487409125785 | validation: 0.011822052334085124]
	TIME [epoch: 7.76 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01257992943454873		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.01257992943454873 | validation: 0.013691702214097188]
	TIME [epoch: 7.77 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013033341334723125		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.013033341334723125 | validation: 0.011505106336494052]
	TIME [epoch: 7.77 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01244997571499025		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.01244997571499025 | validation: 0.011719912385590848]
	TIME [epoch: 7.76 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011716564125467697		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.011716564125467697 | validation: 0.012961424884900872]
	TIME [epoch: 7.81 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012459593343660515		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.012459593343660515 | validation: 0.014216262400489016]
	TIME [epoch: 7.77 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012118613852527		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.012118613852527 | validation: 0.013460671853025886]
	TIME [epoch: 7.76 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012111052115437388		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.012111052115437388 | validation: 0.014334536184843018]
	TIME [epoch: 7.77 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01349806694017217		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.01349806694017217 | validation: 0.013791508810072213]
	TIME [epoch: 7.77 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012699017408260523		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.012699017408260523 | validation: 0.011995616634739857]
	TIME [epoch: 7.81 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012372914955110538		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.012372914955110538 | validation: 0.012399228469240698]
	TIME [epoch: 7.77 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012849879981039865		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.012849879981039865 | validation: 0.012597179590986864]
	TIME [epoch: 7.75 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012059196863872433		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.012059196863872433 | validation: 0.013399714461706845]
	TIME [epoch: 7.76 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013617089131042362		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.013617089131042362 | validation: 0.011960480595117819]
	TIME [epoch: 7.78 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011957577482799997		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.011957577482799997 | validation: 0.013511425723806644]
	TIME [epoch: 7.8 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012624094818194985		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.012624094818194985 | validation: 0.01141245298775968]
	TIME [epoch: 7.77 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012132085899871388		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.012132085899871388 | validation: 0.01347293517951352]
	TIME [epoch: 7.76 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012704468946615472		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.012704468946615472 | validation: 0.012297342092304544]
	TIME [epoch: 7.77 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011902014370164042		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.011902014370164042 | validation: 0.014159714335585494]
	TIME [epoch: 7.77 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01217541524903875		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.01217541524903875 | validation: 0.01313913060079874]
	TIME [epoch: 7.82 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013970774245237797		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.013970774245237797 | validation: 0.012371321631715062]
	TIME [epoch: 7.77 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01211240180546588		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.01211240180546588 | validation: 0.01197217658430325]
	TIME [epoch: 7.77 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011944585919010117		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.011944585919010117 | validation: 0.011720888384096045]
	TIME [epoch: 7.76 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011877919623569735		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.011877919623569735 | validation: 0.014106779806257522]
	TIME [epoch: 7.78 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013757094716192592		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.013757094716192592 | validation: 0.012028957514914063]
	TIME [epoch: 7.8 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012542085658143388		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.012542085658143388 | validation: 0.012499217673180074]
	TIME [epoch: 7.77 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012204143664889177		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.012204143664889177 | validation: 0.013294075040766584]
	TIME [epoch: 7.76 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012730295736398684		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.012730295736398684 | validation: 0.011635939527778243]
	TIME [epoch: 7.77 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011500736790665995		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.011500736790665995 | validation: 0.012673121796922284]
	TIME [epoch: 7.76 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012116979612422422		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.012116979612422422 | validation: 0.012738492164543765]
	TIME [epoch: 7.81 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012460694603277696		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.012460694603277696 | validation: 0.013310547017296503]
	TIME [epoch: 7.75 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01169147028568978		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.01169147028568978 | validation: 0.011500475528400788]
	TIME [epoch: 7.77 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_fix_noise_small_20240627_193206/states/model_phi1_1a_v_mmd1_fix_noise_small_742.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 6019.350 seconds.
