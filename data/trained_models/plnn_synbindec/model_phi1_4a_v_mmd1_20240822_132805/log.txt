Args:
Namespace(name='model_phi1_4a_v_mmd1', outdir='out/model_training/model_phi1_4a_v_mmd1', training_data='data/training_data/data_phi1_4a/training', validation_data='data/training_data/data_phi1_4a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3747503091

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.720321821710577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.720321821710577 | validation: 6.279829150327067]
	TIME [epoch: 45.8 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.233216641435215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.233216641435215 | validation: 6.498374122512743]
	TIME [epoch: 0.962 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.94646613048794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.94646613048794 | validation: 6.244318830867356]
	TIME [epoch: 0.939 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.435794997032917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.435794997032917 | validation: 6.222960750734874]
	TIME [epoch: 0.942 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.387499715626281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.387499715626281 | validation: 6.150939331017733]
	TIME [epoch: 0.935 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.310759051026856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.310759051026856 | validation: 6.216966442873053]
	TIME [epoch: 0.94 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.343757343018971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.343757343018971 | validation: 6.069197595303251]
	TIME [epoch: 0.942 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.4590016340144745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4590016340144745 | validation: 6.041346975622446]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.151375654532077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.151375654532077 | validation: 6.174819122786568]
	TIME [epoch: 0.94 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.3084727714558735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3084727714558735 | validation: 5.9416789247571735]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.254568894540682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.254568894540682 | validation: 5.953012624668043]
	TIME [epoch: 0.938 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.043766004426097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.043766004426097 | validation: 6.092254218820577]
	TIME [epoch: 0.937 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.15076131850284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.15076131850284 | validation: 5.86144424338483]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.082260493742176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.082260493742176 | validation: 5.895356480956002]
	TIME [epoch: 0.938 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9325530996468605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9325530996468605 | validation: 5.964878279039756]
	TIME [epoch: 0.939 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9658535596959132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9658535596959132 | validation: 5.78219806517977]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.97005454011862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.97005454011862 | validation: 5.850963989300626]
	TIME [epoch: 0.937 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8393372219343633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8393372219343633 | validation: 5.805714066294042]
	TIME [epoch: 0.938 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8094393591919125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8094393591919125 | validation: 5.71446407792766]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7969613609883006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7969613609883006 | validation: 5.818015850446891]
	TIME [epoch: 0.943 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7925282157706763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7925282157706763 | validation: 5.606319115513834]
	TIME [epoch: 0.951 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.843294292891203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.843294292891203 | validation: 5.786493798371956]
	TIME [epoch: 0.937 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.749233154794805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.749233154794805 | validation: 5.569352707905764]
	TIME [epoch: 0.934 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7245164092462075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7245164092462075 | validation: 5.6845694624905985]
	TIME [epoch: 0.94 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6695217355753575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6695217355753575 | validation: 5.4933244383340485]
	TIME [epoch: 0.94 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6538099534685218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6538099534685218 | validation: 5.627255967651017]
	TIME [epoch: 0.936 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.611152609119813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.611152609119813 | validation: 5.3381302600076275]
	TIME [epoch: 0.933 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.588089192768806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.588089192768806 | validation: 5.398949737069046]
	TIME [epoch: 0.939 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4837847157592274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4837847157592274 | validation: 4.924208144205658]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3470913326124903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3470913326124903 | validation: 4.754666693683328]
	TIME [epoch: 0.941 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.09702174401094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.09702174401094 | validation: 4.316788316189218]
	TIME [epoch: 0.939 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9893329182378636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9893329182378636 | validation: 5.542641004243319]
	TIME [epoch: 0.936 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.458107531826565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.458107531826565 | validation: 3.8238592450058393]
	TIME [epoch: 0.933 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5649333779393637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5649333779393637 | validation: 2.875220595646967]
	TIME [epoch: 0.939 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1962062573022205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1962062573022205 | validation: 2.0962631039772006]
	TIME [epoch: 0.944 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.350681368639055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.350681368639055 | validation: 1.9997960163647304]
	TIME [epoch: 0.94 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6976088709820436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6976088709820436 | validation: 1.368257023468977]
	TIME [epoch: 0.939 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2097553738178113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2097553738178113 | validation: 2.0680858065281047]
	TIME [epoch: 0.939 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8381176787220077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8381176787220077 | validation: 1.5157086724512823]
	TIME [epoch: 0.938 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3264000384605887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3264000384605887 | validation: 1.4413503388956643]
	TIME [epoch: 0.937 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7237412970720527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7237412970720527 | validation: 1.6435609850624553]
	TIME [epoch: 0.938 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.392968320053134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.392968320053134 | validation: 0.8510325725334532]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2261207825123943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2261207825123943 | validation: 0.8887561330387155]
	TIME [epoch: 0.939 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0622050507077148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0622050507077148 | validation: 1.0987945785724789]
	TIME [epoch: 0.939 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0565181088372628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0565181088372628 | validation: 0.7797644915881239]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0028403629411304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0028403629411304 | validation: 1.1223142911339987]
	TIME [epoch: 0.937 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.002601847137804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.002601847137804 | validation: 0.862290065158814]
	TIME [epoch: 0.934 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1247912520755055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1247912520755055 | validation: 1.8117275032174938]
	TIME [epoch: 0.936 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.336989453086666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.336989453086666 | validation: 0.6705304822684801]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3520936336529799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3520936336529799 | validation: 0.9615654438808751]
	TIME [epoch: 0.94 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.940511023162444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.940511023162444 | validation: 1.126808007011897]
	TIME [epoch: 0.938 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0017633642372477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0017633642372477 | validation: 0.6769584436267904]
	TIME [epoch: 0.936 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.059453177249948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.059453177249948 | validation: 1.287055475058402]
	TIME [epoch: 0.934 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0352672995766004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0352672995766004 | validation: 0.7451923883164344]
	TIME [epoch: 0.935 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8864762757686984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8864762757686984 | validation: 0.8354060273171122]
	TIME [epoch: 0.937 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8610172708459678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8610172708459678 | validation: 0.8141639004020041]
	TIME [epoch: 0.938 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8529245431870197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8529245431870197 | validation: 0.8104539836501197]
	TIME [epoch: 0.936 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8449647577715499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8449647577715499 | validation: 0.9150405479873719]
	TIME [epoch: 0.935 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8517870020159842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8517870020159842 | validation: 0.6923102177299154]
	TIME [epoch: 0.936 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8907208725061676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8907208725061676 | validation: 1.3011552438241245]
	TIME [epoch: 0.948 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0842978797326333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0842978797326333 | validation: 0.7777744333040808]
	TIME [epoch: 0.936 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4643283882384248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4643283882384248 | validation: 1.0343463045407153]
	TIME [epoch: 0.935 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9390922614552244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9390922614552244 | validation: 1.1405006752664597]
	TIME [epoch: 0.938 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0406003198809664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0406003198809664 | validation: 0.7170444256323603]
	TIME [epoch: 0.948 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1770576215419184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1770576215419184 | validation: 0.9644414752151784]
	TIME [epoch: 0.935 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9197597450068463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9197597450068463 | validation: 0.9698453723726725]
	TIME [epoch: 0.936 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9248441293478941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9248441293478941 | validation: 0.6366996519424796]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9846234669105765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9846234669105765 | validation: 0.9232468824146225]
	TIME [epoch: 0.936 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8751111352142564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8751111352142564 | validation: 0.8298200597803063]
	TIME [epoch: 0.932 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8330177286700386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8330177286700386 | validation: 0.6630434403716626]
	TIME [epoch: 0.933 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8604670378524082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8604670378524082 | validation: 0.9121066208723243]
	TIME [epoch: 0.934 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8431253420742918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8431253420742918 | validation: 0.7034737727457112]
	TIME [epoch: 0.93 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8188867791324427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8188867791324427 | validation: 0.7708764170871488]
	TIME [epoch: 0.931 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8074732467210034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8074732467210034 | validation: 0.696565658807493]
	TIME [epoch: 0.931 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8099585412462997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8099585412462997 | validation: 0.7799478161619959]
	TIME [epoch: 0.937 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8080162227511325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8080162227511325 | validation: 0.6496745100804157]
	TIME [epoch: 0.931 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8657340756326695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8657340756326695 | validation: 0.9883168252812118]
	TIME [epoch: 0.932 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9393099252294984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9393099252294984 | validation: 0.6565486893226908]
	TIME [epoch: 0.932 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2526682235796376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2526682235796376 | validation: 1.3788085829749306]
	TIME [epoch: 0.932 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.091173674346294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.091173674346294 | validation: 0.7013739234040687]
	TIME [epoch: 0.931 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.813328604426299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.813328604426299 | validation: 0.6242354376173489]
	TIME [epoch: 0.93 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.879051458813652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.879051458813652 | validation: 1.0962309349673092]
	TIME [epoch: 0.937 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9294307925930515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9294307925930515 | validation: 0.5994593787033566]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9008175951330462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9008175951330462 | validation: 0.8331277140894683]
	TIME [epoch: 0.94 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8814309486155559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8814309486155559 | validation: 0.6982590336914645]
	TIME [epoch: 0.941 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8959987968284813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8959987968284813 | validation: 0.7837064242161764]
	TIME [epoch: 0.939 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.821801924932481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.821801924932481 | validation: 0.6752252707827305]
	TIME [epoch: 0.938 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7957791067314055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7957791067314055 | validation: 0.7838082699024936]
	TIME [epoch: 0.937 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8075027876347797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8075027876347797 | validation: 0.631958432045558]
	TIME [epoch: 0.937 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8249155034203736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8249155034203736 | validation: 1.017097546589698]
	TIME [epoch: 0.941 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9120160968127428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9120160968127428 | validation: 0.6830606920753853]
	TIME [epoch: 0.937 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2311089577398013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2311089577398013 | validation: 1.1347321792590488]
	TIME [epoch: 0.938 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9589159186110401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9589159186110401 | validation: 0.6788854580123435]
	TIME [epoch: 0.939 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8087584423459943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8087584423459943 | validation: 0.64315870249236]
	TIME [epoch: 0.941 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8206157063650484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8206157063650484 | validation: 0.889983066319179]
	TIME [epoch: 0.938 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8447360393743106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8447360393743106 | validation: 0.5976481545425212]
	TIME [epoch: 0.938 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8602839475163856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8602839475163856 | validation: 0.8913435516204594]
	TIME [epoch: 0.941 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8755053321719031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8755053321719031 | validation: 0.6278964045219863]
	TIME [epoch: 0.938 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9590475541000038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9590475541000038 | validation: 0.8042435915943287]
	TIME [epoch: 0.936 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8253807670419778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8253807670419778 | validation: 0.6776623006787793]
	TIME [epoch: 0.937 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8089952199115078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8089952199115078 | validation: 0.7642396650566518]
	TIME [epoch: 0.942 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8101383601648121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8101383601648121 | validation: 0.6541562482381075]
	TIME [epoch: 0.947 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8126799492736124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8126799492736124 | validation: 0.8114942718150657]
	TIME [epoch: 0.937 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8365430987862972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8365430987862972 | validation: 0.6229225630737633]
	TIME [epoch: 0.937 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.926046121525717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.926046121525717 | validation: 0.857682704560787]
	TIME [epoch: 0.945 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8470270125769946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8470270125769946 | validation: 0.6126713531423695]
	TIME [epoch: 0.943 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8980474124719752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8980474124719752 | validation: 1.372329756766205]
	TIME [epoch: 0.936 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.084563485651361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.084563485651361 | validation: 0.5921063682323393]
	TIME [epoch: 0.937 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.886784310480143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.886784310480143 | validation: 0.7932015955387128]
	TIME [epoch: 0.933 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8202740777599007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8202740777599007 | validation: 0.6661238558543594]
	TIME [epoch: 0.931 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8135568923066291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8135568923066291 | validation: 0.7391855277034968]
	TIME [epoch: 0.932 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8113905788595409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8113905788595409 | validation: 0.7885781136104498]
	TIME [epoch: 0.93 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8659609535347295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8659609535347295 | validation: 0.7539706751398265]
	TIME [epoch: 0.932 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8463013340451931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8463013340451931 | validation: 0.6820687410516182]
	TIME [epoch: 0.932 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8650455701916553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8650455701916553 | validation: 0.7700486315562167]
	TIME [epoch: 0.932 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8211742868534306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8211742868534306 | validation: 0.6263276651593603]
	TIME [epoch: 0.936 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8277179437452707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8277179437452707 | validation: 0.9153154654502693]
	TIME [epoch: 0.93 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8635233865345251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8635233865345251 | validation: 0.6757205940406618]
	TIME [epoch: 0.932 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0547074284411118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0547074284411118 | validation: 1.2498016001175878]
	TIME [epoch: 0.929 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.040652954928777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.040652954928777 | validation: 0.6027719903345267]
	TIME [epoch: 0.931 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.82719005919129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.82719005919129 | validation: 0.6990828048904781]
	TIME [epoch: 0.932 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8273575605882301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8273575605882301 | validation: 0.8361824509618121]
	TIME [epoch: 0.932 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8422223822462749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8422223822462749 | validation: 0.6660355116816028]
	TIME [epoch: 0.932 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7777992133396796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7777992133396796 | validation: 0.6458173704953017]
	TIME [epoch: 0.931 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7630323261086521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7630323261086521 | validation: 0.7405401007205143]
	TIME [epoch: 0.931 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7759928235774678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7759928235774678 | validation: 0.6172567896677674]
	TIME [epoch: 0.931 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7770761201222354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7770761201222354 | validation: 0.800946836712681]
	TIME [epoch: 0.93 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.803658025451959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.803658025451959 | validation: 0.576358157537306]
	TIME [epoch: 0.93 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9185981893274956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9185981893274956 | validation: 1.1938943488500795]
	TIME [epoch: 0.936 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1386055844151448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1386055844151448 | validation: 0.6239194355264961]
	TIME [epoch: 0.936 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.042902134495731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.042902134495731 | validation: 0.7168643940774024]
	TIME [epoch: 0.937 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7869948191623541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7869948191623541 | validation: 0.886989090627531]
	TIME [epoch: 0.931 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9385432568374175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9385432568374175 | validation: 0.6698946174083363]
	TIME [epoch: 0.93 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8510259803319852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8510259803319852 | validation: 0.6607640344286371]
	TIME [epoch: 0.931 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.776002443689016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.776002443689016 | validation: 0.840423151662252]
	TIME [epoch: 0.931 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8243884737440099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8243884737440099 | validation: 0.5867107890857228]
	TIME [epoch: 0.932 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8329506681750566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8329506681750566 | validation: 0.7865184308604972]
	TIME [epoch: 0.929 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7967191308368672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7967191308368672 | validation: 0.6867332225626265]
	TIME [epoch: 0.931 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8092699608209688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8092699608209688 | validation: 0.8536502278087416]
	TIME [epoch: 0.931 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8413006039800053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8413006039800053 | validation: 0.6684230257440786]
	TIME [epoch: 0.932 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7830687678104846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7830687678104846 | validation: 0.6610632657093647]
	TIME [epoch: 0.932 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7675876397345585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7675876397345585 | validation: 0.6842124449223614]
	TIME [epoch: 0.931 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7694020060076955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7694020060076955 | validation: 0.6206759343535553]
	TIME [epoch: 0.93 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7836844169865543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7836844169865543 | validation: 0.7782353036103752]
	TIME [epoch: 0.93 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8088455235613279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8088455235613279 | validation: 0.5986925381843434]
	TIME [epoch: 0.932 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9086487867780829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9086487867780829 | validation: 0.989520537158499]
	TIME [epoch: 0.932 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.891360992749618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.891360992749618 | validation: 0.7066751769918863]
	TIME [epoch: 0.93 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9552270839072151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9552270839072151 | validation: 0.9269059576204829]
	TIME [epoch: 0.935 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8461740984137514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8461740984137514 | validation: 0.6477139136094544]
	TIME [epoch: 0.943 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7472296714393324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7472296714393324 | validation: 0.6330223767255704]
	TIME [epoch: 0.931 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7670457904189636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7670457904189636 | validation: 0.756187714739705]
	TIME [epoch: 0.932 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7633351163444764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7633351163444764 | validation: 0.6174054759242783]
	TIME [epoch: 0.935 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7486709430180193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7486709430180193 | validation: 0.7133810902365854]
	TIME [epoch: 0.933 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7422243842617599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7422243842617599 | validation: 0.6026852427718927]
	TIME [epoch: 0.93 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.737872109290461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.737872109290461 | validation: 0.7334198901174296]
	TIME [epoch: 0.93 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7507935337264899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7507935337264899 | validation: 0.5778224847115662]
	TIME [epoch: 0.935 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8823498951232904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8823498951232904 | validation: 0.9923017908076587]
	TIME [epoch: 0.932 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0588250267383823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0588250267383823 | validation: 0.5892213165451401]
	TIME [epoch: 0.93 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.951620387112327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.951620387112327 | validation: 0.6904883775794357]
	TIME [epoch: 0.93 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7394672412050457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7394672412050457 | validation: 0.8136877131139624]
	TIME [epoch: 0.932 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8872024341093131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8872024341093131 | validation: 0.8061104056034711]
	TIME [epoch: 0.934 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8673553465549574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8673553465549574 | validation: 0.6096571386069244]
	TIME [epoch: 0.934 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.786837826213978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.786837826213978 | validation: 0.8884175383405947]
	TIME [epoch: 0.936 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8188088918592175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8188088918592175 | validation: 0.584329498451986]
	TIME [epoch: 0.94 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7729573916469598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7729573916469598 | validation: 0.6897199057201621]
	TIME [epoch: 0.935 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.713687571017912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.713687571017912 | validation: 0.6527329758850056]
	TIME [epoch: 0.937 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7143145190243233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7143145190243233 | validation: 0.592394427451587]
	TIME [epoch: 0.937 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7193037104284988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7193037104284988 | validation: 0.7069635907387792]
	TIME [epoch: 0.94 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7199182490965097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7199182490965097 | validation: 0.564634427455595]
	TIME [epoch: 0.939 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7427556166153072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7427556166153072 | validation: 0.7142914164975682]
	TIME [epoch: 0.933 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7532252365074851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7532252365074851 | validation: 0.5926320326224908]
	TIME [epoch: 0.933 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8044829372105524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8044829372105524 | validation: 0.7380559494985296]
	TIME [epoch: 0.934 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7786809581628521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7786809581628521 | validation: 0.5303975806967628]
	TIME [epoch: 0.93 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7600111711138228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7600111711138228 | validation: 0.7941324427266424]
	TIME [epoch: 0.941 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7450890566978131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7450890566978131 | validation: 0.5675237192754785]
	TIME [epoch: 0.939 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7913858256926024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7913858256926024 | validation: 1.0476939031965469]
	TIME [epoch: 0.939 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9037629383232669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9037629383232669 | validation: 0.5586023349735741]
	TIME [epoch: 0.937 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7187758634690402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7187758634690402 | validation: 0.5589991763110955]
	TIME [epoch: 0.936 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7108176598057004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7108176598057004 | validation: 0.7345153815091083]
	TIME [epoch: 0.939 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7507893762525752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7507893762525752 | validation: 0.5521831332105757]
	TIME [epoch: 0.938 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7955623441718307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7955623441718307 | validation: 0.6923028642880843]
	TIME [epoch: 0.938 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7510371419939438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7510371419939438 | validation: 0.5838884837264982]
	TIME [epoch: 0.937 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7219188633449283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7219188633449283 | validation: 0.6171898402630616]
	TIME [epoch: 0.936 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6911042447976746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6911042447976746 | validation: 0.6276955194474356]
	TIME [epoch: 0.938 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6966980025353563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6966980025353563 | validation: 0.5568825727129474]
	TIME [epoch: 0.938 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7305616492864144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7305616492864144 | validation: 0.8303286407532298]
	TIME [epoch: 0.937 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8031123443972464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8031123443972464 | validation: 0.5728038582445371]
	TIME [epoch: 0.938 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7574796259454635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7574796259454635 | validation: 0.6112592547013832]
	TIME [epoch: 0.936 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7284341453924559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7284341453924559 | validation: 0.6523793883668525]
	TIME [epoch: 0.938 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7165780793885782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7165780793885782 | validation: 0.5133412933401043]
	TIME [epoch: 0.936 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_190.pth
	Model improved!!!
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7136635694284189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7136635694284189 | validation: 0.6768781574067431]
	TIME [epoch: 0.939 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6940716763026643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6940716763026643 | validation: 0.5488226481851026]
	TIME [epoch: 0.938 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6805648873799598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6805648873799598 | validation: 0.6017646000695446]
	TIME [epoch: 0.936 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6603130860184356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6603130860184356 | validation: 0.5234128485757533]
	TIME [epoch: 0.941 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6677638662541581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6677638662541581 | validation: 0.6131811656488376]
	TIME [epoch: 0.95 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6557652540849686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6557652540849686 | validation: 0.5302371577309988]
	TIME [epoch: 0.937 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6490953400300193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6490953400300193 | validation: 0.6554180832213714]
	TIME [epoch: 0.935 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.664741195871751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.664741195871751 | validation: 0.5141724692558882]
	TIME [epoch: 0.94 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6816409350946826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6816409350946826 | validation: 0.6653275352775383]
	TIME [epoch: 0.939 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6999972778696524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6999972778696524 | validation: 0.806834424454258]
	TIME [epoch: 0.938 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8379680162573432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8379680162573432 | validation: 0.41956841374924675]
	TIME [epoch: 43.5 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8613396236941909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8613396236941909 | validation: 0.9194815380246368]
	TIME [epoch: 1.84 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8825738553318736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8825738553318736 | validation: 0.5544402596924385]
	TIME [epoch: 1.83 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7679882043526084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7679882043526084 | validation: 0.5281893054500966]
	TIME [epoch: 1.83 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.677002152874918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.677002152874918 | validation: 0.6168677375230738]
	TIME [epoch: 1.84 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6412956331866255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6412956331866255 | validation: 0.6213074738449806]
	TIME [epoch: 1.84 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6392140547298023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6392140547298023 | validation: 0.6329968500129235]
	TIME [epoch: 1.84 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6306958404656385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6306958404656385 | validation: 0.5052427532404483]
	TIME [epoch: 1.85 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6048804300528897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6048804300528897 | validation: 0.5856024535113968]
	TIME [epoch: 1.83 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5913320762166664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5913320762166664 | validation: 0.5792854049036835]
	TIME [epoch: 1.84 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6197013310063835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6197013310063835 | validation: 0.6480463820960116]
	TIME [epoch: 1.83 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6918203259332908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6918203259332908 | validation: 0.6293076600989569]
	TIME [epoch: 1.83 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7371180341374808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7371180341374808 | validation: 0.5893638069743649]
	TIME [epoch: 1.84 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6585868663019667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6585868663019667 | validation: 0.5675162927664282]
	TIME [epoch: 1.83 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5829274768955293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5829274768955293 | validation: 0.5966778577895567]
	TIME [epoch: 1.83 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6173933147997556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6173933147997556 | validation: 0.6221479984257119]
	TIME [epoch: 1.83 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6186158940998848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6186158940998848 | validation: 0.47583475417481735]
	TIME [epoch: 1.84 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5809374297981487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5809374297981487 | validation: 0.7480903491863917]
	TIME [epoch: 1.83 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6077544251952055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6077544251952055 | validation: 0.5558518030619878]
	TIME [epoch: 1.83 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6486441334174541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6486441334174541 | validation: 0.40595607221282554]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6084059371016827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6084059371016827 | validation: 0.648826377368227]
	TIME [epoch: 1.83 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5714283550949029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5714283550949029 | validation: 0.773043984279261]
	TIME [epoch: 1.83 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7214871968621789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7214871968621789 | validation: 0.45192143562042975]
	TIME [epoch: 1.83 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6235132734887272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6235132734887272 | validation: 0.5774879273484741]
	TIME [epoch: 1.83 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5481442950633837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5481442950633837 | validation: 0.694828216933838]
	TIME [epoch: 1.83 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6485189035052628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6485189035052628 | validation: 0.6072404357796315]
	TIME [epoch: 1.83 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5565051201677237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5565051201677237 | validation: 0.47290040949136647]
	TIME [epoch: 1.83 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5799607000570088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5799607000570088 | validation: 0.5240952347012686]
	TIME [epoch: 1.83 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5098552106617934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5098552106617934 | validation: 0.7548914252175152]
	TIME [epoch: 1.83 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6657906135099261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6657906135099261 | validation: 0.47384170415401167]
	TIME [epoch: 1.83 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6673649265034943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6673649265034943 | validation: 0.6989554160624266]
	TIME [epoch: 1.83 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6874310972624789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6874310972624789 | validation: 0.4338548060245508]
	TIME [epoch: 1.83 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5457797542816983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5457797542816983 | validation: 0.6136016007147704]
	TIME [epoch: 1.84 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5251513548461779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5251513548461779 | validation: 0.6174183429398372]
	TIME [epoch: 1.83 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5220832313833902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5220832313833902 | validation: 0.4836003566103533]
	TIME [epoch: 1.85 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5318077357976457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5318077357976457 | validation: 0.5557940300151069]
	TIME [epoch: 1.83 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5003194768280318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5003194768280318 | validation: 0.5200160476384345]
	TIME [epoch: 1.83 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.501783674035871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.501783674035871 | validation: 0.5512913159345987]
	TIME [epoch: 1.83 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5474363369305388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5474363369305388 | validation: 0.5144720852039691]
	TIME [epoch: 1.83 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4735015489832556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4735015489832556 | validation: 0.4118322168818422]
	TIME [epoch: 1.83 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.526056407870929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.526056407870929 | validation: 0.9070900976935997]
	TIME [epoch: 1.83 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6617732793063006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6617732793063006 | validation: 0.6376158841696572]
	TIME [epoch: 1.83 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6531907753782181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6531907753782181 | validation: 0.39817071824052475]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_243.pth
	Model improved!!!
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6161427238147641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6161427238147641 | validation: 0.5588691783578207]
	TIME [epoch: 1.84 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4762293862382528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4762293862382528 | validation: 0.6595274739069877]
	TIME [epoch: 1.84 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5328239171482645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5328239171482645 | validation: 0.5131164493588516]
	TIME [epoch: 1.84 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.543484396324457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.543484396324457 | validation: 0.4817363106566111]
	TIME [epoch: 1.84 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4796556575094334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4796556575094334 | validation: 0.47033784348616453]
	TIME [epoch: 1.84 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4128684879373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4128684879373 | validation: 0.5044127632362279]
	TIME [epoch: 1.84 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43309370080459353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43309370080459353 | validation: 0.4804425540027806]
	TIME [epoch: 1.84 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5222691524797072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5222691524797072 | validation: 0.5538978662013262]
	TIME [epoch: 1.84 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45641182583305134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45641182583305134 | validation: 0.36294368296376867]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_252.pth
	Model improved!!!
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4832570185398846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4832570185398846 | validation: 0.6540768030309532]
	TIME [epoch: 1.84 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4353007862143142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4353007862143142 | validation: 0.3713261301501025]
	TIME [epoch: 1.84 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37010934011954566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37010934011954566 | validation: 0.4675672172111652]
	TIME [epoch: 1.83 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40545068357375824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40545068357375824 | validation: 0.7760835374168833]
	TIME [epoch: 1.84 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7396358240053783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7396358240053783 | validation: 0.3520253805084967]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_257.pth
	Model improved!!!
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5783046994665153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5783046994665153 | validation: 0.57935368593031]
	TIME [epoch: 1.84 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4856658000387442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4856658000387442 | validation: 0.5051221018044286]
	TIME [epoch: 1.83 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4704860221617695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4704860221617695 | validation: 0.400758785910748]
	TIME [epoch: 1.84 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42669244326193495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42669244326193495 | validation: 0.5694414271727618]
	TIME [epoch: 1.83 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41852443969704944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41852443969704944 | validation: 0.3512757828094051]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_262.pth
	Model improved!!!
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35319479615822985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35319479615822985 | validation: 0.51534418263437]
	TIME [epoch: 1.84 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3376838996838428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3376838996838428 | validation: 0.31906192937785854]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_264.pth
	Model improved!!!
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3561703126369584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3561703126369584 | validation: 0.6154056168140719]
	TIME [epoch: 1.84 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4503767881091575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4503767881091575 | validation: 0.540745693377437]
	TIME [epoch: 1.84 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6072234302599918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6072234302599918 | validation: 0.3184775476245133]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43198101130365585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43198101130365585 | validation: 0.5349907471854097]
	TIME [epoch: 1.84 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3512758170546076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3512758170546076 | validation: 0.46572115638906786]
	TIME [epoch: 1.84 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3512073398562181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3512073398562181 | validation: 0.4267206690733799]
	TIME [epoch: 1.84 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.569507668100518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.569507668100518 | validation: 0.46842486118973503]
	TIME [epoch: 1.84 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3116643014057187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3116643014057187 | validation: 0.4738296289315147]
	TIME [epoch: 1.84 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39048446738992904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39048446738992904 | validation: 0.40888863274195925]
	TIME [epoch: 1.84 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4959488655973054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4959488655973054 | validation: 0.4043480770848035]
	TIME [epoch: 1.84 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33644146826527954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33644146826527954 | validation: 0.5791595706784901]
	TIME [epoch: 1.84 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41567100402706203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41567100402706203 | validation: 0.2427700036541677]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_276.pth
	Model improved!!!
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37588666709490753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37588666709490753 | validation: 0.6268502799695682]
	TIME [epoch: 1.84 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3718595915624252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3718595915624252 | validation: 0.3491427841489211]
	TIME [epoch: 1.83 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2557058596065367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2557058596065367 | validation: 0.35840651629753667]
	TIME [epoch: 1.84 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3143703851576521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3143703851576521 | validation: 0.5075529502123909]
	TIME [epoch: 1.83 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4371610610796941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4371610610796941 | validation: 0.346136632328137]
	TIME [epoch: 1.84 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4223952723350884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4223952723350884 | validation: 0.3048138604306506]
	TIME [epoch: 1.84 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2232528026562904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2232528026562904 | validation: 0.4729346265737583]
	TIME [epoch: 1.85 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30317073182441556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30317073182441556 | validation: 0.2882839236355939]
	TIME [epoch: 1.83 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29054787947375726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29054787947375726 | validation: 0.38099314401569906]
	TIME [epoch: 1.84 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3161221418945633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3161221418945633 | validation: 0.4182455408001469]
	TIME [epoch: 1.84 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3147930401937152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3147930401937152 | validation: 0.2784743466392919]
	TIME [epoch: 1.83 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.483570411937674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.483570411937674 | validation: 0.7183466179628466]
	TIME [epoch: 1.83 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44244109179889046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44244109179889046 | validation: 0.3781511705096569]
	TIME [epoch: 1.84 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24729965182582928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24729965182582928 | validation: 0.28121354205593085]
	TIME [epoch: 1.84 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3748670639296893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3748670639296893 | validation: 0.46743913813820337]
	TIME [epoch: 1.84 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.312098679317066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.312098679317066 | validation: 0.48891600638594374]
	TIME [epoch: 1.84 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3881087538934921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3881087538934921 | validation: 0.3777694990580686]
	TIME [epoch: 1.84 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32866663641873817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32866663641873817 | validation: 0.3242609503019579]
	TIME [epoch: 1.84 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20413728101657178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20413728101657178 | validation: 0.30984046751520866]
	TIME [epoch: 1.83 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2014281738639876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2014281738639876 | validation: 0.33354310781172725]
	TIME [epoch: 1.84 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25691664484545906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25691664484545906 | validation: 0.3374200953115103]
	TIME [epoch: 1.84 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27490907569658307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27490907569658307 | validation: 0.32155078488308386]
	TIME [epoch: 1.84 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2793452619594895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2793452619594895 | validation: 0.26505841198326613]
	TIME [epoch: 1.83 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3266819313531718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3266819313531718 | validation: 0.8867420108472029]
	TIME [epoch: 1.84 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5863350079417073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5863350079417073 | validation: 0.6679804072282534]
	TIME [epoch: 1.84 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49722897757745543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49722897757745543 | validation: 0.49721196940260975]
	TIME [epoch: 1.84 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4792713850417698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4792713850417698 | validation: 0.5131783278391951]
	TIME [epoch: 1.84 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3873432483193846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3873432483193846 | validation: 0.4814117135277858]
	TIME [epoch: 1.84 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34801572639638695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34801572639638695 | validation: 0.44311698605403066]
	TIME [epoch: 1.84 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31407993837736226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31407993837736226 | validation: 0.3290044287603964]
	TIME [epoch: 1.84 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29892622956270387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29892622956270387 | validation: 0.30521367590631915]
	TIME [epoch: 1.84 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2022558577254893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2022558577254893 | validation: 0.30100934081506764]
	TIME [epoch: 1.85 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20305837929526718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20305837929526718 | validation: 0.3260338010071845]
	TIME [epoch: 1.84 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.227683454440099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.227683454440099 | validation: 0.2421698055207315]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_310.pth
	Model improved!!!
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2824025554521207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2824025554521207 | validation: 0.4041857697270579]
	TIME [epoch: 1.84 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22206843887487324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22206843887487324 | validation: 0.2546425003342476]
	TIME [epoch: 1.84 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23466786790276573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23466786790276573 | validation: 0.5112461689976091]
	TIME [epoch: 1.83 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3123716107280784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3123716107280784 | validation: 0.22949184934841504]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_314.pth
	Model improved!!!
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2785280960550245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2785280960550245 | validation: 0.24610029032909378]
	TIME [epoch: 1.83 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18154729398300043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18154729398300043 | validation: 0.34288183566715963]
	TIME [epoch: 1.84 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21632854630997408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21632854630997408 | validation: 0.22216074799184582]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_317.pth
	Model improved!!!
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24023790076367543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24023790076367543 | validation: 0.31763165633884677]
	TIME [epoch: 1.84 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22915273999285934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22915273999285934 | validation: 0.2935753684828334]
	TIME [epoch: 1.84 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19721431707694453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19721431707694453 | validation: 0.21985125901653169]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_320.pth
	Model improved!!!
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1992638777739243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1992638777739243 | validation: 0.3285387966757862]
	TIME [epoch: 1.84 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1897853871230837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1897853871230837 | validation: 0.28275695054366107]
	TIME [epoch: 1.84 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2612918181574297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2612918181574297 | validation: 0.6006079972260738]
	TIME [epoch: 1.84 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34737630613628656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34737630613628656 | validation: 0.213594526081963]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_324.pth
	Model improved!!!
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22211314174359167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22211314174359167 | validation: 0.23804817813779922]
	TIME [epoch: 1.84 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18631893782832618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18631893782832618 | validation: 0.3718011509116376]
	TIME [epoch: 1.83 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26722215579737557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26722215579737557 | validation: 0.21258689803320335]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_327.pth
	Model improved!!!
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25159280884165147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25159280884165147 | validation: 0.302957999064504]
	TIME [epoch: 1.84 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17803639941657828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17803639941657828 | validation: 0.23097652390085877]
	TIME [epoch: 1.84 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14768175402619532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14768175402619532 | validation: 0.26912378675816423]
	TIME [epoch: 1.84 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14948438541827036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14948438541827036 | validation: 0.35339846160979244]
	TIME [epoch: 1.96 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19811417500508988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19811417500508988 | validation: 0.32402764507718107]
	TIME [epoch: 1.84 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4041718270459464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4041718270459464 | validation: 0.39425376719111427]
	TIME [epoch: 1.84 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19876055693243824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19876055693243824 | validation: 0.24322039186292216]
	TIME [epoch: 1.84 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27700824602667884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27700824602667884 | validation: 0.2549745561066265]
	TIME [epoch: 1.84 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23572934643589719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23572934643589719 | validation: 0.26547096414409915]
	TIME [epoch: 1.84 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14827936581686618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14827936581686618 | validation: 0.2699314324693926]
	TIME [epoch: 1.83 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15306296957364157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15306296957364157 | validation: 0.2509695772035226]
	TIME [epoch: 1.84 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1416740632937471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1416740632937471 | validation: 0.33581532450683044]
	TIME [epoch: 1.83 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18962872854604268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18962872854604268 | validation: 0.1767098392080806]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_340.pth
	Model improved!!!
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2075244093507538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2075244093507538 | validation: 0.48129752945158766]
	TIME [epoch: 1.84 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28511965712687665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28511965712687665 | validation: 0.19659642217846884]
	TIME [epoch: 1.84 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14412000855158616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14412000855158616 | validation: 0.2839770864400933]
	TIME [epoch: 1.84 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20423579757299734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20423579757299734 | validation: 0.34432889851612]
	TIME [epoch: 1.84 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40096465898455186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40096465898455186 | validation: 0.43251679329557524]
	TIME [epoch: 1.84 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2284647441806497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2284647441806497 | validation: 0.3715270491472859]
	TIME [epoch: 1.84 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.335242226336254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.335242226336254 | validation: 0.19646937487336516]
	TIME [epoch: 1.84 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2459769644410174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2459769644410174 | validation: 0.4757472781802003]
	TIME [epoch: 1.84 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23559523814477173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23559523814477173 | validation: 0.2464200598854308]
	TIME [epoch: 1.84 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16353077074924866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16353077074924866 | validation: 0.2082690538821355]
	TIME [epoch: 1.84 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18215949208626225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18215949208626225 | validation: 0.3494451214696061]
	TIME [epoch: 1.84 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17037244127997006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17037244127997006 | validation: 0.1711952735031373]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_352.pth
	Model improved!!!
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16312039002136097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16312039002136097 | validation: 0.28222719748962044]
	TIME [epoch: 1.83 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18513657131871106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18513657131871106 | validation: 0.33438079196391074]
	TIME [epoch: 1.84 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23499958053473102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23499958053473102 | validation: 0.35164621673584423]
	TIME [epoch: 1.84 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20643643903551717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20643643903551717 | validation: 0.20004758932441868]
	TIME [epoch: 1.85 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14345777730248524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14345777730248524 | validation: 0.1885291869279624]
	TIME [epoch: 1.83 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13051878704026426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13051878704026426 | validation: 0.17380635299159078]
	TIME [epoch: 1.84 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14113565334534806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14113565334534806 | validation: 0.18648503696315571]
	TIME [epoch: 1.83 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11552901380904118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11552901380904118 | validation: 0.20145660109845265]
	TIME [epoch: 1.83 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11601600096239988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11601600096239988 | validation: 0.21781414574988356]
	TIME [epoch: 1.83 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1484185020025184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1484185020025184 | validation: 0.3346922850622188]
	TIME [epoch: 1.83 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22586208519753054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22586208519753054 | validation: 0.259342254568565]
	TIME [epoch: 1.84 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.257135570533208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.257135570533208 | validation: 0.3280785562573247]
	TIME [epoch: 1.83 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2024619717925863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2024619717925863 | validation: 0.14261608203542808]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_365.pth
	Model improved!!!
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09869664089588037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09869664089588037 | validation: 0.23080643847083504]
	TIME [epoch: 1.84 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10532977618310792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10532977618310792 | validation: 0.21555426526647634]
	TIME [epoch: 1.84 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1171336100619934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1171336100619934 | validation: 0.29702924156195093]
	TIME [epoch: 1.84 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1336411789827777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1336411789827777 | validation: 0.16596806625973726]
	TIME [epoch: 1.84 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19412859864285995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19412859864285995 | validation: 0.5874734996082003]
	TIME [epoch: 1.84 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4853735631461811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4853735631461811 | validation: 0.2438562607360003]
	TIME [epoch: 1.84 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2759324587103172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2759324587103172 | validation: 0.15377472023405506]
	TIME [epoch: 1.84 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14934977512691383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14934977512691383 | validation: 0.3413497904513385]
	TIME [epoch: 1.84 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1800821648415767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1800821648415767 | validation: 0.18813710864150268]
	TIME [epoch: 1.83 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11896870306484929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11896870306484929 | validation: 0.18977794778497548]
	TIME [epoch: 1.84 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1501899628542026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1501899628542026 | validation: 0.22901814214315827]
	TIME [epoch: 1.84 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17041809341228079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17041809341228079 | validation: 0.2419809496797282]
	TIME [epoch: 1.84 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13441271285586895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13441271285586895 | validation: 0.25997848239730703]
	TIME [epoch: 1.84 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17490350819989392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17490350819989392 | validation: 0.37421819409285395]
	TIME [epoch: 1.85 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1922531106651816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1922531106651816 | validation: 0.16859253881904737]
	TIME [epoch: 1.86 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16105235360055184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16105235360055184 | validation: 0.22420472302851926]
	TIME [epoch: 1.84 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11296866333057909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11296866333057909 | validation: 0.12642484417528585]
	TIME [epoch: 1.85 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_382.pth
	Model improved!!!
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10286833896099797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10286833896099797 | validation: 0.17847586917496053]
	TIME [epoch: 1.84 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08793438654195637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08793438654195637 | validation: 0.14283145819198104]
	TIME [epoch: 1.84 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09546431233855987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09546431233855987 | validation: 0.18246211324902484]
	TIME [epoch: 1.84 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13785936136637703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13785936136637703 | validation: 0.34364751453654246]
	TIME [epoch: 1.84 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23763540749366596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23763540749366596 | validation: 0.2458969733438731]
	TIME [epoch: 1.84 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2081891490880758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2081891490880758 | validation: 0.272638924184166]
	TIME [epoch: 1.84 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14890564449044005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14890564449044005 | validation: 0.15790653968939872]
	TIME [epoch: 1.84 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07918029927124388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07918029927124388 | validation: 0.14904286684365836]
	TIME [epoch: 1.84 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07124971757315907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07124971757315907 | validation: 0.18022496278331657]
	TIME [epoch: 1.84 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09272800980750477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09272800980750477 | validation: 0.15847263298140313]
	TIME [epoch: 1.84 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1600373983835546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1600373983835546 | validation: 0.35703382919284776]
	TIME [epoch: 1.84 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21071879166931176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21071879166931176 | validation: 0.18923446452357504]
	TIME [epoch: 1.84 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22646387389146644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22646387389146644 | validation: 0.31738534793325773]
	TIME [epoch: 1.84 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15251000165718623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15251000165718623 | validation: 0.27794855241964933]
	TIME [epoch: 1.84 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17546272766666637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17546272766666637 | validation: 0.23692078232543828]
	TIME [epoch: 1.84 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12217411980843496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12217411980843496 | validation: 0.12964160359634078]
	TIME [epoch: 1.84 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08421007032968136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08421007032968136 | validation: 0.22384041354251913]
	TIME [epoch: 1.84 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12616078832146052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12616078832146052 | validation: 0.17403307370310916]
	TIME [epoch: 1.84 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16672056645669764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16672056645669764 | validation: 0.30937210350209404]
	TIME [epoch: 1.84 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16978929870689272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16978929870689272 | validation: 0.16251438400332355]
	TIME [epoch: 1.83 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09543157685608696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09543157685608696 | validation: 0.13996382437310464]
	TIME [epoch: 1.84 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13106296976292112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13106296976292112 | validation: 0.34046524424763197]
	TIME [epoch: 1.84 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1829194571394276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1829194571394276 | validation: 0.17269651777687992]
	TIME [epoch: 1.85 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20080404270898872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20080404270898872 | validation: 0.2605731258989374]
	TIME [epoch: 1.84 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14963619392630648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14963619392630648 | validation: 0.2775892334594569]
	TIME [epoch: 1.84 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14684849019391766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14684849019391766 | validation: 0.20011633067950602]
	TIME [epoch: 1.84 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08073016401868553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08073016401868553 | validation: 0.1302620637554667]
	TIME [epoch: 1.84 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07431852233117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07431852233117 | validation: 0.19683379022613876]
	TIME [epoch: 1.83 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09873256373212773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09873256373212773 | validation: 0.15106453856928065]
	TIME [epoch: 1.84 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15359965880493925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15359965880493925 | validation: 0.1745176640608529]
	TIME [epoch: 1.84 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09221560019235217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09221560019235217 | validation: 0.18010758635404864]
	TIME [epoch: 1.84 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09641000769640863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09641000769640863 | validation: 0.19427294862401384]
	TIME [epoch: 1.84 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14201824412601757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14201824412601757 | validation: 0.20077630023517729]
	TIME [epoch: 1.83 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16334347589720102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16334347589720102 | validation: 0.159738934508091]
	TIME [epoch: 1.83 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14255892248018612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14255892248018612 | validation: 0.33964294407250933]
	TIME [epoch: 1.84 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18138286248913577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18138286248913577 | validation: 0.11921639061013084]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_418.pth
	Model improved!!!
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08790733260281536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08790733260281536 | validation: 0.15484957168450889]
	TIME [epoch: 1.83 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06378365070512122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06378365070512122 | validation: 0.14638788338759584]
	TIME [epoch: 1.83 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06956757514425126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06956757514425126 | validation: 0.19702300630322417]
	TIME [epoch: 1.83 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11952797488780065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11952797488780065 | validation: 0.28656103188264437]
	TIME [epoch: 1.83 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21953664284712104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21953664284712104 | validation: 0.31137696991600683]
	TIME [epoch: 1.83 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1756575418904498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1756575418904498 | validation: 0.15059855372125863]
	TIME [epoch: 1.83 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16603647676998864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16603647676998864 | validation: 0.32910464434414133]
	TIME [epoch: 1.83 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19256544018540744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19256544018540744 | validation: 0.10279185644639761]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_426.pth
	Model improved!!!
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10800171697426166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10800171697426166 | validation: 0.13615874297059669]
	TIME [epoch: 1.84 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05984366607385199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05984366607385199 | validation: 0.17931220669396253]
	TIME [epoch: 1.83 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06444393196101758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06444393196101758 | validation: 0.11657826961551852]
	TIME [epoch: 1.85 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0676509552495992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0676509552495992 | validation: 0.22043405664391147]
	TIME [epoch: 1.83 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09312952077120241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09312952077120241 | validation: 0.12931061242642147]
	TIME [epoch: 1.83 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07757274551698548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07757274551698548 | validation: 0.20303807552807626]
	TIME [epoch: 1.84 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11379279923507292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11379279923507292 | validation: 0.2335068289586889]
	TIME [epoch: 1.84 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18841138218887943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18841138218887943 | validation: 0.22091263507783465]
	TIME [epoch: 1.84 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24486780871255634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24486780871255634 | validation: 0.19584076669887582]
	TIME [epoch: 1.83 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08059224428764385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08059224428764385 | validation: 0.12128511165540408]
	TIME [epoch: 1.84 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056046865017681066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056046865017681066 | validation: 0.12219847079354679]
	TIME [epoch: 1.84 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07534129493440235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07534129493440235 | validation: 0.15873404142256645]
	TIME [epoch: 1.84 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10768421125994529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10768421125994529 | validation: 0.2169994935204847]
	TIME [epoch: 1.84 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14406579543829845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14406579543829845 | validation: 0.17987391816340415]
	TIME [epoch: 1.84 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16453187696474167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16453187696474167 | validation: 0.3472132007291997]
	TIME [epoch: 1.84 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1790974272300615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1790974272300615 | validation: 0.11951915235887559]
	TIME [epoch: 1.84 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11716342666287378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11716342666287378 | validation: 0.17955376890001495]
	TIME [epoch: 1.84 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11533204526941107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11533204526941107 | validation: 0.1826250396606278]
	TIME [epoch: 1.84 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14313647383690314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14313647383690314 | validation: 0.24373432964905806]
	TIME [epoch: 1.84 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1268859185542437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1268859185542437 | validation: 0.1986344682367308]
	TIME [epoch: 1.84 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10184343852657915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10184343852657915 | validation: 0.14925503580547725]
	TIME [epoch: 1.83 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06375896196067432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06375896196067432 | validation: 0.14533237900760884]
	TIME [epoch: 1.84 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06406710035822853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06406710035822853 | validation: 0.08790141270003948]
	TIME [epoch: 1.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_449.pth
	Model improved!!!
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05243606894336142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05243606894336142 | validation: 0.1514418139164303]
	TIME [epoch: 1.84 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07736754872729976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07736754872729976 | validation: 0.09965611298782982]
	TIME [epoch: 1.84 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1373980305941947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1373980305941947 | validation: 0.3513767738127331]
	TIME [epoch: 1.83 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20351107375001293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20351107375001293 | validation: 0.14355634353675453]
	TIME [epoch: 1.83 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0993581798580129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0993581798580129 | validation: 0.24463930763741157]
	TIME [epoch: 1.84 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1470001906082383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1470001906082383 | validation: 0.33975656633319523]
	TIME [epoch: 1.83 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17125409879271453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17125409879271453 | validation: 0.212059296952374]
	TIME [epoch: 1.84 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14198213267470572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14198213267470572 | validation: 0.13282723197670585]
	TIME [epoch: 1.84 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15288249746644864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15288249746644864 | validation: 0.1963344477691988]
	TIME [epoch: 1.83 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09146059934478863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09146059934478863 | validation: 0.13203274552420827]
	TIME [epoch: 1.84 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0549044903565853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0549044903565853 | validation: 0.11586703724992947]
	TIME [epoch: 1.84 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04166747551326538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04166747551326538 | validation: 0.10983767377085947]
	TIME [epoch: 1.84 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042971896938294556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042971896938294556 | validation: 0.12667845331838465]
	TIME [epoch: 1.82 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05533478944937963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05533478944937963 | validation: 0.16024201783521927]
	TIME [epoch: 1.83 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09923596730783413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09923596730783413 | validation: 0.27632684008735836]
	TIME [epoch: 1.83 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1854881779772392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1854881779772392 | validation: 0.19264271224855312]
	TIME [epoch: 1.83 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18454970550817856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18454970550817856 | validation: 0.19198082682046175]
	TIME [epoch: 1.83 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1002303940863164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1002303940863164 | validation: 0.11201479939041613]
	TIME [epoch: 1.84 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1302558769242786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1302558769242786 | validation: 0.32814229791956423]
	TIME [epoch: 1.84 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2245470464765493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2245470464765493 | validation: 0.09544568726731256]
	TIME [epoch: 1.84 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08094587274574948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08094587274574948 | validation: 0.11411349876621912]
	TIME [epoch: 1.83 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0509874080069058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0509874080069058 | validation: 0.18622536372259416]
	TIME [epoch: 1.83 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06861140667796513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06861140667796513 | validation: 0.098046251095457]
	TIME [epoch: 1.83 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06065883589177183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06065883589177183 | validation: 0.1421115725987138]
	TIME [epoch: 1.83 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05437663892731157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05437663892731157 | validation: 0.09376555952514498]
	TIME [epoch: 1.84 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056066578916867046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056066578916867046 | validation: 0.14369736145873044]
	TIME [epoch: 1.84 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06320839651384097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06320839651384097 | validation: 0.12591759880550324]
	TIME [epoch: 1.83 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0979303284815582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0979303284815582 | validation: 0.19890894676509294]
	TIME [epoch: 1.84 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13189116229456183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13189116229456183 | validation: 0.22240570563103002]
	TIME [epoch: 1.84 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17604085193588723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17604085193588723 | validation: 0.14678023899290568]
	TIME [epoch: 1.85 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12614359653032892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12614359653032892 | validation: 0.19270886861100722]
	TIME [epoch: 1.84 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09738992147959447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09738992147959447 | validation: 0.12631122984640128]
	TIME [epoch: 1.84 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11875668530547845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11875668530547845 | validation: 0.2710100605905154]
	TIME [epoch: 1.84 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1434277740943959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1434277740943959 | validation: 0.14918542673288224]
	TIME [epoch: 1.84 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10317981857920089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10317981857920089 | validation: 0.14439689919488827]
	TIME [epoch: 1.84 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08321302694943494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08321302694943494 | validation: 0.24501467643914823]
	TIME [epoch: 1.84 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11382329354455338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11382329354455338 | validation: 0.08464319602649097]
	TIME [epoch: 1.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_486.pth
	Model improved!!!
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06078760810002228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06078760810002228 | validation: 0.11925304566989525]
	TIME [epoch: 1.83 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06006727007714793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06006727007714793 | validation: 0.09638086391937906]
	TIME [epoch: 1.84 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06946842675543159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06946842675543159 | validation: 0.13948402665627843]
	TIME [epoch: 1.84 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08207222507833904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08207222507833904 | validation: 0.12645685599269782]
	TIME [epoch: 1.84 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09075923587218547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09075923587218547 | validation: 0.16397273370859336]
	TIME [epoch: 1.83 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09778543174637239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09778543174637239 | validation: 0.1364079051646674]
	TIME [epoch: 1.84 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10229743651839951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10229743651839951 | validation: 0.25163419253017644]
	TIME [epoch: 1.84 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11714521954736289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11714521954736289 | validation: 0.10631149549015668]
	TIME [epoch: 1.84 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058762565887362755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058762565887362755 | validation: 0.08890001550107657]
	TIME [epoch: 1.84 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05903552023940531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05903552023940531 | validation: 0.15309811776254245]
	TIME [epoch: 1.83 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08139304983167378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08139304983167378 | validation: 0.11917386590722363]
	TIME [epoch: 1.84 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11071088015411748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11071088015411748 | validation: 0.19508059118758414]
	TIME [epoch: 1.84 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10880200566310044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10880200566310044 | validation: 0.1370058647859372]
	TIME [epoch: 1.84 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11427903023627653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11427903023627653 | validation: 0.2590902207864307]
	TIME [epoch: 1.84 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14661424094317854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14661424094317854 | validation: 0.4054570904684335]
	TIME [epoch: 45.6 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1876695360785175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1876695360785175 | validation: 0.10610385665507663]
	TIME [epoch: 3.64 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0388471024678905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0388471024678905 | validation: 0.12513223275013724]
	TIME [epoch: 3.63 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08714534624914257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08714534624914257 | validation: 0.2805832610579188]
	TIME [epoch: 3.64 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11957410343682245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11957410343682245 | validation: 0.12596593409603343]
	TIME [epoch: 3.63 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13287181232802023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13287181232802023 | validation: 0.14409091437432126]
	TIME [epoch: 3.64 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09655215302792414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09655215302792414 | validation: 0.13506151155306317]
	TIME [epoch: 3.64 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06711412103126423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06711412103126423 | validation: 0.11281022100696961]
	TIME [epoch: 3.63 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053240398834378554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053240398834378554 | validation: 0.08666497574455098]
	TIME [epoch: 3.64 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03747006519928429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03747006519928429 | validation: 0.12201873338472802]
	TIME [epoch: 3.63 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04774751977236728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04774751977236728 | validation: 0.11690684614767527]
	TIME [epoch: 3.63 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06698301692560976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06698301692560976 | validation: 0.25376138265477033]
	TIME [epoch: 3.64 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14752652118359838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14752652118359838 | validation: 0.1970753256128064]
	TIME [epoch: 3.63 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2539524244232645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2539524244232645 | validation: 0.16018067404292347]
	TIME [epoch: 3.63 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08710053913474973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08710053913474973 | validation: 0.08102591350316186]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_515.pth
	Model improved!!!
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04789825093494203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04789825093494203 | validation: 0.07573307427199444]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_516.pth
	Model improved!!!
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03230462759011899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03230462759011899 | validation: 0.08490615758573786]
	TIME [epoch: 3.63 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030434848431731353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030434848431731353 | validation: 0.05897212205275951]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_518.pth
	Model improved!!!
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041289562469525365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041289562469525365 | validation: 0.147902792743159]
	TIME [epoch: 3.64 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08326515232391189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08326515232391189 | validation: 0.12825758150793115]
	TIME [epoch: 3.64 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1566372782132974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1566372782132974 | validation: 0.3025777431742218]
	TIME [epoch: 3.63 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18810694497493674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18810694497493674 | validation: 0.15905545339060545]
	TIME [epoch: 3.63 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09917580357138756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09917580357138756 | validation: 0.13889162658961005]
	TIME [epoch: 3.64 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11205352890295377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11205352890295377 | validation: 0.2177579056685869]
	TIME [epoch: 3.63 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08650204541077887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08650204541077887 | validation: 0.11000769840089607]
	TIME [epoch: 3.63 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05079493376306038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05079493376306038 | validation: 0.08556040835668116]
	TIME [epoch: 3.63 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046252752887440435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046252752887440435 | validation: 0.16466085760539761]
	TIME [epoch: 3.63 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.076529792030106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.076529792030106 | validation: 0.2409213647994526]
	TIME [epoch: 3.63 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15585888423736768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15585888423736768 | validation: 0.21637622588729882]
	TIME [epoch: 3.63 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13519095362435332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13519095362435332 | validation: 0.1292586795848507]
	TIME [epoch: 3.63 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07649587389880645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07649587389880645 | validation: 0.09235038844073967]
	TIME [epoch: 3.63 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04446470260720421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04446470260720421 | validation: 0.09041547438964359]
	TIME [epoch: 3.63 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04878602271294232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04878602271294232 | validation: 0.09838817160969468]
	TIME [epoch: 3.63 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09022477340920106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09022477340920106 | validation: 0.17278535381537252]
	TIME [epoch: 3.65 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15380541085235963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15380541085235963 | validation: 0.104860091686774]
	TIME [epoch: 3.63 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08869448550529857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08869448550529857 | validation: 0.09915743734070454]
	TIME [epoch: 3.63 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041528794490004926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041528794490004926 | validation: 0.10219786235322277]
	TIME [epoch: 3.64 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04272972483357712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04272972483357712 | validation: 0.14747688925057098]
	TIME [epoch: 3.63 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07185985706637872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07185985706637872 | validation: 0.20116276299963307]
	TIME [epoch: 3.63 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13364363071565816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13364363071565816 | validation: 0.19830114668906562]
	TIME [epoch: 3.63 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13846938312264256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13846938312264256 | validation: 0.10788824739469074]
	TIME [epoch: 3.63 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10372722569492529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10372722569492529 | validation: 0.17519297477483842]
	TIME [epoch: 3.63 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08351499448191003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08351499448191003 | validation: 0.08912316949026411]
	TIME [epoch: 3.63 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06455390670911147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06455390670911147 | validation: 0.09478961178655983]
	TIME [epoch: 3.63 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058299625623088255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058299625623088255 | validation: 0.0642825615986231]
	TIME [epoch: 3.63 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05670664886969475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05670664886969475 | validation: 0.0967999491903268]
	TIME [epoch: 3.63 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06344229216196029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06344229216196029 | validation: 0.09402217774214855]
	TIME [epoch: 3.64 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07808352471255109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07808352471255109 | validation: 0.15569738456967572]
	TIME [epoch: 3.64 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10248589485358336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10248589485358336 | validation: 0.09446792839265053]
	TIME [epoch: 3.63 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059306962865027756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059306962865027756 | validation: 0.07495720034603283]
	TIME [epoch: 3.63 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05000524565908615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05000524565908615 | validation: 0.13477272572400248]
	TIME [epoch: 3.63 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0551530997150828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0551530997150828 | validation: 0.17057800204887502]
	TIME [epoch: 3.63 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10367065020513155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10367065020513155 | validation: 0.3310561333043331]
	TIME [epoch: 3.62 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1845848512296644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1845848512296644 | validation: 0.2119301370257162]
	TIME [epoch: 3.63 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18158198345257381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18158198345257381 | validation: 0.13036094538342052]
	TIME [epoch: 3.63 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07576169201631458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07576169201631458 | validation: 0.056979068373700086]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_556.pth
	Model improved!!!
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04160723486831992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04160723486831992 | validation: 0.09848403222135796]
	TIME [epoch: 3.63 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043884264880584105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043884264880584105 | validation: 0.07969676308747704]
	TIME [epoch: 3.63 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04944599993051274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04944599993051274 | validation: 0.12411963825785499]
	TIME [epoch: 3.63 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06987258485294659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06987258485294659 | validation: 0.10086193890294129]
	TIME [epoch: 3.64 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07436900627126367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07436900627126367 | validation: 0.1011183076192963]
	TIME [epoch: 3.64 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08686629763779788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08686629763779788 | validation: 0.12212903743871878]
	TIME [epoch: 3.63 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07209753647271476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07209753647271476 | validation: 0.10901650254905625]
	TIME [epoch: 3.63 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06744160244046313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06744160244046313 | validation: 0.13539520318866205]
	TIME [epoch: 3.64 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06348424712909581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06348424712909581 | validation: 0.08393713710011602]
	TIME [epoch: 3.63 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061281002547913487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061281002547913487 | validation: 0.13050681707355913]
	TIME [epoch: 3.63 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07431948159789212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07431948159789212 | validation: 0.12801435750078505]
	TIME [epoch: 3.63 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11791844418831264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11791844418831264 | validation: 0.22309133145738846]
	TIME [epoch: 3.63 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14429424714165887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14429424714165887 | validation: 0.1481494314034173]
	TIME [epoch: 3.63 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1188434472762199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1188434472762199 | validation: 0.11953649968951506]
	TIME [epoch: 3.63 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0805265875545145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0805265875545145 | validation: 0.06938202171881078]
	TIME [epoch: 3.63 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0642061309255608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0642061309255608 | validation: 0.14095507608708507]
	TIME [epoch: 3.63 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06924493200071499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06924493200071499 | validation: 0.09504917072456855]
	TIME [epoch: 3.64 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04936362896646315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04936362896646315 | validation: 0.07746825463817394]
	TIME [epoch: 3.64 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04852329793371693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04852329793371693 | validation: 0.09727283500220629]
	TIME [epoch: 3.65 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0453226594053208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0453226594053208 | validation: 0.11278697672219368]
	TIME [epoch: 3.65 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049680000114717696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049680000114717696 | validation: 0.13074363072313172]
	TIME [epoch: 3.63 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058450823884113455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058450823884113455 | validation: 0.12413154860714148]
	TIME [epoch: 3.64 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06328303149266051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06328303149266051 | validation: 0.11641027832251175]
	TIME [epoch: 3.64 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04898634855448855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04898634855448855 | validation: 0.06935010750059027]
	TIME [epoch: 3.63 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07244445294572308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07244445294572308 | validation: 0.17385508492596957]
	TIME [epoch: 3.63 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15137536548368608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15137536548368608 | validation: 0.16027044985492261]
	TIME [epoch: 3.63 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19011589484665536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19011589484665536 | validation: 0.27476128187308774]
	TIME [epoch: 3.63 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11802401158054196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11802401158054196 | validation: 0.11446566755873278]
	TIME [epoch: 3.63 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05928993836485448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05928993836485448 | validation: 0.11200413091423839]
	TIME [epoch: 3.63 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08836403500488668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08836403500488668 | validation: 0.12075884026861022]
	TIME [epoch: 3.63 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060846211361610156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060846211361610156 | validation: 0.10155781004800989]
	TIME [epoch: 3.64 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05533871631471527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05533871631471527 | validation: 0.09444100629150216]
	TIME [epoch: 3.64 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0676087125956554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0676087125956554 | validation: 0.1754505155590688]
	TIME [epoch: 3.64 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09209521838103259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09209521838103259 | validation: 0.17301023166385976]
	TIME [epoch: 3.64 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10380512960752877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10380512960752877 | validation: 0.1320318726647874]
	TIME [epoch: 3.63 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08452324876191807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08452324876191807 | validation: 0.08983355699825538]
	TIME [epoch: 3.64 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0416593998511917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0416593998511917 | validation: 0.05723433523333815]
	TIME [epoch: 3.64 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031415704430044225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031415704430044225 | validation: 0.07661934460962422]
	TIME [epoch: 3.64 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050904675602760956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050904675602760956 | validation: 0.12936785354058875]
	TIME [epoch: 3.63 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10962529974360524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10962529974360524 | validation: 0.1858015826063975]
	TIME [epoch: 3.63 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14896073077545768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14896073077545768 | validation: 0.1257753108558135]
	TIME [epoch: 3.63 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08778697292583068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08778697292583068 | validation: 0.07481925103985658]
	TIME [epoch: 3.63 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03431247537618119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03431247537618119 | validation: 0.06387389237331996]
	TIME [epoch: 3.64 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024589968809358362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024589968809358362 | validation: 0.05541805344550754]
	TIME [epoch: 3.62 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_600.pth
	Model improved!!!
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025752497049278355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025752497049278355 | validation: 0.06575386725303915]
	TIME [epoch: 3.65 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03513380160946792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03513380160946792 | validation: 0.057829195263736394]
	TIME [epoch: 3.65 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06562174705644151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06562174705644151 | validation: 0.18008841163446349]
	TIME [epoch: 3.63 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13307856474636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13307856474636 | validation: 0.17444629688692204]
	TIME [epoch: 3.63 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13964928948894106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13964928948894106 | validation: 0.22219410598524894]
	TIME [epoch: 3.64 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11262231347342706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11262231347342706 | validation: 0.150406706512167]
	TIME [epoch: 3.63 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08155254432182557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08155254432182557 | validation: 0.08322846014793489]
	TIME [epoch: 3.63 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05635573768453957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05635573768453957 | validation: 0.06854881875427542]
	TIME [epoch: 3.63 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06627326841590422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06627326841590422 | validation: 0.10049424539075527]
	TIME [epoch: 3.63 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07947615791350382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07947615791350382 | validation: 0.08394219168183796]
	TIME [epoch: 3.63 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07106536990550179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07106536990550179 | validation: 0.1991368174046254]
	TIME [epoch: 3.63 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08631307671651275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08631307671651275 | validation: 0.07477885196219317]
	TIME [epoch: 3.63 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03510483895355421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03510483895355421 | validation: 0.07872976399941939]
	TIME [epoch: 3.63 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03401277391700454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03401277391700454 | validation: 0.05874931073422377]
	TIME [epoch: 3.63 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02537514657121928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02537514657121928 | validation: 0.05933830744484821]
	TIME [epoch: 3.64 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030748566206460312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030748566206460312 | validation: 0.1475850859978296]
	TIME [epoch: 3.64 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07507225608790943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07507225608790943 | validation: 0.2117886006344319]
	TIME [epoch: 3.63 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16525264838582288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16525264838582288 | validation: 0.2167902241866374]
	TIME [epoch: 3.63 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1293802472758657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1293802472758657 | validation: 0.0786654822052814]
	TIME [epoch: 3.63 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07865280771957203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07865280771957203 | validation: 0.10589730143188994]
	TIME [epoch: 3.63 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07976493133328953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07976493133328953 | validation: 0.0710508327057326]
	TIME [epoch: 3.63 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07242305834913594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07242305834913594 | validation: 0.12541211320044648]
	TIME [epoch: 3.63 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05410203733942482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05410203733942482 | validation: 0.06643119410521071]
	TIME [epoch: 3.63 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044614310710797016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044614310710797016 | validation: 0.09748962951055412]
	TIME [epoch: 3.63 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039138175823097344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039138175823097344 | validation: 0.05015252316686586]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_625.pth
	Model improved!!!
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030130230318498357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030130230318498357 | validation: 0.07741469190304578]
	TIME [epoch: 3.63 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03358962449029703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03358962449029703 | validation: 0.09355939192267376]
	TIME [epoch: 3.63 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08243491599630971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08243491599630971 | validation: 0.17399179103253448]
	TIME [epoch: 3.63 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1612543221680314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1612543221680314 | validation: 0.13908209016859865]
	TIME [epoch: 3.64 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12053043026246787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12053043026246787 | validation: 0.11456987332652867]
	TIME [epoch: 3.64 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06087115932264577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06087115932264577 | validation: 0.12426467247523108]
	TIME [epoch: 3.64 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06589701462525156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06589701462525156 | validation: 0.1388375622728106]
	TIME [epoch: 3.62 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10310729978343386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10310729978343386 | validation: 0.152497821877222]
	TIME [epoch: 3.63 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08608761798643133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08608761798643133 | validation: 0.09933754691281908]
	TIME [epoch: 3.63 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06529711937590128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06529711937590128 | validation: 0.12002278818797246]
	TIME [epoch: 3.63 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056032034573563386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056032034573563386 | validation: 0.052853427483538384]
	TIME [epoch: 3.63 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042533025273371106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042533025273371106 | validation: 0.08625685939231478]
	TIME [epoch: 3.63 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06053694730014763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06053694730014763 | validation: 0.05414182702889434]
	TIME [epoch: 3.63 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06363986846178624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06363986846178624 | validation: 0.061186337447991394]
	TIME [epoch: 3.63 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0413306950514693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0413306950514693 | validation: 0.06927724530088407]
	TIME [epoch: 3.62 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03464709371522931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03464709371522931 | validation: 0.09937003136446898]
	TIME [epoch: 3.63 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06000368705469261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06000368705469261 | validation: 0.2060898135915652]
	TIME [epoch: 3.64 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10999889797333903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10999889797333903 | validation: 0.18272994299201548]
	TIME [epoch: 3.64 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10864173197341823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10864173197341823 | validation: 0.09930094600319778]
	TIME [epoch: 3.63 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045306329666887565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045306329666887565 | validation: 0.06579186273227781]
	TIME [epoch: 3.62 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06008774569337463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06008774569337463 | validation: 0.16433886036255144]
	TIME [epoch: 3.63 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10135003568528675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10135003568528675 | validation: 0.0680253179418369]
	TIME [epoch: 3.63 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07812286770366496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07812286770366496 | validation: 0.08755180527547309]
	TIME [epoch: 3.63 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049809254930885734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049809254930885734 | validation: 0.04794503491053514]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_649.pth
	Model improved!!!
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052944961503862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052944961503862 | validation: 0.09289847891012439]
	TIME [epoch: 3.63 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07034582398242072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07034582398242072 | validation: 0.14807265363963157]
	TIME [epoch: 3.63 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09101565233542175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09101565233542175 | validation: 0.13845469401938196]
	TIME [epoch: 3.63 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09504723699238293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09504723699238293 | validation: 0.11201391877134058]
	TIME [epoch: 3.63 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07280741241536229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07280741241536229 | validation: 0.06086534072026106]
	TIME [epoch: 3.63 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05318113889435055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05318113889435055 | validation: 0.0754712375075351]
	TIME [epoch: 3.63 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03837546970853376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03837546970853376 | validation: 0.07879839631389897]
	TIME [epoch: 3.64 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038956068511574966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038956068511574966 | validation: 0.11694103758456015]
	TIME [epoch: 3.64 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05309993684562919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05309993684562919 | validation: 0.12809587376739806]
	TIME [epoch: 3.63 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07657193205408716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07657193205408716 | validation: 0.12790446556901736]
	TIME [epoch: 3.63 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06857044146806578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06857044146806578 | validation: 0.07924606997980416]
	TIME [epoch: 3.63 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08589852461175763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08589852461175763 | validation: 0.12353478460010808]
	TIME [epoch: 3.63 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08545392170195867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08545392170195867 | validation: 0.07936422790779868]
	TIME [epoch: 3.63 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06978720415401565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06978720415401565 | validation: 0.09131823046721368]
	TIME [epoch: 3.63 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0449154024130419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0449154024130419 | validation: 0.0530634498877573]
	TIME [epoch: 3.63 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04485263910188793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04485263910188793 | validation: 0.11064359550293207]
	TIME [epoch: 3.63 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0835193770311182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0835193770311182 | validation: 0.0743547474005802]
	TIME [epoch: 3.63 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07837243653354561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07837243653354561 | validation: 0.08782050779819407]
	TIME [epoch: 3.62 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06267322770116594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06267322770116594 | validation: 0.09278574081088468]
	TIME [epoch: 3.63 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05107595242476485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05107595242476485 | validation: 0.050275763776360796]
	TIME [epoch: 3.63 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046572192091673896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046572192091673896 | validation: 0.14228610294809116]
	TIME [epoch: 3.64 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06489539001156334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06489539001156334 | validation: 0.267930462700008]
	TIME [epoch: 3.64 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13977484125598658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13977484125598658 | validation: 0.15708436457113717]
	TIME [epoch: 3.64 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08847163369794302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08847163369794302 | validation: 0.0814309514874585]
	TIME [epoch: 3.63 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06355346773540087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06355346773540087 | validation: 0.07595353394338564]
	TIME [epoch: 3.64 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04583128763963781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04583128763963781 | validation: 0.050717692602062894]
	TIME [epoch: 3.63 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03090410920425325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03090410920425325 | validation: 0.038915112943693]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_676.pth
	Model improved!!!
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023408881145078428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023408881145078428 | validation: 0.07260801073852487]
	TIME [epoch: 3.62 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033263956631876646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033263956631876646 | validation: 0.09690658551657506]
	TIME [epoch: 3.63 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07538196989331426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07538196989331426 | validation: 0.11004764464001991]
	TIME [epoch: 3.63 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.076062925822124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.076062925822124 | validation: 0.09210267367665599]
	TIME [epoch: 3.63 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08222202834786912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08222202834786912 | validation: 0.09106371039320234]
	TIME [epoch: 3.62 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06444398674826812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06444398674826812 | validation: 0.10860421478647338]
	TIME [epoch: 3.63 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05888458866643741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05888458866643741 | validation: 0.1311300695939546]
	TIME [epoch: 3.64 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.068771243199745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.068771243199745 | validation: 0.0999554091041655]
	TIME [epoch: 3.65 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09516875237543339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09516875237543339 | validation: 0.1505241028758687]
	TIME [epoch: 3.63 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12954005586802095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12954005586802095 | validation: 0.08908777115785765]
	TIME [epoch: 3.63 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08718462579011846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08718462579011846 | validation: 0.08958047470177444]
	TIME [epoch: 3.63 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037543031708945095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037543031708945095 | validation: 0.09063257499361037]
	TIME [epoch: 3.63 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03753552242441519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03753552242441519 | validation: 0.062408505100996714]
	TIME [epoch: 3.63 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0429232949236601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0429232949236601 | validation: 0.126481489512231]
	TIME [epoch: 3.63 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06271614677729108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06271614677729108 | validation: 0.07113371089968186]
	TIME [epoch: 3.62 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05265784993709101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05265784993709101 | validation: 0.08897569631168517]
	TIME [epoch: 3.63 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040513086299633655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040513086299633655 | validation: 0.06401090542834041]
	TIME [epoch: 3.63 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03216479851554089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03216479851554089 | validation: 0.057851290492667774]
	TIME [epoch: 3.63 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04750290900120007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04750290900120007 | validation: 0.1449000957390598]
	TIME [epoch: 3.63 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09522962959526683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09522962959526683 | validation: 0.11343739910289763]
	TIME [epoch: 3.64 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09579572484323937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09579572484323937 | validation: 0.10310628141081009]
	TIME [epoch: 3.64 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0684243658416205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0684243658416205 | validation: 0.08866181646125393]
	TIME [epoch: 3.64 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07956283669273642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07956283669273642 | validation: 0.05438315854722225]
	TIME [epoch: 3.64 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04757806404400022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04757806404400022 | validation: 0.05337681120972268]
	TIME [epoch: 3.63 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03166215426828285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03166215426828285 | validation: 0.07572549530984642]
	TIME [epoch: 3.65 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03697737905902399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03697737905902399 | validation: 0.12055274326644516]
	TIME [epoch: 3.64 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0865135714510797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0865135714510797 | validation: 0.18441686051461778]
	TIME [epoch: 3.64 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14251282300915322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14251282300915322 | validation: 0.1106769565413533]
	TIME [epoch: 3.64 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07499628264138108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07499628264138108 | validation: 0.05028250664887706]
	TIME [epoch: 3.64 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03417000853786677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03417000853786677 | validation: 0.07619471227554026]
	TIME [epoch: 3.64 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034282400488289136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034282400488289136 | validation: 0.04034014287604346]
	TIME [epoch: 3.64 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03768459357350405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03768459357350405 | validation: 0.07714101546070397]
	TIME [epoch: 3.64 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038814385200239755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038814385200239755 | validation: 0.04706212444054249]
	TIME [epoch: 3.64 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04893449903017572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04893449903017572 | validation: 0.1314718076921315]
	TIME [epoch: 3.64 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08005676693190493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08005676693190493 | validation: 0.11266313866988412]
	TIME [epoch: 3.65 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0950139033698899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0950139033698899 | validation: 0.08398893140587421]
	TIME [epoch: 3.64 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041518398871480205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041518398871480205 | validation: 0.029003059007330467]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_713.pth
	Model improved!!!
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016290343016010227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016290343016010227 | validation: 0.0444270503941446]
	TIME [epoch: 3.62 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021629051688409895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021629051688409895 | validation: 0.09309029944393966]
	TIME [epoch: 3.63 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0503095657107208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0503095657107208 | validation: 0.2710261396985433]
	TIME [epoch: 3.62 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16429351083385993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16429351083385993 | validation: 0.21378051390586747]
	TIME [epoch: 3.62 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1732281698487021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1732281698487021 | validation: 0.09202020524916296]
	TIME [epoch: 3.62 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05909230829049266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05909230829049266 | validation: 0.05466355421160962]
	TIME [epoch: 3.63 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030843617985854558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030843617985854558 | validation: 0.12696595723176918]
	TIME [epoch: 3.62 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0672483221417407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0672483221417407 | validation: 0.10496378806232941]
	TIME [epoch: 3.63 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09479568727753203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09479568727753203 | validation: 0.14768024994205423]
	TIME [epoch: 3.62 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.086426202765346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.086426202765346 | validation: 0.05850005960073749]
	TIME [epoch: 3.64 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058005000072257784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058005000072257784 | validation: 0.051188720959327166]
	TIME [epoch: 3.63 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032830027540877896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032830027540877896 | validation: 0.05189131612564474]
	TIME [epoch: 3.63 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021969422155825057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021969422155825057 | validation: 0.05851977524124488]
	TIME [epoch: 3.63 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029198764539517683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029198764539517683 | validation: 0.07598667946065794]
	TIME [epoch: 3.63 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04248176375435735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04248176375435735 | validation: 0.14084454690185616]
	TIME [epoch: 3.62 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0721070755833206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0721070755833206 | validation: 0.1788865587027201]
	TIME [epoch: 3.63 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10006221781014922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10006221781014922 | validation: 0.1025426650701613]
	TIME [epoch: 3.63 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07592246982880384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07592246982880384 | validation: 0.08263746928331325]
	TIME [epoch: 3.62 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09109524745543737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09109524745543737 | validation: 0.12419742835636494]
	TIME [epoch: 3.62 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06430019914104391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06430019914104391 | validation: 0.04925870305925928]
	TIME [epoch: 3.62 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0458566052260133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0458566052260133 | validation: 0.07835904683110002]
	TIME [epoch: 3.63 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04440405398253157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04440405398253157 | validation: 0.02916152961819949]
	TIME [epoch: 3.63 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039415822612080424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039415822612080424 | validation: 0.04189701997587099]
	TIME [epoch: 3.63 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034826648188759364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034826648188759364 | validation: 0.08391006636032942]
	TIME [epoch: 3.64 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05003965535614194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05003965535614194 | validation: 0.13750614083912013]
	TIME [epoch: 3.63 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10113350446893073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10113350446893073 | validation: 0.21225666594644235]
	TIME [epoch: 3.63 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1260712033199797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1260712033199797 | validation: 0.08206672319549152]
	TIME [epoch: 3.62 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053916775272985815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053916775272985815 | validation: 0.047822597043564376]
	TIME [epoch: 3.62 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02023838886327443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02023838886327443 | validation: 0.04263888514274483]
	TIME [epoch: 3.62 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019129624820142622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019129624820142622 | validation: 0.0726535445446823]
	TIME [epoch: 3.62 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031593805817308675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031593805817308675 | validation: 0.06942260896449458]
	TIME [epoch: 3.62 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05446067293691859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05446067293691859 | validation: 0.1363107210461137]
	TIME [epoch: 3.63 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09262396423875802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09262396423875802 | validation: 0.09180527884657075]
	TIME [epoch: 3.62 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10577933030392432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10577933030392432 | validation: 0.1350133071712307]
	TIME [epoch: 3.62 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07384420915233675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07384420915233675 | validation: 0.15363370978743573]
	TIME [epoch: 3.63 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07118332006757681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07118332006757681 | validation: 0.06369724526101503]
	TIME [epoch: 3.63 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043184461878523414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043184461878523414 | validation: 0.08831957923362183]
	TIME [epoch: 3.63 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04041058604926403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04041058604926403 | validation: 0.03522883626767489]
	TIME [epoch: 3.63 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03573633456259806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03573633456259806 | validation: 0.049769191287713746]
	TIME [epoch: 3.63 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04488772073543028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04488772073543028 | validation: 0.08463553556004631]
	TIME [epoch: 3.62 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06293824400083331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06293824400083331 | validation: 0.07268949649862802]
	TIME [epoch: 3.62 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07170708957709174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07170708957709174 | validation: 0.09899661598706781]
	TIME [epoch: 3.62 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06599884905191422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06599884905191422 | validation: 0.07872449409509352]
	TIME [epoch: 3.63 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0739387441613974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0739387441613974 | validation: 0.11582396841243843]
	TIME [epoch: 3.62 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06624044381133574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06624044381133574 | validation: 0.13098406110906805]
	TIME [epoch: 3.63 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06678263458120362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06678263458120362 | validation: 0.1506907235629858]
	TIME [epoch: 3.62 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08258284770798366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08258284770798366 | validation: 0.1260233881787702]
	TIME [epoch: 3.63 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0648815976086059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0648815976086059 | validation: 0.05443496340026569]
	TIME [epoch: 3.62 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03831924622148463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03831924622148463 | validation: 0.07112961035169453]
	TIME [epoch: 3.64 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02772189626765167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02772189626765167 | validation: 0.03552885771734215]
	TIME [epoch: 3.63 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025425092424704712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025425092424704712 | validation: 0.058051082084987604]
	TIME [epoch: 3.62 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037038582632671596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037038582632671596 | validation: 0.06252675345669752]
	TIME [epoch: 3.62 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05846416093412004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05846416093412004 | validation: 0.10317055584214421]
	TIME [epoch: 3.62 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06636825350632428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06636825350632428 | validation: 0.05986315668738036]
	TIME [epoch: 3.62 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05334101087965079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05334101087965079 | validation: 0.09493753752753359]
	TIME [epoch: 3.62 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046033850641375516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046033850641375516 | validation: 0.08900015763828817]
	TIME [epoch: 3.62 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056868105370418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056868105370418 | validation: 0.12151236299768045]
	TIME [epoch: 3.62 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06793553064754967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06793553064754967 | validation: 0.09085191273351334]
	TIME [epoch: 3.62 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06319186543323695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06319186543323695 | validation: 0.07716022304675835]
	TIME [epoch: 3.63 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10404551556760878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10404551556760878 | validation: 0.11208677197731083]
	TIME [epoch: 3.63 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09427547934441471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09427547934441471 | validation: 0.06805781087466027]
	TIME [epoch: 3.63 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061429019556534445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061429019556534445 | validation: 0.09918882257786826]
	TIME [epoch: 3.64 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044110572773622486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044110572773622486 | validation: 0.09263457162628429]
	TIME [epoch: 3.64 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06070622784957424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06070622784957424 | validation: 0.13294179648089374]
	TIME [epoch: 3.62 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08346773029713728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08346773029713728 | validation: 0.11982124876541356]
	TIME [epoch: 3.63 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07800682327908416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07800682327908416 | validation: 0.0675462684574174]
	TIME [epoch: 3.63 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037234930456375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037234930456375 | validation: 0.06233411032092151]
	TIME [epoch: 3.63 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019803802645548957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019803802645548957 | validation: 0.03613319244289511]
	TIME [epoch: 3.63 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016215404940647788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016215404940647788 | validation: 0.04465274868561973]
	TIME [epoch: 3.63 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016753952958491566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016753952958491566 | validation: 0.04577014916027403]
	TIME [epoch: 3.62 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01912364484845353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01912364484845353 | validation: 0.05763788459080084]
	TIME [epoch: 3.62 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030565476419623618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030565476419623618 | validation: 0.07473918257822885]
	TIME [epoch: 3.62 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0499681392347552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0499681392347552 | validation: 0.1205732607993617]
	TIME [epoch: 3.62 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07764792487713355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07764792487713355 | validation: 0.1134761371524983]
	TIME [epoch: 3.62 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12066904872440555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12066904872440555 | validation: 0.17671441907253893]
	TIME [epoch: 3.64 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13430720311292338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13430720311292338 | validation: 0.15522420581953092]
	TIME [epoch: 3.63 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09691244187013512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09691244187013512 | validation: 0.08973415399501705]
	TIME [epoch: 3.63 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07989990366850684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07989990366850684 | validation: 0.09766582395548835]
	TIME [epoch: 3.62 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04146491216180149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04146491216180149 | validation: 0.0884570332272971]
	TIME [epoch: 3.62 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03686975827912024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03686975827912024 | validation: 0.060149242167923944]
	TIME [epoch: 3.62 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04541728906194253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04541728906194253 | validation: 0.10852283686690622]
	TIME [epoch: 3.63 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04733317488768525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04733317488768525 | validation: 0.06060288811263172]
	TIME [epoch: 3.62 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04093350584444283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04093350584444283 | validation: 0.06618712167575101]
	TIME [epoch: 3.63 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04317110255650732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04317110255650732 | validation: 0.04707983904151211]
	TIME [epoch: 3.62 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0562346605087091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0562346605087091 | validation: 0.05984023693403251]
	TIME [epoch: 3.62 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043612560964700446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043612560964700446 | validation: 0.0289690300796963]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_799.pth
	Model improved!!!
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03501749434131579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03501749434131579 | validation: 0.08257594241279086]
	TIME [epoch: 3.64 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0447253103570461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0447253103570461 | validation: 0.06989264282952175]
	TIME [epoch: 3.64 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06099731331249226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06099731331249226 | validation: 0.09595701395163811]
	TIME [epoch: 3.64 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06108234048663943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06108234048663943 | validation: 0.10226889483210289]
	TIME [epoch: 3.62 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0744542996601052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0744542996601052 | validation: 0.16423807425100578]
	TIME [epoch: 3.64 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10613583843365575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10613583843365575 | validation: 0.18439493894408268]
	TIME [epoch: 3.63 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1006517265145888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1006517265145888 | validation: 0.07127408013144361]
	TIME [epoch: 3.63 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05127427334788445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05127427334788445 | validation: 0.0682848538385978]
	TIME [epoch: 3.62 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03159243775146648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03159243775146648 | validation: 0.05000383533748483]
	TIME [epoch: 3.63 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03035519310724989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03035519310724989 | validation: 0.060566462469342996]
	TIME [epoch: 3.63 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029437859662493612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029437859662493612 | validation: 0.04247638002406508]
	TIME [epoch: 3.63 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03228557359807955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03228557359807955 | validation: 0.04709424799539801]
	TIME [epoch: 3.63 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04058617156466914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04058617156466914 | validation: 0.06573583737276306]
	TIME [epoch: 3.63 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046638668156919624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046638668156919624 | validation: 0.04784907476814202]
	TIME [epoch: 3.64 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036996860440965174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036996860440965174 | validation: 0.048575283552961174]
	TIME [epoch: 3.64 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035649765134292445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035649765134292445 | validation: 0.07706115569290728]
	TIME [epoch: 3.63 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06018883084230179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06018883084230179 | validation: 0.12403176869807307]
	TIME [epoch: 3.63 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12167050113739475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12167050113739475 | validation: 0.1099166848011199]
	TIME [epoch: 3.63 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0839704494127514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0839704494127514 | validation: 0.04651548847429469]
	TIME [epoch: 3.62 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023592535070825982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023592535070825982 | validation: 0.06116668211396821]
	TIME [epoch: 3.63 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02055766056462888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02055766056462888 | validation: 0.07710553417489108]
	TIME [epoch: 3.62 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047728064082675443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047728064082675443 | validation: 0.24088791875342183]
	TIME [epoch: 3.63 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13017272569262295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13017272569262295 | validation: 0.21235085985220536]
	TIME [epoch: 3.63 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14504498184104064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14504498184104064 | validation: 0.09655099152320468]
	TIME [epoch: 3.63 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041087153494561726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041087153494561726 | validation: 0.05698955577694745]
	TIME [epoch: 3.63 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02384569595946168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02384569595946168 | validation: 0.033947372387612584]
	TIME [epoch: 3.63 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03331575159426723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03331575159426723 | validation: 0.05966440255037622]
	TIME [epoch: 3.64 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030930461180095587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030930461180095587 | validation: 0.028055330526578704]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_827.pth
	Model improved!!!
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027529578561840608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027529578561840608 | validation: 0.08689694208527107]
	TIME [epoch: 3.64 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04048421821632747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04048421821632747 | validation: 0.1291541021931781]
	TIME [epoch: 3.63 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0846689433918639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0846689433918639 | validation: 0.16297095306140938]
	TIME [epoch: 3.64 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09199691106352535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09199691106352535 | validation: 0.0808050909238591]
	TIME [epoch: 3.63 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05556342767155708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05556342767155708 | validation: 0.05326631861554927]
	TIME [epoch: 3.63 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04287485183501019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04287485183501019 | validation: 0.05842726496702788]
	TIME [epoch: 3.63 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05199654086806109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05199654086806109 | validation: 0.09017081575007382]
	TIME [epoch: 3.63 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062142410961908576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062142410961908576 | validation: 0.08960298486464298]
	TIME [epoch: 3.63 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05692846933351034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05692846933351034 | validation: 0.0474045725016255]
	TIME [epoch: 3.63 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04107951897516947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04107951897516947 | validation: 0.0682878679244818]
	TIME [epoch: 3.63 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02816068926787694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02816068926787694 | validation: 0.06376911824462823]
	TIME [epoch: 3.63 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03804960759719024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03804960759719024 | validation: 0.09966559364982874]
	TIME [epoch: 3.64 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07725596764958802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07725596764958802 | validation: 0.08107586941328322]
	TIME [epoch: 3.65 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09992397787452252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09992397787452252 | validation: 0.10050378481240543]
	TIME [epoch: 3.63 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060936181563662956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060936181563662956 | validation: 0.04938723520669124]
	TIME [epoch: 3.63 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032151159559752215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032151159559752215 | validation: 0.05817050659149542]
	TIME [epoch: 3.63 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02859018197825415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02859018197825415 | validation: 0.05706248503285082]
	TIME [epoch: 3.63 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036734810472847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036734810472847 | validation: 0.09511019563294591]
	TIME [epoch: 3.63 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061231169184042075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061231169184042075 | validation: 0.14258580332511983]
	TIME [epoch: 3.63 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08228006332854816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08228006332854816 | validation: 0.0899489798344245]
	TIME [epoch: 3.63 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06887117894012264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06887117894012264 | validation: 0.06412969605504164]
	TIME [epoch: 3.63 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051724439932588955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051724439932588955 | validation: 0.038869738611367825]
	TIME [epoch: 3.63 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03634797396381178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03634797396381178 | validation: 0.06949660268451878]
	TIME [epoch: 3.63 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0367480695197143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0367480695197143 | validation: 0.05796427849213914]
	TIME [epoch: 3.63 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048948772092840986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048948772092840986 | validation: 0.12072370826751065]
	TIME [epoch: 3.65 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06079967578470585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06079967578470585 | validation: 0.08948016964611155]
	TIME [epoch: 3.65 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05790641683202228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05790641683202228 | validation: 0.08178250910314477]
	TIME [epoch: 3.64 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045092332133251374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045092332133251374 | validation: 0.05916364013407482]
	TIME [epoch: 3.63 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03725298823848883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03725298823848883 | validation: 0.05524959652975073]
	TIME [epoch: 3.64 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03014747996845322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03014747996845322 | validation: 0.05747424679791146]
	TIME [epoch: 3.63 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03855249361791035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03855249361791035 | validation: 0.08324610976016512]
	TIME [epoch: 3.63 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04682228225027602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04682228225027602 | validation: 0.15101318872758182]
	TIME [epoch: 3.64 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0694224646382131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0694224646382131 | validation: 0.08351653401528451]
	TIME [epoch: 3.63 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05267051597237124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05267051597237124 | validation: 0.10927171518189308]
	TIME [epoch: 3.64 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049003651487377925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049003651487377925 | validation: 0.06107836044311432]
	TIME [epoch: 3.63 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056190182374590024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056190182374590024 | validation: 0.08119519258976438]
	TIME [epoch: 3.64 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0625515394237274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0625515394237274 | validation: 0.04649451434355409]
	TIME [epoch: 3.64 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06064907048206861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06064907048206861 | validation: 0.05016449286276096]
	TIME [epoch: 3.64 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04238118394228642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04238118394228642 | validation: 0.07766713338350317]
	TIME [epoch: 3.64 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0436484645494982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0436484645494982 | validation: 0.07325517408885161]
	TIME [epoch: 3.64 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05796518350064015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05796518350064015 | validation: 0.14006478630681374]
	TIME [epoch: 3.61 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07544959315520895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07544959315520895 | validation: 0.10253594230526553]
	TIME [epoch: 3.63 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06783379393712755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06783379393712755 | validation: 0.09634058262704032]
	TIME [epoch: 3.63 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05204593368897271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05204593368897271 | validation: 0.06033819533319851]
	TIME [epoch: 3.63 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02669669182496863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02669669182496863 | validation: 0.03800733796711636]
	TIME [epoch: 3.59 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024278261265519952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024278261265519952 | validation: 0.07679947964711328]
	TIME [epoch: 3.59 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04269972258538109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04269972258538109 | validation: 0.06594877891148836]
	TIME [epoch: 3.6 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06400038646327985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06400038646327985 | validation: 0.11669396880556411]
	TIME [epoch: 3.6 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05847382066953264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05847382066953264 | validation: 0.07340992057472492]
	TIME [epoch: 3.59 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04619523501602237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04619523501602237 | validation: 0.07937560444947415]
	TIME [epoch: 3.6 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03780782919488674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03780782919488674 | validation: 0.1040645329720693]
	TIME [epoch: 3.61 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048652092547385985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048652092547385985 | validation: 0.06873840649830483]
	TIME [epoch: 3.6 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05589753911027431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05589753911027431 | validation: 0.07939534303091501]
	TIME [epoch: 3.64 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0652685472736042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0652685472736042 | validation: 0.03888567131086961]
	TIME [epoch: 3.63 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0461085561415341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0461085561415341 | validation: 0.04630508739097196]
	TIME [epoch: 3.6 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04473664344340244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04473664344340244 | validation: 0.0836696147233355]
	TIME [epoch: 3.63 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07480156789472805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07480156789472805 | validation: 0.06967957225794745]
	TIME [epoch: 3.62 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060297278622266914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060297278622266914 | validation: 0.09406160088306302]
	TIME [epoch: 3.62 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04188504490818787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04188504490818787 | validation: 0.07630567271913812]
	TIME [epoch: 3.63 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03976708038391549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03976708038391549 | validation: 0.0963133676467155]
	TIME [epoch: 3.62 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04966889210682866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04966889210682866 | validation: 0.06785338183888442]
	TIME [epoch: 3.62 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041342918086201746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041342918086201746 | validation: 0.05019110980708466]
	TIME [epoch: 3.63 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03315832327277838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03315832327277838 | validation: 0.0860289711841426]
	TIME [epoch: 3.62 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04305044736220186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04305044736220186 | validation: 0.05714958205578918]
	TIME [epoch: 3.64 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0677073557687135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0677073557687135 | validation: 0.12417071410873928]
	TIME [epoch: 3.64 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07665660368214544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07665660368214544 | validation: 0.05625158512222631]
	TIME [epoch: 3.63 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05198589125186093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05198589125186093 | validation: 0.03558917126326242]
	TIME [epoch: 3.64 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018840001401633735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018840001401633735 | validation: 0.036670100717054566]
	TIME [epoch: 3.64 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018670804802723644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018670804802723644 | validation: 0.08586480357577558]
	TIME [epoch: 3.63 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04131965710628935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04131965710628935 | validation: 0.2140486215390458]
	TIME [epoch: 3.63 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11265922086483826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11265922086483826 | validation: 0.16732523076253736]
	TIME [epoch: 3.63 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12351786241367002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12351786241367002 | validation: 0.073222220327204]
	TIME [epoch: 3.63 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06907453942757168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06907453942757168 | validation: 0.05862434207063277]
	TIME [epoch: 3.64 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033386669991847466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033386669991847466 | validation: 0.07195688680729109]
	TIME [epoch: 3.63 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037277485750807936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037277485750807936 | validation: 0.07634260266137044]
	TIME [epoch: 3.63 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04933221641054028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04933221641054028 | validation: 0.08192061490781084]
	TIME [epoch: 3.64 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051268362995314065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051268362995314065 | validation: 0.06871309007259584]
	TIME [epoch: 3.64 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056301004912735715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056301004912735715 | validation: 0.06558323671203609]
	TIME [epoch: 3.63 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0481474698082224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0481474698082224 | validation: 0.03253386546353816]
	TIME [epoch: 3.64 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0316070969796312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0316070969796312 | validation: 0.03213168974062609]
	TIME [epoch: 3.64 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01915511574529923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01915511574529923 | validation: 0.027979954510570816]
	TIME [epoch: 3.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_908.pth
	Model improved!!!
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020959698722517368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020959698722517368 | validation: 0.07515290896696243]
	TIME [epoch: 3.63 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04003016748342133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04003016748342133 | validation: 0.13268990352329127]
	TIME [epoch: 3.63 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08476478346065801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08476478346065801 | validation: 0.1483026944972354]
	TIME [epoch: 3.64 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08128103901802977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08128103901802977 | validation: 0.06299710898964947]
	TIME [epoch: 3.63 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03540690249640627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03540690249640627 | validation: 0.03556836123571649]
	TIME [epoch: 3.63 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013719524563451007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013719524563451007 | validation: 0.04098003778415731]
	TIME [epoch: 3.64 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02196345165190021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02196345165190021 | validation: 0.07608958419710568]
	TIME [epoch: 3.64 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0514533173568389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0514533173568389 | validation: 0.11546917895485086]
	TIME [epoch: 3.64 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10034311596147745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10034311596147745 | validation: 0.12032143512377012]
	TIME [epoch: 3.64 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09104173471672898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09104173471672898 | validation: 0.06422382900264693]
	TIME [epoch: 3.63 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06647135729443983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06647135729443983 | validation: 0.0827927858464911]
	TIME [epoch: 3.63 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04495716042605762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04495716042605762 | validation: 0.0419012264761947]
	TIME [epoch: 3.64 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03595068108008345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03595068108008345 | validation: 0.07320428447089543]
	TIME [epoch: 3.63 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040633751711125515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040633751711125515 | validation: 0.05215521803241055]
	TIME [epoch: 3.63 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032680109519933456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032680109519933456 | validation: 0.031672789437366004]
	TIME [epoch: 3.63 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026907079473151106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026907079473151106 | validation: 0.04799759308310238]
	TIME [epoch: 3.63 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0328957019831632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0328957019831632 | validation: 0.0342725802237079]
	TIME [epoch: 3.63 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03565735528583835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03565735528583835 | validation: 0.08414072691602144]
	TIME [epoch: 3.63 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04589363302358509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04589363302358509 | validation: 0.1140126579091898]
	TIME [epoch: 3.62 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07316365807866478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07316365807866478 | validation: 0.1617976320157701]
	TIME [epoch: 3.64 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07726801322046088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07726801322046088 | validation: 0.053574277245744255]
	TIME [epoch: 3.64 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03030020162585462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03030020162585462 | validation: 0.03146803891733473]
	TIME [epoch: 3.65 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024692874749035462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024692874749035462 | validation: 0.051490452711765604]
	TIME [epoch: 3.63 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0355501794147061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0355501794147061 | validation: 0.03150587335557011]
	TIME [epoch: 3.64 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052985346827570425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052985346827570425 | validation: 0.08914909842751867]
	TIME [epoch: 3.64 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055166460279084666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055166460279084666 | validation: 0.0669767851349006]
	TIME [epoch: 3.64 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058374195234221736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058374195234221736 | validation: 0.14139133526881964]
	TIME [epoch: 3.63 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07061979770385303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07061979770385303 | validation: 0.141415682155287]
	TIME [epoch: 3.64 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08865382463937174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08865382463937174 | validation: 0.07495885810802581]
	TIME [epoch: 3.63 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05178679374090473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05178679374090473 | validation: 0.047125560757558686]
	TIME [epoch: 3.63 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022461037545663844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022461037545663844 | validation: 0.034870412531436815]
	TIME [epoch: 3.63 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021103701183835817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021103701183835817 | validation: 0.04075530395994666]
	TIME [epoch: 3.62 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04107652196737432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04107652196737432 | validation: 0.0602487630796825]
	TIME [epoch: 3.63 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06766751419895224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06766751419895224 | validation: 0.10844078818307903]
	TIME [epoch: 3.63 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07395131657968267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07395131657968267 | validation: 0.07653288841187587]
	TIME [epoch: 3.64 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04957348985789941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04957348985789941 | validation: 0.058515495394322874]
	TIME [epoch: 3.63 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02670917527207725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02670917527207725 | validation: 0.07547612913670415]
	TIME [epoch: 3.63 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03514940780384539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03514940780384539 | validation: 0.0720503147111284]
	TIME [epoch: 3.62 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04321439561163899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04321439561163899 | validation: 0.09336697983897499]
	TIME [epoch: 3.64 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05791249053220067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05791249053220067 | validation: 0.07718467266126586]
	TIME [epoch: 3.63 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05464220034103891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05464220034103891 | validation: 0.07226158202987334]
	TIME [epoch: 3.64 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03966043100156733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03966043100156733 | validation: 0.029253017401138337]
	TIME [epoch: 3.64 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045450899466527146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045450899466527146 | validation: 0.07244567882325037]
	TIME [epoch: 3.62 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05247212600040357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05247212600040357 | validation: 0.04583715563488239]
	TIME [epoch: 3.64 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045661000206708185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045661000206708185 | validation: 0.07179262724730119]
	TIME [epoch: 3.63 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04949364755779165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04949364755779165 | validation: 0.11223508020440871]
	TIME [epoch: 3.64 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062161057502889534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062161057502889534 | validation: 0.06637263020094614]
	TIME [epoch: 3.63 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051370821355973234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051370821355973234 | validation: 0.0813585022884777]
	TIME [epoch: 3.65 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03485530043897648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03485530043897648 | validation: 0.03370416861297867]
	TIME [epoch: 3.63 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029671180577957063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029671180577957063 | validation: 0.07227971968426483]
	TIME [epoch: 3.63 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03830257199352875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03830257199352875 | validation: 0.05782735558915778]
	TIME [epoch: 3.63 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050228659543832814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050228659543832814 | validation: 0.10550648217087591]
	TIME [epoch: 3.63 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0512215133296718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0512215133296718 | validation: 0.09035193051618229]
	TIME [epoch: 3.63 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0589487866586386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0589487866586386 | validation: 0.10960427152915656]
	TIME [epoch: 3.63 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05805986794528129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05805986794528129 | validation: 0.11207759589264776]
	TIME [epoch: 3.62 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06123441497564074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06123441497564074 | validation: 0.0469364866332367]
	TIME [epoch: 3.64 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03374472736000222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03374472736000222 | validation: 0.06163672882628544]
	TIME [epoch: 3.64 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030726064302214612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030726064302214612 | validation: 0.031650588539180534]
	TIME [epoch: 3.63 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03756676451788687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03756676451788687 | validation: 0.07331943415861052]
	TIME [epoch: 3.62 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049103857117434445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049103857117434445 | validation: 0.07051807653483082]
	TIME [epoch: 3.62 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07418660444419788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07418660444419788 | validation: 0.09458984877854638]
	TIME [epoch: 3.64 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060629294013856506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060629294013856506 | validation: 0.07340483886308939]
	TIME [epoch: 3.61 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03934416733937863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03934416733937863 | validation: 0.05964905892654753]
	TIME [epoch: 3.61 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028131793073705513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028131793073705513 | validation: 0.057717554177295266]
	TIME [epoch: 3.64 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030390880486040484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030390880486040484 | validation: 0.05175322707667475]
	TIME [epoch: 3.61 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03168119532536404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03168119532536404 | validation: 0.03745428598010784]
	TIME [epoch: 3.62 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03006570511912832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03006570511912832 | validation: 0.06626295430422244]
	TIME [epoch: 3.62 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032119540855229445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032119540855229445 | validation: 0.04726972834069494]
	TIME [epoch: 3.61 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046883171165945696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046883171165945696 | validation: 0.12460383307048895]
	TIME [epoch: 3.62 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05618200118971771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05618200118971771 | validation: 0.07633359304828355]
	TIME [epoch: 3.61 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05847660274341725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05847660274341725 | validation: 0.07897553039607597]
	TIME [epoch: 3.6 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050383513096222576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050383513096222576 | validation: 0.057187527621369916]
	TIME [epoch: 3.63 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05643857533310195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05643857533310195 | validation: 0.06825243896213012]
	TIME [epoch: 3.61 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09015100592969404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09015100592969404 | validation: 0.12846725934522008]
	TIME [epoch: 3.63 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07141758625737475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07141758625737475 | validation: 0.06594654094085979]
	TIME [epoch: 3.62 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048651988999989036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048651988999989036 | validation: 0.07037385158734576]
	TIME [epoch: 3.63 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02663012564614905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02663012564614905 | validation: 0.040280945458564636]
	TIME [epoch: 3.64 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022178662154467482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022178662154467482 | validation: 0.04333247591968756]
	TIME [epoch: 3.64 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03163011298215179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03163011298215179 | validation: 0.08571062123994902]
	TIME [epoch: 3.6 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04732234030719745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04732234030719745 | validation: 0.08658705571759427]
	TIME [epoch: 3.63 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05686013769323246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05686013769323246 | validation: 0.10715817553447837]
	TIME [epoch: 3.61 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048112577822488796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048112577822488796 | validation: 0.06851500842818732]
	TIME [epoch: 3.62 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0391118652980027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0391118652980027 | validation: 0.051969246713326124]
	TIME [epoch: 3.6 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03050673202633349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03050673202633349 | validation: 0.05584214077642135]
	TIME [epoch: 3.61 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03833872985196659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03833872985196659 | validation: 0.04793334514507124]
	TIME [epoch: 3.64 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0455980799267351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0455980799267351 | validation: 0.07024622751984133]
	TIME [epoch: 3.64 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04318022889207823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04318022889207823 | validation: 0.04892713239072555]
	TIME [epoch: 3.64 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027376971746776585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027376971746776585 | validation: 0.07501550598520183]
	TIME [epoch: 3.63 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03474222763789469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03474222763789469 | validation: 0.07023079568940953]
	TIME [epoch: 3.63 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07606198519132655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07606198519132655 | validation: 0.16220499569728897]
	TIME [epoch: 3.63 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1167882489390066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1167882489390066 | validation: 0.14447470565369908]
	TIME [epoch: 3.62 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08793855691297978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08793855691297978 | validation: 0.08071908788301617]
	TIME [epoch: 3.62 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053934394952840706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053934394952840706 | validation: 0.08102450891135834]
	TIME [epoch: 50.4 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03568601264422837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03568601264422837 | validation: 0.047862953049630744]
	TIME [epoch: 7.9 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032066311417364546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032066311417364546 | validation: 0.042064234369519865]
	TIME [epoch: 7.89 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03246062074025688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03246062074025688 | validation: 0.044374834002787855]
	TIME [epoch: 7.87 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02904261608761365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02904261608761365 | validation: 0.045753185617238616]
	TIME [epoch: 7.85 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0291499648052156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0291499648052156 | validation: 0.04393117265401612]
	TIME [epoch: 7.9 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03491810551673741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03491810551673741 | validation: 0.06622515515883233]
	TIME [epoch: 7.89 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03892461891028622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03892461891028622 | validation: 0.045044762916851167]
	TIME [epoch: 7.88 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0329592490998861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0329592490998861 | validation: 0.06186714703188409]
	TIME [epoch: 7.85 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20240822_132805/states/model_phi1_4a_v_mmd1_1009.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 2872.713 seconds.
