Args:
Namespace(name='model_phi1_2b_v_mmd1', outdir='out/model_training/model_phi1_2b_v_mmd1', training_data='data/training_data/data_phi1_2b/training', validation_data='data/training_data/data_phi1_2b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 793260640

Training model...

Saving initial model state to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 2/2] avg loss: 5.381434082034431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.381434082034431 | validation: 4.484527148143985]
	TIME [epoch: 93.2 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 2/2] avg loss: 5.276207774670791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.276207774670791 | validation: 4.443516390503373]
	TIME [epoch: 3.54 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.940950289310665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.940950289310665 | validation: 4.173303356195944]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.752528210272333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.752528210272333 | validation: 3.9961467491686893]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.639765328559933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.639765328559933 | validation: 3.988315598051073]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.546603972726704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.546603972726704 | validation: 3.7015315901973787]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.385895460488811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.385895460488811 | validation: 3.602772915087391]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.184219673006702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.184219673006702 | validation: 3.4603898072053014]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.020110067777974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.020110067777974 | validation: 3.4413277431783684]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.88454077650195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.88454077650195 | validation: 3.438413546605704]
	TIME [epoch: 3.48 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.7872748918250227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7872748918250227 | validation: 3.192581239483821]
	TIME [epoch: 3.5 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.61219819540987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.61219819540987 | validation: 3.1415894774224298]
	TIME [epoch: 3.48 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.458609891340337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.458609891340337 | validation: 3.0220544268082956]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.3695501969149526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3695501969149526 | validation: 3.020032028820742]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.396802752507159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.396802752507159 | validation: 3.0161711504542836]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.20655200677037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.20655200677037 | validation: 2.801223314084178]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.130597059836229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.130597059836229 | validation: 2.744522942011759]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.093076642432272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.093076642432272 | validation: 2.7101504450035314]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.034871648368067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.034871648368067 | validation: 2.7243638170332987]
	TIME [epoch: 3.46 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.9742552203625117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9742552203625117 | validation: 2.699727910368196]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.869629429299449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.869629429299449 | validation: 2.630735146813539]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.912230423822665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.912230423822665 | validation: 2.7195979082802055]
	TIME [epoch: 3.47 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.907204875481497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.907204875481497 | validation: 2.571622654450371]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.8645274945546095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8645274945546095 | validation: 2.5848110811726714]
	TIME [epoch: 3.5 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.8841625766459096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8841625766459096 | validation: 2.6267496674483155]
	TIME [epoch: 3.46 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.7781018784203106		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 2.7781018784203106 | validation: 2.5280113620756137]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.82331069045649		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 2.82331069045649 | validation: 2.498625027844954]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.757600533948798		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 2.757600533948798 | validation: 2.514772513353198]
	TIME [epoch: 3.47 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.7504973370939885		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 2.7504973370939885 | validation: 2.576787982977587]
	TIME [epoch: 3.46 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.7319859433098084		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 2.7319859433098084 | validation: 2.57463888845575]
	TIME [epoch: 3.46 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.756322608737273		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 2.756322608737273 | validation: 2.5694610673861806]
	TIME [epoch: 3.46 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.6910250607279793		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 2.6910250607279793 | validation: 2.498194280504043]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.6534048399299306		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 2.6534048399299306 | validation: 2.4722796648986662]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.629808850282914		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 2.629808850282914 | validation: 2.424531273200962]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.6944684028763413		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 2.6944684028763413 | validation: 2.3990749716003035]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.6024342923948587		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 2.6024342923948587 | validation: 2.5542725605206287]
	TIME [epoch: 3.5 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.5789735488063776		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 2.5789735488063776 | validation: 2.4084282451667747]
	TIME [epoch: 3.47 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.51035928667528		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 2.51035928667528 | validation: 2.426865107074653]
	TIME [epoch: 3.46 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.448271390937088		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 2.448271390937088 | validation: 2.295112075092456]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.686632854840501		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 2.686632854840501 | validation: 2.1782860214410333]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.3552070515575085		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 2.3552070515575085 | validation: 2.1905443038303334]
	TIME [epoch: 3.46 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.2299799286332975		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 2.2299799286332975 | validation: 2.060589166609748]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.1355370452309463		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 2.1355370452309463 | validation: 2.0735059127179847]
	TIME [epoch: 3.46 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.8988372356481606		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.8988372356481606 | validation: 2.100170320647388]
	TIME [epoch: 3.46 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.7453400837524136		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.7453400837524136 | validation: 1.7078958030818585]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.806444728108477		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.806444728108477 | validation: 1.6720972344790228]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5860795565659978		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.5860795565659978 | validation: 1.691614948823967]
	TIME [epoch: 3.47 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5585900108580888		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.5585900108580888 | validation: 1.7118584089950888]
	TIME [epoch: 3.49 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5798835796931245		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.5798835796931245 | validation: 1.8781885565417253]
	TIME [epoch: 3.48 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.6097507057687919		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.6097507057687919 | validation: 1.6187157344391867]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5839397202349819		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.5839397202349819 | validation: 2.011951181295693]
	TIME [epoch: 3.47 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.6807070463549105		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.6807070463549105 | validation: 1.8873948485765255]
	TIME [epoch: 3.46 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.8055186596861594		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.8055186596861594 | validation: 1.580906431314153]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5615863845765046		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.5615863845765046 | validation: 1.6464976243135057]
	TIME [epoch: 3.47 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4774494826072333		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.4774494826072333 | validation: 1.5909094481899968]
	TIME [epoch: 3.46 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5113053821414055		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.5113053821414055 | validation: 1.584016996643473]
	TIME [epoch: 3.46 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4846139664049143		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.4846139664049143 | validation: 1.5874703169856452]
	TIME [epoch: 3.46 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.527228799889089		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.527228799889089 | validation: 1.7331660148772312]
	TIME [epoch: 3.47 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5980096272680329		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.5980096272680329 | validation: 1.7525366115579093]
	TIME [epoch: 3.47 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5402962106270877		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.5402962106270877 | validation: 1.6011868772919264]
	TIME [epoch: 3.47 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5149436232597355		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.5149436232597355 | validation: 1.5869713062506277]
	TIME [epoch: 3.49 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.42568589256301		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.42568589256301 | validation: 1.5763993950676294]
	TIME [epoch: 3.48 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4346406381679235		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.4346406381679235 | validation: 1.4690144513323413]
	TIME [epoch: 3.48 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.412805965423316		[learning rate: 0.0076127]
	Learning Rate: 0.00761269
	LOSS [training: 1.412805965423316 | validation: 1.5486278045792614]
	TIME [epoch: 3.47 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4459700957180732		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.4459700957180732 | validation: 1.7356904236110369]
	TIME [epoch: 3.47 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4473690632328808		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.4473690632328808 | validation: 1.839246667941978]
	TIME [epoch: 3.46 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5032946509863707		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.5032946509863707 | validation: 1.4406006555156496]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3897497628727415		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.3897497628727415 | validation: 1.487875945264303]
	TIME [epoch: 3.47 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3490505243582303		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.3490505243582303 | validation: 1.7614118538817252]
	TIME [epoch: 3.46 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3888331067253672		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.3888331067253672 | validation: 1.392270607611829]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2806858464867512		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.2806858464867512 | validation: 1.487976217609668]
	TIME [epoch: 3.47 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3327247097196062		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.3327247097196062 | validation: 1.7241800640554612]
	TIME [epoch: 3.46 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2886438754143765		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.2886438754143765 | validation: 1.288843046461998]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3514346763091143		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.3514346763091143 | validation: 1.4705649133802747]
	TIME [epoch: 3.5 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2693234429780855		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.2693234429780855 | validation: 1.3852890332920405]
	TIME [epoch: 3.47 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1573164037998351		[learning rate: 0.0069922]
	Learning Rate: 0.00699222
	LOSS [training: 1.1573164037998351 | validation: 1.433299349008445]
	TIME [epoch: 3.46 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1629253492129223		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.1629253492129223 | validation: 1.4513641929955026]
	TIME [epoch: 3.47 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2456144171200787		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.2456144171200787 | validation: 1.4246539562176679]
	TIME [epoch: 3.46 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3429356509677555		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.3429356509677555 | validation: 1.3454060268151404]
	TIME [epoch: 3.47 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.403488327119936		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.403488327119936 | validation: 1.4104810446703444]
	TIME [epoch: 3.47 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1022320466817144		[learning rate: 0.0067489]
	Learning Rate: 0.00674887
	LOSS [training: 1.1022320466817144 | validation: 1.183586794084455]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0320534309479772		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.0320534309479772 | validation: 1.2814068062488921]
	TIME [epoch: 3.47 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0115646332361568		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.0115646332361568 | validation: 1.1470422962078144]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0206002646056909		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.0206002646056909 | validation: 1.059943232152299]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9533939716511407		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.9533939716511407 | validation: 1.0639742705293076]
	TIME [epoch: 3.47 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9289141980865212		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.9289141980865212 | validation: 1.0372770636033932]
	TIME [epoch: 3.49 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9991835428418583		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.9991835428418583 | validation: 0.9809373266629827]
	TIME [epoch: 3.49 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9046764104108785		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.9046764104108785 | validation: 0.9501887072865931]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9436854342265981		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.9436854342265981 | validation: 0.9992462950972314]
	TIME [epoch: 3.47 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.079052277131702		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.079052277131702 | validation: 1.074177082836372]
	TIME [epoch: 3.47 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9068683434451547		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.9068683434451547 | validation: 1.4669731335655563]
	TIME [epoch: 3.47 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9557312246060485		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.9557312246060485 | validation: 0.8995540381369402]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8211326527389138		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.8211326527389138 | validation: 1.2035020270927559]
	TIME [epoch: 3.47 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2327123158417663		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.2327123158417663 | validation: 1.2626559693979258]
	TIME [epoch: 3.46 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8644631219720612		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.8644631219720612 | validation: 0.9609608763875621]
	TIME [epoch: 3.47 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.800433941285684		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.800433941285684 | validation: 1.0307736340500788]
	TIME [epoch: 3.47 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7603556469509993		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.7603556469509993 | validation: 1.094786248920325]
	TIME [epoch: 3.47 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9606133306453152		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.9606133306453152 | validation: 1.054261057002728]
	TIME [epoch: 3.47 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7992670885243496		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.7992670885243496 | validation: 0.8236360098246337]
	TIME [epoch: 3.49 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7554537015867553		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.7554537015867553 | validation: 0.7556800711558269]
	TIME [epoch: 3.5 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7601205537410862		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.7601205537410862 | validation: 1.2787266114415883]
	TIME [epoch: 3.47 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8896200177253748		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.8896200177253748 | validation: 1.1321372793328688]
	TIME [epoch: 3.47 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9525233177502717		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.9525233177502717 | validation: 1.2191432497258778]
	TIME [epoch: 3.46 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.993659265210143		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.993659265210143 | validation: 0.8473558587579686]
	TIME [epoch: 3.46 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7793926913915868		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.7793926913915868 | validation: 0.9248449909118737]
	TIME [epoch: 3.46 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6862665396714357		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.6862665396714357 | validation: 0.9918841085318004]
	TIME [epoch: 3.47 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.778067818721272		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.778067818721272 | validation: 0.8116471235520248]
	TIME [epoch: 3.46 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6108472571914392		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.6108472571914392 | validation: 0.9142245269455485]
	TIME [epoch: 3.47 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7406687183900675		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.7406687183900675 | validation: 0.7726720673773205]
	TIME [epoch: 3.46 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6259700749341408		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.6259700749341408 | validation: 1.9403043889369584]
	TIME [epoch: 3.46 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9051605864545172		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.9051605864545172 | validation: 0.7462730163691221]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.685809374250371		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.685809374250371 | validation: 0.6950653442201549]
	TIME [epoch: 3.5 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5896575273894142		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.5896575273894142 | validation: 1.4384527866128987]
	TIME [epoch: 3.48 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8321877613436459		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.8321877613436459 | validation: 1.6413526640286833]
	TIME [epoch: 3.47 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.967210922303894		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.967210922303894 | validation: 0.8170224258762204]
	TIME [epoch: 3.47 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6016097672228067		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.6016097672228067 | validation: 0.7008170348988065]
	TIME [epoch: 3.47 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6608076715728419		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.6608076715728419 | validation: 0.630512235771438]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6973310895742688		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.6973310895742688 | validation: 0.5605668196226911]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6195583950679346		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.6195583950679346 | validation: 0.5329880826777373]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.49835324110389156		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.49835324110389156 | validation: 0.8546808941741688]
	TIME [epoch: 3.47 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6610886317725919		[learning rate: 0.0050834]
	Learning Rate: 0.00508339
	LOSS [training: 0.6610886317725919 | validation: 1.1773670177336466]
	TIME [epoch: 3.46 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5952752675347652		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.5952752675347652 | validation: 0.5837142599709515]
	TIME [epoch: 3.46 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6158476214575295		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.6158476214575295 | validation: 0.5579670166613632]
	TIME [epoch: 3.47 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5382758845762385		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.5382758845762385 | validation: 0.8683722935766774]
	TIME [epoch: 3.49 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6794942838165439		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.6794942838165439 | validation: 0.7818557340239853]
	TIME [epoch: 3.5 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.515588163787498		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.515588163787498 | validation: 1.9641378593308538]
	TIME [epoch: 3.47 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9258859565002646		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.9258859565002646 | validation: 0.817744381659464]
	TIME [epoch: 3.46 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5964755317942214		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.5964755317942214 | validation: 0.7721819509337171]
	TIME [epoch: 3.46 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5946900466546408		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.5946900466546408 | validation: 0.7771855332912161]
	TIME [epoch: 3.47 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4730235034194856		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.4730235034194856 | validation: 0.49474289693746737]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4306872889419834		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.4306872889419834 | validation: 0.5045839490086367]
	TIME [epoch: 3.47 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.43055002493912753		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.43055002493912753 | validation: 0.5544756464924001]
	TIME [epoch: 3.46 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4467465346718161		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.4467465346718161 | validation: 1.1313957841002626]
	TIME [epoch: 3.47 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8602867428173955		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.8602867428173955 | validation: 0.7871277446116083]
	TIME [epoch: 3.46 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7303905917115765		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.7303905917115765 | validation: 1.9927600720622742]
	TIME [epoch: 3.46 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0282323072512625		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 1.0282323072512625 | validation: 0.5696532218367835]
	TIME [epoch: 3.47 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5646443026929313		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.5646443026929313 | validation: 0.5701400946016224]
	TIME [epoch: 3.47 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.45649158998760353		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.45649158998760353 | validation: 0.611752574626064]
	TIME [epoch: 3.5 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.544429277998969		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.544429277998969 | validation: 1.0759429104804659]
	TIME [epoch: 3.48 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5484121047415373		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.5484121047415373 | validation: 0.6477928129544784]
	TIME [epoch: 3.47 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5995265830917009		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.5995265830917009 | validation: 0.4617851559667376]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_141.pth
	Model improved!!!
EPOCH 142/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.45640844737476705		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.45640844737476705 | validation: 1.0890019045466977]
	TIME [epoch: 3.47 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8310061610021835		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.8310061610021835 | validation: 0.6744836331013645]
	TIME [epoch: 3.47 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.41291545179428896		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.41291545179428896 | validation: 0.7266343796217853]
	TIME [epoch: 3.47 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4750700458428968		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.4750700458428968 | validation: 0.5149891590472224]
	TIME [epoch: 3.47 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4383526159779833		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.4383526159779833 | validation: 1.262957793541841]
	TIME [epoch: 3.47 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5597203795138902		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.5597203795138902 | validation: 0.44600887352034935]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.37117673316401384		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.37117673316401384 | validation: 0.529635048278337]
	TIME [epoch: 3.47 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.38572884083703995		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.38572884083703995 | validation: 0.5011571423251042]
	TIME [epoch: 3.47 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7477084362127466		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.7477084362127466 | validation: 0.6093802927545978]
	TIME [epoch: 3.48 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5724465362127799		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.5724465362127799 | validation: 0.651867264543799]
	TIME [epoch: 3.5 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7760930991621006		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.7760930991621006 | validation: 0.900926461392129]
	TIME [epoch: 3.48 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5387984209490457		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.5387984209490457 | validation: 0.7790228572824601]
	TIME [epoch: 3.47 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5361418556541342		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.5361418556541342 | validation: 0.47160163430457386]
	TIME [epoch: 3.47 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5095401011114322		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.5095401011114322 | validation: 0.445931513926856]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4432163016135201		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.4432163016135201 | validation: 0.5110213609467416]
	TIME [epoch: 3.47 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.32507086667796214		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.32507086667796214 | validation: 0.41489250748574275]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3310479195225866		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.3310479195225866 | validation: 0.3742464371234797]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_158.pth
	Model improved!!!
EPOCH 159/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4005534938027625		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.4005534938027625 | validation: 2.2300420475751186]
	TIME [epoch: 3.48 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.989361018571222		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 1.989361018571222 | validation: 2.05282878659261]
	TIME [epoch: 3.47 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.8820546925290902		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 1.8820546925290902 | validation: 2.1202513979964777]
	TIME [epoch: 3.47 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5381741376380758		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 1.5381741376380758 | validation: 1.342488449977243]
	TIME [epoch: 3.48 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9665011148184623		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.9665011148184623 | validation: 1.2264907988210736]
	TIME [epoch: 3.49 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7512088673629473		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.7512088673629473 | validation: 0.5506539240184808]
	TIME [epoch: 3.49 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6053796853629612		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.6053796853629612 | validation: 1.1704032471990764]
	TIME [epoch: 3.47 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5308292545330253		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.5308292545330253 | validation: 0.41842850418112304]
	TIME [epoch: 3.47 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3940733764943455		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.3940733764943455 | validation: 0.5400141305294387]
	TIME [epoch: 3.47 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.35028080755877		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.35028080755877 | validation: 0.3649947727656341]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_168.pth
	Model improved!!!
EPOCH 169/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3152393524767515		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.3152393524767515 | validation: 0.4123338577497155]
	TIME [epoch: 3.47 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3201610663568773		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.3201610663568773 | validation: 0.35308224723414977]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.32515219338744566		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.32515219338744566 | validation: 0.3830312120070968]
	TIME [epoch: 3.47 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2950030629488406		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.2950030629488406 | validation: 0.34069155745037083]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3487894148297106		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.3487894148297106 | validation: 0.38060804814281746]
	TIME [epoch: 3.47 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3856780369839008		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.3856780369839008 | validation: 0.5008392364865814]
	TIME [epoch: 3.47 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5827721750717286		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.5827721750717286 | validation: 0.7334701094545082]
	TIME [epoch: 3.47 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4498132879760397		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.4498132879760397 | validation: 0.3820541653655281]
	TIME [epoch: 3.49 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.35646396593603114		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.35646396593603114 | validation: 0.7274609056374772]
	TIME [epoch: 3.48 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.36099251527322906		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.36099251527322906 | validation: 0.8652494488640396]
	TIME [epoch: 3.47 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0950229588953193		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 1.0950229588953193 | validation: 0.8711428745251223]
	TIME [epoch: 3.46 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6590612002255825		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.6590612002255825 | validation: 0.9638604612101856]
	TIME [epoch: 3.47 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.47951293138712614		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.47951293138712614 | validation: 0.32437233437533125]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.315173211533723		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.315173211533723 | validation: 0.3829256149081809]
	TIME [epoch: 3.47 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2822743259511083		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.2822743259511083 | validation: 0.585349249196836]
	TIME [epoch: 3.47 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3959224049357326		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.3959224049357326 | validation: 0.6513582617184541]
	TIME [epoch: 3.47 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.31293577265922423		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.31293577265922423 | validation: 0.2942107744218077]
	TIME [epoch: 3.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2798715515911298		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.2798715515911298 | validation: 0.3622868588710121]
	TIME [epoch: 3.47 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4552702305676549		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.4552702305676549 | validation: 0.4120410660690075]
	TIME [epoch: 3.47 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3797334266003737		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.3797334266003737 | validation: 0.5306250573605368]
	TIME [epoch: 3.47 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.34426817895522976		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.34426817895522976 | validation: 0.5291991105541044]
	TIME [epoch: 3.5 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3476894242167127		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.3476894242167127 | validation: 0.4831394233173468]
	TIME [epoch: 3.48 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3292259340975139		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.3292259340975139 | validation: 0.36066335726715243]
	TIME [epoch: 3.47 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.27018905210545846		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.27018905210545846 | validation: 0.2972515119655664]
	TIME [epoch: 3.46 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4078113305947284		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.4078113305947284 | validation: 0.3506103087236685]
	TIME [epoch: 3.47 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3383212638144053		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.3383212638144053 | validation: 0.5262465061596203]
	TIME [epoch: 3.47 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.32157269731741867		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.32157269731741867 | validation: 0.33224396911154724]
	TIME [epoch: 3.47 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3229457654829997		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.3229457654829997 | validation: 0.27196757517219183]
	TIME [epoch: 3.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3205901109388933		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.3205901109388933 | validation: 0.30469707118847394]
	TIME [epoch: 3.47 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2471712183918299		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.2471712183918299 | validation: 0.43648476870442077]
	TIME [epoch: 3.47 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2541166111812486		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.2541166111812486 | validation: 0.35665643245603224]
	TIME [epoch: 3.47 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4089563323219795		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.4089563323219795 | validation: 0.32854722004237963]
	TIME [epoch: 3.47 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.353817850530729		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.353817850530729 | validation: 0.24212507667939331]
	TIME [epoch: 95.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3520636005966708		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.3520636005966708 | validation: 0.36585361752640494]
	TIME [epoch: 6.89 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24305601922582876		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.24305601922582876 | validation: 0.4038087494177173]
	TIME [epoch: 6.84 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2523581834049452		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.2523581834049452 | validation: 0.3502858007879773]
	TIME [epoch: 6.84 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.35677487437745153		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.35677487437745153 | validation: 0.2541631585042784]
	TIME [epoch: 6.84 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.249783923953517		[learning rate: 0.0027837]
	Learning Rate: 0.00278366
	LOSS [training: 0.249783923953517 | validation: 0.24619644956857278]
	TIME [epoch: 6.86 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.213157985838142		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.213157985838142 | validation: 0.38954917617756657]
	TIME [epoch: 6.86 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.30140271156239484		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.30140271156239484 | validation: 0.2420945439080521]
	TIME [epoch: 6.84 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.35917703837502546		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.35917703837502546 | validation: 0.3018461861445257]
	TIME [epoch: 6.85 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.25264648293566433		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.25264648293566433 | validation: 0.5851096647846606]
	TIME [epoch: 6.84 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3037622235686142		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.3037622235686142 | validation: 0.30270172019153874]
	TIME [epoch: 6.84 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2462051642932946		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.2462051642932946 | validation: 0.2460188952963962]
	TIME [epoch: 6.85 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19042660360370006		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.19042660360370006 | validation: 0.6408851300860663]
	TIME [epoch: 6.88 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.289982881200056		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.289982881200056 | validation: 0.21994201765905502]
	TIME [epoch: 6.86 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_214.pth
	Model improved!!!
EPOCH 215/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22461202819663212		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.22461202819663212 | validation: 0.4579677550159394]
	TIME [epoch: 6.86 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2778036497779437		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.2778036497779437 | validation: 0.7292165974385327]
	TIME [epoch: 6.85 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3228051894760183		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.3228051894760183 | validation: 0.23431562254411464]
	TIME [epoch: 6.85 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24106305651377324		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.24106305651377324 | validation: 0.23482951773880775]
	TIME [epoch: 6.85 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24716969190031293		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.24716969190031293 | validation: 0.25848138602406945]
	TIME [epoch: 6.86 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17449708387206303		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.17449708387206303 | validation: 0.22587314225570904]
	TIME [epoch: 6.88 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3505684727774617		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.3505684727774617 | validation: 0.2197903945328281]
	TIME [epoch: 6.85 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19479863303423198		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.19479863303423198 | validation: 0.25608857754461073]
	TIME [epoch: 6.85 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18953820793242832		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.18953820793242832 | validation: 0.29047256629525003]
	TIME [epoch: 6.85 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23024877816875466		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.23024877816875466 | validation: 0.578932791199345]
	TIME [epoch: 6.85 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6087355925850134		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.6087355925850134 | validation: 0.5502456432373063]
	TIME [epoch: 6.85 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3218891046867639		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.3218891046867639 | validation: 0.2016913220588056]
	TIME [epoch: 6.86 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21446261515251686		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.21446261515251686 | validation: 0.5329336372052798]
	TIME [epoch: 6.89 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.27515656971999025		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.27515656971999025 | validation: 0.5037786089120617]
	TIME [epoch: 6.85 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2992413776629542		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.2992413776629542 | validation: 1.3455084777075097]
	TIME [epoch: 6.85 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7615543474873906		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.7615543474873906 | validation: 0.2629269505084018]
	TIME [epoch: 6.85 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24374188828871962		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.24374188828871962 | validation: 0.22479751685655752]
	TIME [epoch: 6.85 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18054922880956065		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.18054922880956065 | validation: 0.22424576802724572]
	TIME [epoch: 6.86 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23046081788512762		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.23046081788512762 | validation: 0.19364309085833975]
	TIME [epoch: 6.86 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_233.pth
	Model improved!!!
EPOCH 234/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18575373193375344		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.18575373193375344 | validation: 0.20945060264972512]
	TIME [epoch: 6.9 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17745343553297538		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.17745343553297538 | validation: 0.22743795855110377]
	TIME [epoch: 6.86 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23799904013194745		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.23799904013194745 | validation: 0.16549942667756193]
	TIME [epoch: 6.85 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1586069165078366		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.1586069165078366 | validation: 0.1666728405293965]
	TIME [epoch: 6.85 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16219370863782034		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.16219370863782034 | validation: 0.2692736428230269]
	TIME [epoch: 6.85 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21732741557048982		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.21732741557048982 | validation: 0.8936874965010393]
	TIME [epoch: 6.85 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4559856108852902		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.4559856108852902 | validation: 0.8745448683124899]
	TIME [epoch: 6.86 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.660283594571352		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.660283594571352 | validation: 0.23854282492652837]
	TIME [epoch: 6.89 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2300402258984		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.2300402258984 | validation: 0.19169252934554779]
	TIME [epoch: 6.88 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15978772779208572		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.15978772779208572 | validation: 0.17726820369333068]
	TIME [epoch: 6.86 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18135341371058233		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.18135341371058233 | validation: 0.918056359995862]
	TIME [epoch: 6.86 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4112903140832045		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.4112903140832045 | validation: 0.32042715651403486]
	TIME [epoch: 6.86 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4033827481152934		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.4033827481152934 | validation: 0.21248996798972783]
	TIME [epoch: 6.85 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20296599319939207		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.20296599319939207 | validation: 0.2112900225475427]
	TIME [epoch: 6.85 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14999166829372923		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.14999166829372923 | validation: 0.1564953762867241]
	TIME [epoch: 6.88 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14496517594024297		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.14496517594024297 | validation: 0.1819605270992688]
	TIME [epoch: 6.88 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2144381629902028		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.2144381629902028 | validation: 0.20733352705056257]
	TIME [epoch: 6.86 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15636355455338186		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.15636355455338186 | validation: 0.2044172846511536]
	TIME [epoch: 6.86 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17274959957293293		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.17274959957293293 | validation: 0.19660915122709985]
	TIME [epoch: 6.86 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16422915693680726		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.16422915693680726 | validation: 0.15969312948064598]
	TIME [epoch: 6.85 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1392481479105939		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.1392481479105939 | validation: 0.33615580591251976]
	TIME [epoch: 6.86 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19167352765267348		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.19167352765267348 | validation: 0.3553815350155552]
	TIME [epoch: 6.87 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.26928621342474823		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.26928621342474823 | validation: 0.6364045662818548]
	TIME [epoch: 6.88 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.516651803125878		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.516651803125878 | validation: 0.1697005369831308]
	TIME [epoch: 6.85 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22318822164021124		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.22318822164021124 | validation: 0.15501512829626557]
	TIME [epoch: 6.85 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.144330410137409		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.144330410137409 | validation: 0.18575622515108534]
	TIME [epoch: 6.86 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24131542279316334		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.24131542279316334 | validation: 0.3091633824069692]
	TIME [epoch: 6.86 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1942368630188534		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.1942368630188534 | validation: 0.2034726821064884]
	TIME [epoch: 6.86 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15829996717966321		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.15829996717966321 | validation: 0.14273535374100202]
	TIME [epoch: 6.87 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_262.pth
	Model improved!!!
EPOCH 263/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14000574540934238		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.14000574540934238 | validation: 0.15452115917398246]
	TIME [epoch: 6.88 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14473741788567945		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.14473741788567945 | validation: 0.15707334822815366]
	TIME [epoch: 6.86 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13641768482129796		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.13641768482129796 | validation: 0.20247146142921088]
	TIME [epoch: 6.86 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14278378555293525		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.14278378555293525 | validation: 0.4234967645138091]
	TIME [epoch: 6.86 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19373147641980912		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.19373147641980912 | validation: 0.25936141284811404]
	TIME [epoch: 6.85 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.26457763694238806		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.26457763694238806 | validation: 0.26962858434086184]
	TIME [epoch: 6.85 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1433560444957162		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.1433560444957162 | validation: 0.13949802327929714]
	TIME [epoch: 6.85 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_269.pth
	Model improved!!!
EPOCH 270/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1214976568299213		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.1214976568299213 | validation: 0.1734810884480821]
	TIME [epoch: 6.88 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1210584357218962		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.1210584357218962 | validation: 0.1710175054445766]
	TIME [epoch: 6.87 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15081996587393764		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.15081996587393764 | validation: 0.13323447114929546]
	TIME [epoch: 6.84 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_272.pth
	Model improved!!!
EPOCH 273/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1224727698129105		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.1224727698129105 | validation: 0.14167973863744954]
	TIME [epoch: 6.85 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11771099883089067		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.11771099883089067 | validation: 0.21880075052777134]
	TIME [epoch: 6.85 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18081844431587452		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.18081844431587452 | validation: 0.3494631916544009]
	TIME [epoch: 6.85 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2025650185763001		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.2025650185763001 | validation: 0.303421289352545]
	TIME [epoch: 6.86 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15121147891634829		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.15121147891634829 | validation: 0.14377546359018087]
	TIME [epoch: 6.87 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2691367337474021		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.2691367337474021 | validation: 0.4221125877729384]
	TIME [epoch: 6.86 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19884319819389867		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.19884319819389867 | validation: 0.1875997343865214]
	TIME [epoch: 6.84 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15220071773391397		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.15220071773391397 | validation: 0.16870397455539218]
	TIME [epoch: 6.84 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12411614140270666		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.12411614140270666 | validation: 0.14110219387383638]
	TIME [epoch: 6.84 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10733153412205063		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.10733153412205063 | validation: 0.17204112597653107]
	TIME [epoch: 6.85 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15313973026600367		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.15313973026600367 | validation: 0.34523479701227333]
	TIME [epoch: 6.84 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15017935731407156		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.15017935731407156 | validation: 0.1196622980548594]
	TIME [epoch: 6.86 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_284.pth
	Model improved!!!
EPOCH 285/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10310093146953009		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.10310093146953009 | validation: 0.12989936091652893]
	TIME [epoch: 6.88 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11280693970701301		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.11280693970701301 | validation: 0.12415727807765889]
	TIME [epoch: 6.85 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12512933646765018		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.12512933646765018 | validation: 0.1512874695223575]
	TIME [epoch: 6.85 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14511900571219405		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.14511900571219405 | validation: 0.13241937387757322]
	TIME [epoch: 6.85 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11033389889266368		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.11033389889266368 | validation: 0.4049713397196266]
	TIME [epoch: 6.84 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16073977656068822		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.16073977656068822 | validation: 0.19865653840456152]
	TIME [epoch: 6.84 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17323966658106754		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.17323966658106754 | validation: 0.11043224578854703]
	TIME [epoch: 6.86 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0996796316292831		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.0996796316292831 | validation: 0.13752842854265132]
	TIME [epoch: 6.89 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11313532872939108		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.11313532872939108 | validation: 0.1278821444193351]
	TIME [epoch: 6.86 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13616703397782942		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.13616703397782942 | validation: 0.23256489822329962]
	TIME [epoch: 6.86 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1380135788774972		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.1380135788774972 | validation: 0.12752722657508145]
	TIME [epoch: 6.86 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10995144823196995		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.10995144823196995 | validation: 0.13181832646703492]
	TIME [epoch: 6.85 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13519217963766963		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.13519217963766963 | validation: 0.15195428867563643]
	TIME [epoch: 6.85 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11421693176165264		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.11421693176165264 | validation: 0.14319530448147053]
	TIME [epoch: 6.86 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10956164308032293		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.10956164308032293 | validation: 0.6708138816504079]
	TIME [epoch: 6.88 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.32503118734126046		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.32503118734126046 | validation: 0.23615554829821905]
	TIME [epoch: 6.87 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12383832874319761		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.12383832874319761 | validation: 0.2182758241357612]
	TIME [epoch: 6.85 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13376330463754096		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.13376330463754096 | validation: 0.3211219697758108]
	TIME [epoch: 6.85 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17925038433632223		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.17925038433632223 | validation: 0.16962449900904553]
	TIME [epoch: 6.85 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11772127164845558		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.11772127164845558 | validation: 0.14571172452525039]
	TIME [epoch: 6.84 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10834361141532109		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.10834361141532109 | validation: 0.1710714881155828]
	TIME [epoch: 6.85 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10129923429473542		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.10129923429473542 | validation: 0.11144998618140606]
	TIME [epoch: 6.86 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1028319881343037		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.1028319881343037 | validation: 0.13213017539830116]
	TIME [epoch: 6.87 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10850344429630536		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.10850344429630536 | validation: 0.17005540573388744]
	TIME [epoch: 6.85 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10032366591245967		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.10032366591245967 | validation: 0.8021675509575386]
	TIME [epoch: 6.85 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3399881517867702		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.3399881517867702 | validation: 0.1159535642922247]
	TIME [epoch: 6.85 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1268790037210238		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.1268790037210238 | validation: 0.2315350272498424]
	TIME [epoch: 6.85 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1411567937940117		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.1411567937940117 | validation: 0.1297767827519769]
	TIME [epoch: 6.86 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13056413695953176		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.13056413695953176 | validation: 0.10037451537290548]
	TIME [epoch: 6.87 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_313.pth
	Model improved!!!
EPOCH 314/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08905100317966744		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.08905100317966744 | validation: 0.10622329432941645]
	TIME [epoch: 6.9 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08610256171335712		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.08610256171335712 | validation: 0.17217899006112036]
	TIME [epoch: 6.86 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12064391489580964		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.12064391489580964 | validation: 0.13330744706508707]
	TIME [epoch: 6.85 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09135606335950303		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.09135606335950303 | validation: 0.174403822755958]
	TIME [epoch: 6.86 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14229760270711583		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.14229760270711583 | validation: 0.1828300903484771]
	TIME [epoch: 6.85 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09809759769959639		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.09809759769959639 | validation: 0.14512419628413417]
	TIME [epoch: 6.86 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1220882403930946		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.1220882403930946 | validation: 0.18171607761823086]
	TIME [epoch: 6.87 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11902483550246182		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.11902483550246182 | validation: 0.10529672513137243]
	TIME [epoch: 6.89 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0918217271722446		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.0918217271722446 | validation: 0.0977396174134817]
	TIME [epoch: 6.88 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_322.pth
	Model improved!!!
EPOCH 323/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13091074798305133		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.13091074798305133 | validation: 0.14058732049270256]
	TIME [epoch: 6.85 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11183354949271913		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.11183354949271913 | validation: 0.09753454732548658]
	TIME [epoch: 6.84 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_324.pth
	Model improved!!!
EPOCH 325/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09909660167822705		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.09909660167822705 | validation: 0.25774152702815406]
	TIME [epoch: 6.84 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1372829659794071		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.1372829659794071 | validation: 0.22279250026439257]
	TIME [epoch: 6.85 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11545100263864748		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.11545100263864748 | validation: 0.1143250498729453]
	TIME [epoch: 6.86 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09868470722207989		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.09868470722207989 | validation: 0.19831057665471907]
	TIME [epoch: 6.88 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10613734706386117		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.10613734706386117 | validation: 0.10920862334024423]
	TIME [epoch: 6.87 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08713220352224077		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.08713220352224077 | validation: 0.09794857188094475]
	TIME [epoch: 6.85 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08773022184141482		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.08773022184141482 | validation: 0.1895847562465504]
	TIME [epoch: 6.85 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0974743633619633		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.0974743633619633 | validation: 0.10048489616547496]
	TIME [epoch: 6.85 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07422677679821496		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.07422677679821496 | validation: 0.10627375795434023]
	TIME [epoch: 6.86 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10956729386044509		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.10956729386044509 | validation: 0.313053348413766]
	TIME [epoch: 6.85 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20037319722921146		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.20037319722921146 | validation: 0.1251804509712957]
	TIME [epoch: 6.87 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10081534155698824		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.10081534155698824 | validation: 0.1801733040055934]
	TIME [epoch: 6.89 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09931423676950608		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.09931423676950608 | validation: 0.09456524071126925]
	TIME [epoch: 6.86 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_337.pth
	Model improved!!!
EPOCH 338/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09065274336152739		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.09065274336152739 | validation: 0.09572630726742633]
	TIME [epoch: 6.87 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07587564236781652		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.07587564236781652 | validation: 0.09644634622482277]
	TIME [epoch: 6.86 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07705744448071487		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.07705744448071487 | validation: 0.13796072744917542]
	TIME [epoch: 6.86 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08667037444458958		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.08667037444458958 | validation: 0.10270677381646785]
	TIME [epoch: 6.86 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07528225765901536		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.07528225765901536 | validation: 0.1140734149841515]
	TIME [epoch: 6.87 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09040765969168643		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.09040765969168643 | validation: 0.0922271438227783]
	TIME [epoch: 6.88 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_343.pth
	Model improved!!!
EPOCH 344/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07424444583107288		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.07424444583107288 | validation: 0.09379422123677364]
	TIME [epoch: 6.87 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07773399487483715		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.07773399487483715 | validation: 0.10906467134587335]
	TIME [epoch: 6.86 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1266516819215048		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.1266516819215048 | validation: 0.09173147291532502]
	TIME [epoch: 6.86 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_346.pth
	Model improved!!!
EPOCH 347/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07514198798845045		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.07514198798845045 | validation: 0.11220085013240083]
	TIME [epoch: 6.86 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10941860123385563		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.10941860123385563 | validation: 0.10208935005948223]
	TIME [epoch: 6.86 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07281422221430457		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.07281422221430457 | validation: 0.0850966790194257]
	TIME [epoch: 6.85 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_349.pth
	Model improved!!!
EPOCH 350/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08900899907674557		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.08900899907674557 | validation: 0.11012358011268837]
	TIME [epoch: 6.9 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3814949350991642		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.3814949350991642 | validation: 0.35497306356717445]
	TIME [epoch: 6.86 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.28541449706441707		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.28541449706441707 | validation: 0.2655333229905454]
	TIME [epoch: 6.86 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1293393342954128		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.1293393342954128 | validation: 0.0766562762360319]
	TIME [epoch: 6.86 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_353.pth
	Model improved!!!
EPOCH 354/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0842144880830981		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.0842144880830981 | validation: 0.09155200381686168]
	TIME [epoch: 6.85 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07899948825446051		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.07899948825446051 | validation: 0.09576628919748648]
	TIME [epoch: 6.85 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0785016026274779		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.0785016026274779 | validation: 0.09275531699723397]
	TIME [epoch: 6.86 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07299823053536078		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.07299823053536078 | validation: 0.10065301096954771]
	TIME [epoch: 6.88 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07809824393300277		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.07809824393300277 | validation: 0.08727801037982179]
	TIME [epoch: 6.87 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07427698524659629		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.07427698524659629 | validation: 0.11431860756082167]
	TIME [epoch: 6.85 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08201865280058104		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.08201865280058104 | validation: 0.12792263602000284]
	TIME [epoch: 6.85 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08372324196192896		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.08372324196192896 | validation: 0.14058745849628576]
	TIME [epoch: 6.85 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08293159408321939		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.08293159408321939 | validation: 0.07745565314314845]
	TIME [epoch: 6.85 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07363130775330806		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.07363130775330806 | validation: 0.07980203994015717]
	TIME [epoch: 6.85 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07333418342749563		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.07333418342749563 | validation: 0.13780697608789147]
	TIME [epoch: 6.87 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11153506343934866		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.11153506343934866 | validation: 0.48077551660474493]
	TIME [epoch: 6.88 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.44786413333156494		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.44786413333156494 | validation: 0.1416042771399589]
	TIME [epoch: 6.85 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13630352603955087		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.13630352603955087 | validation: 0.231689351452623]
	TIME [epoch: 6.85 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1058149517330278		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.1058149517330278 | validation: 0.08585254831170451]
	TIME [epoch: 6.86 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07605115751174293		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.07605115751174293 | validation: 0.09102237981994493]
	TIME [epoch: 6.86 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06918404431227021		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.06918404431227021 | validation: 0.09021377928753734]
	TIME [epoch: 6.87 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07083021003112575		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.07083021003112575 | validation: 0.08030278685961759]
	TIME [epoch: 6.89 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07035708579078957		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.07035708579078957 | validation: 0.0930062561335825]
	TIME [epoch: 6.88 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07230877672405789		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.07230877672405789 | validation: 0.07628836188689532]
	TIME [epoch: 6.86 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_373.pth
	Model improved!!!
EPOCH 374/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09790141645799233		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.09790141645799233 | validation: 0.09222164569331906]
	TIME [epoch: 6.86 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08049690289360593		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.08049690289360593 | validation: 0.10249911812254525]
	TIME [epoch: 6.85 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10087043626882469		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.10087043626882469 | validation: 0.15980298648205127]
	TIME [epoch: 6.85 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09073620347128483		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.09073620347128483 | validation: 0.08542559582255281]
	TIME [epoch: 6.86 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07630468264931003		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.07630468264931003 | validation: 0.09589969647510155]
	TIME [epoch: 6.89 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06930561380961078		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.06930561380961078 | validation: 0.08418182499790253]
	TIME [epoch: 6.86 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07182027472661606		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.07182027472661606 | validation: 0.08869363421064996]
	TIME [epoch: 6.85 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08347521243035302		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.08347521243035302 | validation: 0.08770066433149047]
	TIME [epoch: 6.85 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07302584267817372		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.07302584267817372 | validation: 0.19992930922288696]
	TIME [epoch: 6.85 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09354813710596468		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.09354813710596468 | validation: 0.0784123046307128]
	TIME [epoch: 6.85 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08151868192875941		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.08151868192875941 | validation: 0.07605490294037476]
	TIME [epoch: 6.86 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_384.pth
	Model improved!!!
EPOCH 385/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07289317470352935		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.07289317470352935 | validation: 0.08336444214193989]
	TIME [epoch: 6.9 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07269860925293514		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.07269860925293514 | validation: 0.12325317370864904]
	TIME [epoch: 6.86 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07454560259874443		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.07454560259874443 | validation: 0.1250546531797838]
	TIME [epoch: 6.85 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07481705351943535		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.07481705351943535 | validation: 0.08151996042833021]
	TIME [epoch: 6.86 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08677832072580247		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.08677832072580247 | validation: 0.11464627410859114]
	TIME [epoch: 6.85 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09884807501413656		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.09884807501413656 | validation: 0.12396829293607295]
	TIME [epoch: 6.85 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07761391492769044		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.07761391492769044 | validation: 0.08572760777410358]
	TIME [epoch: 6.86 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07797333572023316		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.07797333572023316 | validation: 0.08170758457471035]
	TIME [epoch: 6.88 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06420206364813512		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.06420206364813512 | validation: 0.09974766316916117]
	TIME [epoch: 6.86 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08124702811039176		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.08124702811039176 | validation: 0.08809659486159196]
	TIME [epoch: 6.86 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06478290459543579		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.06478290459543579 | validation: 0.07355019291491292]
	TIME [epoch: 6.85 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_395.pth
	Model improved!!!
EPOCH 396/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08666033237743484		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.08666033237743484 | validation: 0.0717643672484943]
	TIME [epoch: 6.86 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_396.pth
	Model improved!!!
EPOCH 397/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06359237273770205		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.06359237273770205 | validation: 0.09071789356732968]
	TIME [epoch: 6.86 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07263570473685085		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.07263570473685085 | validation: 0.07511924689942716]
	TIME [epoch: 6.86 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06709111239655302		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.06709111239655302 | validation: 0.07202775219023104]
	TIME [epoch: 6.89 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06596918815595064		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.06596918815595064 | validation: 0.08942805059118959]
	TIME [epoch: 6.85 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07092586511162191		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.07092586511162191 | validation: 0.08790531141946058]
	TIME [epoch: 6.86 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08025022246214063		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.08025022246214063 | validation: 0.08204201784788008]
	TIME [epoch: 6.85 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07699698048542428		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.07699698048542428 | validation: 0.07558532836679795]
	TIME [epoch: 6.86 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10512601745996791		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.10512601745996791 | validation: 0.07136021655783756]
	TIME [epoch: 6.85 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_404.pth
	Model improved!!!
EPOCH 405/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0672895049059184		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.0672895049059184 | validation: 0.0928245274164184]
	TIME [epoch: 6.88 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07389962838648448		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.07389962838648448 | validation: 0.07718341014982137]
	TIME [epoch: 6.88 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06319180355136521		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.06319180355136521 | validation: 0.08985905142355712]
	TIME [epoch: 6.85 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0668844618266457		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.0668844618266457 | validation: 0.09380150718619268]
	TIME [epoch: 6.84 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06300924765872387		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.06300924765872387 | validation: 0.08359916607569558]
	TIME [epoch: 6.85 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0682528877028396		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.0682528877028396 | validation: 0.09622404026568218]
	TIME [epoch: 6.86 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06877367656586883		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.06877367656586883 | validation: 0.06883891898739607]
	TIME [epoch: 6.87 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_411.pth
	Model improved!!!
EPOCH 412/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06241082576111172		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.06241082576111172 | validation: 0.07705397344724016]
	TIME [epoch: 6.89 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06381282939370066		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.06381282939370066 | validation: 0.0685375334686849]
	TIME [epoch: 6.88 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_413.pth
	Model improved!!!
EPOCH 414/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06622233109422167		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.06622233109422167 | validation: 0.08047439964657127]
	TIME [epoch: 6.86 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08080863780550158		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.08080863780550158 | validation: 0.0753384653495677]
	TIME [epoch: 6.86 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0611009605264044		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.0611009605264044 | validation: 0.11030077488822948]
	TIME [epoch: 6.85 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07675520483455567		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.07675520483455567 | validation: 0.11232577784524383]
	TIME [epoch: 6.85 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07717615198112114		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.07717615198112114 | validation: 0.07008828936923324]
	TIME [epoch: 6.86 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06283963501733603		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.06283963501733603 | validation: 0.0940380977777626]
	TIME [epoch: 6.89 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06740065623093358		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.06740065623093358 | validation: 0.07423171836527741]
	TIME [epoch: 6.88 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06575589285848055		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.06575589285848055 | validation: 0.06249659882462458]
	TIME [epoch: 6.86 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_421.pth
	Model improved!!!
EPOCH 422/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06837689701790525		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.06837689701790525 | validation: 0.06515616924744629]
	TIME [epoch: 6.86 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13309300938529448		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.13309300938529448 | validation: 0.09727925211471934]
	TIME [epoch: 6.85 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08069943595779364		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.08069943595779364 | validation: 0.08431865808224749]
	TIME [epoch: 6.86 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06222158107030823		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.06222158107030823 | validation: 0.08446682284396784]
	TIME [epoch: 6.85 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0633574893706834		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.0633574893706834 | validation: 0.07108874856176203]
	TIME [epoch: 6.87 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06763243250117175		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.06763243250117175 | validation: 0.10181814035535262]
	TIME [epoch: 6.88 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06883631073072215		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.06883631073072215 | validation: 0.08444684888966558]
	TIME [epoch: 6.85 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06870506938649613		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.06870506938649613 | validation: 0.11897145387157067]
	TIME [epoch: 6.86 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07306535872374903		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.07306535872374903 | validation: 0.07317152841472362]
	TIME [epoch: 6.85 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06452448497735674		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.06452448497735674 | validation: 0.07102220245735874]
	TIME [epoch: 6.85 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056954627266683294		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.056954627266683294 | validation: 0.06778071523881644]
	TIME [epoch: 6.86 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06371586049817887		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.06371586049817887 | validation: 0.070612574124051]
	TIME [epoch: 6.89 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.059470694818589576		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.059470694818589576 | validation: 0.07556071525011039]
	TIME [epoch: 6.86 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05976136251640013		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.05976136251640013 | validation: 0.08173866836198884]
	TIME [epoch: 6.84 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0685768640861941		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.0685768640861941 | validation: 0.07645852338190239]
	TIME [epoch: 6.85 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06166908921775391		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.06166908921775391 | validation: 0.07258367052098812]
	TIME [epoch: 6.84 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06127479522296092		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.06127479522296092 | validation: 0.07980992009756818]
	TIME [epoch: 6.84 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06571143962546422		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.06571143962546422 | validation: 0.06987648172409848]
	TIME [epoch: 6.85 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.058790968122026624		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.058790968122026624 | validation: 0.08192598685038531]
	TIME [epoch: 6.88 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06369184654065875		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.06369184654065875 | validation: 0.07035480953586132]
	TIME [epoch: 6.86 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05989906344078836		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.05989906344078836 | validation: 0.0806550199093211]
	TIME [epoch: 6.85 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06415458417276139		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.06415458417276139 | validation: 0.09198911656072305]
	TIME [epoch: 6.84 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07959204717364537		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.07959204717364537 | validation: 0.07667349013116811]
	TIME [epoch: 6.85 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07365975018294198		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.07365975018294198 | validation: 0.06613757928510379]
	TIME [epoch: 6.83 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06159317042167964		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.06159317042167964 | validation: 0.07076553041019851]
	TIME [epoch: 6.86 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056125950149309316		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.056125950149309316 | validation: 0.20949163204491308]
	TIME [epoch: 6.87 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15599586996358686		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.15599586996358686 | validation: 0.12143651994802235]
	TIME [epoch: 6.85 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07558297419608714		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.07558297419608714 | validation: 0.08023834527410947]
	TIME [epoch: 6.84 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06681168277050883		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.06681168277050883 | validation: 0.07618969338268355]
	TIME [epoch: 6.84 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05962199892405706		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.05962199892405706 | validation: 0.06834346427875233]
	TIME [epoch: 6.84 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.058369908155504		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.058369908155504 | validation: 0.07459178572917285]
	TIME [epoch: 6.85 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05894224063119457		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.05894224063119457 | validation: 0.0635410500547209]
	TIME [epoch: 6.85 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05739509810099776		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.05739509810099776 | validation: 0.07157646040803757]
	TIME [epoch: 6.88 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05585123227705575		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.05585123227705575 | validation: 0.07070226448659818]
	TIME [epoch: 6.85 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0571456544381615		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.0571456544381615 | validation: 0.06917742650258246]
	TIME [epoch: 6.83 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05674732706967381		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.05674732706967381 | validation: 0.07251167576049125]
	TIME [epoch: 6.85 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06450713335162675		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.06450713335162675 | validation: 0.06525030950500721]
	TIME [epoch: 6.85 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05687420731635361		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.05687420731635361 | validation: 0.0737770432448793]
	TIME [epoch: 6.85 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06049722613233036		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.06049722613233036 | validation: 0.06791877419121055]
	TIME [epoch: 6.87 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.058911891495696214		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.058911891495696214 | validation: 0.07078055369180443]
	TIME [epoch: 6.87 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056423974689402676		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.056423974689402676 | validation: 0.08148786900643053]
	TIME [epoch: 6.85 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07728124512822616		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.07728124512822616 | validation: 0.06388601800960636]
	TIME [epoch: 6.87 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06383157775114343		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.06383157775114343 | validation: 0.07919203122688144]
	TIME [epoch: 6.85 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06695086945107268		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.06695086945107268 | validation: 0.06045654393010067]
	TIME [epoch: 6.85 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_465.pth
	Model improved!!!
EPOCH 466/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05387410404419003		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.05387410404419003 | validation: 0.06458510456692668]
	TIME [epoch: 6.85 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05915995824970502		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.05915995824970502 | validation: 0.0636648358237363]
	TIME [epoch: 6.86 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05903775409942905		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.05903775409942905 | validation: 0.06599784514820549]
	TIME [epoch: 6.88 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.059717652467853904		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.059717652467853904 | validation: 0.09582859352719234]
	TIME [epoch: 6.85 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06631263569755981		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.06631263569755981 | validation: 0.07060029582827967]
	TIME [epoch: 6.85 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08343610737409261		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.08343610737409261 | validation: 0.06789781889890727]
	TIME [epoch: 6.87 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054117509259837535		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.054117509259837535 | validation: 0.06634441739205325]
	TIME [epoch: 6.85 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.055638788257960245		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.055638788257960245 | validation: 0.06063433958417208]
	TIME [epoch: 6.85 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053769221400810994		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.053769221400810994 | validation: 0.06269312272336446]
	TIME [epoch: 6.86 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05235148650764865		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.05235148650764865 | validation: 0.06871025759541649]
	TIME [epoch: 6.88 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0542572847696837		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.0542572847696837 | validation: 0.06447100812328231]
	TIME [epoch: 6.85 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05253087575005609		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.05253087575005609 | validation: 0.08586270287571773]
	TIME [epoch: 6.86 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06074488477677751		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.06074488477677751 | validation: 0.06194972515880717]
	TIME [epoch: 6.85 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05934710305732827		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.05934710305732827 | validation: 0.06521186383153946]
	TIME [epoch: 6.85 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05473523879451638		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.05473523879451638 | validation: 0.06676753189057337]
	TIME [epoch: 6.85 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.057560552582548184		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.057560552582548184 | validation: 0.07042865559823352]
	TIME [epoch: 6.86 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05845248063242425		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.05845248063242425 | validation: 0.0701324948737209]
	TIME [epoch: 6.87 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05616112513731246		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.05616112513731246 | validation: 0.06444939923439859]
	TIME [epoch: 6.87 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0619136230442008		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.0619136230442008 | validation: 0.059749891492154185]
	TIME [epoch: 6.85 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_484.pth
	Model improved!!!
EPOCH 485/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05569098601284		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.05569098601284 | validation: 0.06934152682203128]
	TIME [epoch: 6.84 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05334312262692234		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.05334312262692234 | validation: 0.07019120116899658]
	TIME [epoch: 6.84 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056847405795640765		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.056847405795640765 | validation: 0.06651481889063356]
	TIME [epoch: 6.85 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0548733604965677		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.0548733604965677 | validation: 0.06087917624337105]
	TIME [epoch: 6.86 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05552267668484438		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.05552267668484438 | validation: 0.06919056048149595]
	TIME [epoch: 6.88 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05269113394742951		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.05269113394742951 | validation: 0.09192044074968793]
	TIME [epoch: 6.86 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07060715710690083		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.07060715710690083 | validation: 0.07655198709370117]
	TIME [epoch: 6.85 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.060297185369112716		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.060297185369112716 | validation: 0.07396132657546324]
	TIME [epoch: 6.85 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.058042833577990754		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.058042833577990754 | validation: 0.06871114942490335]
	TIME [epoch: 6.85 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0592696409424239		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.0592696409424239 | validation: 0.07432225682214295]
	TIME [epoch: 6.86 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06262579317491193		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.06262579317491193 | validation: 0.0673703688623019]
	TIME [epoch: 6.86 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056179133450927364		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.056179133450927364 | validation: 0.08131403059365848]
	TIME [epoch: 6.91 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05837255046666083		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.05837255046666083 | validation: 0.08534296061135684]
	TIME [epoch: 6.86 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06016787023862326		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.06016787023862326 | validation: 0.08468086410632249]
	TIME [epoch: 6.85 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06626771818392528		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.06626771818392528 | validation: 0.10774379944026219]
	TIME [epoch: 6.84 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06871355173281227		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.06871355173281227 | validation: 0.06051600185526458]
	TIME [epoch: 6.84 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0545725236194014		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.0545725236194014 | validation: 0.07263500350623611]
	TIME [epoch: 102 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05541653972516121		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.05541653972516121 | validation: 0.06248608747615125]
	TIME [epoch: 14.9 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05131974925294875		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.05131974925294875 | validation: 0.07048084676192048]
	TIME [epoch: 14.9 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06361076122978548		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.06361076122978548 | validation: 0.06252824063692493]
	TIME [epoch: 14.9 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05546936203583176		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.05546936203583176 | validation: 0.07748338425468096]
	TIME [epoch: 14.9 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054896834380931146		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.054896834380931146 | validation: 0.06572652757906257]
	TIME [epoch: 14.9 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05308549722157231		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.05308549722157231 | validation: 0.0884686943466813]
	TIME [epoch: 14.9 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06248903804200079		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.06248903804200079 | validation: 0.05893230780206824]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_508.pth
	Model improved!!!
EPOCH 509/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05427675418440983		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.05427675418440983 | validation: 0.07468835955216445]
	TIME [epoch: 14.9 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06658562417684097		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.06658562417684097 | validation: 0.06821092001387564]
	TIME [epoch: 14.9 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054226304283266386		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.054226304283266386 | validation: 0.06305780337913997]
	TIME [epoch: 14.9 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05541685801760189		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.05541685801760189 | validation: 0.06240590378341371]
	TIME [epoch: 14.9 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05586940254703695		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.05586940254703695 | validation: 0.06287415480576744]
	TIME [epoch: 14.9 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05198267889267381		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.05198267889267381 | validation: 0.06481111267246853]
	TIME [epoch: 15 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05184498509722387		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.05184498509722387 | validation: 0.06216069534441962]
	TIME [epoch: 14.9 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05313286739664684		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.05313286739664684 | validation: 0.08369963666613867]
	TIME [epoch: 14.9 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05926894294576722		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.05926894294576722 | validation: 0.05781644747997383]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_517.pth
	Model improved!!!
EPOCH 518/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053961996855412794		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.053961996855412794 | validation: 0.06311084948163684]
	TIME [epoch: 14.9 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05175308126262081		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.05175308126262081 | validation: 0.058594310285475903]
	TIME [epoch: 14.9 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05150753884622826		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.05150753884622826 | validation: 0.06380774828571141]
	TIME [epoch: 15 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050660682969388054		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.050660682969388054 | validation: 0.07103595117420047]
	TIME [epoch: 14.9 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0571413369020116		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.0571413369020116 | validation: 0.06457134717877995]
	TIME [epoch: 14.9 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08452855725492348		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.08452855725492348 | validation: 0.08732235918251374]
	TIME [epoch: 14.9 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06943003627510326		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.06943003627510326 | validation: 0.07824985702455459]
	TIME [epoch: 14.9 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06628755529312015		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.06628755529312015 | validation: 0.06521671336076663]
	TIME [epoch: 14.9 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0529672887345769		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.0529672887345769 | validation: 0.06248527683111378]
	TIME [epoch: 14.9 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0531651952378689		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.0531651952378689 | validation: 0.06135414831312089]
	TIME [epoch: 14.9 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052618645714065874		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.052618645714065874 | validation: 0.06335271065917443]
	TIME [epoch: 14.9 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04937803473660653		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.04937803473660653 | validation: 0.0676404467226435]
	TIME [epoch: 14.9 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054862985222242644		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.054862985222242644 | validation: 0.0647660684479182]
	TIME [epoch: 15 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05221286136211592		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.05221286136211592 | validation: 0.0695414545092089]
	TIME [epoch: 14.9 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053241299828680984		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.053241299828680984 | validation: 0.06493777165056198]
	TIME [epoch: 14.9 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050963925064512555		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.050963925064512555 | validation: 0.06398789702592808]
	TIME [epoch: 14.9 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05316097500894801		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.05316097500894801 | validation: 0.0643005809026924]
	TIME [epoch: 14.9 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05203026431254801		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.05203026431254801 | validation: 0.06660119268695185]
	TIME [epoch: 14.9 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05039344441139344		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.05039344441139344 | validation: 0.06421472737406508]
	TIME [epoch: 14.9 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05314947927634181		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.05314947927634181 | validation: 0.059772831869227216]
	TIME [epoch: 14.9 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.055511219090943614		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.055511219090943614 | validation: 0.07333879765541218]
	TIME [epoch: 14.9 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.055785474723919204		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.055785474723919204 | validation: 0.07599683737403198]
	TIME [epoch: 14.9 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0628867041541533		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.0628867041541533 | validation: 0.09549759054190672]
	TIME [epoch: 14.9 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06577173740095538		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.06577173740095538 | validation: 0.06219143849748663]
	TIME [epoch: 14.9 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05612699010682209		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.05612699010682209 | validation: 0.06058448951907376]
	TIME [epoch: 14.9 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049724013601414624		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.049724013601414624 | validation: 0.06347011151541314]
	TIME [epoch: 15 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05145624211176768		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.05145624211176768 | validation: 0.06165505666861677]
	TIME [epoch: 14.9 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053799794701613106		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.053799794701613106 | validation: 0.0688020195966651]
	TIME [epoch: 14.9 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053289275868740146		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.053289275868740146 | validation: 0.06710733605752524]
	TIME [epoch: 14.9 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054018486520678866		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.054018486520678866 | validation: 0.06219785146440107]
	TIME [epoch: 14.9 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04973047676344057		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.04973047676344057 | validation: 0.0697623391379842]
	TIME [epoch: 14.9 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05582281594134948		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.05582281594134948 | validation: 0.06311997409167625]
	TIME [epoch: 14.9 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049351034498901186		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.049351034498901186 | validation: 0.06419000260765068]
	TIME [epoch: 14.9 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05430004548750164		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.05430004548750164 | validation: 0.057621607212628656]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_551.pth
	Model improved!!!
EPOCH 552/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051010579404424994		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.051010579404424994 | validation: 0.06529743203774285]
	TIME [epoch: 15 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05438454615028716		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.05438454615028716 | validation: 0.05781218757523457]
	TIME [epoch: 14.9 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05280322688101223		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.05280322688101223 | validation: 0.059207345632860967]
	TIME [epoch: 14.9 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05320584271539128		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.05320584271539128 | validation: 0.0580314847305167]
	TIME [epoch: 14.9 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05241943970019573		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.05241943970019573 | validation: 0.05967060635780777]
	TIME [epoch: 14.9 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05119457886917847		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.05119457886917847 | validation: 0.06216475613013156]
	TIME [epoch: 14.9 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04834126499498456		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.04834126499498456 | validation: 0.055135497953777035]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_558.pth
	Model improved!!!
EPOCH 559/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05031704258905306		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.05031704258905306 | validation: 0.07010424941332508]
	TIME [epoch: 15 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05527024131783077		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.05527024131783077 | validation: 0.058952008764016475]
	TIME [epoch: 14.9 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04946258387141568		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.04946258387141568 | validation: 0.06741968610742737]
	TIME [epoch: 14.9 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054790306199033176		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.054790306199033176 | validation: 0.06830368870174307]
	TIME [epoch: 15 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05439094614456703		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.05439094614456703 | validation: 0.07426117141662168]
	TIME [epoch: 14.9 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05333264911510409		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.05333264911510409 | validation: 0.06317041675548145]
	TIME [epoch: 14.9 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05061183882407279		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.05061183882407279 | validation: 0.06476169527759479]
	TIME [epoch: 14.9 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05253124209995069		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.05253124209995069 | validation: 0.06246309779530704]
	TIME [epoch: 15 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05137794408506502		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.05137794408506502 | validation: 0.06257860552211934]
	TIME [epoch: 14.9 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04920348417855856		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.04920348417855856 | validation: 0.05724624135585095]
	TIME [epoch: 14.9 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05223446789923494		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.05223446789923494 | validation: 0.06164392957940837]
	TIME [epoch: 14.9 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049269328926619546		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.049269328926619546 | validation: 0.06056881154293601]
	TIME [epoch: 14.9 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051399797904401995		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.051399797904401995 | validation: 0.058490073835208926]
	TIME [epoch: 14.9 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04863010302993478		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.04863010302993478 | validation: 0.06132238602983257]
	TIME [epoch: 15 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04985249206016937		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.04985249206016937 | validation: 0.06074155301756974]
	TIME [epoch: 14.9 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04843671027927793		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.04843671027927793 | validation: 0.06852890921411674]
	TIME [epoch: 14.9 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05389451128507086		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.05389451128507086 | validation: 0.06214123991385243]
	TIME [epoch: 14.9 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04828652597703831		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.04828652597703831 | validation: 0.05748459014109704]
	TIME [epoch: 14.9 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05318437345063251		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.05318437345063251 | validation: 0.062295042302895225]
	TIME [epoch: 14.9 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05149592937445763		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.05149592937445763 | validation: 0.060815936835667044]
	TIME [epoch: 14.9 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052507685390166176		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.052507685390166176 | validation: 0.06345818081298504]
	TIME [epoch: 14.9 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05527336821867866		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.05527336821867866 | validation: 0.061319009420117344]
	TIME [epoch: 14.9 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05422730957524759		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.05422730957524759 | validation: 0.06110354564120144]
	TIME [epoch: 14.9 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049334926187984574		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.049334926187984574 | validation: 0.06200878937766962]
	TIME [epoch: 14.9 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048825592319729785		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.048825592319729785 | validation: 0.0612069231955418]
	TIME [epoch: 14.9 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04993689120651737		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.04993689120651737 | validation: 0.062447596592525106]
	TIME [epoch: 14.9 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0497958127557467		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.0497958127557467 | validation: 0.06868043320522717]
	TIME [epoch: 14.9 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050657307641932575		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.050657307641932575 | validation: 0.05978753243977848]
	TIME [epoch: 14.9 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04841814926633704		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.04841814926633704 | validation: 0.06782389391387887]
	TIME [epoch: 14.9 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05346886179717987		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.05346886179717987 | validation: 0.062475063323871344]
	TIME [epoch: 15 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051143191025354084		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.051143191025354084 | validation: 0.06733561279978427]
	TIME [epoch: 14.9 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.055259800810823656		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.055259800810823656 | validation: 0.05972256893920044]
	TIME [epoch: 14.9 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04905874360114863		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.04905874360114863 | validation: 0.06553540897874359]
	TIME [epoch: 14.9 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053214371355355346		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.053214371355355346 | validation: 0.05546574673292696]
	TIME [epoch: 14.9 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05121708716947672		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.05121708716947672 | validation: 0.05580704464072174]
	TIME [epoch: 14.9 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05138268857642064		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.05138268857642064 | validation: 0.05884805391796297]
	TIME [epoch: 14.9 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05068266422051476		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.05068266422051476 | validation: 0.055030008822256574]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_595.pth
	Model improved!!!
EPOCH 596/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050027541772525566		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.050027541772525566 | validation: 0.06284008531840711]
	TIME [epoch: 14.9 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053818186335669364		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.053818186335669364 | validation: 0.06181308068725382]
	TIME [epoch: 14.9 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053610846069938656		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.053610846069938656 | validation: 0.06446035931186503]
	TIME [epoch: 15 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049814371284249176		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.049814371284249176 | validation: 0.06743418728730378]
	TIME [epoch: 14.9 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05514463850681331		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.05514463850681331 | validation: 0.056975224373549196]
	TIME [epoch: 14.9 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047176308848449755		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.047176308848449755 | validation: 0.09337782544525951]
	TIME [epoch: 15 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07419602384725081		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.07419602384725081 | validation: 0.06207905401788416]
	TIME [epoch: 14.9 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05109038159385164		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.05109038159385164 | validation: 0.055818847112452975]
	TIME [epoch: 14.9 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04729641076810749		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.04729641076810749 | validation: 0.05591022707895846]
	TIME [epoch: 14.9 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04788871895835766		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.04788871895835766 | validation: 0.05959430981584586]
	TIME [epoch: 14.9 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04921083581330715		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.04921083581330715 | validation: 0.058373220086463534]
	TIME [epoch: 14.9 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05194543024904373		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.05194543024904373 | validation: 0.06247481332182541]
	TIME [epoch: 14.9 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04874830962935175		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.04874830962935175 | validation: 0.06185715313077675]
	TIME [epoch: 14.9 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05035073154513986		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.05035073154513986 | validation: 0.057362050785860055]
	TIME [epoch: 14.9 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04974766198098207		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.04974766198098207 | validation: 0.05886653301285427]
	TIME [epoch: 14.9 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048048010134997376		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.048048010134997376 | validation: 0.05886140695875928]
	TIME [epoch: 14.9 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05007256809535722		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.05007256809535722 | validation: 0.06239967795546202]
	TIME [epoch: 14.9 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05016993950525014		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.05016993950525014 | validation: 0.05589666461760076]
	TIME [epoch: 14.9 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05172578979275491		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.05172578979275491 | validation: 0.05889483576264063]
	TIME [epoch: 14.9 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04925393961390512		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.04925393961390512 | validation: 0.059019842567174546]
	TIME [epoch: 14.9 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04910453943797288		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.04910453943797288 | validation: 0.05587935061467239]
	TIME [epoch: 14.9 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04957228599792982		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.04957228599792982 | validation: 0.06076806625291414]
	TIME [epoch: 15 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04691525880193366		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.04691525880193366 | validation: 0.05907998645644792]
	TIME [epoch: 14.9 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05190619580576153		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.05190619580576153 | validation: 0.057980328293128026]
	TIME [epoch: 14.9 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04819958477656591		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.04819958477656591 | validation: 0.05983974240338467]
	TIME [epoch: 14.9 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06627659335980673		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.06627659335980673 | validation: 0.06823091679676706]
	TIME [epoch: 14.9 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05731061031475637		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.05731061031475637 | validation: 0.06561747557489182]
	TIME [epoch: 14.9 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05050653760863214		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.05050653760863214 | validation: 0.06556327519822895]
	TIME [epoch: 14.9 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04857665384159271		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.04857665384159271 | validation: 0.06006955382361532]
	TIME [epoch: 14.9 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04655542502697051		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.04655542502697051 | validation: 0.05876959365288025]
	TIME [epoch: 14.9 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049785619859833576		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.049785619859833576 | validation: 0.058922624815121095]
	TIME [epoch: 14.9 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05142764646896704		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.05142764646896704 | validation: 0.05940416167420688]
	TIME [epoch: 15 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05107857984028738		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.05107857984028738 | validation: 0.061381432795020986]
	TIME [epoch: 14.9 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05044888834183643		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.05044888834183643 | validation: 0.05813676146410385]
	TIME [epoch: 14.9 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05285054492074605		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.05285054492074605 | validation: 0.05998364411944229]
	TIME [epoch: 15 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05141363514636674		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.05141363514636674 | validation: 0.06091439757313902]
	TIME [epoch: 14.9 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04969218309360257		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.04969218309360257 | validation: 0.06460643173579332]
	TIME [epoch: 14.9 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04934942761385382		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.04934942761385382 | validation: 0.05493171517878046]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_633.pth
	Model improved!!!
EPOCH 634/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04914118280097174		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.04914118280097174 | validation: 0.05809128044027948]
	TIME [epoch: 15 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04907330726001802		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.04907330726001802 | validation: 0.05981686606239378]
	TIME [epoch: 14.9 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04810856262153872		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.04810856262153872 | validation: 0.056063975937544176]
	TIME [epoch: 14.9 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04730760519309789		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.04730760519309789 | validation: 0.05873548827581476]
	TIME [epoch: 14.9 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04787264680602252		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.04787264680602252 | validation: 0.05727568691342147]
	TIME [epoch: 14.9 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04789339021318424		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.04789339021318424 | validation: 0.05665877031394878]
	TIME [epoch: 14.9 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04781566041790519		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.04781566041790519 | validation: 0.06528326796330176]
	TIME [epoch: 14.9 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0500803187578782		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.0500803187578782 | validation: 0.05412282350713204]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_641.pth
	Model improved!!!
EPOCH 642/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04969586317456466		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.04969586317456466 | validation: 0.05491185032702886]
	TIME [epoch: 14.9 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04715739963816373		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.04715739963816373 | validation: 0.058858094339088174]
	TIME [epoch: 15 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05032732197087854		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.05032732197087854 | validation: 0.05448865917219772]
	TIME [epoch: 14.9 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049317423212826804		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.049317423212826804 | validation: 0.05735753356229152]
	TIME [epoch: 14.9 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05041396074584033		[learning rate: 0.00012324]
	Learning Rate: 0.000123245
	LOSS [training: 0.05041396074584033 | validation: 0.060712966683194106]
	TIME [epoch: 14.9 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0483414929035797		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.0483414929035797 | validation: 0.06526471786757342]
	TIME [epoch: 14.9 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05481987023459865		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.05481987023459865 | validation: 0.05562189639690349]
	TIME [epoch: 14.9 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04816185447103394		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.04816185447103394 | validation: 0.05594351827498977]
	TIME [epoch: 14.9 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04949130040395355		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.04949130040395355 | validation: 0.057220277422432214]
	TIME [epoch: 14.9 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0471559316825467		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.0471559316825467 | validation: 0.059905272765940466]
	TIME [epoch: 14.9 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048552888015977895		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.048552888015977895 | validation: 0.05647918320223686]
	TIME [epoch: 14.9 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047712942991398916		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.047712942991398916 | validation: 0.057844083853102246]
	TIME [epoch: 15 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0474041258115145		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.0474041258115145 | validation: 0.05661122474637375]
	TIME [epoch: 14.9 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04621024647775128		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.04621024647775128 | validation: 0.06267914166214678]
	TIME [epoch: 14.9 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04730549558599865		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.04730549558599865 | validation: 0.0594612669900118]
	TIME [epoch: 15 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04515856894143078		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.04515856894143078 | validation: 0.05569133501185333]
	TIME [epoch: 14.9 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04980806250230117		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.04980806250230117 | validation: 0.05988353768959731]
	TIME [epoch: 14.9 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04834640981299047		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.04834640981299047 | validation: 0.0628780044798507]
	TIME [epoch: 14.9 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054193703120629516		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.054193703120629516 | validation: 0.05656043699027984]
	TIME [epoch: 14.9 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05111588421108178		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.05111588421108178 | validation: 0.053875536354041166]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_661.pth
	Model improved!!!
EPOCH 662/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04948438384656549		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.04948438384656549 | validation: 0.06285659128652839]
	TIME [epoch: 14.9 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0482895066078808		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.0482895066078808 | validation: 0.0551205300604561]
	TIME [epoch: 14.9 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04750587215663148		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.04750587215663148 | validation: 0.06810176072726247]
	TIME [epoch: 14.9 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0545195365825363		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.0545195365825363 | validation: 0.05865381004893147]
	TIME [epoch: 14.9 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04763962981495727		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.04763962981495727 | validation: 0.06151329685353843]
	TIME [epoch: 14.9 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05059112010253074		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.05059112010253074 | validation: 0.057670944508615964]
	TIME [epoch: 14.9 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04697183167709733		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.04697183167709733 | validation: 0.05858287910245902]
	TIME [epoch: 14.9 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04545185662740782		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.04545185662740782 | validation: 0.061518269298917086]
	TIME [epoch: 15 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0453465016412196		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.0453465016412196 | validation: 0.061509685826029385]
	TIME [epoch: 14.9 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04688479113102682		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.04688479113102682 | validation: 0.05772206662324575]
	TIME [epoch: 14.9 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04632367892200209		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.04632367892200209 | validation: 0.05593211287377297]
	TIME [epoch: 14.9 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046726499073927855		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.046726499073927855 | validation: 0.05769029209622887]
	TIME [epoch: 14.9 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050805018629777654		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.050805018629777654 | validation: 0.06093094716836448]
	TIME [epoch: 14.9 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04469661980105663		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.04469661980105663 | validation: 0.05647685575552659]
	TIME [epoch: 15 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048453578731478784		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.048453578731478784 | validation: 0.055171187412565506]
	TIME [epoch: 15 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048388992468348095		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.048388992468348095 | validation: 0.05670467846071815]
	TIME [epoch: 14.9 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047607578296106026		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.047607578296106026 | validation: 0.05722947639926266]
	TIME [epoch: 14.9 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049270799466669686		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.049270799466669686 | validation: 0.06000908056159962]
	TIME [epoch: 14.9 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04952418317795457		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.04952418317795457 | validation: 0.05440298509282401]
	TIME [epoch: 14.9 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048838600913897884		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.048838600913897884 | validation: 0.05443597440522098]
	TIME [epoch: 14.9 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047681288284280185		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.047681288284280185 | validation: 0.05856248031328988]
	TIME [epoch: 15 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04655130976346124		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.04655130976346124 | validation: 0.0606072592147193]
	TIME [epoch: 14.9 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04833524169578762		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.04833524169578762 | validation: 0.055408637427572585]
	TIME [epoch: 14.9 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047990955873878874		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.047990955873878874 | validation: 0.06966646292536526]
	TIME [epoch: 15 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054945854026699145		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.054945854026699145 | validation: 0.07062444021468686]
	TIME [epoch: 14.9 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05275183692248289		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.05275183692248289 | validation: 0.06024975785425449]
	TIME [epoch: 14.9 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047351650832515794		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.047351650832515794 | validation: 0.055326080475873556]
	TIME [epoch: 14.9 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04636179157733467		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.04636179157733467 | validation: 0.057790598736320566]
	TIME [epoch: 14.9 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04722003009632581		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.04722003009632581 | validation: 0.05500753807333386]
	TIME [epoch: 14.9 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04557598522869935		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.04557598522869935 | validation: 0.058743382701829695]
	TIME [epoch: 14.9 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04582056173864045		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.04582056173864045 | validation: 0.061145440538395684]
	TIME [epoch: 15 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04663145084514973		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.04663145084514973 | validation: 0.055240563923578914]
	TIME [epoch: 14.9 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04586061549363128		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.04586061549363128 | validation: 0.058765186737261]
	TIME [epoch: 14.9 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04739214525812198		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.04739214525812198 | validation: 0.055372015530519375]
	TIME [epoch: 14.9 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047208639721621606		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.047208639721621606 | validation: 0.06790322469945131]
	TIME [epoch: 14.9 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05860292098304616		[learning rate: 8.5871e-05]
	Learning Rate: 8.58709e-05
	LOSS [training: 0.05860292098304616 | validation: 0.07194363907503429]
	TIME [epoch: 14.9 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05819491819965815		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.05819491819965815 | validation: 0.05799274278211402]
	TIME [epoch: 14.9 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049804812388668615		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.049804812388668615 | validation: 0.055754984601176096]
	TIME [epoch: 14.9 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04922367423834007		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.04922367423834007 | validation: 0.056356935066838476]
	TIME [epoch: 14.9 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04774128117053275		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.04774128117053275 | validation: 0.05623009538993732]
	TIME [epoch: 15 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04786098417723238		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.04786098417723238 | validation: 0.055261611017120196]
	TIME [epoch: 14.9 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04593150740703579		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.04593150740703579 | validation: 0.05404638517341272]
	TIME [epoch: 14.9 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047322138435512626		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.047322138435512626 | validation: 0.056196233581819494]
	TIME [epoch: 14.9 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049225892293153856		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.049225892293153856 | validation: 0.05846836981756646]
	TIME [epoch: 15 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04741421087966216		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.04741421087966216 | validation: 0.054766672517586434]
	TIME [epoch: 14.9 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048072537163549284		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.048072537163549284 | validation: 0.058859788227267985]
	TIME [epoch: 14.9 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045833249300895385		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.045833249300895385 | validation: 0.053851272420674716]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_708.pth
	Model improved!!!
EPOCH 709/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045538481369785555		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.045538481369785555 | validation: 0.05774052446292313]
	TIME [epoch: 14.9 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04888945304019385		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.04888945304019385 | validation: 0.053301039027875356]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_710.pth
	Model improved!!!
EPOCH 711/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04914807648476785		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.04914807648476785 | validation: 0.05902485514968954]
	TIME [epoch: 14.9 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04675652108737323		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.04675652108737323 | validation: 0.0566586256919996]
	TIME [epoch: 14.9 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04646846234565527		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.04646846234565527 | validation: 0.061192876822056236]
	TIME [epoch: 14.9 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047822518423618024		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.047822518423618024 | validation: 0.05529446040808367]
	TIME [epoch: 14.9 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04507467016445349		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.04507467016445349 | validation: 0.05729629364164958]
	TIME [epoch: 14.9 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047297119109096605		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.047297119109096605 | validation: 0.05608750105219001]
	TIME [epoch: 14.9 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04555460331368205		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.04555460331368205 | validation: 0.057730551139285914]
	TIME [epoch: 14.9 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04609227170512841		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.04609227170512841 | validation: 0.05874265882669908]
	TIME [epoch: 14.9 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04752794930959686		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.04752794930959686 | validation: 0.05588727620016598]
	TIME [epoch: 14.9 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046840550518859145		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.046840550518859145 | validation: 0.05609262428655522]
	TIME [epoch: 14.9 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04651859961455877		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.04651859961455877 | validation: 0.0577324031225076]
	TIME [epoch: 14.9 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046022960415786615		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.046022960415786615 | validation: 0.058211582654547234]
	TIME [epoch: 14.9 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04842829959704652		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.04842829959704652 | validation: 0.05892411695977826]
	TIME [epoch: 14.9 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04951246912916134		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.04951246912916134 | validation: 0.056771882702431255]
	TIME [epoch: 15 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047054557638557724		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.047054557638557724 | validation: 0.056238294657454495]
	TIME [epoch: 14.9 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047201041182353576		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.047201041182353576 | validation: 0.0642311887832147]
	TIME [epoch: 14.9 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04720202107570605		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.04720202107570605 | validation: 0.05847844450767046]
	TIME [epoch: 14.9 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047110060908407594		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.047110060908407594 | validation: 0.055575964299457874]
	TIME [epoch: 14.9 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04823554048443372		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.04823554048443372 | validation: 0.057058842444963814]
	TIME [epoch: 14.9 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04816338519219339		[learning rate: 6.7968e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.04816338519219339 | validation: 0.06030782521380127]
	TIME [epoch: 14.9 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04452358895929874		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.04452358895929874 | validation: 0.054686247616531874]
	TIME [epoch: 14.9 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04561323253977342		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.04561323253977342 | validation: 0.055090779520008626]
	TIME [epoch: 14.9 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044886535767642455		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.044886535767642455 | validation: 0.05137905702128982]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_733.pth
	Model improved!!!
EPOCH 734/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04420189642997638		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.04420189642997638 | validation: 0.060136003014576216]
	TIME [epoch: 14.9 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04699986050789791		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.04699986050789791 | validation: 0.05922269088102135]
	TIME [epoch: 14.9 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04516588882855685		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.04516588882855685 | validation: 0.06075739652868626]
	TIME [epoch: 14.9 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04533729914093275		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.04533729914093275 | validation: 0.05333582591679309]
	TIME [epoch: 15 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04509217522802986		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.04509217522802986 | validation: 0.055990957057918435]
	TIME [epoch: 14.9 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05076279663975237		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.05076279663975237 | validation: 0.05336557824578162]
	TIME [epoch: 14.9 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045165846018157416		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.045165846018157416 | validation: 0.05709780093613216]
	TIME [epoch: 15 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04597031723866424		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.04597031723866424 | validation: 0.05929738773041118]
	TIME [epoch: 14.9 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04663702588610319		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.04663702588610319 | validation: 0.05829431131008023]
	TIME [epoch: 14.9 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046820013648933075		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.046820013648933075 | validation: 0.059968102311096706]
	TIME [epoch: 14.9 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04841443984094098		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.04841443984094098 | validation: 0.05368781861246677]
	TIME [epoch: 14.9 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046545368988636024		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.046545368988636024 | validation: 0.05470452959986674]
	TIME [epoch: 14.9 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04501874868402504		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.04501874868402504 | validation: 0.059483777180016364]
	TIME [epoch: 14.9 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04819440669133418		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.04819440669133418 | validation: 0.05098476457834903]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_747.pth
	Model improved!!!
EPOCH 748/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046145838804135135		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.046145838804135135 | validation: 0.05750735291211312]
	TIME [epoch: 14.9 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0476764120880903		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.0476764120880903 | validation: 0.05543880062309858]
	TIME [epoch: 14.9 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04673353928031879		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.04673353928031879 | validation: 0.054115027388590045]
	TIME [epoch: 14.9 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04504991431832356		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.04504991431832356 | validation: 0.05313171830106936]
	TIME [epoch: 14.9 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04822620563287411		[learning rate: 5.8159e-05]
	Learning Rate: 5.81588e-05
	LOSS [training: 0.04822620563287411 | validation: 0.06042926322255511]
	TIME [epoch: 14.9 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048507031461809874		[learning rate: 5.7748e-05]
	Learning Rate: 5.77482e-05
	LOSS [training: 0.048507031461809874 | validation: 0.060014403166552335]
	TIME [epoch: 15 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049768934687834325		[learning rate: 5.7341e-05]
	Learning Rate: 5.73405e-05
	LOSS [training: 0.049768934687834325 | validation: 0.05906457924055711]
	TIME [epoch: 14.9 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04683832889515863		[learning rate: 5.6936e-05]
	Learning Rate: 5.69357e-05
	LOSS [training: 0.04683832889515863 | validation: 0.057771751560737876]
	TIME [epoch: 14.9 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0454998733148837		[learning rate: 5.6534e-05]
	Learning Rate: 5.65337e-05
	LOSS [training: 0.0454998733148837 | validation: 0.059663724285586806]
	TIME [epoch: 14.9 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0472976970979334		[learning rate: 5.6135e-05]
	Learning Rate: 5.61346e-05
	LOSS [training: 0.0472976970979334 | validation: 0.05385355153818008]
	TIME [epoch: 14.9 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0471778654701059		[learning rate: 5.5738e-05]
	Learning Rate: 5.57383e-05
	LOSS [training: 0.0471778654701059 | validation: 0.06000820593377576]
	TIME [epoch: 14.9 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047735732496714825		[learning rate: 5.5345e-05]
	Learning Rate: 5.53448e-05
	LOSS [training: 0.047735732496714825 | validation: 0.05236821347708073]
	TIME [epoch: 14.9 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045889280847761026		[learning rate: 5.4954e-05]
	Learning Rate: 5.49541e-05
	LOSS [training: 0.045889280847761026 | validation: 0.05830897390742778]
	TIME [epoch: 14.9 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046053125841920534		[learning rate: 5.4566e-05]
	Learning Rate: 5.45661e-05
	LOSS [training: 0.046053125841920534 | validation: 0.05878639003337876]
	TIME [epoch: 14.9 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046467596247421694		[learning rate: 5.4181e-05]
	Learning Rate: 5.41809e-05
	LOSS [training: 0.046467596247421694 | validation: 0.05695433893704124]
	TIME [epoch: 14.9 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045661490002994604		[learning rate: 5.3798e-05]
	Learning Rate: 5.37984e-05
	LOSS [training: 0.045661490002994604 | validation: 0.05617555314780718]
	TIME [epoch: 14.9 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047492746236570516		[learning rate: 5.3419e-05]
	Learning Rate: 5.34186e-05
	LOSS [training: 0.047492746236570516 | validation: 0.05871070975204226]
	TIME [epoch: 14.9 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04533347858282555		[learning rate: 5.3041e-05]
	Learning Rate: 5.30415e-05
	LOSS [training: 0.04533347858282555 | validation: 0.05735334745541394]
	TIME [epoch: 14.9 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04758101260501313		[learning rate: 5.2667e-05]
	Learning Rate: 5.2667e-05
	LOSS [training: 0.04758101260501313 | validation: 0.05441839596890674]
	TIME [epoch: 14.9 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04592396093387186		[learning rate: 5.2295e-05]
	Learning Rate: 5.22952e-05
	LOSS [training: 0.04592396093387186 | validation: 0.05363892102264313]
	TIME [epoch: 14.9 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04552638077761949		[learning rate: 5.1926e-05]
	Learning Rate: 5.1926e-05
	LOSS [training: 0.04552638077761949 | validation: 0.05676309741209356]
	TIME [epoch: 14.9 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045523092512616915		[learning rate: 5.1559e-05]
	Learning Rate: 5.15594e-05
	LOSS [training: 0.045523092512616915 | validation: 0.0512881792378718]
	TIME [epoch: 15 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04830259990128193		[learning rate: 5.1195e-05]
	Learning Rate: 5.11954e-05
	LOSS [training: 0.04830259990128193 | validation: 0.056962933654976115]
	TIME [epoch: 14.9 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04570004196587327		[learning rate: 5.0834e-05]
	Learning Rate: 5.0834e-05
	LOSS [training: 0.04570004196587327 | validation: 0.053203617777410644]
	TIME [epoch: 14.9 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04447727366738824		[learning rate: 5.0475e-05]
	Learning Rate: 5.04751e-05
	LOSS [training: 0.04447727366738824 | validation: 0.0558094637361442]
	TIME [epoch: 14.9 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045324043339934256		[learning rate: 5.0119e-05]
	Learning Rate: 5.01187e-05
	LOSS [training: 0.045324043339934256 | validation: 0.061282219470342195]
	TIME [epoch: 14.9 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04718610557870116		[learning rate: 4.9765e-05]
	Learning Rate: 4.97649e-05
	LOSS [training: 0.04718610557870116 | validation: 0.05704781173953077]
	TIME [epoch: 14.9 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04331052383215951		[learning rate: 4.9414e-05]
	Learning Rate: 4.94136e-05
	LOSS [training: 0.04331052383215951 | validation: 0.05776438727340816]
	TIME [epoch: 14.9 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04600864302497011		[learning rate: 4.9065e-05]
	Learning Rate: 4.90647e-05
	LOSS [training: 0.04600864302497011 | validation: 0.058530639456122614]
	TIME [epoch: 14.9 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047273347245718314		[learning rate: 4.8718e-05]
	Learning Rate: 4.87183e-05
	LOSS [training: 0.047273347245718314 | validation: 0.055676362394364465]
	TIME [epoch: 14.9 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045290132603013386		[learning rate: 4.8374e-05]
	Learning Rate: 4.83744e-05
	LOSS [training: 0.045290132603013386 | validation: 0.05449508654734412]
	TIME [epoch: 14.9 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049282549333244025		[learning rate: 4.8033e-05]
	Learning Rate: 4.80329e-05
	LOSS [training: 0.049282549333244025 | validation: 0.053412175014557334]
	TIME [epoch: 14.9 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04747400678374538		[learning rate: 4.7694e-05]
	Learning Rate: 4.76938e-05
	LOSS [training: 0.04747400678374538 | validation: 0.05394441170483133]
	TIME [epoch: 14.9 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04888101441383133		[learning rate: 4.7357e-05]
	Learning Rate: 4.73571e-05
	LOSS [training: 0.04888101441383133 | validation: 0.05887761387089401]
	TIME [epoch: 14.9 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04766330921562509		[learning rate: 4.7023e-05]
	Learning Rate: 4.70227e-05
	LOSS [training: 0.04766330921562509 | validation: 0.057686725408766605]
	TIME [epoch: 14.9 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047200156749180114		[learning rate: 4.6691e-05]
	Learning Rate: 4.66907e-05
	LOSS [training: 0.047200156749180114 | validation: 0.05851201539760999]
	TIME [epoch: 14.9 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048706826906331616		[learning rate: 4.6361e-05]
	Learning Rate: 4.63611e-05
	LOSS [training: 0.048706826906331616 | validation: 0.05708500060166021]
	TIME [epoch: 14.9 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04722234724340817		[learning rate: 4.6034e-05]
	Learning Rate: 4.60338e-05
	LOSS [training: 0.04722234724340817 | validation: 0.05939486315768494]
	TIME [epoch: 14.9 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04502282802550228		[learning rate: 4.5709e-05]
	Learning Rate: 4.57088e-05
	LOSS [training: 0.04502282802550228 | validation: 0.05543907442555573]
	TIME [epoch: 14.9 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04638956150483612		[learning rate: 4.5386e-05]
	Learning Rate: 4.53861e-05
	LOSS [training: 0.04638956150483612 | validation: 0.05927817777867966]
	TIME [epoch: 14.9 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050774657779074917		[learning rate: 4.5066e-05]
	Learning Rate: 4.50657e-05
	LOSS [training: 0.050774657779074917 | validation: 0.05871980004566768]
	TIME [epoch: 14.9 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04826682314422513		[learning rate: 4.4748e-05]
	Learning Rate: 4.47476e-05
	LOSS [training: 0.04826682314422513 | validation: 0.05730207652540752]
	TIME [epoch: 14.9 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04654849202504296		[learning rate: 4.4432e-05]
	Learning Rate: 4.44316e-05
	LOSS [training: 0.04654849202504296 | validation: 0.05319387547584482]
	TIME [epoch: 14.9 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04733843477466386		[learning rate: 4.4118e-05]
	Learning Rate: 4.4118e-05
	LOSS [training: 0.04733843477466386 | validation: 0.05571023924205437]
	TIME [epoch: 14.9 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04477461295585909		[learning rate: 4.3807e-05]
	Learning Rate: 4.38065e-05
	LOSS [training: 0.04477461295585909 | validation: 0.05483644347395477]
	TIME [epoch: 15 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04419835029031127		[learning rate: 4.3497e-05]
	Learning Rate: 4.34972e-05
	LOSS [training: 0.04419835029031127 | validation: 0.054885223885829615]
	TIME [epoch: 14.9 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04693876856238103		[learning rate: 4.319e-05]
	Learning Rate: 4.31902e-05
	LOSS [training: 0.04693876856238103 | validation: 0.05734118171450791]
	TIME [epoch: 14.9 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04500387632657178		[learning rate: 4.2885e-05]
	Learning Rate: 4.28852e-05
	LOSS [training: 0.04500387632657178 | validation: 0.05539716648343854]
	TIME [epoch: 15 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044726803785427		[learning rate: 4.2582e-05]
	Learning Rate: 4.25825e-05
	LOSS [training: 0.044726803785427 | validation: 0.05698756237749434]
	TIME [epoch: 15 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048377741948727855		[learning rate: 4.2282e-05]
	Learning Rate: 4.22819e-05
	LOSS [training: 0.048377741948727855 | validation: 0.06130155752483033]
	TIME [epoch: 14.9 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04446932614930184		[learning rate: 4.1983e-05]
	Learning Rate: 4.19833e-05
	LOSS [training: 0.04446932614930184 | validation: 0.054699457519104794]
	TIME [epoch: 15 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04751621743533693		[learning rate: 4.1687e-05]
	Learning Rate: 4.16869e-05
	LOSS [training: 0.04751621743533693 | validation: 0.05978030834208684]
	TIME [epoch: 14.9 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044959422826493		[learning rate: 4.1393e-05]
	Learning Rate: 4.13926e-05
	LOSS [training: 0.044959422826493 | validation: 0.055775799790885885]
	TIME [epoch: 14.9 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04639898639711981		[learning rate: 4.11e-05]
	Learning Rate: 4.11004e-05
	LOSS [training: 0.04639898639711981 | validation: 0.05644771997123615]
	TIME [epoch: 14.9 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045928453896097746		[learning rate: 4.081e-05]
	Learning Rate: 4.08103e-05
	LOSS [training: 0.045928453896097746 | validation: 0.05509872799576143]
	TIME [epoch: 15 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044360916887460106		[learning rate: 4.0522e-05]
	Learning Rate: 4.05221e-05
	LOSS [training: 0.044360916887460106 | validation: 0.05610082052328039]
	TIME [epoch: 14.9 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045373835350442066		[learning rate: 4.0236e-05]
	Learning Rate: 4.02361e-05
	LOSS [training: 0.045373835350442066 | validation: 0.05509402794995893]
	TIME [epoch: 14.9 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0449858015423761		[learning rate: 3.9952e-05]
	Learning Rate: 3.9952e-05
	LOSS [training: 0.0449858015423761 | validation: 0.05685692801814684]
	TIME [epoch: 15 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04664211447378658		[learning rate: 3.967e-05]
	Learning Rate: 3.967e-05
	LOSS [training: 0.04664211447378658 | validation: 0.05738179897984187]
	TIME [epoch: 14.9 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048236851777676754		[learning rate: 3.939e-05]
	Learning Rate: 3.93899e-05
	LOSS [training: 0.048236851777676754 | validation: 0.056697724437299125]
	TIME [epoch: 14.9 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04747532055660608		[learning rate: 3.9112e-05]
	Learning Rate: 3.91118e-05
	LOSS [training: 0.04747532055660608 | validation: 0.05888933523609929]
	TIME [epoch: 14.9 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04664206996070886		[learning rate: 3.8836e-05]
	Learning Rate: 3.88357e-05
	LOSS [training: 0.04664206996070886 | validation: 0.05491149075972941]
	TIME [epoch: 14.9 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046380644596962436		[learning rate: 3.8561e-05]
	Learning Rate: 3.85615e-05
	LOSS [training: 0.046380644596962436 | validation: 0.057746300446255276]
	TIME [epoch: 14.9 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04625257211138062		[learning rate: 3.8289e-05]
	Learning Rate: 3.82893e-05
	LOSS [training: 0.04625257211138062 | validation: 0.05383472252976201]
	TIME [epoch: 14.9 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04384653164991755		[learning rate: 3.8019e-05]
	Learning Rate: 3.8019e-05
	LOSS [training: 0.04384653164991755 | validation: 0.05278093662336182]
	TIME [epoch: 14.9 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04501749908913329		[learning rate: 3.7751e-05]
	Learning Rate: 3.77505e-05
	LOSS [training: 0.04501749908913329 | validation: 0.06122655108441173]
	TIME [epoch: 14.9 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04654484289826602		[learning rate: 3.7484e-05]
	Learning Rate: 3.7484e-05
	LOSS [training: 0.04654484289826602 | validation: 0.05363328067972646]
	TIME [epoch: 14.9 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04743219437922862		[learning rate: 3.7219e-05]
	Learning Rate: 3.72194e-05
	LOSS [training: 0.04743219437922862 | validation: 0.056464661530576256]
	TIME [epoch: 15 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04673116841716569		[learning rate: 3.6957e-05]
	Learning Rate: 3.69566e-05
	LOSS [training: 0.04673116841716569 | validation: 0.05521842053602055]
	TIME [epoch: 14.9 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0439389281664367		[learning rate: 3.6696e-05]
	Learning Rate: 3.66957e-05
	LOSS [training: 0.0439389281664367 | validation: 0.05517804982256681]
	TIME [epoch: 14.9 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04552895135731819		[learning rate: 3.6437e-05]
	Learning Rate: 3.64367e-05
	LOSS [training: 0.04552895135731819 | validation: 0.053709448610412304]
	TIME [epoch: 15 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0465935912937681		[learning rate: 3.6179e-05]
	Learning Rate: 3.61794e-05
	LOSS [training: 0.0465935912937681 | validation: 0.056901248594551125]
	TIME [epoch: 14.9 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044925935421707495		[learning rate: 3.5924e-05]
	Learning Rate: 3.5924e-05
	LOSS [training: 0.044925935421707495 | validation: 0.055084331938639555]
	TIME [epoch: 14.9 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04436036925331477		[learning rate: 3.567e-05]
	Learning Rate: 3.56704e-05
	LOSS [training: 0.04436036925331477 | validation: 0.05701234493941792]
	TIME [epoch: 15 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04526623618197141		[learning rate: 3.5419e-05]
	Learning Rate: 3.54186e-05
	LOSS [training: 0.04526623618197141 | validation: 0.06145546017275706]
	TIME [epoch: 14.9 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046594767521778595		[learning rate: 3.5169e-05]
	Learning Rate: 3.51685e-05
	LOSS [training: 0.046594767521778595 | validation: 0.054628665393881726]
	TIME [epoch: 14.9 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047706663009185094		[learning rate: 3.492e-05]
	Learning Rate: 3.49202e-05
	LOSS [training: 0.047706663009185094 | validation: 0.05677449235377996]
	TIME [epoch: 14.9 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04712585416663668		[learning rate: 3.4674e-05]
	Learning Rate: 3.46737e-05
	LOSS [training: 0.04712585416663668 | validation: 0.05685284720244706]
	TIME [epoch: 14.9 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04856866816595614		[learning rate: 3.4429e-05]
	Learning Rate: 3.44289e-05
	LOSS [training: 0.04856866816595614 | validation: 0.05326300898863676]
	TIME [epoch: 14.9 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04445385612922739		[learning rate: 3.4186e-05]
	Learning Rate: 3.41858e-05
	LOSS [training: 0.04445385612922739 | validation: 0.05307574025115598]
	TIME [epoch: 14.9 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04866951525252926		[learning rate: 3.3944e-05]
	Learning Rate: 3.39445e-05
	LOSS [training: 0.04866951525252926 | validation: 0.056706650459665144]
	TIME [epoch: 14.9 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04959238310315828		[learning rate: 3.3705e-05]
	Learning Rate: 3.37049e-05
	LOSS [training: 0.04959238310315828 | validation: 0.05811877878148541]
	TIME [epoch: 14.9 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04593726300228222		[learning rate: 3.3467e-05]
	Learning Rate: 3.34669e-05
	LOSS [training: 0.04593726300228222 | validation: 0.060433468034163385]
	TIME [epoch: 14.9 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04629984485132435		[learning rate: 3.3231e-05]
	Learning Rate: 3.32306e-05
	LOSS [training: 0.04629984485132435 | validation: 0.05338865431505731]
	TIME [epoch: 15 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04471642849690207		[learning rate: 3.2996e-05]
	Learning Rate: 3.2996e-05
	LOSS [training: 0.04471642849690207 | validation: 0.054576998687281365]
	TIME [epoch: 14.9 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045414141063721436		[learning rate: 3.2763e-05]
	Learning Rate: 3.27631e-05
	LOSS [training: 0.045414141063721436 | validation: 0.05426836841860819]
	TIME [epoch: 14.9 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04240630430340718		[learning rate: 3.2532e-05]
	Learning Rate: 3.25318e-05
	LOSS [training: 0.04240630430340718 | validation: 0.061795575648259764]
	TIME [epoch: 15 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04569509376856355		[learning rate: 3.2302e-05]
	Learning Rate: 3.23021e-05
	LOSS [training: 0.04569509376856355 | validation: 0.056360400042180715]
	TIME [epoch: 14.9 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04737683091232396		[learning rate: 3.2074e-05]
	Learning Rate: 3.20741e-05
	LOSS [training: 0.04737683091232396 | validation: 0.05307595172004849]
	TIME [epoch: 14.9 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04431724502091053		[learning rate: 3.1848e-05]
	Learning Rate: 3.18476e-05
	LOSS [training: 0.04431724502091053 | validation: 0.05768415590866265]
	TIME [epoch: 14.9 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04561482664269813		[learning rate: 3.1623e-05]
	Learning Rate: 3.16228e-05
	LOSS [training: 0.04561482664269813 | validation: 0.05469281756863557]
	TIME [epoch: 14.9 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04767740708364735		[learning rate: 3.14e-05]
	Learning Rate: 3.13995e-05
	LOSS [training: 0.04767740708364735 | validation: 0.058771860771748535]
	TIME [epoch: 14.9 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04530748403463459		[learning rate: 3.1178e-05]
	Learning Rate: 3.11778e-05
	LOSS [training: 0.04530748403463459 | validation: 0.05957617708422898]
	TIME [epoch: 14.9 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04541732636385242		[learning rate: 3.0958e-05]
	Learning Rate: 3.09577e-05
	LOSS [training: 0.04541732636385242 | validation: 0.05758313981389693]
	TIME [epoch: 14.9 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045675737246336875		[learning rate: 3.0739e-05]
	Learning Rate: 3.07392e-05
	LOSS [training: 0.045675737246336875 | validation: 0.0561979123097375]
	TIME [epoch: 14.9 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046068335712901864		[learning rate: 3.0522e-05]
	Learning Rate: 3.05222e-05
	LOSS [training: 0.046068335712901864 | validation: 0.060955272202953825]
	TIME [epoch: 14.9 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047818165866248555		[learning rate: 3.0307e-05]
	Learning Rate: 3.03067e-05
	LOSS [training: 0.047818165866248555 | validation: 0.05857741876709552]
	TIME [epoch: 14.9 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04893474210878215		[learning rate: 3.0093e-05]
	Learning Rate: 3.00927e-05
	LOSS [training: 0.04893474210878215 | validation: 0.053386442244070464]
	TIME [epoch: 14.9 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0429109325816831		[learning rate: 2.988e-05]
	Learning Rate: 2.98803e-05
	LOSS [training: 0.0429109325816831 | validation: 0.05266360767581379]
	TIME [epoch: 14.9 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043405512324892676		[learning rate: 2.9669e-05]
	Learning Rate: 2.96693e-05
	LOSS [training: 0.043405512324892676 | validation: 0.05392699976016179]
	TIME [epoch: 14.9 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04445970561926594		[learning rate: 2.946e-05]
	Learning Rate: 2.94599e-05
	LOSS [training: 0.04445970561926594 | validation: 0.057045922187177724]
	TIME [epoch: 14.9 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240807_171303/states/model_phi1_2b_v_mmd1_848.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 8318.934 seconds.
