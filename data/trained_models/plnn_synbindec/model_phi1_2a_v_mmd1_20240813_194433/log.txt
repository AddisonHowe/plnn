Args:
Namespace(name='model_phi1_2a_v_mmd1', outdir='out/model_training/model_phi1_2a_v_mmd1', training_data='data/training_data/data_phi1_2a/training', validation_data='data/training_data/data_phi1_2a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 890413555

Training model...

Saving initial model state to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 2/2] avg loss: 5.424564264618388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.424564264618388 | validation: 5.837841982497935]
	TIME [epoch: 103 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 2/2] avg loss: 5.367658556139617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.367658556139617 | validation: 5.5870911987595635]
	TIME [epoch: 1.74 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 2/2] avg loss: 5.203062859927779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.203062859927779 | validation: 5.059048814913032]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.794844632940714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.794844632940714 | validation: 5.0217510759590205]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.756040013153977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.756040013153977 | validation: 4.97509203179478]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.683895887098931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.683895887098931 | validation: 4.8760613190858555]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.53205063303237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.53205063303237 | validation: 4.928616918177278]
	TIME [epoch: 1.64 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.6938095686053725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6938095686053725 | validation: 4.7568445789140075]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.324254303085107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.324254303085107 | validation: 4.369570612725076]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.384869621247129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.384869621247129 | validation: 4.195687276682996]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.981889348239837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.981889348239837 | validation: 4.101497670874786]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.6999515939480867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6999515939480867 | validation: 3.928721135411488]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.5634007106476635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5634007106476635 | validation: 3.4772300360280997]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.2421664251546893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2421664251546893 | validation: 3.1924218895515315]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.101491920135998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.101491920135998 | validation: 3.2048299313665702]
	TIME [epoch: 1.64 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.915609517722781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.915609517722781 | validation: 2.585129938041131]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.5264866882321537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5264866882321537 | validation: 3.083685063816631]
	TIME [epoch: 1.64 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.8928698752341977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8928698752341977 | validation: 2.5363722791670513]
	TIME [epoch: 1.92 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.2743612661423915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2743612661423915 | validation: 2.2504771050253534]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.123428286776652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.123428286776652 | validation: 2.080002861205409]
	TIME [epoch: 1.65 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.9669961576717023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9669961576717023 | validation: 2.0975343950547862]
	TIME [epoch: 1.64 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.224670613840453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.224670613840453 | validation: 1.9882612618066557]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.8691845590145633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8691845590145633 | validation: 2.4553898645529553]
	TIME [epoch: 1.63 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.2041868067884405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2041868067884405 | validation: 2.1811746229390234]
	TIME [epoch: 1.64 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.0860587889264366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0860587889264366 | validation: 1.73670771213691]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.7574151080031397		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.7574151080031397 | validation: 1.7087972588183786]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.6418173832257206		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.6418173832257206 | validation: 1.615871851963611]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.6425720054364827		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.6425720054364827 | validation: 1.6219039281037357]
	TIME [epoch: 1.64 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.6427187345971856		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.6427187345971856 | validation: 1.7280874432131097]
	TIME [epoch: 1.64 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5101681236068756		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.5101681236068756 | validation: 1.4759135369652476]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5102696294100486		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.5102696294100486 | validation: 1.3365985528168567]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4467658669032009		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.4467658669032009 | validation: 1.5715361638158631]
	TIME [epoch: 1.64 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.6208640731711705		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.6208640731711705 | validation: 1.3360020622998974]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3872971983811664		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.3872971983811664 | validation: 1.3163808388273037]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.352366366349707		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.352366366349707 | validation: 1.363184886942414]
	TIME [epoch: 1.63 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3045331576301362		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.3045331576301362 | validation: 1.251216214762019]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.287426533539412		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.287426533539412 | validation: 1.189003006719475]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2654022793260524		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.2654022793260524 | validation: 1.8196987113303238]
	TIME [epoch: 1.65 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.6427300513924101		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.6427300513924101 | validation: 2.087561768041111]
	TIME [epoch: 1.65 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.716816132023275		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.716816132023275 | validation: 1.3952909336112826]
	TIME [epoch: 1.65 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3792860057191634		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.3792860057191634 | validation: 1.2427659017107384]
	TIME [epoch: 1.64 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2367655015764778		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.2367655015764778 | validation: 1.1464713068826853]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1788813542061365		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.1788813542061365 | validation: 1.1409322997095621]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2573748518720562		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.2573748518720562 | validation: 1.2487679470059079]
	TIME [epoch: 1.63 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.285149351416118		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 1.285149351416118 | validation: 1.114185570351582]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2406153718126758		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.2406153718126758 | validation: 1.3670553154816096]
	TIME [epoch: 1.63 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.38601088134557		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.38601088134557 | validation: 1.161189945533845]
	TIME [epoch: 1.64 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1887562711227906		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.1887562711227906 | validation: 1.172330085345182]
	TIME [epoch: 1.63 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.212464108816018		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.212464108816018 | validation: 1.0771126198205188]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1565321783668656		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.1565321783668656 | validation: 1.0201602160177796]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0868106514672702		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.0868106514672702 | validation: 1.014332385975699]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1043876643593484		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.1043876643593484 | validation: 1.492683443542932]
	TIME [epoch: 1.63 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3932436945970657		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.3932436945970657 | validation: 1.0552806403694628]
	TIME [epoch: 1.64 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0787948740056805		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.0787948740056805 | validation: 0.9791345112819386]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.095919586416759		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.095919586416759 | validation: 1.0755345185360878]
	TIME [epoch: 1.63 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1259317783338445		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.1259317783338445 | validation: 1.0892233843997146]
	TIME [epoch: 1.64 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2853822866328604		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.2853822866328604 | validation: 1.0746783819482717]
	TIME [epoch: 1.63 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2589328796150787		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.2589328796150787 | validation: 0.9671350728416366]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0953632951065133		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.0953632951065133 | validation: 1.1066835128548564]
	TIME [epoch: 1.65 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1022885890100151		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.1022885890100151 | validation: 0.9386611278977409]
	TIME [epoch: 1.65 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9908660392707411		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.9908660392707411 | validation: 0.8884118731908679]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9781554667810547		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.9781554667810547 | validation: 0.9583856582927297]
	TIME [epoch: 1.64 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2393613517516715		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.2393613517516715 | validation: 1.1431284256691265]
	TIME [epoch: 1.63 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.110838451773374		[learning rate: 0.0076127]
	Learning Rate: 0.00761269
	LOSS [training: 1.110838451773374 | validation: 0.918623366039395]
	TIME [epoch: 1.64 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0703415549721225		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.0703415549721225 | validation: 1.168262108718937]
	TIME [epoch: 1.63 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.082482750536251		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.082482750536251 | validation: 0.922517513527081]
	TIME [epoch: 1.63 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0086965488194815		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.0086965488194815 | validation: 0.8506382709766598]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9821266111221265		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.9821266111221265 | validation: 0.8851772006768975]
	TIME [epoch: 1.64 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.020395899954814		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.020395899954814 | validation: 1.005413748344487]
	TIME [epoch: 1.63 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0457069897286355		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.0457069897286355 | validation: 0.9681343054886636]
	TIME [epoch: 1.63 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.086288570764299		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.086288570764299 | validation: 1.0029026374685899]
	TIME [epoch: 1.63 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.058840078176392		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.058840078176392 | validation: 0.873132669507627]
	TIME [epoch: 1.63 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9012340939571084		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.9012340939571084 | validation: 0.7708500048411376]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8617271812396532		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.8617271812396532 | validation: 0.7457346590088936]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9676897147875538		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.9676897147875538 | validation: 1.104242221505141]
	TIME [epoch: 1.63 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9781087667141712		[learning rate: 0.0069922]
	Learning Rate: 0.00699222
	LOSS [training: 0.9781087667141712 | validation: 0.7821857216144129]
	TIME [epoch: 1.64 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9317157314021747		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.9317157314021747 | validation: 0.9409147648139595]
	TIME [epoch: 1.64 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9329504095291694		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.9329504095291694 | validation: 0.7915112911983546]
	TIME [epoch: 1.65 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8321198850998476		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.8321198850998476 | validation: 0.7045511131915165]
	TIME [epoch: 1.65 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7933883573643331		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.7933883573643331 | validation: 0.6680688552913154]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.863281603672891		[learning rate: 0.0067489]
	Learning Rate: 0.00674887
	LOSS [training: 0.863281603672891 | validation: 1.1315546780564405]
	TIME [epoch: 1.63 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0496615032942147		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.0496615032942147 | validation: 0.8369428796207701]
	TIME [epoch: 1.63 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8486813655286869		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.8486813655286869 | validation: 0.642874608251061]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7316557714689556		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.7316557714689556 | validation: 0.6459581662130444]
	TIME [epoch: 1.63 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.813278641847655		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.813278641847655 | validation: 0.7339809880014947]
	TIME [epoch: 1.63 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9130663746609917		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.9130663746609917 | validation: 0.6426221746419896]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9332191776413521		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.9332191776413521 | validation: 0.904875820737516]
	TIME [epoch: 1.63 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8187286357115806		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.8187286357115806 | validation: 0.7074172978791448]
	TIME [epoch: 1.63 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7802506499668069		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.7802506499668069 | validation: 0.608779443762242]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6480472715650447		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.6480472715650447 | validation: 0.769753382096932]
	TIME [epoch: 1.63 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8341521260307319		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.8341521260307319 | validation: 0.5635806869038166]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6469840569173604		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.6469840569173604 | validation: 0.5650946354458359]
	TIME [epoch: 1.64 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7440110654737523		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.7440110654737523 | validation: 0.612998289469352]
	TIME [epoch: 1.63 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7541543848435042		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.7541543848435042 | validation: 0.5163894831391098]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5523124508749984		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.5523124508749984 | validation: 0.5227156303772748]
	TIME [epoch: 1.64 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6907944814277496		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.6907944814277496 | validation: 0.764110157738593]
	TIME [epoch: 1.63 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7930637391531337		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.7930637391531337 | validation: 0.6888917779716246]
	TIME [epoch: 1.63 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6626552370825175		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.6626552370825175 | validation: 0.6600623167800084]
	TIME [epoch: 1.65 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7703684510018689		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.7703684510018689 | validation: 0.4805603461066243]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5713333344305567		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.5713333344305567 | validation: 0.45225555475445756]
	TIME [epoch: 1.66 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5660531075058901		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.5660531075058901 | validation: 0.5193098994592472]
	TIME [epoch: 1.64 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6139956146873351		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.6139956146873351 | validation: 0.42863720440330316]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4861552917608937		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.4861552917608937 | validation: 0.4873550787903099]
	TIME [epoch: 1.64 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.586085586513141		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.586085586513141 | validation: 0.4521631192872764]
	TIME [epoch: 1.64 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5530698370237541		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.5530698370237541 | validation: 0.4113249802180521]
	TIME [epoch: 1.96 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.509926712115537		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.509926712115537 | validation: 0.4896057713305664]
	TIME [epoch: 1.63 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5676150498101977		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.5676150498101977 | validation: 0.4158966359315952]
	TIME [epoch: 1.63 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5304725517420662		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.5304725517420662 | validation: 0.5023809413688356]
	TIME [epoch: 1.63 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5658512562530795		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.5658512562530795 | validation: 0.692635135708833]
	TIME [epoch: 1.63 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6334497177067955		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.6334497177067955 | validation: 0.4116522380080892]
	TIME [epoch: 1.63 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4497093520506522		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.4497093520506522 | validation: 0.3693314060420949]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4972860669158744		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.4972860669158744 | validation: 0.4791677395306859]
	TIME [epoch: 1.63 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.608734222307565		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.608734222307565 | validation: 0.35254730299632575]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.381147015960142		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.381147015960142 | validation: 0.3939883562506601]
	TIME [epoch: 1.63 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5132738729942762		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.5132738729942762 | validation: 0.5974450730632096]
	TIME [epoch: 1.63 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4896235116251224		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.4896235116251224 | validation: 0.39917704204915255]
	TIME [epoch: 1.63 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4456180260003533		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.4456180260003533 | validation: 0.3224810115665613]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.36629902010499965		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.36629902010499965 | validation: 0.35609289134093375]
	TIME [epoch: 1.65 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.37244420444780024		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.37244420444780024 | validation: 0.3393123733872585]
	TIME [epoch: 1.64 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.43485772778845		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.43485772778845 | validation: 0.3108247714298483]
	TIME [epoch: 1.65 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4032155176982161		[learning rate: 0.0050834]
	Learning Rate: 0.00508339
	LOSS [training: 0.4032155176982161 | validation: 0.36236744961360184]
	TIME [epoch: 1.64 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4464446110127962		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.4464446110127962 | validation: 0.31465696424518413]
	TIME [epoch: 1.63 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3662206694955542		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.3662206694955542 | validation: 0.2963991246860874]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.33559235205882354		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.33559235205882354 | validation: 0.5385605544241521]
	TIME [epoch: 1.63 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6256505877081775		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.6256505877081775 | validation: 0.40212317308195183]
	TIME [epoch: 1.63 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3944244165132839		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.3944244165132839 | validation: 0.2604718385277048]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.36965907363554507		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.36965907363554507 | validation: 0.42934812818924384]
	TIME [epoch: 1.63 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4661533008729864		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.4661533008729864 | validation: 0.324357546054346]
	TIME [epoch: 1.64 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.34418370190068537		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.34418370190068537 | validation: 0.40039044607416285]
	TIME [epoch: 1.64 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3802595973331516		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.3802595973331516 | validation: 0.3829939430672775]
	TIME [epoch: 1.64 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.35908536155890847		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.35908536155890847 | validation: 0.3644816433766677]
	TIME [epoch: 1.64 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.31627326616295937		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.31627326616295937 | validation: 0.2761922665644198]
	TIME [epoch: 1.64 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2903140922144513		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.2903140922144513 | validation: 0.3301068987033067]
	TIME [epoch: 1.64 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4485281852709719		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.4485281852709719 | validation: 0.23750302189861122]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.26176455770062723		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.26176455770062723 | validation: 0.4023145508158685]
	TIME [epoch: 1.64 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3513465501710657		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.3513465501710657 | validation: 0.2821828470438114]
	TIME [epoch: 1.63 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.28810321853921117		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.28810321853921117 | validation: 0.32454132003684855]
	TIME [epoch: 1.63 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3231155439367557		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.3231155439367557 | validation: 0.40527732909125797]
	TIME [epoch: 1.64 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3555537706044175		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.3555537706044175 | validation: 0.4291168441125959]
	TIME [epoch: 1.65 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3263281519574214		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.3263281519574214 | validation: 0.27836795252824204]
	TIME [epoch: 1.65 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.29469530256232873		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.29469530256232873 | validation: 0.46063271585262894]
	TIME [epoch: 1.66 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3541220309471631		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.3541220309471631 | validation: 0.21747268257728225]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23956664058675065		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.23956664058675065 | validation: 0.2716061147084863]
	TIME [epoch: 1.64 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3493965135506935		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.3493965135506935 | validation: 0.21890014476323635]
	TIME [epoch: 1.63 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2424162632779062		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.2424162632779062 | validation: 0.5118199161585114]
	TIME [epoch: 1.63 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3762046159640049		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.3762046159640049 | validation: 0.21844950586175482]
	TIME [epoch: 1.63 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22077030937607522		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.22077030937607522 | validation: 0.2252354914938618]
	TIME [epoch: 1.63 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22315009127425883		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.22315009127425883 | validation: 0.24733858194108313]
	TIME [epoch: 1.63 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3041253037269388		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.3041253037269388 | validation: 0.20618183020671196]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_149.pth
	Model improved!!!
EPOCH 150/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24389682425244322		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.24389682425244322 | validation: 0.2349205116195619]
	TIME [epoch: 1.63 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23539565161963338		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.23539565161963338 | validation: 0.24772686554913187]
	TIME [epoch: 1.63 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2404804722738988		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.2404804722738988 | validation: 0.446773230744673]
	TIME [epoch: 1.63 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.33158743791764045		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.33158743791764045 | validation: 0.20366581252391858]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_153.pth
	Model improved!!!
EPOCH 154/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19968095585819762		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.19968095585819762 | validation: 0.23641782304266967]
	TIME [epoch: 1.63 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.33457337363639145		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.33457337363639145 | validation: 0.1781375784228553]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23542448096102359		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.23542448096102359 | validation: 0.30522772592510417]
	TIME [epoch: 1.64 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3142440920622541		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.3142440920622541 | validation: 0.2715249777567248]
	TIME [epoch: 1.64 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2899011958925353		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.2899011958925353 | validation: 0.3928826992161715]
	TIME [epoch: 1.63 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3243288075888602		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.3243288075888602 | validation: 0.21805006852520067]
	TIME [epoch: 1.63 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21196140723753448		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.21196140723753448 | validation: 0.20963703744424356]
	TIME [epoch: 1.65 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20116952549372463		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.20116952549372463 | validation: 0.24722073493694632]
	TIME [epoch: 1.65 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23773506105721298		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.23773506105721298 | validation: 0.22551656638454728]
	TIME [epoch: 1.65 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.27663146917094095		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.27663146917094095 | validation: 0.23396484834706177]
	TIME [epoch: 1.64 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23525087182950327		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.23525087182950327 | validation: 0.1638916322640293]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16854865712338046		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.16854865712338046 | validation: 0.2779756114750403]
	TIME [epoch: 1.64 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3111091506390877		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.3111091506390877 | validation: 0.3382450498692571]
	TIME [epoch: 1.63 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24141936283076593		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.24141936283076593 | validation: 0.19202154830113913]
	TIME [epoch: 1.63 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20732407730418873		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.20732407730418873 | validation: 0.1682395164578889]
	TIME [epoch: 1.64 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17128889369989653		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.17128889369989653 | validation: 0.15481040383979194]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15837168532434964		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.15837168532434964 | validation: 0.18500971483783657]
	TIME [epoch: 1.64 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18960158945939587		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.18960158945939587 | validation: 0.29016752804918466]
	TIME [epoch: 1.63 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2454380239275726		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.2454380239275726 | validation: 0.24212727521398555]
	TIME [epoch: 1.63 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20886054478329577		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.20886054478329577 | validation: 0.15298557968177406]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16210656661249845		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.16210656661249845 | validation: 0.18258575924672324]
	TIME [epoch: 1.63 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20786427188927514		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.20786427188927514 | validation: 0.22605556821032177]
	TIME [epoch: 1.63 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21698120653925923		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.21698120653925923 | validation: 0.25094209199051826]
	TIME [epoch: 1.63 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21056176692501233		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.21056176692501233 | validation: 0.15969037876290348]
	TIME [epoch: 1.64 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15487841930628		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.15487841930628 | validation: 0.22556597167402642]
	TIME [epoch: 1.64 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20347572924302307		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.20347572924302307 | validation: 0.28873122515255617]
	TIME [epoch: 1.63 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22368738060243742		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.22368738060243742 | validation: 0.17044005198617784]
	TIME [epoch: 1.63 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18542242779879442		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.18542242779879442 | validation: 0.17392811470206226]
	TIME [epoch: 1.63 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18442019582829652		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.18442019582829652 | validation: 0.13283835485460224]
	TIME [epoch: 1.65 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_182.pth
	Model improved!!!
EPOCH 183/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15523554745982465		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.15523554745982465 | validation: 0.15139589866804204]
	TIME [epoch: 1.66 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22116871949625205		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.22116871949625205 | validation: 0.18716220453188093]
	TIME [epoch: 1.65 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2084241888522257		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.2084241888522257 | validation: 0.12372144873568514]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15203533647176923		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.15203533647176923 | validation: 0.1471222936396601]
	TIME [epoch: 1.64 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15651727006385088		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.15651727006385088 | validation: 0.13594866178493567]
	TIME [epoch: 1.64 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12998124674763817		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.12998124674763817 | validation: 0.13713482435057195]
	TIME [epoch: 1.63 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1282017112861511		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.1282017112861511 | validation: 0.1634913933289047]
	TIME [epoch: 1.63 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.33137469148705334		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.33137469148705334 | validation: 0.34609481573518697]
	TIME [epoch: 1.63 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23582019457231843		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.23582019457231843 | validation: 0.1698233084405181]
	TIME [epoch: 1.64 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1493486282681117		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.1493486282681117 | validation: 0.199595049973661]
	TIME [epoch: 1.64 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15121451431024066		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.15121451431024066 | validation: 0.11954675989088553]
	TIME [epoch: 1.64 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13511679964953507		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.13511679964953507 | validation: 0.11734167638509839]
	TIME [epoch: 1.63 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14098197705461066		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.14098197705461066 | validation: 0.14525441525261915]
	TIME [epoch: 1.63 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14422530527411181		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.14422530527411181 | validation: 0.20067768830619268]
	TIME [epoch: 1.63 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16239078779200683		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.16239078779200683 | validation: 0.11992402669537856]
	TIME [epoch: 1.63 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11324403167164224		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.11324403167164224 | validation: 0.1313375555823588]
	TIME [epoch: 1.63 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11655258165169027		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.11655258165169027 | validation: 0.12057301606921533]
	TIME [epoch: 1.63 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13877305632161147		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.13877305632161147 | validation: 0.12492923793073052]
	TIME [epoch: 1.63 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13542347506610874		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.13542347506610874 | validation: 0.12989239180494416]
	TIME [epoch: 104 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15647800550283755		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.15647800550283755 | validation: 0.12477286674739006]
	TIME [epoch: 3.21 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18135087456992502		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.18135087456992502 | validation: 0.1300550722235057]
	TIME [epoch: 3.19 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11792460141296712		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.11792460141296712 | validation: 0.11724140457981105]
	TIME [epoch: 3.21 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_204.pth
	Model improved!!!
EPOCH 205/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09739168418973282		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.09739168418973282 | validation: 0.09752306952157044]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_205.pth
	Model improved!!!
EPOCH 206/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1128152649974283		[learning rate: 0.0027837]
	Learning Rate: 0.00278366
	LOSS [training: 0.1128152649974283 | validation: 0.13086223534438665]
	TIME [epoch: 3.18 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15248703417558726		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.15248703417558726 | validation: 0.3236405362476451]
	TIME [epoch: 3.18 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17284835970127913		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.17284835970127913 | validation: 0.22325329022894946]
	TIME [epoch: 3.18 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22345661102556585		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.22345661102556585 | validation: 0.12730372318692249]
	TIME [epoch: 3.2 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16261576678456718		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.16261576678456718 | validation: 0.2721709730653576]
	TIME [epoch: 3.2 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2192788037855195		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.2192788037855195 | validation: 0.1299726533797034]
	TIME [epoch: 3.2 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11673090661333768		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.11673090661333768 | validation: 0.14883122523827266]
	TIME [epoch: 3.19 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1211314848384398		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.1211314848384398 | validation: 0.09007907266893955]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_213.pth
	Model improved!!!
EPOCH 214/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09182139687182853		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.09182139687182853 | validation: 0.11903418173505048]
	TIME [epoch: 3.19 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10776348442811404		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.10776348442811404 | validation: 0.11967685124251365]
	TIME [epoch: 3.2 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12322331888421584		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.12322331888421584 | validation: 0.14458930952984902]
	TIME [epoch: 3.19 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11490117102685014		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.11490117102685014 | validation: 0.10172910623800868]
	TIME [epoch: 3.2 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11976887887256782		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.11976887887256782 | validation: 0.08680118612975737]
	TIME [epoch: 3.18 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_218.pth
	Model improved!!!
EPOCH 219/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08983649699327344		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.08983649699327344 | validation: 0.0974884905003381]
	TIME [epoch: 3.2 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10056014296125243		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.10056014296125243 | validation: 0.09263687535926164]
	TIME [epoch: 3.2 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11238992317709667		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.11238992317709667 | validation: 0.2446562711167097]
	TIME [epoch: 3.19 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15200143954883652		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.15200143954883652 | validation: 0.11766061127300971]
	TIME [epoch: 3.18 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09959607296232076		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.09959607296232076 | validation: 0.09975936701486325]
	TIME [epoch: 3.18 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10689968527481605		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.10689968527481605 | validation: 0.15720081466384403]
	TIME [epoch: 3.19 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12977354276021083		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.12977354276021083 | validation: 0.08696504047081495]
	TIME [epoch: 3.19 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08601879077486582		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.08601879077486582 | validation: 0.09748806640208141]
	TIME [epoch: 3.19 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11781890143237023		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.11781890143237023 | validation: 0.11793731850306927]
	TIME [epoch: 3.2 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12757299920002535		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.12757299920002535 | validation: 0.0942402345420919]
	TIME [epoch: 3.2 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09565466169676273		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.09565466169676273 | validation: 0.10864860182317843]
	TIME [epoch: 3.18 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1139515931493087		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.1139515931493087 | validation: 0.11405844914787687]
	TIME [epoch: 3.18 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11812841576262884		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.11812841576262884 | validation: 0.16532720402685805]
	TIME [epoch: 3.2 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10972553560487715		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.10972553560487715 | validation: 0.08528976228563222]
	TIME [epoch: 3.2 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_232.pth
	Model improved!!!
EPOCH 233/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09824238967636649		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.09824238967636649 | validation: 0.14024431599634096]
	TIME [epoch: 3.18 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13747620548475428		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.13747620548475428 | validation: 0.1190501340116946]
	TIME [epoch: 3.2 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10727041593997086		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.10727041593997086 | validation: 0.08726696000124874]
	TIME [epoch: 3.19 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09014375650128746		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.09014375650128746 | validation: 0.08027255689831797]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08326119007666218		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.08326119007666218 | validation: 0.08715024812386168]
	TIME [epoch: 3.19 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08676828289361369		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.08676828289361369 | validation: 0.07867788792763682]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_238.pth
	Model improved!!!
EPOCH 239/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0820571612010342		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.0820571612010342 | validation: 0.10901377116559746]
	TIME [epoch: 3.19 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13225413471889794		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.13225413471889794 | validation: 0.09291450979779527]
	TIME [epoch: 3.19 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09871878217614878		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.09871878217614878 | validation: 0.09170378585054062]
	TIME [epoch: 3.19 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07984678640732468		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.07984678640732468 | validation: 0.07661140016697193]
	TIME [epoch: 3.18 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09784103917418102		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.09784103917418102 | validation: 0.1508656896234004]
	TIME [epoch: 3.2 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12779609770353173		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.12779609770353173 | validation: 0.07367618635158113]
	TIME [epoch: 3.18 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_244.pth
	Model improved!!!
EPOCH 245/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07792974518095894		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.07792974518095894 | validation: 0.08848744387783758]
	TIME [epoch: 3.19 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08196636247585351		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.08196636247585351 | validation: 0.07280811708013157]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07205237006538233		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.07205237006538233 | validation: 0.07961109672927713]
	TIME [epoch: 3.2 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07676756593611386		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.07676756593611386 | validation: 0.13098072916566417]
	TIME [epoch: 3.18 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1017577423778922		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.1017577423778922 | validation: 0.1756369211942545]
	TIME [epoch: 3.2 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09829512146793995		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.09829512146793995 | validation: 0.08532520197645395]
	TIME [epoch: 3.21 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1678374713586641		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.1678374713586641 | validation: 0.07943240556751019]
	TIME [epoch: 3.2 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09193386540344553		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.09193386540344553 | validation: 0.12337200695208697]
	TIME [epoch: 3.18 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08846404594059125		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.08846404594059125 | validation: 0.09267686930843377]
	TIME [epoch: 3.19 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08396216312916924		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.08396216312916924 | validation: 0.06409308799149455]
	TIME [epoch: 3.2 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_254.pth
	Model improved!!!
EPOCH 255/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06801769975174492		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.06801769975174492 | validation: 0.07024466912288084]
	TIME [epoch: 3.2 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07669401167979475		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.07669401167979475 | validation: 0.10687890596216479]
	TIME [epoch: 3.18 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1111729908329305		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.1111729908329305 | validation: 0.07097246283594359]
	TIME [epoch: 3.18 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07762548481381007		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.07762548481381007 | validation: 0.07606907375143296]
	TIME [epoch: 3.19 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08401125159000489		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.08401125159000489 | validation: 0.0860184515741874]
	TIME [epoch: 3.18 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0704040366513666		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.0704040366513666 | validation: 0.07347450389494885]
	TIME [epoch: 3.2 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07035060472937668		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.07035060472937668 | validation: 0.06901726013947733]
	TIME [epoch: 3.21 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07261148186841698		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.07261148186841698 | validation: 0.07426424529166731]
	TIME [epoch: 3.2 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07368577189092683		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.07368577189092683 | validation: 0.06144682037559686]
	TIME [epoch: 3.2 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_263.pth
	Model improved!!!
EPOCH 264/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.059923419462833596		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.059923419462833596 | validation: 0.07275564875401992]
	TIME [epoch: 3.19 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07242175338289741		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.07242175338289741 | validation: 0.2160465795724365]
	TIME [epoch: 3.18 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17366125192965282		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.17366125192965282 | validation: 0.07350595206680371]
	TIME [epoch: 3.2 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07599176641891194		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.07599176641891194 | validation: 0.06722415405177631]
	TIME [epoch: 3.19 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06540778125552796		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.06540778125552796 | validation: 0.06465819448050256]
	TIME [epoch: 3.19 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05745305135663149		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.05745305135663149 | validation: 0.059911369170980666]
	TIME [epoch: 3.2 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_269.pth
	Model improved!!!
EPOCH 270/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05722681270709477		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.05722681270709477 | validation: 0.06428701092916099]
	TIME [epoch: 3.19 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06711446636608415		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.06711446636608415 | validation: 0.1631828817136494]
	TIME [epoch: 3.21 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11033831029191173		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.11033831029191173 | validation: 0.07823283659058955]
	TIME [epoch: 3.21 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.066401661039684		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.066401661039684 | validation: 0.059901646995875794]
	TIME [epoch: 3.21 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_273.pth
	Model improved!!!
EPOCH 274/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056138510268780265		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.056138510268780265 | validation: 0.05907776850177378]
	TIME [epoch: 3.21 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_274.pth
	Model improved!!!
EPOCH 275/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05574800006058209		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.05574800006058209 | validation: 0.07147154942202276]
	TIME [epoch: 3.18 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06016513641532216		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.06016513641532216 | validation: 0.06134666919839737]
	TIME [epoch: 3.18 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05784297892638168		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.05784297892638168 | validation: 0.0649367676934596]
	TIME [epoch: 3.19 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08318077769532456		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.08318077769532456 | validation: 0.1279126595696368]
	TIME [epoch: 3.2 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12825041460463027		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.12825041460463027 | validation: 0.06517441978111625]
	TIME [epoch: 3.2 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07304464521923962		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.07304464521923962 | validation: 0.15103067934770562]
	TIME [epoch: 3.2 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0850556124557382		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.0850556124557382 | validation: 0.07381979757919677]
	TIME [epoch: 3.19 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0786303943971849		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.0786303943971849 | validation: 0.06161188039697063]
	TIME [epoch: 3.21 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.057410911373867626		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.057410911373867626 | validation: 0.06523178678387735]
	TIME [epoch: 3.2 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.059784914752490204		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.059784914752490204 | validation: 0.07793196551884121]
	TIME [epoch: 3.21 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07656230103721959		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.07656230103721959 | validation: 0.06904424213457964]
	TIME [epoch: 3.22 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05567730100874545		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.05567730100874545 | validation: 0.057997653620435054]
	TIME [epoch: 3.21 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_286.pth
	Model improved!!!
EPOCH 287/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.062252881266277205		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.062252881266277205 | validation: 0.07937143212713799]
	TIME [epoch: 3.2 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06456684107168827		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.06456684107168827 | validation: 0.0625345388899015]
	TIME [epoch: 3.19 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06402058086362408		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.06402058086362408 | validation: 0.08021735054303303]
	TIME [epoch: 3.19 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07049409952403704		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.07049409952403704 | validation: 0.061581844077393894]
	TIME [epoch: 3.2 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06405946589785767		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.06405946589785767 | validation: 0.06970496644085585]
	TIME [epoch: 3.18 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06099997267745674		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.06099997267745674 | validation: 0.058542834551102166]
	TIME [epoch: 3.2 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052821841864371624		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.052821841864371624 | validation: 0.06297136800279475]
	TIME [epoch: 3.18 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.060403624737193067		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.060403624737193067 | validation: 0.0739917358798214]
	TIME [epoch: 3.2 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06199918285455136		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.06199918285455136 | validation: 0.12661375135480038]
	TIME [epoch: 3.2 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.091008014546322		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.091008014546322 | validation: 0.10609203946605517]
	TIME [epoch: 3.22 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06607747438024376		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.06607747438024376 | validation: 0.06066420510827635]
	TIME [epoch: 3.21 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0592555328198118		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.0592555328198118 | validation: 0.05187741747639314]
	TIME [epoch: 3.2 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_298.pth
	Model improved!!!
EPOCH 299/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053863721508942086		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.053863721508942086 | validation: 0.05241742664205438]
	TIME [epoch: 3.19 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05079010703804612		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.05079010703804612 | validation: 0.059623370445035184]
	TIME [epoch: 3.18 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049377551510741066		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.049377551510741066 | validation: 0.05694467956177682]
	TIME [epoch: 3.18 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05053655908087262		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.05053655908087262 | validation: 0.05907184092961133]
	TIME [epoch: 3.19 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06600514538397981		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.06600514538397981 | validation: 0.05590208556196119]
	TIME [epoch: 3.19 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17865956975627184		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.17865956975627184 | validation: 0.1824474869318937]
	TIME [epoch: 3.19 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1287929637036369		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.1287929637036369 | validation: 0.0975589572613826]
	TIME [epoch: 3.18 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0813295862392068		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.0813295862392068 | validation: 0.054758906845886496]
	TIME [epoch: 3.19 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056236245640783306		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.056236245640783306 | validation: 0.05695088358133511]
	TIME [epoch: 3.19 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051336869896523835		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.051336869896523835 | validation: 0.05102877704647966]
	TIME [epoch: 3.2 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_308.pth
	Model improved!!!
EPOCH 309/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04719447936130099		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.04719447936130099 | validation: 0.06218023118723476]
	TIME [epoch: 3.2 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0549008249000372		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.0549008249000372 | validation: 0.05640222684001266]
	TIME [epoch: 3.19 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05442351062593505		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.05442351062593505 | validation: 0.050803732660977236]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_311.pth
	Model improved!!!
EPOCH 312/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04822910324811655		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.04822910324811655 | validation: 0.05641498855038355]
	TIME [epoch: 3.19 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046464173637054074		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.046464173637054074 | validation: 0.04958158607583589]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_313.pth
	Model improved!!!
EPOCH 314/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05160590498618938		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.05160590498618938 | validation: 0.0946401923306631]
	TIME [epoch: 3.18 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07487420525338231		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.07487420525338231 | validation: 0.05340280451819766]
	TIME [epoch: 3.18 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05260787110290032		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.05260787110290032 | validation: 0.04835264519905338]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_316.pth
	Model improved!!!
EPOCH 317/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04463526403395336		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.04463526403395336 | validation: 0.10768593037017658]
	TIME [epoch: 3.19 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06905836102598412		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.06905836102598412 | validation: 0.05757229777106426]
	TIME [epoch: 3.19 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05168592270089504		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.05168592270089504 | validation: 0.04757972282317743]
	TIME [epoch: 3.2 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_319.pth
	Model improved!!!
EPOCH 320/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04625877185075986		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.04625877185075986 | validation: 0.05472622264156022]
	TIME [epoch: 3.2 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05449805128614639		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.05449805128614639 | validation: 0.057852777359533675]
	TIME [epoch: 3.18 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05393967594769904		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.05393967594769904 | validation: 0.05554695394814416]
	TIME [epoch: 3.18 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04864178617817302		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.04864178617817302 | validation: 0.05134033878122599]
	TIME [epoch: 3.2 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04684786887665618		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.04684786887665618 | validation: 0.059460618867480046]
	TIME [epoch: 3.19 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051501704222177674		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.051501704222177674 | validation: 0.06674271660681506]
	TIME [epoch: 3.18 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05125499826589134		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.05125499826589134 | validation: 0.05056682572936211]
	TIME [epoch: 3.2 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04327282466326053		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.04327282466326053 | validation: 0.05234410749954637]
	TIME [epoch: 3.2 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049635432366591015		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.049635432366591015 | validation: 0.06270241115635826]
	TIME [epoch: 3.18 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05286662364990814		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.05286662364990814 | validation: 0.1051706294002524]
	TIME [epoch: 3.2 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06957743319007798		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.06957743319007798 | validation: 0.0639648114064685]
	TIME [epoch: 3.2 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053180459398533816		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.053180459398533816 | validation: 0.04958262663715082]
	TIME [epoch: 3.21 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04244452902350401		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.04244452902350401 | validation: 0.0504400712368474]
	TIME [epoch: 3.2 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0422144948122628		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.0422144948122628 | validation: 0.05136181042307805]
	TIME [epoch: 3.2 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05159450467672281		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.05159450467672281 | validation: 0.10431163178461236]
	TIME [epoch: 3.2 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.061488250313938654		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.061488250313938654 | validation: 0.05977446337431594]
	TIME [epoch: 3.19 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05893040143707236		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.05893040143707236 | validation: 0.053390495915588546]
	TIME [epoch: 3.19 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04782301719921177		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.04782301719921177 | validation: 0.046993724454872085]
	TIME [epoch: 3.21 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_337.pth
	Model improved!!!
EPOCH 338/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04884900650130734		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.04884900650130734 | validation: 0.05340383411661647]
	TIME [epoch: 3.2 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04564647233121273		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.04564647233121273 | validation: 0.0613771420371581]
	TIME [epoch: 3.19 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04648339935464167		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.04648339935464167 | validation: 0.054119046133851434]
	TIME [epoch: 3.2 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048002068241895554		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.048002068241895554 | validation: 0.047725760608937746]
	TIME [epoch: 3.19 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043537412233673814		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.043537412233673814 | validation: 0.050580026765627055]
	TIME [epoch: 3.2 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04943820074276327		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.04943820074276327 | validation: 0.0748154144127975]
	TIME [epoch: 3.21 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05069676599491191		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.05069676599491191 | validation: 0.04712481325055861]
	TIME [epoch: 3.2 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04151908105303462		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.04151908105303462 | validation: 0.046761501914373414]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_345.pth
	Model improved!!!
EPOCH 346/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042177930931922986		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.042177930931922986 | validation: 0.04327027601182659]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_346.pth
	Model improved!!!
EPOCH 347/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04695600637299795		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.04695600637299795 | validation: 0.08364222781716335]
	TIME [epoch: 3.19 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07262262845597689		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.07262262845597689 | validation: 0.053666166015156815]
	TIME [epoch: 3.21 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048493736547452565		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.048493736547452565 | validation: 0.048474315356659275]
	TIME [epoch: 3.2 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04200266541616213		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.04200266541616213 | validation: 0.0837977112421919]
	TIME [epoch: 3.2 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0513955620023121		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.0513955620023121 | validation: 0.04581848999299171]
	TIME [epoch: 3.2 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04180801092801764		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.04180801092801764 | validation: 0.042637457614607716]
	TIME [epoch: 3.2 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_352.pth
	Model improved!!!
EPOCH 353/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039286450257727576		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.039286450257727576 | validation: 0.058252773309807375]
	TIME [epoch: 3.2 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05199542027641022		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.05199542027641022 | validation: 0.06927334160437795]
	TIME [epoch: 3.21 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046975110967801686		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.046975110967801686 | validation: 0.04358019024799606]
	TIME [epoch: 3.21 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03863651045585369		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.03863651045585369 | validation: 0.044474985999321315]
	TIME [epoch: 3.2 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.037993575941678376		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.037993575941678376 | validation: 0.04491520466479722]
	TIME [epoch: 3.2 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03961678762717201		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.03961678762717201 | validation: 0.05209136917178739]
	TIME [epoch: 3.19 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05037568547109151		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.05037568547109151 | validation: 0.047281990034557436]
	TIME [epoch: 3.2 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04775699185681089		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.04775699185681089 | validation: 0.04872760896908847]
	TIME [epoch: 3.2 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047466808898461346		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.047466808898461346 | validation: 0.06239890134021151]
	TIME [epoch: 3.18 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046670190841851575		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.046670190841851575 | validation: 0.04042176225090318]
	TIME [epoch: 3.2 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_362.pth
	Model improved!!!
EPOCH 363/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03838536737741145		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.03838536737741145 | validation: 0.03957284174488945]
	TIME [epoch: 3.2 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_363.pth
	Model improved!!!
EPOCH 364/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0382480693586353		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.0382480693586353 | validation: 0.044572910915986974]
	TIME [epoch: 3.2 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04044749909583166		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.04044749909583166 | validation: 0.049200954142402525]
	TIME [epoch: 3.21 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04809520091176756		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.04809520091176756 | validation: 0.04289405961866963]
	TIME [epoch: 3.22 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041749293453908265		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.041749293453908265 | validation: 0.051097250967616414]
	TIME [epoch: 3.21 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044860069107100414		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.044860069107100414 | validation: 0.04215246952958023]
	TIME [epoch: 3.2 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03749189641521522		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.03749189641521522 | validation: 0.04289111452462982]
	TIME [epoch: 3.2 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03892303976575012		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.03892303976575012 | validation: 0.05123734866855274]
	TIME [epoch: 3.19 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042447883672254355		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.042447883672254355 | validation: 0.044265482996512555]
	TIME [epoch: 3.19 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04267706972288332		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.04267706972288332 | validation: 0.07776047010447074]
	TIME [epoch: 3.2 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05787425223383164		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.05787425223383164 | validation: 0.05097125116373644]
	TIME [epoch: 3.19 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04807368583699689		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.04807368583699689 | validation: 0.06389172569804276]
	TIME [epoch: 3.19 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04752640020457277		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.04752640020457277 | validation: 0.04841546454672146]
	TIME [epoch: 3.19 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03875027495730321		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.03875027495730321 | validation: 0.04017310336279641]
	TIME [epoch: 3.2 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038338877270963106		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.038338877270963106 | validation: 0.046333462206981944]
	TIME [epoch: 3.21 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04095881522367261		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.04095881522367261 | validation: 0.045557490997406545]
	TIME [epoch: 3.21 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03905332852223725		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.03905332852223725 | validation: 0.045963024203726055]
	TIME [epoch: 3.21 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040193002827835556		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.040193002827835556 | validation: 0.0427346503672949]
	TIME [epoch: 3.2 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038176415249369544		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.038176415249369544 | validation: 0.06122743457622424]
	TIME [epoch: 3.19 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0434160170062337		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.0434160170062337 | validation: 0.03888437825721697]
	TIME [epoch: 3.2 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_382.pth
	Model improved!!!
EPOCH 383/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038338357675631804		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.038338357675631804 | validation: 0.04568020934541538]
	TIME [epoch: 3.2 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04177821838278961		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.04177821838278961 | validation: 0.03727390751217065]
	TIME [epoch: 3.18 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_384.pth
	Model improved!!!
EPOCH 385/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03520114405352121		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.03520114405352121 | validation: 0.04285493775786362]
	TIME [epoch: 3.19 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04061614802863826		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.04061614802863826 | validation: 0.043952816658246706]
	TIME [epoch: 3.18 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03809670370245675		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.03809670370245675 | validation: 0.048775112457044564]
	TIME [epoch: 3.19 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04349963004381413		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.04349963004381413 | validation: 0.04340811869888141]
	TIME [epoch: 3.19 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03842842183159223		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.03842842183159223 | validation: 0.04261530320412099]
	TIME [epoch: 3.2 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0362724842637599		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.0362724842637599 | validation: 0.04301512105023281]
	TIME [epoch: 3.2 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.037241519288266994		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.037241519288266994 | validation: 0.03895200081544277]
	TIME [epoch: 3.19 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03950788954463878		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.03950788954463878 | validation: 0.0505120887580102]
	TIME [epoch: 3.18 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.037663007350128515		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.037663007350128515 | validation: 0.04267715459798557]
	TIME [epoch: 3.2 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042897231667106406		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.042897231667106406 | validation: 0.04125731363354302]
	TIME [epoch: 3.2 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04469802697623139		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.04469802697623139 | validation: 0.04236624685685321]
	TIME [epoch: 3.18 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.037270151908784244		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.037270151908784244 | validation: 0.037428906943218355]
	TIME [epoch: 3.19 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03692180515881134		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.03692180515881134 | validation: 0.04103503380618262]
	TIME [epoch: 3.2 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03673591873431396		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.03673591873431396 | validation: 0.040222649513610084]
	TIME [epoch: 3.18 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03463573053757699		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.03463573053757699 | validation: 0.03812056162388001]
	TIME [epoch: 3.19 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.034606464436959		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.034606464436959 | validation: 0.040916906592379294]
	TIME [epoch: 3.2 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03514170108259354		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.03514170108259354 | validation: 0.044301285445334106]
	TIME [epoch: 3.2 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04081012857289394		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.04081012857289394 | validation: 0.03720817676493537]
	TIME [epoch: 3.2 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_402.pth
	Model improved!!!
EPOCH 403/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03384605623971271		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.03384605623971271 | validation: 0.03709103373720337]
	TIME [epoch: 3.2 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_403.pth
	Model improved!!!
EPOCH 404/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.032195876919203996		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.032195876919203996 | validation: 0.03554295682147645]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_404.pth
	Model improved!!!
EPOCH 405/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0341040192659257		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.0341040192659257 | validation: 0.037104895335983344]
	TIME [epoch: 3.2 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.034079040061650495		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.034079040061650495 | validation: 0.04212292695893588]
	TIME [epoch: 3.19 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03628024253826153		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.03628024253826153 | validation: 0.04620111964677112]
	TIME [epoch: 3.19 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03794117146822344		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.03794117146822344 | validation: 0.03639770802537607]
	TIME [epoch: 3.19 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03521987376018677		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.03521987376018677 | validation: 0.039247305825236084]
	TIME [epoch: 3.2 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04342109717372887		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.04342109717372887 | validation: 0.04035802889564186]
	TIME [epoch: 3.19 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03336498100320531		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.03336498100320531 | validation: 0.04559640972948435]
	TIME [epoch: 3.19 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03559833096940795		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.03559833096940795 | validation: 0.03665996432496532]
	TIME [epoch: 3.2 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03393708604955589		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.03393708604955589 | validation: 0.0364107459794902]
	TIME [epoch: 3.21 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03417882590229576		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.03417882590229576 | validation: 0.035977417462776405]
	TIME [epoch: 3.2 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03469000314660897		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.03469000314660897 | validation: 0.056827177568470805]
	TIME [epoch: 3.19 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03868615530392097		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.03868615530392097 | validation: 0.05421444457213314]
	TIME [epoch: 3.2 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03734290003648334		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.03734290003648334 | validation: 0.040799312072927384]
	TIME [epoch: 3.19 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03682162586948867		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.03682162586948867 | validation: 0.0410307916750666]
	TIME [epoch: 3.2 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038040868129890826		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.038040868129890826 | validation: 0.04616839050368828]
	TIME [epoch: 3.18 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03262453467589581		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.03262453467589581 | validation: 0.037030664653760835]
	TIME [epoch: 3.2 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03059169121762547		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.03059169121762547 | validation: 0.0427916303348167]
	TIME [epoch: 3.2 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03525558244319306		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.03525558244319306 | validation: 0.03445020210799042]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_422.pth
	Model improved!!!
EPOCH 423/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030479958979980855		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.030479958979980855 | validation: 0.03852809360022522]
	TIME [epoch: 3.2 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.031847491302650785		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.031847491302650785 | validation: 0.042988016091759354]
	TIME [epoch: 3.19 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03974168207781616		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.03974168207781616 | validation: 0.04799752188053467]
	TIME [epoch: 3.2 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03548647465648156		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.03548647465648156 | validation: 0.03650786258308723]
	TIME [epoch: 3.2 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.032276344470731015		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.032276344470731015 | validation: 0.033374000105118665]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_427.pth
	Model improved!!!
EPOCH 428/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02901021594382942		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.02901021594382942 | validation: 0.03683283851143525]
	TIME [epoch: 3.2 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03326759535229747		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.03326759535229747 | validation: 0.0376532838716764]
	TIME [epoch: 3.2 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030141748079265125		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.030141748079265125 | validation: 0.03238710987895498]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_430.pth
	Model improved!!!
EPOCH 431/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02994455410109764		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.02994455410109764 | validation: 0.03237457165041384]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_431.pth
	Model improved!!!
EPOCH 432/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03251818831442001		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.03251818831442001 | validation: 0.039947601272312346]
	TIME [epoch: 3.2 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03749469374302809		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.03749469374302809 | validation: 0.046804426074202626]
	TIME [epoch: 3.21 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.037099135362760355		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.037099135362760355 | validation: 0.03267240323491294]
	TIME [epoch: 3.2 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.031008631555647034		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.031008631555647034 | validation: 0.036656322457417914]
	TIME [epoch: 3.22 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03178718116966082		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.03178718116966082 | validation: 0.04457869718200353]
	TIME [epoch: 3.22 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03806888966843408		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.03806888966843408 | validation: 0.044296029607367865]
	TIME [epoch: 3.2 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03752287230714937		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.03752287230714937 | validation: 0.03587884365650382]
	TIME [epoch: 3.2 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030408836517342497		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.030408836517342497 | validation: 0.03562747477345659]
	TIME [epoch: 3.19 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030841066051334076		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.030841066051334076 | validation: 0.03801068488798107]
	TIME [epoch: 3.2 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03278172992479011		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.03278172992479011 | validation: 0.035100626862927274]
	TIME [epoch: 3.2 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03119076774749002		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.03119076774749002 | validation: 0.035044170912405845]
	TIME [epoch: 3.2 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029282041898985083		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.029282041898985083 | validation: 0.04420779965645366]
	TIME [epoch: 3.2 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03427648230611924		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.03427648230611924 | validation: 0.034300136357744364]
	TIME [epoch: 3.2 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03189060976127381		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.03189060976127381 | validation: 0.03224359184109657]
	TIME [epoch: 3.19 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_445.pth
	Model improved!!!
EPOCH 446/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029428420936438097		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.029428420936438097 | validation: 0.03757860650462859]
	TIME [epoch: 3.21 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03021346662691875		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.03021346662691875 | validation: 0.03233985976838069]
	TIME [epoch: 3.22 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03467749841491867		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.03467749841491867 | validation: 0.03719217564846371]
	TIME [epoch: 3.22 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03156949298989322		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.03156949298989322 | validation: 0.03221712097662887]
	TIME [epoch: 3.2 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_449.pth
	Model improved!!!
EPOCH 450/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028799649127275586		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.028799649127275586 | validation: 0.0411202512720442]
	TIME [epoch: 3.18 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03570326887505719		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.03570326887505719 | validation: 0.032128748535190176]
	TIME [epoch: 3.2 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_451.pth
	Model improved!!!
EPOCH 452/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030631126265518713		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.030631126265518713 | validation: 0.041394727790450926]
	TIME [epoch: 3.2 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03189447724040138		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.03189447724040138 | validation: 0.032673548461706094]
	TIME [epoch: 3.18 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028596556637317003		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.028596556637317003 | validation: 0.03389026963816393]
	TIME [epoch: 3.18 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030856078224842747		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.030856078224842747 | validation: 0.03617883620718015]
	TIME [epoch: 3.18 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029671190915163664		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.029671190915163664 | validation: 0.03219401374271737]
	TIME [epoch: 3.18 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030893561202574072		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.030893561202574072 | validation: 0.03334718000260893]
	TIME [epoch: 3.19 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029169857640563665		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.029169857640563665 | validation: 0.030280251006172362]
	TIME [epoch: 3.2 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_458.pth
	Model improved!!!
EPOCH 459/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027867707230631214		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.027867707230631214 | validation: 0.03187893489044564]
	TIME [epoch: 3.19 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030651695264104065		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.030651695264104065 | validation: 0.03411769475176281]
	TIME [epoch: 3.19 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.031081373177399285		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.031081373177399285 | validation: 0.03440262956728678]
	TIME [epoch: 3.18 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030301884320204735		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.030301884320204735 | validation: 0.03337487429359586]
	TIME [epoch: 3.18 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026753735499683323		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.026753735499683323 | validation: 0.03190616471354617]
	TIME [epoch: 3.18 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.031107180798597193		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.031107180798597193 | validation: 0.03828918628270071]
	TIME [epoch: 3.2 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029732824667056602		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.029732824667056602 | validation: 0.03193848602572842]
	TIME [epoch: 3.19 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02619784841570156		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.02619784841570156 | validation: 0.0322044560622203]
	TIME [epoch: 3.19 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029066885863414625		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.029066885863414625 | validation: 0.03146392852474754]
	TIME [epoch: 3.18 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027764086086527737		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.027764086086527737 | validation: 0.030547450281210305]
	TIME [epoch: 3.19 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030502506421142387		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.030502506421142387 | validation: 0.05288428711321208]
	TIME [epoch: 3.2 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03404991159489214		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.03404991159489214 | validation: 0.03857711878534076]
	TIME [epoch: 3.21 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03252323371724177		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.03252323371724177 | validation: 0.04040119967423591]
	TIME [epoch: 3.21 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03128518185355103		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.03128518185355103 | validation: 0.031114377241441105]
	TIME [epoch: 3.19 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0302863632355566		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.0302863632355566 | validation: 0.03348622840458723]
	TIME [epoch: 3.19 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027021060260302533		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.027021060260302533 | validation: 0.03263736104045721]
	TIME [epoch: 3.2 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027201441661821674		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.027201441661821674 | validation: 0.03292420926772619]
	TIME [epoch: 3.19 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027599088069763338		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.027599088069763338 | validation: 0.036381458877836975]
	TIME [epoch: 3.2 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028213144779317852		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.028213144779317852 | validation: 0.04061959722419581]
	TIME [epoch: 3.19 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029692900205469802		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.029692900205469802 | validation: 0.034412920585722366]
	TIME [epoch: 3.18 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02724065931804448		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.02724065931804448 | validation: 0.03062575174283973]
	TIME [epoch: 3.18 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.030418293846322705		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.030418293846322705 | validation: 0.031729193437077606]
	TIME [epoch: 3.2 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02816393412502728		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.02816393412502728 | validation: 0.032220906262082925]
	TIME [epoch: 3.18 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02818542891213153		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.02818542891213153 | validation: 0.034005077077041174]
	TIME [epoch: 3.2 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028538457388961512		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.028538457388961512 | validation: 0.031079547449610768]
	TIME [epoch: 3.21 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02742486542574213		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.02742486542574213 | validation: 0.029245269309794538]
	TIME [epoch: 3.21 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_484.pth
	Model improved!!!
EPOCH 485/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02848743683622143		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.02848743683622143 | validation: 0.03148158780976538]
	TIME [epoch: 3.2 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026722533954841504		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.026722533954841504 | validation: 0.031014486790203067]
	TIME [epoch: 3.18 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02585757272226434		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.02585757272226434 | validation: 0.03560360663441683]
	TIME [epoch: 3.19 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029576111515181153		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.029576111515181153 | validation: 0.030005810740332428]
	TIME [epoch: 3.18 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02636509401337391		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.02636509401337391 | validation: 0.03021770126413245]
	TIME [epoch: 3.18 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026182140768725017		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.026182140768725017 | validation: 0.035394624424251724]
	TIME [epoch: 3.18 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028738819434728585		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.028738819434728585 | validation: 0.029890045460711806]
	TIME [epoch: 3.18 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025629830955872637		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.025629830955872637 | validation: 0.032405134213894723]
	TIME [epoch: 3.18 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027564757975007143		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.027564757975007143 | validation: 0.03126510779521164]
	TIME [epoch: 3.18 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.029237516351438928		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.029237516351438928 | validation: 0.03773324785388918]
	TIME [epoch: 3.2 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02963061934096064		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.02963061934096064 | validation: 0.033263603009468276]
	TIME [epoch: 3.2 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028440594294956406		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.028440594294956406 | validation: 0.02953523449586903]
	TIME [epoch: 3.2 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025753808500861387		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.025753808500861387 | validation: 0.03004341296079667]
	TIME [epoch: 3.19 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028130096989549637		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.028130096989549637 | validation: 0.030562938069468415]
	TIME [epoch: 3.2 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026640523703145236		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.026640523703145236 | validation: 0.036204828832391095]
	TIME [epoch: 3.18 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02711829443465357		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.02711829443465357 | validation: 0.027491840588391433]
	TIME [epoch: 3.2 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_500.pth
	Model improved!!!
EPOCH 501/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026280886517403897		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.026280886517403897 | validation: 0.030285911733884643]
	TIME [epoch: 108 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025677323484398256		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.025677323484398256 | validation: 0.029453025328124174]
	TIME [epoch: 6.31 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026008662455387046		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.026008662455387046 | validation: 0.03003470799250779]
	TIME [epoch: 6.31 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025818310327011144		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.025818310327011144 | validation: 0.03158463688454289]
	TIME [epoch: 6.27 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025475019754297328		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.025475019754297328 | validation: 0.03162816369504531]
	TIME [epoch: 6.3 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027578512060023423		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.027578512060023423 | validation: 0.02863577879799006]
	TIME [epoch: 6.31 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028170452975830228		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.028170452975830228 | validation: 0.030079560377361855]
	TIME [epoch: 6.29 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02802681686593235		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.02802681686593235 | validation: 0.02955935203163107]
	TIME [epoch: 6.3 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025312379795362876		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.025312379795362876 | validation: 0.02821976462797935]
	TIME [epoch: 6.3 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02608333420114068		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.02608333420114068 | validation: 0.03021423105243022]
	TIME [epoch: 6.29 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023500269170623977		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.023500269170623977 | validation: 0.02936626521650869]
	TIME [epoch: 6.28 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024887191765594865		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.024887191765594865 | validation: 0.030755602456935052]
	TIME [epoch: 6.32 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.027825030858767027		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.027825030858767027 | validation: 0.03486423491105612]
	TIME [epoch: 6.33 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.031491819700668555		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.031491819700668555 | validation: 0.03073234670216894]
	TIME [epoch: 6.29 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025878482264947864		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.025878482264947864 | validation: 0.029381353901432785]
	TIME [epoch: 6.28 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025512915965797817		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.025512915965797817 | validation: 0.028424223407917677]
	TIME [epoch: 6.31 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026257803625555597		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.026257803625555597 | validation: 0.026776987150992144]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_517.pth
	Model improved!!!
EPOCH 518/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02443094134617298		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.02443094134617298 | validation: 0.03080834525030874]
	TIME [epoch: 6.31 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024162376264057844		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.024162376264057844 | validation: 0.027067171945041903]
	TIME [epoch: 6.29 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025820576257740112		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.025820576257740112 | validation: 0.029024264342351305]
	TIME [epoch: 6.27 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025287439445734014		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.025287439445734014 | validation: 0.033340273848327315]
	TIME [epoch: 6.26 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026231612827701006		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.026231612827701006 | validation: 0.03574776732221088]
	TIME [epoch: 6.29 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026715304436505133		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.026715304436505133 | validation: 0.02719846943211264]
	TIME [epoch: 6.28 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025763754401390292		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.025763754401390292 | validation: 0.030425567588466814]
	TIME [epoch: 6.28 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02560946586453373		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.02560946586453373 | validation: 0.029114700701998637]
	TIME [epoch: 6.35 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0271654811461541		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.0271654811461541 | validation: 0.028019021265450767]
	TIME [epoch: 6.33 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023863524165739877		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.023863524165739877 | validation: 0.028283352324318525]
	TIME [epoch: 6.29 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02473460597521835		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.02473460597521835 | validation: 0.030167622142182278]
	TIME [epoch: 6.3 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0240941917756123		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.0240941917756123 | validation: 0.03022221382469502]
	TIME [epoch: 6.31 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024078740428165046		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.024078740428165046 | validation: 0.026355834595160412]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_530.pth
	Model improved!!!
EPOCH 531/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02444608396344288		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.02444608396344288 | validation: 0.028543836822474494]
	TIME [epoch: 6.32 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02496969718147037		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.02496969718147037 | validation: 0.03044536525272485]
	TIME [epoch: 6.3 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02451777941601564		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.02451777941601564 | validation: 0.03101942554459198]
	TIME [epoch: 6.29 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026302312491413217		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.026302312491413217 | validation: 0.030743743330539336]
	TIME [epoch: 6.28 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024761149336046458		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.024761149336046458 | validation: 0.03170733133892177]
	TIME [epoch: 6.27 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025122778424041516		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.025122778424041516 | validation: 0.02799932972051153]
	TIME [epoch: 6.28 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025267233187558143		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.025267233187558143 | validation: 0.02850898105082189]
	TIME [epoch: 6.32 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024706550222495204		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.024706550222495204 | validation: 0.028012578996392735]
	TIME [epoch: 6.29 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024765214574021917		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.024765214574021917 | validation: 0.02950615544166408]
	TIME [epoch: 6.29 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02361723083003579		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.02361723083003579 | validation: 0.024545628380998503]
	TIME [epoch: 6.28 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_540.pth
	Model improved!!!
EPOCH 541/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02499424100876194		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.02499424100876194 | validation: 0.029468211370642247]
	TIME [epoch: 6.28 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025191281122287697		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.025191281122287697 | validation: 0.029362397362548483]
	TIME [epoch: 6.29 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024073216525101852		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.024073216525101852 | validation: 0.02812065971668779]
	TIME [epoch: 6.31 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023749563984750754		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.023749563984750754 | validation: 0.032471574923454594]
	TIME [epoch: 6.32 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025314850613296504		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.025314850613296504 | validation: 0.027596193423857104]
	TIME [epoch: 6.28 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023295879735912316		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.023295879735912316 | validation: 0.026279749184927044]
	TIME [epoch: 6.28 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022756558216976232		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.022756558216976232 | validation: 0.027991951450519956]
	TIME [epoch: 6.31 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023330417856386405		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.023330417856386405 | validation: 0.028793122752490238]
	TIME [epoch: 6.32 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023559835215604022		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.023559835215604022 | validation: 0.026688972652785706]
	TIME [epoch: 6.27 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02347994531838269		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.02347994531838269 | validation: 0.026878862609415768]
	TIME [epoch: 6.32 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023879723761343532		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.023879723761343532 | validation: 0.028047887620533774]
	TIME [epoch: 6.29 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02419753095870069		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.02419753095870069 | validation: 0.02791189958517167]
	TIME [epoch: 6.29 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023151509203466106		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.023151509203466106 | validation: 0.02639457311846113]
	TIME [epoch: 6.28 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022275045406070094		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.022275045406070094 | validation: 0.0292779766283033]
	TIME [epoch: 6.28 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023161809047439343		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.023161809047439343 | validation: 0.03173161385239116]
	TIME [epoch: 6.3 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.028028235370305563		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.028028235370305563 | validation: 0.025042991676588185]
	TIME [epoch: 6.3 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023902334194061348		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.023902334194061348 | validation: 0.02532137398470496]
	TIME [epoch: 6.31 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0234738503981174		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.0234738503981174 | validation: 0.027827291876621064]
	TIME [epoch: 6.28 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022837900756418623		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.022837900756418623 | validation: 0.026284109935522035]
	TIME [epoch: 6.3 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023842735179152797		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.023842735179152797 | validation: 0.02698334610400477]
	TIME [epoch: 6.26 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0242913646375941		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.0242913646375941 | validation: 0.02636666128807782]
	TIME [epoch: 6.31 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023045345881153742		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.023045345881153742 | validation: 0.02755504322457891]
	TIME [epoch: 6.29 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02285949745235387		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.02285949745235387 | validation: 0.025453515549757978]
	TIME [epoch: 6.34 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023370966780057588		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.023370966780057588 | validation: 0.02895988070201827]
	TIME [epoch: 6.28 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023491377321914468		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.023491377321914468 | validation: 0.023740591617222907]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_565.pth
	Model improved!!!
EPOCH 566/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022772456443345745		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.022772456443345745 | validation: 0.02454473279050698]
	TIME [epoch: 6.28 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024289698380007583		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.024289698380007583 | validation: 0.028476792751501192]
	TIME [epoch: 6.32 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024001306588087516		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.024001306588087516 | validation: 0.028209547113915434]
	TIME [epoch: 6.27 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023540585073514444		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.023540585073514444 | validation: 0.027189949221971633]
	TIME [epoch: 6.33 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.024820806671832674		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.024820806671832674 | validation: 0.030731215891390558]
	TIME [epoch: 6.33 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023137499005835394		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.023137499005835394 | validation: 0.02953084366131893]
	TIME [epoch: 6.28 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02312354591415494		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.02312354591415494 | validation: 0.029726645664584363]
	TIME [epoch: 6.28 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021865454836676533		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.021865454836676533 | validation: 0.025248022035179802]
	TIME [epoch: 6.29 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022585827132830187		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.022585827132830187 | validation: 0.0268855229806223]
	TIME [epoch: 6.32 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023646153732823007		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.023646153732823007 | validation: 0.02548352315841589]
	TIME [epoch: 6.3 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.025135685491547103		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.025135685491547103 | validation: 0.026086696045478593]
	TIME [epoch: 6.32 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023348616736311847		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.023348616736311847 | validation: 0.025400086424397464]
	TIME [epoch: 6.32 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021962751752611764		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.021962751752611764 | validation: 0.02545241988722128]
	TIME [epoch: 6.31 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02250198883122983		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.02250198883122983 | validation: 0.023537255906562016]
	TIME [epoch: 6.28 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_579.pth
	Model improved!!!
EPOCH 580/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021741481738733935		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.021741481738733935 | validation: 0.024453936143336293]
	TIME [epoch: 6.29 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021253377295044		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.021253377295044 | validation: 0.026488406156243417]
	TIME [epoch: 6.28 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021520027312447468		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.021520027312447468 | validation: 0.02610086481994879]
	TIME [epoch: 6.32 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022903981329972598		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.022903981329972598 | validation: 0.024554236075864478]
	TIME [epoch: 6.27 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02201761654283367		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.02201761654283367 | validation: 0.02528101841425147]
	TIME [epoch: 6.27 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02373552573516799		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.02373552573516799 | validation: 0.02488486727404234]
	TIME [epoch: 6.29 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022441342795596543		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.022441342795596543 | validation: 0.024298683141728946]
	TIME [epoch: 6.32 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021784019180493275		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.021784019180493275 | validation: 0.026029046897855592]
	TIME [epoch: 6.26 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020284965104880134		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.020284965104880134 | validation: 0.024893373594994214]
	TIME [epoch: 6.32 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02139728679488294		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.02139728679488294 | validation: 0.02751123671123458]
	TIME [epoch: 6.3 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021382071253776776		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.021382071253776776 | validation: 0.029511688007070987]
	TIME [epoch: 6.3 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02392614442604883		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.02392614442604883 | validation: 0.03969287731968482]
	TIME [epoch: 6.3 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.026102747222479558		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.026102747222479558 | validation: 0.02526391397970809]
	TIME [epoch: 6.28 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022554137475934606		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.022554137475934606 | validation: 0.02545688602557901]
	TIME [epoch: 6.3 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021455874539529875		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.021455874539529875 | validation: 0.026474894860378663]
	TIME [epoch: 6.3 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02174276369027324		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.02174276369027324 | validation: 0.024298805263195646]
	TIME [epoch: 6.3 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020972176174068023		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.020972176174068023 | validation: 0.025872271155945193]
	TIME [epoch: 6.3 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021515759666103064		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.021515759666103064 | validation: 0.025064134691408682]
	TIME [epoch: 6.27 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022212689725785023		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.022212689725785023 | validation: 0.028235957187992034]
	TIME [epoch: 6.3 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022245288897377577		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.022245288897377577 | validation: 0.025162835840666437]
	TIME [epoch: 6.27 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022172785708557567		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.022172785708557567 | validation: 0.026448347236041354]
	TIME [epoch: 6.31 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021117687415131633		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.021117687415131633 | validation: 0.02669114246404223]
	TIME [epoch: 6.36 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022275718742536192		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.022275718742536192 | validation: 0.027313290968579956]
	TIME [epoch: 6.29 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022163731765164958		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.022163731765164958 | validation: 0.02611551698282434]
	TIME [epoch: 6.27 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020487511859628996		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.020487511859628996 | validation: 0.027206972651941733]
	TIME [epoch: 6.3 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02146979288740483		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.02146979288740483 | validation: 0.026020137189576478]
	TIME [epoch: 6.29 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021952309897826677		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.021952309897826677 | validation: 0.026014107843183688]
	TIME [epoch: 6.29 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02257092038899409		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.02257092038899409 | validation: 0.02619115626363616]
	TIME [epoch: 6.32 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022083280154442705		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.022083280154442705 | validation: 0.025343468375879304]
	TIME [epoch: 6.32 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02113110976647738		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.02113110976647738 | validation: 0.025143207548696107]
	TIME [epoch: 6.3 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02103339949555564		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.02103339949555564 | validation: 0.02372979224638703]
	TIME [epoch: 6.31 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021544741000981898		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.021544741000981898 | validation: 0.026274935203750644]
	TIME [epoch: 6.28 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023584559340168267		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.023584559340168267 | validation: 0.026317738189873197]
	TIME [epoch: 6.29 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022287915870629064		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.022287915870629064 | validation: 0.027223727793882435]
	TIME [epoch: 6.31 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02257471160685012		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.02257471160685012 | validation: 0.024167940898196308]
	TIME [epoch: 6.34 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02129627485736458		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.02129627485736458 | validation: 0.02718041150452042]
	TIME [epoch: 6.32 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021042930412357268		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.021042930412357268 | validation: 0.025968254670254733]
	TIME [epoch: 6.32 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02115512313070492		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.02115512313070492 | validation: 0.024266634225434985]
	TIME [epoch: 6.3 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0212163047976656		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.0212163047976656 | validation: 0.023465305300733603]
	TIME [epoch: 6.29 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_618.pth
	Model improved!!!
EPOCH 619/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02194887721668816		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.02194887721668816 | validation: 0.023552038931650466]
	TIME [epoch: 6.3 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02087870832682081		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.02087870832682081 | validation: 0.025828751908125724]
	TIME [epoch: 6.32 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021055161580728194		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.021055161580728194 | validation: 0.0247007497918241]
	TIME [epoch: 6.27 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02212682047322507		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.02212682047322507 | validation: 0.02343097811767707]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_622.pth
	Model improved!!!
EPOCH 623/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02040120378894144		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.02040120378894144 | validation: 0.024777735871270724]
	TIME [epoch: 6.3 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021377691349921166		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.021377691349921166 | validation: 0.02425080058319571]
	TIME [epoch: 6.28 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021153347488199824		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.021153347488199824 | validation: 0.022808886712771165]
	TIME [epoch: 6.29 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_625.pth
	Model improved!!!
EPOCH 626/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.023042438579653483		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.023042438579653483 | validation: 0.022467442292196844]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_626.pth
	Model improved!!!
EPOCH 627/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021512801511426535		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.021512801511426535 | validation: 0.024635651610953924]
	TIME [epoch: 6.3 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019746483141672794		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.019746483141672794 | validation: 0.023396380841259074]
	TIME [epoch: 6.28 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019844415770653545		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.019844415770653545 | validation: 0.022314964594950415]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_629.pth
	Model improved!!!
EPOCH 630/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020086457875857305		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.020086457875857305 | validation: 0.025908822433104398]
	TIME [epoch: 6.32 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02155128927421909		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.02155128927421909 | validation: 0.023186608802667763]
	TIME [epoch: 6.29 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01996211244001699		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.01996211244001699 | validation: 0.023149621338039668]
	TIME [epoch: 6.3 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02072363846165308		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.02072363846165308 | validation: 0.024541316033048977]
	TIME [epoch: 6.32 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02061093605413001		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.02061093605413001 | validation: 0.023025859263737444]
	TIME [epoch: 6.28 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02131044643179613		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.02131044643179613 | validation: 0.025380215617935842]
	TIME [epoch: 6.29 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02054731883072699		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.02054731883072699 | validation: 0.024425102184241022]
	TIME [epoch: 6.29 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01959568448133473		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.01959568448133473 | validation: 0.025507539389956382]
	TIME [epoch: 6.3 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020607931532806927		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.020607931532806927 | validation: 0.021541678642493386]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_638.pth
	Model improved!!!
EPOCH 639/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020192036323378723		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.020192036323378723 | validation: 0.02072243519649384]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_639.pth
	Model improved!!!
EPOCH 640/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020569380069489628		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.020569380069489628 | validation: 0.02325081500616624]
	TIME [epoch: 6.31 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020404056411905763		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.020404056411905763 | validation: 0.0225663941819501]
	TIME [epoch: 6.31 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019584421747382584		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.019584421747382584 | validation: 0.024935922588791393]
	TIME [epoch: 6.3 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0200708607740505		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.0200708607740505 | validation: 0.0237466085961108]
	TIME [epoch: 6.32 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02220850957968063		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.02220850957968063 | validation: 0.023385554601967097]
	TIME [epoch: 6.3 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020999468573088202		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.020999468573088202 | validation: 0.02424456723325621]
	TIME [epoch: 6.35 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.022312480061258008		[learning rate: 0.00012324]
	Learning Rate: 0.000123245
	LOSS [training: 0.022312480061258008 | validation: 0.024237398497843538]
	TIME [epoch: 6.33 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02083164035049149		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.02083164035049149 | validation: 0.02264669251392409]
	TIME [epoch: 6.31 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020375971541598956		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.020375971541598956 | validation: 0.023174211449491406]
	TIME [epoch: 6.3 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02015516721126521		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.02015516721126521 | validation: 0.02278722703337374]
	TIME [epoch: 6.3 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0210966399981954		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.0210966399981954 | validation: 0.025239797410945394]
	TIME [epoch: 6.33 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020137670863123472		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.020137670863123472 | validation: 0.023657408935437538]
	TIME [epoch: 6.34 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019904073835778205		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.019904073835778205 | validation: 0.026123657774054784]
	TIME [epoch: 6.31 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020575761756402407		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.020575761756402407 | validation: 0.023903613132168725]
	TIME [epoch: 6.3 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021512322743091276		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.021512322743091276 | validation: 0.023530203359738627]
	TIME [epoch: 6.29 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020064090507657485		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.020064090507657485 | validation: 0.024958157790102478]
	TIME [epoch: 6.3 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01981127059082078		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.01981127059082078 | validation: 0.026526162637773222]
	TIME [epoch: 6.3 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02048239844698226		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.02048239844698226 | validation: 0.02348472441796336]
	TIME [epoch: 6.32 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020081773536105685		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.020081773536105685 | validation: 0.02374313895418262]
	TIME [epoch: 6.32 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019695023811310046		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.019695023811310046 | validation: 0.02332445142020392]
	TIME [epoch: 6.3 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019714188864478278		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.019714188864478278 | validation: 0.022925443206866576]
	TIME [epoch: 6.31 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02014440644145949		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.02014440644145949 | validation: 0.02163000491369882]
	TIME [epoch: 6.32 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019370942134798445		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.019370942134798445 | validation: 0.023009179948563337]
	TIME [epoch: 6.3 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020350604156945822		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.020350604156945822 | validation: 0.025245341334961875]
	TIME [epoch: 6.31 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020269614984079122		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.020269614984079122 | validation: 0.026876978117046132]
	TIME [epoch: 6.34 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019234212665468375		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.019234212665468375 | validation: 0.022272782819918624]
	TIME [epoch: 6.33 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018991351796462497		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.018991351796462497 | validation: 0.024340389480144643]
	TIME [epoch: 6.31 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019958457820912275		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.019958457820912275 | validation: 0.02525714600727473]
	TIME [epoch: 6.31 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019830876623604762		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.019830876623604762 | validation: 0.021681218156528]
	TIME [epoch: 6.3 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019069791402049896		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.019069791402049896 | validation: 0.023065114564318635]
	TIME [epoch: 6.32 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018762284563630778		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.018762284563630778 | validation: 0.024369933654795867]
	TIME [epoch: 6.35 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01938439173035161		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.01938439173035161 | validation: 0.02380538153523197]
	TIME [epoch: 6.31 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019544068530472518		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.019544068530472518 | validation: 0.024278185758257277]
	TIME [epoch: 6.32 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02040045905989371		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.02040045905989371 | validation: 0.02176294056444815]
	TIME [epoch: 6.32 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020395211748889232		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.020395211748889232 | validation: 0.023390772670606436]
	TIME [epoch: 6.32 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01958888341020036		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.01958888341020036 | validation: 0.02413362631034916]
	TIME [epoch: 6.29 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020484110476861363		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.020484110476861363 | validation: 0.021719420230173987]
	TIME [epoch: 6.33 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01946621725051923		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.01946621725051923 | validation: 0.02405847889674776]
	TIME [epoch: 6.33 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.021345174108997687		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.021345174108997687 | validation: 0.02305959155210301]
	TIME [epoch: 6.31 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019867602492040393		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.019867602492040393 | validation: 0.0230633799495711]
	TIME [epoch: 6.3 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019474490348853395		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.019474490348853395 | validation: 0.021761841480049306]
	TIME [epoch: 6.31 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019274730361069928		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.019274730361069928 | validation: 0.02207105225296225]
	TIME [epoch: 6.31 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020193484871049654		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.020193484871049654 | validation: 0.020269884707499977]
	TIME [epoch: 6.3 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_682.pth
	Model improved!!!
EPOCH 683/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02010225387495234		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.02010225387495234 | validation: 0.021471194727046363]
	TIME [epoch: 6.31 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018665909215572287		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.018665909215572287 | validation: 0.02223285185869065]
	TIME [epoch: 6.29 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01890641762124909		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.01890641762124909 | validation: 0.02264986015605185]
	TIME [epoch: 6.3 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020206685608080553		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.020206685608080553 | validation: 0.021828806746704545]
	TIME [epoch: 6.29 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019750540219939296		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.019750540219939296 | validation: 0.022642978430640934]
	TIME [epoch: 6.31 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020522549482401623		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.020522549482401623 | validation: 0.02417550203161104]
	TIME [epoch: 6.33 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01995805099748639		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.01995805099748639 | validation: 0.02236743357346048]
	TIME [epoch: 6.35 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02032710287763407		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.02032710287763407 | validation: 0.024371352944683667]
	TIME [epoch: 6.33 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01984115057633723		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.01984115057633723 | validation: 0.021442149238662885]
	TIME [epoch: 6.31 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01977051606345933		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.01977051606345933 | validation: 0.02367712332651785]
	TIME [epoch: 6.29 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02042565249143036		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.02042565249143036 | validation: 0.021519336349864845]
	TIME [epoch: 6.3 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018795520556072726		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.018795520556072726 | validation: 0.022046391347038133]
	TIME [epoch: 6.32 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019778929599643794		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.019778929599643794 | validation: 0.02188151357022706]
	TIME [epoch: 6.32 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019869994303868502		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.019869994303868502 | validation: 0.02145213755947692]
	TIME [epoch: 6.34 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01878021390747945		[learning rate: 8.5871e-05]
	Learning Rate: 8.58709e-05
	LOSS [training: 0.01878021390747945 | validation: 0.022856341118623502]
	TIME [epoch: 6.31 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0203712357368309		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.0203712357368309 | validation: 0.02186496815209331]
	TIME [epoch: 6.3 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018380933353187016		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.018380933353187016 | validation: 0.02447468946411557]
	TIME [epoch: 6.3 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01887466773082532		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.01887466773082532 | validation: 0.021377402967869132]
	TIME [epoch: 6.31 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01897253490036123		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.01897253490036123 | validation: 0.023345745629295245]
	TIME [epoch: 6.31 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019918183303257377		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.019918183303257377 | validation: 0.02340512165854755]
	TIME [epoch: 6.35 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01874266572709024		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.01874266572709024 | validation: 0.020691255333633137]
	TIME [epoch: 6.32 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0187342740300122		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.0187342740300122 | validation: 0.025165160537601285]
	TIME [epoch: 6.31 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019627273369852084		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.019627273369852084 | validation: 0.022017602687231616]
	TIME [epoch: 6.29 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019343027936252595		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.019343027936252595 | validation: 0.024258862663907733]
	TIME [epoch: 6.29 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018586652473256967		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.018586652473256967 | validation: 0.02099527898781782]
	TIME [epoch: 6.32 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019388155697542757		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.019388155697542757 | validation: 0.019687411583960857]
	TIME [epoch: 6.32 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_708.pth
	Model improved!!!
EPOCH 709/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01996221168233426		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.01996221168233426 | validation: 0.023024410245752688]
	TIME [epoch: 6.3 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019580979372188036		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.019580979372188036 | validation: 0.02239718741458163]
	TIME [epoch: 6.29 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01939659553030619		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.01939659553030619 | validation: 0.02135917503530295]
	TIME [epoch: 6.28 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01854771049830873		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.01854771049830873 | validation: 0.023456800519219712]
	TIME [epoch: 6.32 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01876519914564804		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.01876519914564804 | validation: 0.02234013304655441]
	TIME [epoch: 6.32 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019715286686417775		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.019715286686417775 | validation: 0.020123939237941996]
	TIME [epoch: 6.32 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01896214992802331		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.01896214992802331 | validation: 0.022034515449971773]
	TIME [epoch: 6.32 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.020092586924933487		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.020092586924933487 | validation: 0.020917452103331454]
	TIME [epoch: 6.32 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01882863875850992		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.01882863875850992 | validation: 0.025122082615551312]
	TIME [epoch: 6.32 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017949795588990278		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.017949795588990278 | validation: 0.023738246999192594]
	TIME [epoch: 6.3 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017982470693414132		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.017982470693414132 | validation: 0.022076309842807784]
	TIME [epoch: 6.29 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018797123345151018		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.018797123345151018 | validation: 0.02203869208404586]
	TIME [epoch: 6.32 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01745653083534449		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.01745653083534449 | validation: 0.021451846084792114]
	TIME [epoch: 6.33 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01882983251133261		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.01882983251133261 | validation: 0.02304198511902908]
	TIME [epoch: 6.31 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018124185996064468		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.018124185996064468 | validation: 0.02267287669712306]
	TIME [epoch: 6.29 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019389809935194645		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.019389809935194645 | validation: 0.02332585960529245]
	TIME [epoch: 6.29 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018914586027995256		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.018914586027995256 | validation: 0.022937648367222202]
	TIME [epoch: 6.3 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019668058491513516		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.019668058491513516 | validation: 0.022036468030154473]
	TIME [epoch: 6.29 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018655896596102686		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.018655896596102686 | validation: 0.022076367590967583]
	TIME [epoch: 6.32 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01989960652968242		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.01989960652968242 | validation: 0.022513565084461302]
	TIME [epoch: 6.33 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01837637311535131		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.01837637311535131 | validation: 0.024311113961325254]
	TIME [epoch: 6.32 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019321687393215776		[learning rate: 6.7968e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.019321687393215776 | validation: 0.022292950485597186]
	TIME [epoch: 6.32 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01883360221623337		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.01883360221623337 | validation: 0.02025404716055107]
	TIME [epoch: 6.32 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018600204960871225		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.018600204960871225 | validation: 0.020345195568278858]
	TIME [epoch: 6.31 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018032170865632545		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.018032170865632545 | validation: 0.021931455666960316]
	TIME [epoch: 6.32 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018875462033723065		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.018875462033723065 | validation: 0.019965160830471753]
	TIME [epoch: 6.33 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018257990606791362		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.018257990606791362 | validation: 0.0246053783726179]
	TIME [epoch: 6.3 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01868153551565021		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.01868153551565021 | validation: 0.02330517415259439]
	TIME [epoch: 6.29 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01875519429020763		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.01875519429020763 | validation: 0.022593961117487616]
	TIME [epoch: 6.32 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01914480978611927		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.01914480978611927 | validation: 0.022189451611764124]
	TIME [epoch: 6.32 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01766769224425007		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.01766769224425007 | validation: 0.02182020502322729]
	TIME [epoch: 6.33 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019790590998864953		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.019790590998864953 | validation: 0.021755167880524978]
	TIME [epoch: 6.36 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01857323453615968		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.01857323453615968 | validation: 0.019850866621755916]
	TIME [epoch: 6.33 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01851510422944233		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.01851510422944233 | validation: 0.019612183867363988]
	TIME [epoch: 6.33 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_742.pth
	Model improved!!!
EPOCH 743/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017299879572898854		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.017299879572898854 | validation: 0.021911254791623726]
	TIME [epoch: 6.33 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01860006005251816		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.01860006005251816 | validation: 0.023269512111463733]
	TIME [epoch: 6.33 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01890611894875039		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.01890611894875039 | validation: 0.021317379893681033]
	TIME [epoch: 6.31 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017576988919195374		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.017576988919195374 | validation: 0.022185810228627253]
	TIME [epoch: 6.34 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017813929766583476		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.017813929766583476 | validation: 0.022439583451176054]
	TIME [epoch: 6.31 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017779251056031793		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.017779251056031793 | validation: 0.023153489197282096]
	TIME [epoch: 6.32 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018087696236266736		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.018087696236266736 | validation: 0.022988755094420766]
	TIME [epoch: 6.3 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018079790622258254		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.018079790622258254 | validation: 0.02282805735528643]
	TIME [epoch: 6.3 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018423476045275028		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.018423476045275028 | validation: 0.021818247843656643]
	TIME [epoch: 6.33 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01810087642620718		[learning rate: 5.8159e-05]
	Learning Rate: 5.81588e-05
	LOSS [training: 0.01810087642620718 | validation: 0.022812575507334468]
	TIME [epoch: 6.33 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01817737750897567		[learning rate: 5.7748e-05]
	Learning Rate: 5.77482e-05
	LOSS [training: 0.01817737750897567 | validation: 0.021798967103831564]
	TIME [epoch: 6.32 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018642728440714317		[learning rate: 5.7341e-05]
	Learning Rate: 5.73405e-05
	LOSS [training: 0.018642728440714317 | validation: 0.021433576587621263]
	TIME [epoch: 6.3 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018130436331682098		[learning rate: 5.6936e-05]
	Learning Rate: 5.69357e-05
	LOSS [training: 0.018130436331682098 | validation: 0.02009999267263245]
	TIME [epoch: 6.32 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018140518394767845		[learning rate: 5.6534e-05]
	Learning Rate: 5.65337e-05
	LOSS [training: 0.018140518394767845 | validation: 0.02106695202161259]
	TIME [epoch: 6.3 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018875260680282606		[learning rate: 5.6135e-05]
	Learning Rate: 5.61346e-05
	LOSS [training: 0.018875260680282606 | validation: 0.022136403601384663]
	TIME [epoch: 6.3 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01750557363866663		[learning rate: 5.5738e-05]
	Learning Rate: 5.57383e-05
	LOSS [training: 0.01750557363866663 | validation: 0.02316767130035785]
	TIME [epoch: 6.31 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01808623427244164		[learning rate: 5.5345e-05]
	Learning Rate: 5.53448e-05
	LOSS [training: 0.01808623427244164 | validation: 0.023318859928281588]
	TIME [epoch: 6.34 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017724547104415234		[learning rate: 5.4954e-05]
	Learning Rate: 5.49541e-05
	LOSS [training: 0.017724547104415234 | validation: 0.0218343002821088]
	TIME [epoch: 6.31 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017913682232615257		[learning rate: 5.4566e-05]
	Learning Rate: 5.45661e-05
	LOSS [training: 0.017913682232615257 | validation: 0.02541126873598423]
	TIME [epoch: 6.3 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01875816254086484		[learning rate: 5.4181e-05]
	Learning Rate: 5.41809e-05
	LOSS [training: 0.01875816254086484 | validation: 0.020947469893074222]
	TIME [epoch: 6.33 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.02002385862511727		[learning rate: 5.3798e-05]
	Learning Rate: 5.37984e-05
	LOSS [training: 0.02002385862511727 | validation: 0.021099736254391818]
	TIME [epoch: 6.32 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01864327906009592		[learning rate: 5.3419e-05]
	Learning Rate: 5.34186e-05
	LOSS [training: 0.01864327906009592 | validation: 0.021283076764642757]
	TIME [epoch: 6.3 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017850404905458594		[learning rate: 5.3041e-05]
	Learning Rate: 5.30415e-05
	LOSS [training: 0.017850404905458594 | validation: 0.020604766638690514]
	TIME [epoch: 6.35 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01779279483867481		[learning rate: 5.2667e-05]
	Learning Rate: 5.2667e-05
	LOSS [training: 0.01779279483867481 | validation: 0.021408896601413215]
	TIME [epoch: 6.31 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018103175543609448		[learning rate: 5.2295e-05]
	Learning Rate: 5.22952e-05
	LOSS [training: 0.018103175543609448 | validation: 0.021037932144627354]
	TIME [epoch: 6.3 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01825427299336426		[learning rate: 5.1926e-05]
	Learning Rate: 5.1926e-05
	LOSS [training: 0.01825427299336426 | validation: 0.02170905802364856]
	TIME [epoch: 6.32 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017383798396157155		[learning rate: 5.1559e-05]
	Learning Rate: 5.15594e-05
	LOSS [training: 0.017383798396157155 | validation: 0.022105962803322117]
	TIME [epoch: 6.32 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018545242544312397		[learning rate: 5.1195e-05]
	Learning Rate: 5.11954e-05
	LOSS [training: 0.018545242544312397 | validation: 0.021643350680935522]
	TIME [epoch: 6.32 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018412134424712623		[learning rate: 5.0834e-05]
	Learning Rate: 5.0834e-05
	LOSS [training: 0.018412134424712623 | validation: 0.022176164636838743]
	TIME [epoch: 6.34 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019931877165867917		[learning rate: 5.0475e-05]
	Learning Rate: 5.04751e-05
	LOSS [training: 0.019931877165867917 | validation: 0.020728781894235035]
	TIME [epoch: 6.35 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018379662818371792		[learning rate: 5.0119e-05]
	Learning Rate: 5.01187e-05
	LOSS [training: 0.018379662818371792 | validation: 0.02220973116522658]
	TIME [epoch: 6.33 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017679337261923802		[learning rate: 4.9765e-05]
	Learning Rate: 4.97649e-05
	LOSS [training: 0.017679337261923802 | validation: 0.02124785246216119]
	TIME [epoch: 6.32 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017347214959533126		[learning rate: 4.9414e-05]
	Learning Rate: 4.94136e-05
	LOSS [training: 0.017347214959533126 | validation: 0.022112582829876905]
	TIME [epoch: 6.33 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01798371523451934		[learning rate: 4.9065e-05]
	Learning Rate: 4.90647e-05
	LOSS [training: 0.01798371523451934 | validation: 0.022917526910293785]
	TIME [epoch: 6.3 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017835069822380166		[learning rate: 4.8718e-05]
	Learning Rate: 4.87183e-05
	LOSS [training: 0.017835069822380166 | validation: 0.021820487009986823]
	TIME [epoch: 6.31 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018097231684304528		[learning rate: 4.8374e-05]
	Learning Rate: 4.83744e-05
	LOSS [training: 0.018097231684304528 | validation: 0.02176689063460076]
	TIME [epoch: 6.33 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018381165624177714		[learning rate: 4.8033e-05]
	Learning Rate: 4.80329e-05
	LOSS [training: 0.018381165624177714 | validation: 0.021604798537946973]
	TIME [epoch: 6.3 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016631749379954312		[learning rate: 4.7694e-05]
	Learning Rate: 4.76938e-05
	LOSS [training: 0.016631749379954312 | validation: 0.019013087356599463]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_780.pth
	Model improved!!!
EPOCH 781/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017916247958822927		[learning rate: 4.7357e-05]
	Learning Rate: 4.73571e-05
	LOSS [training: 0.017916247958822927 | validation: 0.020436671725343226]
	TIME [epoch: 6.32 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018884099000903903		[learning rate: 4.7023e-05]
	Learning Rate: 4.70227e-05
	LOSS [training: 0.018884099000903903 | validation: 0.019831621229514554]
	TIME [epoch: 6.32 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017810607528135194		[learning rate: 4.6691e-05]
	Learning Rate: 4.66907e-05
	LOSS [training: 0.017810607528135194 | validation: 0.018949109298139633]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_783.pth
	Model improved!!!
EPOCH 784/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01809073429018506		[learning rate: 4.6361e-05]
	Learning Rate: 4.63611e-05
	LOSS [training: 0.01809073429018506 | validation: 0.020814208089956394]
	TIME [epoch: 6.33 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018912509030223967		[learning rate: 4.6034e-05]
	Learning Rate: 4.60338e-05
	LOSS [training: 0.018912509030223967 | validation: 0.020327429431460587]
	TIME [epoch: 6.31 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01746092200158022		[learning rate: 4.5709e-05]
	Learning Rate: 4.57088e-05
	LOSS [training: 0.01746092200158022 | validation: 0.021589956574902692]
	TIME [epoch: 6.29 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018308114752150925		[learning rate: 4.5386e-05]
	Learning Rate: 4.53861e-05
	LOSS [training: 0.018308114752150925 | validation: 0.020514777692911663]
	TIME [epoch: 6.3 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0175077803067129		[learning rate: 4.5066e-05]
	Learning Rate: 4.50657e-05
	LOSS [training: 0.0175077803067129 | validation: 0.020972430257692187]
	TIME [epoch: 6.3 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01673957723458585		[learning rate: 4.4748e-05]
	Learning Rate: 4.47476e-05
	LOSS [training: 0.01673957723458585 | validation: 0.020747694574987437]
	TIME [epoch: 6.31 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018306438814083125		[learning rate: 4.4432e-05]
	Learning Rate: 4.44316e-05
	LOSS [training: 0.018306438814083125 | validation: 0.021484707691431338]
	TIME [epoch: 6.33 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01693668292495533		[learning rate: 4.4118e-05]
	Learning Rate: 4.4118e-05
	LOSS [training: 0.01693668292495533 | validation: 0.021142897517361942]
	TIME [epoch: 6.35 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018026033425163204		[learning rate: 4.3807e-05]
	Learning Rate: 4.38065e-05
	LOSS [training: 0.018026033425163204 | validation: 0.022060744664992634]
	TIME [epoch: 6.31 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018625714855774234		[learning rate: 4.3497e-05]
	Learning Rate: 4.34972e-05
	LOSS [training: 0.018625714855774234 | validation: 0.024320902034365877]
	TIME [epoch: 6.29 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0186465795206417		[learning rate: 4.319e-05]
	Learning Rate: 4.31902e-05
	LOSS [training: 0.0186465795206417 | validation: 0.021126859006952527]
	TIME [epoch: 6.33 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017671399887136827		[learning rate: 4.2885e-05]
	Learning Rate: 4.28852e-05
	LOSS [training: 0.017671399887136827 | validation: 0.020984085136290576]
	TIME [epoch: 6.32 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018704581914133225		[learning rate: 4.2582e-05]
	Learning Rate: 4.25825e-05
	LOSS [training: 0.018704581914133225 | validation: 0.021058991773003917]
	TIME [epoch: 6.33 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01768382575214368		[learning rate: 4.2282e-05]
	Learning Rate: 4.22819e-05
	LOSS [training: 0.01768382575214368 | validation: 0.021229328989290364]
	TIME [epoch: 6.34 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017555713282441442		[learning rate: 4.1983e-05]
	Learning Rate: 4.19833e-05
	LOSS [training: 0.017555713282441442 | validation: 0.020121797308621826]
	TIME [epoch: 6.32 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0175210903737717		[learning rate: 4.1687e-05]
	Learning Rate: 4.16869e-05
	LOSS [training: 0.0175210903737717 | validation: 0.020345276629453235]
	TIME [epoch: 6.31 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01824559175030365		[learning rate: 4.1393e-05]
	Learning Rate: 4.13926e-05
	LOSS [training: 0.01824559175030365 | validation: 0.020401197364683474]
	TIME [epoch: 6.29 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018192530782137895		[learning rate: 4.11e-05]
	Learning Rate: 4.11004e-05
	LOSS [training: 0.018192530782137895 | validation: 0.021049681284645946]
	TIME [epoch: 6.32 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018770152906343394		[learning rate: 4.081e-05]
	Learning Rate: 4.08103e-05
	LOSS [training: 0.018770152906343394 | validation: 0.022055976084600987]
	TIME [epoch: 6.31 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018474172640944725		[learning rate: 4.0522e-05]
	Learning Rate: 4.05221e-05
	LOSS [training: 0.018474172640944725 | validation: 0.0202692243345526]
	TIME [epoch: 6.35 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01752041461687514		[learning rate: 4.0236e-05]
	Learning Rate: 4.02361e-05
	LOSS [training: 0.01752041461687514 | validation: 0.0202732424478263]
	TIME [epoch: 6.31 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01776202567458289		[learning rate: 3.9952e-05]
	Learning Rate: 3.9952e-05
	LOSS [training: 0.01776202567458289 | validation: 0.02165148844818108]
	TIME [epoch: 6.33 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018864136821038692		[learning rate: 3.967e-05]
	Learning Rate: 3.967e-05
	LOSS [training: 0.018864136821038692 | validation: 0.01981382140832155]
	TIME [epoch: 6.3 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017054052470364553		[learning rate: 3.939e-05]
	Learning Rate: 3.93899e-05
	LOSS [training: 0.017054052470364553 | validation: 0.02141790524646854]
	TIME [epoch: 6.32 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017831676602701105		[learning rate: 3.9112e-05]
	Learning Rate: 3.91118e-05
	LOSS [training: 0.017831676602701105 | validation: 0.02000597465031811]
	TIME [epoch: 6.3 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017458843566213054		[learning rate: 3.8836e-05]
	Learning Rate: 3.88357e-05
	LOSS [training: 0.017458843566213054 | validation: 0.020657955793887156]
	TIME [epoch: 6.33 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017976324081751288		[learning rate: 3.8561e-05]
	Learning Rate: 3.85615e-05
	LOSS [training: 0.017976324081751288 | validation: 0.021625510157861058]
	TIME [epoch: 6.35 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01694382582530749		[learning rate: 3.8289e-05]
	Learning Rate: 3.82893e-05
	LOSS [training: 0.01694382582530749 | validation: 0.019519599871467674]
	TIME [epoch: 6.32 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016856590719336845		[learning rate: 3.8019e-05]
	Learning Rate: 3.8019e-05
	LOSS [training: 0.016856590719336845 | validation: 0.019091741827401677]
	TIME [epoch: 6.32 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017514007472474863		[learning rate: 3.7751e-05]
	Learning Rate: 3.77505e-05
	LOSS [training: 0.017514007472474863 | validation: 0.019540245492997363]
	TIME [epoch: 6.31 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01815765506765654		[learning rate: 3.7484e-05]
	Learning Rate: 3.7484e-05
	LOSS [training: 0.01815765506765654 | validation: 0.021552642861185273]
	TIME [epoch: 6.32 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017489462229450987		[learning rate: 3.7219e-05]
	Learning Rate: 3.72194e-05
	LOSS [training: 0.017489462229450987 | validation: 0.02034029669855121]
	TIME [epoch: 6.31 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017647840761811945		[learning rate: 3.6957e-05]
	Learning Rate: 3.69566e-05
	LOSS [training: 0.017647840761811945 | validation: 0.02094098991524598]
	TIME [epoch: 6.34 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018227399336696093		[learning rate: 3.6696e-05]
	Learning Rate: 3.66957e-05
	LOSS [training: 0.018227399336696093 | validation: 0.022186020550958887]
	TIME [epoch: 6.33 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01912836192503762		[learning rate: 3.6437e-05]
	Learning Rate: 3.64367e-05
	LOSS [training: 0.01912836192503762 | validation: 0.02138920493102521]
	TIME [epoch: 6.29 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017190239602460834		[learning rate: 3.6179e-05]
	Learning Rate: 3.61794e-05
	LOSS [training: 0.017190239602460834 | validation: 0.021892628159136004]
	TIME [epoch: 6.3 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01713188039515034		[learning rate: 3.5924e-05]
	Learning Rate: 3.5924e-05
	LOSS [training: 0.01713188039515034 | validation: 0.023894912108531133]
	TIME [epoch: 6.32 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01605511268760089		[learning rate: 3.567e-05]
	Learning Rate: 3.56704e-05
	LOSS [training: 0.01605511268760089 | validation: 0.023146188537781654]
	TIME [epoch: 6.31 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01686567001669327		[learning rate: 3.5419e-05]
	Learning Rate: 3.54186e-05
	LOSS [training: 0.01686567001669327 | validation: 0.020737638582652637]
	TIME [epoch: 6.33 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017273121908688994		[learning rate: 3.5169e-05]
	Learning Rate: 3.51685e-05
	LOSS [training: 0.017273121908688994 | validation: 0.019596239554084533]
	TIME [epoch: 6.32 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017070653042513914		[learning rate: 3.492e-05]
	Learning Rate: 3.49202e-05
	LOSS [training: 0.017070653042513914 | validation: 0.020078908360201264]
	TIME [epoch: 6.33 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018185006773681046		[learning rate: 3.4674e-05]
	Learning Rate: 3.46737e-05
	LOSS [training: 0.018185006773681046 | validation: 0.020412906202550277]
	TIME [epoch: 6.3 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018303446314140694		[learning rate: 3.4429e-05]
	Learning Rate: 3.44289e-05
	LOSS [training: 0.018303446314140694 | validation: 0.019674314471568505]
	TIME [epoch: 6.31 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0173870483212911		[learning rate: 3.4186e-05]
	Learning Rate: 3.41858e-05
	LOSS [training: 0.0173870483212911 | validation: 0.021341007786723862]
	TIME [epoch: 6.33 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017276820870937355		[learning rate: 3.3944e-05]
	Learning Rate: 3.39445e-05
	LOSS [training: 0.017276820870937355 | validation: 0.021236757641725914]
	TIME [epoch: 6.31 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018225767353375943		[learning rate: 3.3705e-05]
	Learning Rate: 3.37049e-05
	LOSS [training: 0.018225767353375943 | validation: 0.021270532075144732]
	TIME [epoch: 6.32 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01696310466062809		[learning rate: 3.3467e-05]
	Learning Rate: 3.34669e-05
	LOSS [training: 0.01696310466062809 | validation: 0.0202838342173345]
	TIME [epoch: 6.29 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016942861552820557		[learning rate: 3.3231e-05]
	Learning Rate: 3.32306e-05
	LOSS [training: 0.016942861552820557 | validation: 0.020337071276140168]
	TIME [epoch: 6.29 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01701407694400197		[learning rate: 3.2996e-05]
	Learning Rate: 3.2996e-05
	LOSS [training: 0.01701407694400197 | validation: 0.019735799207786037]
	TIME [epoch: 6.31 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01857003064264545		[learning rate: 3.2763e-05]
	Learning Rate: 3.27631e-05
	LOSS [training: 0.01857003064264545 | validation: 0.022167296090030125]
	TIME [epoch: 6.32 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01741592256148906		[learning rate: 3.2532e-05]
	Learning Rate: 3.25318e-05
	LOSS [training: 0.01741592256148906 | validation: 0.02094113355304667]
	TIME [epoch: 6.3 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018094947881441413		[learning rate: 3.2302e-05]
	Learning Rate: 3.23021e-05
	LOSS [training: 0.018094947881441413 | validation: 0.02333349823307619]
	TIME [epoch: 6.37 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017980565083822047		[learning rate: 3.2074e-05]
	Learning Rate: 3.20741e-05
	LOSS [training: 0.017980565083822047 | validation: 0.019633237999300404]
	TIME [epoch: 6.3 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0179427534183777		[learning rate: 3.1848e-05]
	Learning Rate: 3.18476e-05
	LOSS [training: 0.0179427534183777 | validation: 0.01991567954107717]
	TIME [epoch: 6.32 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018193286189746923		[learning rate: 3.1623e-05]
	Learning Rate: 3.16228e-05
	LOSS [training: 0.018193286189746923 | validation: 0.02013491950424698]
	TIME [epoch: 6.29 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01675859318203168		[learning rate: 3.14e-05]
	Learning Rate: 3.13995e-05
	LOSS [training: 0.01675859318203168 | validation: 0.02021640771325289]
	TIME [epoch: 6.32 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017749082647606926		[learning rate: 3.1178e-05]
	Learning Rate: 3.11778e-05
	LOSS [training: 0.017749082647606926 | validation: 0.020696741271378778]
	TIME [epoch: 6.32 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017281950524609284		[learning rate: 3.0958e-05]
	Learning Rate: 3.09577e-05
	LOSS [training: 0.017281950524609284 | validation: 0.018822247880078347]
	TIME [epoch: 6.37 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_841.pth
	Model improved!!!
EPOCH 842/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017469295875082114		[learning rate: 3.0739e-05]
	Learning Rate: 3.07392e-05
	LOSS [training: 0.017469295875082114 | validation: 0.021606557504455528]
	TIME [epoch: 6.3 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0175771882443121		[learning rate: 3.0522e-05]
	Learning Rate: 3.05222e-05
	LOSS [training: 0.0175771882443121 | validation: 0.020535818170013095]
	TIME [epoch: 6.28 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017521294555703094		[learning rate: 3.0307e-05]
	Learning Rate: 3.03067e-05
	LOSS [training: 0.017521294555703094 | validation: 0.02027056052744527]
	TIME [epoch: 6.31 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017784953435904895		[learning rate: 3.0093e-05]
	Learning Rate: 3.00927e-05
	LOSS [training: 0.017784953435904895 | validation: 0.02224533375019044]
	TIME [epoch: 6.31 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016943414077203733		[learning rate: 2.988e-05]
	Learning Rate: 2.98803e-05
	LOSS [training: 0.016943414077203733 | validation: 0.019740085296337914]
	TIME [epoch: 6.32 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017336889492438598		[learning rate: 2.9669e-05]
	Learning Rate: 2.96693e-05
	LOSS [training: 0.017336889492438598 | validation: 0.019904080423862165]
	TIME [epoch: 6.34 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01725352639144824		[learning rate: 2.946e-05]
	Learning Rate: 2.94599e-05
	LOSS [training: 0.01725352639144824 | validation: 0.02224498891645145]
	TIME [epoch: 6.33 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.019273433707543586		[learning rate: 2.9252e-05]
	Learning Rate: 2.92519e-05
	LOSS [training: 0.019273433707543586 | validation: 0.02283260941204235]
	TIME [epoch: 6.32 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01835275305351474		[learning rate: 2.9045e-05]
	Learning Rate: 2.90454e-05
	LOSS [training: 0.01835275305351474 | validation: 0.01980963574526113]
	TIME [epoch: 6.29 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01821186142922504		[learning rate: 2.884e-05]
	Learning Rate: 2.88403e-05
	LOSS [training: 0.01821186142922504 | validation: 0.021772701784242517]
	TIME [epoch: 6.29 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01708048883509022		[learning rate: 2.8637e-05]
	Learning Rate: 2.86367e-05
	LOSS [training: 0.01708048883509022 | validation: 0.020729376595699834]
	TIME [epoch: 6.3 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01648764748737407		[learning rate: 2.8435e-05]
	Learning Rate: 2.84345e-05
	LOSS [training: 0.01648764748737407 | validation: 0.023307087845513635]
	TIME [epoch: 6.34 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017582343782984147		[learning rate: 2.8234e-05]
	Learning Rate: 2.82338e-05
	LOSS [training: 0.017582343782984147 | validation: 0.01822529903972201]
	TIME [epoch: 6.33 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_854.pth
	Model improved!!!
EPOCH 855/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01731042864175667		[learning rate: 2.8034e-05]
	Learning Rate: 2.80345e-05
	LOSS [training: 0.01731042864175667 | validation: 0.01985991826675091]
	TIME [epoch: 6.27 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017904292242897173		[learning rate: 2.7837e-05]
	Learning Rate: 2.78366e-05
	LOSS [training: 0.017904292242897173 | validation: 0.021390563973876887]
	TIME [epoch: 6.29 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01796853259897832		[learning rate: 2.764e-05]
	Learning Rate: 2.764e-05
	LOSS [training: 0.01796853259897832 | validation: 0.02113506361872707]
	TIME [epoch: 6.26 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017441251358034707		[learning rate: 2.7445e-05]
	Learning Rate: 2.74449e-05
	LOSS [training: 0.017441251358034707 | validation: 0.019996166858447753]
	TIME [epoch: 6.3 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01814783268411818		[learning rate: 2.7251e-05]
	Learning Rate: 2.72511e-05
	LOSS [training: 0.01814783268411818 | validation: 0.020673680838738037]
	TIME [epoch: 6.27 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016535082424136184		[learning rate: 2.7059e-05]
	Learning Rate: 2.70587e-05
	LOSS [training: 0.016535082424136184 | validation: 0.0196979905309055]
	TIME [epoch: 6.31 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017533128403499035		[learning rate: 2.6868e-05]
	Learning Rate: 2.68677e-05
	LOSS [training: 0.017533128403499035 | validation: 0.020213471046142097]
	TIME [epoch: 6.3 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018210067146706192		[learning rate: 2.6678e-05]
	Learning Rate: 2.6678e-05
	LOSS [training: 0.018210067146706192 | validation: 0.021469472120799346]
	TIME [epoch: 6.28 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01785935103119136		[learning rate: 2.649e-05]
	Learning Rate: 2.64897e-05
	LOSS [training: 0.01785935103119136 | validation: 0.020008897401946685]
	TIME [epoch: 6.29 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01745752656612811		[learning rate: 2.6303e-05]
	Learning Rate: 2.63027e-05
	LOSS [training: 0.01745752656612811 | validation: 0.021279724698890903]
	TIME [epoch: 6.27 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01759805050866775		[learning rate: 2.6117e-05]
	Learning Rate: 2.6117e-05
	LOSS [training: 0.01759805050866775 | validation: 0.019490075683518723]
	TIME [epoch: 6.27 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017791577755048613		[learning rate: 2.5933e-05]
	Learning Rate: 2.59326e-05
	LOSS [training: 0.017791577755048613 | validation: 0.020138160171335153]
	TIME [epoch: 6.32 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01692302910748457		[learning rate: 2.575e-05]
	Learning Rate: 2.57495e-05
	LOSS [training: 0.01692302910748457 | validation: 0.020762034535737647]
	TIME [epoch: 6.3 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017040512060123802		[learning rate: 2.5568e-05]
	Learning Rate: 2.55677e-05
	LOSS [training: 0.017040512060123802 | validation: 0.01927707587044636]
	TIME [epoch: 6.32 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01748819344016931		[learning rate: 2.5387e-05]
	Learning Rate: 2.53872e-05
	LOSS [training: 0.01748819344016931 | validation: 0.022672409772867854]
	TIME [epoch: 6.29 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017238627026122604		[learning rate: 2.5208e-05]
	Learning Rate: 2.5208e-05
	LOSS [training: 0.017238627026122604 | validation: 0.020581411785060313]
	TIME [epoch: 6.28 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017528129393233206		[learning rate: 2.503e-05]
	Learning Rate: 2.503e-05
	LOSS [training: 0.017528129393233206 | validation: 0.01959361473716518]
	TIME [epoch: 6.31 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01778330525602296		[learning rate: 2.4853e-05]
	Learning Rate: 2.48533e-05
	LOSS [training: 0.01778330525602296 | validation: 0.0187211831416277]
	TIME [epoch: 6.3 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017247141691838754		[learning rate: 2.4678e-05]
	Learning Rate: 2.46779e-05
	LOSS [training: 0.017247141691838754 | validation: 0.022230100654944754]
	TIME [epoch: 6.3 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0176781347838578		[learning rate: 2.4504e-05]
	Learning Rate: 2.45037e-05
	LOSS [training: 0.0176781347838578 | validation: 0.021358548572749728]
	TIME [epoch: 6.28 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017166063348042992		[learning rate: 2.4331e-05]
	Learning Rate: 2.43307e-05
	LOSS [training: 0.017166063348042992 | validation: 0.021165619099952866]
	TIME [epoch: 6.3 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01777138664582116		[learning rate: 2.4159e-05]
	Learning Rate: 2.41589e-05
	LOSS [training: 0.01777138664582116 | validation: 0.022758858398532242]
	TIME [epoch: 6.31 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017047378053103145		[learning rate: 2.3988e-05]
	Learning Rate: 2.39883e-05
	LOSS [training: 0.017047378053103145 | validation: 0.01890243502064703]
	TIME [epoch: 6.31 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0176726738851877		[learning rate: 2.3819e-05]
	Learning Rate: 2.3819e-05
	LOSS [training: 0.0176726738851877 | validation: 0.020757204415322517]
	TIME [epoch: 6.29 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017194599528265686		[learning rate: 2.3651e-05]
	Learning Rate: 2.36508e-05
	LOSS [training: 0.017194599528265686 | validation: 0.019814767688993623]
	TIME [epoch: 6.32 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017258531173504163		[learning rate: 2.3484e-05]
	Learning Rate: 2.34838e-05
	LOSS [training: 0.017258531173504163 | validation: 0.018332227936793732]
	TIME [epoch: 6.32 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01618166159400531		[learning rate: 2.3318e-05]
	Learning Rate: 2.33181e-05
	LOSS [training: 0.01618166159400531 | validation: 0.021593801378976936]
	TIME [epoch: 6.31 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016796279082847483		[learning rate: 2.3153e-05]
	Learning Rate: 2.31534e-05
	LOSS [training: 0.016796279082847483 | validation: 0.018582077613186393]
	TIME [epoch: 6.28 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017095224103780032		[learning rate: 2.299e-05]
	Learning Rate: 2.299e-05
	LOSS [training: 0.017095224103780032 | validation: 0.020694395310704373]
	TIME [epoch: 6.28 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017950425390347108		[learning rate: 2.2828e-05]
	Learning Rate: 2.28277e-05
	LOSS [training: 0.017950425390347108 | validation: 0.01858252234253351]
	TIME [epoch: 6.31 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017442128339195187		[learning rate: 2.2667e-05]
	Learning Rate: 2.26665e-05
	LOSS [training: 0.017442128339195187 | validation: 0.019169511990593854]
	TIME [epoch: 6.3 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016844496697177583		[learning rate: 2.2506e-05]
	Learning Rate: 2.25065e-05
	LOSS [training: 0.016844496697177583 | validation: 0.019094063401826157]
	TIME [epoch: 6.29 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016281723160469115		[learning rate: 2.2348e-05]
	Learning Rate: 2.23476e-05
	LOSS [training: 0.016281723160469115 | validation: 0.020822411257627807]
	TIME [epoch: 6.3 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01703288667281197		[learning rate: 2.219e-05]
	Learning Rate: 2.21898e-05
	LOSS [training: 0.01703288667281197 | validation: 0.02053426491694184]
	TIME [epoch: 6.29 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016519048784501007		[learning rate: 2.2033e-05]
	Learning Rate: 2.20332e-05
	LOSS [training: 0.016519048784501007 | validation: 0.019353080908512333]
	TIME [epoch: 6.27 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01724701025780949		[learning rate: 2.1878e-05]
	Learning Rate: 2.18776e-05
	LOSS [training: 0.01724701025780949 | validation: 0.022889178580896636]
	TIME [epoch: 6.29 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01712196465021663		[learning rate: 2.1723e-05]
	Learning Rate: 2.17232e-05
	LOSS [training: 0.01712196465021663 | validation: 0.019808645019619597]
	TIME [epoch: 6.28 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017288127551903072		[learning rate: 2.157e-05]
	Learning Rate: 2.15698e-05
	LOSS [training: 0.017288127551903072 | validation: 0.020288030972144627]
	TIME [epoch: 6.33 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017454485726673172		[learning rate: 2.1418e-05]
	Learning Rate: 2.14175e-05
	LOSS [training: 0.017454485726673172 | validation: 0.01999796347118842]
	TIME [epoch: 6.27 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016883008124837867		[learning rate: 2.1266e-05]
	Learning Rate: 2.12663e-05
	LOSS [training: 0.016883008124837867 | validation: 0.019559025005994535]
	TIME [epoch: 6.29 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017111492268007233		[learning rate: 2.1116e-05]
	Learning Rate: 2.11162e-05
	LOSS [training: 0.017111492268007233 | validation: 0.020385211797624016]
	TIME [epoch: 6.29 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01707152659255704		[learning rate: 2.0967e-05]
	Learning Rate: 2.09671e-05
	LOSS [training: 0.01707152659255704 | validation: 0.022373997556299915]
	TIME [epoch: 6.3 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017979838482832793		[learning rate: 2.0819e-05]
	Learning Rate: 2.08191e-05
	LOSS [training: 0.017979838482832793 | validation: 0.02050708776064284]
	TIME [epoch: 6.29 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017595043885395553		[learning rate: 2.0672e-05]
	Learning Rate: 2.06721e-05
	LOSS [training: 0.017595043885395553 | validation: 0.02107301357412148]
	TIME [epoch: 6.32 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01595317807436977		[learning rate: 2.0526e-05]
	Learning Rate: 2.05262e-05
	LOSS [training: 0.01595317807436977 | validation: 0.020707102530009083]
	TIME [epoch: 6.32 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01716599413611694		[learning rate: 2.0381e-05]
	Learning Rate: 2.03813e-05
	LOSS [training: 0.01716599413611694 | validation: 0.019316588228218814]
	TIME [epoch: 6.28 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016762926442405665		[learning rate: 2.0237e-05]
	Learning Rate: 2.02374e-05
	LOSS [training: 0.016762926442405665 | validation: 0.019934691985370748]
	TIME [epoch: 6.28 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01738323970611122		[learning rate: 2.0094e-05]
	Learning Rate: 2.00945e-05
	LOSS [training: 0.01738323970611122 | validation: 0.021846149277944173]
	TIME [epoch: 6.32 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017062544701768483		[learning rate: 1.9953e-05]
	Learning Rate: 1.99526e-05
	LOSS [training: 0.017062544701768483 | validation: 0.019065528350385164]
	TIME [epoch: 6.3 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017448350788690354		[learning rate: 1.9812e-05]
	Learning Rate: 1.98118e-05
	LOSS [training: 0.017448350788690354 | validation: 0.02070778696610953]
	TIME [epoch: 6.31 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01775997197235709		[learning rate: 1.9672e-05]
	Learning Rate: 1.96719e-05
	LOSS [training: 0.01775997197235709 | validation: 0.01829238255097825]
	TIME [epoch: 6.31 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017611776849028527		[learning rate: 1.9533e-05]
	Learning Rate: 1.9533e-05
	LOSS [training: 0.017611776849028527 | validation: 0.018486657571457798]
	TIME [epoch: 6.32 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017559601968609867		[learning rate: 1.9395e-05]
	Learning Rate: 1.93951e-05
	LOSS [training: 0.017559601968609867 | validation: 0.020532297830867564]
	TIME [epoch: 6.31 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016199246716486662		[learning rate: 1.9258e-05]
	Learning Rate: 1.92582e-05
	LOSS [training: 0.016199246716486662 | validation: 0.018319319124982993]
	TIME [epoch: 6.33 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0165444929399086		[learning rate: 1.9122e-05]
	Learning Rate: 1.91222e-05
	LOSS [training: 0.0165444929399086 | validation: 0.02006765387854419]
	TIME [epoch: 6.29 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016822247130449487		[learning rate: 1.8987e-05]
	Learning Rate: 1.89872e-05
	LOSS [training: 0.016822247130449487 | validation: 0.022792074084999864]
	TIME [epoch: 6.31 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016591823106755044		[learning rate: 1.8853e-05]
	Learning Rate: 1.88532e-05
	LOSS [training: 0.016591823106755044 | validation: 0.020861479031268372]
	TIME [epoch: 6.33 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01726718604547596		[learning rate: 1.872e-05]
	Learning Rate: 1.87201e-05
	LOSS [training: 0.01726718604547596 | validation: 0.020666525737299338]
	TIME [epoch: 6.32 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018209861344704033		[learning rate: 1.8588e-05]
	Learning Rate: 1.85879e-05
	LOSS [training: 0.018209861344704033 | validation: 0.019649025090543212]
	TIME [epoch: 6.3 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016974135593504275		[learning rate: 1.8457e-05]
	Learning Rate: 1.84567e-05
	LOSS [training: 0.016974135593504275 | validation: 0.020391065142608907]
	TIME [epoch: 6.29 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01755663581630905		[learning rate: 1.8326e-05]
	Learning Rate: 1.83264e-05
	LOSS [training: 0.01755663581630905 | validation: 0.018621422797764815]
	TIME [epoch: 6.3 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016315192528400035		[learning rate: 1.8197e-05]
	Learning Rate: 1.8197e-05
	LOSS [training: 0.016315192528400035 | validation: 0.020256290821666353]
	TIME [epoch: 6.3 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01754847748421557		[learning rate: 1.8069e-05]
	Learning Rate: 1.80685e-05
	LOSS [training: 0.01754847748421557 | validation: 0.02010884366927155]
	TIME [epoch: 6.35 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017703333164784334		[learning rate: 1.7941e-05]
	Learning Rate: 1.7941e-05
	LOSS [training: 0.017703333164784334 | validation: 0.01772319062718643]
	TIME [epoch: 6.31 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_918.pth
	Model improved!!!
EPOCH 919/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016705349847303458		[learning rate: 1.7814e-05]
	Learning Rate: 1.78143e-05
	LOSS [training: 0.016705349847303458 | validation: 0.021393214529592683]
	TIME [epoch: 6.32 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016973872685646603		[learning rate: 1.7689e-05]
	Learning Rate: 1.76886e-05
	LOSS [training: 0.016973872685646603 | validation: 0.0189715927667383]
	TIME [epoch: 6.26 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016441248208437977		[learning rate: 1.7564e-05]
	Learning Rate: 1.75637e-05
	LOSS [training: 0.016441248208437977 | validation: 0.021030144094899228]
	TIME [epoch: 6.28 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017153406756233808		[learning rate: 1.744e-05]
	Learning Rate: 1.74397e-05
	LOSS [training: 0.017153406756233808 | validation: 0.019759292831599336]
	TIME [epoch: 6.29 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017573417997566565		[learning rate: 1.7317e-05]
	Learning Rate: 1.73166e-05
	LOSS [training: 0.017573417997566565 | validation: 0.018740519540724084]
	TIME [epoch: 6.31 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016871969175150422		[learning rate: 1.7194e-05]
	Learning Rate: 1.71943e-05
	LOSS [training: 0.016871969175150422 | validation: 0.02062083958367414]
	TIME [epoch: 6.3 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017008699865647466		[learning rate: 1.7073e-05]
	Learning Rate: 1.70729e-05
	LOSS [training: 0.017008699865647466 | validation: 0.018702974650014106]
	TIME [epoch: 6.28 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016693498620805484		[learning rate: 1.6952e-05]
	Learning Rate: 1.69524e-05
	LOSS [training: 0.016693498620805484 | validation: 0.01876784688877269]
	TIME [epoch: 6.28 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016998090405029455		[learning rate: 1.6833e-05]
	Learning Rate: 1.68327e-05
	LOSS [training: 0.016998090405029455 | validation: 0.019698451498443927]
	TIME [epoch: 6.28 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016277440988912525		[learning rate: 1.6714e-05]
	Learning Rate: 1.67139e-05
	LOSS [training: 0.016277440988912525 | validation: 0.02031340806356187]
	TIME [epoch: 6.3 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016447923765909013		[learning rate: 1.6596e-05]
	Learning Rate: 1.65959e-05
	LOSS [training: 0.016447923765909013 | validation: 0.021535947735446648]
	TIME [epoch: 6.31 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017856639226719824		[learning rate: 1.6479e-05]
	Learning Rate: 1.64787e-05
	LOSS [training: 0.017856639226719824 | validation: 0.018584719111985056]
	TIME [epoch: 6.33 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.018116165410335158		[learning rate: 1.6362e-05]
	Learning Rate: 1.63624e-05
	LOSS [training: 0.018116165410335158 | validation: 0.02030290107426022]
	TIME [epoch: 6.29 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016496463942369408		[learning rate: 1.6247e-05]
	Learning Rate: 1.62469e-05
	LOSS [training: 0.016496463942369408 | validation: 0.021198722008480737]
	TIME [epoch: 6.3 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01777767498094749		[learning rate: 1.6132e-05]
	Learning Rate: 1.61322e-05
	LOSS [training: 0.01777767498094749 | validation: 0.018491630526289077]
	TIME [epoch: 6.3 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016594023570206968		[learning rate: 1.6018e-05]
	Learning Rate: 1.60183e-05
	LOSS [training: 0.016594023570206968 | validation: 0.019951903119668487]
	TIME [epoch: 6.29 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016751198208420512		[learning rate: 1.5905e-05]
	Learning Rate: 1.59052e-05
	LOSS [training: 0.016751198208420512 | validation: 0.019497280604029322]
	TIME [epoch: 6.3 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01637015975589273		[learning rate: 1.5793e-05]
	Learning Rate: 1.57929e-05
	LOSS [training: 0.01637015975589273 | validation: 0.019586293562521617]
	TIME [epoch: 6.32 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01812562564454198		[learning rate: 1.5681e-05]
	Learning Rate: 1.56814e-05
	LOSS [training: 0.01812562564454198 | validation: 0.020192439096063882]
	TIME [epoch: 6.3 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017524101960049714		[learning rate: 1.5571e-05]
	Learning Rate: 1.55707e-05
	LOSS [training: 0.017524101960049714 | validation: 0.020674961009450404]
	TIME [epoch: 6.28 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01644132701752203		[learning rate: 1.5461e-05]
	Learning Rate: 1.54608e-05
	LOSS [training: 0.01644132701752203 | validation: 0.02062376750478627]
	TIME [epoch: 6.3 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01674340131914566		[learning rate: 1.5352e-05]
	Learning Rate: 1.53516e-05
	LOSS [training: 0.01674340131914566 | validation: 0.02195455710790737]
	TIME [epoch: 6.29 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017722856145639586		[learning rate: 1.5243e-05]
	Learning Rate: 1.52432e-05
	LOSS [training: 0.017722856145639586 | validation: 0.021710526370770724]
	TIME [epoch: 6.3 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017101115748319174		[learning rate: 1.5136e-05]
	Learning Rate: 1.51356e-05
	LOSS [training: 0.017101115748319174 | validation: 0.019982054497028175]
	TIME [epoch: 6.31 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016182558050953255		[learning rate: 1.5029e-05]
	Learning Rate: 1.50288e-05
	LOSS [training: 0.016182558050953255 | validation: 0.02004647438224344]
	TIME [epoch: 6.33 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016347575858732224		[learning rate: 1.4923e-05]
	Learning Rate: 1.49227e-05
	LOSS [training: 0.016347575858732224 | validation: 0.021825682480501304]
	TIME [epoch: 6.29 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017402782741288904		[learning rate: 1.4817e-05]
	Learning Rate: 1.48173e-05
	LOSS [training: 0.017402782741288904 | validation: 0.019328875392892127]
	TIME [epoch: 6.32 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016957060900093524		[learning rate: 1.4713e-05]
	Learning Rate: 1.47127e-05
	LOSS [training: 0.016957060900093524 | validation: 0.018917861029947077]
	TIME [epoch: 6.29 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01768624455380592		[learning rate: 1.4609e-05]
	Learning Rate: 1.46088e-05
	LOSS [training: 0.01768624455380592 | validation: 0.02099085288121212]
	TIME [epoch: 6.29 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017023738890454575		[learning rate: 1.4506e-05]
	Learning Rate: 1.45057e-05
	LOSS [training: 0.017023738890454575 | validation: 0.01998420237023122]
	TIME [epoch: 6.29 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016758483772811694		[learning rate: 1.4403e-05]
	Learning Rate: 1.44033e-05
	LOSS [training: 0.016758483772811694 | validation: 0.019813327154394118]
	TIME [epoch: 6.33 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017332264320111347		[learning rate: 1.4302e-05]
	Learning Rate: 1.43016e-05
	LOSS [training: 0.017332264320111347 | validation: 0.021088464618462435]
	TIME [epoch: 6.3 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017364143993958844		[learning rate: 1.4201e-05]
	Learning Rate: 1.42006e-05
	LOSS [training: 0.017364143993958844 | validation: 0.020297925360898035]
	TIME [epoch: 6.31 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016589737029491833		[learning rate: 1.41e-05]
	Learning Rate: 1.41004e-05
	LOSS [training: 0.016589737029491833 | validation: 0.02022142046782755]
	TIME [epoch: 6.27 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016848552394588885		[learning rate: 1.4001e-05]
	Learning Rate: 1.40008e-05
	LOSS [training: 0.016848552394588885 | validation: 0.019375158994097808]
	TIME [epoch: 6.31 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017027826021448825		[learning rate: 1.3902e-05]
	Learning Rate: 1.3902e-05
	LOSS [training: 0.017027826021448825 | validation: 0.01867764548765979]
	TIME [epoch: 6.32 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017325584652360858		[learning rate: 1.3804e-05]
	Learning Rate: 1.38038e-05
	LOSS [training: 0.017325584652360858 | validation: 0.019383680792290248]
	TIME [epoch: 6.31 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01690146655794038		[learning rate: 1.3706e-05]
	Learning Rate: 1.37064e-05
	LOSS [training: 0.01690146655794038 | validation: 0.02015152557317848]
	TIME [epoch: 6.33 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016768524368208464		[learning rate: 1.361e-05]
	Learning Rate: 1.36096e-05
	LOSS [training: 0.016768524368208464 | validation: 0.020504100926477843]
	TIME [epoch: 6.29 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016811226334458397		[learning rate: 1.3514e-05]
	Learning Rate: 1.35135e-05
	LOSS [training: 0.016811226334458397 | validation: 0.019357400315190367]
	TIME [epoch: 6.28 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017285328787749		[learning rate: 1.3418e-05]
	Learning Rate: 1.34181e-05
	LOSS [training: 0.017285328787749 | validation: 0.019525942265535945]
	TIME [epoch: 6.31 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016327906552595476		[learning rate: 1.3323e-05]
	Learning Rate: 1.33234e-05
	LOSS [training: 0.016327906552595476 | validation: 0.019143911123630692]
	TIME [epoch: 6.29 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01684644237929883		[learning rate: 1.3229e-05]
	Learning Rate: 1.32294e-05
	LOSS [training: 0.01684644237929883 | validation: 0.018600993603008376]
	TIME [epoch: 6.3 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017382194233188042		[learning rate: 1.3136e-05]
	Learning Rate: 1.3136e-05
	LOSS [training: 0.017382194233188042 | validation: 0.018116235853315944]
	TIME [epoch: 6.32 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01659112306159926		[learning rate: 1.3043e-05]
	Learning Rate: 1.30432e-05
	LOSS [training: 0.01659112306159926 | validation: 0.019652435300058203]
	TIME [epoch: 6.29 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01671584797456284		[learning rate: 1.2951e-05]
	Learning Rate: 1.29511e-05
	LOSS [training: 0.01671584797456284 | validation: 0.020265296072185992]
	TIME [epoch: 6.29 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016632609500586884		[learning rate: 1.286e-05]
	Learning Rate: 1.28597e-05
	LOSS [training: 0.016632609500586884 | validation: 0.023960465740794374]
	TIME [epoch: 6.29 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01651576784028124		[learning rate: 1.2769e-05]
	Learning Rate: 1.27689e-05
	LOSS [training: 0.01651576784028124 | validation: 0.020800846591031954]
	TIME [epoch: 6.29 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016840770798259797		[learning rate: 1.2679e-05]
	Learning Rate: 1.26788e-05
	LOSS [training: 0.016840770798259797 | validation: 0.020105188364643818]
	TIME [epoch: 6.3 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017290814006856357		[learning rate: 1.2589e-05]
	Learning Rate: 1.25893e-05
	LOSS [training: 0.017290814006856357 | validation: 0.020381859782856362]
	TIME [epoch: 6.32 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016384637449918138		[learning rate: 1.25e-05]
	Learning Rate: 1.25004e-05
	LOSS [training: 0.016384637449918138 | validation: 0.019291643580517045]
	TIME [epoch: 6.33 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017083974239016926		[learning rate: 1.2412e-05]
	Learning Rate: 1.24121e-05
	LOSS [training: 0.017083974239016926 | validation: 0.018604751994089952]
	TIME [epoch: 6.32 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01632022253097007		[learning rate: 1.2325e-05]
	Learning Rate: 1.23245e-05
	LOSS [training: 0.01632022253097007 | validation: 0.01907749116932187]
	TIME [epoch: 6.29 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016588530258903333		[learning rate: 1.2237e-05]
	Learning Rate: 1.22375e-05
	LOSS [training: 0.016588530258903333 | validation: 0.02008375014875304]
	TIME [epoch: 6.29 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01631937279227943		[learning rate: 1.2151e-05]
	Learning Rate: 1.21511e-05
	LOSS [training: 0.01631937279227943 | validation: 0.022233988052995465]
	TIME [epoch: 6.29 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01609445270467247		[learning rate: 1.2065e-05]
	Learning Rate: 1.20653e-05
	LOSS [training: 0.01609445270467247 | validation: 0.020194257743436785]
	TIME [epoch: 6.32 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017242617452439914		[learning rate: 1.198e-05]
	Learning Rate: 1.19801e-05
	LOSS [training: 0.017242617452439914 | validation: 0.01840260275701667]
	TIME [epoch: 6.32 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016526683541426834		[learning rate: 1.1896e-05]
	Learning Rate: 1.18956e-05
	LOSS [training: 0.016526683541426834 | validation: 0.019957691168104072]
	TIME [epoch: 6.29 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016664331579152723		[learning rate: 1.1812e-05]
	Learning Rate: 1.18116e-05
	LOSS [training: 0.016664331579152723 | validation: 0.019368935188545097]
	TIME [epoch: 6.3 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017113208051127385		[learning rate: 1.1728e-05]
	Learning Rate: 1.17282e-05
	LOSS [training: 0.017113208051127385 | validation: 0.020006968945872255]
	TIME [epoch: 6.31 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016687631914510684		[learning rate: 1.1645e-05]
	Learning Rate: 1.16454e-05
	LOSS [training: 0.016687631914510684 | validation: 0.020712469946203085]
	TIME [epoch: 6.29 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01656747815216347		[learning rate: 1.1563e-05]
	Learning Rate: 1.15632e-05
	LOSS [training: 0.01656747815216347 | validation: 0.02277789506707195]
	TIME [epoch: 6.31 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01603232256255886		[learning rate: 1.1482e-05]
	Learning Rate: 1.14815e-05
	LOSS [training: 0.01603232256255886 | validation: 0.01783105303845339]
	TIME [epoch: 6.33 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016631278256207083		[learning rate: 1.14e-05]
	Learning Rate: 1.14005e-05
	LOSS [training: 0.016631278256207083 | validation: 0.02106537635033074]
	TIME [epoch: 6.3 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01779467753276056		[learning rate: 1.132e-05]
	Learning Rate: 1.132e-05
	LOSS [training: 0.01779467753276056 | validation: 0.019365137039648934]
	TIME [epoch: 6.32 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016650633094142525		[learning rate: 1.124e-05]
	Learning Rate: 1.12401e-05
	LOSS [training: 0.016650633094142525 | validation: 0.019934100625573794]
	TIME [epoch: 6.28 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01673165530694153		[learning rate: 1.1161e-05]
	Learning Rate: 1.11607e-05
	LOSS [training: 0.01673165530694153 | validation: 0.020534017408282514]
	TIME [epoch: 6.3 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01709689250256812		[learning rate: 1.1082e-05]
	Learning Rate: 1.10819e-05
	LOSS [training: 0.01709689250256812 | validation: 0.020967333393072402]
	TIME [epoch: 6.31 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01678624580297723		[learning rate: 1.1004e-05]
	Learning Rate: 1.10037e-05
	LOSS [training: 0.01678624580297723 | validation: 0.01858472136204937]
	TIME [epoch: 6.32 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017483802783988884		[learning rate: 1.0926e-05]
	Learning Rate: 1.0926e-05
	LOSS [training: 0.017483802783988884 | validation: 0.020291364819010893]
	TIME [epoch: 6.31 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01715182664806755		[learning rate: 1.0849e-05]
	Learning Rate: 1.08489e-05
	LOSS [training: 0.01715182664806755 | validation: 0.0184729019631108]
	TIME [epoch: 6.28 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016942547370535074		[learning rate: 1.0772e-05]
	Learning Rate: 1.07723e-05
	LOSS [training: 0.016942547370535074 | validation: 0.02109220199595607]
	TIME [epoch: 6.29 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01742970450103841		[learning rate: 1.0696e-05]
	Learning Rate: 1.06962e-05
	LOSS [training: 0.01742970450103841 | validation: 0.021471389781159908]
	TIME [epoch: 6.32 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015870375091478296		[learning rate: 1.0621e-05]
	Learning Rate: 1.06207e-05
	LOSS [training: 0.015870375091478296 | validation: 0.019742683340938635]
	TIME [epoch: 6.31 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017070388382720632		[learning rate: 1.0546e-05]
	Learning Rate: 1.05457e-05
	LOSS [training: 0.017070388382720632 | validation: 0.018309159642822988]
	TIME [epoch: 6.31 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016830779210276745		[learning rate: 1.0471e-05]
	Learning Rate: 1.04713e-05
	LOSS [training: 0.016830779210276745 | validation: 0.020783532112778426]
	TIME [epoch: 6.3 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016719945935435486		[learning rate: 1.0397e-05]
	Learning Rate: 1.03974e-05
	LOSS [training: 0.016719945935435486 | validation: 0.0223598634840198]
	TIME [epoch: 6.29 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016268312772383754		[learning rate: 1.0324e-05]
	Learning Rate: 1.0324e-05
	LOSS [training: 0.016268312772383754 | validation: 0.02007796487188583]
	TIME [epoch: 6.3 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015708053730002787		[learning rate: 1.0251e-05]
	Learning Rate: 1.02511e-05
	LOSS [training: 0.015708053730002787 | validation: 0.019978716175075174]
	TIME [epoch: 6.29 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01687368930741851		[learning rate: 1.0179e-05]
	Learning Rate: 1.01787e-05
	LOSS [training: 0.01687368930741851 | validation: 0.018951400262816376]
	TIME [epoch: 6.3 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016864650268132504		[learning rate: 1.0107e-05]
	Learning Rate: 1.01068e-05
	LOSS [training: 0.016864650268132504 | validation: 0.018885095632924398]
	TIME [epoch: 6.28 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016745104842947008		[learning rate: 1.0035e-05]
	Learning Rate: 1.00355e-05
	LOSS [training: 0.016745104842947008 | validation: 0.020582040839371638]
	TIME [epoch: 6.34 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016992278545001092		[learning rate: 9.9646e-06]
	Learning Rate: 9.96464e-06
	LOSS [training: 0.016992278545001092 | validation: 0.020046854415167504]
	TIME [epoch: 115 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01676945022902223		[learning rate: 9.8943e-06]
	Learning Rate: 9.89429e-06
	LOSS [training: 0.01676945022902223 | validation: 0.020577928419808128]
	TIME [epoch: 13.7 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015199929577973614		[learning rate: 9.8244e-06]
	Learning Rate: 9.82444e-06
	LOSS [training: 0.015199929577973614 | validation: 0.02039806788290266]
	TIME [epoch: 13.7 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01614365669458228		[learning rate: 9.7551e-06]
	Learning Rate: 9.75508e-06
	LOSS [training: 0.01614365669458228 | validation: 0.019631732882074916]
	TIME [epoch: 13.8 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.015471181416159285		[learning rate: 9.6862e-06]
	Learning Rate: 9.68621e-06
	LOSS [training: 0.015471181416159285 | validation: 0.0205399088759982]
	TIME [epoch: 13.7 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01573150590322408		[learning rate: 9.6178e-06]
	Learning Rate: 9.61783e-06
	LOSS [training: 0.01573150590322408 | validation: 0.0196779082558846]
	TIME [epoch: 13.7 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016055834789250073		[learning rate: 9.5499e-06]
	Learning Rate: 9.54993e-06
	LOSS [training: 0.016055834789250073 | validation: 0.02158720663388168]
	TIME [epoch: 13.8 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.01676583926076415		[learning rate: 9.4825e-06]
	Learning Rate: 9.48251e-06
	LOSS [training: 0.01676583926076415 | validation: 0.019940936067864398]
	TIME [epoch: 13.8 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016338364780355115		[learning rate: 9.4156e-06]
	Learning Rate: 9.41556e-06
	LOSS [training: 0.016338364780355115 | validation: 0.02127363261546282]
	TIME [epoch: 13.7 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016333677614472956		[learning rate: 9.3491e-06]
	Learning Rate: 9.34909e-06
	LOSS [training: 0.016333677614472956 | validation: 0.020599902913957188]
	TIME [epoch: 13.7 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017090380741737597		[learning rate: 9.2831e-06]
	Learning Rate: 9.28308e-06
	LOSS [training: 0.017090380741737597 | validation: 0.02124267778030453]
	TIME [epoch: 13.7 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016648336097911914		[learning rate: 9.2176e-06]
	Learning Rate: 9.21755e-06
	LOSS [training: 0.016648336097911914 | validation: 0.020235232375294118]
	TIME [epoch: 13.7 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.017821934272833195		[learning rate: 9.1525e-06]
	Learning Rate: 9.15248e-06
	LOSS [training: 0.017821934272833195 | validation: 0.01933042380230597]
	TIME [epoch: 13.8 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016709704157448373		[learning rate: 9.0879e-06]
	Learning Rate: 9.08786e-06
	LOSS [training: 0.016709704157448373 | validation: 0.020376139173529907]
	TIME [epoch: 13.7 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016026482288438445		[learning rate: 9.0237e-06]
	Learning Rate: 9.0237e-06
	LOSS [training: 0.016026482288438445 | validation: 0.021284494076673946]
	TIME [epoch: 13.8 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016260720174808882		[learning rate: 8.96e-06]
	Learning Rate: 8.96e-06
	LOSS [training: 0.016260720174808882 | validation: 0.019636414261963837]
	TIME [epoch: 13.8 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016489919992077075		[learning rate: 8.8967e-06]
	Learning Rate: 8.89674e-06
	LOSS [training: 0.016489919992077075 | validation: 0.020074577593434107]
	TIME [epoch: 13.8 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016244860915166758		[learning rate: 8.8339e-06]
	Learning Rate: 8.83393e-06
	LOSS [training: 0.016244860915166758 | validation: 0.020898672208568666]
	TIME [epoch: 13.7 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.016353733726333808		[learning rate: 8.7716e-06]
	Learning Rate: 8.77157e-06
	LOSS [training: 0.016353733726333808 | validation: 0.020454397000163046]
	TIME [epoch: 13.8 sec]
	Saving model to: out/model_training/model_phi1_2a_v_mmd1_20240813_194433/states/model_phi1_2a_v_mmd1_1019.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 5234.069 seconds.
