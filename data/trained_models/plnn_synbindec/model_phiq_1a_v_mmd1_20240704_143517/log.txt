Args:
Namespace(name='model_phiq_1a_v_mmd1', outdir='out/model_training/model_phiq_1a_v_mmd1', training_data='data/training_data/data_phiq_1a/training', validation_data='data/training_data/data_phiq_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=1000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[100, 250, 500], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.01, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1026348517

Training model...

Saving initial model state to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_0.pth
EPOCH 1/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.855971431161102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.855971431161102 | validation: 4.947540220702631]
	TIME [epoch: 98 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.756850774513587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.756850774513587 | validation: 4.737380247719876]
	TIME [epoch: 8.23 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.606438638090391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.606438638090391 | validation: 4.773024909088908]
	TIME [epoch: 8.11 sec]
EPOCH 4/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.534097704022947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.534097704022947 | validation: 4.759304298784096]
	TIME [epoch: 8.1 sec]
EPOCH 5/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.377045288521573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.377045288521573 | validation: 4.568148421338728]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.2772534044257435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2772534044257435 | validation: 4.37380763514888]
	TIME [epoch: 8.13 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/1000:
	Training over batches...
		[batch 4/4] avg loss: 4.2165822198624605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2165822198624605 | validation: 3.9815669297688627]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.9475814640239992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9475814640239992 | validation: 3.868578273057101]
	TIME [epoch: 8.16 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.6206748236591877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6206748236591877 | validation: 3.56081971343032]
	TIME [epoch: 8.12 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.3882633915067633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3882633915067633 | validation: 3.471147921775229]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.2628058609928607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2628058609928607 | validation: 3.27924181141711]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/1000:
	Training over batches...
		[batch 4/4] avg loss: 3.0060193786415685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0060193786415685 | validation: 3.0606104543129673]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.830618407570955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.830618407570955 | validation: 2.727117462763781]
	TIME [epoch: 8.15 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.589051471258943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.589051471258943 | validation: 2.5762321416212663]
	TIME [epoch: 8.13 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.472625825485412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.472625825485412 | validation: 2.508599659257623]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.271511652150512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.271511652150512 | validation: 2.3123494431110654]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.1464477644527538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1464477644527538 | validation: 2.2225207367431175]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.0895592223290147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0895592223290147 | validation: 1.9412885787770988]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/1000:
	Training over batches...
		[batch 4/4] avg loss: 2.027588594476741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.027588594476741 | validation: 2.0220411259827986]
	TIME [epoch: 8.15 sec]
EPOCH 20/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.986998927877516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.986998927877516 | validation: 1.7936209275399402]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7718259904992684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7718259904992684 | validation: 1.7897771175696149]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.8581449355272883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8581449355272883 | validation: 1.7690305928292216]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7721266097805888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7721266097805888 | validation: 1.710723260785576]
	TIME [epoch: 8.12 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6818703236197248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6818703236197248 | validation: 1.721568727757976]
	TIME [epoch: 8.16 sec]
EPOCH 25/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6870374863532742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6870374863532742 | validation: 1.62275925397735]
	TIME [epoch: 8.12 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.7097622200954925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7097622200954925 | validation: 1.639206852885243]
	TIME [epoch: 8.1 sec]
EPOCH 27/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6111669571602412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6111669571602412 | validation: 1.594171956954697]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6121745257280682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6121745257280682 | validation: 1.5908247285003774]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5592216193135378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5592216193135378 | validation: 1.7521810338164618]
	TIME [epoch: 8.13 sec]
EPOCH 30/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.6380253484154725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6380253484154725 | validation: 1.5681184542511004]
	TIME [epoch: 8.15 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5381920204943293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5381920204943293 | validation: 1.5642067318640862]
	TIME [epoch: 8.13 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.5263066510669678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5263066510669678 | validation: 1.5047876560634355]
	TIME [epoch: 8.14 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.502840201744732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.502840201744732 | validation: 1.486908224149413]
	TIME [epoch: 8.13 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4723251941774609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4723251941774609 | validation: 1.4914174312459432]
	TIME [epoch: 8.12 sec]
EPOCH 35/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.462029360287705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.462029360287705 | validation: 1.5259912045211927]
	TIME [epoch: 8.16 sec]
EPOCH 36/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4646299063207229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4646299063207229 | validation: 1.4519161166416015]
	TIME [epoch: 8.12 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.423703330951156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.423703330951156 | validation: 1.4368770892007854]
	TIME [epoch: 8.12 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.4231387847453676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4231387847453676 | validation: 1.4153206661144542]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.413452141643857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.413452141643857 | validation: 1.4047236437577086]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3933566309722911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3933566309722911 | validation: 1.4142002883808773]
	TIME [epoch: 8.14 sec]
EPOCH 41/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.380991754797793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.380991754797793 | validation: 1.3946274643846084]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.381239005291419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.381239005291419 | validation: 1.3732977630520178]
	TIME [epoch: 8.13 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3585191438511235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3585191438511235 | validation: 1.3813839762504094]
	TIME [epoch: 8.1 sec]
EPOCH 44/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3640497927055855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3640497927055855 | validation: 1.3614180054397784]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.34044392656752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.34044392656752 | validation: 1.3440235844145534]
	TIME [epoch: 8.13 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3348621215084886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3348621215084886 | validation: 1.3351133076542]
	TIME [epoch: 8.16 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3271425885721286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3271425885721286 | validation: 1.3244144548941386]
	TIME [epoch: 8.12 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.319541524648043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.319541524648043 | validation: 1.3496389061127192]
	TIME [epoch: 8.12 sec]
EPOCH 49/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3141569543531066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3141569543531066 | validation: 1.3183458693877776]
	TIME [epoch: 8.13 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.3044654696565168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3044654696565168 | validation: 1.303904658862197]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2982328672110504		[learning rate: 0.0099456]
	Learning Rate: 0.00994561
	LOSS [training: 1.2982328672110504 | validation: 1.2955670899611111]
	TIME [epoch: 8.14 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2896166707148913		[learning rate: 0.0098736]
	Learning Rate: 0.00987356
	LOSS [training: 1.2896166707148913 | validation: 1.2996091432621153]
	TIME [epoch: 8.11 sec]
EPOCH 53/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2837677798882525		[learning rate: 0.009802]
	Learning Rate: 0.00980202
	LOSS [training: 1.2837677798882525 | validation: 1.2973110296219992]
	TIME [epoch: 8.1 sec]
EPOCH 54/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2871747552504882		[learning rate: 0.009731]
	Learning Rate: 0.00973101
	LOSS [training: 1.2871747552504882 | validation: 1.2725099576354728]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.266453933851483		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 1.266453933851483 | validation: 1.2592727601992293]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.264356809955908		[learning rate: 0.0095905]
	Learning Rate: 0.00959052
	LOSS [training: 1.264356809955908 | validation: 1.2486100841821322]
	TIME [epoch: 8.13 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.262916119635343		[learning rate: 0.009521]
	Learning Rate: 0.00952104
	LOSS [training: 1.262916119635343 | validation: 1.2995984614923204]
	TIME [epoch: 8.16 sec]
EPOCH 58/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2710254039753859		[learning rate: 0.0094521]
	Learning Rate: 0.00945206
	LOSS [training: 1.2710254039753859 | validation: 1.2578819031404398]
	TIME [epoch: 8.11 sec]
EPOCH 59/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2413288425503972		[learning rate: 0.0093836]
	Learning Rate: 0.00938358
	LOSS [training: 1.2413288425503972 | validation: 1.236783067809108]
	TIME [epoch: 8.12 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2206385409749745		[learning rate: 0.0093156]
	Learning Rate: 0.00931559
	LOSS [training: 1.2206385409749745 | validation: 1.250901313729596]
	TIME [epoch: 8.12 sec]
EPOCH 61/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2398465689474902		[learning rate: 0.0092481]
	Learning Rate: 0.0092481
	LOSS [training: 1.2398465689474902 | validation: 1.2269426482694874]
	TIME [epoch: 8.12 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2192759502572414		[learning rate: 0.0091811]
	Learning Rate: 0.0091811
	LOSS [training: 1.2192759502572414 | validation: 1.2258428778773829]
	TIME [epoch: 8.15 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2531283075412694		[learning rate: 0.0091146]
	Learning Rate: 0.00911458
	LOSS [training: 1.2531283075412694 | validation: 1.216894341515661]
	TIME [epoch: 8.12 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1967752451235318		[learning rate: 0.0090485]
	Learning Rate: 0.00904855
	LOSS [training: 1.1967752451235318 | validation: 1.1947229895854052]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1755186897305645		[learning rate: 0.008983]
	Learning Rate: 0.00898299
	LOSS [training: 1.1755186897305645 | validation: 1.2788460939133]
	TIME [epoch: 8.1 sec]
EPOCH 66/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2397043555791263		[learning rate: 0.0089179]
	Learning Rate: 0.00891791
	LOSS [training: 1.2397043555791263 | validation: 1.2069887422102976]
	TIME [epoch: 8.1 sec]
EPOCH 67/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1735801284240734		[learning rate: 0.0088533]
	Learning Rate: 0.0088533
	LOSS [training: 1.1735801284240734 | validation: 1.1754670241366654]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1749924561332143		[learning rate: 0.0087892]
	Learning Rate: 0.00878916
	LOSS [training: 1.1749924561332143 | validation: 1.4587655338752086]
	TIME [epoch: 8.17 sec]
EPOCH 69/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2686213235556445		[learning rate: 0.0087255]
	Learning Rate: 0.00872548
	LOSS [training: 1.2686213235556445 | validation: 1.139244815166841]
	TIME [epoch: 8.12 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1816033964837036		[learning rate: 0.0086623]
	Learning Rate: 0.00866227
	LOSS [training: 1.1816033964837036 | validation: 1.1615868181919355]
	TIME [epoch: 8.12 sec]
EPOCH 71/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1589224483155693		[learning rate: 0.0085995]
	Learning Rate: 0.00859951
	LOSS [training: 1.1589224483155693 | validation: 1.1646957450523097]
	TIME [epoch: 8.11 sec]
EPOCH 72/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1403426206918794		[learning rate: 0.0085372]
	Learning Rate: 0.00853721
	LOSS [training: 1.1403426206918794 | validation: 1.166010599573751]
	TIME [epoch: 8.11 sec]
EPOCH 73/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.2156245840626712		[learning rate: 0.0084754]
	Learning Rate: 0.00847535
	LOSS [training: 1.2156245840626712 | validation: 1.213227206390894]
	TIME [epoch: 8.16 sec]
EPOCH 74/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.17446277342862		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 1.17446277342862 | validation: 1.2021775485037407]
	TIME [epoch: 8.12 sec]
EPOCH 75/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1480334840286246		[learning rate: 0.008353]
	Learning Rate: 0.00835299
	LOSS [training: 1.1480334840286246 | validation: 1.1165149050925347]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1987516811935075		[learning rate: 0.0082925]
	Learning Rate: 0.00829248
	LOSS [training: 1.1987516811935075 | validation: 1.1681711916753619]
	TIME [epoch: 8.12 sec]
EPOCH 77/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1402715196888233		[learning rate: 0.0082324]
	Learning Rate: 0.0082324
	LOSS [training: 1.1402715196888233 | validation: 1.0766412430300591]
	TIME [epoch: 8.11 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1495017451261809		[learning rate: 0.0081728]
	Learning Rate: 0.00817275
	LOSS [training: 1.1495017451261809 | validation: 1.1670846669598816]
	TIME [epoch: 8.12 sec]
EPOCH 79/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1272789404347188		[learning rate: 0.0081135]
	Learning Rate: 0.00811354
	LOSS [training: 1.1272789404347188 | validation: 1.1091546536449246]
	TIME [epoch: 8.14 sec]
EPOCH 80/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1079841989255745		[learning rate: 0.0080548]
	Learning Rate: 0.00805476
	LOSS [training: 1.1079841989255745 | validation: 1.0854907442863093]
	TIME [epoch: 8.11 sec]
EPOCH 81/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1370454192133912		[learning rate: 0.0079964]
	Learning Rate: 0.0079964
	LOSS [training: 1.1370454192133912 | validation: 1.1175490037218565]
	TIME [epoch: 8.1 sec]
EPOCH 82/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1151216838964682		[learning rate: 0.0079385]
	Learning Rate: 0.00793847
	LOSS [training: 1.1151216838964682 | validation: 1.1021564911424835]
	TIME [epoch: 8.11 sec]
EPOCH 83/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0967217123358994		[learning rate: 0.007881]
	Learning Rate: 0.00788096
	LOSS [training: 1.0967217123358994 | validation: 1.1183483334600204]
	TIME [epoch: 8.1 sec]
EPOCH 84/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1096024751344182		[learning rate: 0.0078239]
	Learning Rate: 0.00782386
	LOSS [training: 1.1096024751344182 | validation: 1.0320570545429955]
	TIME [epoch: 8.13 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0592395527845804		[learning rate: 0.0077672]
	Learning Rate: 0.00776718
	LOSS [training: 1.0592395527845804 | validation: 1.0994010595105905]
	TIME [epoch: 8.12 sec]
EPOCH 86/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.13536074173135		[learning rate: 0.0077109]
	Learning Rate: 0.0077109
	LOSS [training: 1.13536074173135 | validation: 1.095837594916885]
	TIME [epoch: 8.09 sec]
EPOCH 87/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.075037966250863		[learning rate: 0.007655]
	Learning Rate: 0.00765504
	LOSS [training: 1.075037966250863 | validation: 1.2994317268868274]
	TIME [epoch: 8.1 sec]
EPOCH 88/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.1577450285517359		[learning rate: 0.0075996]
	Learning Rate: 0.00759958
	LOSS [training: 1.1577450285517359 | validation: 1.0213488356014415]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.06891543660733		[learning rate: 0.0075445]
	Learning Rate: 0.00754452
	LOSS [training: 1.06891543660733 | validation: 1.0357089828117683]
	TIME [epoch: 8.1 sec]
EPOCH 90/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0535056898154882		[learning rate: 0.0074899]
	Learning Rate: 0.00748986
	LOSS [training: 1.0535056898154882 | validation: 1.0638447475600614]
	TIME [epoch: 8.14 sec]
EPOCH 91/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0663988760667347		[learning rate: 0.0074356]
	Learning Rate: 0.0074356
	LOSS [training: 1.0663988760667347 | validation: 1.0211657017348124]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_91.pth
	Model improved!!!
EPOCH 92/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0824633565791886		[learning rate: 0.0073817]
	Learning Rate: 0.00738173
	LOSS [training: 1.0824633565791886 | validation: 0.9958580461885547]
	TIME [epoch: 8.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0223454075904683		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 1.0223454075904683 | validation: 1.0140571695407337]
	TIME [epoch: 8.1 sec]
EPOCH 94/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0241006981879244		[learning rate: 0.0072752]
	Learning Rate: 0.00727515
	LOSS [training: 1.0241006981879244 | validation: 1.1166059944810116]
	TIME [epoch: 8.1 sec]
EPOCH 95/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0495676058102408		[learning rate: 0.0072224]
	Learning Rate: 0.00722244
	LOSS [training: 1.0495676058102408 | validation: 1.0797454894828702]
	TIME [epoch: 8.11 sec]
EPOCH 96/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.085648290826394		[learning rate: 0.0071701]
	Learning Rate: 0.00717012
	LOSS [training: 1.085648290826394 | validation: 1.1488300827855769]
	TIME [epoch: 8.14 sec]
EPOCH 97/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0896315926974236		[learning rate: 0.0071182]
	Learning Rate: 0.00711817
	LOSS [training: 1.0896315926974236 | validation: 1.036612984011772]
	TIME [epoch: 8.1 sec]
EPOCH 98/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0514017302503937		[learning rate: 0.0070666]
	Learning Rate: 0.0070666
	LOSS [training: 1.0514017302503937 | validation: 1.0220629593562576]
	TIME [epoch: 8.1 sec]
EPOCH 99/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0027447355344583		[learning rate: 0.0070154]
	Learning Rate: 0.0070154
	LOSS [training: 1.0027447355344583 | validation: 0.9339351125246587]
	TIME [epoch: 8.09 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_99.pth
	Model improved!!!
EPOCH 100/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9820236357128909		[learning rate: 0.0069646]
	Learning Rate: 0.00696458
	LOSS [training: 0.9820236357128909 | validation: 0.9645368692146039]
	TIME [epoch: 8.12 sec]
EPOCH 101/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0206633741626585		[learning rate: 0.0069141]
	Learning Rate: 0.00691412
	LOSS [training: 1.0206633741626585 | validation: 0.9979883438014181]
	TIME [epoch: 106 sec]
EPOCH 102/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0656649326035605		[learning rate: 0.006864]
	Learning Rate: 0.00686403
	LOSS [training: 1.0656649326035605 | validation: 1.000030042989402]
	TIME [epoch: 16.1 sec]
EPOCH 103/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9889049889601478		[learning rate: 0.0068143]
	Learning Rate: 0.0068143
	LOSS [training: 0.9889049889601478 | validation: 1.208569427731701]
	TIME [epoch: 16.1 sec]
EPOCH 104/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0374948587702566		[learning rate: 0.0067649]
	Learning Rate: 0.00676493
	LOSS [training: 1.0374948587702566 | validation: 0.9376788598435137]
	TIME [epoch: 16 sec]
EPOCH 105/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9822779599402208		[learning rate: 0.0067159]
	Learning Rate: 0.00671592
	LOSS [training: 0.9822779599402208 | validation: 0.9545358828182464]
	TIME [epoch: 16 sec]
EPOCH 106/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9837155650991146		[learning rate: 0.0066673]
	Learning Rate: 0.00666726
	LOSS [training: 0.9837155650991146 | validation: 0.9403663529372213]
	TIME [epoch: 16.1 sec]
EPOCH 107/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.99296014930605		[learning rate: 0.006619]
	Learning Rate: 0.00661896
	LOSS [training: 0.99296014930605 | validation: 0.9344389157492194]
	TIME [epoch: 16 sec]
EPOCH 108/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9664912389833141		[learning rate: 0.006571]
	Learning Rate: 0.006571
	LOSS [training: 0.9664912389833141 | validation: 0.8811478239714108]
	TIME [epoch: 16 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_108.pth
	Model improved!!!
EPOCH 109/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9463384543699513		[learning rate: 0.0065234]
	Learning Rate: 0.00652339
	LOSS [training: 0.9463384543699513 | validation: 0.9359266372572754]
	TIME [epoch: 16.1 sec]
EPOCH 110/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9545210252552923		[learning rate: 0.0064761]
	Learning Rate: 0.00647613
	LOSS [training: 0.9545210252552923 | validation: 0.8913568394152046]
	TIME [epoch: 16 sec]
EPOCH 111/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9410094650523809		[learning rate: 0.0064292]
	Learning Rate: 0.00642921
	LOSS [training: 0.9410094650523809 | validation: 1.0045111690941495]
	TIME [epoch: 16 sec]
EPOCH 112/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9554395410081301		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.9554395410081301 | validation: 0.854179632150564]
	TIME [epoch: 16.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_112.pth
	Model improved!!!
EPOCH 113/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9013725406246679		[learning rate: 0.0063364]
	Learning Rate: 0.00633639
	LOSS [training: 0.9013725406246679 | validation: 0.9652125777293539]
	TIME [epoch: 16 sec]
EPOCH 114/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8996548983096554		[learning rate: 0.0062905]
	Learning Rate: 0.00629049
	LOSS [training: 0.8996548983096554 | validation: 0.912545488512541]
	TIME [epoch: 16 sec]
EPOCH 115/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0391608953762312		[learning rate: 0.0062449]
	Learning Rate: 0.00624491
	LOSS [training: 1.0391608953762312 | validation: 1.0315009135608602]
	TIME [epoch: 16.1 sec]
EPOCH 116/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.967892178056236		[learning rate: 0.0061997]
	Learning Rate: 0.00619967
	LOSS [training: 0.967892178056236 | validation: 0.9677078410840163]
	TIME [epoch: 16 sec]
EPOCH 117/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9603845371427097		[learning rate: 0.0061548]
	Learning Rate: 0.00615475
	LOSS [training: 0.9603845371427097 | validation: 0.9801495291930966]
	TIME [epoch: 16 sec]
EPOCH 118/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8763677771513648		[learning rate: 0.0061102]
	Learning Rate: 0.00611016
	LOSS [training: 0.8763677771513648 | validation: 0.8378825523401112]
	TIME [epoch: 16.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_118.pth
	Model improved!!!
EPOCH 119/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9789208974697885		[learning rate: 0.0060659]
	Learning Rate: 0.00606589
	LOSS [training: 0.9789208974697885 | validation: 1.0472780066473961]
	TIME [epoch: 16 sec]
EPOCH 120/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.032036698570418		[learning rate: 0.0060219]
	Learning Rate: 0.00602195
	LOSS [training: 1.032036698570418 | validation: 0.9522674687824977]
	TIME [epoch: 16 sec]
EPOCH 121/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9041606296448359		[learning rate: 0.0059783]
	Learning Rate: 0.00597832
	LOSS [training: 0.9041606296448359 | validation: 0.9114488408267162]
	TIME [epoch: 16.1 sec]
EPOCH 122/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8819362710452667		[learning rate: 0.005935]
	Learning Rate: 0.005935
	LOSS [training: 0.8819362710452667 | validation: 1.0358562228625454]
	TIME [epoch: 16 sec]
EPOCH 123/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9000183083862182		[learning rate: 0.005892]
	Learning Rate: 0.00589201
	LOSS [training: 0.9000183083862182 | validation: 1.0288186900141687]
	TIME [epoch: 16 sec]
EPOCH 124/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8990588380923491		[learning rate: 0.0058493]
	Learning Rate: 0.00584932
	LOSS [training: 0.8990588380923491 | validation: 0.8686907078284388]
	TIME [epoch: 16.1 sec]
EPOCH 125/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9376794532361246		[learning rate: 0.0058069]
	Learning Rate: 0.00580694
	LOSS [training: 0.9376794532361246 | validation: 0.9242102271121762]
	TIME [epoch: 16 sec]
EPOCH 126/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8427740172153937		[learning rate: 0.0057649]
	Learning Rate: 0.00576487
	LOSS [training: 0.8427740172153937 | validation: 0.8709281046912445]
	TIME [epoch: 16 sec]
EPOCH 127/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8655776582136896		[learning rate: 0.0057231]
	Learning Rate: 0.0057231
	LOSS [training: 0.8655776582136896 | validation: 1.0069734333911105]
	TIME [epoch: 16.1 sec]
EPOCH 128/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9178304142553901		[learning rate: 0.0056816]
	Learning Rate: 0.00568164
	LOSS [training: 0.9178304142553901 | validation: 0.995208699270848]
	TIME [epoch: 16 sec]
EPOCH 129/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.915527317858456		[learning rate: 0.0056405]
	Learning Rate: 0.00564048
	LOSS [training: 0.915527317858456 | validation: 0.9809674020099837]
	TIME [epoch: 16 sec]
EPOCH 130/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9158874574879942		[learning rate: 0.0055996]
	Learning Rate: 0.00559961
	LOSS [training: 0.9158874574879942 | validation: 0.9135561413485792]
	TIME [epoch: 16.1 sec]
EPOCH 131/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.838844349037082		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.838844349037082 | validation: 0.8531701725817309]
	TIME [epoch: 16 sec]
EPOCH 132/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8758660550336292		[learning rate: 0.0055188]
	Learning Rate: 0.00551877
	LOSS [training: 0.8758660550336292 | validation: 0.8725118595925516]
	TIME [epoch: 16 sec]
EPOCH 133/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8416759513475254		[learning rate: 0.0054788]
	Learning Rate: 0.00547878
	LOSS [training: 0.8416759513475254 | validation: 0.9667434534976562]
	TIME [epoch: 16.1 sec]
EPOCH 134/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8584125337119393		[learning rate: 0.0054391]
	Learning Rate: 0.00543909
	LOSS [training: 0.8584125337119393 | validation: 0.7959655510575034]
	TIME [epoch: 16 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_134.pth
	Model improved!!!
EPOCH 135/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8981275183588411		[learning rate: 0.0053997]
	Learning Rate: 0.00539968
	LOSS [training: 0.8981275183588411 | validation: 0.8352464096054917]
	TIME [epoch: 16 sec]
EPOCH 136/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8468615750610657		[learning rate: 0.0053606]
	Learning Rate: 0.00536056
	LOSS [training: 0.8468615750610657 | validation: 0.8968009574530404]
	TIME [epoch: 16.1 sec]
EPOCH 137/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8979267576060533		[learning rate: 0.0053217]
	Learning Rate: 0.00532173
	LOSS [training: 0.8979267576060533 | validation: 0.8904190193831998]
	TIME [epoch: 16 sec]
EPOCH 138/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8665193652594942		[learning rate: 0.0052832]
	Learning Rate: 0.00528317
	LOSS [training: 0.8665193652594942 | validation: 0.7798715800211221]
	TIME [epoch: 16 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_138.pth
	Model improved!!!
EPOCH 139/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.792672259192378		[learning rate: 0.0052449]
	Learning Rate: 0.0052449
	LOSS [training: 0.792672259192378 | validation: 0.8337495083220434]
	TIME [epoch: 16.1 sec]
EPOCH 140/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7775015452293002		[learning rate: 0.0052069]
	Learning Rate: 0.0052069
	LOSS [training: 0.7775015452293002 | validation: 0.9444869838553553]
	TIME [epoch: 16 sec]
EPOCH 141/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.858417216848241		[learning rate: 0.0051692]
	Learning Rate: 0.00516917
	LOSS [training: 0.858417216848241 | validation: 0.9900084723086002]
	TIME [epoch: 16 sec]
EPOCH 142/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8800808444499572		[learning rate: 0.0051317]
	Learning Rate: 0.00513172
	LOSS [training: 0.8800808444499572 | validation: 0.9037557320887053]
	TIME [epoch: 16.1 sec]
EPOCH 143/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7814498152827453		[learning rate: 0.0050945]
	Learning Rate: 0.00509454
	LOSS [training: 0.7814498152827453 | validation: 0.9175164349373954]
	TIME [epoch: 16 sec]
EPOCH 144/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8563487370252978		[learning rate: 0.0050576]
	Learning Rate: 0.00505763
	LOSS [training: 0.8563487370252978 | validation: 0.9347526423388507]
	TIME [epoch: 16 sec]
EPOCH 145/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8093785448504179		[learning rate: 0.005021]
	Learning Rate: 0.00502099
	LOSS [training: 0.8093785448504179 | validation: 0.7193444968388636]
	TIME [epoch: 16.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_145.pth
	Model improved!!!
EPOCH 146/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7277127752522746		[learning rate: 0.0049846]
	Learning Rate: 0.00498461
	LOSS [training: 0.7277127752522746 | validation: 1.0019194487360625]
	TIME [epoch: 16 sec]
EPOCH 147/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8386931582529118		[learning rate: 0.0049485]
	Learning Rate: 0.0049485
	LOSS [training: 0.8386931582529118 | validation: 1.028668744124922]
	TIME [epoch: 16 sec]
EPOCH 148/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.903272485221521		[learning rate: 0.0049126]
	Learning Rate: 0.00491265
	LOSS [training: 0.903272485221521 | validation: 1.019385780407406]
	TIME [epoch: 16.1 sec]
EPOCH 149/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8945508716191859		[learning rate: 0.0048771]
	Learning Rate: 0.00487706
	LOSS [training: 0.8945508716191859 | validation: 0.8282900526452552]
	TIME [epoch: 16 sec]
EPOCH 150/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7567979858231242		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.7567979858231242 | validation: 0.8057697885647073]
	TIME [epoch: 16 sec]
EPOCH 151/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8247570595220093		[learning rate: 0.0048066]
	Learning Rate: 0.00480665
	LOSS [training: 0.8247570595220093 | validation: 1.1279184220802734]
	TIME [epoch: 16.1 sec]
EPOCH 152/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8425799883170111		[learning rate: 0.0047718]
	Learning Rate: 0.00477182
	LOSS [training: 0.8425799883170111 | validation: 0.8258948948830213]
	TIME [epoch: 16 sec]
EPOCH 153/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8086867797773679		[learning rate: 0.0047373]
	Learning Rate: 0.00473725
	LOSS [training: 0.8086867797773679 | validation: 0.8201749475740222]
	TIME [epoch: 16 sec]
EPOCH 154/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8171242469657927		[learning rate: 0.0047029]
	Learning Rate: 0.00470293
	LOSS [training: 0.8171242469657927 | validation: 0.9787165734757359]
	TIME [epoch: 16 sec]
EPOCH 155/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8135314851245657		[learning rate: 0.0046689]
	Learning Rate: 0.00466886
	LOSS [training: 0.8135314851245657 | validation: 1.0765621778796892]
	TIME [epoch: 16 sec]
EPOCH 156/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.871227545942671		[learning rate: 0.004635]
	Learning Rate: 0.00463503
	LOSS [training: 0.871227545942671 | validation: 0.7112998443415602]
	TIME [epoch: 16 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_156.pth
	Model improved!!!
EPOCH 157/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7593345806818044		[learning rate: 0.0046015]
	Learning Rate: 0.00460145
	LOSS [training: 0.7593345806818044 | validation: 0.9402688190218553]
	TIME [epoch: 16.1 sec]
EPOCH 158/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7446770193337111		[learning rate: 0.0045681]
	Learning Rate: 0.00456811
	LOSS [training: 0.7446770193337111 | validation: 0.88354369575053]
	TIME [epoch: 16 sec]
EPOCH 159/1000:
	Training over batches...
		[batch 4/4] avg loss: 1.0164287185194432		[learning rate: 0.004535]
	Learning Rate: 0.00453502
	LOSS [training: 1.0164287185194432 | validation: 0.9406192088679188]
	TIME [epoch: 18 sec]
EPOCH 160/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9981565240571477		[learning rate: 0.0045022]
	Learning Rate: 0.00450216
	LOSS [training: 0.9981565240571477 | validation: 0.8917984480519013]
	TIME [epoch: 17 sec]
EPOCH 161/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8145331857676492		[learning rate: 0.0044695]
	Learning Rate: 0.00446954
	LOSS [training: 0.8145331857676492 | validation: 0.7877824421717072]
	TIME [epoch: 16 sec]
EPOCH 162/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7872455469247972		[learning rate: 0.0044372]
	Learning Rate: 0.00443716
	LOSS [training: 0.7872455469247972 | validation: 0.7531645795513551]
	TIME [epoch: 16.1 sec]
EPOCH 163/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7410247311430579		[learning rate: 0.004405]
	Learning Rate: 0.00440501
	LOSS [training: 0.7410247311430579 | validation: 0.9466465012273152]
	TIME [epoch: 16 sec]
EPOCH 164/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8132489289912751		[learning rate: 0.0043731]
	Learning Rate: 0.0043731
	LOSS [training: 0.8132489289912751 | validation: 0.7313744278067416]
	TIME [epoch: 16 sec]
EPOCH 165/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7171852981697627		[learning rate: 0.0043414]
	Learning Rate: 0.00434142
	LOSS [training: 0.7171852981697627 | validation: 0.8483269289865756]
	TIME [epoch: 16.1 sec]
EPOCH 166/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7255918030889938		[learning rate: 0.00431]
	Learning Rate: 0.00430996
	LOSS [training: 0.7255918030889938 | validation: 0.6867515065825032]
	TIME [epoch: 16 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_166.pth
	Model improved!!!
EPOCH 167/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8789267080457386		[learning rate: 0.0042787]
	Learning Rate: 0.00427874
	LOSS [training: 0.8789267080457386 | validation: 0.7716333159502524]
	TIME [epoch: 16 sec]
EPOCH 168/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7467023490771518		[learning rate: 0.0042477]
	Learning Rate: 0.00424774
	LOSS [training: 0.7467023490771518 | validation: 0.8894466939732139]
	TIME [epoch: 16.1 sec]
EPOCH 169/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8406968220314979		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.8406968220314979 | validation: 0.7183224348588111]
	TIME [epoch: 16 sec]
EPOCH 170/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7105963448223731		[learning rate: 0.0041864]
	Learning Rate: 0.00418641
	LOSS [training: 0.7105963448223731 | validation: 0.6766365139545409]
	TIME [epoch: 16 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_170.pth
	Model improved!!!
EPOCH 171/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7182766901722163		[learning rate: 0.0041561]
	Learning Rate: 0.00415608
	LOSS [training: 0.7182766901722163 | validation: 0.7709369768648142]
	TIME [epoch: 16.1 sec]
EPOCH 172/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7206134156481208		[learning rate: 0.004126]
	Learning Rate: 0.00412597
	LOSS [training: 0.7206134156481208 | validation: 0.7182692708955172]
	TIME [epoch: 16 sec]
EPOCH 173/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7682412518253525		[learning rate: 0.0040961]
	Learning Rate: 0.00409608
	LOSS [training: 0.7682412518253525 | validation: 0.7787990501892911]
	TIME [epoch: 16 sec]
EPOCH 174/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7293766104873919		[learning rate: 0.0040664]
	Learning Rate: 0.0040664
	LOSS [training: 0.7293766104873919 | validation: 0.7053403893650632]
	TIME [epoch: 16.1 sec]
EPOCH 175/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7313928007208357		[learning rate: 0.0040369]
	Learning Rate: 0.00403694
	LOSS [training: 0.7313928007208357 | validation: 0.86839314554529]
	TIME [epoch: 16 sec]
EPOCH 176/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6761687593490759		[learning rate: 0.0040077]
	Learning Rate: 0.0040077
	LOSS [training: 0.6761687593490759 | validation: 0.9622480561270688]
	TIME [epoch: 16 sec]
EPOCH 177/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8191451820663603		[learning rate: 0.0039787]
	Learning Rate: 0.00397866
	LOSS [training: 0.8191451820663603 | validation: 0.6609207406477525]
	TIME [epoch: 16.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_177.pth
	Model improved!!!
EPOCH 178/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7480835469070237		[learning rate: 0.0039498]
	Learning Rate: 0.00394984
	LOSS [training: 0.7480835469070237 | validation: 0.8017388375309984]
	TIME [epoch: 16 sec]
EPOCH 179/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6499731933748902		[learning rate: 0.0039212]
	Learning Rate: 0.00392122
	LOSS [training: 0.6499731933748902 | validation: 0.6520397412463355]
	TIME [epoch: 16 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_179.pth
	Model improved!!!
EPOCH 180/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7674716282726173		[learning rate: 0.0038928]
	Learning Rate: 0.00389281
	LOSS [training: 0.7674716282726173 | validation: 0.6503391750805518]
	TIME [epoch: 16.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_180.pth
	Model improved!!!
EPOCH 181/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7422982739231269		[learning rate: 0.0038646]
	Learning Rate: 0.00386461
	LOSS [training: 0.7422982739231269 | validation: 0.7374596338033]
	TIME [epoch: 16 sec]
EPOCH 182/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6824313584232624		[learning rate: 0.0038366]
	Learning Rate: 0.00383661
	LOSS [training: 0.6824313584232624 | validation: 0.7436996995228722]
	TIME [epoch: 16 sec]
EPOCH 183/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6236820195603008		[learning rate: 0.0038088]
	Learning Rate: 0.00380881
	LOSS [training: 0.6236820195603008 | validation: 0.7930557031657632]
	TIME [epoch: 16.1 sec]
EPOCH 184/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6196635924619658		[learning rate: 0.0037812]
	Learning Rate: 0.00378122
	LOSS [training: 0.6196635924619658 | validation: 0.7599676399739034]
	TIME [epoch: 16 sec]
EPOCH 185/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6604935013277912		[learning rate: 0.0037538]
	Learning Rate: 0.00375382
	LOSS [training: 0.6604935013277912 | validation: 0.8662542520780225]
	TIME [epoch: 16 sec]
EPOCH 186/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8146068166047478		[learning rate: 0.0037266]
	Learning Rate: 0.00372663
	LOSS [training: 0.8146068166047478 | validation: 0.7578335165571163]
	TIME [epoch: 16.1 sec]
EPOCH 187/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7346924913726978		[learning rate: 0.0036996]
	Learning Rate: 0.00369963
	LOSS [training: 0.7346924913726978 | validation: 0.8628867579675397]
	TIME [epoch: 16 sec]
EPOCH 188/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7179533687487383		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.7179533687487383 | validation: 0.7309910151587728]
	TIME [epoch: 16 sec]
EPOCH 189/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6127791486774196		[learning rate: 0.0036462]
	Learning Rate: 0.00364621
	LOSS [training: 0.6127791486774196 | validation: 0.7413638296057201]
	TIME [epoch: 16.1 sec]
EPOCH 190/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6797391668124375		[learning rate: 0.0036198]
	Learning Rate: 0.0036198
	LOSS [training: 0.6797391668124375 | validation: 0.7712174109016263]
	TIME [epoch: 16 sec]
EPOCH 191/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6755295317112759		[learning rate: 0.0035936]
	Learning Rate: 0.00359357
	LOSS [training: 0.6755295317112759 | validation: 0.6599779883746741]
	TIME [epoch: 16 sec]
EPOCH 192/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6018474589060863		[learning rate: 0.0035675]
	Learning Rate: 0.00356754
	LOSS [training: 0.6018474589060863 | validation: 0.8363917767766286]
	TIME [epoch: 16.1 sec]
EPOCH 193/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6865822307653431		[learning rate: 0.0035417]
	Learning Rate: 0.00354169
	LOSS [training: 0.6865822307653431 | validation: 0.5918642423925596]
	TIME [epoch: 16 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_193.pth
	Model improved!!!
EPOCH 194/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6266495475174418		[learning rate: 0.003516]
	Learning Rate: 0.00351603
	LOSS [training: 0.6266495475174418 | validation: 0.9406364064847981]
	TIME [epoch: 16 sec]
EPOCH 195/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6882510534416202		[learning rate: 0.0034906]
	Learning Rate: 0.00349056
	LOSS [training: 0.6882510534416202 | validation: 0.6868588277229155]
	TIME [epoch: 16.1 sec]
EPOCH 196/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.645077273187572		[learning rate: 0.0034653]
	Learning Rate: 0.00346527
	LOSS [training: 0.645077273187572 | validation: 0.7331036974005044]
	TIME [epoch: 16 sec]
EPOCH 197/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6767836109288254		[learning rate: 0.0034402]
	Learning Rate: 0.00344016
	LOSS [training: 0.6767836109288254 | validation: 0.9460901131731171]
	TIME [epoch: 16 sec]
EPOCH 198/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7420472367656434		[learning rate: 0.0034152]
	Learning Rate: 0.00341524
	LOSS [training: 0.7420472367656434 | validation: 0.8910786753984062]
	TIME [epoch: 16.1 sec]
EPOCH 199/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.710398418376525		[learning rate: 0.0033905]
	Learning Rate: 0.0033905
	LOSS [training: 0.710398418376525 | validation: 0.6807284007713961]
	TIME [epoch: 16 sec]
EPOCH 200/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6229045473311419		[learning rate: 0.0033659]
	Learning Rate: 0.00336593
	LOSS [training: 0.6229045473311419 | validation: 0.8211779472052743]
	TIME [epoch: 16 sec]
EPOCH 201/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6300156538130091		[learning rate: 0.0033415]
	Learning Rate: 0.00334155
	LOSS [training: 0.6300156538130091 | validation: 0.5934346764033301]
	TIME [epoch: 16.1 sec]
EPOCH 202/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5852276466469493		[learning rate: 0.0033173]
	Learning Rate: 0.00331734
	LOSS [training: 0.5852276466469493 | validation: 0.6441343735391722]
	TIME [epoch: 16 sec]
EPOCH 203/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5846829440530494		[learning rate: 0.0032933]
	Learning Rate: 0.0032933
	LOSS [training: 0.5846829440530494 | validation: 0.7530845093175461]
	TIME [epoch: 16 sec]
EPOCH 204/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6598605943020226		[learning rate: 0.0032694]
	Learning Rate: 0.00326944
	LOSS [training: 0.6598605943020226 | validation: 0.9447375005029031]
	TIME [epoch: 16.1 sec]
EPOCH 205/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6007299508354481		[learning rate: 0.0032458]
	Learning Rate: 0.00324576
	LOSS [training: 0.6007299508354481 | validation: 0.8176683334056619]
	TIME [epoch: 16 sec]
EPOCH 206/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7318529942667475		[learning rate: 0.0032222]
	Learning Rate: 0.00322224
	LOSS [training: 0.7318529942667475 | validation: 0.7886944730726689]
	TIME [epoch: 16 sec]
EPOCH 207/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6096685290320005		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.6096685290320005 | validation: 0.668291002787663]
	TIME [epoch: 16.1 sec]
EPOCH 208/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5640327044785709		[learning rate: 0.0031757]
	Learning Rate: 0.00317572
	LOSS [training: 0.5640327044785709 | validation: 0.8733761127823003]
	TIME [epoch: 16 sec]
EPOCH 209/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6132648656801879		[learning rate: 0.0031527]
	Learning Rate: 0.00315271
	LOSS [training: 0.6132648656801879 | validation: 0.5842190147873938]
	TIME [epoch: 16 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_209.pth
	Model improved!!!
EPOCH 210/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.591044595579586		[learning rate: 0.0031299]
	Learning Rate: 0.00312987
	LOSS [training: 0.591044595579586 | validation: 1.137265674871228]
	TIME [epoch: 16.1 sec]
EPOCH 211/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8691460228942035		[learning rate: 0.0031072]
	Learning Rate: 0.00310719
	LOSS [training: 0.8691460228942035 | validation: 0.9030423999063253]
	TIME [epoch: 16 sec]
EPOCH 212/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7026980780852322		[learning rate: 0.0030847]
	Learning Rate: 0.00308468
	LOSS [training: 0.7026980780852322 | validation: 0.6318594018379086]
	TIME [epoch: 16 sec]
EPOCH 213/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5859167605581355		[learning rate: 0.0030623]
	Learning Rate: 0.00306233
	LOSS [training: 0.5859167605581355 | validation: 0.5713917720397264]
	TIME [epoch: 16.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_213.pth
	Model improved!!!
EPOCH 214/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.567947909863164		[learning rate: 0.0030401]
	Learning Rate: 0.00304015
	LOSS [training: 0.567947909863164 | validation: 0.8448052450426955]
	TIME [epoch: 16 sec]
EPOCH 215/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5759674039235664		[learning rate: 0.0030181]
	Learning Rate: 0.00301812
	LOSS [training: 0.5759674039235664 | validation: 0.682299382951919]
	TIME [epoch: 16 sec]
EPOCH 216/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5592427641577261		[learning rate: 0.0029963]
	Learning Rate: 0.00299626
	LOSS [training: 0.5592427641577261 | validation: 0.6915561801805343]
	TIME [epoch: 16.1 sec]
EPOCH 217/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6418697402224163		[learning rate: 0.0029745]
	Learning Rate: 0.00297455
	LOSS [training: 0.6418697402224163 | validation: 0.8449952256986422]
	TIME [epoch: 16 sec]
EPOCH 218/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5451802577529279		[learning rate: 0.002953]
	Learning Rate: 0.002953
	LOSS [training: 0.5451802577529279 | validation: 0.6938799752869836]
	TIME [epoch: 16 sec]
EPOCH 219/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5183594155555652		[learning rate: 0.0029316]
	Learning Rate: 0.0029316
	LOSS [training: 0.5183594155555652 | validation: 0.6775199725393445]
	TIME [epoch: 16.1 sec]
EPOCH 220/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6015675982287991		[learning rate: 0.0029104]
	Learning Rate: 0.00291036
	LOSS [training: 0.6015675982287991 | validation: 0.6643351281822509]
	TIME [epoch: 16 sec]
EPOCH 221/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5887838270544		[learning rate: 0.0028893]
	Learning Rate: 0.00288928
	LOSS [training: 0.5887838270544 | validation: 0.770778017995696]
	TIME [epoch: 16 sec]
EPOCH 222/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5686451312868884		[learning rate: 0.0028683]
	Learning Rate: 0.00286835
	LOSS [training: 0.5686451312868884 | validation: 0.5464652515221483]
	TIME [epoch: 16.1 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_222.pth
	Model improved!!!
EPOCH 223/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.558931275065868		[learning rate: 0.0028476]
	Learning Rate: 0.00284757
	LOSS [training: 0.558931275065868 | validation: 0.6834684333219921]
	TIME [epoch: 16 sec]
EPOCH 224/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5594096275593659		[learning rate: 0.0028269]
	Learning Rate: 0.00282693
	LOSS [training: 0.5594096275593659 | validation: 0.705741755778788]
	TIME [epoch: 16.1 sec]
EPOCH 225/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4889133760658075		[learning rate: 0.0028065]
	Learning Rate: 0.00280645
	LOSS [training: 0.4889133760658075 | validation: 0.8831655765329636]
	TIME [epoch: 16 sec]
EPOCH 226/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6450543450712821		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.6450543450712821 | validation: 0.7071910471862384]
	TIME [epoch: 16 sec]
EPOCH 227/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5909362829340691		[learning rate: 0.0027659]
	Learning Rate: 0.00276594
	LOSS [training: 0.5909362829340691 | validation: 0.7481704120483188]
	TIME [epoch: 16.1 sec]
EPOCH 228/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5917086833232277		[learning rate: 0.0027459]
	Learning Rate: 0.0027459
	LOSS [training: 0.5917086833232277 | validation: 0.6440626762244652]
	TIME [epoch: 16 sec]
EPOCH 229/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5975965506642169		[learning rate: 0.002726]
	Learning Rate: 0.002726
	LOSS [training: 0.5975965506642169 | validation: 0.8506610243690846]
	TIME [epoch: 16 sec]
EPOCH 230/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.9474662442051515		[learning rate: 0.0027063]
	Learning Rate: 0.00270625
	LOSS [training: 0.9474662442051515 | validation: 0.8914786173053786]
	TIME [epoch: 16 sec]
EPOCH 231/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.8055112116263515		[learning rate: 0.0026866]
	Learning Rate: 0.00268665
	LOSS [training: 0.8055112116263515 | validation: 0.7696078515531883]
	TIME [epoch: 16.1 sec]
EPOCH 232/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.7254483782330521		[learning rate: 0.0026672]
	Learning Rate: 0.00266718
	LOSS [training: 0.7254483782330521 | validation: 0.6494557208994058]
	TIME [epoch: 16 sec]
EPOCH 233/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6172553681931805		[learning rate: 0.0026479]
	Learning Rate: 0.00264786
	LOSS [training: 0.6172553681931805 | validation: 0.6203118996589161]
	TIME [epoch: 16 sec]
EPOCH 234/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6003091625544064		[learning rate: 0.0026287]
	Learning Rate: 0.00262867
	LOSS [training: 0.6003091625544064 | validation: 0.5813778137880325]
	TIME [epoch: 16.1 sec]
EPOCH 235/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5422534841498196		[learning rate: 0.0026096]
	Learning Rate: 0.00260963
	LOSS [training: 0.5422534841498196 | validation: 0.6980933509639453]
	TIME [epoch: 16 sec]
EPOCH 236/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.615288816865474		[learning rate: 0.0025907]
	Learning Rate: 0.00259072
	LOSS [training: 0.615288816865474 | validation: 0.6859734094763078]
	TIME [epoch: 16 sec]
EPOCH 237/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.563337449066427		[learning rate: 0.002572]
	Learning Rate: 0.00257195
	LOSS [training: 0.563337449066427 | validation: 0.7287125748157374]
	TIME [epoch: 16.1 sec]
EPOCH 238/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.519440044004555		[learning rate: 0.0025533]
	Learning Rate: 0.00255332
	LOSS [training: 0.519440044004555 | validation: 0.5720227453246556]
	TIME [epoch: 16 sec]
EPOCH 239/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5537682824862556		[learning rate: 0.0025348]
	Learning Rate: 0.00253482
	LOSS [training: 0.5537682824862556 | validation: 0.7112806617979273]
	TIME [epoch: 16.1 sec]
EPOCH 240/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5832504151604025		[learning rate: 0.0025165]
	Learning Rate: 0.00251646
	LOSS [training: 0.5832504151604025 | validation: 0.5252386248115006]
	TIME [epoch: 16 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_240.pth
	Model improved!!!
EPOCH 241/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5037349308651283		[learning rate: 0.0024982]
	Learning Rate: 0.00249823
	LOSS [training: 0.5037349308651283 | validation: 0.6021893972260481]
	TIME [epoch: 16 sec]
EPOCH 242/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5443163374830507		[learning rate: 0.0024801]
	Learning Rate: 0.00248013
	LOSS [training: 0.5443163374830507 | validation: 0.5505225226768662]
	TIME [epoch: 16.1 sec]
EPOCH 243/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5358632034182353		[learning rate: 0.0024622]
	Learning Rate: 0.00246216
	LOSS [training: 0.5358632034182353 | validation: 0.8243669327580203]
	TIME [epoch: 16 sec]
EPOCH 244/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6020772700480603		[learning rate: 0.0024443]
	Learning Rate: 0.00244432
	LOSS [training: 0.6020772700480603 | validation: 0.6968761335170959]
	TIME [epoch: 16 sec]
EPOCH 245/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.49065020116308594		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.49065020116308594 | validation: 0.5664054060734316]
	TIME [epoch: 16.1 sec]
EPOCH 246/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5558619725746797		[learning rate: 0.002409]
	Learning Rate: 0.00240903
	LOSS [training: 0.5558619725746797 | validation: 0.58184215460416]
	TIME [epoch: 16 sec]
EPOCH 247/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5321526177279441		[learning rate: 0.0023916]
	Learning Rate: 0.00239158
	LOSS [training: 0.5321526177279441 | validation: 0.5204254898610027]
	TIME [epoch: 16 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_247.pth
	Model improved!!!
EPOCH 248/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4530667476421336		[learning rate: 0.0023742]
	Learning Rate: 0.00237425
	LOSS [training: 0.4530667476421336 | validation: 0.6726549598409743]
	TIME [epoch: 16.1 sec]
EPOCH 249/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.597550039959232		[learning rate: 0.002357]
	Learning Rate: 0.00235705
	LOSS [training: 0.597550039959232 | validation: 0.7938319764804902]
	TIME [epoch: 16 sec]
EPOCH 250/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.638476641651388		[learning rate: 0.00234]
	Learning Rate: 0.00233997
	LOSS [training: 0.638476641651388 | validation: 0.6802960326523824]
	TIME [epoch: 16 sec]
EPOCH 251/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6005090688830031		[learning rate: 0.002323]
	Learning Rate: 0.00232302
	LOSS [training: 0.6005090688830031 | validation: 0.8070522454504137]
	TIME [epoch: 125 sec]
EPOCH 252/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5994100630061028		[learning rate: 0.0023062]
	Learning Rate: 0.00230619
	LOSS [training: 0.5994100630061028 | validation: 0.6007898332089457]
	TIME [epoch: 34.9 sec]
EPOCH 253/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5032596037046995		[learning rate: 0.0022895]
	Learning Rate: 0.00228948
	LOSS [training: 0.5032596037046995 | validation: 0.6326409301900563]
	TIME [epoch: 34.8 sec]
EPOCH 254/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5430287241667817		[learning rate: 0.0022729]
	Learning Rate: 0.00227289
	LOSS [training: 0.5430287241667817 | validation: 0.5786611656059306]
	TIME [epoch: 34.8 sec]
EPOCH 255/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5050925073561056		[learning rate: 0.0022564]
	Learning Rate: 0.00225643
	LOSS [training: 0.5050925073561056 | validation: 0.6518017458670216]
	TIME [epoch: 34.8 sec]
EPOCH 256/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5079882759595165		[learning rate: 0.0022401]
	Learning Rate: 0.00224008
	LOSS [training: 0.5079882759595165 | validation: 0.5240873795525749]
	TIME [epoch: 34.9 sec]
EPOCH 257/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4706164219175364		[learning rate: 0.0022238]
	Learning Rate: 0.00222385
	LOSS [training: 0.4706164219175364 | validation: 0.5668714021349579]
	TIME [epoch: 34.9 sec]
EPOCH 258/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4533755310116583		[learning rate: 0.0022077]
	Learning Rate: 0.00220774
	LOSS [training: 0.4533755310116583 | validation: 0.5205924016381475]
	TIME [epoch: 34.8 sec]
EPOCH 259/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.48863452218590053		[learning rate: 0.0021917]
	Learning Rate: 0.00219174
	LOSS [training: 0.48863452218590053 | validation: 0.5973024346381482]
	TIME [epoch: 34.9 sec]
EPOCH 260/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5241319315079017		[learning rate: 0.0021759]
	Learning Rate: 0.00217586
	LOSS [training: 0.5241319315079017 | validation: 0.47132557915271733]
	TIME [epoch: 34.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_260.pth
	Model improved!!!
EPOCH 261/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5126847898656396		[learning rate: 0.0021601]
	Learning Rate: 0.0021601
	LOSS [training: 0.5126847898656396 | validation: 0.9618833180954283]
	TIME [epoch: 34.8 sec]
EPOCH 262/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.6412379809863956		[learning rate: 0.0021444]
	Learning Rate: 0.00214445
	LOSS [training: 0.6412379809863956 | validation: 0.5896239621736811]
	TIME [epoch: 34.8 sec]
EPOCH 263/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.492661832898415		[learning rate: 0.0021289]
	Learning Rate: 0.00212891
	LOSS [training: 0.492661832898415 | validation: 0.4819815634524771]
	TIME [epoch: 34.9 sec]
EPOCH 264/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4496233701016129		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.4496233701016129 | validation: 0.594643362613087]
	TIME [epoch: 34.9 sec]
EPOCH 265/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.543202234905245		[learning rate: 0.0020982]
	Learning Rate: 0.00209818
	LOSS [training: 0.543202234905245 | validation: 0.568074059639484]
	TIME [epoch: 34.8 sec]
EPOCH 266/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5097178082245464		[learning rate: 0.002083]
	Learning Rate: 0.00208298
	LOSS [training: 0.5097178082245464 | validation: 0.5551871113435833]
	TIME [epoch: 34.9 sec]
EPOCH 267/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4507558068472969		[learning rate: 0.0020679]
	Learning Rate: 0.00206788
	LOSS [training: 0.4507558068472969 | validation: 0.5320522467713402]
	TIME [epoch: 34.9 sec]
EPOCH 268/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.48507988146030656		[learning rate: 0.0020529]
	Learning Rate: 0.0020529
	LOSS [training: 0.48507988146030656 | validation: 0.5170058204369028]
	TIME [epoch: 34.8 sec]
EPOCH 269/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.45795792417279496		[learning rate: 0.002038]
	Learning Rate: 0.00203803
	LOSS [training: 0.45795792417279496 | validation: 0.49177351961961385]
	TIME [epoch: 34.8 sec]
EPOCH 270/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4512923463032023		[learning rate: 0.0020233]
	Learning Rate: 0.00202326
	LOSS [training: 0.4512923463032023 | validation: 0.4443429449127729]
	TIME [epoch: 34.9 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_270.pth
	Model improved!!!
EPOCH 271/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.43829168532474094		[learning rate: 0.0020086]
	Learning Rate: 0.00200861
	LOSS [training: 0.43829168532474094 | validation: 0.5002353008826633]
	TIME [epoch: 34.8 sec]
EPOCH 272/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4805110243662001		[learning rate: 0.0019941]
	Learning Rate: 0.00199405
	LOSS [training: 0.4805110243662001 | validation: 0.739840045904701]
	TIME [epoch: 34.8 sec]
EPOCH 273/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4888402372285227		[learning rate: 0.0019796]
	Learning Rate: 0.00197961
	LOSS [training: 0.4888402372285227 | validation: 0.5348866335863603]
	TIME [epoch: 34.8 sec]
EPOCH 274/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4679408937263819		[learning rate: 0.0019653]
	Learning Rate: 0.00196527
	LOSS [training: 0.4679408937263819 | validation: 0.5224761841635264]
	TIME [epoch: 34.8 sec]
EPOCH 275/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4431444979284875		[learning rate: 0.001951]
	Learning Rate: 0.00195103
	LOSS [training: 0.4431444979284875 | validation: 0.5248950456473049]
	TIME [epoch: 34.7 sec]
EPOCH 276/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.49327969683696415		[learning rate: 0.0019369]
	Learning Rate: 0.00193689
	LOSS [training: 0.49327969683696415 | validation: 0.457210489586594]
	TIME [epoch: 34.8 sec]
EPOCH 277/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.41576544918865355		[learning rate: 0.0019229]
	Learning Rate: 0.00192286
	LOSS [training: 0.41576544918865355 | validation: 0.4982699660093419]
	TIME [epoch: 34.8 sec]
EPOCH 278/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4358315567576379		[learning rate: 0.0019089]
	Learning Rate: 0.00190893
	LOSS [training: 0.4358315567576379 | validation: 0.52641435994204]
	TIME [epoch: 34.8 sec]
EPOCH 279/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.42296532499932293		[learning rate: 0.0018951]
	Learning Rate: 0.0018951
	LOSS [training: 0.42296532499932293 | validation: 0.6184360577387904]
	TIME [epoch: 34.7 sec]
EPOCH 280/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4315776003624666		[learning rate: 0.0018814]
	Learning Rate: 0.00188137
	LOSS [training: 0.4315776003624666 | validation: 0.6983101819964125]
	TIME [epoch: 34.8 sec]
EPOCH 281/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.515290290134352		[learning rate: 0.0018677]
	Learning Rate: 0.00186774
	LOSS [training: 0.515290290134352 | validation: 0.5697017539627083]
	TIME [epoch: 34.8 sec]
EPOCH 282/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.46490210999041837		[learning rate: 0.0018542]
	Learning Rate: 0.00185421
	LOSS [training: 0.46490210999041837 | validation: 0.5565125324600536]
	TIME [epoch: 34.8 sec]
EPOCH 283/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4691498810442847		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.4691498810442847 | validation: 0.4447847188103933]
	TIME [epoch: 34.8 sec]
EPOCH 284/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4338013761183219		[learning rate: 0.0018274]
	Learning Rate: 0.00182744
	LOSS [training: 0.4338013761183219 | validation: 0.4687730476179004]
	TIME [epoch: 34.8 sec]
EPOCH 285/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.481019071052823		[learning rate: 0.0018142]
	Learning Rate: 0.0018142
	LOSS [training: 0.481019071052823 | validation: 0.4330491476774908]
	TIME [epoch: 34.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_285.pth
	Model improved!!!
EPOCH 286/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.39987168199806133		[learning rate: 0.0018011]
	Learning Rate: 0.00180105
	LOSS [training: 0.39987168199806133 | validation: 0.4928292495110197]
	TIME [epoch: 34.8 sec]
EPOCH 287/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4591474261800105		[learning rate: 0.001788]
	Learning Rate: 0.001788
	LOSS [training: 0.4591474261800105 | validation: 0.581889576945762]
	TIME [epoch: 34.8 sec]
EPOCH 288/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.5017291434427434		[learning rate: 0.001775]
	Learning Rate: 0.00177505
	LOSS [training: 0.5017291434427434 | validation: 0.5591939202968401]
	TIME [epoch: 34.8 sec]
EPOCH 289/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.43987626465300833		[learning rate: 0.0017622]
	Learning Rate: 0.00176219
	LOSS [training: 0.43987626465300833 | validation: 0.49872615904014883]
	TIME [epoch: 34.8 sec]
EPOCH 290/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4209899583640949		[learning rate: 0.0017494]
	Learning Rate: 0.00174942
	LOSS [training: 0.4209899583640949 | validation: 0.4554912273784961]
	TIME [epoch: 34.8 sec]
EPOCH 291/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.40760531265752165		[learning rate: 0.0017367]
	Learning Rate: 0.00173675
	LOSS [training: 0.40760531265752165 | validation: 0.5264033263045877]
	TIME [epoch: 34.8 sec]
EPOCH 292/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.439155965946604		[learning rate: 0.0017242]
	Learning Rate: 0.00172417
	LOSS [training: 0.439155965946604 | validation: 0.7093928690811271]
	TIME [epoch: 34.8 sec]
EPOCH 293/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.43977221200317484		[learning rate: 0.0017117]
	Learning Rate: 0.00171167
	LOSS [training: 0.43977221200317484 | validation: 0.5499129704094003]
	TIME [epoch: 34.8 sec]
EPOCH 294/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4361016664796512		[learning rate: 0.0016993]
	Learning Rate: 0.00169927
	LOSS [training: 0.4361016664796512 | validation: 0.47867703308607923]
	TIME [epoch: 34.8 sec]
EPOCH 295/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4267362724116256		[learning rate: 0.001687]
	Learning Rate: 0.00168696
	LOSS [training: 0.4267362724116256 | validation: 0.5284303601359188]
	TIME [epoch: 34.8 sec]
EPOCH 296/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4168801744033164		[learning rate: 0.0016747]
	Learning Rate: 0.00167474
	LOSS [training: 0.4168801744033164 | validation: 0.44409058599777124]
	TIME [epoch: 34.8 sec]
EPOCH 297/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.42179721013573335		[learning rate: 0.0016626]
	Learning Rate: 0.00166261
	LOSS [training: 0.42179721013573335 | validation: 0.4595040897359629]
	TIME [epoch: 34.8 sec]
EPOCH 298/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4149664261149749		[learning rate: 0.0016506]
	Learning Rate: 0.00165056
	LOSS [training: 0.4149664261149749 | validation: 0.5088309571540522]
	TIME [epoch: 34.8 sec]
EPOCH 299/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.40484229865500354		[learning rate: 0.0016386]
	Learning Rate: 0.0016386
	LOSS [training: 0.40484229865500354 | validation: 0.48969195944030697]
	TIME [epoch: 34.8 sec]
EPOCH 300/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4147670237697732		[learning rate: 0.0016267]
	Learning Rate: 0.00162673
	LOSS [training: 0.4147670237697732 | validation: 0.5933324565486381]
	TIME [epoch: 34.8 sec]
EPOCH 301/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.48609820594993997		[learning rate: 0.0016149]
	Learning Rate: 0.00161495
	LOSS [training: 0.48609820594993997 | validation: 0.47457860017842285]
	TIME [epoch: 34.8 sec]
EPOCH 302/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.39793268758474787		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.39793268758474787 | validation: 0.44326285412239524]
	TIME [epoch: 34.8 sec]
EPOCH 303/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.39014907953828337		[learning rate: 0.0015916]
	Learning Rate: 0.00159163
	LOSS [training: 0.39014907953828337 | validation: 0.5733526264544396]
	TIME [epoch: 34.8 sec]
EPOCH 304/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.43532485943306726		[learning rate: 0.0015801]
	Learning Rate: 0.0015801
	LOSS [training: 0.43532485943306726 | validation: 0.54201366251284]
	TIME [epoch: 34.8 sec]
EPOCH 305/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.41776750254738165		[learning rate: 0.0015687]
	Learning Rate: 0.00156865
	LOSS [training: 0.41776750254738165 | validation: 0.4369219965936263]
	TIME [epoch: 34.8 sec]
EPOCH 306/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.42658322895808976		[learning rate: 0.0015573]
	Learning Rate: 0.00155729
	LOSS [training: 0.42658322895808976 | validation: 0.4353050921730728]
	TIME [epoch: 34.8 sec]
EPOCH 307/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.40383380493384996		[learning rate: 0.001546]
	Learning Rate: 0.001546
	LOSS [training: 0.40383380493384996 | validation: 0.4262083485055814]
	TIME [epoch: 34.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_307.pth
	Model improved!!!
EPOCH 308/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.41619773212400357		[learning rate: 0.0015348]
	Learning Rate: 0.0015348
	LOSS [training: 0.41619773212400357 | validation: 0.41405683604940935]
	TIME [epoch: 34.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_308.pth
	Model improved!!!
EPOCH 309/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4294918231948882		[learning rate: 0.0015237]
	Learning Rate: 0.00152368
	LOSS [training: 0.4294918231948882 | validation: 0.4987601373155379]
	TIME [epoch: 34.8 sec]
EPOCH 310/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4486686572970665		[learning rate: 0.0015126]
	Learning Rate: 0.00151264
	LOSS [training: 0.4486686572970665 | validation: 0.519515707043275]
	TIME [epoch: 34.8 sec]
EPOCH 311/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.45323716002034425		[learning rate: 0.0015017]
	Learning Rate: 0.00150169
	LOSS [training: 0.45323716002034425 | validation: 0.45102117260163344]
	TIME [epoch: 34.8 sec]
EPOCH 312/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.39069878642421185		[learning rate: 0.0014908]
	Learning Rate: 0.00149081
	LOSS [training: 0.39069878642421185 | validation: 0.5081569835170874]
	TIME [epoch: 34.8 sec]
EPOCH 313/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4129714849713213		[learning rate: 0.00148]
	Learning Rate: 0.00148001
	LOSS [training: 0.4129714849713213 | validation: 0.4722943684700782]
	TIME [epoch: 34.8 sec]
EPOCH 314/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.38003890814048163		[learning rate: 0.0014693]
	Learning Rate: 0.00146928
	LOSS [training: 0.38003890814048163 | validation: 0.4041294171058971]
	TIME [epoch: 34.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_314.pth
	Model improved!!!
EPOCH 315/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.38158982140944486		[learning rate: 0.0014586]
	Learning Rate: 0.00145864
	LOSS [training: 0.38158982140944486 | validation: 0.5659880609954426]
	TIME [epoch: 34.8 sec]
EPOCH 316/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4192661366083136		[learning rate: 0.0014481]
	Learning Rate: 0.00144807
	LOSS [training: 0.4192661366083136 | validation: 0.5851026576323431]
	TIME [epoch: 34.8 sec]
EPOCH 317/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.431442702137794		[learning rate: 0.0014376]
	Learning Rate: 0.00143758
	LOSS [training: 0.431442702137794 | validation: 0.4863394249497326]
	TIME [epoch: 34.8 sec]
EPOCH 318/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4015287526998471		[learning rate: 0.0014272]
	Learning Rate: 0.00142716
	LOSS [training: 0.4015287526998471 | validation: 0.4596531182856849]
	TIME [epoch: 34.8 sec]
EPOCH 319/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4095850489836195		[learning rate: 0.0014168]
	Learning Rate: 0.00141682
	LOSS [training: 0.4095850489836195 | validation: 0.5011147207821213]
	TIME [epoch: 34.8 sec]
EPOCH 320/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.4100029910342753		[learning rate: 0.0014066]
	Learning Rate: 0.00140656
	LOSS [training: 0.4100029910342753 | validation: 0.44720774785845396]
	TIME [epoch: 34.8 sec]
EPOCH 321/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.38917948807818714		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.38917948807818714 | validation: 0.47016442010521564]
	TIME [epoch: 34.8 sec]
EPOCH 322/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3934221404106528		[learning rate: 0.0013863]
	Learning Rate: 0.00138625
	LOSS [training: 0.3934221404106528 | validation: 0.44544045516804415]
	TIME [epoch: 34.8 sec]
EPOCH 323/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.376029500153173		[learning rate: 0.0013762]
	Learning Rate: 0.00137621
	LOSS [training: 0.376029500153173 | validation: 0.4099772479556824]
	TIME [epoch: 34.8 sec]
EPOCH 324/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3925839438932216		[learning rate: 0.0013662]
	Learning Rate: 0.00136624
	LOSS [training: 0.3925839438932216 | validation: 0.466054241606802]
	TIME [epoch: 34.8 sec]
EPOCH 325/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.42100331420083503		[learning rate: 0.0013563]
	Learning Rate: 0.00135634
	LOSS [training: 0.42100331420083503 | validation: 0.44282385430597737]
	TIME [epoch: 34.8 sec]
EPOCH 326/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3838504632748875		[learning rate: 0.0013465]
	Learning Rate: 0.00134651
	LOSS [training: 0.3838504632748875 | validation: 0.5286743314173136]
	TIME [epoch: 34.8 sec]
EPOCH 327/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.38214451154967966		[learning rate: 0.0013368]
	Learning Rate: 0.00133676
	LOSS [training: 0.38214451154967966 | validation: 0.4207140324906345]
	TIME [epoch: 34.8 sec]
EPOCH 328/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.39993487474768075		[learning rate: 0.0013271]
	Learning Rate: 0.00132707
	LOSS [training: 0.39993487474768075 | validation: 0.47095006951240725]
	TIME [epoch: 34.8 sec]
EPOCH 329/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3807344195029111		[learning rate: 0.0013175]
	Learning Rate: 0.00131746
	LOSS [training: 0.3807344195029111 | validation: 0.4869941214315887]
	TIME [epoch: 34.8 sec]
EPOCH 330/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3822014053898447		[learning rate: 0.0013079]
	Learning Rate: 0.00130791
	LOSS [training: 0.3822014053898447 | validation: 0.4550640109930677]
	TIME [epoch: 34.8 sec]
EPOCH 331/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3865863794025639		[learning rate: 0.0012984]
	Learning Rate: 0.00129844
	LOSS [training: 0.3865863794025639 | validation: 0.4664618841949943]
	TIME [epoch: 34.8 sec]
EPOCH 332/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3893542953754517		[learning rate: 0.001289]
	Learning Rate: 0.00128903
	LOSS [training: 0.3893542953754517 | validation: 0.4072409184859425]
	TIME [epoch: 34.8 sec]
EPOCH 333/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.36705499203043823		[learning rate: 0.0012797]
	Learning Rate: 0.00127969
	LOSS [training: 0.36705499203043823 | validation: 0.5638318984205922]
	TIME [epoch: 34.8 sec]
EPOCH 334/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.43900172347819943		[learning rate: 0.0012704]
	Learning Rate: 0.00127042
	LOSS [training: 0.43900172347819943 | validation: 0.45074622699209144]
	TIME [epoch: 34.8 sec]
EPOCH 335/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.36045957950747953		[learning rate: 0.0012612]
	Learning Rate: 0.00126122
	LOSS [training: 0.36045957950747953 | validation: 0.4421968396346519]
	TIME [epoch: 34.8 sec]
EPOCH 336/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3771622173977941		[learning rate: 0.0012521]
	Learning Rate: 0.00125208
	LOSS [training: 0.3771622173977941 | validation: 0.5505752691724036]
	TIME [epoch: 34.8 sec]
EPOCH 337/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3840592024113694		[learning rate: 0.001243]
	Learning Rate: 0.00124301
	LOSS [training: 0.3840592024113694 | validation: 0.42420479915530446]
	TIME [epoch: 34.8 sec]
EPOCH 338/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3631011924188293		[learning rate: 0.001234]
	Learning Rate: 0.001234
	LOSS [training: 0.3631011924188293 | validation: 0.4418923487637356]
	TIME [epoch: 34.8 sec]
EPOCH 339/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3962017174219878		[learning rate: 0.0012251]
	Learning Rate: 0.00122506
	LOSS [training: 0.3962017174219878 | validation: 0.4257082672569798]
	TIME [epoch: 34.8 sec]
EPOCH 340/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3783314763117731		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.3783314763117731 | validation: 0.44681375631183384]
	TIME [epoch: 34.8 sec]
EPOCH 341/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.35900729888818494		[learning rate: 0.0012074]
	Learning Rate: 0.00120737
	LOSS [training: 0.35900729888818494 | validation: 0.40136096390247084]
	TIME [epoch: 34.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_341.pth
	Model improved!!!
EPOCH 342/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.38947631796195004		[learning rate: 0.0011986]
	Learning Rate: 0.00119863
	LOSS [training: 0.38947631796195004 | validation: 0.4279566592270556]
	TIME [epoch: 34.8 sec]
EPOCH 343/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.42320626526003324		[learning rate: 0.0011899]
	Learning Rate: 0.00118994
	LOSS [training: 0.42320626526003324 | validation: 0.4566183491538074]
	TIME [epoch: 34.8 sec]
EPOCH 344/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.41067996861909895		[learning rate: 0.0011813]
	Learning Rate: 0.00118132
	LOSS [training: 0.41067996861909895 | validation: 0.3926460538519203]
	TIME [epoch: 34.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_344.pth
	Model improved!!!
EPOCH 345/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3888918291581333		[learning rate: 0.0011728]
	Learning Rate: 0.00117276
	LOSS [training: 0.3888918291581333 | validation: 0.4342168492463453]
	TIME [epoch: 34.8 sec]
EPOCH 346/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.36785862687759985		[learning rate: 0.0011643]
	Learning Rate: 0.00116427
	LOSS [training: 0.36785862687759985 | validation: 0.4135226766794521]
	TIME [epoch: 34.8 sec]
EPOCH 347/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.35885132199608794		[learning rate: 0.0011558]
	Learning Rate: 0.00115583
	LOSS [training: 0.35885132199608794 | validation: 0.4308249858034059]
	TIME [epoch: 34.8 sec]
EPOCH 348/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.36649963862296836		[learning rate: 0.0011475]
	Learning Rate: 0.00114746
	LOSS [training: 0.36649963862296836 | validation: 0.48664231731068164]
	TIME [epoch: 34.8 sec]
EPOCH 349/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.38076785260758805		[learning rate: 0.0011391]
	Learning Rate: 0.00113914
	LOSS [training: 0.38076785260758805 | validation: 0.4025856848295207]
	TIME [epoch: 34.8 sec]
EPOCH 350/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3414294707370064		[learning rate: 0.0011309]
	Learning Rate: 0.00113089
	LOSS [training: 0.3414294707370064 | validation: 0.40998725619723414]
	TIME [epoch: 34.8 sec]
EPOCH 351/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3622813633537549		[learning rate: 0.0011227]
	Learning Rate: 0.0011227
	LOSS [training: 0.3622813633537549 | validation: 0.41168494356751184]
	TIME [epoch: 34.8 sec]
EPOCH 352/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3459616830187934		[learning rate: 0.0011146]
	Learning Rate: 0.00111456
	LOSS [training: 0.3459616830187934 | validation: 0.38400042602923423]
	TIME [epoch: 34.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_352.pth
	Model improved!!!
EPOCH 353/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3424846893704633		[learning rate: 0.0011065]
	Learning Rate: 0.00110649
	LOSS [training: 0.3424846893704633 | validation: 0.4348631780770239]
	TIME [epoch: 34.8 sec]
EPOCH 354/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.41952032692426977		[learning rate: 0.0010985]
	Learning Rate: 0.00109847
	LOSS [training: 0.41952032692426977 | validation: 0.3965997653552323]
	TIME [epoch: 34.8 sec]
EPOCH 355/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.39674649712592336		[learning rate: 0.0010905]
	Learning Rate: 0.00109051
	LOSS [training: 0.39674649712592336 | validation: 0.4099460763092653]
	TIME [epoch: 34.8 sec]
EPOCH 356/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3414063769460355		[learning rate: 0.0010826]
	Learning Rate: 0.00108261
	LOSS [training: 0.3414063769460355 | validation: 0.416915896234181]
	TIME [epoch: 34.8 sec]
EPOCH 357/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.34073064046271934		[learning rate: 0.0010748]
	Learning Rate: 0.00107477
	LOSS [training: 0.34073064046271934 | validation: 0.3847756982977407]
	TIME [epoch: 34.8 sec]
EPOCH 358/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.357153900417113		[learning rate: 0.001067]
	Learning Rate: 0.00106698
	LOSS [training: 0.357153900417113 | validation: 0.4196840932625613]
	TIME [epoch: 34.8 sec]
EPOCH 359/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3684192788693713		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.3684192788693713 | validation: 0.4068076164637736]
	TIME [epoch: 34.8 sec]
EPOCH 360/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.35284910416334336		[learning rate: 0.0010516]
	Learning Rate: 0.00105158
	LOSS [training: 0.35284910416334336 | validation: 0.44231194546745056]
	TIME [epoch: 34.8 sec]
EPOCH 361/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.36070763961728025		[learning rate: 0.001044]
	Learning Rate: 0.00104396
	LOSS [training: 0.36070763961728025 | validation: 0.37882061544084]
	TIME [epoch: 34.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_361.pth
	Model improved!!!
EPOCH 362/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.336291892348489		[learning rate: 0.0010364]
	Learning Rate: 0.0010364
	LOSS [training: 0.336291892348489 | validation: 0.4001336066078387]
	TIME [epoch: 34.8 sec]
EPOCH 363/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.37566049922522715		[learning rate: 0.0010289]
	Learning Rate: 0.00102889
	LOSS [training: 0.37566049922522715 | validation: 0.3881890241227316]
	TIME [epoch: 34.8 sec]
EPOCH 364/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.33501202183843753		[learning rate: 0.0010214]
	Learning Rate: 0.00102143
	LOSS [training: 0.33501202183843753 | validation: 0.4365914465693841]
	TIME [epoch: 34.8 sec]
EPOCH 365/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.35480872309857037		[learning rate: 0.001014]
	Learning Rate: 0.00101403
	LOSS [training: 0.35480872309857037 | validation: 0.3890217387630929]
	TIME [epoch: 34.8 sec]
EPOCH 366/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3328446011126071		[learning rate: 0.0010067]
	Learning Rate: 0.00100669
	LOSS [training: 0.3328446011126071 | validation: 0.46596567931112676]
	TIME [epoch: 34.8 sec]
EPOCH 367/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3433197732461548		[learning rate: 0.00099939]
	Learning Rate: 0.000999394
	LOSS [training: 0.3433197732461548 | validation: 0.42273489091922856]
	TIME [epoch: 34.8 sec]
EPOCH 368/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.34034140174503236		[learning rate: 0.00099215]
	Learning Rate: 0.000992154
	LOSS [training: 0.34034140174503236 | validation: 0.39712547725244995]
	TIME [epoch: 34.8 sec]
EPOCH 369/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3746311271296443		[learning rate: 0.00098497]
	Learning Rate: 0.000984966
	LOSS [training: 0.3746311271296443 | validation: 0.369871631312759]
	TIME [epoch: 34.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_369.pth
	Model improved!!!
EPOCH 370/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.33074922977914495		[learning rate: 0.00097783]
	Learning Rate: 0.00097783
	LOSS [training: 0.33074922977914495 | validation: 0.3992939026589374]
	TIME [epoch: 34.8 sec]
EPOCH 371/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3558929313063288		[learning rate: 0.00097075]
	Learning Rate: 0.000970745
	LOSS [training: 0.3558929313063288 | validation: 0.40497424211789207]
	TIME [epoch: 34.8 sec]
EPOCH 372/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3461418481077393		[learning rate: 0.00096371]
	Learning Rate: 0.000963712
	LOSS [training: 0.3461418481077393 | validation: 0.3635057485571974]
	TIME [epoch: 34.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_372.pth
	Model improved!!!
EPOCH 373/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.32805622008343294		[learning rate: 0.00095673]
	Learning Rate: 0.00095673
	LOSS [training: 0.32805622008343294 | validation: 0.46143850027405814]
	TIME [epoch: 34.8 sec]
EPOCH 374/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3737645739268065		[learning rate: 0.0009498]
	Learning Rate: 0.000949799
	LOSS [training: 0.3737645739268065 | validation: 0.395338129863853]
	TIME [epoch: 34.8 sec]
EPOCH 375/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3337686037553853		[learning rate: 0.00094292]
	Learning Rate: 0.000942918
	LOSS [training: 0.3337686037553853 | validation: 0.40116487987541305]
	TIME [epoch: 34.8 sec]
EPOCH 376/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3211624501223631		[learning rate: 0.00093609]
	Learning Rate: 0.000936086
	LOSS [training: 0.3211624501223631 | validation: 0.3544596539003675]
	TIME [epoch: 34.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_376.pth
	Model improved!!!
EPOCH 377/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3488822056529729		[learning rate: 0.0009293]
	Learning Rate: 0.000929304
	LOSS [training: 0.3488822056529729 | validation: 0.42092766427457745]
	TIME [epoch: 34.8 sec]
EPOCH 378/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.34295518634036226		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.34295518634036226 | validation: 0.37417275150751317]
	TIME [epoch: 34.8 sec]
EPOCH 379/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3164046168623126		[learning rate: 0.00091589]
	Learning Rate: 0.000915888
	LOSS [training: 0.3164046168623126 | validation: 0.40390899393236246]
	TIME [epoch: 34.8 sec]
EPOCH 380/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3403118435614819		[learning rate: 0.00090925]
	Learning Rate: 0.000909252
	LOSS [training: 0.3403118435614819 | validation: 0.39943632412065366]
	TIME [epoch: 34.8 sec]
EPOCH 381/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.35279528792768017		[learning rate: 0.00090266]
	Learning Rate: 0.000902664
	LOSS [training: 0.35279528792768017 | validation: 0.40650818013361734]
	TIME [epoch: 34.8 sec]
EPOCH 382/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3295280685760187		[learning rate: 0.00089612]
	Learning Rate: 0.000896125
	LOSS [training: 0.3295280685760187 | validation: 0.3980484842457913]
	TIME [epoch: 34.8 sec]
EPOCH 383/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3408831597080503		[learning rate: 0.00088963]
	Learning Rate: 0.000889632
	LOSS [training: 0.3408831597080503 | validation: 0.36128679399777]
	TIME [epoch: 34.8 sec]
EPOCH 384/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.32681680342843705		[learning rate: 0.00088319]
	Learning Rate: 0.000883187
	LOSS [training: 0.32681680342843705 | validation: 0.3780935242190238]
	TIME [epoch: 34.8 sec]
EPOCH 385/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.32471766649598366		[learning rate: 0.00087679]
	Learning Rate: 0.000876788
	LOSS [training: 0.32471766649598366 | validation: 0.3579341812307315]
	TIME [epoch: 34.8 sec]
EPOCH 386/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3103133240553517		[learning rate: 0.00087044]
	Learning Rate: 0.000870436
	LOSS [training: 0.3103133240553517 | validation: 0.3712635927437387]
	TIME [epoch: 34.8 sec]
EPOCH 387/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.33553073613079465		[learning rate: 0.00086413]
	Learning Rate: 0.00086413
	LOSS [training: 0.33553073613079465 | validation: 0.35426379325676627]
	TIME [epoch: 34.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_387.pth
	Model improved!!!
EPOCH 388/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.31786968894556333		[learning rate: 0.00085787]
	Learning Rate: 0.000857869
	LOSS [training: 0.31786968894556333 | validation: 0.4302386310402082]
	TIME [epoch: 34.8 sec]
EPOCH 389/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.36797655330225215		[learning rate: 0.00085165]
	Learning Rate: 0.000851654
	LOSS [training: 0.36797655330225215 | validation: 0.40257890420245546]
	TIME [epoch: 34.8 sec]
EPOCH 390/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3277954026806738		[learning rate: 0.00084548]
	Learning Rate: 0.000845484
	LOSS [training: 0.3277954026806738 | validation: 0.37789799269164365]
	TIME [epoch: 34.7 sec]
EPOCH 391/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3757555675861247		[learning rate: 0.00083936]
	Learning Rate: 0.000839358
	LOSS [training: 0.3757555675861247 | validation: 0.39906176841785845]
	TIME [epoch: 34.8 sec]
EPOCH 392/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.35211563224806264		[learning rate: 0.00083328]
	Learning Rate: 0.000833277
	LOSS [training: 0.35211563224806264 | validation: 0.3655985450390864]
	TIME [epoch: 34.8 sec]
EPOCH 393/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.31478818535282704		[learning rate: 0.00082724]
	Learning Rate: 0.00082724
	LOSS [training: 0.31478818535282704 | validation: 0.35313459886899595]
	TIME [epoch: 34.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_393.pth
	Model improved!!!
EPOCH 394/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.314172204242152		[learning rate: 0.00082125]
	Learning Rate: 0.000821247
	LOSS [training: 0.314172204242152 | validation: 0.39889319951502056]
	TIME [epoch: 34.8 sec]
EPOCH 395/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.33425614954478533		[learning rate: 0.0008153]
	Learning Rate: 0.000815297
	LOSS [training: 0.33425614954478533 | validation: 0.37141785774623204]
	TIME [epoch: 34.8 sec]
EPOCH 396/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3169355692218006		[learning rate: 0.00080939]
	Learning Rate: 0.00080939
	LOSS [training: 0.3169355692218006 | validation: 0.41592093710955097]
	TIME [epoch: 34.8 sec]
EPOCH 397/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.32143595254212165		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.32143595254212165 | validation: 0.3945751443330282]
	TIME [epoch: 34.8 sec]
EPOCH 398/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3340351190561085		[learning rate: 0.0007977]
	Learning Rate: 0.000797705
	LOSS [training: 0.3340351190561085 | validation: 0.3669213687968288]
	TIME [epoch: 34.8 sec]
EPOCH 399/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3122466919959639		[learning rate: 0.00079193]
	Learning Rate: 0.000791925
	LOSS [training: 0.3122466919959639 | validation: 0.3561537857084569]
	TIME [epoch: 34.8 sec]
EPOCH 400/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.31704156939340905		[learning rate: 0.00078619]
	Learning Rate: 0.000786188
	LOSS [training: 0.31704156939340905 | validation: 0.36121374905004144]
	TIME [epoch: 34.8 sec]
EPOCH 401/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.31432589093065555		[learning rate: 0.00078049]
	Learning Rate: 0.000780492
	LOSS [training: 0.31432589093065555 | validation: 0.40716118034799714]
	TIME [epoch: 34.8 sec]
EPOCH 402/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3241805394217713		[learning rate: 0.00077484]
	Learning Rate: 0.000774838
	LOSS [training: 0.3241805394217713 | validation: 0.35675243436396187]
	TIME [epoch: 34.8 sec]
EPOCH 403/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3158163445908624		[learning rate: 0.00076922]
	Learning Rate: 0.000769224
	LOSS [training: 0.3158163445908624 | validation: 0.38924207485914775]
	TIME [epoch: 34.8 sec]
EPOCH 404/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.34605839229993546		[learning rate: 0.00076365]
	Learning Rate: 0.000763651
	LOSS [training: 0.34605839229993546 | validation: 0.40240717702138007]
	TIME [epoch: 34.7 sec]
EPOCH 405/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.33806446697558434		[learning rate: 0.00075812]
	Learning Rate: 0.000758118
	LOSS [training: 0.33806446697558434 | validation: 0.36096188625049774]
	TIME [epoch: 34.8 sec]
EPOCH 406/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30979260715869794		[learning rate: 0.00075263]
	Learning Rate: 0.000752626
	LOSS [training: 0.30979260715869794 | validation: 0.39432719152749707]
	TIME [epoch: 34.8 sec]
EPOCH 407/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.31696397852665803		[learning rate: 0.00074717]
	Learning Rate: 0.000747173
	LOSS [training: 0.31696397852665803 | validation: 0.3566401876373466]
	TIME [epoch: 34.8 sec]
EPOCH 408/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3064365465678688		[learning rate: 0.00074176]
	Learning Rate: 0.00074176
	LOSS [training: 0.3064365465678688 | validation: 0.3914987238441587]
	TIME [epoch: 34.7 sec]
EPOCH 409/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3083482871482097		[learning rate: 0.00073639]
	Learning Rate: 0.000736386
	LOSS [training: 0.3083482871482097 | validation: 0.34618849602172946]
	TIME [epoch: 34.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_409.pth
	Model improved!!!
EPOCH 410/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3228970401390512		[learning rate: 0.00073105]
	Learning Rate: 0.000731051
	LOSS [training: 0.3228970401390512 | validation: 0.42183402878245596]
	TIME [epoch: 34.8 sec]
EPOCH 411/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.32083167101997373		[learning rate: 0.00072575]
	Learning Rate: 0.000725754
	LOSS [training: 0.32083167101997373 | validation: 0.3585407639943831]
	TIME [epoch: 34.8 sec]
EPOCH 412/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30358021004533986		[learning rate: 0.0007205]
	Learning Rate: 0.000720496
	LOSS [training: 0.30358021004533986 | validation: 0.39200583571844694]
	TIME [epoch: 34.8 sec]
EPOCH 413/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.314796977996432		[learning rate: 0.00071528]
	Learning Rate: 0.000715276
	LOSS [training: 0.314796977996432 | validation: 0.3520735566354826]
	TIME [epoch: 34.8 sec]
EPOCH 414/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30647499222463526		[learning rate: 0.00071009]
	Learning Rate: 0.000710094
	LOSS [training: 0.30647499222463526 | validation: 0.35321737013808724]
	TIME [epoch: 34.8 sec]
EPOCH 415/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3055271226101873		[learning rate: 0.00070495]
	Learning Rate: 0.000704949
	LOSS [training: 0.3055271226101873 | validation: 0.33240088132390255]
	TIME [epoch: 34.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_415.pth
	Model improved!!!
EPOCH 416/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.31625218513669123		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.31625218513669123 | validation: 0.37377674827446794]
	TIME [epoch: 34.8 sec]
EPOCH 417/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.31261941683350486		[learning rate: 0.00069477]
	Learning Rate: 0.000694772
	LOSS [training: 0.31261941683350486 | validation: 0.3376033522099454]
	TIME [epoch: 34.8 sec]
EPOCH 418/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2946413425068708		[learning rate: 0.00068974]
	Learning Rate: 0.000689738
	LOSS [training: 0.2946413425068708 | validation: 0.3464938682036909]
	TIME [epoch: 34.8 sec]
EPOCH 419/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3043129796643085		[learning rate: 0.00068474]
	Learning Rate: 0.000684741
	LOSS [training: 0.3043129796643085 | validation: 0.3794110481473653]
	TIME [epoch: 34.8 sec]
EPOCH 420/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.31285801732358054		[learning rate: 0.00067978]
	Learning Rate: 0.00067978
	LOSS [training: 0.31285801732358054 | validation: 0.36485569320292277]
	TIME [epoch: 34.8 sec]
EPOCH 421/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3043199924931099		[learning rate: 0.00067486]
	Learning Rate: 0.000674855
	LOSS [training: 0.3043199924931099 | validation: 0.33999797148885125]
	TIME [epoch: 34.8 sec]
EPOCH 422/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29547496399847994		[learning rate: 0.00066997]
	Learning Rate: 0.000669966
	LOSS [training: 0.29547496399847994 | validation: 0.3347966283468009]
	TIME [epoch: 34.8 sec]
EPOCH 423/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3101164894266398		[learning rate: 0.00066511]
	Learning Rate: 0.000665112
	LOSS [training: 0.3101164894266398 | validation: 0.3573674854779516]
	TIME [epoch: 34.8 sec]
EPOCH 424/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3047556554431966		[learning rate: 0.00066029]
	Learning Rate: 0.000660293
	LOSS [training: 0.3047556554431966 | validation: 0.35507255993537834]
	TIME [epoch: 34.8 sec]
EPOCH 425/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3317621868071823		[learning rate: 0.00065551]
	Learning Rate: 0.00065551
	LOSS [training: 0.3317621868071823 | validation: 0.34136274997712557]
	TIME [epoch: 34.8 sec]
EPOCH 426/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2958704531673304		[learning rate: 0.00065076]
	Learning Rate: 0.00065076
	LOSS [training: 0.2958704531673304 | validation: 0.3586949952097464]
	TIME [epoch: 34.8 sec]
EPOCH 427/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2886167744769419		[learning rate: 0.00064605]
	Learning Rate: 0.000646046
	LOSS [training: 0.2886167744769419 | validation: 0.34355159518367906]
	TIME [epoch: 34.8 sec]
EPOCH 428/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3305420328111495		[learning rate: 0.00064137]
	Learning Rate: 0.000641365
	LOSS [training: 0.3305420328111495 | validation: 0.33568762183489304]
	TIME [epoch: 34.8 sec]
EPOCH 429/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29862667851780056		[learning rate: 0.00063672]
	Learning Rate: 0.000636718
	LOSS [training: 0.29862667851780056 | validation: 0.3290800164413599]
	TIME [epoch: 34.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_429.pth
	Model improved!!!
EPOCH 430/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3047452669643962		[learning rate: 0.00063211]
	Learning Rate: 0.000632105
	LOSS [training: 0.3047452669643962 | validation: 0.34632489844004]
	TIME [epoch: 34.8 sec]
EPOCH 431/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2986212880789765		[learning rate: 0.00062753]
	Learning Rate: 0.000627526
	LOSS [training: 0.2986212880789765 | validation: 0.34924757869375167]
	TIME [epoch: 34.8 sec]
EPOCH 432/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3009343574002493		[learning rate: 0.00062298]
	Learning Rate: 0.000622979
	LOSS [training: 0.3009343574002493 | validation: 0.3543182412855662]
	TIME [epoch: 34.7 sec]
EPOCH 433/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2979179758113677		[learning rate: 0.00061847]
	Learning Rate: 0.000618466
	LOSS [training: 0.2979179758113677 | validation: 0.3495165480702611]
	TIME [epoch: 34.8 sec]
EPOCH 434/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2951707856261085		[learning rate: 0.00061399]
	Learning Rate: 0.000613985
	LOSS [training: 0.2951707856261085 | validation: 0.3371462446689544]
	TIME [epoch: 34.8 sec]
EPOCH 435/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29653812296716897		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.29653812296716897 | validation: 0.3344226237866427]
	TIME [epoch: 34.8 sec]
EPOCH 436/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.283616948913598		[learning rate: 0.00060512]
	Learning Rate: 0.000605121
	LOSS [training: 0.283616948913598 | validation: 0.3801975597954329]
	TIME [epoch: 34.8 sec]
EPOCH 437/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.31977157746419016		[learning rate: 0.00060074]
	Learning Rate: 0.000600737
	LOSS [training: 0.31977157746419016 | validation: 0.3350749911866051]
	TIME [epoch: 34.8 sec]
EPOCH 438/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28555055433446036		[learning rate: 0.00059638]
	Learning Rate: 0.000596384
	LOSS [training: 0.28555055433446036 | validation: 0.35020125093092225]
	TIME [epoch: 34.8 sec]
EPOCH 439/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3004271369408802		[learning rate: 0.00059206]
	Learning Rate: 0.000592064
	LOSS [training: 0.3004271369408802 | validation: 0.35197324166571753]
	TIME [epoch: 34.7 sec]
EPOCH 440/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29615044253550454		[learning rate: 0.00058777]
	Learning Rate: 0.000587774
	LOSS [training: 0.29615044253550454 | validation: 0.3325014872732663]
	TIME [epoch: 34.8 sec]
EPOCH 441/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.3065953722181206		[learning rate: 0.00058352]
	Learning Rate: 0.000583516
	LOSS [training: 0.3065953722181206 | validation: 0.36573659824543575]
	TIME [epoch: 34.8 sec]
EPOCH 442/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2901394447857429		[learning rate: 0.00057929]
	Learning Rate: 0.000579288
	LOSS [training: 0.2901394447857429 | validation: 0.34613632684411094]
	TIME [epoch: 34.8 sec]
EPOCH 443/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29502077314116704		[learning rate: 0.00057509]
	Learning Rate: 0.000575091
	LOSS [training: 0.29502077314116704 | validation: 0.3260467034482528]
	TIME [epoch: 34.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_443.pth
	Model improved!!!
EPOCH 444/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2893183593847931		[learning rate: 0.00057093]
	Learning Rate: 0.000570925
	LOSS [training: 0.2893183593847931 | validation: 0.34881857119720716]
	TIME [epoch: 34.8 sec]
EPOCH 445/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29534480842094324		[learning rate: 0.00056679]
	Learning Rate: 0.000566789
	LOSS [training: 0.29534480842094324 | validation: 0.34365551942638095]
	TIME [epoch: 34.8 sec]
EPOCH 446/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2801956384585378		[learning rate: 0.00056268]
	Learning Rate: 0.000562682
	LOSS [training: 0.2801956384585378 | validation: 0.35664881433687523]
	TIME [epoch: 34.8 sec]
EPOCH 447/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.30158904455135266		[learning rate: 0.00055861]
	Learning Rate: 0.000558606
	LOSS [training: 0.30158904455135266 | validation: 0.31998359994748793]
	TIME [epoch: 34.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_447.pth
	Model improved!!!
EPOCH 448/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2845419582413154		[learning rate: 0.00055456]
	Learning Rate: 0.000554559
	LOSS [training: 0.2845419582413154 | validation: 0.3330254289744864]
	TIME [epoch: 34.8 sec]
EPOCH 449/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29392044969322173		[learning rate: 0.00055054]
	Learning Rate: 0.000550541
	LOSS [training: 0.29392044969322173 | validation: 0.36015698663040324]
	TIME [epoch: 34.8 sec]
EPOCH 450/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.29262288971707506		[learning rate: 0.00054655]
	Learning Rate: 0.000546552
	LOSS [training: 0.29262288971707506 | validation: 0.33717602898881416]
	TIME [epoch: 34.8 sec]
EPOCH 451/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.279171500355598		[learning rate: 0.00054259]
	Learning Rate: 0.000542592
	LOSS [training: 0.279171500355598 | validation: 0.3297422657426181]
	TIME [epoch: 34.8 sec]
EPOCH 452/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2898044336782546		[learning rate: 0.00053866]
	Learning Rate: 0.000538661
	LOSS [training: 0.2898044336782546 | validation: 0.3805896524916132]
	TIME [epoch: 34.8 sec]
EPOCH 453/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2892447983341053		[learning rate: 0.00053476]
	Learning Rate: 0.000534759
	LOSS [training: 0.2892447983341053 | validation: 0.3305439460643337]
	TIME [epoch: 34.8 sec]
EPOCH 454/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2803056934742184		[learning rate: 0.00053088]
	Learning Rate: 0.000530884
	LOSS [training: 0.2803056934742184 | validation: 0.33636203047283897]
	TIME [epoch: 34.8 sec]
EPOCH 455/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2975339571505524		[learning rate: 0.00052704]
	Learning Rate: 0.000527038
	LOSS [training: 0.2975339571505524 | validation: 0.33778678683443986]
	TIME [epoch: 34.8 sec]
EPOCH 456/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28769277764487744		[learning rate: 0.00052322]
	Learning Rate: 0.00052322
	LOSS [training: 0.28769277764487744 | validation: 0.32598361500434514]
	TIME [epoch: 34.8 sec]
EPOCH 457/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2828612932418706		[learning rate: 0.00051943]
	Learning Rate: 0.000519429
	LOSS [training: 0.2828612932418706 | validation: 0.35701248721626866]
	TIME [epoch: 34.8 sec]
EPOCH 458/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28985319526528186		[learning rate: 0.00051567]
	Learning Rate: 0.000515666
	LOSS [training: 0.28985319526528186 | validation: 0.3254912036522535]
	TIME [epoch: 34.8 sec]
EPOCH 459/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28382316983038136		[learning rate: 0.00051193]
	Learning Rate: 0.00051193
	LOSS [training: 0.28382316983038136 | validation: 0.3365586937484816]
	TIME [epoch: 34.8 sec]
EPOCH 460/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2903979152027788		[learning rate: 0.00050822]
	Learning Rate: 0.000508221
	LOSS [training: 0.2903979152027788 | validation: 0.3211822498413147]
	TIME [epoch: 34.8 sec]
EPOCH 461/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2729991673543508		[learning rate: 0.00050454]
	Learning Rate: 0.000504539
	LOSS [training: 0.2729991673543508 | validation: 0.32931471334845164]
	TIME [epoch: 34.8 sec]
EPOCH 462/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2779327786895548		[learning rate: 0.00050088]
	Learning Rate: 0.000500884
	LOSS [training: 0.2779327786895548 | validation: 0.3283420256583329]
	TIME [epoch: 34.8 sec]
EPOCH 463/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2745854076094949		[learning rate: 0.00049725]
	Learning Rate: 0.000497255
	LOSS [training: 0.2745854076094949 | validation: 0.33188946196672403]
	TIME [epoch: 34.8 sec]
EPOCH 464/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28997895789790046		[learning rate: 0.00049365]
	Learning Rate: 0.000493652
	LOSS [training: 0.28997895789790046 | validation: 0.3216227768097055]
	TIME [epoch: 34.8 sec]
EPOCH 465/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2766066938637495		[learning rate: 0.00049008]
	Learning Rate: 0.000490076
	LOSS [training: 0.2766066938637495 | validation: 0.3066652694526713]
	TIME [epoch: 34.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_465.pth
	Model improved!!!
EPOCH 466/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2746338303053327		[learning rate: 0.00048653]
	Learning Rate: 0.000486525
	LOSS [training: 0.2746338303053327 | validation: 0.3256988912876586]
	TIME [epoch: 34.8 sec]
EPOCH 467/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2786223806982022		[learning rate: 0.000483]
	Learning Rate: 0.000483
	LOSS [training: 0.2786223806982022 | validation: 0.3164056379050293]
	TIME [epoch: 34.8 sec]
EPOCH 468/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2895739349716042		[learning rate: 0.0004795]
	Learning Rate: 0.000479501
	LOSS [training: 0.2895739349716042 | validation: 0.32122158865935097]
	TIME [epoch: 34.8 sec]
EPOCH 469/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2659555653545972		[learning rate: 0.00047603]
	Learning Rate: 0.000476027
	LOSS [training: 0.2659555653545972 | validation: 0.3197492650055823]
	TIME [epoch: 36.1 sec]
EPOCH 470/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2866655333628031		[learning rate: 0.00047258]
	Learning Rate: 0.000472578
	LOSS [training: 0.2866655333628031 | validation: 0.33190683329205733]
	TIME [epoch: 34.7 sec]
EPOCH 471/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27892657248667774		[learning rate: 0.00046915]
	Learning Rate: 0.000469154
	LOSS [training: 0.27892657248667774 | validation: 0.3138808142085708]
	TIME [epoch: 34.8 sec]
EPOCH 472/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2804407325573066		[learning rate: 0.00046576]
	Learning Rate: 0.000465755
	LOSS [training: 0.2804407325573066 | validation: 0.32229565717250147]
	TIME [epoch: 34.8 sec]
EPOCH 473/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2750473957768502		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.2750473957768502 | validation: 0.34101268504694776]
	TIME [epoch: 34.8 sec]
EPOCH 474/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.28281778002600055		[learning rate: 0.00045903]
	Learning Rate: 0.000459031
	LOSS [training: 0.28281778002600055 | validation: 0.3200618724128508]
	TIME [epoch: 34.7 sec]
EPOCH 475/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27157238314820364		[learning rate: 0.00045571]
	Learning Rate: 0.000455706
	LOSS [training: 0.27157238314820364 | validation: 0.3136417548446895]
	TIME [epoch: 34.8 sec]
EPOCH 476/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2752743730012164		[learning rate: 0.0004524]
	Learning Rate: 0.000452404
	LOSS [training: 0.2752743730012164 | validation: 0.3433875705460966]
	TIME [epoch: 34.8 sec]
EPOCH 477/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2917908771311018		[learning rate: 0.00044913]
	Learning Rate: 0.000449126
	LOSS [training: 0.2917908771311018 | validation: 0.30625868958386393]
	TIME [epoch: 34.7 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_477.pth
	Model improved!!!
EPOCH 478/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2722038906347954		[learning rate: 0.00044587]
	Learning Rate: 0.000445872
	LOSS [training: 0.2722038906347954 | validation: 0.32045568319680773]
	TIME [epoch: 34.8 sec]
EPOCH 479/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2769889682922084		[learning rate: 0.00044264]
	Learning Rate: 0.000442642
	LOSS [training: 0.2769889682922084 | validation: 0.328452981970931]
	TIME [epoch: 34.8 sec]
EPOCH 480/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2692088786027054		[learning rate: 0.00043944]
	Learning Rate: 0.000439435
	LOSS [training: 0.2692088786027054 | validation: 0.33034400437614464]
	TIME [epoch: 34.8 sec]
EPOCH 481/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.276479296756694		[learning rate: 0.00043625]
	Learning Rate: 0.000436251
	LOSS [training: 0.276479296756694 | validation: 0.3224615376407104]
	TIME [epoch: 34.8 sec]
EPOCH 482/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27038758609757174		[learning rate: 0.00043309]
	Learning Rate: 0.000433091
	LOSS [training: 0.27038758609757174 | validation: 0.3228592892018828]
	TIME [epoch: 34.8 sec]
EPOCH 483/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27492481508572314		[learning rate: 0.00042995]
	Learning Rate: 0.000429953
	LOSS [training: 0.27492481508572314 | validation: 0.3181321086628792]
	TIME [epoch: 34.8 sec]
EPOCH 484/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2735298124916545		[learning rate: 0.00042684]
	Learning Rate: 0.000426838
	LOSS [training: 0.2735298124916545 | validation: 0.32274573237721216]
	TIME [epoch: 34.7 sec]
EPOCH 485/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26796848110756616		[learning rate: 0.00042375]
	Learning Rate: 0.000423746
	LOSS [training: 0.26796848110756616 | validation: 0.31739221274798346]
	TIME [epoch: 34.8 sec]
EPOCH 486/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2741279978094123		[learning rate: 0.00042068]
	Learning Rate: 0.000420676
	LOSS [training: 0.2741279978094123 | validation: 0.3149635380777108]
	TIME [epoch: 34.8 sec]
EPOCH 487/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2703027272424505		[learning rate: 0.00041763]
	Learning Rate: 0.000417628
	LOSS [training: 0.2703027272424505 | validation: 0.3270061607877191]
	TIME [epoch: 34.8 sec]
EPOCH 488/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2692393890015317		[learning rate: 0.0004146]
	Learning Rate: 0.000414602
	LOSS [training: 0.2692393890015317 | validation: 0.3131379858821862]
	TIME [epoch: 34.8 sec]
EPOCH 489/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27295510114278315		[learning rate: 0.0004116]
	Learning Rate: 0.000411598
	LOSS [training: 0.27295510114278315 | validation: 0.3104359264739024]
	TIME [epoch: 34.8 sec]
EPOCH 490/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27112320914109406		[learning rate: 0.00040862]
	Learning Rate: 0.000408616
	LOSS [training: 0.27112320914109406 | validation: 0.3232859882766844]
	TIME [epoch: 34.8 sec]
EPOCH 491/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2693555928832694		[learning rate: 0.00040566]
	Learning Rate: 0.000405656
	LOSS [training: 0.2693555928832694 | validation: 0.30849393109849277]
	TIME [epoch: 34.7 sec]
EPOCH 492/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2654334296312642		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.2654334296312642 | validation: 0.3150598305114592]
	TIME [epoch: 34.8 sec]
EPOCH 493/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27340390541837506		[learning rate: 0.0003998]
	Learning Rate: 0.000399799
	LOSS [training: 0.27340390541837506 | validation: 0.302493851337269]
	TIME [epoch: 34.8 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_493.pth
	Model improved!!!
EPOCH 494/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2595959772581679		[learning rate: 0.0003969]
	Learning Rate: 0.000396903
	LOSS [training: 0.2595959772581679 | validation: 0.31833098380454083]
	TIME [epoch: 34.8 sec]
EPOCH 495/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2630911811648561		[learning rate: 0.00039403]
	Learning Rate: 0.000394027
	LOSS [training: 0.2630911811648561 | validation: 0.3398874154346748]
	TIME [epoch: 34.8 sec]
EPOCH 496/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2847591159164118		[learning rate: 0.00039117]
	Learning Rate: 0.000391173
	LOSS [training: 0.2847591159164118 | validation: 0.31821124433672965]
	TIME [epoch: 34.8 sec]
EPOCH 497/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26487684486351654		[learning rate: 0.00038834]
	Learning Rate: 0.000388339
	LOSS [training: 0.26487684486351654 | validation: 0.31769682457320547]
	TIME [epoch: 34.8 sec]
EPOCH 498/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26382438169867206		[learning rate: 0.00038553]
	Learning Rate: 0.000385525
	LOSS [training: 0.26382438169867206 | validation: 0.30269533326745646]
	TIME [epoch: 34.7 sec]
EPOCH 499/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.263843634462952		[learning rate: 0.00038273]
	Learning Rate: 0.000382732
	LOSS [training: 0.263843634462952 | validation: 0.3079394592791783]
	TIME [epoch: 34.8 sec]
EPOCH 500/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2661096406755729		[learning rate: 0.00037996]
	Learning Rate: 0.000379959
	LOSS [training: 0.2661096406755729 | validation: 0.3248449709822654]
	TIME [epoch: 34.8 sec]
EPOCH 501/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26799351096772484		[learning rate: 0.00037721]
	Learning Rate: 0.000377206
	LOSS [training: 0.26799351096772484 | validation: 0.319723522391821]
	TIME [epoch: 164 sec]
EPOCH 502/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2726669218906487		[learning rate: 0.00037447]
	Learning Rate: 0.000374474
	LOSS [training: 0.2726669218906487 | validation: 0.3063896247364687]
	TIME [epoch: 74.7 sec]
EPOCH 503/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2579458873985856		[learning rate: 0.00037176]
	Learning Rate: 0.00037176
	LOSS [training: 0.2579458873985856 | validation: 0.3167138659087964]
	TIME [epoch: 74.5 sec]
EPOCH 504/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2717333839923543		[learning rate: 0.00036907]
	Learning Rate: 0.000369067
	LOSS [training: 0.2717333839923543 | validation: 0.31078947689877756]
	TIME [epoch: 74.5 sec]
EPOCH 505/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2625431984415567		[learning rate: 0.00036639]
	Learning Rate: 0.000366393
	LOSS [training: 0.2625431984415567 | validation: 0.31769386007090983]
	TIME [epoch: 74.5 sec]
EPOCH 506/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26534059373960295		[learning rate: 0.00036374]
	Learning Rate: 0.000363739
	LOSS [training: 0.26534059373960295 | validation: 0.3037715580987621]
	TIME [epoch: 74.5 sec]
EPOCH 507/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25910763291435773		[learning rate: 0.0003611]
	Learning Rate: 0.000361103
	LOSS [training: 0.25910763291435773 | validation: 0.307557066951051]
	TIME [epoch: 74.5 sec]
EPOCH 508/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2630125860548687		[learning rate: 0.00035849]
	Learning Rate: 0.000358487
	LOSS [training: 0.2630125860548687 | validation: 0.29782765123506516]
	TIME [epoch: 74.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_508.pth
	Model improved!!!
EPOCH 509/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2588512908431191		[learning rate: 0.00035589]
	Learning Rate: 0.00035589
	LOSS [training: 0.2588512908431191 | validation: 0.3032256939903218]
	TIME [epoch: 74.5 sec]
EPOCH 510/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26868552705532284		[learning rate: 0.00035331]
	Learning Rate: 0.000353312
	LOSS [training: 0.26868552705532284 | validation: 0.31712504072270953]
	TIME [epoch: 74.5 sec]
EPOCH 511/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25700815396242677		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.25700815396242677 | validation: 0.30175258927973353]
	TIME [epoch: 74.5 sec]
EPOCH 512/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26357649705970765		[learning rate: 0.00034821]
	Learning Rate: 0.000348211
	LOSS [training: 0.26357649705970765 | validation: 0.30283006607486496]
	TIME [epoch: 74.5 sec]
EPOCH 513/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2597400757044937		[learning rate: 0.00034569]
	Learning Rate: 0.000345688
	LOSS [training: 0.2597400757044937 | validation: 0.32416343077175436]
	TIME [epoch: 74.5 sec]
EPOCH 514/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2673834898788058		[learning rate: 0.00034318]
	Learning Rate: 0.000343183
	LOSS [training: 0.2673834898788058 | validation: 0.3008849678391271]
	TIME [epoch: 74.6 sec]
EPOCH 515/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2591274821317139		[learning rate: 0.0003407]
	Learning Rate: 0.000340697
	LOSS [training: 0.2591274821317139 | validation: 0.3268288051017345]
	TIME [epoch: 74.6 sec]
EPOCH 516/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2699245370300523		[learning rate: 0.00033823]
	Learning Rate: 0.000338229
	LOSS [training: 0.2699245370300523 | validation: 0.2937160236676466]
	TIME [epoch: 74.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_516.pth
	Model improved!!!
EPOCH 517/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25658406822428925		[learning rate: 0.00033578]
	Learning Rate: 0.000335778
	LOSS [training: 0.25658406822428925 | validation: 0.2959156251074775]
	TIME [epoch: 74.5 sec]
EPOCH 518/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2586955488087483		[learning rate: 0.00033335]
	Learning Rate: 0.000333346
	LOSS [training: 0.2586955488087483 | validation: 0.30053131163992186]
	TIME [epoch: 74.5 sec]
EPOCH 519/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2599339808474029		[learning rate: 0.00033093]
	Learning Rate: 0.000330931
	LOSS [training: 0.2599339808474029 | validation: 0.3180905140838951]
	TIME [epoch: 74.5 sec]
EPOCH 520/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.27028397529605197		[learning rate: 0.00032853]
	Learning Rate: 0.000328533
	LOSS [training: 0.27028397529605197 | validation: 0.30495499476615157]
	TIME [epoch: 74.5 sec]
EPOCH 521/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25575137463241826		[learning rate: 0.00032615]
	Learning Rate: 0.000326153
	LOSS [training: 0.25575137463241826 | validation: 0.2977245819636636]
	TIME [epoch: 74.5 sec]
EPOCH 522/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2521303163433672		[learning rate: 0.00032379]
	Learning Rate: 0.00032379
	LOSS [training: 0.2521303163433672 | validation: 0.2981611574347339]
	TIME [epoch: 74.5 sec]
EPOCH 523/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26553533708710814		[learning rate: 0.00032144]
	Learning Rate: 0.000321444
	LOSS [training: 0.26553533708710814 | validation: 0.31767830880069736]
	TIME [epoch: 74.5 sec]
EPOCH 524/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26828457142490636		[learning rate: 0.00031912]
	Learning Rate: 0.000319115
	LOSS [training: 0.26828457142490636 | validation: 0.30151180914954834]
	TIME [epoch: 74.5 sec]
EPOCH 525/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2679744432735793		[learning rate: 0.0003168]
	Learning Rate: 0.000316803
	LOSS [training: 0.2679744432735793 | validation: 0.29759873549598914]
	TIME [epoch: 74.5 sec]
EPOCH 526/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2528536304408821		[learning rate: 0.00031451]
	Learning Rate: 0.000314508
	LOSS [training: 0.2528536304408821 | validation: 0.3000656247334832]
	TIME [epoch: 74.5 sec]
EPOCH 527/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25269059553691625		[learning rate: 0.00031223]
	Learning Rate: 0.000312229
	LOSS [training: 0.25269059553691625 | validation: 0.293962456013729]
	TIME [epoch: 74.5 sec]
EPOCH 528/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2584731790609118		[learning rate: 0.00030997]
	Learning Rate: 0.000309967
	LOSS [training: 0.2584731790609118 | validation: 0.3051975221881822]
	TIME [epoch: 74.5 sec]
EPOCH 529/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25854053196533827		[learning rate: 0.00030772]
	Learning Rate: 0.000307722
	LOSS [training: 0.25854053196533827 | validation: 0.296514649363085]
	TIME [epoch: 74.5 sec]
EPOCH 530/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2676690610246286		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.2676690610246286 | validation: 0.29608795909487473]
	TIME [epoch: 74.5 sec]
EPOCH 531/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25506370860341937		[learning rate: 0.00030328]
	Learning Rate: 0.000303279
	LOSS [training: 0.25506370860341937 | validation: 0.29511491735134965]
	TIME [epoch: 74.5 sec]
EPOCH 532/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25239077597646653		[learning rate: 0.00030108]
	Learning Rate: 0.000301082
	LOSS [training: 0.25239077597646653 | validation: 0.29846514711664374]
	TIME [epoch: 74.5 sec]
EPOCH 533/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26760967383768447		[learning rate: 0.0002989]
	Learning Rate: 0.0002989
	LOSS [training: 0.26760967383768447 | validation: 0.2943402465421892]
	TIME [epoch: 74.5 sec]
EPOCH 534/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2534113883011413		[learning rate: 0.00029673]
	Learning Rate: 0.000296735
	LOSS [training: 0.2534113883011413 | validation: 0.29686719442644893]
	TIME [epoch: 74.5 sec]
EPOCH 535/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2540875236961907		[learning rate: 0.00029458]
	Learning Rate: 0.000294585
	LOSS [training: 0.2540875236961907 | validation: 0.3120708117630434]
	TIME [epoch: 74.6 sec]
EPOCH 536/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25376144863820266		[learning rate: 0.00029245]
	Learning Rate: 0.000292451
	LOSS [training: 0.25376144863820266 | validation: 0.30244674842025143]
	TIME [epoch: 74.6 sec]
EPOCH 537/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2529406423135814		[learning rate: 0.00029033]
	Learning Rate: 0.000290332
	LOSS [training: 0.2529406423135814 | validation: 0.29595112478542485]
	TIME [epoch: 74.6 sec]
EPOCH 538/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.26515001304979374		[learning rate: 0.00028823]
	Learning Rate: 0.000288228
	LOSS [training: 0.26515001304979374 | validation: 0.29844480064445034]
	TIME [epoch: 74.5 sec]
EPOCH 539/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24854390165532664		[learning rate: 0.00028614]
	Learning Rate: 0.00028614
	LOSS [training: 0.24854390165532664 | validation: 0.2995920965908128]
	TIME [epoch: 74.5 sec]
EPOCH 540/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2597589333147379		[learning rate: 0.00028407]
	Learning Rate: 0.000284067
	LOSS [training: 0.2597589333147379 | validation: 0.32093615424521793]
	TIME [epoch: 74.5 sec]
EPOCH 541/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25578987828940847		[learning rate: 0.00028201]
	Learning Rate: 0.000282009
	LOSS [training: 0.25578987828940847 | validation: 0.2952455194808348]
	TIME [epoch: 74.6 sec]
EPOCH 542/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25354043924253467		[learning rate: 0.00027997]
	Learning Rate: 0.000279966
	LOSS [training: 0.25354043924253467 | validation: 0.2963515309517163]
	TIME [epoch: 74.6 sec]
EPOCH 543/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2581276467461693		[learning rate: 0.00027794]
	Learning Rate: 0.000277938
	LOSS [training: 0.2581276467461693 | validation: 0.2936633200996712]
	TIME [epoch: 74.6 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_543.pth
	Model improved!!!
EPOCH 544/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2527170763403239		[learning rate: 0.00027592]
	Learning Rate: 0.000275924
	LOSS [training: 0.2527170763403239 | validation: 0.2899306206779396]
	TIME [epoch: 74.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_544.pth
	Model improved!!!
EPOCH 545/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2547067785465785		[learning rate: 0.00027392]
	Learning Rate: 0.000273925
	LOSS [training: 0.2547067785465785 | validation: 0.29236713746691023]
	TIME [epoch: 74.5 sec]
EPOCH 546/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2506301850030701		[learning rate: 0.00027194]
	Learning Rate: 0.00027194
	LOSS [training: 0.2506301850030701 | validation: 0.29141693749569675]
	TIME [epoch: 74.4 sec]
EPOCH 547/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25514970247951607		[learning rate: 0.00026997]
	Learning Rate: 0.00026997
	LOSS [training: 0.25514970247951607 | validation: 0.29381496983810784]
	TIME [epoch: 74.5 sec]
EPOCH 548/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24969335520490038		[learning rate: 0.00026801]
	Learning Rate: 0.000268014
	LOSS [training: 0.24969335520490038 | validation: 0.29942200899689786]
	TIME [epoch: 74.5 sec]
EPOCH 549/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2519808745117008		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.2519808745117008 | validation: 0.2950528479009226]
	TIME [epoch: 74.5 sec]
EPOCH 550/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2496496048755635		[learning rate: 0.00026414]
	Learning Rate: 0.000264145
	LOSS [training: 0.2496496048755635 | validation: 0.2901246849568851]
	TIME [epoch: 74.5 sec]
EPOCH 551/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24992350739034713		[learning rate: 0.00026223]
	Learning Rate: 0.000262231
	LOSS [training: 0.24992350739034713 | validation: 0.2877853316903996]
	TIME [epoch: 74.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_551.pth
	Model improved!!!
EPOCH 552/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2497147615289436		[learning rate: 0.00026033]
	Learning Rate: 0.000260331
	LOSS [training: 0.2497147615289436 | validation: 0.2957216420831954]
	TIME [epoch: 74.4 sec]
EPOCH 553/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25113209286928023		[learning rate: 0.00025845]
	Learning Rate: 0.000258445
	LOSS [training: 0.25113209286928023 | validation: 0.2850974135231574]
	TIME [epoch: 74.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_553.pth
	Model improved!!!
EPOCH 554/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2495131812683281		[learning rate: 0.00025657]
	Learning Rate: 0.000256573
	LOSS [training: 0.2495131812683281 | validation: 0.28647280057367547]
	TIME [epoch: 74.4 sec]
EPOCH 555/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25387318559178657		[learning rate: 0.00025471]
	Learning Rate: 0.000254714
	LOSS [training: 0.25387318559178657 | validation: 0.31114113593708115]
	TIME [epoch: 74.5 sec]
EPOCH 556/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2584639179242153		[learning rate: 0.00025287]
	Learning Rate: 0.000252868
	LOSS [training: 0.2584639179242153 | validation: 0.28683384442762194]
	TIME [epoch: 74.5 sec]
EPOCH 557/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24608884500283368		[learning rate: 0.00025104]
	Learning Rate: 0.000251037
	LOSS [training: 0.24608884500283368 | validation: 0.29938190164212874]
	TIME [epoch: 74.5 sec]
EPOCH 558/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24931888819102221		[learning rate: 0.00024922]
	Learning Rate: 0.000249218
	LOSS [training: 0.24931888819102221 | validation: 0.2914121303150493]
	TIME [epoch: 74.5 sec]
EPOCH 559/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24992514395785853		[learning rate: 0.00024741]
	Learning Rate: 0.000247412
	LOSS [training: 0.24992514395785853 | validation: 0.29446496464603367]
	TIME [epoch: 74.4 sec]
EPOCH 560/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2520773357317788		[learning rate: 0.00024562]
	Learning Rate: 0.00024562
	LOSS [training: 0.2520773357317788 | validation: 0.30180146706672634]
	TIME [epoch: 74.4 sec]
EPOCH 561/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25233661401402896		[learning rate: 0.00024384]
	Learning Rate: 0.00024384
	LOSS [training: 0.25233661401402896 | validation: 0.2889157359704397]
	TIME [epoch: 74.4 sec]
EPOCH 562/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2464462450720294		[learning rate: 0.00024207]
	Learning Rate: 0.000242074
	LOSS [training: 0.2464462450720294 | validation: 0.28645118715192786]
	TIME [epoch: 74.4 sec]
EPOCH 563/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2525807826668005		[learning rate: 0.00024032]
	Learning Rate: 0.00024032
	LOSS [training: 0.2525807826668005 | validation: 0.29879821867985035]
	TIME [epoch: 74.5 sec]
EPOCH 564/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24825144201812652		[learning rate: 0.00023858]
	Learning Rate: 0.000238579
	LOSS [training: 0.24825144201812652 | validation: 0.2901948843105089]
	TIME [epoch: 74.4 sec]
EPOCH 565/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2476818722986561		[learning rate: 0.00023685]
	Learning Rate: 0.00023685
	LOSS [training: 0.2476818722986561 | validation: 0.28857215499903166]
	TIME [epoch: 74.4 sec]
EPOCH 566/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24500924866463114		[learning rate: 0.00023513]
	Learning Rate: 0.000235134
	LOSS [training: 0.24500924866463114 | validation: 0.2912900278626124]
	TIME [epoch: 74.5 sec]
EPOCH 567/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25166441210764623		[learning rate: 0.00023343]
	Learning Rate: 0.000233431
	LOSS [training: 0.25166441210764623 | validation: 0.286099978146679]
	TIME [epoch: 74.5 sec]
EPOCH 568/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24600218583291233		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.24600218583291233 | validation: 0.284652262037612]
	TIME [epoch: 74.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_568.pth
	Model improved!!!
EPOCH 569/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24507009331953025		[learning rate: 0.00023006]
	Learning Rate: 0.000230061
	LOSS [training: 0.24507009331953025 | validation: 0.2911664948846781]
	TIME [epoch: 74.4 sec]
EPOCH 570/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2492299248455687		[learning rate: 0.00022839]
	Learning Rate: 0.000228394
	LOSS [training: 0.2492299248455687 | validation: 0.28127060767287204]
	TIME [epoch: 74.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_570.pth
	Model improved!!!
EPOCH 571/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24636987350520656		[learning rate: 0.00022674]
	Learning Rate: 0.000226739
	LOSS [training: 0.24636987350520656 | validation: 0.2949041550208319]
	TIME [epoch: 74.5 sec]
EPOCH 572/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24936187154571604		[learning rate: 0.0002251]
	Learning Rate: 0.000225096
	LOSS [training: 0.24936187154571604 | validation: 0.2980113208071111]
	TIME [epoch: 74.5 sec]
EPOCH 573/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24983931839086987		[learning rate: 0.00022347]
	Learning Rate: 0.000223466
	LOSS [training: 0.24983931839086987 | validation: 0.2838517357721839]
	TIME [epoch: 74.5 sec]
EPOCH 574/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2449397437900092		[learning rate: 0.00022185]
	Learning Rate: 0.000221847
	LOSS [training: 0.2449397437900092 | validation: 0.2793237105126349]
	TIME [epoch: 74.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_574.pth
	Model improved!!!
EPOCH 575/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.25071245194933595		[learning rate: 0.00022024]
	Learning Rate: 0.000220239
	LOSS [training: 0.25071245194933595 | validation: 0.29356387721636873]
	TIME [epoch: 74.5 sec]
EPOCH 576/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24882833378055402		[learning rate: 0.00021864]
	Learning Rate: 0.000218644
	LOSS [training: 0.24882833378055402 | validation: 0.2972534418899132]
	TIME [epoch: 74.5 sec]
EPOCH 577/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24914276472218247		[learning rate: 0.00021706]
	Learning Rate: 0.00021706
	LOSS [training: 0.24914276472218247 | validation: 0.2891882421668083]
	TIME [epoch: 74.5 sec]
EPOCH 578/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2449768802374518		[learning rate: 0.00021549]
	Learning Rate: 0.000215487
	LOSS [training: 0.2449768802374518 | validation: 0.28755315600627696]
	TIME [epoch: 74.5 sec]
EPOCH 579/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24857764467659701		[learning rate: 0.00021393]
	Learning Rate: 0.000213926
	LOSS [training: 0.24857764467659701 | validation: 0.2843287842119877]
	TIME [epoch: 74.5 sec]
EPOCH 580/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24384788151010245		[learning rate: 0.00021238]
	Learning Rate: 0.000212376
	LOSS [training: 0.24384788151010245 | validation: 0.2850347784214907]
	TIME [epoch: 74.5 sec]
EPOCH 581/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2463941244481762		[learning rate: 0.00021084]
	Learning Rate: 0.000210837
	LOSS [training: 0.2463941244481762 | validation: 0.28545167318235337]
	TIME [epoch: 74.5 sec]
EPOCH 582/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2508231595237034		[learning rate: 0.00020931]
	Learning Rate: 0.00020931
	LOSS [training: 0.2508231595237034 | validation: 0.2836674333818771]
	TIME [epoch: 74.5 sec]
EPOCH 583/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24552614531966577		[learning rate: 0.00020779]
	Learning Rate: 0.000207793
	LOSS [training: 0.24552614531966577 | validation: 0.29225458941453986]
	TIME [epoch: 74.5 sec]
EPOCH 584/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2449608252753134		[learning rate: 0.00020629]
	Learning Rate: 0.000206288
	LOSS [training: 0.2449608252753134 | validation: 0.2877688848289446]
	TIME [epoch: 74.5 sec]
EPOCH 585/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24728404265988244		[learning rate: 0.00020479]
	Learning Rate: 0.000204793
	LOSS [training: 0.24728404265988244 | validation: 0.2824741741711307]
	TIME [epoch: 74.5 sec]
EPOCH 586/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24253254098998459		[learning rate: 0.00020331]
	Learning Rate: 0.00020331
	LOSS [training: 0.24253254098998459 | validation: 0.28238798406238796]
	TIME [epoch: 74.5 sec]
EPOCH 587/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.243490700166818		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 0.243490700166818 | validation: 0.2886785062863111]
	TIME [epoch: 74.5 sec]
EPOCH 588/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24912503982214212		[learning rate: 0.00020037]
	Learning Rate: 0.000200374
	LOSS [training: 0.24912503982214212 | validation: 0.2839528437889207]
	TIME [epoch: 74.5 sec]
EPOCH 589/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24374574397746962		[learning rate: 0.00019892]
	Learning Rate: 0.000198923
	LOSS [training: 0.24374574397746962 | validation: 0.2986855786289851]
	TIME [epoch: 74.5 sec]
EPOCH 590/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2586762748944563		[learning rate: 0.00019748]
	Learning Rate: 0.000197482
	LOSS [training: 0.2586762748944563 | validation: 0.29529260635430693]
	TIME [epoch: 74.5 sec]
EPOCH 591/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24863787291077555		[learning rate: 0.00019605]
	Learning Rate: 0.000196051
	LOSS [training: 0.24863787291077555 | validation: 0.2852729374051416]
	TIME [epoch: 74.5 sec]
EPOCH 592/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24406109020473304		[learning rate: 0.00019463]
	Learning Rate: 0.00019463
	LOSS [training: 0.24406109020473304 | validation: 0.2838960941134324]
	TIME [epoch: 74.5 sec]
EPOCH 593/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24370440584529174		[learning rate: 0.00019322]
	Learning Rate: 0.00019322
	LOSS [training: 0.24370440584529174 | validation: 0.28066953924048643]
	TIME [epoch: 74.5 sec]
EPOCH 594/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23845856612475116		[learning rate: 0.00019182]
	Learning Rate: 0.00019182
	LOSS [training: 0.23845856612475116 | validation: 0.2809568817451806]
	TIME [epoch: 74.5 sec]
EPOCH 595/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2437269796525499		[learning rate: 0.00019043]
	Learning Rate: 0.000190431
	LOSS [training: 0.2437269796525499 | validation: 0.27930705613431445]
	TIME [epoch: 74.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_595.pth
	Model improved!!!
EPOCH 596/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2437530158516034		[learning rate: 0.00018905]
	Learning Rate: 0.000189051
	LOSS [training: 0.2437530158516034 | validation: 0.288820348771922]
	TIME [epoch: 74.5 sec]
EPOCH 597/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24762937846168812		[learning rate: 0.00018768]
	Learning Rate: 0.000187681
	LOSS [training: 0.24762937846168812 | validation: 0.2725776626199765]
	TIME [epoch: 74.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_597.pth
	Model improved!!!
EPOCH 598/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24063972922599108		[learning rate: 0.00018632]
	Learning Rate: 0.000186322
	LOSS [training: 0.24063972922599108 | validation: 0.28131205832030126]
	TIME [epoch: 74.5 sec]
EPOCH 599/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2403126111708216		[learning rate: 0.00018497]
	Learning Rate: 0.000184972
	LOSS [training: 0.2403126111708216 | validation: 0.28138847074051543]
	TIME [epoch: 74.5 sec]
EPOCH 600/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24431726042263868		[learning rate: 0.00018363]
	Learning Rate: 0.000183632
	LOSS [training: 0.24431726042263868 | validation: 0.28349259194216225]
	TIME [epoch: 74.4 sec]
EPOCH 601/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24307549211633456		[learning rate: 0.0001823]
	Learning Rate: 0.000182301
	LOSS [training: 0.24307549211633456 | validation: 0.2801032291320826]
	TIME [epoch: 74.5 sec]
EPOCH 602/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23977397863426786		[learning rate: 0.00018098]
	Learning Rate: 0.00018098
	LOSS [training: 0.23977397863426786 | validation: 0.281393945230591]
	TIME [epoch: 74.5 sec]
EPOCH 603/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24444568507429473		[learning rate: 0.00017967]
	Learning Rate: 0.000179669
	LOSS [training: 0.24444568507429473 | validation: 0.2848181913244755]
	TIME [epoch: 74.5 sec]
EPOCH 604/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24396798774287368		[learning rate: 0.00017837]
	Learning Rate: 0.000178368
	LOSS [training: 0.24396798774287368 | validation: 0.28789545094432384]
	TIME [epoch: 74.5 sec]
EPOCH 605/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24360568489578377		[learning rate: 0.00017708]
	Learning Rate: 0.000177075
	LOSS [training: 0.24360568489578377 | validation: 0.28543986290914575]
	TIME [epoch: 74.4 sec]
EPOCH 606/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24403670817325526		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 0.24403670817325526 | validation: 0.27834368679824156]
	TIME [epoch: 74.4 sec]
EPOCH 607/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2460138188532508		[learning rate: 0.00017452]
	Learning Rate: 0.000174519
	LOSS [training: 0.2460138188532508 | validation: 0.2829355152643762]
	TIME [epoch: 74.4 sec]
EPOCH 608/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.241774918808519		[learning rate: 0.00017325]
	Learning Rate: 0.000173254
	LOSS [training: 0.241774918808519 | validation: 0.2812194108128602]
	TIME [epoch: 74.5 sec]
EPOCH 609/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24105911057848733		[learning rate: 0.000172]
	Learning Rate: 0.000171999
	LOSS [training: 0.24105911057848733 | validation: 0.27897895451023236]
	TIME [epoch: 74.5 sec]
EPOCH 610/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24058844955274578		[learning rate: 0.00017075]
	Learning Rate: 0.000170753
	LOSS [training: 0.24058844955274578 | validation: 0.27918203249820045]
	TIME [epoch: 74.4 sec]
EPOCH 611/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24053545772631085		[learning rate: 0.00016952]
	Learning Rate: 0.000169516
	LOSS [training: 0.24053545772631085 | validation: 0.27601331572928983]
	TIME [epoch: 74.4 sec]
EPOCH 612/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2407623363908071		[learning rate: 0.00016829]
	Learning Rate: 0.000168288
	LOSS [training: 0.2407623363908071 | validation: 0.2802043873355944]
	TIME [epoch: 74.4 sec]
EPOCH 613/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24237068531868988		[learning rate: 0.00016707]
	Learning Rate: 0.000167069
	LOSS [training: 0.24237068531868988 | validation: 0.2792762900181373]
	TIME [epoch: 74.5 sec]
EPOCH 614/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2438396964153001		[learning rate: 0.00016586]
	Learning Rate: 0.000165858
	LOSS [training: 0.2438396964153001 | validation: 0.27795227059591693]
	TIME [epoch: 74.5 sec]
EPOCH 615/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24135815699484442		[learning rate: 0.00016466]
	Learning Rate: 0.000164657
	LOSS [training: 0.24135815699484442 | validation: 0.28156914415359513]
	TIME [epoch: 74.4 sec]
EPOCH 616/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2381712290385558		[learning rate: 0.00016346]
	Learning Rate: 0.000163464
	LOSS [training: 0.2381712290385558 | validation: 0.2823852550199911]
	TIME [epoch: 74.4 sec]
EPOCH 617/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24001249861676402		[learning rate: 0.00016228]
	Learning Rate: 0.000162279
	LOSS [training: 0.24001249861676402 | validation: 0.27682572814272444]
	TIME [epoch: 74.4 sec]
EPOCH 618/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23695612445098227		[learning rate: 0.0001611]
	Learning Rate: 0.000161104
	LOSS [training: 0.23695612445098227 | validation: 0.2811454672797232]
	TIME [epoch: 74.4 sec]
EPOCH 619/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2436773658726829		[learning rate: 0.00015994]
	Learning Rate: 0.000159936
	LOSS [training: 0.2436773658726829 | validation: 0.27906776591798255]
	TIME [epoch: 74.5 sec]
EPOCH 620/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2419172706554871		[learning rate: 0.00015878]
	Learning Rate: 0.000158778
	LOSS [training: 0.2419172706554871 | validation: 0.2805195908455782]
	TIME [epoch: 74.5 sec]
EPOCH 621/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2382010383981005		[learning rate: 0.00015763]
	Learning Rate: 0.000157627
	LOSS [training: 0.2382010383981005 | validation: 0.2774660460508224]
	TIME [epoch: 74.6 sec]
EPOCH 622/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2418433334454314		[learning rate: 0.00015649]
	Learning Rate: 0.000156485
	LOSS [training: 0.2418433334454314 | validation: 0.277461154635097]
	TIME [epoch: 74.4 sec]
EPOCH 623/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24219189585871903		[learning rate: 0.00015535]
	Learning Rate: 0.000155352
	LOSS [training: 0.24219189585871903 | validation: 0.2827907948314472]
	TIME [epoch: 74.4 sec]
EPOCH 624/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2404826774161222		[learning rate: 0.00015423]
	Learning Rate: 0.000154226
	LOSS [training: 0.2404826774161222 | validation: 0.27762333178301096]
	TIME [epoch: 74.5 sec]
EPOCH 625/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24000775901441238		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 0.24000775901441238 | validation: 0.28759538804996604]
	TIME [epoch: 74.4 sec]
EPOCH 626/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24136877191410352		[learning rate: 0.000152]
	Learning Rate: 0.000152
	LOSS [training: 0.24136877191410352 | validation: 0.27786284711979964]
	TIME [epoch: 74.5 sec]
EPOCH 627/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23794642057645723		[learning rate: 0.0001509]
	Learning Rate: 0.000150898
	LOSS [training: 0.23794642057645723 | validation: 0.2743618455172022]
	TIME [epoch: 74.5 sec]
EPOCH 628/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2413534956803352		[learning rate: 0.00014981]
	Learning Rate: 0.000149805
	LOSS [training: 0.2413534956803352 | validation: 0.27631639291299537]
	TIME [epoch: 74.5 sec]
EPOCH 629/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23727521843720284		[learning rate: 0.00014872]
	Learning Rate: 0.00014872
	LOSS [training: 0.23727521843720284 | validation: 0.28006562126332263]
	TIME [epoch: 74.4 sec]
EPOCH 630/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23825913940177573		[learning rate: 0.00014764]
	Learning Rate: 0.000147642
	LOSS [training: 0.23825913940177573 | validation: 0.2805936316195283]
	TIME [epoch: 74.4 sec]
EPOCH 631/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2407841896239326		[learning rate: 0.00014657]
	Learning Rate: 0.000146573
	LOSS [training: 0.2407841896239326 | validation: 0.2798823489938258]
	TIME [epoch: 74.4 sec]
EPOCH 632/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23953148457693463		[learning rate: 0.00014551]
	Learning Rate: 0.000145511
	LOSS [training: 0.23953148457693463 | validation: 0.2763689355616622]
	TIME [epoch: 74.4 sec]
EPOCH 633/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2380591855910576		[learning rate: 0.00014446]
	Learning Rate: 0.000144456
	LOSS [training: 0.2380591855910576 | validation: 0.2806716525642352]
	TIME [epoch: 74.4 sec]
EPOCH 634/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23730189593642553		[learning rate: 0.00014341]
	Learning Rate: 0.00014341
	LOSS [training: 0.23730189593642553 | validation: 0.27651215037353477]
	TIME [epoch: 74.5 sec]
EPOCH 635/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2377087950376501		[learning rate: 0.00014237]
	Learning Rate: 0.000142371
	LOSS [training: 0.2377087950376501 | validation: 0.2824380619510688]
	TIME [epoch: 74.4 sec]
EPOCH 636/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24320449270281258		[learning rate: 0.00014134]
	Learning Rate: 0.000141339
	LOSS [training: 0.24320449270281258 | validation: 0.2783423954827411]
	TIME [epoch: 74.5 sec]
EPOCH 637/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.237085628055291		[learning rate: 0.00014032]
	Learning Rate: 0.000140315
	LOSS [training: 0.237085628055291 | validation: 0.2814326101113365]
	TIME [epoch: 74.4 sec]
EPOCH 638/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24168857922564058		[learning rate: 0.0001393]
	Learning Rate: 0.000139299
	LOSS [training: 0.24168857922564058 | validation: 0.27381191945334377]
	TIME [epoch: 74.5 sec]
EPOCH 639/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.24095709152780992		[learning rate: 0.00013829]
	Learning Rate: 0.00013829
	LOSS [training: 0.24095709152780992 | validation: 0.27095419402516535]
	TIME [epoch: 74.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_639.pth
	Model improved!!!
EPOCH 640/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23989332080576192		[learning rate: 0.00013729]
	Learning Rate: 0.000137288
	LOSS [training: 0.23989332080576192 | validation: 0.27743662251202816]
	TIME [epoch: 74.5 sec]
EPOCH 641/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23956850041298944		[learning rate: 0.00013629]
	Learning Rate: 0.000136293
	LOSS [training: 0.23956850041298944 | validation: 0.2759840194465022]
	TIME [epoch: 74.5 sec]
EPOCH 642/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2418573173988004		[learning rate: 0.00013531]
	Learning Rate: 0.000135306
	LOSS [training: 0.2418573173988004 | validation: 0.27468708435128486]
	TIME [epoch: 74.4 sec]
EPOCH 643/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23955560749479093		[learning rate: 0.00013433]
	Learning Rate: 0.000134325
	LOSS [training: 0.23955560749479093 | validation: 0.2717396806909539]
	TIME [epoch: 74.5 sec]
EPOCH 644/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23768688167434185		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 0.23768688167434185 | validation: 0.26967856733067685]
	TIME [epoch: 74.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_644.pth
	Model improved!!!
EPOCH 645/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23683751085797844		[learning rate: 0.00013239]
	Learning Rate: 0.000132386
	LOSS [training: 0.23683751085797844 | validation: 0.2728743608042161]
	TIME [epoch: 74.5 sec]
EPOCH 646/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23481113293755404		[learning rate: 0.00013143]
	Learning Rate: 0.000131427
	LOSS [training: 0.23481113293755404 | validation: 0.2711973173150691]
	TIME [epoch: 74.5 sec]
EPOCH 647/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23838165356418936		[learning rate: 0.00013047]
	Learning Rate: 0.000130475
	LOSS [training: 0.23838165356418936 | validation: 0.27391174622031267]
	TIME [epoch: 74.5 sec]
EPOCH 648/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23563060982386508		[learning rate: 0.00012953]
	Learning Rate: 0.000129529
	LOSS [training: 0.23563060982386508 | validation: 0.27225721671471603]
	TIME [epoch: 74.5 sec]
EPOCH 649/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23475872482847346		[learning rate: 0.00012859]
	Learning Rate: 0.000128591
	LOSS [training: 0.23475872482847346 | validation: 0.27461928960407855]
	TIME [epoch: 74.5 sec]
EPOCH 650/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23657839059704483		[learning rate: 0.00012766]
	Learning Rate: 0.000127659
	LOSS [training: 0.23657839059704483 | validation: 0.27233204284321655]
	TIME [epoch: 74.5 sec]
EPOCH 651/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2346388129635546		[learning rate: 0.00012673]
	Learning Rate: 0.000126735
	LOSS [training: 0.2346388129635546 | validation: 0.27171581399948114]
	TIME [epoch: 74.5 sec]
EPOCH 652/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2372431203295417		[learning rate: 0.00012582]
	Learning Rate: 0.000125816
	LOSS [training: 0.2372431203295417 | validation: 0.278361127615376]
	TIME [epoch: 74.5 sec]
EPOCH 653/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2419144646220296		[learning rate: 0.0001249]
	Learning Rate: 0.000124905
	LOSS [training: 0.2419144646220296 | validation: 0.2658152576560584]
	TIME [epoch: 74.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_653.pth
	Model improved!!!
EPOCH 654/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23758596651777425		[learning rate: 0.000124]
	Learning Rate: 0.000124
	LOSS [training: 0.23758596651777425 | validation: 0.2737820066390797]
	TIME [epoch: 74.5 sec]
EPOCH 655/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23923043682735715		[learning rate: 0.0001231]
	Learning Rate: 0.000123101
	LOSS [training: 0.23923043682735715 | validation: 0.27457371663105573]
	TIME [epoch: 74.4 sec]
EPOCH 656/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23563635533762275		[learning rate: 0.00012221]
	Learning Rate: 0.00012221
	LOSS [training: 0.23563635533762275 | validation: 0.2700599051380412]
	TIME [epoch: 74.5 sec]
EPOCH 657/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2350348028790062		[learning rate: 0.00012132]
	Learning Rate: 0.000121324
	LOSS [training: 0.2350348028790062 | validation: 0.27128962599793827]
	TIME [epoch: 74.4 sec]
EPOCH 658/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23467454548427805		[learning rate: 0.00012045]
	Learning Rate: 0.000120445
	LOSS [training: 0.23467454548427805 | validation: 0.270459548265763]
	TIME [epoch: 74.5 sec]
EPOCH 659/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23747801580163508		[learning rate: 0.00011957]
	Learning Rate: 0.000119573
	LOSS [training: 0.23747801580163508 | validation: 0.2750859152806884]
	TIME [epoch: 74.5 sec]
EPOCH 660/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23568158547275359		[learning rate: 0.00011871]
	Learning Rate: 0.000118706
	LOSS [training: 0.23568158547275359 | validation: 0.2722956678867]
	TIME [epoch: 74.4 sec]
EPOCH 661/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23691979506911773		[learning rate: 0.00011785]
	Learning Rate: 0.000117846
	LOSS [training: 0.23691979506911773 | validation: 0.2729524869507002]
	TIME [epoch: 74.5 sec]
EPOCH 662/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23523670424971493		[learning rate: 0.00011699]
	Learning Rate: 0.000116992
	LOSS [training: 0.23523670424971493 | validation: 0.2715254351748828]
	TIME [epoch: 74.5 sec]
EPOCH 663/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23252041280798613		[learning rate: 0.00011614]
	Learning Rate: 0.000116145
	LOSS [training: 0.23252041280798613 | validation: 0.27317593219795333]
	TIME [epoch: 74.5 sec]
EPOCH 664/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2383013630534379		[learning rate: 0.0001153]
	Learning Rate: 0.000115303
	LOSS [training: 0.2383013630534379 | validation: 0.27305017465240583]
	TIME [epoch: 74.5 sec]
EPOCH 665/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23754624240592573		[learning rate: 0.00011447]
	Learning Rate: 0.000114468
	LOSS [training: 0.23754624240592573 | validation: 0.2738604789232097]
	TIME [epoch: 74.4 sec]
EPOCH 666/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23511686947612093		[learning rate: 0.00011364]
	Learning Rate: 0.000113639
	LOSS [training: 0.23511686947612093 | validation: 0.27800125702185075]
	TIME [epoch: 74.4 sec]
EPOCH 667/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23867167211141355		[learning rate: 0.00011282]
	Learning Rate: 0.000112815
	LOSS [training: 0.23867167211141355 | validation: 0.2747817070938625]
	TIME [epoch: 74.5 sec]
EPOCH 668/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.237492334514243		[learning rate: 0.000112]
	Learning Rate: 0.000111998
	LOSS [training: 0.237492334514243 | validation: 0.2734663346112265]
	TIME [epoch: 74.5 sec]
EPOCH 669/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23594329384369045		[learning rate: 0.00011119]
	Learning Rate: 0.000111187
	LOSS [training: 0.23594329384369045 | validation: 0.2662026211828009]
	TIME [epoch: 74.5 sec]
EPOCH 670/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23695378406684442		[learning rate: 0.00011038]
	Learning Rate: 0.000110381
	LOSS [training: 0.23695378406684442 | validation: 0.271386098910364]
	TIME [epoch: 74.5 sec]
EPOCH 671/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23434572776961812		[learning rate: 0.00010958]
	Learning Rate: 0.000109581
	LOSS [training: 0.23434572776961812 | validation: 0.27276454798785027]
	TIME [epoch: 74.5 sec]
EPOCH 672/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23490030531099038		[learning rate: 0.00010879]
	Learning Rate: 0.000108787
	LOSS [training: 0.23490030531099038 | validation: 0.2707518178787423]
	TIME [epoch: 74.4 sec]
EPOCH 673/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23306945696305592		[learning rate: 0.000108]
	Learning Rate: 0.000107999
	LOSS [training: 0.23306945696305592 | validation: 0.27188790407500496]
	TIME [epoch: 74.4 sec]
EPOCH 674/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23431918916789388		[learning rate: 0.00010722]
	Learning Rate: 0.000107217
	LOSS [training: 0.23431918916789388 | validation: 0.27057273595382736]
	TIME [epoch: 74.5 sec]
EPOCH 675/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2339926948520858		[learning rate: 0.00010644]
	Learning Rate: 0.00010644
	LOSS [training: 0.2339926948520858 | validation: 0.26917605766690655]
	TIME [epoch: 74.5 sec]
EPOCH 676/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23567908897742557		[learning rate: 0.00010567]
	Learning Rate: 0.000105669
	LOSS [training: 0.23567908897742557 | validation: 0.27588331081762]
	TIME [epoch: 74.5 sec]
EPOCH 677/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23512562327859643		[learning rate: 0.0001049]
	Learning Rate: 0.000104903
	LOSS [training: 0.23512562327859643 | validation: 0.26319710523512674]
	TIME [epoch: 74.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_677.pth
	Model improved!!!
EPOCH 678/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2336258849840219		[learning rate: 0.00010414]
	Learning Rate: 0.000104143
	LOSS [training: 0.2336258849840219 | validation: 0.26903415824186616]
	TIME [epoch: 74.5 sec]
EPOCH 679/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23266104402758594		[learning rate: 0.00010339]
	Learning Rate: 0.000103389
	LOSS [training: 0.23266104402758594 | validation: 0.27081501375761785]
	TIME [epoch: 74.5 sec]
EPOCH 680/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23552703661625338		[learning rate: 0.00010264]
	Learning Rate: 0.00010264
	LOSS [training: 0.23552703661625338 | validation: 0.26835864105794016]
	TIME [epoch: 74.5 sec]
EPOCH 681/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23291973245726905		[learning rate: 0.0001019]
	Learning Rate: 0.000101896
	LOSS [training: 0.23291973245726905 | validation: 0.26403303440133646]
	TIME [epoch: 74.5 sec]
EPOCH 682/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2333833960205608		[learning rate: 0.00010116]
	Learning Rate: 0.000101158
	LOSS [training: 0.2333833960205608 | validation: 0.26868523331056027]
	TIME [epoch: 74.5 sec]
EPOCH 683/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2358367879686535		[learning rate: 0.00010043]
	Learning Rate: 0.000100425
	LOSS [training: 0.2358367879686535 | validation: 0.2720125305287694]
	TIME [epoch: 74.5 sec]
EPOCH 684/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23427198484792916		[learning rate: 9.9697e-05]
	Learning Rate: 9.96975e-05
	LOSS [training: 0.23427198484792916 | validation: 0.2671905045271383]
	TIME [epoch: 74.5 sec]
EPOCH 685/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23422870634687049		[learning rate: 9.8975e-05]
	Learning Rate: 9.89752e-05
	LOSS [training: 0.23422870634687049 | validation: 0.2685288673416247]
	TIME [epoch: 74.5 sec]
EPOCH 686/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23580547498543422		[learning rate: 9.8258e-05]
	Learning Rate: 9.82581e-05
	LOSS [training: 0.23580547498543422 | validation: 0.26799217817916915]
	TIME [epoch: 74.5 sec]
EPOCH 687/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23154843815791382		[learning rate: 9.7546e-05]
	Learning Rate: 9.75463e-05
	LOSS [training: 0.23154843815791382 | validation: 0.270197901643637]
	TIME [epoch: 74.5 sec]
EPOCH 688/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23296740841683325		[learning rate: 9.684e-05]
	Learning Rate: 9.68396e-05
	LOSS [training: 0.23296740841683325 | validation: 0.264614166747061]
	TIME [epoch: 74.5 sec]
EPOCH 689/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23454343135555009		[learning rate: 9.6138e-05]
	Learning Rate: 9.61379e-05
	LOSS [training: 0.23454343135555009 | validation: 0.2624242578820808]
	TIME [epoch: 74.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_689.pth
	Model improved!!!
EPOCH 690/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23214372548266002		[learning rate: 9.5441e-05]
	Learning Rate: 9.54414e-05
	LOSS [training: 0.23214372548266002 | validation: 0.27337578104623605]
	TIME [epoch: 74.5 sec]
EPOCH 691/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2370835277964862		[learning rate: 9.475e-05]
	Learning Rate: 9.475e-05
	LOSS [training: 0.2370835277964862 | validation: 0.2663424957490008]
	TIME [epoch: 74.5 sec]
EPOCH 692/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23198072372527784		[learning rate: 9.4064e-05]
	Learning Rate: 9.40635e-05
	LOSS [training: 0.23198072372527784 | validation: 0.2674500949660209]
	TIME [epoch: 74.5 sec]
EPOCH 693/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23090348393377624		[learning rate: 9.3382e-05]
	Learning Rate: 9.3382e-05
	LOSS [training: 0.23090348393377624 | validation: 0.2721524239202629]
	TIME [epoch: 74.5 sec]
EPOCH 694/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23212939149143413		[learning rate: 9.2705e-05]
	Learning Rate: 9.27055e-05
	LOSS [training: 0.23212939149143413 | validation: 0.27161541819099355]
	TIME [epoch: 74.5 sec]
EPOCH 695/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.232183446145042		[learning rate: 9.2034e-05]
	Learning Rate: 9.20338e-05
	LOSS [training: 0.232183446145042 | validation: 0.27593115415155733]
	TIME [epoch: 74.5 sec]
EPOCH 696/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23219443165025305		[learning rate: 9.1367e-05]
	Learning Rate: 9.13671e-05
	LOSS [training: 0.23219443165025305 | validation: 0.26761900007460504]
	TIME [epoch: 74.5 sec]
EPOCH 697/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23258969120030748		[learning rate: 9.0705e-05]
	Learning Rate: 9.07051e-05
	LOSS [training: 0.23258969120030748 | validation: 0.26735408328566196]
	TIME [epoch: 74.5 sec]
EPOCH 698/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23268078259784544		[learning rate: 9.0048e-05]
	Learning Rate: 9.00479e-05
	LOSS [training: 0.23268078259784544 | validation: 0.2685960946609088]
	TIME [epoch: 74.5 sec]
EPOCH 699/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23460832510445395		[learning rate: 8.9396e-05]
	Learning Rate: 8.93955e-05
	LOSS [training: 0.23460832510445395 | validation: 0.268468145547381]
	TIME [epoch: 74.5 sec]
EPOCH 700/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23205317021087862		[learning rate: 8.8748e-05]
	Learning Rate: 8.87479e-05
	LOSS [training: 0.23205317021087862 | validation: 0.2761038183451812]
	TIME [epoch: 74.5 sec]
EPOCH 701/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23491292064958796		[learning rate: 8.8105e-05]
	Learning Rate: 8.81049e-05
	LOSS [training: 0.23491292064958796 | validation: 0.2691694611908033]
	TIME [epoch: 74.5 sec]
EPOCH 702/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23288204295788173		[learning rate: 8.7467e-05]
	Learning Rate: 8.74666e-05
	LOSS [training: 0.23288204295788173 | validation: 0.2698586145415992]
	TIME [epoch: 74.5 sec]
EPOCH 703/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23411482967734892		[learning rate: 8.6833e-05]
	Learning Rate: 8.68329e-05
	LOSS [training: 0.23411482967734892 | validation: 0.26824077095296567]
	TIME [epoch: 74.5 sec]
EPOCH 704/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2346801126239101		[learning rate: 8.6204e-05]
	Learning Rate: 8.62038e-05
	LOSS [training: 0.2346801126239101 | validation: 0.26575125508115915]
	TIME [epoch: 74.5 sec]
EPOCH 705/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2312034379374479		[learning rate: 8.5579e-05]
	Learning Rate: 8.55793e-05
	LOSS [training: 0.2312034379374479 | validation: 0.269381173893983]
	TIME [epoch: 74.5 sec]
EPOCH 706/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2341960240472029		[learning rate: 8.4959e-05]
	Learning Rate: 8.49592e-05
	LOSS [training: 0.2341960240472029 | validation: 0.2669555948725734]
	TIME [epoch: 74.5 sec]
EPOCH 707/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23568265667930177		[learning rate: 8.4344e-05]
	Learning Rate: 8.43437e-05
	LOSS [training: 0.23568265667930177 | validation: 0.26925370020337586]
	TIME [epoch: 74.5 sec]
EPOCH 708/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2328773032024507		[learning rate: 8.3733e-05]
	Learning Rate: 8.37327e-05
	LOSS [training: 0.2328773032024507 | validation: 0.27019009622234036]
	TIME [epoch: 74.5 sec]
EPOCH 709/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23409623414460412		[learning rate: 8.3126e-05]
	Learning Rate: 8.3126e-05
	LOSS [training: 0.23409623414460412 | validation: 0.2724928872260403]
	TIME [epoch: 74.5 sec]
EPOCH 710/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2318721219366068		[learning rate: 8.2524e-05]
	Learning Rate: 8.25238e-05
	LOSS [training: 0.2318721219366068 | validation: 0.26949789612823]
	TIME [epoch: 74.5 sec]
EPOCH 711/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23258462510400282		[learning rate: 8.1926e-05]
	Learning Rate: 8.19259e-05
	LOSS [training: 0.23258462510400282 | validation: 0.26484074075954134]
	TIME [epoch: 74.5 sec]
EPOCH 712/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23251633771551594		[learning rate: 8.1332e-05]
	Learning Rate: 8.13323e-05
	LOSS [training: 0.23251633771551594 | validation: 0.2670527952499396]
	TIME [epoch: 74.4 sec]
EPOCH 713/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23295028376148194		[learning rate: 8.0743e-05]
	Learning Rate: 8.07431e-05
	LOSS [training: 0.23295028376148194 | validation: 0.2669962766514025]
	TIME [epoch: 74.5 sec]
EPOCH 714/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23319522075617388		[learning rate: 8.0158e-05]
	Learning Rate: 8.01581e-05
	LOSS [training: 0.23319522075617388 | validation: 0.2721051125482578]
	TIME [epoch: 74.5 sec]
EPOCH 715/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23329870549460516		[learning rate: 7.9577e-05]
	Learning Rate: 7.95774e-05
	LOSS [training: 0.23329870549460516 | validation: 0.2671301673780316]
	TIME [epoch: 74.5 sec]
EPOCH 716/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23217305301518662		[learning rate: 7.9001e-05]
	Learning Rate: 7.90008e-05
	LOSS [training: 0.23217305301518662 | validation: 0.2690762078198473]
	TIME [epoch: 74.5 sec]
EPOCH 717/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23112153966040122		[learning rate: 7.8428e-05]
	Learning Rate: 7.84285e-05
	LOSS [training: 0.23112153966040122 | validation: 0.26342641863178906]
	TIME [epoch: 74.5 sec]
EPOCH 718/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23311616791351245		[learning rate: 7.786e-05]
	Learning Rate: 7.78603e-05
	LOSS [training: 0.23311616791351245 | validation: 0.2612831699554758]
	TIME [epoch: 74.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_718.pth
	Model improved!!!
EPOCH 719/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2322562208057446		[learning rate: 7.7296e-05]
	Learning Rate: 7.72962e-05
	LOSS [training: 0.2322562208057446 | validation: 0.26915672132678714]
	TIME [epoch: 74.5 sec]
EPOCH 720/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23335755793315033		[learning rate: 7.6736e-05]
	Learning Rate: 7.67362e-05
	LOSS [training: 0.23335755793315033 | validation: 0.2680184872510114]
	TIME [epoch: 74.5 sec]
EPOCH 721/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23376692626372897		[learning rate: 7.618e-05]
	Learning Rate: 7.61802e-05
	LOSS [training: 0.23376692626372897 | validation: 0.2689758044976249]
	TIME [epoch: 74.5 sec]
EPOCH 722/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2314325978772592		[learning rate: 7.5628e-05]
	Learning Rate: 7.56283e-05
	LOSS [training: 0.2314325978772592 | validation: 0.2655262501965805]
	TIME [epoch: 74.4 sec]
EPOCH 723/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23202090943447112		[learning rate: 7.508e-05]
	Learning Rate: 7.50804e-05
	LOSS [training: 0.23202090943447112 | validation: 0.26561865564382947]
	TIME [epoch: 74.5 sec]
EPOCH 724/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23148222231638443		[learning rate: 7.4536e-05]
	Learning Rate: 7.45364e-05
	LOSS [training: 0.23148222231638443 | validation: 0.26909632066230554]
	TIME [epoch: 74.4 sec]
EPOCH 725/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23362305460915922		[learning rate: 7.3996e-05]
	Learning Rate: 7.39964e-05
	LOSS [training: 0.23362305460915922 | validation: 0.26347901678057883]
	TIME [epoch: 74.5 sec]
EPOCH 726/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22977298058440818		[learning rate: 7.346e-05]
	Learning Rate: 7.34603e-05
	LOSS [training: 0.22977298058440818 | validation: 0.26707278654780436]
	TIME [epoch: 74.5 sec]
EPOCH 727/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23174184019160846		[learning rate: 7.2928e-05]
	Learning Rate: 7.29281e-05
	LOSS [training: 0.23174184019160846 | validation: 0.263332655270512]
	TIME [epoch: 74.5 sec]
EPOCH 728/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2304314294885832		[learning rate: 7.24e-05]
	Learning Rate: 7.23997e-05
	LOSS [training: 0.2304314294885832 | validation: 0.26244375748473947]
	TIME [epoch: 74.5 sec]
EPOCH 729/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23077025269509593		[learning rate: 7.1875e-05]
	Learning Rate: 7.18752e-05
	LOSS [training: 0.23077025269509593 | validation: 0.26579276586623607]
	TIME [epoch: 74.4 sec]
EPOCH 730/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23003925120631355		[learning rate: 7.1354e-05]
	Learning Rate: 7.13545e-05
	LOSS [training: 0.23003925120631355 | validation: 0.26424863380318686]
	TIME [epoch: 74.5 sec]
EPOCH 731/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23149486275016945		[learning rate: 7.0838e-05]
	Learning Rate: 7.08375e-05
	LOSS [training: 0.23149486275016945 | validation: 0.26075925068768246]
	TIME [epoch: 74.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_731.pth
	Model improved!!!
EPOCH 732/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2306996258248508		[learning rate: 7.0324e-05]
	Learning Rate: 7.03243e-05
	LOSS [training: 0.2306996258248508 | validation: 0.26519681761666286]
	TIME [epoch: 74.5 sec]
EPOCH 733/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23111449266063658		[learning rate: 6.9815e-05]
	Learning Rate: 6.98148e-05
	LOSS [training: 0.23111449266063658 | validation: 0.26369849332972084]
	TIME [epoch: 74.5 sec]
EPOCH 734/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23170707141806907		[learning rate: 6.9309e-05]
	Learning Rate: 6.9309e-05
	LOSS [training: 0.23170707141806907 | validation: 0.26694063769267834]
	TIME [epoch: 74.5 sec]
EPOCH 735/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22965273308816053		[learning rate: 6.8807e-05]
	Learning Rate: 6.88069e-05
	LOSS [training: 0.22965273308816053 | validation: 0.26621535332124086]
	TIME [epoch: 74.5 sec]
EPOCH 736/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23143900794741323		[learning rate: 6.8308e-05]
	Learning Rate: 6.83084e-05
	LOSS [training: 0.23143900794741323 | validation: 0.26835271217986717]
	TIME [epoch: 74.5 sec]
EPOCH 737/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23110572356210293		[learning rate: 6.7813e-05]
	Learning Rate: 6.78134e-05
	LOSS [training: 0.23110572356210293 | validation: 0.2671673797028888]
	TIME [epoch: 74.5 sec]
EPOCH 738/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2302684128725695		[learning rate: 6.7322e-05]
	Learning Rate: 6.73221e-05
	LOSS [training: 0.2302684128725695 | validation: 0.2674055794956945]
	TIME [epoch: 74.5 sec]
EPOCH 739/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23149663607535942		[learning rate: 6.6834e-05]
	Learning Rate: 6.68344e-05
	LOSS [training: 0.23149663607535942 | validation: 0.26738636094101975]
	TIME [epoch: 74.5 sec]
EPOCH 740/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22951902030386112		[learning rate: 6.635e-05]
	Learning Rate: 6.63502e-05
	LOSS [training: 0.22951902030386112 | validation: 0.26429323873679694]
	TIME [epoch: 74.5 sec]
EPOCH 741/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23076120224149485		[learning rate: 6.587e-05]
	Learning Rate: 6.58695e-05
	LOSS [training: 0.23076120224149485 | validation: 0.2638991739332083]
	TIME [epoch: 74.5 sec]
EPOCH 742/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23019143953807233		[learning rate: 6.5392e-05]
	Learning Rate: 6.53923e-05
	LOSS [training: 0.23019143953807233 | validation: 0.2639439904091102]
	TIME [epoch: 74.5 sec]
EPOCH 743/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2286373712395392		[learning rate: 6.4919e-05]
	Learning Rate: 6.49185e-05
	LOSS [training: 0.2286373712395392 | validation: 0.2631605447353503]
	TIME [epoch: 74.5 sec]
EPOCH 744/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23261607355243838		[learning rate: 6.4448e-05]
	Learning Rate: 6.44482e-05
	LOSS [training: 0.23261607355243838 | validation: 0.26578393902395064]
	TIME [epoch: 74.5 sec]
EPOCH 745/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23225198942345796		[learning rate: 6.3981e-05]
	Learning Rate: 6.39813e-05
	LOSS [training: 0.23225198942345796 | validation: 0.26346055917180444]
	TIME [epoch: 74.5 sec]
EPOCH 746/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2295532618397071		[learning rate: 6.3518e-05]
	Learning Rate: 6.35177e-05
	LOSS [training: 0.2295532618397071 | validation: 0.26399830296085525]
	TIME [epoch: 74.5 sec]
EPOCH 747/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2308508720213207		[learning rate: 6.3058e-05]
	Learning Rate: 6.30575e-05
	LOSS [training: 0.2308508720213207 | validation: 0.2695525049241829]
	TIME [epoch: 74.5 sec]
EPOCH 748/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23058473449988737		[learning rate: 6.2601e-05]
	Learning Rate: 6.26007e-05
	LOSS [training: 0.23058473449988737 | validation: 0.26541109105973937]
	TIME [epoch: 74.5 sec]
EPOCH 749/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22994784250551384		[learning rate: 6.2147e-05]
	Learning Rate: 6.21471e-05
	LOSS [training: 0.22994784250551384 | validation: 0.26252776224477625]
	TIME [epoch: 74.5 sec]
EPOCH 750/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23170627687436932		[learning rate: 6.1697e-05]
	Learning Rate: 6.16969e-05
	LOSS [training: 0.23170627687436932 | validation: 0.26093619946203894]
	TIME [epoch: 74.5 sec]
EPOCH 751/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22954011641304162		[learning rate: 6.125e-05]
	Learning Rate: 6.12499e-05
	LOSS [training: 0.22954011641304162 | validation: 0.2661717251340676]
	TIME [epoch: 74.5 sec]
EPOCH 752/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23136868284253598		[learning rate: 6.0806e-05]
	Learning Rate: 6.08061e-05
	LOSS [training: 0.23136868284253598 | validation: 0.264603632754089]
	TIME [epoch: 74.5 sec]
EPOCH 753/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22841547787259317		[learning rate: 6.0366e-05]
	Learning Rate: 6.03656e-05
	LOSS [training: 0.22841547787259317 | validation: 0.26365776308460764]
	TIME [epoch: 74.5 sec]
EPOCH 754/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22819930705477345		[learning rate: 5.9928e-05]
	Learning Rate: 5.99283e-05
	LOSS [training: 0.22819930705477345 | validation: 0.264493667630674]
	TIME [epoch: 74.4 sec]
EPOCH 755/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.230595642244325		[learning rate: 5.9494e-05]
	Learning Rate: 5.94941e-05
	LOSS [training: 0.230595642244325 | validation: 0.2685567612784352]
	TIME [epoch: 74.5 sec]
EPOCH 756/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22960009594483577		[learning rate: 5.9063e-05]
	Learning Rate: 5.9063e-05
	LOSS [training: 0.22960009594483577 | validation: 0.261966735159318]
	TIME [epoch: 74.5 sec]
EPOCH 757/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23049847662315712		[learning rate: 5.8635e-05]
	Learning Rate: 5.86351e-05
	LOSS [training: 0.23049847662315712 | validation: 0.2620247967607414]
	TIME [epoch: 74.5 sec]
EPOCH 758/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23066587973265218		[learning rate: 5.821e-05]
	Learning Rate: 5.82103e-05
	LOSS [training: 0.23066587973265218 | validation: 0.2640455947113437]
	TIME [epoch: 74.4 sec]
EPOCH 759/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23024851124808665		[learning rate: 5.7789e-05]
	Learning Rate: 5.77886e-05
	LOSS [training: 0.23024851124808665 | validation: 0.26787532540082226]
	TIME [epoch: 74.5 sec]
EPOCH 760/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22940657423790986		[learning rate: 5.737e-05]
	Learning Rate: 5.73699e-05
	LOSS [training: 0.22940657423790986 | validation: 0.2652178698495302]
	TIME [epoch: 74.5 sec]
EPOCH 761/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23154484869350522		[learning rate: 5.6954e-05]
	Learning Rate: 5.69543e-05
	LOSS [training: 0.23154484869350522 | validation: 0.26759934289870013]
	TIME [epoch: 74.5 sec]
EPOCH 762/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22936299289358408		[learning rate: 5.6542e-05]
	Learning Rate: 5.65417e-05
	LOSS [training: 0.22936299289358408 | validation: 0.2667462880053188]
	TIME [epoch: 74.4 sec]
EPOCH 763/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23089967760114494		[learning rate: 5.6132e-05]
	Learning Rate: 5.6132e-05
	LOSS [training: 0.23089967760114494 | validation: 0.2668518130873367]
	TIME [epoch: 74.5 sec]
EPOCH 764/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22912721521775928		[learning rate: 5.5725e-05]
	Learning Rate: 5.57253e-05
	LOSS [training: 0.22912721521775928 | validation: 0.2589550175302917]
	TIME [epoch: 74.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_764.pth
	Model improved!!!
EPOCH 765/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22936860626223726		[learning rate: 5.5322e-05]
	Learning Rate: 5.53216e-05
	LOSS [training: 0.22936860626223726 | validation: 0.2610111767093847]
	TIME [epoch: 74.4 sec]
EPOCH 766/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2307578751202564		[learning rate: 5.4921e-05]
	Learning Rate: 5.49208e-05
	LOSS [training: 0.2307578751202564 | validation: 0.2668126369608416]
	TIME [epoch: 74.4 sec]
EPOCH 767/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22966462027976986		[learning rate: 5.4523e-05]
	Learning Rate: 5.45229e-05
	LOSS [training: 0.22966462027976986 | validation: 0.2678120840939028]
	TIME [epoch: 74.5 sec]
EPOCH 768/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22881136369629246		[learning rate: 5.4128e-05]
	Learning Rate: 5.41279e-05
	LOSS [training: 0.22881136369629246 | validation: 0.26311769843956323]
	TIME [epoch: 74.5 sec]
EPOCH 769/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2291107577898027		[learning rate: 5.3736e-05]
	Learning Rate: 5.37357e-05
	LOSS [training: 0.2291107577898027 | validation: 0.2672195363910099]
	TIME [epoch: 74.5 sec]
EPOCH 770/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22918164742320524		[learning rate: 5.3346e-05]
	Learning Rate: 5.33464e-05
	LOSS [training: 0.22918164742320524 | validation: 0.26474470004531897]
	TIME [epoch: 74.4 sec]
EPOCH 771/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2289295631798178		[learning rate: 5.296e-05]
	Learning Rate: 5.29599e-05
	LOSS [training: 0.2289295631798178 | validation: 0.2625042296344513]
	TIME [epoch: 74.6 sec]
EPOCH 772/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22871554882864945		[learning rate: 5.2576e-05]
	Learning Rate: 5.25762e-05
	LOSS [training: 0.22871554882864945 | validation: 0.2628967743197881]
	TIME [epoch: 74.4 sec]
EPOCH 773/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2292269782760154		[learning rate: 5.2195e-05]
	Learning Rate: 5.21953e-05
	LOSS [training: 0.2292269782760154 | validation: 0.2611959967316869]
	TIME [epoch: 74.4 sec]
EPOCH 774/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22631428084968308		[learning rate: 5.1817e-05]
	Learning Rate: 5.18172e-05
	LOSS [training: 0.22631428084968308 | validation: 0.26292795838131855]
	TIME [epoch: 74.4 sec]
EPOCH 775/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2294266066377275		[learning rate: 5.1442e-05]
	Learning Rate: 5.14418e-05
	LOSS [training: 0.2294266066377275 | validation: 0.2598122742212068]
	TIME [epoch: 74.5 sec]
EPOCH 776/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2290492444501993		[learning rate: 5.1069e-05]
	Learning Rate: 5.10691e-05
	LOSS [training: 0.2290492444501993 | validation: 0.26434750303507426]
	TIME [epoch: 74.4 sec]
EPOCH 777/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.23059075671562018		[learning rate: 5.0699e-05]
	Learning Rate: 5.06991e-05
	LOSS [training: 0.23059075671562018 | validation: 0.2610143428511317]
	TIME [epoch: 74.4 sec]
EPOCH 778/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22847553647114327		[learning rate: 5.0332e-05]
	Learning Rate: 5.03318e-05
	LOSS [training: 0.22847553647114327 | validation: 0.2620848885221564]
	TIME [epoch: 74.4 sec]
EPOCH 779/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22927468345129481		[learning rate: 4.9967e-05]
	Learning Rate: 4.99671e-05
	LOSS [training: 0.22927468345129481 | validation: 0.26023579359182325]
	TIME [epoch: 74.4 sec]
EPOCH 780/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22914107068155898		[learning rate: 4.9605e-05]
	Learning Rate: 4.96051e-05
	LOSS [training: 0.22914107068155898 | validation: 0.26325230660177834]
	TIME [epoch: 74.4 sec]
EPOCH 781/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22900730788453963		[learning rate: 4.9246e-05]
	Learning Rate: 4.92457e-05
	LOSS [training: 0.22900730788453963 | validation: 0.2616071131556302]
	TIME [epoch: 74.4 sec]
EPOCH 782/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22811534244735138		[learning rate: 4.8889e-05]
	Learning Rate: 4.88889e-05
	LOSS [training: 0.22811534244735138 | validation: 0.25748220384352904]
	TIME [epoch: 74.4 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_782.pth
	Model improved!!!
EPOCH 783/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22890199003046818		[learning rate: 4.8535e-05]
	Learning Rate: 4.85347e-05
	LOSS [training: 0.22890199003046818 | validation: 0.26461386313559154]
	TIME [epoch: 74.4 sec]
EPOCH 784/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22795678914706266		[learning rate: 4.8183e-05]
	Learning Rate: 4.81831e-05
	LOSS [training: 0.22795678914706266 | validation: 0.2635166765793862]
	TIME [epoch: 74.4 sec]
EPOCH 785/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22838505476937512		[learning rate: 4.7834e-05]
	Learning Rate: 4.7834e-05
	LOSS [training: 0.22838505476937512 | validation: 0.2626791903207222]
	TIME [epoch: 74.4 sec]
EPOCH 786/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2299538213787417		[learning rate: 4.7487e-05]
	Learning Rate: 4.74875e-05
	LOSS [training: 0.2299538213787417 | validation: 0.26388937066400014]
	TIME [epoch: 74.4 sec]
EPOCH 787/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2286939641661656		[learning rate: 4.7143e-05]
	Learning Rate: 4.71434e-05
	LOSS [training: 0.2286939641661656 | validation: 0.2611606342197591]
	TIME [epoch: 74.4 sec]
EPOCH 788/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22959975429656068		[learning rate: 4.6802e-05]
	Learning Rate: 4.68019e-05
	LOSS [training: 0.22959975429656068 | validation: 0.25986868093589377]
	TIME [epoch: 74.4 sec]
EPOCH 789/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22612547057376306		[learning rate: 4.6463e-05]
	Learning Rate: 4.64628e-05
	LOSS [training: 0.22612547057376306 | validation: 0.26057493571734475]
	TIME [epoch: 74.4 sec]
EPOCH 790/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22817647222076154		[learning rate: 4.6126e-05]
	Learning Rate: 4.61262e-05
	LOSS [training: 0.22817647222076154 | validation: 0.26032737514260623]
	TIME [epoch: 74.4 sec]
EPOCH 791/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22784429296228875		[learning rate: 4.5792e-05]
	Learning Rate: 4.5792e-05
	LOSS [training: 0.22784429296228875 | validation: 0.2590803323748565]
	TIME [epoch: 74.4 sec]
EPOCH 792/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22807213589856537		[learning rate: 4.546e-05]
	Learning Rate: 4.54602e-05
	LOSS [training: 0.22807213589856537 | validation: 0.2596997468078055]
	TIME [epoch: 74.4 sec]
EPOCH 793/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22787799746530946		[learning rate: 4.5131e-05]
	Learning Rate: 4.51309e-05
	LOSS [training: 0.22787799746530946 | validation: 0.26172280256026326]
	TIME [epoch: 74.4 sec]
EPOCH 794/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22829958745313345		[learning rate: 4.4804e-05]
	Learning Rate: 4.48039e-05
	LOSS [training: 0.22829958745313345 | validation: 0.26237319700864753]
	TIME [epoch: 74.4 sec]
EPOCH 795/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22739041983210306		[learning rate: 4.4479e-05]
	Learning Rate: 4.44793e-05
	LOSS [training: 0.22739041983210306 | validation: 0.265520523354207]
	TIME [epoch: 74.4 sec]
EPOCH 796/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2304895392655316		[learning rate: 4.4157e-05]
	Learning Rate: 4.41571e-05
	LOSS [training: 0.2304895392655316 | validation: 0.26533982238738313]
	TIME [epoch: 74.4 sec]
EPOCH 797/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2289381140308564		[learning rate: 4.3837e-05]
	Learning Rate: 4.38371e-05
	LOSS [training: 0.2289381140308564 | validation: 0.2633665323091482]
	TIME [epoch: 74.4 sec]
EPOCH 798/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22924795301496598		[learning rate: 4.352e-05]
	Learning Rate: 4.35195e-05
	LOSS [training: 0.22924795301496598 | validation: 0.2576547891781542]
	TIME [epoch: 74.4 sec]
EPOCH 799/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2288605768690572		[learning rate: 4.3204e-05]
	Learning Rate: 4.32042e-05
	LOSS [training: 0.2288605768690572 | validation: 0.2606261870948096]
	TIME [epoch: 74.4 sec]
EPOCH 800/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22702597518931114		[learning rate: 4.2891e-05]
	Learning Rate: 4.28912e-05
	LOSS [training: 0.22702597518931114 | validation: 0.25821909536498283]
	TIME [epoch: 74.5 sec]
EPOCH 801/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2293159165817304		[learning rate: 4.258e-05]
	Learning Rate: 4.25805e-05
	LOSS [training: 0.2293159165817304 | validation: 0.25910213021171236]
	TIME [epoch: 74.4 sec]
EPOCH 802/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22718814647301686		[learning rate: 4.2272e-05]
	Learning Rate: 4.2272e-05
	LOSS [training: 0.22718814647301686 | validation: 0.2659868510292009]
	TIME [epoch: 74.4 sec]
EPOCH 803/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22773219731660105		[learning rate: 4.1966e-05]
	Learning Rate: 4.19657e-05
	LOSS [training: 0.22773219731660105 | validation: 0.2641570504423397]
	TIME [epoch: 74.4 sec]
EPOCH 804/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2275192113214769		[learning rate: 4.1662e-05]
	Learning Rate: 4.16617e-05
	LOSS [training: 0.2275192113214769 | validation: 0.2649871059821364]
	TIME [epoch: 74.4 sec]
EPOCH 805/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22840996705131889		[learning rate: 4.136e-05]
	Learning Rate: 4.13599e-05
	LOSS [training: 0.22840996705131889 | validation: 0.25988815960731837]
	TIME [epoch: 74.4 sec]
EPOCH 806/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22732115412464204		[learning rate: 4.106e-05]
	Learning Rate: 4.10602e-05
	LOSS [training: 0.22732115412464204 | validation: 0.26276748526449045]
	TIME [epoch: 74.4 sec]
EPOCH 807/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22578243720883495		[learning rate: 4.0763e-05]
	Learning Rate: 4.07627e-05
	LOSS [training: 0.22578243720883495 | validation: 0.26101266653810945]
	TIME [epoch: 74.4 sec]
EPOCH 808/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22848292721506727		[learning rate: 4.0467e-05]
	Learning Rate: 4.04674e-05
	LOSS [training: 0.22848292721506727 | validation: 0.26344662222389764]
	TIME [epoch: 74.4 sec]
EPOCH 809/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22686684694239256		[learning rate: 4.0174e-05]
	Learning Rate: 4.01742e-05
	LOSS [training: 0.22686684694239256 | validation: 0.2584781403055961]
	TIME [epoch: 74.5 sec]
EPOCH 810/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22845098358379093		[learning rate: 3.9883e-05]
	Learning Rate: 3.98832e-05
	LOSS [training: 0.22845098358379093 | validation: 0.25846474760551713]
	TIME [epoch: 74.4 sec]
EPOCH 811/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22959174731557908		[learning rate: 3.9594e-05]
	Learning Rate: 3.95942e-05
	LOSS [training: 0.22959174731557908 | validation: 0.2629818710860308]
	TIME [epoch: 74.4 sec]
EPOCH 812/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22785066202587462		[learning rate: 3.9307e-05]
	Learning Rate: 3.93074e-05
	LOSS [training: 0.22785066202587462 | validation: 0.25871954836492894]
	TIME [epoch: 74.5 sec]
EPOCH 813/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22791548249793409		[learning rate: 3.9023e-05]
	Learning Rate: 3.90226e-05
	LOSS [training: 0.22791548249793409 | validation: 0.2589096388333228]
	TIME [epoch: 74.4 sec]
EPOCH 814/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22807668585542243		[learning rate: 3.874e-05]
	Learning Rate: 3.87399e-05
	LOSS [training: 0.22807668585542243 | validation: 0.26457455770322524]
	TIME [epoch: 74.4 sec]
EPOCH 815/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22814692823637167		[learning rate: 3.8459e-05]
	Learning Rate: 3.84592e-05
	LOSS [training: 0.22814692823637167 | validation: 0.2595278463636817]
	TIME [epoch: 74.4 sec]
EPOCH 816/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2277135071650083		[learning rate: 3.8181e-05]
	Learning Rate: 3.81806e-05
	LOSS [training: 0.2277135071650083 | validation: 0.26781564000971425]
	TIME [epoch: 74.5 sec]
EPOCH 817/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22725303514700707		[learning rate: 3.7904e-05]
	Learning Rate: 3.79039e-05
	LOSS [training: 0.22725303514700707 | validation: 0.25630728982591944]
	TIME [epoch: 74.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_817.pth
	Model improved!!!
EPOCH 818/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22886228654395302		[learning rate: 3.7629e-05]
	Learning Rate: 3.76293e-05
	LOSS [training: 0.22886228654395302 | validation: 0.2620857729926875]
	TIME [epoch: 74.5 sec]
EPOCH 819/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2264943814746127		[learning rate: 3.7357e-05]
	Learning Rate: 3.73567e-05
	LOSS [training: 0.2264943814746127 | validation: 0.26463716641243074]
	TIME [epoch: 74.5 sec]
EPOCH 820/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22828838034367402		[learning rate: 3.7086e-05]
	Learning Rate: 3.70861e-05
	LOSS [training: 0.22828838034367402 | validation: 0.25960368797343397]
	TIME [epoch: 74.5 sec]
EPOCH 821/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2274878349366783		[learning rate: 3.6817e-05]
	Learning Rate: 3.68174e-05
	LOSS [training: 0.2274878349366783 | validation: 0.2598725415993831]
	TIME [epoch: 74.5 sec]
EPOCH 822/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22748075475842452		[learning rate: 3.6551e-05]
	Learning Rate: 3.65506e-05
	LOSS [training: 0.22748075475842452 | validation: 0.25805791574719517]
	TIME [epoch: 74.6 sec]
EPOCH 823/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22599197882611757		[learning rate: 3.6286e-05]
	Learning Rate: 3.62858e-05
	LOSS [training: 0.22599197882611757 | validation: 0.2608635731631076]
	TIME [epoch: 74.5 sec]
EPOCH 824/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22663300450643414		[learning rate: 3.6023e-05]
	Learning Rate: 3.60229e-05
	LOSS [training: 0.22663300450643414 | validation: 0.2607141798159476]
	TIME [epoch: 74.5 sec]
EPOCH 825/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2280011137326977		[learning rate: 3.5762e-05]
	Learning Rate: 3.57619e-05
	LOSS [training: 0.2280011137326977 | validation: 0.2572896631982803]
	TIME [epoch: 74.5 sec]
EPOCH 826/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.226641619445624		[learning rate: 3.5503e-05]
	Learning Rate: 3.55029e-05
	LOSS [training: 0.226641619445624 | validation: 0.2635806085925733]
	TIME [epoch: 74.6 sec]
EPOCH 827/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2288291045927536		[learning rate: 3.5246e-05]
	Learning Rate: 3.52456e-05
	LOSS [training: 0.2288291045927536 | validation: 0.25858659500238873]
	TIME [epoch: 74.5 sec]
EPOCH 828/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22875133807157416		[learning rate: 3.499e-05]
	Learning Rate: 3.49903e-05
	LOSS [training: 0.22875133807157416 | validation: 0.2590331502970326]
	TIME [epoch: 74.5 sec]
EPOCH 829/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22769543087741553		[learning rate: 3.4737e-05]
	Learning Rate: 3.47368e-05
	LOSS [training: 0.22769543087741553 | validation: 0.25843370997537163]
	TIME [epoch: 74.5 sec]
EPOCH 830/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.226373205233067		[learning rate: 3.4485e-05]
	Learning Rate: 3.44851e-05
	LOSS [training: 0.226373205233067 | validation: 0.2619525658087723]
	TIME [epoch: 74.5 sec]
EPOCH 831/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22835766248413297		[learning rate: 3.4235e-05]
	Learning Rate: 3.42353e-05
	LOSS [training: 0.22835766248413297 | validation: 0.2626152430102698]
	TIME [epoch: 74.5 sec]
EPOCH 832/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22769418543905232		[learning rate: 3.3987e-05]
	Learning Rate: 3.39872e-05
	LOSS [training: 0.22769418543905232 | validation: 0.26139490593751596]
	TIME [epoch: 74.5 sec]
EPOCH 833/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22757934566918098		[learning rate: 3.3741e-05]
	Learning Rate: 3.3741e-05
	LOSS [training: 0.22757934566918098 | validation: 0.26424122861712007]
	TIME [epoch: 74.5 sec]
EPOCH 834/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22779364355149234		[learning rate: 3.3497e-05]
	Learning Rate: 3.34965e-05
	LOSS [training: 0.22779364355149234 | validation: 0.2617696938350035]
	TIME [epoch: 74.5 sec]
EPOCH 835/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2271448220535743		[learning rate: 3.3254e-05]
	Learning Rate: 3.32539e-05
	LOSS [training: 0.2271448220535743 | validation: 0.26411890255894055]
	TIME [epoch: 74.5 sec]
EPOCH 836/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22718373120230137		[learning rate: 3.3013e-05]
	Learning Rate: 3.3013e-05
	LOSS [training: 0.22718373120230137 | validation: 0.2614866198665191]
	TIME [epoch: 74.5 sec]
EPOCH 837/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22858178034484336		[learning rate: 3.2774e-05]
	Learning Rate: 3.27738e-05
	LOSS [training: 0.22858178034484336 | validation: 0.2593266496707539]
	TIME [epoch: 74.5 sec]
EPOCH 838/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22759530446177073		[learning rate: 3.2536e-05]
	Learning Rate: 3.25363e-05
	LOSS [training: 0.22759530446177073 | validation: 0.25764827084847514]
	TIME [epoch: 74.5 sec]
EPOCH 839/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2277249295739434		[learning rate: 3.2301e-05]
	Learning Rate: 3.23006e-05
	LOSS [training: 0.2277249295739434 | validation: 0.2594937206987176]
	TIME [epoch: 74.5 sec]
EPOCH 840/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22778820780767667		[learning rate: 3.2067e-05]
	Learning Rate: 3.20666e-05
	LOSS [training: 0.22778820780767667 | validation: 0.2557328695077907]
	TIME [epoch: 74.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_840.pth
	Model improved!!!
EPOCH 841/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2284049381348741		[learning rate: 3.1834e-05]
	Learning Rate: 3.18343e-05
	LOSS [training: 0.2284049381348741 | validation: 0.2623643960749751]
	TIME [epoch: 74.5 sec]
EPOCH 842/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22716938054639133		[learning rate: 3.1604e-05]
	Learning Rate: 3.16036e-05
	LOSS [training: 0.22716938054639133 | validation: 0.2605504974892684]
	TIME [epoch: 74.5 sec]
EPOCH 843/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22621920726167272		[learning rate: 3.1375e-05]
	Learning Rate: 3.13747e-05
	LOSS [training: 0.22621920726167272 | validation: 0.2610074870518027]
	TIME [epoch: 74.5 sec]
EPOCH 844/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22642363845668012		[learning rate: 3.1147e-05]
	Learning Rate: 3.11474e-05
	LOSS [training: 0.22642363845668012 | validation: 0.2609352612346411]
	TIME [epoch: 74.5 sec]
EPOCH 845/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22721772750571256		[learning rate: 3.0922e-05]
	Learning Rate: 3.09217e-05
	LOSS [training: 0.22721772750571256 | validation: 0.2616670278312305]
	TIME [epoch: 74.4 sec]
EPOCH 846/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22608663999384482		[learning rate: 3.0698e-05]
	Learning Rate: 3.06977e-05
	LOSS [training: 0.22608663999384482 | validation: 0.2609085320059243]
	TIME [epoch: 74.5 sec]
EPOCH 847/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22749081996486656		[learning rate: 3.0475e-05]
	Learning Rate: 3.04753e-05
	LOSS [training: 0.22749081996486656 | validation: 0.26089927043230937]
	TIME [epoch: 74.5 sec]
EPOCH 848/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2267965771109784		[learning rate: 3.0254e-05]
	Learning Rate: 3.02545e-05
	LOSS [training: 0.2267965771109784 | validation: 0.2618783400163581]
	TIME [epoch: 74.5 sec]
EPOCH 849/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22627695633271314		[learning rate: 3.0035e-05]
	Learning Rate: 3.00353e-05
	LOSS [training: 0.22627695633271314 | validation: 0.2601526033446827]
	TIME [epoch: 74.5 sec]
EPOCH 850/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22774754780500212		[learning rate: 2.9818e-05]
	Learning Rate: 2.98177e-05
	LOSS [training: 0.22774754780500212 | validation: 0.262177723466299]
	TIME [epoch: 74.5 sec]
EPOCH 851/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22874125292549585		[learning rate: 2.9602e-05]
	Learning Rate: 2.96017e-05
	LOSS [training: 0.22874125292549585 | validation: 0.26028994272495687]
	TIME [epoch: 74.5 sec]
EPOCH 852/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.227122819964118		[learning rate: 2.9387e-05]
	Learning Rate: 2.93872e-05
	LOSS [training: 0.227122819964118 | validation: 0.2583552490146227]
	TIME [epoch: 74.5 sec]
EPOCH 853/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.227680414201552		[learning rate: 2.9174e-05]
	Learning Rate: 2.91743e-05
	LOSS [training: 0.227680414201552 | validation: 0.2589491558002474]
	TIME [epoch: 74.5 sec]
EPOCH 854/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22697095665887074		[learning rate: 2.8963e-05]
	Learning Rate: 2.89629e-05
	LOSS [training: 0.22697095665887074 | validation: 0.26185707425960403]
	TIME [epoch: 74.5 sec]
EPOCH 855/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.227119390323697		[learning rate: 2.8753e-05]
	Learning Rate: 2.87531e-05
	LOSS [training: 0.227119390323697 | validation: 0.25910986248572937]
	TIME [epoch: 74.5 sec]
EPOCH 856/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2253473381928604		[learning rate: 2.8545e-05]
	Learning Rate: 2.85448e-05
	LOSS [training: 0.2253473381928604 | validation: 0.26306529364994174]
	TIME [epoch: 74.5 sec]
EPOCH 857/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2260015877934456		[learning rate: 2.8338e-05]
	Learning Rate: 2.8338e-05
	LOSS [training: 0.2260015877934456 | validation: 0.26362982965877285]
	TIME [epoch: 74.5 sec]
EPOCH 858/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22701109108575437		[learning rate: 2.8133e-05]
	Learning Rate: 2.81327e-05
	LOSS [training: 0.22701109108575437 | validation: 0.2607412151839724]
	TIME [epoch: 74.5 sec]
EPOCH 859/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22808207286370932		[learning rate: 2.7929e-05]
	Learning Rate: 2.79288e-05
	LOSS [training: 0.22808207286370932 | validation: 0.2577884053599797]
	TIME [epoch: 74.4 sec]
EPOCH 860/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2254677900890347		[learning rate: 2.7726e-05]
	Learning Rate: 2.77265e-05
	LOSS [training: 0.2254677900890347 | validation: 0.2621703147319914]
	TIME [epoch: 74.5 sec]
EPOCH 861/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2269568031154097		[learning rate: 2.7526e-05]
	Learning Rate: 2.75256e-05
	LOSS [training: 0.2269568031154097 | validation: 0.2589082589356564]
	TIME [epoch: 74.5 sec]
EPOCH 862/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22776600954814294		[learning rate: 2.7326e-05]
	Learning Rate: 2.73262e-05
	LOSS [training: 0.22776600954814294 | validation: 0.2581148990177555]
	TIME [epoch: 74.5 sec]
EPOCH 863/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2258852152932422		[learning rate: 2.7128e-05]
	Learning Rate: 2.71282e-05
	LOSS [training: 0.2258852152932422 | validation: 0.25389668392035675]
	TIME [epoch: 74.5 sec]
	Saving model to: out/model_training/model_phiq_1a_v_mmd1_20240704_143517/states/model_phiq_1a_v_mmd1_863.pth
	Model improved!!!
EPOCH 864/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22616519412273592		[learning rate: 2.6932e-05]
	Learning Rate: 2.69317e-05
	LOSS [training: 0.22616519412273592 | validation: 0.2579331350360027]
	TIME [epoch: 74.5 sec]
EPOCH 865/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22776577174838836		[learning rate: 2.6737e-05]
	Learning Rate: 2.67365e-05
	LOSS [training: 0.22776577174838836 | validation: 0.259039306805537]
	TIME [epoch: 74.5 sec]
EPOCH 866/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2260033115447652		[learning rate: 2.6543e-05]
	Learning Rate: 2.65428e-05
	LOSS [training: 0.2260033115447652 | validation: 0.25896712771223246]
	TIME [epoch: 74.4 sec]
EPOCH 867/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22638502541436478		[learning rate: 2.6351e-05]
	Learning Rate: 2.63505e-05
	LOSS [training: 0.22638502541436478 | validation: 0.2598532931294065]
	TIME [epoch: 74.5 sec]
EPOCH 868/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.226214459603579		[learning rate: 2.616e-05]
	Learning Rate: 2.61596e-05
	LOSS [training: 0.226214459603579 | validation: 0.25883337601538026]
	TIME [epoch: 74.4 sec]
EPOCH 869/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2262948248518247		[learning rate: 2.597e-05]
	Learning Rate: 2.59701e-05
	LOSS [training: 0.2262948248518247 | validation: 0.2622172237197973]
	TIME [epoch: 74.4 sec]
EPOCH 870/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22696269769350827		[learning rate: 2.5782e-05]
	Learning Rate: 2.5782e-05
	LOSS [training: 0.22696269769350827 | validation: 0.258057812845653]
	TIME [epoch: 74.4 sec]
EPOCH 871/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2267380216366761		[learning rate: 2.5595e-05]
	Learning Rate: 2.55952e-05
	LOSS [training: 0.2267380216366761 | validation: 0.25554798725531347]
	TIME [epoch: 74.5 sec]
EPOCH 872/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22713180239973157		[learning rate: 2.541e-05]
	Learning Rate: 2.54097e-05
	LOSS [training: 0.22713180239973157 | validation: 0.2578778131512502]
	TIME [epoch: 74.5 sec]
EPOCH 873/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22661252613854305		[learning rate: 2.5226e-05]
	Learning Rate: 2.52256e-05
	LOSS [training: 0.22661252613854305 | validation: 0.25966400217361213]
	TIME [epoch: 74.5 sec]
EPOCH 874/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2251767164515378		[learning rate: 2.5043e-05]
	Learning Rate: 2.50429e-05
	LOSS [training: 0.2251767164515378 | validation: 0.2568765069534048]
	TIME [epoch: 74.5 sec]
EPOCH 875/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22670913567424056		[learning rate: 2.4861e-05]
	Learning Rate: 2.48614e-05
	LOSS [training: 0.22670913567424056 | validation: 0.2592695768557667]
	TIME [epoch: 74.5 sec]
EPOCH 876/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22667999743033868		[learning rate: 2.4681e-05]
	Learning Rate: 2.46813e-05
	LOSS [training: 0.22667999743033868 | validation: 0.2602856645206154]
	TIME [epoch: 74.5 sec]
EPOCH 877/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2263831816954455		[learning rate: 2.4503e-05]
	Learning Rate: 2.45025e-05
	LOSS [training: 0.2263831816954455 | validation: 0.25788812639174385]
	TIME [epoch: 74.5 sec]
EPOCH 878/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22765529982962396		[learning rate: 2.4325e-05]
	Learning Rate: 2.4325e-05
	LOSS [training: 0.22765529982962396 | validation: 0.2612630404491392]
	TIME [epoch: 74.5 sec]
EPOCH 879/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22512513468140063		[learning rate: 2.4149e-05]
	Learning Rate: 2.41488e-05
	LOSS [training: 0.22512513468140063 | validation: 0.26299697052579696]
	TIME [epoch: 74.5 sec]
EPOCH 880/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2272391339123792		[learning rate: 2.3974e-05]
	Learning Rate: 2.39738e-05
	LOSS [training: 0.2272391339123792 | validation: 0.257922123217011]
	TIME [epoch: 74.5 sec]
EPOCH 881/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22905324786198866		[learning rate: 2.38e-05]
	Learning Rate: 2.38001e-05
	LOSS [training: 0.22905324786198866 | validation: 0.25568059491609607]
	TIME [epoch: 74.5 sec]
EPOCH 882/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22451907199069937		[learning rate: 2.3628e-05]
	Learning Rate: 2.36277e-05
	LOSS [training: 0.22451907199069937 | validation: 0.2573437805780409]
	TIME [epoch: 74.5 sec]
EPOCH 883/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22550674281230312		[learning rate: 2.3457e-05]
	Learning Rate: 2.34565e-05
	LOSS [training: 0.22550674281230312 | validation: 0.26232715656850225]
	TIME [epoch: 74.5 sec]
EPOCH 884/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2268667398888869		[learning rate: 2.3287e-05]
	Learning Rate: 2.32866e-05
	LOSS [training: 0.2268667398888869 | validation: 0.2592357639833854]
	TIME [epoch: 74.5 sec]
EPOCH 885/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2249447548414839		[learning rate: 2.3118e-05]
	Learning Rate: 2.31179e-05
	LOSS [training: 0.2249447548414839 | validation: 0.2623992943295271]
	TIME [epoch: 74.5 sec]
EPOCH 886/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2260183235779148		[learning rate: 2.295e-05]
	Learning Rate: 2.29504e-05
	LOSS [training: 0.2260183235779148 | validation: 0.2562077201799658]
	TIME [epoch: 74.5 sec]
EPOCH 887/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2250052468510695		[learning rate: 2.2784e-05]
	Learning Rate: 2.27841e-05
	LOSS [training: 0.2250052468510695 | validation: 0.26101824583688504]
	TIME [epoch: 74.5 sec]
EPOCH 888/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22540677797979194		[learning rate: 2.2619e-05]
	Learning Rate: 2.2619e-05
	LOSS [training: 0.22540677797979194 | validation: 0.25977523623025656]
	TIME [epoch: 74.5 sec]
EPOCH 889/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22647415553540073		[learning rate: 2.2455e-05]
	Learning Rate: 2.24551e-05
	LOSS [training: 0.22647415553540073 | validation: 0.2618744968599476]
	TIME [epoch: 74.5 sec]
EPOCH 890/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22521841235206155		[learning rate: 2.2292e-05]
	Learning Rate: 2.22925e-05
	LOSS [training: 0.22521841235206155 | validation: 0.2575695121485816]
	TIME [epoch: 74.4 sec]
EPOCH 891/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22518100297404817		[learning rate: 2.2131e-05]
	Learning Rate: 2.2131e-05
	LOSS [training: 0.22518100297404817 | validation: 0.2585317757840258]
	TIME [epoch: 74.5 sec]
EPOCH 892/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2267748306603333		[learning rate: 2.1971e-05]
	Learning Rate: 2.19706e-05
	LOSS [training: 0.2267748306603333 | validation: 0.25987288648974627]
	TIME [epoch: 74.4 sec]
EPOCH 893/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22548595466065977		[learning rate: 2.1811e-05]
	Learning Rate: 2.18114e-05
	LOSS [training: 0.22548595466065977 | validation: 0.25731954666529044]
	TIME [epoch: 74.4 sec]
EPOCH 894/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22584761956478633		[learning rate: 2.1653e-05]
	Learning Rate: 2.16534e-05
	LOSS [training: 0.22584761956478633 | validation: 0.2566717910736803]
	TIME [epoch: 74.4 sec]
EPOCH 895/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22545281885491414		[learning rate: 2.1497e-05]
	Learning Rate: 2.14965e-05
	LOSS [training: 0.22545281885491414 | validation: 0.25957586618457534]
	TIME [epoch: 74.4 sec]
EPOCH 896/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22583593679606057		[learning rate: 2.1341e-05]
	Learning Rate: 2.13408e-05
	LOSS [training: 0.22583593679606057 | validation: 0.2616081275965419]
	TIME [epoch: 74.4 sec]
EPOCH 897/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22597577722572038		[learning rate: 2.1186e-05]
	Learning Rate: 2.11862e-05
	LOSS [training: 0.22597577722572038 | validation: 0.26174451857113923]
	TIME [epoch: 74.5 sec]
EPOCH 898/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2264550168505533		[learning rate: 2.1033e-05]
	Learning Rate: 2.10327e-05
	LOSS [training: 0.2264550168505533 | validation: 0.2570328024036753]
	TIME [epoch: 74.4 sec]
EPOCH 899/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22584046007618935		[learning rate: 2.088e-05]
	Learning Rate: 2.08803e-05
	LOSS [training: 0.22584046007618935 | validation: 0.26069604968575877]
	TIME [epoch: 74.5 sec]
EPOCH 900/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22658833143507823		[learning rate: 2.0729e-05]
	Learning Rate: 2.0729e-05
	LOSS [training: 0.22658833143507823 | validation: 0.2616819733466791]
	TIME [epoch: 74.5 sec]
EPOCH 901/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22549597473548477		[learning rate: 2.0579e-05]
	Learning Rate: 2.05789e-05
	LOSS [training: 0.22549597473548477 | validation: 0.2574837673636242]
	TIME [epoch: 74.5 sec]
EPOCH 902/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22481346770444705		[learning rate: 2.043e-05]
	Learning Rate: 2.04298e-05
	LOSS [training: 0.22481346770444705 | validation: 0.2558781790413236]
	TIME [epoch: 74.4 sec]
EPOCH 903/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2238325838985179		[learning rate: 2.0282e-05]
	Learning Rate: 2.02818e-05
	LOSS [training: 0.2238325838985179 | validation: 0.25431403064059527]
	TIME [epoch: 74.4 sec]
EPOCH 904/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22617844368397091		[learning rate: 2.0135e-05]
	Learning Rate: 2.01348e-05
	LOSS [training: 0.22617844368397091 | validation: 0.26323720876451506]
	TIME [epoch: 74.4 sec]
EPOCH 905/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22468618636898824		[learning rate: 1.9989e-05]
	Learning Rate: 1.99889e-05
	LOSS [training: 0.22468618636898824 | validation: 0.256658432146073]
	TIME [epoch: 74.4 sec]
EPOCH 906/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22744914016419363		[learning rate: 1.9844e-05]
	Learning Rate: 1.98441e-05
	LOSS [training: 0.22744914016419363 | validation: 0.25782320782203066]
	TIME [epoch: 74.4 sec]
EPOCH 907/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22610485328604765		[learning rate: 1.97e-05]
	Learning Rate: 1.97003e-05
	LOSS [training: 0.22610485328604765 | validation: 0.2542774609023293]
	TIME [epoch: 74.5 sec]
EPOCH 908/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22624426325139102		[learning rate: 1.9558e-05]
	Learning Rate: 1.95576e-05
	LOSS [training: 0.22624426325139102 | validation: 0.25962231484859244]
	TIME [epoch: 74.4 sec]
EPOCH 909/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22627131438748577		[learning rate: 1.9416e-05]
	Learning Rate: 1.94159e-05
	LOSS [training: 0.22627131438748577 | validation: 0.2589153029308363]
	TIME [epoch: 74.5 sec]
EPOCH 910/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2265260800712478		[learning rate: 1.9275e-05]
	Learning Rate: 1.92753e-05
	LOSS [training: 0.2265260800712478 | validation: 0.25792024706772065]
	TIME [epoch: 74.4 sec]
EPOCH 911/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22641021611828857		[learning rate: 1.9136e-05]
	Learning Rate: 1.91356e-05
	LOSS [training: 0.22641021611828857 | validation: 0.2582764839291255]
	TIME [epoch: 74.4 sec]
EPOCH 912/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2242290504521824		[learning rate: 1.8997e-05]
	Learning Rate: 1.8997e-05
	LOSS [training: 0.2242290504521824 | validation: 0.2560561063379633]
	TIME [epoch: 74.4 sec]
EPOCH 913/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22573812528307774		[learning rate: 1.8859e-05]
	Learning Rate: 1.88593e-05
	LOSS [training: 0.22573812528307774 | validation: 0.2557788913769613]
	TIME [epoch: 74.4 sec]
EPOCH 914/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22425191159678307		[learning rate: 1.8723e-05]
	Learning Rate: 1.87227e-05
	LOSS [training: 0.22425191159678307 | validation: 0.2555277489515658]
	TIME [epoch: 74.4 sec]
EPOCH 915/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.2260142313532975		[learning rate: 1.8587e-05]
	Learning Rate: 1.85871e-05
	LOSS [training: 0.2260142313532975 | validation: 0.25777322509229394]
	TIME [epoch: 74.4 sec]
EPOCH 916/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22478350216562862		[learning rate: 1.8452e-05]
	Learning Rate: 1.84524e-05
	LOSS [training: 0.22478350216562862 | validation: 0.2551721446217861]
	TIME [epoch: 74.4 sec]
EPOCH 917/1000:
	Training over batches...
		[batch 4/4] avg loss: 0.22606991555239941		[learning rate: 1.8319e-05]
	Learning Rate: 1.83187e-05
	LOSS [training: 0.22606991555239941 | validation: 0.2620158972921751]
	TIME [epoch: 74.5 sec]
EPOCH 918/1000:
	Training over batches...
