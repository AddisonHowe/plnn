Args:
Namespace(name='model_algphi1_1a_v_mmd1', outdir='out/model_training/model_algphi1_1a_v_mmd1', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='binary_choice', nsims_training=None, nsims_validation=None, num_epochs=500, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=False, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, signal_function='sigmoid', solver='heun', confine=False, confinement_factor=1.0, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], phi_final_act='softplus', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=0.0, init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1248660327

Training model...

Saving initial model state to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_0.pth
EPOCH 1/500:
	Training over batches...
		[batch 4/4] avg loss: 2.732352309678559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.732352309678559 | validation: 2.5039373474396864]
	TIME [epoch: 95.7 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/500:
	Training over batches...
		[batch 4/4] avg loss: 2.464916181295382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.464916181295382 | validation: 2.208775275739483]
	TIME [epoch: 4.38 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/500:
	Training over batches...
		[batch 4/4] avg loss: 2.2305552332735115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2305552332735115 | validation: 2.0616409420138186]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/500:
	Training over batches...
		[batch 4/4] avg loss: 2.0163367995381383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0163367995381383 | validation: 1.8535206456829876]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/500:
	Training over batches...
		[batch 4/4] avg loss: 1.832601668629021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.832601668629021 | validation: 1.7611549977138021]
	TIME [epoch: 4.33 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/500:
	Training over batches...
		[batch 4/4] avg loss: 1.7541699641037647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7541699641037647 | validation: 1.6983100323702296]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/500:
	Training over batches...
		[batch 4/4] avg loss: 1.6685964747636424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6685964747636424 | validation: 1.5984723486861545]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/500:
	Training over batches...
		[batch 4/4] avg loss: 1.5600231559284632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5600231559284632 | validation: 1.472149513624748]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/500:
	Training over batches...
		[batch 4/4] avg loss: 1.4700632018030835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4700632018030835 | validation: 1.3674999187794281]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/500:
	Training over batches...
		[batch 4/4] avg loss: 1.3825598474534586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3825598474534586 | validation: 1.3017135953472136]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/500:
	Training over batches...
		[batch 4/4] avg loss: 1.3083170592128408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3083170592128408 | validation: 1.2249556787202338]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/500:
	Training over batches...
		[batch 4/4] avg loss: 1.233682141934863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.233682141934863 | validation: 1.1712312158886307]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1625526880240018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1625526880240018 | validation: 1.1102586149298785]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1103563053517829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1103563053517829 | validation: 1.0815235351986698]
	TIME [epoch: 4.33 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/500:
	Training over batches...
		[batch 4/4] avg loss: 1.1064799343904366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1064799343904366 | validation: 1.0884609012630546]
	TIME [epoch: 4.3 sec]
EPOCH 16/500:
	Training over batches...
		[batch 4/4] avg loss: 1.0833718633803984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0833718633803984 | validation: 1.0669280697542325]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/500:
	Training over batches...
		[batch 4/4] avg loss: 1.0580789849874879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0580789849874879 | validation: 1.028686094988093]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/500:
	Training over batches...
		[batch 4/4] avg loss: 1.0199895528649292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0199895528649292 | validation: 0.993320236660757]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/500:
	Training over batches...
		[batch 4/4] avg loss: 0.9882610558129784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9882610558129784 | validation: 0.9749498664681415]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/500:
	Training over batches...
		[batch 4/4] avg loss: 0.9770595149201581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9770595149201581 | validation: 0.9865900730964253]
	TIME [epoch: 4.29 sec]
EPOCH 21/500:
	Training over batches...
		[batch 4/4] avg loss: 1.0006876476091624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0006876476091624 | validation: 1.0139919651679592]
	TIME [epoch: 4.35 sec]
EPOCH 22/500:
	Training over batches...
		[batch 4/4] avg loss: 0.9952843970275382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9952843970275382 | validation: 1.000800025796813]
	TIME [epoch: 4.31 sec]
EPOCH 23/500:
	Training over batches...
		[batch 4/4] avg loss: 0.9974989141044873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9974989141044873 | validation: 0.9880984047137152]
	TIME [epoch: 4.31 sec]
EPOCH 24/500:
	Training over batches...
		[batch 4/4] avg loss: 0.9822138206775367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9822138206775367 | validation: 0.9652200396700136]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/500:
	Training over batches...
		[batch 4/4] avg loss: 0.9625413523566934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9625413523566934 | validation: 0.9515061884681414]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/500:
	Training over batches...
		[batch 4/4] avg loss: 0.9543768924521062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9543768924521062 | validation: 0.9611669735295705]
	TIME [epoch: 4.3 sec]
EPOCH 27/500:
	Training over batches...
		[batch 4/4] avg loss: 0.9560985672499579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9560985672499579 | validation: 0.9555566854590656]
	TIME [epoch: 4.28 sec]
EPOCH 28/500:
	Training over batches...
		[batch 4/4] avg loss: 0.9399159368111236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9399159368111236 | validation: 0.9395711467626646]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/500:
	Training over batches...
		[batch 4/4] avg loss: 0.948951672442084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.948951672442084 | validation: 0.9538318669605438]
	TIME [epoch: 4.3 sec]
EPOCH 30/500:
	Training over batches...
		[batch 4/4] avg loss: 0.9573082182513624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9573082182513624 | validation: 0.9445288864353263]
	TIME [epoch: 4.3 sec]
EPOCH 31/500:
	Training over batches...
		[batch 4/4] avg loss: 0.9408430559537548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9408430559537548 | validation: 0.9532685580612508]
	TIME [epoch: 4.29 sec]
EPOCH 32/500:
	Training over batches...
		[batch 4/4] avg loss: 0.9446260900982816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9446260900982816 | validation: 0.9258376579832737]
	TIME [epoch: 4.33 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/500:
	Training over batches...
		[batch 4/4] avg loss: 0.9280818708269248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9280818708269248 | validation: 0.919219887002879]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/500:
	Training over batches...
		[batch 4/4] avg loss: 0.9111919265589391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9111919265589391 | validation: 0.8956229782176173]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/500:
	Training over batches...
		[batch 4/4] avg loss: 0.9084436582707689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9084436582707689 | validation: 0.949592197621655]
	TIME [epoch: 4.3 sec]
EPOCH 36/500:
	Training over batches...
		[batch 4/4] avg loss: 0.936611039024974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.936611039024974 | validation: 0.9085253672427342]
	TIME [epoch: 4.29 sec]
EPOCH 37/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8981816066950926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8981816066950926 | validation: 0.8888774289803781]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/500:
	Training over batches...
		[batch 4/4] avg loss: 0.878127289546337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.878127289546337 | validation: 0.8772499656129584]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8783254234427045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8783254234427045 | validation: 0.8851764168992351]
	TIME [epoch: 4.28 sec]
EPOCH 40/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8851160277094413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8851160277094413 | validation: 0.9159977451208101]
	TIME [epoch: 4.28 sec]
EPOCH 41/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8915552226884167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8915552226884167 | validation: 0.8856121730207624]
	TIME [epoch: 4.29 sec]
EPOCH 42/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8745213643109778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8745213643109778 | validation: 0.8843223302443225]
	TIME [epoch: 4.3 sec]
EPOCH 43/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8736299233960618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8736299233960618 | validation: 0.8661860822121636]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8726547034386842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8726547034386842 | validation: 0.8751068107841933]
	TIME [epoch: 4.29 sec]
EPOCH 45/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8689405635311188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8689405635311188 | validation: 0.8710262058778846]
	TIME [epoch: 4.29 sec]
EPOCH 46/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8699976200273145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8699976200273145 | validation: 0.8791386069256235]
	TIME [epoch: 4.28 sec]
EPOCH 47/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8723772598572719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8723772598572719 | validation: 0.8759130438081186]
	TIME [epoch: 4.29 sec]
EPOCH 48/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8643474941450802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8643474941450802 | validation: 0.8715798085526372]
	TIME [epoch: 4.29 sec]
EPOCH 49/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8682036593463477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8682036593463477 | validation: 0.8954566777982753]
	TIME [epoch: 4.29 sec]
EPOCH 50/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8871817049051073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8871817049051073 | validation: 0.9005125969823047]
	TIME [epoch: 4.28 sec]
EPOCH 51/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8987081748453981		[learning rate: 0.0098855]
	Learning Rate: 0.00988553
	LOSS [training: 0.8987081748453981 | validation: 0.9188763442727494]
	TIME [epoch: 4.3 sec]
EPOCH 52/500:
	Training over batches...
		[batch 4/4] avg loss: 0.9118915613205003		[learning rate: 0.0097349]
	Learning Rate: 0.00973494
	LOSS [training: 0.9118915613205003 | validation: 0.907631981708266]
	TIME [epoch: 4.32 sec]
EPOCH 53/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8868312347331921		[learning rate: 0.0095866]
	Learning Rate: 0.00958665
	LOSS [training: 0.8868312347331921 | validation: 0.8898065526689934]
	TIME [epoch: 4.29 sec]
EPOCH 54/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8884927755990233		[learning rate: 0.0094406]
	Learning Rate: 0.00944061
	LOSS [training: 0.8884927755990233 | validation: 0.9042509491943681]
	TIME [epoch: 4.28 sec]
EPOCH 55/500:
	Training over batches...
		[batch 4/4] avg loss: 0.910670534707625		[learning rate: 0.0092968]
	Learning Rate: 0.0092968
	LOSS [training: 0.910670534707625 | validation: 0.8924279149797649]
	TIME [epoch: 4.29 sec]
EPOCH 56/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8789344174407497		[learning rate: 0.0091552]
	Learning Rate: 0.00915517
	LOSS [training: 0.8789344174407497 | validation: 0.8842630637066382]
	TIME [epoch: 4.28 sec]
EPOCH 57/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8813142935049032		[learning rate: 0.0090157]
	Learning Rate: 0.00901571
	LOSS [training: 0.8813142935049032 | validation: 0.8831761665319477]
	TIME [epoch: 4.28 sec]
EPOCH 58/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8771923640600647		[learning rate: 0.0088784]
	Learning Rate: 0.00887837
	LOSS [training: 0.8771923640600647 | validation: 0.8708880508178092]
	TIME [epoch: 4.28 sec]
EPOCH 59/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8571664973165177		[learning rate: 0.0087431]
	Learning Rate: 0.00874312
	LOSS [training: 0.8571664973165177 | validation: 0.8555896223108226]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8507068927349459		[learning rate: 0.0086099]
	Learning Rate: 0.00860994
	LOSS [training: 0.8507068927349459 | validation: 0.8543830224808819]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8537231150194838		[learning rate: 0.0084788]
	Learning Rate: 0.00847878
	LOSS [training: 0.8537231150194838 | validation: 0.8613131819593208]
	TIME [epoch: 4.34 sec]
EPOCH 62/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8576220871834406		[learning rate: 0.0083496]
	Learning Rate: 0.00834962
	LOSS [training: 0.8576220871834406 | validation: 0.8714565378391481]
	TIME [epoch: 4.31 sec]
EPOCH 63/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8496056123088811		[learning rate: 0.0082224]
	Learning Rate: 0.00822243
	LOSS [training: 0.8496056123088811 | validation: 0.8575568914721329]
	TIME [epoch: 4.3 sec]
EPOCH 64/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8560485425576958		[learning rate: 0.0080972]
	Learning Rate: 0.00809717
	LOSS [training: 0.8560485425576958 | validation: 0.8676888310057047]
	TIME [epoch: 4.29 sec]
EPOCH 65/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8684637487708973		[learning rate: 0.0079738]
	Learning Rate: 0.00797382
	LOSS [training: 0.8684637487708973 | validation: 0.8778985975310023]
	TIME [epoch: 4.29 sec]
EPOCH 66/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8674117061267581		[learning rate: 0.0078524]
	Learning Rate: 0.00785236
	LOSS [training: 0.8674117061267581 | validation: 0.8768574882094919]
	TIME [epoch: 4.29 sec]
EPOCH 67/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8652063354646335		[learning rate: 0.0077327]
	Learning Rate: 0.00773274
	LOSS [training: 0.8652063354646335 | validation: 0.8601468278986484]
	TIME [epoch: 4.29 sec]
EPOCH 68/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8582221867896137		[learning rate: 0.0076149]
	Learning Rate: 0.00761494
	LOSS [training: 0.8582221867896137 | validation: 0.8677640657473291]
	TIME [epoch: 4.29 sec]
EPOCH 69/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8626499160310329		[learning rate: 0.0074989]
	Learning Rate: 0.00749894
	LOSS [training: 0.8626499160310329 | validation: 0.8751947961777665]
	TIME [epoch: 4.29 sec]
EPOCH 70/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8731989469768845		[learning rate: 0.0073847]
	Learning Rate: 0.00738471
	LOSS [training: 0.8731989469768845 | validation: 0.8798760028064123]
	TIME [epoch: 4.29 sec]
EPOCH 71/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8722451587337727		[learning rate: 0.0072722]
	Learning Rate: 0.00727221
	LOSS [training: 0.8722451587337727 | validation: 0.8820726882530021]
	TIME [epoch: 4.33 sec]
EPOCH 72/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8714517640564885		[learning rate: 0.0071614]
	Learning Rate: 0.00716143
	LOSS [training: 0.8714517640564885 | validation: 0.8804658176136276]
	TIME [epoch: 4.31 sec]
EPOCH 73/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8659520531167365		[learning rate: 0.0070523]
	Learning Rate: 0.00705234
	LOSS [training: 0.8659520531167365 | validation: 0.869470203456233]
	TIME [epoch: 4.29 sec]
EPOCH 74/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8755692292873434		[learning rate: 0.0069449]
	Learning Rate: 0.00694491
	LOSS [training: 0.8755692292873434 | validation: 0.8915461062635219]
	TIME [epoch: 4.29 sec]
EPOCH 75/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8862602190774267		[learning rate: 0.0068391]
	Learning Rate: 0.00683912
	LOSS [training: 0.8862602190774267 | validation: 0.8855742231017187]
	TIME [epoch: 4.29 sec]
EPOCH 76/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8718134435078644		[learning rate: 0.0067349]
	Learning Rate: 0.00673493
	LOSS [training: 0.8718134435078644 | validation: 0.8769640115733622]
	TIME [epoch: 4.29 sec]
EPOCH 77/500:
	Training over batches...
		[batch 4/4] avg loss: 0.870998406219901		[learning rate: 0.0066323]
	Learning Rate: 0.00663234
	LOSS [training: 0.870998406219901 | validation: 0.8707196892286062]
	TIME [epoch: 4.29 sec]
EPOCH 78/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8695328010799226		[learning rate: 0.0065313]
	Learning Rate: 0.00653131
	LOSS [training: 0.8695328010799226 | validation: 0.883732835735211]
	TIME [epoch: 4.29 sec]
EPOCH 79/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8781785285613535		[learning rate: 0.0064318]
	Learning Rate: 0.00643181
	LOSS [training: 0.8781785285613535 | validation: 0.8753206484245706]
	TIME [epoch: 4.29 sec]
EPOCH 80/500:
	Training over batches...
		[batch 4/4] avg loss: 0.868671385878399		[learning rate: 0.0063338]
	Learning Rate: 0.00633383
	LOSS [training: 0.868671385878399 | validation: 0.8804518838399993]
	TIME [epoch: 4.29 sec]
EPOCH 81/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8693317621918328		[learning rate: 0.0062373]
	Learning Rate: 0.00623735
	LOSS [training: 0.8693317621918328 | validation: 0.8706772278302217]
	TIME [epoch: 4.32 sec]
EPOCH 82/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8679784523747174		[learning rate: 0.0061423]
	Learning Rate: 0.00614233
	LOSS [training: 0.8679784523747174 | validation: 0.8802834734499596]
	TIME [epoch: 4.31 sec]
EPOCH 83/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8937770337711656		[learning rate: 0.0060488]
	Learning Rate: 0.00604876
	LOSS [training: 0.8937770337711656 | validation: 0.9474920567593768]
	TIME [epoch: 4.29 sec]
EPOCH 84/500:
	Training over batches...
		[batch 4/4] avg loss: 0.9108950716369766		[learning rate: 0.0059566]
	Learning Rate: 0.00595662
	LOSS [training: 0.9108950716369766 | validation: 0.924002430814255]
	TIME [epoch: 4.29 sec]
EPOCH 85/500:
	Training over batches...
		[batch 4/4] avg loss: 0.9016028495775406		[learning rate: 0.0058659]
	Learning Rate: 0.00586588
	LOSS [training: 0.9016028495775406 | validation: 0.9186055140945559]
	TIME [epoch: 4.29 sec]
EPOCH 86/500:
	Training over batches...
		[batch 4/4] avg loss: 0.9008295077602768		[learning rate: 0.0057765]
	Learning Rate: 0.00577653
	LOSS [training: 0.9008295077602768 | validation: 0.9256903195720532]
	TIME [epoch: 4.29 sec]
EPOCH 87/500:
	Training over batches...
		[batch 4/4] avg loss: 0.9007472683842721		[learning rate: 0.0056885]
	Learning Rate: 0.00568853
	LOSS [training: 0.9007472683842721 | validation: 0.8955868696129099]
	TIME [epoch: 4.29 sec]
EPOCH 88/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8809026024130333		[learning rate: 0.0056019]
	Learning Rate: 0.00560187
	LOSS [training: 0.8809026024130333 | validation: 0.8756613635306669]
	TIME [epoch: 4.29 sec]
EPOCH 89/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8711252768923473		[learning rate: 0.0055165]
	Learning Rate: 0.00551654
	LOSS [training: 0.8711252768923473 | validation: 0.87405419073342]
	TIME [epoch: 4.28 sec]
EPOCH 90/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8686693438778919		[learning rate: 0.0054325]
	Learning Rate: 0.0054325
	LOSS [training: 0.8686693438778919 | validation: 0.8682074658832628]
	TIME [epoch: 4.29 sec]
EPOCH 91/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8648932177754272		[learning rate: 0.0053497]
	Learning Rate: 0.00534975
	LOSS [training: 0.8648932177754272 | validation: 0.8681646710417503]
	TIME [epoch: 4.32 sec]
EPOCH 92/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8699348754644041		[learning rate: 0.0052683]
	Learning Rate: 0.00526825
	LOSS [training: 0.8699348754644041 | validation: 0.8986176437542578]
	TIME [epoch: 4.33 sec]
EPOCH 93/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8901526999218173		[learning rate: 0.005188]
	Learning Rate: 0.005188
	LOSS [training: 0.8901526999218173 | validation: 0.881699146138532]
	TIME [epoch: 4.29 sec]
EPOCH 94/500:
	Training over batches...
		[batch 4/4] avg loss: 0.883560638810621		[learning rate: 0.005109]
	Learning Rate: 0.00510897
	LOSS [training: 0.883560638810621 | validation: 0.8718337235518029]
	TIME [epoch: 4.29 sec]
EPOCH 95/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8687252882960146		[learning rate: 0.0050311]
	Learning Rate: 0.00503114
	LOSS [training: 0.8687252882960146 | validation: 0.8709121415583896]
	TIME [epoch: 4.29 sec]
EPOCH 96/500:
	Training over batches...
		[batch 4/4] avg loss: 0.86819940807246		[learning rate: 0.0049545]
	Learning Rate: 0.0049545
	LOSS [training: 0.86819940807246 | validation: 0.8741889195424404]
	TIME [epoch: 4.29 sec]
EPOCH 97/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8650624008170505		[learning rate: 0.004879]
	Learning Rate: 0.00487903
	LOSS [training: 0.8650624008170505 | validation: 0.8687376053849678]
	TIME [epoch: 4.29 sec]
EPOCH 98/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8634316953427905		[learning rate: 0.0048047]
	Learning Rate: 0.0048047
	LOSS [training: 0.8634316953427905 | validation: 0.8710739150273414]
	TIME [epoch: 4.29 sec]
EPOCH 99/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8670802718407091		[learning rate: 0.0047315]
	Learning Rate: 0.00473151
	LOSS [training: 0.8670802718407091 | validation: 0.8745412329709703]
	TIME [epoch: 4.29 sec]
EPOCH 100/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8753791989859429		[learning rate: 0.0046594]
	Learning Rate: 0.00465944
	LOSS [training: 0.8753791989859429 | validation: 0.8783222952029675]
	TIME [epoch: 4.29 sec]
EPOCH 101/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8687068324038147		[learning rate: 0.0045885]
	Learning Rate: 0.00458846
	LOSS [training: 0.8687068324038147 | validation: 0.8705122145426023]
	TIME [epoch: 4.33 sec]
EPOCH 102/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8613440340828846		[learning rate: 0.0045186]
	Learning Rate: 0.00451856
	LOSS [training: 0.8613440340828846 | validation: 0.8662550865708707]
	TIME [epoch: 4.3 sec]
EPOCH 103/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8631782495404291		[learning rate: 0.0044497]
	Learning Rate: 0.00444973
	LOSS [training: 0.8631782495404291 | validation: 0.8684997527167225]
	TIME [epoch: 4.29 sec]
EPOCH 104/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8651205653354488		[learning rate: 0.0043819]
	Learning Rate: 0.00438194
	LOSS [training: 0.8651205653354488 | validation: 0.8659206042066865]
	TIME [epoch: 4.29 sec]
EPOCH 105/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8642402600222037		[learning rate: 0.0043152]
	Learning Rate: 0.00431519
	LOSS [training: 0.8642402600222037 | validation: 0.8692464073901556]
	TIME [epoch: 4.3 sec]
EPOCH 106/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8661019486063293		[learning rate: 0.0042495]
	Learning Rate: 0.00424946
	LOSS [training: 0.8661019486063293 | validation: 0.8707472498572459]
	TIME [epoch: 4.29 sec]
EPOCH 107/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8660584675597423		[learning rate: 0.0041847]
	Learning Rate: 0.00418472
	LOSS [training: 0.8660584675597423 | validation: 0.86863878885366]
	TIME [epoch: 4.29 sec]
EPOCH 108/500:
	Training over batches...
		[batch 4/4] avg loss: 0.867055089967887		[learning rate: 0.004121]
	Learning Rate: 0.00412098
	LOSS [training: 0.867055089967887 | validation: 0.8688053900004649]
	TIME [epoch: 4.29 sec]
EPOCH 109/500:
	Training over batches...
		[batch 4/4] avg loss: 0.874133269047557		[learning rate: 0.0040582]
	Learning Rate: 0.0040582
	LOSS [training: 0.874133269047557 | validation: 0.8775295297695096]
	TIME [epoch: 4.29 sec]
EPOCH 110/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8762740967064501		[learning rate: 0.0039964]
	Learning Rate: 0.00399638
	LOSS [training: 0.8762740967064501 | validation: 0.8736210770307178]
	TIME [epoch: 4.29 sec]
EPOCH 111/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8882650494493319		[learning rate: 0.0039355]
	Learning Rate: 0.0039355
	LOSS [training: 0.8882650494493319 | validation: 0.8817559647180118]
	TIME [epoch: 4.33 sec]
EPOCH 112/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8873303891914964		[learning rate: 0.0038755]
	Learning Rate: 0.00387555
	LOSS [training: 0.8873303891914964 | validation: 0.8806670241779663]
	TIME [epoch: 4.31 sec]
EPOCH 113/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8887138694752736		[learning rate: 0.0038165]
	Learning Rate: 0.00381651
	LOSS [training: 0.8887138694752736 | validation: 0.8827874419001007]
	TIME [epoch: 4.3 sec]
EPOCH 114/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8962209543836286		[learning rate: 0.0037584]
	Learning Rate: 0.00375837
	LOSS [training: 0.8962209543836286 | validation: 0.8971882589855089]
	TIME [epoch: 4.29 sec]
EPOCH 115/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8969823174287637		[learning rate: 0.0037011]
	Learning Rate: 0.00370112
	LOSS [training: 0.8969823174287637 | validation: 0.8845394161280358]
	TIME [epoch: 4.3 sec]
EPOCH 116/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8867055089162402		[learning rate: 0.0036447]
	Learning Rate: 0.00364474
	LOSS [training: 0.8867055089162402 | validation: 0.8784071476292885]
	TIME [epoch: 4.29 sec]
EPOCH 117/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8806761162223031		[learning rate: 0.0035892]
	Learning Rate: 0.00358922
	LOSS [training: 0.8806761162223031 | validation: 0.8747654022744902]
	TIME [epoch: 4.29 sec]
EPOCH 118/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8830479890829669		[learning rate: 0.0035345]
	Learning Rate: 0.00353454
	LOSS [training: 0.8830479890829669 | validation: 0.8813978728512012]
	TIME [epoch: 4.29 sec]
EPOCH 119/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8848095131666879		[learning rate: 0.0034807]
	Learning Rate: 0.0034807
	LOSS [training: 0.8848095131666879 | validation: 0.8743897039569875]
	TIME [epoch: 4.29 sec]
EPOCH 120/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8839990161207838		[learning rate: 0.0034277]
	Learning Rate: 0.00342768
	LOSS [training: 0.8839990161207838 | validation: 0.8826970208110073]
	TIME [epoch: 4.29 sec]
EPOCH 121/500:
	Training over batches...
		[batch 4/4] avg loss: 0.899408624557746		[learning rate: 0.0033755]
	Learning Rate: 0.00337546
	LOSS [training: 0.899408624557746 | validation: 0.8957529331740732]
	TIME [epoch: 4.33 sec]
EPOCH 122/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8970177457540109		[learning rate: 0.003324]
	Learning Rate: 0.00332404
	LOSS [training: 0.8970177457540109 | validation: 0.886256986649548]
	TIME [epoch: 4.3 sec]
EPOCH 123/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8931615013451626		[learning rate: 0.0032734]
	Learning Rate: 0.00327341
	LOSS [training: 0.8931615013451626 | validation: 0.8869400479030467]
	TIME [epoch: 4.29 sec]
EPOCH 124/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8866733159422933		[learning rate: 0.0032235]
	Learning Rate: 0.00322354
	LOSS [training: 0.8866733159422933 | validation: 0.8802721929848063]
	TIME [epoch: 4.29 sec]
EPOCH 125/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8901702758355805		[learning rate: 0.0031744]
	Learning Rate: 0.00317444
	LOSS [training: 0.8901702758355805 | validation: 0.8895773567927066]
	TIME [epoch: 4.29 sec]
EPOCH 126/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8955376564436355		[learning rate: 0.0031261]
	Learning Rate: 0.00312608
	LOSS [training: 0.8955376564436355 | validation: 0.8871583352639945]
	TIME [epoch: 4.3 sec]
EPOCH 127/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8944040135478969		[learning rate: 0.0030785]
	Learning Rate: 0.00307846
	LOSS [training: 0.8944040135478969 | validation: 0.8801928079440474]
	TIME [epoch: 4.29 sec]
EPOCH 128/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8820491359193686		[learning rate: 0.0030316]
	Learning Rate: 0.00303156
	LOSS [training: 0.8820491359193686 | validation: 0.86846603189301]
	TIME [epoch: 4.29 sec]
EPOCH 129/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8696758800808614		[learning rate: 0.0029854]
	Learning Rate: 0.00298538
	LOSS [training: 0.8696758800808614 | validation: 0.8662160117352601]
	TIME [epoch: 4.29 sec]
EPOCH 130/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8638761827424586		[learning rate: 0.0029399]
	Learning Rate: 0.0029399
	LOSS [training: 0.8638761827424586 | validation: 0.8650561653721135]
	TIME [epoch: 4.29 sec]
EPOCH 131/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8636406298867254		[learning rate: 0.0028951]
	Learning Rate: 0.00289512
	LOSS [training: 0.8636406298867254 | validation: 0.8668231569103055]
	TIME [epoch: 4.33 sec]
EPOCH 132/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8610727636731202		[learning rate: 0.002851]
	Learning Rate: 0.00285102
	LOSS [training: 0.8610727636731202 | validation: 0.8698129847474996]
	TIME [epoch: 4.3 sec]
EPOCH 133/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8616563881676182		[learning rate: 0.0028076]
	Learning Rate: 0.00280759
	LOSS [training: 0.8616563881676182 | validation: 0.8670026779149045]
	TIME [epoch: 4.29 sec]
EPOCH 134/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8615555303203221		[learning rate: 0.0027648]
	Learning Rate: 0.00276482
	LOSS [training: 0.8615555303203221 | validation: 0.8691157843471796]
	TIME [epoch: 4.29 sec]
EPOCH 135/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8645848405422321		[learning rate: 0.0027227]
	Learning Rate: 0.0027227
	LOSS [training: 0.8645848405422321 | validation: 0.8667160655717789]
	TIME [epoch: 4.3 sec]
EPOCH 136/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8626379874158544		[learning rate: 0.0026812]
	Learning Rate: 0.00268123
	LOSS [training: 0.8626379874158544 | validation: 0.86201983056134]
	TIME [epoch: 4.29 sec]
EPOCH 137/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8598499857157962		[learning rate: 0.0026404]
	Learning Rate: 0.00264038
	LOSS [training: 0.8598499857157962 | validation: 0.8649443004099902]
	TIME [epoch: 4.29 sec]
EPOCH 138/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8596865223460028		[learning rate: 0.0026002]
	Learning Rate: 0.00260016
	LOSS [training: 0.8596865223460028 | validation: 0.8584197066522589]
	TIME [epoch: 4.29 sec]
EPOCH 139/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8581006056066774		[learning rate: 0.0025606]
	Learning Rate: 0.00256055
	LOSS [training: 0.8581006056066774 | validation: 0.8562919435718916]
	TIME [epoch: 4.29 sec]
EPOCH 140/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8567989499500781		[learning rate: 0.0025215]
	Learning Rate: 0.00252154
	LOSS [training: 0.8567989499500781 | validation: 0.8651647388430865]
	TIME [epoch: 4.29 sec]
EPOCH 141/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8538195283485124		[learning rate: 0.0024831]
	Learning Rate: 0.00248313
	LOSS [training: 0.8538195283485124 | validation: 0.8633793007778606]
	TIME [epoch: 4.32 sec]
EPOCH 142/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8542834418741643		[learning rate: 0.0024453]
	Learning Rate: 0.00244531
	LOSS [training: 0.8542834418741643 | validation: 0.8632924157065822]
	TIME [epoch: 4.32 sec]
EPOCH 143/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8558321733535581		[learning rate: 0.0024081]
	Learning Rate: 0.00240806
	LOSS [training: 0.8558321733535581 | validation: 0.8641462133302147]
	TIME [epoch: 4.3 sec]
EPOCH 144/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8571492560549854		[learning rate: 0.0023714]
	Learning Rate: 0.00237137
	LOSS [training: 0.8571492560549854 | validation: 0.8712345795392294]
	TIME [epoch: 4.29 sec]
EPOCH 145/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8591610116724951		[learning rate: 0.0023352]
	Learning Rate: 0.00233525
	LOSS [training: 0.8591610116724951 | validation: 0.8704962144911064]
	TIME [epoch: 4.3 sec]
EPOCH 146/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8602404834294036		[learning rate: 0.0022997]
	Learning Rate: 0.00229968
	LOSS [training: 0.8602404834294036 | validation: 0.8692662681272214]
	TIME [epoch: 4.3 sec]
EPOCH 147/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8626810219971819		[learning rate: 0.0022646]
	Learning Rate: 0.00226464
	LOSS [training: 0.8626810219971819 | validation: 0.8625526845286613]
	TIME [epoch: 4.3 sec]
EPOCH 148/500:
	Training over batches...
		[batch 4/4] avg loss: 0.859832928003571		[learning rate: 0.0022301]
	Learning Rate: 0.00223015
	LOSS [training: 0.859832928003571 | validation: 0.8714784430440472]
	TIME [epoch: 4.29 sec]
EPOCH 149/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8602606801571883		[learning rate: 0.0021962]
	Learning Rate: 0.00219617
	LOSS [training: 0.8602606801571883 | validation: 0.8717750929838911]
	TIME [epoch: 4.29 sec]
EPOCH 150/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8577594253550599		[learning rate: 0.0021627]
	Learning Rate: 0.00216272
	LOSS [training: 0.8577594253550599 | validation: 0.8660885268166161]
	TIME [epoch: 4.29 sec]
EPOCH 151/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8548323020143945		[learning rate: 0.0021298]
	Learning Rate: 0.00212977
	LOSS [training: 0.8548323020143945 | validation: 0.8606080425839358]
	TIME [epoch: 4.32 sec]
EPOCH 152/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8573334661327804		[learning rate: 0.0020973]
	Learning Rate: 0.00209733
	LOSS [training: 0.8573334661327804 | validation: 0.8678778856085992]
	TIME [epoch: 4.32 sec]
EPOCH 153/500:
	Training over batches...
		[batch 4/4] avg loss: 0.858883777359858		[learning rate: 0.0020654]
	Learning Rate: 0.00206538
	LOSS [training: 0.858883777359858 | validation: 0.8669994921835256]
	TIME [epoch: 4.3 sec]
EPOCH 154/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8624897441529678		[learning rate: 0.0020339]
	Learning Rate: 0.00203392
	LOSS [training: 0.8624897441529678 | validation: 0.8765284998363889]
	TIME [epoch: 4.29 sec]
EPOCH 155/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8666190153282659		[learning rate: 0.0020029]
	Learning Rate: 0.00200293
	LOSS [training: 0.8666190153282659 | validation: 0.8712566543978424]
	TIME [epoch: 4.3 sec]
EPOCH 156/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8662311012678786		[learning rate: 0.0019724]
	Learning Rate: 0.00197242
	LOSS [training: 0.8662311012678786 | validation: 0.8696223036634984]
	TIME [epoch: 4.3 sec]
EPOCH 157/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8608181193013316		[learning rate: 0.0019424]
	Learning Rate: 0.00194238
	LOSS [training: 0.8608181193013316 | validation: 0.8657041268195738]
	TIME [epoch: 4.29 sec]
EPOCH 158/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8590372509443361		[learning rate: 0.0019128]
	Learning Rate: 0.00191279
	LOSS [training: 0.8590372509443361 | validation: 0.8672548391050059]
	TIME [epoch: 4.29 sec]
EPOCH 159/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8573370619835842		[learning rate: 0.0018836]
	Learning Rate: 0.00188365
	LOSS [training: 0.8573370619835842 | validation: 0.868001171844035]
	TIME [epoch: 4.29 sec]
EPOCH 160/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8595485445571708		[learning rate: 0.001855]
	Learning Rate: 0.00185495
	LOSS [training: 0.8595485445571708 | validation: 0.8685953450470902]
	TIME [epoch: 4.29 sec]
EPOCH 161/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8556888822387027		[learning rate: 0.0018267]
	Learning Rate: 0.0018267
	LOSS [training: 0.8556888822387027 | validation: 0.8670164356646324]
	TIME [epoch: 4.32 sec]
	Saving model to: out/model_training/model_algphi1_1a_v_mmd1_20240703_143343/states/model_algphi1_1a_v_mmd1_161.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 805.853 seconds.
