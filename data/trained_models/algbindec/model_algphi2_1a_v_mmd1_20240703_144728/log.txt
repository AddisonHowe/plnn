Args:
Namespace(name='model_algphi2_1a_v_mmd1', outdir='out/model_training/model_algphi2_1a_v_mmd1', training_data='data/training_data/data_phi2_1a/training', validation_data='data/training_data/data_phi2_1a/validation', model_type='binary_flip', nsims_training=None, nsims_validation=None, num_epochs=500, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=False, cf_reduction_factor=0.1, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, signal_function='sigmoid', solver='heun', confine=False, confinement_factor=1.0, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], phi_final_act='softplus', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=0.0, init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1132767451

Training model...

Saving initial model state to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_0.pth
EPOCH 1/500:
	Training over batches...
		[batch 4/4] avg loss: 2.397890124765606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.397890124765606 | validation: 1.2718177436588092]
	TIME [epoch: 95.7 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/500:
	Training over batches...
		[batch 4/4] avg loss: 1.305054780026553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.305054780026553 | validation: 0.7830289173800089]
	TIME [epoch: 4.38 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/500:
	Training over batches...
		[batch 4/4] avg loss: 0.9493511853621948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9493511853621948 | validation: 0.6825270607636864]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/500:
	Training over batches...
		[batch 4/4] avg loss: 0.8498888962875014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8498888962875014 | validation: 0.6244662897218645]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/500:
	Training over batches...
		[batch 4/4] avg loss: 0.787856690910215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.787856690910215 | validation: 0.5834530158608864]
	TIME [epoch: 4.34 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/500:
	Training over batches...
		[batch 4/4] avg loss: 0.7379476929314888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7379476929314888 | validation: 0.5410413103560536]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/500:
	Training over batches...
		[batch 4/4] avg loss: 0.6871667786293967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6871667786293967 | validation: 0.5091632495375807]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/500:
	Training over batches...
		[batch 4/4] avg loss: 0.626896979588736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.626896979588736 | validation: 0.4714689744632854]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/500:
	Training over batches...
		[batch 4/4] avg loss: 0.5446993752125601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5446993752125601 | validation: 0.4471867286466381]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/500:
	Training over batches...
		[batch 4/4] avg loss: 0.448006296456267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.448006296456267 | validation: 0.34362226254347605]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/500:
	Training over batches...
		[batch 4/4] avg loss: 0.3377889063925243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3377889063925243 | validation: 0.2235287036169525]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/500:
	Training over batches...
		[batch 4/4] avg loss: 0.23863185257918323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23863185257918323 | validation: 0.1600699531527907]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/500:
	Training over batches...
		[batch 4/4] avg loss: 0.17733671779933302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17733671779933302 | validation: 0.13115182792898863]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/500:
	Training over batches...
		[batch 4/4] avg loss: 0.14509232495678442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14509232495678442 | validation: 0.11186347882101552]
	TIME [epoch: 4.33 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/500:
	Training over batches...
		[batch 4/4] avg loss: 0.12524573948060003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12524573948060003 | validation: 0.11090037644641787]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/500:
	Training over batches...
		[batch 4/4] avg loss: 0.11360400883355973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11360400883355973 | validation: 0.08489179963438197]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0957375035054297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0957375035054297 | validation: 0.0844006231129349]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/500:
	Training over batches...
		[batch 4/4] avg loss: 0.08312781167168265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08312781167168265 | validation: 0.06146426044193674]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/500:
	Training over batches...
		[batch 4/4] avg loss: 0.06807895476845163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06807895476845163 | validation: 0.05119060601607994]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/500:
	Training over batches...
		[batch 4/4] avg loss: 0.055442685355348006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055442685355348006 | validation: 0.04043737402292651]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/500:
	Training over batches...
		[batch 4/4] avg loss: 0.04752427217534508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04752427217534508 | validation: 0.03531704766318358]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/500:
	Training over batches...
		[batch 4/4] avg loss: 0.03672003813206787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03672003813206787 | validation: 0.028805475913474515]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/500:
	Training over batches...
		[batch 4/4] avg loss: 0.02918248740589402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02918248740589402 | validation: 0.0236969551429936]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/500:
	Training over batches...
		[batch 4/4] avg loss: 0.023114442936274293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023114442936274293 | validation: 0.015047246861380665]
	TIME [epoch: 4.32 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/500:
	Training over batches...
		[batch 4/4] avg loss: 0.015359226907496932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015359226907496932 | validation: 0.010342118420202898]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/500:
	Training over batches...
		[batch 4/4] avg loss: 0.011755361953204448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011755361953204448 | validation: 0.014749297943207288]
	TIME [epoch: 4.31 sec]
EPOCH 27/500:
	Training over batches...
		[batch 4/4] avg loss: 0.016687406197872637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016687406197872637 | validation: 0.012762615796872462]
	TIME [epoch: 4.3 sec]
EPOCH 28/500:
	Training over batches...
		[batch 4/4] avg loss: 0.013270484362137236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013270484362137236 | validation: 0.009446296361530155]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/500:
	Training over batches...
		[batch 4/4] avg loss: 0.01110277088546873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01110277088546873 | validation: 0.009363836869794473]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/500:
	Training over batches...
		[batch 4/4] avg loss: 0.006688033964818043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.006688033964818043 | validation: 0.00465893373991663]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0034859445339373383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0034859445339373383 | validation: 0.002875008310302206]
	TIME [epoch: 4.29 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002659596215843014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.002659596215843014 | validation: 0.003276102262347105]
	TIME [epoch: 4.29 sec]
EPOCH 33/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0021521145995963345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0021521145995963345 | validation: 0.002362688689901314]
	TIME [epoch: 4.31 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002263462417515391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.002263462417515391 | validation: 0.002119834225474256]
	TIME [epoch: 4.32 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0022796594640640084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0022796594640640084 | validation: 0.002113589904029429]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002059578281691136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.002059578281691136 | validation: 0.002236467846315526]
	TIME [epoch: 4.31 sec]
EPOCH 37/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0018872268233034112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0018872268233034112 | validation: 0.0025833897098910532]
	TIME [epoch: 4.3 sec]
EPOCH 38/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0026011611587907026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0026011611587907026 | validation: 0.0025836130036199405]
	TIME [epoch: 4.3 sec]
EPOCH 39/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001913873116892145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.001913873116892145 | validation: 0.001797628305973996]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/500:
	Training over batches...
		[batch 4/4] avg loss: 0.014189227557398952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014189227557398952 | validation: 0.011082313181195392]
	TIME [epoch: 4.3 sec]
EPOCH 41/500:
	Training over batches...
		[batch 4/4] avg loss: 0.006568780193806673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.006568780193806673 | validation: 0.003570004716927689]
	TIME [epoch: 4.3 sec]
EPOCH 42/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002311214290629355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.002311214290629355 | validation: 0.0035340685336254837]
	TIME [epoch: 4.3 sec]
EPOCH 43/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002433863228069393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.002433863228069393 | validation: 0.0033112364818469555]
	TIME [epoch: 4.31 sec]
EPOCH 44/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002560841361533647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.002560841361533647 | validation: 0.00268484282249286]
	TIME [epoch: 4.32 sec]
EPOCH 45/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004530007625988661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.004530007625988661 | validation: 0.0030825690963172656]
	TIME [epoch: 4.31 sec]
EPOCH 46/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0041391102208025755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0041391102208025755 | validation: 0.002480342040062504]
	TIME [epoch: 4.3 sec]
EPOCH 47/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002049215155440641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.002049215155440641 | validation: 0.0032951822578697236]
	TIME [epoch: 4.3 sec]
EPOCH 48/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0022239195208913903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0022239195208913903 | validation: 0.0016226618648636554]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0023055954311047384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0023055954311047384 | validation: 0.002917009339210715]
	TIME [epoch: 4.3 sec]
EPOCH 50/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0027643143720546487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0027643143720546487 | validation: 0.0032510865535539607]
	TIME [epoch: 4.29 sec]
EPOCH 51/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0030928820803726334		[learning rate: 0.0098855]
	Learning Rate: 0.00988553
	LOSS [training: 0.0030928820803726334 | validation: 0.0035961170171237666]
	TIME [epoch: 4.29 sec]
EPOCH 52/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004656794718663642		[learning rate: 0.0097349]
	Learning Rate: 0.00973494
	LOSS [training: 0.004656794718663642 | validation: 0.011363491174849954]
	TIME [epoch: 4.29 sec]
EPOCH 53/500:
	Training over batches...
		[batch 4/4] avg loss: 0.009302438050657386		[learning rate: 0.0095866]
	Learning Rate: 0.00958665
	LOSS [training: 0.009302438050657386 | validation: 0.0029587027975446862]
	TIME [epoch: 4.3 sec]
EPOCH 54/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0016616063032684428		[learning rate: 0.0094406]
	Learning Rate: 0.00944061
	LOSS [training: 0.0016616063032684428 | validation: 0.002339660704252293]
	TIME [epoch: 4.31 sec]
EPOCH 55/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0044673820439635895		[learning rate: 0.0092968]
	Learning Rate: 0.0092968
	LOSS [training: 0.0044673820439635895 | validation: 0.003410565782580105]
	TIME [epoch: 4.32 sec]
EPOCH 56/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003133924011540997		[learning rate: 0.0091552]
	Learning Rate: 0.00915517
	LOSS [training: 0.003133924011540997 | validation: 0.0031377046327913972]
	TIME [epoch: 4.3 sec]
EPOCH 57/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0025072228891176206		[learning rate: 0.0090157]
	Learning Rate: 0.00901571
	LOSS [training: 0.0025072228891176206 | validation: 0.0025467514877552376]
	TIME [epoch: 4.29 sec]
EPOCH 58/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00212391063351097		[learning rate: 0.0088784]
	Learning Rate: 0.00887837
	LOSS [training: 0.00212391063351097 | validation: 0.002226970963421568]
	TIME [epoch: 4.3 sec]
EPOCH 59/500:
	Training over batches...
		[batch 4/4] avg loss: 0.005404227715729528		[learning rate: 0.0087431]
	Learning Rate: 0.00874312
	LOSS [training: 0.005404227715729528 | validation: 0.009479607227714988]
	TIME [epoch: 4.29 sec]
EPOCH 60/500:
	Training over batches...
		[batch 4/4] avg loss: 0.008259468025691854		[learning rate: 0.0086099]
	Learning Rate: 0.00860994
	LOSS [training: 0.008259468025691854 | validation: 0.005135450900374081]
	TIME [epoch: 4.29 sec]
EPOCH 61/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004063335928168413		[learning rate: 0.0084788]
	Learning Rate: 0.00847878
	LOSS [training: 0.004063335928168413 | validation: 0.002304621621165734]
	TIME [epoch: 4.3 sec]
EPOCH 62/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0024413307549223883		[learning rate: 0.0083496]
	Learning Rate: 0.00834962
	LOSS [training: 0.0024413307549223883 | validation: 0.0018195673891226699]
	TIME [epoch: 4.3 sec]
EPOCH 63/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004019408228542954		[learning rate: 0.0082224]
	Learning Rate: 0.00822243
	LOSS [training: 0.004019408228542954 | validation: 0.0034236865979176117]
	TIME [epoch: 4.3 sec]
EPOCH 64/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0022517197528309626		[learning rate: 0.0080972]
	Learning Rate: 0.00809717
	LOSS [training: 0.0022517197528309626 | validation: 0.0029454145771904616]
	TIME [epoch: 4.29 sec]
EPOCH 65/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0019703591376238148		[learning rate: 0.0079738]
	Learning Rate: 0.00797382
	LOSS [training: 0.0019703591376238148 | validation: 0.0016789693356487416]
	TIME [epoch: 4.32 sec]
EPOCH 66/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0032849378236571255		[learning rate: 0.0078524]
	Learning Rate: 0.00785236
	LOSS [training: 0.0032849378236571255 | validation: 0.00275458646864296]
	TIME [epoch: 4.32 sec]
EPOCH 67/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0022796425010635064		[learning rate: 0.0077327]
	Learning Rate: 0.00773274
	LOSS [training: 0.0022796425010635064 | validation: 0.0014607561723878892]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0033642238735905		[learning rate: 0.0076149]
	Learning Rate: 0.00761494
	LOSS [training: 0.0033642238735905 | validation: 0.003004413137017903]
	TIME [epoch: 4.3 sec]
EPOCH 69/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003621062211179582		[learning rate: 0.0074989]
	Learning Rate: 0.00749894
	LOSS [training: 0.003621062211179582 | validation: 0.0038045308007336608]
	TIME [epoch: 4.29 sec]
EPOCH 70/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002996172318958468		[learning rate: 0.0073847]
	Learning Rate: 0.00738471
	LOSS [training: 0.002996172318958468 | validation: 0.003246571074792093]
	TIME [epoch: 4.29 sec]
EPOCH 71/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003902690404817879		[learning rate: 0.0072722]
	Learning Rate: 0.00727221
	LOSS [training: 0.003902690404817879 | validation: 0.0038215373980473935]
	TIME [epoch: 4.29 sec]
EPOCH 72/500:
	Training over batches...
		[batch 4/4] avg loss: 0.007388249071534112		[learning rate: 0.0071614]
	Learning Rate: 0.00716143
	LOSS [training: 0.007388249071534112 | validation: 0.0036907822225936316]
	TIME [epoch: 4.29 sec]
EPOCH 73/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0025590443572448642		[learning rate: 0.0070523]
	Learning Rate: 0.00705234
	LOSS [training: 0.0025590443572448642 | validation: 0.001917793443256334]
	TIME [epoch: 4.29 sec]
EPOCH 74/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0016993152708092618		[learning rate: 0.0069449]
	Learning Rate: 0.00694491
	LOSS [training: 0.0016993152708092618 | validation: 0.006534939477866507]
	TIME [epoch: 4.29 sec]
EPOCH 75/500:
	Training over batches...
		[batch 4/4] avg loss: 0.009510788454875551		[learning rate: 0.0068391]
	Learning Rate: 0.00683912
	LOSS [training: 0.009510788454875551 | validation: 0.005845021210416138]
	TIME [epoch: 4.3 sec]
EPOCH 76/500:
	Training over batches...
		[batch 4/4] avg loss: 0.006882314749934237		[learning rate: 0.0067349]
	Learning Rate: 0.00673493
	LOSS [training: 0.006882314749934237 | validation: 0.0042615193199774986]
	TIME [epoch: 4.32 sec]
EPOCH 77/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0038510040267832734		[learning rate: 0.0066323]
	Learning Rate: 0.00663234
	LOSS [training: 0.0038510040267832734 | validation: 0.002292048920700936]
	TIME [epoch: 4.3 sec]
EPOCH 78/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003385583136472376		[learning rate: 0.0065313]
	Learning Rate: 0.00653131
	LOSS [training: 0.003385583136472376 | validation: 0.005747995603898986]
	TIME [epoch: 4.29 sec]
EPOCH 79/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003678685879307126		[learning rate: 0.0064318]
	Learning Rate: 0.00643181
	LOSS [training: 0.003678685879307126 | validation: 0.0030196338622374514]
	TIME [epoch: 4.29 sec]
EPOCH 80/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0019002828054642653		[learning rate: 0.0063338]
	Learning Rate: 0.00633383
	LOSS [training: 0.0019002828054642653 | validation: 0.0017201353459827217]
	TIME [epoch: 4.29 sec]
EPOCH 81/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0018032453312840139		[learning rate: 0.0062373]
	Learning Rate: 0.00623735
	LOSS [training: 0.0018032453312840139 | validation: 0.0030597128964843833]
	TIME [epoch: 4.29 sec]
EPOCH 82/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0056299435363209764		[learning rate: 0.0061423]
	Learning Rate: 0.00614233
	LOSS [training: 0.0056299435363209764 | validation: 0.00907940833919773]
	TIME [epoch: 4.29 sec]
EPOCH 83/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00990511435787329		[learning rate: 0.0060488]
	Learning Rate: 0.00604876
	LOSS [training: 0.00990511435787329 | validation: 0.003690364504783176]
	TIME [epoch: 4.29 sec]
EPOCH 84/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00357246325468533		[learning rate: 0.0059566]
	Learning Rate: 0.00595662
	LOSS [training: 0.00357246325468533 | validation: 0.0029461276423564005]
	TIME [epoch: 4.29 sec]
EPOCH 85/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002103887576900973		[learning rate: 0.0058659]
	Learning Rate: 0.00586588
	LOSS [training: 0.002103887576900973 | validation: 0.004491019878186935]
	TIME [epoch: 4.29 sec]
EPOCH 86/500:
	Training over batches...
		[batch 4/4] avg loss: 0.004562652741326284		[learning rate: 0.0057765]
	Learning Rate: 0.00577653
	LOSS [training: 0.004562652741326284 | validation: 0.002732762936864766]
	TIME [epoch: 4.32 sec]
EPOCH 87/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0023059390944351627		[learning rate: 0.0056885]
	Learning Rate: 0.00568853
	LOSS [training: 0.0023059390944351627 | validation: 0.002813615005286112]
	TIME [epoch: 4.31 sec]
EPOCH 88/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0027196367496912636		[learning rate: 0.0056019]
	Learning Rate: 0.00560187
	LOSS [training: 0.0027196367496912636 | validation: 0.0017608758241065487]
	TIME [epoch: 4.3 sec]
EPOCH 89/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0020052054816495973		[learning rate: 0.0055165]
	Learning Rate: 0.00551654
	LOSS [training: 0.0020052054816495973 | validation: 0.0031082211049982566]
	TIME [epoch: 4.3 sec]
EPOCH 90/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003144108285406289		[learning rate: 0.0054325]
	Learning Rate: 0.0054325
	LOSS [training: 0.003144108285406289 | validation: 0.0020649287865640363]
	TIME [epoch: 4.29 sec]
EPOCH 91/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001984357198854431		[learning rate: 0.0053497]
	Learning Rate: 0.00534975
	LOSS [training: 0.001984357198854431 | validation: 0.00299728970824044]
	TIME [epoch: 4.29 sec]
EPOCH 92/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0020029380133645214		[learning rate: 0.0052683]
	Learning Rate: 0.00526825
	LOSS [training: 0.0020029380133645214 | validation: 0.0015172969578978934]
	TIME [epoch: 4.29 sec]
EPOCH 93/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0028855280296485935		[learning rate: 0.005188]
	Learning Rate: 0.005188
	LOSS [training: 0.0028855280296485935 | validation: 0.0030743174273441553]
	TIME [epoch: 4.29 sec]
EPOCH 94/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0017830541446043722		[learning rate: 0.005109]
	Learning Rate: 0.00510897
	LOSS [training: 0.0017830541446043722 | validation: 0.0023036041541046686]
	TIME [epoch: 4.29 sec]
EPOCH 95/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003253096791316334		[learning rate: 0.0050311]
	Learning Rate: 0.00503114
	LOSS [training: 0.003253096791316334 | validation: 0.0023141804051496884]
	TIME [epoch: 4.29 sec]
EPOCH 96/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0025852556039071784		[learning rate: 0.0049545]
	Learning Rate: 0.0049545
	LOSS [training: 0.0025852556039071784 | validation: 0.0023872534373936306]
	TIME [epoch: 4.3 sec]
EPOCH 97/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0020544582414410294		[learning rate: 0.004879]
	Learning Rate: 0.00487903
	LOSS [training: 0.0020544582414410294 | validation: 0.0027211921640575175]
	TIME [epoch: 4.32 sec]
EPOCH 98/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0020331533663891033		[learning rate: 0.0048047]
	Learning Rate: 0.0048047
	LOSS [training: 0.0020331533663891033 | validation: 0.003045182622804462]
	TIME [epoch: 4.32 sec]
EPOCH 99/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0036938216385330224		[learning rate: 0.0047315]
	Learning Rate: 0.00473151
	LOSS [training: 0.0036938216385330224 | validation: 0.0036102863341146655]
	TIME [epoch: 4.3 sec]
EPOCH 100/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0031605281979705663		[learning rate: 0.0046594]
	Learning Rate: 0.00465944
	LOSS [training: 0.0031605281979705663 | validation: 0.0019716006582306093]
	TIME [epoch: 4.3 sec]
EPOCH 101/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0017953079188838834		[learning rate: 0.0045885]
	Learning Rate: 0.00458846
	LOSS [training: 0.0017953079188838834 | validation: 0.002768403893788088]
	TIME [epoch: 4.29 sec]
EPOCH 102/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0017911383958518386		[learning rate: 0.0045186]
	Learning Rate: 0.00451856
	LOSS [training: 0.0017911383958518386 | validation: 0.0022564508037421164]
	TIME [epoch: 4.29 sec]
EPOCH 103/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0020201515953137475		[learning rate: 0.0044497]
	Learning Rate: 0.00444973
	LOSS [training: 0.0020201515953137475 | validation: 0.002230413070148269]
	TIME [epoch: 4.29 sec]
EPOCH 104/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0018608599336285099		[learning rate: 0.0043819]
	Learning Rate: 0.00438194
	LOSS [training: 0.0018608599336285099 | validation: 0.0025144223135526358]
	TIME [epoch: 4.29 sec]
EPOCH 105/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0017745653430231495		[learning rate: 0.0043152]
	Learning Rate: 0.00431519
	LOSS [training: 0.0017745653430231495 | validation: 0.00395751511066086]
	TIME [epoch: 4.29 sec]
EPOCH 106/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0038160144720851756		[learning rate: 0.0042495]
	Learning Rate: 0.00424946
	LOSS [training: 0.0038160144720851756 | validation: 0.0040254302438148945]
	TIME [epoch: 4.29 sec]
EPOCH 107/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0025536459032361515		[learning rate: 0.0041847]
	Learning Rate: 0.00418472
	LOSS [training: 0.0025536459032361515 | validation: 0.0019513940123959755]
	TIME [epoch: 4.29 sec]
EPOCH 108/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001993643060965264		[learning rate: 0.004121]
	Learning Rate: 0.00412098
	LOSS [training: 0.001993643060965264 | validation: 0.0017370529767802019]
	TIME [epoch: 4.31 sec]
EPOCH 109/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0021809690873260297		[learning rate: 0.0040582]
	Learning Rate: 0.0040582
	LOSS [training: 0.0021809690873260297 | validation: 0.0028273770292845672]
	TIME [epoch: 4.32 sec]
EPOCH 110/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0015946359514611248		[learning rate: 0.0039964]
	Learning Rate: 0.00399638
	LOSS [training: 0.0015946359514611248 | validation: 0.0027919699221769173]
	TIME [epoch: 4.3 sec]
EPOCH 111/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0019061457378345328		[learning rate: 0.0039355]
	Learning Rate: 0.0039355
	LOSS [training: 0.0019061457378345328 | validation: 0.0025844916408093617]
	TIME [epoch: 4.29 sec]
EPOCH 112/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0016354459146876393		[learning rate: 0.0038755]
	Learning Rate: 0.00387555
	LOSS [training: 0.0016354459146876393 | validation: 0.0020937925174500313]
	TIME [epoch: 4.3 sec]
EPOCH 113/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0017779913558614898		[learning rate: 0.0038165]
	Learning Rate: 0.00381651
	LOSS [training: 0.0017779913558614898 | validation: 0.002324802618954379]
	TIME [epoch: 4.29 sec]
EPOCH 114/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0025228485462896674		[learning rate: 0.0037584]
	Learning Rate: 0.00375837
	LOSS [training: 0.0025228485462896674 | validation: 0.004054854544991758]
	TIME [epoch: 4.29 sec]
EPOCH 115/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002930319271797271		[learning rate: 0.0037011]
	Learning Rate: 0.00370112
	LOSS [training: 0.002930319271797271 | validation: 0.002836882147756843]
	TIME [epoch: 4.29 sec]
EPOCH 116/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0026569772702612323		[learning rate: 0.0036447]
	Learning Rate: 0.00364474
	LOSS [training: 0.0026569772702612323 | validation: 0.0029651865286516297]
	TIME [epoch: 4.29 sec]
EPOCH 117/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001858184939708016		[learning rate: 0.0035892]
	Learning Rate: 0.00358922
	LOSS [training: 0.001858184939708016 | validation: 0.0016063980592876282]
	TIME [epoch: 4.29 sec]
EPOCH 118/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0019522975533443699		[learning rate: 0.0035345]
	Learning Rate: 0.00353454
	LOSS [training: 0.0019522975533443699 | validation: 0.0019294216916920385]
	TIME [epoch: 4.29 sec]
EPOCH 119/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0018223477944671975		[learning rate: 0.0034807]
	Learning Rate: 0.0034807
	LOSS [training: 0.0018223477944671975 | validation: 0.0020794490636167054]
	TIME [epoch: 4.32 sec]
EPOCH 120/500:
	Training over batches...
		[batch 4/4] avg loss: 0.00185474720706858		[learning rate: 0.0034277]
	Learning Rate: 0.00342768
	LOSS [training: 0.00185474720706858 | validation: 0.0017593508762901197]
	TIME [epoch: 4.32 sec]
EPOCH 121/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0019964633214534957		[learning rate: 0.0033755]
	Learning Rate: 0.00337546
	LOSS [training: 0.0019964633214534957 | validation: 0.002093882148562527]
	TIME [epoch: 4.3 sec]
EPOCH 122/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0027678823972864326		[learning rate: 0.003324]
	Learning Rate: 0.00332404
	LOSS [training: 0.0027678823972864326 | validation: 0.003689860870261968]
	TIME [epoch: 4.29 sec]
EPOCH 123/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0033424993858622498		[learning rate: 0.0032734]
	Learning Rate: 0.00327341
	LOSS [training: 0.0033424993858622498 | validation: 0.004630652481200928]
	TIME [epoch: 4.29 sec]
EPOCH 124/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0034821008217371897		[learning rate: 0.0032235]
	Learning Rate: 0.00322354
	LOSS [training: 0.0034821008217371897 | validation: 0.004214891051348434]
	TIME [epoch: 4.29 sec]
EPOCH 125/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0020444544451947615		[learning rate: 0.0031744]
	Learning Rate: 0.00317444
	LOSS [training: 0.0020444544451947615 | validation: 0.002307021284625444]
	TIME [epoch: 4.3 sec]
EPOCH 126/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0017396936409883376		[learning rate: 0.0031261]
	Learning Rate: 0.00312608
	LOSS [training: 0.0017396936409883376 | validation: 0.00242460143837985]
	TIME [epoch: 4.29 sec]
EPOCH 127/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002179915678320092		[learning rate: 0.0030785]
	Learning Rate: 0.00307846
	LOSS [training: 0.002179915678320092 | validation: 0.00276329160720686]
	TIME [epoch: 4.29 sec]
EPOCH 128/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002285942339688967		[learning rate: 0.0030316]
	Learning Rate: 0.00303156
	LOSS [training: 0.002285942339688967 | validation: 0.002765544212726588]
	TIME [epoch: 4.29 sec]
EPOCH 129/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0017948829551578227		[learning rate: 0.0029854]
	Learning Rate: 0.00298538
	LOSS [training: 0.0017948829551578227 | validation: 0.0024569292572419153]
	TIME [epoch: 4.3 sec]
EPOCH 130/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002142875545092247		[learning rate: 0.0029399]
	Learning Rate: 0.0029399
	LOSS [training: 0.002142875545092247 | validation: 0.002339563207920306]
	TIME [epoch: 4.34 sec]
EPOCH 131/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003338053118124379		[learning rate: 0.0028951]
	Learning Rate: 0.00289512
	LOSS [training: 0.003338053118124379 | validation: 0.0036020347453269213]
	TIME [epoch: 4.3 sec]
EPOCH 132/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003092195194422148		[learning rate: 0.002851]
	Learning Rate: 0.00285102
	LOSS [training: 0.003092195194422148 | validation: 0.0028204257414514754]
	TIME [epoch: 4.3 sec]
EPOCH 133/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002476549988958805		[learning rate: 0.0028076]
	Learning Rate: 0.00280759
	LOSS [training: 0.002476549988958805 | validation: 0.003585963016890755]
	TIME [epoch: 4.29 sec]
EPOCH 134/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0017370723330816214		[learning rate: 0.0027648]
	Learning Rate: 0.00276482
	LOSS [training: 0.0017370723330816214 | validation: 0.0025267438588883317]
	TIME [epoch: 4.29 sec]
EPOCH 135/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0018965661520307922		[learning rate: 0.0027227]
	Learning Rate: 0.0027227
	LOSS [training: 0.0018965661520307922 | validation: 0.002274710893630388]
	TIME [epoch: 4.29 sec]
EPOCH 136/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003506633011484666		[learning rate: 0.0026812]
	Learning Rate: 0.00268123
	LOSS [training: 0.003506633011484666 | validation: 0.0023848899313238096]
	TIME [epoch: 4.29 sec]
EPOCH 137/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002522264792014077		[learning rate: 0.0026404]
	Learning Rate: 0.00264038
	LOSS [training: 0.002522264792014077 | validation: 0.0022700447027071825]
	TIME [epoch: 4.29 sec]
EPOCH 138/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0030282031186132934		[learning rate: 0.0026002]
	Learning Rate: 0.00260016
	LOSS [training: 0.0030282031186132934 | validation: 0.0025329771469546473]
	TIME [epoch: 4.29 sec]
EPOCH 139/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002760487424567612		[learning rate: 0.0025606]
	Learning Rate: 0.00256055
	LOSS [training: 0.002760487424567612 | validation: 0.0021030817171834417]
	TIME [epoch: 4.3 sec]
EPOCH 140/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002571599676694619		[learning rate: 0.0025215]
	Learning Rate: 0.00252154
	LOSS [training: 0.002571599676694619 | validation: 0.0028161900000597314]
	TIME [epoch: 4.3 sec]
EPOCH 141/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0021623906313613514		[learning rate: 0.0024831]
	Learning Rate: 0.00248313
	LOSS [training: 0.0021623906313613514 | validation: 0.0026327072814044178]
	TIME [epoch: 4.33 sec]
EPOCH 142/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0019122908227301775		[learning rate: 0.0024453]
	Learning Rate: 0.00244531
	LOSS [training: 0.0019122908227301775 | validation: 0.0025504372994123456]
	TIME [epoch: 4.3 sec]
EPOCH 143/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0017398176434056524		[learning rate: 0.0024081]
	Learning Rate: 0.00240806
	LOSS [training: 0.0017398176434056524 | validation: 0.0024061844930696546]
	TIME [epoch: 4.3 sec]
EPOCH 144/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001966868294337385		[learning rate: 0.0023714]
	Learning Rate: 0.00237137
	LOSS [training: 0.001966868294337385 | validation: 0.0021842009374092104]
	TIME [epoch: 4.3 sec]
EPOCH 145/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0017260197647308957		[learning rate: 0.0023352]
	Learning Rate: 0.00233525
	LOSS [training: 0.0017260197647308957 | validation: 0.0030573467828799443]
	TIME [epoch: 4.29 sec]
EPOCH 146/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002138222331619853		[learning rate: 0.0022997]
	Learning Rate: 0.00229968
	LOSS [training: 0.002138222331619853 | validation: 0.002710508878177117]
	TIME [epoch: 4.3 sec]
EPOCH 147/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0019374966953286878		[learning rate: 0.0022646]
	Learning Rate: 0.00226464
	LOSS [training: 0.0019374966953286878 | validation: 0.0027857731636504596]
	TIME [epoch: 4.29 sec]
EPOCH 148/500:
	Training over batches...
		[batch 4/4] avg loss: 0.001924589109306038		[learning rate: 0.0022301]
	Learning Rate: 0.00223015
	LOSS [training: 0.001924589109306038 | validation: 0.002317089255035303]
	TIME [epoch: 4.29 sec]
EPOCH 149/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0020354515189543888		[learning rate: 0.0021962]
	Learning Rate: 0.00219617
	LOSS [training: 0.0020354515189543888 | validation: 0.0035209666319072535]
	TIME [epoch: 4.29 sec]
EPOCH 150/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002177615901729335		[learning rate: 0.0021627]
	Learning Rate: 0.00216272
	LOSS [training: 0.002177615901729335 | validation: 0.0022476630271228764]
	TIME [epoch: 4.29 sec]
EPOCH 151/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002731217558111107		[learning rate: 0.0021298]
	Learning Rate: 0.00212977
	LOSS [training: 0.002731217558111107 | validation: 0.0035214233456642785]
	TIME [epoch: 4.32 sec]
EPOCH 152/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0034839761739646406		[learning rate: 0.0020973]
	Learning Rate: 0.00209733
	LOSS [training: 0.0034839761739646406 | validation: 0.0028097681905575825]
	TIME [epoch: 4.32 sec]
EPOCH 153/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002359138116496627		[learning rate: 0.0020654]
	Learning Rate: 0.00206538
	LOSS [training: 0.002359138116496627 | validation: 0.002935677397667755]
	TIME [epoch: 4.3 sec]
EPOCH 154/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0021561554990376574		[learning rate: 0.0020339]
	Learning Rate: 0.00203392
	LOSS [training: 0.0021561554990376574 | validation: 0.002430174182532623]
	TIME [epoch: 4.29 sec]
EPOCH 155/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0020907863276074185		[learning rate: 0.0020029]
	Learning Rate: 0.00200293
	LOSS [training: 0.0020907863276074185 | validation: 0.002679915397112362]
	TIME [epoch: 4.29 sec]
EPOCH 156/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0022665229352823455		[learning rate: 0.0019724]
	Learning Rate: 0.00197242
	LOSS [training: 0.0022665229352823455 | validation: 0.0037623414645578217]
	TIME [epoch: 4.29 sec]
EPOCH 157/500:
	Training over batches...
		[batch 4/4] avg loss: 0.003794071037561571		[learning rate: 0.0019424]
	Learning Rate: 0.00194238
	LOSS [training: 0.003794071037561571 | validation: 0.002617221726461132]
	TIME [epoch: 4.29 sec]
EPOCH 158/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0027843016617346414		[learning rate: 0.0019128]
	Learning Rate: 0.00191279
	LOSS [training: 0.0027843016617346414 | validation: 0.0025228100411093823]
	TIME [epoch: 4.3 sec]
EPOCH 159/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002207337495028488		[learning rate: 0.0018836]
	Learning Rate: 0.00188365
	LOSS [training: 0.002207337495028488 | validation: 0.0018242663775175907]
	TIME [epoch: 4.29 sec]
EPOCH 160/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002459499471838363		[learning rate: 0.001855]
	Learning Rate: 0.00185495
	LOSS [training: 0.002459499471838363 | validation: 0.0024020890011129865]
	TIME [epoch: 4.29 sec]
EPOCH 161/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002306216846429139		[learning rate: 0.0018267]
	Learning Rate: 0.0018267
	LOSS [training: 0.002306216846429139 | validation: 0.0030551532267922687]
	TIME [epoch: 4.29 sec]
EPOCH 162/500:
	Training over batches...
		[batch 4/4] avg loss: 0.002445003570367763		[learning rate: 0.0017989]
	Learning Rate: 0.00179887
	LOSS [training: 0.002445003570367763 | validation: 0.0024156476069637127]
	TIME [epoch: 4.32 sec]
EPOCH 163/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0023336547094403794		[learning rate: 0.0017715]
	Learning Rate: 0.00177147
	LOSS [training: 0.0023336547094403794 | validation: 0.0023797752971013468]
	TIME [epoch: 4.32 sec]
EPOCH 164/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0017806902530710846		[learning rate: 0.0017445]
	Learning Rate: 0.00174448
	LOSS [training: 0.0017806902530710846 | validation: 0.0029077292256121743]
	TIME [epoch: 4.31 sec]
EPOCH 165/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0018105926624611063		[learning rate: 0.0017179]
	Learning Rate: 0.00171791
	LOSS [training: 0.0018105926624611063 | validation: 0.0018804973306800496]
	TIME [epoch: 4.3 sec]
EPOCH 166/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0018601245941691584		[learning rate: 0.0016917]
	Learning Rate: 0.00169174
	LOSS [training: 0.0018601245941691584 | validation: 0.002467693684063262]
	TIME [epoch: 4.3 sec]
EPOCH 167/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0019509896734844663		[learning rate: 0.001666]
	Learning Rate: 0.00166597
	LOSS [training: 0.0019509896734844663 | validation: 0.0020581474235299]
	TIME [epoch: 4.29 sec]
EPOCH 168/500:
	Training over batches...
		[batch 4/4] avg loss: 0.0018912825944570587		[learning rate: 0.0016406]
	Learning Rate: 0.00164059
	LOSS [training: 0.0018912825944570587 | validation: 0.00267070336442753]
	TIME [epoch: 4.3 sec]
	Saving model to: out/model_training/model_algphi2_1a_v_mmd1_20240703_144728/states/model_algphi2_1a_v_mmd1_168.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 840.779 seconds.
